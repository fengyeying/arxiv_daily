<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Mon 30 Oct 23  to  Tue 31 Oct 23, announced Wed,  1 Nov 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item359">Cross-lists</a></li>
<li><a href="#item416">Replacements</a></li>
</ul>
<small>[ total of 703 entries:  <b>1-703</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Wed,  1 Nov 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19801" title="Abstract">arXiv:2310.19801</a> [<a href="/pdf/2310.19801" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SyMPox: An Automated Monkeypox Detection System Based on Symptoms Using  XGBoost
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farzipour%2C+A">Alireza Farzipour</a>, 
<a href="/search/cs?searchtype=author&query=Elmi%2C+R">Roya Elmi</a>, 
<a href="/search/cs?searchtype=author&query=Nasiri%2C+H">Hamid Nasiri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Monkeypox is a zoonotic disease. About 87000 cases of monkeypox were
confirmed by the World Health Organization until 10th June 2023. The most
prevalent methods for identifying this disease are image-based recognition
techniques. Still, they are not too fast and could only be available to a few
individuals. This study presents an independent application named SyMPox,
developed to diagnose Monkeypox cases based on symptoms. SyMPox utilizes the
robust XGBoost algorithm to analyze symptom patterns and provide accurate
assessments. Developed using the Gradio framework, SyMPox offers a
user-friendly platform for individuals to assess their symptoms and obtain
reliable Monkeypox diagnoses.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19802" title="Abstract">arXiv:2310.19802</a> [<a href="/pdf/2310.19802" title="Download PDF">pdf</a>, <a href="/format/2310.19802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Thermodynamics of Learning Generative Parametric  Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parsi%2C+S+S">Shervin Sadat Parsi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We have formulated generative machine learning problems as the time evolution
of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic
process. Then, we have studied the thermodynamic exchange between the model's
parameters, denoted as $\Theta$, and the model's generated samples, denoted as
$X$. We demonstrate that the training dataset and the action of the Stochastic
Gradient Descent (SGD) optimizer serve as a work source that governs the time
evolution of these two subsystems. Our findings reveal that the model learns
through the dissipation of heat during the generation of samples $X$, leading
to an increase in the entropy of the model's parameters, $\Theta$. Thus, the
parameter subsystem acts as a heat reservoir, effectively storing the learned
information. Furthermore, the role of the model's parameters as a heat
reservoir provides valuable thermodynamic insights into the generalization
power of over-parameterized models. This approach offers an unambiguous
framework for computing information-theoretic quantities within deterministic
neural networks by establishing connections with thermodynamic variables. To
illustrate the utility of this framework, we introduce two
information-theoretic metrics: Memorized-information (M-info) and
Learned-information (L-info), which trace the dynamic flow of information
during the learning process of PPMs.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19803" title="Abstract">arXiv:2310.19803</a> [<a href="/pdf/2310.19803" title="Download PDF">pdf</a>, <a href="/format/2310.19803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShanshuiDaDA: An Interactive, Generative System towards Chinese Shanshui  Painting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A+L">Aven Le Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiufeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+C">Cheng-Hung Lo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaizhu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, Machine Learning for Creativity and Design Workshop, the 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montreal, Canada. See: <a href="https://nips2018creativity.github.io/doc/shanshui_dada.pdf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Human-Computer Interaction (cs.HC); Multimedia (cs.MM)

</div>
<p class="mathjax">Shanshui, which means mountain and water, is an East Asian traditional brush
painting involving natural landscapes. This paper proposes an interactive and
generative system based on a Generative Adversarial Network(GAN), which helps
users draw Shanshui easily. We name this system and installation ShanshuiDaDA.
ShanshuiDaDA is trained with CycleGAN and wrapped with a web-based interface.
When participants scribble lines and sketch the landscape, the ShanshuiDaDA
will assist them in generating and creating a Chinese "Shanshui" painting in
real time.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19804" title="Abstract">arXiv:2310.19804</a> [<a href="/pdf/2310.19804" title="Download PDF">pdf</a>, <a href="/format/2310.19804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Kernel Perspective on Behavioural Metrics for Markov Decision  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
<a href="/search/cs?searchtype=author&query=Kastner%2C+T">Tyler Kastner</a>, 
<a href="/search/cs?searchtype=author&query=Panangaden%2C+P">Prakash Panangaden</a>, 
<a href="/search/cs?searchtype=author&query=Rowland%2C+M">Mark Rowland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Behavioural metrics have been shown to be an effective mechanism for
constructing representations in reinforcement learning. We present a novel
perspective on behavioural metrics for Markov decision processes via the use of
positive definite kernels. We leverage this new perspective to define a new
metric that is provably equivalent to the recently introduced MICo distance
(Castro et al., 2021). The kernel perspective further enables us to provide new
theoretical results, which has so far eluded prior work. These include bounding
value function differences by means of our metric, and the demonstration that
our metric can be provably embedded into a finite-dimensional Euclidean space
with low distortion error. These are two crucial properties when using
behavioural metrics for reinforcement learning representations. We complement
our theory with strong empirical results that demonstrate the effectiveness of
these methods in practice.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19805" title="Abstract">arXiv:2310.19805</a> [<a href="/pdf/2310.19805" title="Download PDF">pdf</a>, <a href="/format/2310.19805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SERA:Sample Efficient Reward Augmentation in offline-to-online  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+X">Xiao Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zifeng Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Donglin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 11 Figures, and 6 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A prospective application of offline reinforcement learning (RL) involves
initializing a pre-trained policy using existing static datasets for subsequent
online fine-tuning. However, direct fine-tuning of the offline pre-trained
policy often results in sub-optimal performance. A primary reason is that
offline conservative methods diminish the agent's capability of exploration,
thereby impacting online fine-tuning performance. To enhance exploration during
online fine-tuning and thus enhance the overall online fine-tuning performance,
we introduce a generalized reward augmentation framework called Sample
Efficient Reward Augmentation (SERA). SERA aims to improve the performance of
online fine-tuning by designing intrinsic rewards that encourage the agent to
explore. Specifically, it implicitly implements State Marginal Matching (SMM)
and penalizes out-of-distribution (OOD) state actions, thus encouraging agents
to cover the target state density, and achieving better online fine-tuning
results. Additionally, SERA can be effortlessly plugged into various RL
algorithms to improve online fine-tuning and ensure sustained asymptotic
improvement, showing the versatility as well as the effectiveness of SERA.
Moreover, extensive experimental results will demonstrate that when conducting
offline-to-online problems, SERA consistently and effectively enhances the
performance of various offline algorithms.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19806" title="Abstract">arXiv:2310.19806</a> [<a href="/pdf/2310.19806" title="Download PDF">pdf</a>, <a href="/format/2310.19806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Verification of Equivalence Properties in Advanced Logic  Programs -- Bachelor Thesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heuer%2C+J">Jan Heuer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Bachelor Thesis at the University of Potsdam
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the increase in industrial applications using Answer Set Programming,
the need for formal verification tools, particularly for critical applications,
has also increased. During the program optimisation process, it would be
desirable to have a tool which can automatically verify whether an optimised
subprogram can replace the original subprogram. Formally this corresponds to
the problem of verifying the strong equivalence of two programs. In order to do
so, the translation tool anthem was developed. It can be used in conjunction
with an automated theorem prover for classical logic to verify that two
programs are strongly equivalent. With the current version of anthem, only the
strong equivalence of positive programs with a restricted input language can be
verified. This is a result of the translation $\tau^*$ implemented in anthem
that produces formulas in the logic of here-and-there, which coincides with
classical logic only for positive programs. This thesis extends anthem in order
to overcome these limitations. First, the transformation $\sigma^*$ is
presented, which transforms formulas from the logic of here-and-there to
classical logic. A theorem formalises how $\sigma^*$ can be used to express
equivalence in the logic of here-and-there in classical logic. Second, the
translation $\tau^*$ is extended to programs containing pools. Another theorem
shows how $\sigma^*$ can be combined with $\tau^*$ to express the strong
equivalence of two programs in classical logic. With $\sigma^*$ and the
extended $\tau^*$, it is possible to express the strong equivalence of logic
programs containing negation, simple choices, and pools. Both the extended
$\tau^*$ and $\sigma^*$ are implemented in a new version of anthem. Several
examples of logic programs containing pools, negation, and simple choice rules,
which the new version of anthem can translate to classical logic, are
presented. Some a...
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19807" title="Abstract">arXiv:2310.19807</a> [<a href="/pdf/2310.19807" title="Download PDF">pdf</a>, <a href="/format/2310.19807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Communication Efficiency in Federated Natural Policy Gradient  via ADMM-based Gradient Updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+G">Guangchen Lan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+J">James Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C">Christopher Brinton</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Federated reinforcement learning (FedRL) enables agents to collaboratively
train a global policy without sharing their individual data. However, high
communication overhead remains a critical bottleneck, particularly for natural
policy gradient (NPG) methods, which are second-order. To address this issue,
we propose the FedNPG-ADMM framework, which leverages the alternating direction
method of multipliers (ADMM) to approximate global NPG directions efficiently.
We theoretically demonstrate that using ADMM-based gradient updates reduces
communication complexity from ${O}({d^{2}})$ to ${O}({d})$ at each iteration,
where $d$ is the number of model parameters. Furthermore, we show that
achieving an $\epsilon$-error stationary convergence requires
${O}(\frac{1}{(1-\gamma)^{2}{\epsilon}})$ iterations for discount factor
$\gamma$, demonstrating that FedNPG-ADMM maintains the same convergence rate as
the standard FedNPG. Through evaluation of the proposed algorithms in MuJoCo
environments, we demonstrate that FedNPG-ADMM maintains the reward performance
of standard FedNPG, and that its convergence rate improves when the number of
federated agents increases.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19809" title="Abstract">arXiv:2310.19809</a> [<a href="/pdf/2310.19809" title="Download PDF">pdf</a>, <a href="/format/2310.19809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MgNO: Efficient Parameterization of Linear Operators via Multigrid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Juncai He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinchao Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this work, we propose a concise neural operator architecture for operator
learning. Drawing an analogy with a conventional fully connected neural
network, we define the neural operator as follows: the output of the $i$-th
neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) =
\sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here,
$\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input
neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form
of a function rather than a scalar. Given its new universal approximation
property, the efficient parameterization of the bounded linear operators
between two neurons (Banach spaces) plays a critical role. As a result, we
introduce MgNO, utilizing multigrid structures to parameterize these linear
operators between neurons. This approach offers both mathematical rigor and
practical expressivity. Additionally, MgNO obviates the need for conventional
lifting and projecting operators typically required in previous neural
operators. Moreover, it seamlessly accommodates diverse boundary conditions.
Our empirical observations reveal that MgNO exhibits superior ease of training
compared to other CNN-based models, while also displaying a reduced
susceptibility to overfitting when contrasted with spectral-type neural
operators. We demonstrate the efficiency and accuracy of our method with
consistently state-of-the-art performance on different types of partial
differential equations (PDEs).
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19810" title="Abstract">arXiv:2310.19810</a> [<a href="/pdf/2310.19810" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advantages of Machine Learning in Bus Transport Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roshanzamir%2C+A">Amirsadegh Roshanzamir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Supervised Machine Learning is an innovative method that aims to mimic human
learning by using past experiences. In this study, we utilize supervised
machine learning algorithms to analyze the factors that contribute to the
punctuality of Tehran BRT bus system. We gather publicly available datasets of
2020 to 2022 from Municipality of Tehran to train and test our models. By
employing various algorithms and leveraging Python's Sci Kit Learn and Stats
Models libraries, we construct accurate models capable of predicting whether a
bus route will meet the prescribed standards for on-time performance on any
given day. Furthermore, we delve deeper into the decision-making process of
each algorithm to determine the most influential factor it considers. This
investigation allows us to uncover the key feature that significantly impacts
the effectiveness of bus routes, providing valuable insights for improving
their performance.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19811" title="Abstract">arXiv:2310.19811</a> [<a href="/pdf/2310.19811" title="Download PDF">pdf</a>, <a href="/ps/2310.19811" title="Download PostScript">ps</a>, <a href="/format/2310.19811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Historical Context for Data Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zliobaite%2C+I">Indre Zliobaite</a>, 
<a href="/search/cs?searchtype=author&query=Read%2C+J">Jesse Read</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Databases (cs.DB); Systems and Control (eess.SY)

</div>
<p class="mathjax">Machine learning from data streams is an active and growing research area.
Research on learning from streaming data typically makes strict assumptions
linked to computational resource constraints, including requirements for stream
mining algorithms to inspect each instance not more than once and be ready to
give a prediction at any time. Here we review the historical context of data
streams research placing the common assumptions used in machine learning over
data streams in their historical context.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19813" title="Abstract">arXiv:2310.19813</a> [<a href="/pdf/2310.19813" title="Download PDF">pdf</a>, <a href="/ps/2310.19813" title="Download PostScript">ps</a>, <a href="/format/2310.19813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Genetic Improvement Mutations Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brownlee%2C+A+E+I">Alexander E.I. Brownlee</a>, 
<a href="/search/cs?searchtype=author&query=Callan%2C+J">James Callan</a>, 
<a href="/search/cs?searchtype=author&query=Even-Mendoza%2C+K">Karine Even-Mendoza</a>, 
<a href="/search/cs?searchtype=author&query=Geiger%2C+A">Alina Geiger</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+C">Carol Hanna</a>, 
<a href="/search/cs?searchtype=author&query=Petke%2C+J">Justyna Petke</a>, 
<a href="/search/cs?searchtype=author&query=Sarro%2C+F">Federica Sarro</a>, 
<a href="/search/cs?searchtype=author&query=Sobania%2C+D">Dominik Sobania</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the Symposium on Search-Based Software Engineering (SSBSE) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Large language models (LLMs) have been successfully applied to software
engineering tasks, including program repair. However, their application in
search-based techniques such as Genetic Improvement (GI) is still largely
unexplored. In this paper, we evaluate the use of LLMs as mutation operators
for GI to improve the search process. We expand the Gin Java GI toolkit to call
OpenAI's API to generate edits for the JCodec tool. We randomly sample the
space of edits using 5 different edit types. We find that the number of patches
passing unit tests is up to 75% higher with LLM-based edits than with standard
Insert edits. Further, we observe that the patches found with LLMs are
generally less diverse compared to standard edits. We ran GI with local search
to find runtime improvements. Although many improving patches are found by
LLM-enhanced GI, the best improving patch was found by standard GI.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19814" title="Abstract">arXiv:2310.19814</a> [<a href="/pdf/2310.19814" title="Download PDF">pdf</a>, <a href="/format/2310.19814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Gradient Fields for Scalable and Generalizable Irregular  Packing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+T">Tianyang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mingdong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baoquan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">The packing problem, also known as cutting or nesting, has diverse
applications in logistics, manufacturing, layout design, and atlas generation.
It involves arranging irregularly shaped pieces to minimize waste while
avoiding overlap. Recent advances in machine learning, particularly
reinforcement learning, have shown promise in addressing the packing problem.
In this work, we delve deeper into a novel machine learning-based approach that
formulates the packing problem as conditional generative modeling. To tackle
the challenges of irregular packing, including object validity constraints and
collision avoidance, our method employs the score-based diffusion model to
learn a series of gradient fields. These gradient fields encode the
correlations between constraint satisfaction and the spatial relationships of
polygons, learned from teacher examples. During the testing phase, packing
solutions are generated using a coarse-to-fine refinement mechanism guided by
the learned gradient fields. To enhance packing feasibility and optimality, we
introduce two key architectural designs: multi-scale feature extraction and
coarse-to-fine relation extraction. We conduct experiments on two typical
industrial packing domains, considering translations only. Empirically, our
approach demonstrates spatial utilization rates comparable to, or even
surpassing, those achieved by the teacher algorithm responsible for training
data generation. Additionally, it exhibits some level of generalization to
shape variations. We are hopeful that this method could pave the way for new
possibilities in solving the packing problem.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19815" title="Abstract">arXiv:2310.19815</a> [<a href="/pdf/2310.19815" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training binary neural networks without floating point precision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fontana%2C+F">Federico Fontana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 74 pages, Master's thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">The main goal of this work is to improve the efficiency of training binary
neural networks, which are low latency and low energy networks. The main
contribution of this work is the proposal of two solutions comprised of
topology changes and strategy training that allow the network to achieve near
the state-of-the-art performance and efficient training. The time required for
training and the memory required in the process are two factors that contribute
to efficient training.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19816" title="Abstract">arXiv:2310.19816</a> [<a href="/pdf/2310.19816" title="Download PDF">pdf</a>, <a href="/format/2310.19816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking GPUs on SVBRDF Extractor Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kandel%2C+N">Narayan Kandel</a>, 
<a href="/search/cs?searchtype=author&query=Lambert%2C+M">Melanie Lambert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the maturity of deep learning, its use is emerging in every field. Also,
as different types of GPUs are becoming more available in the markets, it
creates a difficult decision for users. How can users select GPUs to achieve
optimal performance for a specific task? Analysis of GPU architecture is well
studied, but existing works that benchmark GPUs do not study tasks for networks
with significantly larger input. In this work, we tried to differentiate the
performance of different GPUs on neural network models that operate on bigger
input images (256x256).
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19818" title="Abstract">arXiv:2310.19818</a> [<a href="/pdf/2310.19818" title="Download PDF">pdf</a>, <a href="/format/2310.19818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> piHyFlow Operational Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barros%2C+F+J">Fernando J. Barros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Simulation models have been described using different perspectives, or
worldviews. In the process interaction world view (PI), every entity is modeled
by a sequence of actions describing its life cycle, offering a comprehensive
model that groups the events involving each entity. In this paper we describe
piHyFlow, a formalism for representing hybrid models using a set of
communicating processes. This set is dynamic, enabling processes to be created
and destroyed at runtime. Processes are encapsulated into piHyFlow base models
and communicate through shared memory. piHyFlow, however, can guarantee
modularity by enforcing that models can only communicate by input and output
interfaces. piHyFlow extends current PI approaches by providing support for
HyFlow concepts of sampling and dense (continuous) outputs, in addition to the
more traditional event-based communication. In this paper we present piHyFlow
operational semantics using the concepts of simulator and component.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19819" title="Abstract">arXiv:2310.19819</a> [<a href="/pdf/2310.19819" title="Download PDF">pdf</a>, <a href="/format/2310.19819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning and Knowledge: Why Robustness Matters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vandenburgh%2C+J">Jonathan Vandenburgh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Trusting machine learning algorithms requires having confidence in their
outputs. Confidence is typically interpreted in terms of model reliability,
where a model is reliable if it produces a high proportion of correct outputs.
However, model reliability does not address concerns about the robustness of
machine learning models, such as models relying on the wrong features or
variations in performance based on context. I argue that the epistemic
dimension of trust can instead be understood through the concept of knowledge,
where the trustworthiness of an algorithm depends on whether its users are in
the position to know that its outputs are correct. Knowledge requires beliefs
to be formed for the right reasons and to be robust to error, so machine
learning algorithms can only provide knowledge if they work well across
counterfactual scenarios and if they make decisions based on the right
features. This, I argue, can explain why we should care about model properties
like interpretability, causal shortcut independence, and distribution shift
robustness even if such properties are not required for model reliability.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19820" title="Abstract">arXiv:2310.19820</a> [<a href="/pdf/2310.19820" title="Download PDF">pdf</a>, <a href="/format/2310.19820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shunyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yonggan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dass%2C+J">Jyotikrishna Dass</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+H">Haoran You</a>, 
<a href="/search/cs?searchtype=author&query=Yingyan">Yingyan</a> (Celine)Lin
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Boosting the task accuracy of tiny neural networks (TNNs) has become a
fundamental challenge for enabling the deployments of TNNs on edge devices
which are constrained by strict limitations in terms of memory, computation,
bandwidth, and power supply. To this end, we propose a framework called
NetDistiller to boost the achievable accuracy of TNNs by treating them as
sub-networks of a weight-sharing teacher constructed by expanding the number of
channels of the TNN. Specifically, the target TNN model is jointly trained with
the weight-sharing teacher model via (1) gradient surgery to tackle the
gradient conflicts between them and (2) uncertainty-aware distillation to
mitigate the overfitting of the teacher model. Extensive experiments across
diverse tasks validate NetDistiller's effectiveness in boosting TNNs'
achievable accuracy over state-of-the-art methods. Our code is available at
https://github.com/GATECH-EIC/NetDistiller.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19821" title="Abstract">arXiv:2310.19821</a> [<a href="/pdf/2310.19821" title="Download PDF">pdf</a>, <a href="/format/2310.19821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Risk-Averse Framework for Non-Stationary Stochastic Multi-Armed  Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alami%2C+R">Reda Alami</a>, 
<a href="/search/cs?searchtype=author&query=Mahfoud%2C+M">Mohammed Mahfoud</a>, 
<a href="/search/cs?searchtype=author&query=Achab%2C+M">Mastane Achab</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In a typical stochastic multi-armed bandit problem, the objective is often to
maximize the expected sum of rewards over some time horizon $T$. While the
choice of a strategy that accomplishes that is optimal with no additional
information, it is no longer the case when provided additional
environment-specific knowledge. In particular, in areas of high volatility like
healthcare or finance, a naive reward maximization approach often does not
accurately capture the complexity of the learning problem and results in
unreliable solutions. To tackle problems of this nature, we propose a framework
of adaptive risk-aware strategies that operate in non-stationary environments.
Our framework incorporates various risk measures prevalent in the literature to
map multiple families of multi-armed bandit algorithms into a risk-sensitive
setting. In addition, we equip the resulting algorithms with the Restarted
Bayesian Online Change-Point Detection (R-BOCPD) algorithm and impose a
(tunable) forced exploration strategy to detect local (per-arm) switches. We
provide finite-time theoretical guarantees and an asymptotic regret bound of
order $\tilde O(\sqrt{K_T T})$ up to time horizon $T$ with $K_T$ the total
number of change-points. In practice, our framework compares favorably to the
state-of-the-art in both synthetic and real-world environments and manages to
perform efficiently with respect to both risk-sensitivity and non-stationarity.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19822" title="Abstract">arXiv:2310.19822</a> [<a href="/pdf/2310.19822" title="Download PDF">pdf</a>, <a href="/format/2310.19822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuXi-Extreme: Improving extreme rainfall and wind forecasts with  diffusion model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xiaohui Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chensen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph); Applications (stat.AP)

</div>
<p class="mathjax">Significant advancements in the development of machine learning (ML) models
for weather forecasting have produced remarkable results. State-of-the-art
ML-based weather forecast models, such as FuXi, have demonstrated superior
statistical forecast performance in comparison to the high-resolution forecasts
(HRES) of the European Centre for Medium-Range Weather Forecasts (ECMWF).
However, ML models face a common challenge: as forecast lead times increase,
they tend to generate increasingly smooth predictions, leading to an
underestimation of the intensity of extreme weather events. To address this
challenge, we developed the FuXi-Extreme model, which employs a denoising
diffusion probabilistic model (DDPM) to restore finer-scale details in the
surface forecast data generated by the FuXi model in 5-day forecasts. An
evaluation of extreme total precipitation ($\textrm{TP}$), 10-meter wind speed
($\textrm{WS10}$), and 2-meter temperature ($\textrm{T2M}$) illustrates the
superior performance of FuXi-Extreme over both FuXi and HRES. Moreover, when
evaluating tropical cyclone (TC) forecasts based on International Best Track
Archive for Climate Stewardship (IBTrACS) dataset, both FuXi and FuXi-Extreme
shows superior performance in TC track forecasts compared to HRES, but they
show inferior performance in TC intensity forecasts in comparison to HRES.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19825" title="Abstract">arXiv:2310.19825</a> [<a href="/pdf/2310.19825" title="Download PDF">pdf</a>, <a href="/format/2310.19825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of the Security Challenges and Requirements for IoT Operating  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jawad%2C+A">Alvi Jawad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The Internet of Things (IoT) is becoming an integral part of our modern lives
as we converge towards a world surrounded by ubiquitous connectivity. The
inherent complexity presented by the vast IoT ecosystem ends up in an
insufficient understanding of individual system components and their
interactions, leading to numerous security challenges. In order to create a
secure IoT platform from the ground up, there is a need for a unifying
operating system (OS) that can act as a cornerstone regulating the development
of stable and secure solutions. In this paper, we present a classification of
the security challenges stemming from the manifold aspects of IoT development.
We also specify security requirements to direct the secure development of an
unifying IoT OS to resolve many of those ensuing challenges. Survey of several
modern IoT OSs confirm that while the developers of the OSs have taken many
alternative approaches to implement security, we are far from engineering an
adequately secure and unified architecture. More broadly, the study presented
in this paper can help address the growing need for a secure and unified
platform to base IoT development on and assure the safe, secure, and reliable
operation of IoT in critical domains.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19833" title="Abstract">arXiv:2310.19833</a> [<a href="/pdf/2310.19833" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomial Time Algorithm for Boolean Satisfiability Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Margaryan%2C+S+G">Stepan G. Margaryan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">This is the latest in a series of articles aimed at exploring the
relationship between the complexity classes of P and NP. In the previous
papers, we have proved that the sat CNF problem is polynomially reduced to the
problem of finding a special covering for a set under the special decomposition
of this set and vice versa. That is, these problems are polinomially
equivalent. This means that the problem of finding a special covering for a set
under a special decomposition of this set, is an NP-complete problem. We also
described algorithmic procedures that determine whether there is a special
covering for a set under a special decomposition of this set. In this article
we prove that all these algorithmic procedures have polynomial time complexity
with respect to the length of input data. In addition, we will describe an
algorithm that, given any Boolean function in conjunctive normal form (CNF),
determines in polynomial time whether this function is satisfiable. We will
prove that the time complexity of this algorithm is bounded by the cube of the
length of the input data. Also, if the function is not satisfiable, the
algorithm deduces this result noting the reason for this result. We have
implemented an algorithm in Python, and successfully tested it on Boolean
functions represented in CNF with tens of thousands of variables and tens of
thousands of clauses.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19834" title="Abstract">arXiv:2310.19834</a> [<a href="/pdf/2310.19834" title="Download PDF">pdf</a>, <a href="/format/2310.19834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination  Datasets based Recommendation System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shakshi Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+A">Anwitaman Datta</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rajesh Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Misinformation has emerged as a major societal threat in recent years in
general; specifically in the context of the COVID-19 pandemic, it has wrecked
havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable
solutions for combating misinformation are the need of the hour. This work
explored how existing information obtained from social media and augmented with
more curated fact checked data repositories can be harnessed to facilitate
automated rebuttal of misinformation at scale. While the ideas herein can be
generalized and reapplied in the broader context of misinformation mitigation
using a multitude of information sources and catering to the spectrum of social
media platforms, this work serves as a proof of concept, and as such, it is
confined in its scope to only rebuttal of tweets, and in the specific context
of misinformation regarding COVID-19. It leverages two publicly available
datasets, viz. FaCov (fact-checked articles) and misleading (social media
Twitter) data on COVID-19 Vaccination.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19837" title="Abstract">arXiv:2310.19837</a> [<a href="/pdf/2310.19837" title="Download PDF">pdf</a>, <a href="/format/2310.19837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Variable-Length Coding with Zero Leakage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zamani%2C+A">Amirreza Zamani</a>, 
<a href="/search/cs?searchtype=author&query=Oechtering%2C+T+J">Tobias J. Oechtering</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnd%C3%BCz%2C+D">Deniz G&#xfc;nd&#xfc;z</a>, 
<a href="/search/cs?searchtype=author&query=Skoglund%2C+M">Mikael Skoglund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2306.13184">arXiv:2306.13184</a>, <a href="/abs/2309.09034">arXiv:2309.09034</a>, <a href="/abs/2211.15525">arXiv:2211.15525</a>, <a href="/abs/2310.19122">arXiv:2310.19122</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A private compression design problem is studied, where an encoder observes
useful data $Y$, wishes to compress it using variable length code and
communicates it through an unsecured channel. Since $Y$ is correlated with
private attribute $X$, the encoder uses a private compression mechanism to
design encoded message $\cal C$ and sends it over the channel. An adversary is
assumed to have access to the output of the encoder, i.e., $\cal C$, and tries
to estimate $X$. Furthermore, it is assumed that both encoder and decoder have
access to a shared secret key $W$. The design goal is to encode message $\cal
C$ with minimum possible average length that satisfies a perfect privacy
constraint. To do so we first consider two different privacy mechanism design
problems and find upper bounds on the entropy of the optimizers by solving a
linear program. We use the obtained optimizers to design $\cal C$. In two cases
we strengthen the existing bounds: 1. $|\mathcal{X}|\geq |\mathcal{Y}|$; 2. The
realization of $(X,Y)$ follows a specific joint distribution. In particular,
considering the second case we use two-part construction coding to achieve the
upper bounds. Furthermore, in a numerical example we study the obtained bounds
and show that they can improve the existing results.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19841" title="Abstract">arXiv:2310.19841</a> [<a href="/pdf/2310.19841" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An interpretable clustering approach to safety climate analysis:  examining driver group distinction in safety climate perceptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kailai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tianxiang Lan</a>, 
<a href="/search/cs?searchtype=author&query=Goh%2C+Y+M">Yang Miang Goh</a>, 
<a href="/search/cs?searchtype=author&query=Safiena%2C+S">Sufiana Safiena</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yueng-Hsiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lytle%2C+B">Bailey Lytle</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yimin He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Journal:Accident Analysis and Prevention
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The transportation industry, particularly the trucking sector, is prone to
workplace accidents and fatalities. Accidents involving large trucks accounted
for a considerable percentage of overall traffic fatalities. Recognizing the
crucial role of safety climate in accident prevention, researchers have sought
to understand its factors and measure its impact within organizations. While
existing data-driven safety climate studies have made remarkable progress,
clustering employees based on their safety climate perception is innovative and
has not been extensively utilized in research. Identifying clusters of drivers
based on their safety climate perception allows the organization to profile its
workforce and devise more impactful interventions. The lack of utilizing the
clustering approach could be due to difficulties interpreting or explaining the
factors influencing employees' cluster membership. Moreover, existing
safety-related studies did not compare multiple clustering algorithms,
resulting in potential bias. To address these issues, this study introduces an
interpretable clustering approach for safety climate analysis. This study
compares 5 algorithms for clustering truck drivers based on their safety
climate perceptions. It proposes a novel method for quantitatively evaluating
partial dependence plots (QPDP). To better interpret the clustering results,
this study introduces different interpretable machine learning measures (SHAP,
PFI, and QPDP). Drawing on data collected from more than 7,000 American truck
drivers, this study significantly contributes to the scientific literature. It
highlights the critical role of supervisory care promotion in distinguishing
various driver groups. The Python code is available at
https://github.com/NUS-DBE/truck-driver-safety-climate.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19842" title="Abstract">arXiv:2310.19842</a> [<a href="/pdf/2310.19842" title="Download PDF">pdf</a>, <a href="/format/2310.19842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Musical Form Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atassi%2C+L">Lilac Atassi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">While recent generative models can produce engaging music, their utility is
limited. The variation in the music is often left to chance, resulting in
compositions that lack structure. Pieces extending beyond a minute can become
incoherent or repetitive. This paper introduces an approach for generating
structured, arbitrarily long musical pieces. Central to this approach is the
creation of musical segments using a conditional generative model, with
transitions between these segments. The generation of prompts that determine
the high-level composition is distinct from the creation of finer, lower-level
details. A large language model is then used to suggest the musical form.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19843" title="Abstract">arXiv:2310.19843</a> [<a href="/pdf/2310.19843" title="Download PDF">pdf</a>, <a href="/format/2310.19843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling the Telemarketing Process using Genetic Algorithms and Extreme  Boosting: Feature Selection and Cost-Sensitive Analytical Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghatasheh%2C+N">Nazeeh Ghatasheh</a>, 
<a href="/search/cs?searchtype=author&query=Altaharwa%2C+I">Ismail Altaharwa</a>, 
<a href="/search/cs?searchtype=author&query=Aldebei%2C+K">Khaled Aldebei</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, vol. 11, pp. 67806-67824, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Currently, almost all direct marketing activities take place virtually rather
than in person, weakening interpersonal skills at an alarming pace.
Furthermore, businesses have been striving to sense and foster the tendency of
their clients to accept a marketing offer. The digital transformation and the
increased virtual presence forced firms to seek novel marketing research
approaches. This research aims at leveraging the power of telemarketing data in
modeling the willingness of clients to make a term deposit and finding the most
significant characteristics of the clients. Real-world data from a Portuguese
bank and national socio-economic metrics are used to model the telemarketing
decision-making process. This research makes two key contributions. First,
propose a novel genetic algorithm-based classifier to select the best
discriminating features and tune classifier parameters simultaneously. Second,
build an explainable prediction model. The best-generated classification models
were intensively validated using 50 times repeated 10-fold stratified
cross-validation and the selected features have been analyzed. The models
significantly outperform the related works in terms of class of interest
accuracy, they attained an average of 89.07\% and 0.059 in terms of geometric
mean and type I error respectively. The model is expected to maximize the
potential profit margin at the least possible cost and provide more insights to
support marketing decision-making.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19845" title="Abstract">arXiv:2310.19845</a> [<a href="/pdf/2310.19845" title="Download PDF">pdf</a>, <a href="/format/2310.19845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modified Genetic Algorithm for Feature Selection and Hyper Parameter  Optimization: Case of XGBoost in Spam Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghatasheh%2C+N">Nazeeh Ghatasheh</a>, 
<a href="/search/cs?searchtype=author&query=Altaharwa%2C+I">Ismail Altaharwa</a>, 
<a href="/search/cs?searchtype=author&query=Aldebei%2C+K">Khaled Aldebei</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, 2022, vol. 10, pp. 84365-84383
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Recently, spam on online social networks has attracted attention in the
research and business world. Twitter has become the preferred medium to spread
spam content. Many research efforts attempted to encounter social networks
spam. Twitter brought extra challenges represented by the feature space size,
and imbalanced data distributions. Usually, the related research works focus on
part of these main challenges or produce black-box models. In this paper, we
propose a modified genetic algorithm for simultaneous dimensionality reduction
and hyper parameter optimization over imbalanced datasets. The algorithm
initialized an eXtreme Gradient Boosting classifier and reduced the features
space of tweets dataset; to generate a spam prediction model. The model is
validated using a 50 times repeated 10-fold stratified cross-validation, and
analyzed using nonparametric statistical tests. The resulted prediction model
attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy
respectively, utilizing less than 10\% of the total feature space. The
empirical results show that the modified genetic algorithm outperforms $Chi^2$
and $PCA$ feature selection methods. In addition, eXtreme Gradient Boosting
outperforms many machine learning algorithms, including BERT-based deep
learning model, in spam prediction. Furthermore, the proposed approach is
applied to SMS spam modeling and compared to related works.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19848" title="Abstract">arXiv:2310.19848</a> [<a href="/pdf/2310.19848" title="Download PDF">pdf</a>, <a href="/format/2310.19848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Exploration in Continuous-time Model-based Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Treven%2C+L">Lenart Treven</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCbotter%2C+J">Jonas H&#xfc;botter</a>, 
<a href="/search/cs?searchtype=author&query=Sukhija%2C+B">Bhavya Sukhija</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Optimization and Control (math.OC)

</div>
<p class="mathjax">Reinforcement learning algorithms typically consider discrete-time dynamics,
even though the underlying systems are often continuous in time. In this paper,
we introduce a model-based reinforcement learning algorithm that represents
continuous-time dynamics using nonlinear ordinary differential equations
(ODEs). We capture epistemic uncertainty using well-calibrated probabilistic
models, and use the optimistic principle for exploration. Our regret bounds
surface the importance of the measurement selection strategy(MSS), since in
continuous time we not only must decide how to explore, but also when to
observe the underlying system. Our analysis demonstrates that the regret is
sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of
MSS, such as equidistant sampling. Additionally, we propose an adaptive,
data-dependent, practical MSS that, when combined with GP dynamics, also
achieves sublinear regret with significantly fewer samples. We showcase the
benefits of continuous-time modeling over its discrete-time counterpart, as
well as our proposed adaptive MSS over standard baselines, on several
applications.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19852" title="Abstract">arXiv:2310.19852</a> [<a href="/pdf/2310.19852" title="Download PDF">pdf</a>, <a href="/format/2310.19852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Alignment: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+T">Tianyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Borong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+H">Hantao Lou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yawen Duan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhonghao He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiayi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fanzhi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+K+Y">Kwan Yee Ng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Juntao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=O%27Gara%2C+A">Aidan O&#x27;Gara</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yingshan Lei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tse%2C+B">Brian Tse</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=McAleer%2C+S">Stephen McAleer</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yike Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wen Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Continually updated; 55 pages (excluding references), 802 citations. Abstract on arXiv webpage is abridged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">AI alignment aims to build AI systems that are in accordance with human
intentions and values. With the emergence of AI systems possessing superhuman
capabilities, the potential large-scale risks associated with misaligned
systems become apparent. Hundreds of AI experts and public figures have
expressed their concerns about AI risks, arguing that mitigating the risk of
extinction from AI should be a global priority, alongside other societal-scale
risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date
systematic survey on AI alignment, in this paper, we delve into the core
concepts, methodology, and practice of alignment research. To begin with, we
identify four principles as the key objectives of AI alignment: Robustness,
Interpretability, Controllability, and Ethicality (RICE). We outline the
landscape of current alignment research and decompose them into two key
components: forward alignment and backward alignment. The former aims to make
AI systems aligned via alignment training, while the latter aims to gain
evidence about the systems' alignment and govern them appropriately to avoid
exacerbating misalignment risks. On forward alignment, we discuss how to
conduct learning from various types of feedback (a.k.a., outer alignment) and
how to overcome the distribution shift to avoid goal misgeneralization (a.k.a.,
inner alignment). On backward alignment, we discuss verification techniques
that can tell the degree of value alignment for various AI systems deployed,
which can further improve the assurance of forward alignment outcomes.
<br />Based on this, we also release a constantly updated website featuring
tutorials, collections of papers, blogs, and other learning resources at
https://www.alignmentsurvey.com.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19854" title="Abstract">arXiv:2310.19854</a> [<a href="/pdf/2310.19854" title="Download PDF">pdf</a>, <a href="/format/2310.19854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic  Block Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dreveton%2C+M">Maximilien Dreveton</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+F+S">Felipe S. Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=Figueiredo%2C+D+R">Daniel R. Figueiredo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Network clustering tackles the problem of identifying sets of nodes
(communities) that have similar connection patterns. However, in many
scenarios, nodes also have attributes that are correlated with the clustering
structure. Thus, network information (edges) and node information (attributes)
can be jointly leveraged to design high-performance clustering algorithms.
Under a general model for the network and node attributes, this work
establishes an information-theoretic criterion for the exact recovery of
community labels and characterizes a phase transition determined by the
Chernoff-Hellinger divergence of the model. The criterion shows how network and
attribute information can be exchanged in order to have exact recovery (e.g.,
more reliable network information requires less reliable attribute
information). This work also presents an iterative clustering algorithm that
maximizes the joint likelihood, assuming that the probability distribution of
network interactions and node attributes belong to exponential families. This
covers a broad range of possible interactions (e.g., edges with weights) and
attributes (e.g., non-Gaussian models), as well as sparse networks, while also
exploring the connection between exponential families and Bregman divergences.
Extensive numerical experiments using synthetic data indicate that the proposed
algorithm outperforms classic algorithms that leverage only network or only
attribute information as well as state-of-the-art algorithms that also leverage
both sources of information. The contributions of this work provide insights
into the fundamental limits and practical techniques for inferring community
labels on node-attributed networks.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19855" title="Abstract">arXiv:2310.19855</a> [<a href="/pdf/2310.19855" title="Download PDF">pdf</a>, <a href="/format/2310.19855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GI-1.0: A Fast and Scalable Two-level Radiance Caching Scheme for  Real-time Global Illumination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boiss%C3%A9%2C+G">Guillaume Boiss&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Meunier%2C+S">Sylvain Meunier</a>, 
<a href="/search/cs?searchtype=author&query=de+Dinechin%2C+H">Heloise de Dinechin</a>, 
<a href="/search/cs?searchtype=author&query=Bartels%2C+P">Pieterjan Bartels</a>, 
<a href="/search/cs?searchtype=author&query=Veselov%2C+A">Alexander Veselov</a>, 
<a href="/search/cs?searchtype=author&query=Eto%2C+K">Kenta Eto</a>, 
<a href="/search/cs?searchtype=author&query=Harada%2C+T">Takahiro Harada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Real-time global illumination is key to enabling more dynamic and physically
realistic worlds in performance-critical applications such as games or any
other applications with real-time constraints.Hardware-accelerated ray tracing
in modern GPUs allows arbitrary intersection queries against the geometry,
making it possible to evaluate indirect lighting entirely at runtime. However,
only a small number of rays can be traced at each pixel to maintain high
framerates at ever-increasing image resolutions. Existing solutions, such as
probe-based techniques, approximate the irradiance signal at the cost of a few
rays per frame but suffer from a lack of details and slow response times to
changes in lighting. On the other hand, reservoir-based resampling techniques
capture much more details but typically suffer from poorer performance and
increased amounts of noise, making them impractical for the current generation
of hardware and gaming consoles. To find a balance that achieves high lighting
fidelity while maintaining a low runtime cost, we propose a solution that
dynamically estimates global illumination without needing any content
preprocessing, thus enabling easy integration into existing real-time rendering
pipelines.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19858" title="Abstract">arXiv:2310.19858</a> [<a href="/pdf/2310.19858" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iGEM: a model system for team science and innovation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santolini%2C+M">Marc Santolini</a>, 
<a href="/search/cs?searchtype=author&query=Blondel%2C+L">Leo Blondel</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+M+J">Megan J. Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Ward%2C+R+N">Robert N. Ward</a>, 
<a href="/search/cs?searchtype=author&query=Jeyaram%2C+R">Rathin Jeyaram</a>, 
<a href="/search/cs?searchtype=author&query=Brink%2C+K+R">Kathryn R. Brink</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+A">Abhijeet Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Barabasi%2C+A">Albert-Laszlo Barabasi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 78 pages including SI, 7 figures, 18 SI figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Teams are a primary source of innovation in science and technology. Rather
than examining the lone genius, scholarly and policy attention has shifted to
understanding how team interactions produce new and useful ideas. Yet the
organizational roots of innovation remain unclear, in part because of the
limitations of current data. This paper introduces the international
Genetically Engineered Machine (iGEM) competition, a model system for studying
team science and innovation. By combining digital laboratory notebooks with
performance data from 2,406 teams over multiple years of participation, we
reveal shared dynamical and organizational patterns across teams and identify
features associated with team performance and success. This dataset makes
visible organizational behavior that is typically hidden, and thus
understudied, creating new opportunities for the science of science and
innovation.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19859" title="Abstract">arXiv:2310.19859</a> [<a href="/pdf/2310.19859" title="Download PDF">pdf</a>, <a href="/format/2310.19859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner  from Backbone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zeyinzi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Chaojie Mao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+A">Ao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yiliang Lv</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yujun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Deli Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Parameter-efficient tuning has become a trend in transferring large-scale
foundation models to downstream applications. Existing methods typically embed
some light-weight tuners into the backbone, where both the design and the
learning of the tuners are highly dependent on the base model. This work offers
a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners
from the backbone. With both theoretical and empirical evidence, we show that
popular tuning approaches have their equivalent counterparts under our
unbinding formulation, and hence can be integrated into our framework
effortlessly. Thanks to the structural disentanglement, we manage to free the
design of tuners from the network architecture, facilitating flexible
combination of various tuning strategies. We further propose a memory-efficient
variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners)
is effectively detached from the main branch, such that the gradients are
back-propagated only to the tuners but not to the backbone. Such a detachment
also allows one-time backbone forward for multi-task inference. Extensive
experiments on both discriminative and generative tasks demonstrate the
superiority of our method over existing alternatives from the perspectives of
efficacy and efficiency. Project page:
$\href{https://res-tuning.github.io/}{\textit{https://res-tuning.github.io/}}$.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19861" title="Abstract">arXiv:2310.19861</a> [<a href="/pdf/2310.19861" title="Download PDF">pdf</a>, <a href="/ps/2310.19861" title="Download PostScript">ps</a>, <a href="/format/2310.19861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posterior Sampling for Competitive RL: Function Approximation and  Partial Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shuang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Z">Ziyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper investigates posterior sampling algorithms for competitive
reinforcement learning (RL) in the context of general function approximations.
Focusing on zero-sum Markov games (MGs) under two critical settings, namely
self-play and adversarial learning, we first propose the self-play and
adversarial generalized eluder coefficient (GEC) as complexity measures for
function approximation, capturing the exploration-exploitation trade-off in
MGs. Based on self-play GEC, we propose a model-based self-play posterior
sampling method to control both players to learn Nash equilibrium, which can
successfully handle the partial observability of states. Furthermore, we
identify a set of partially observable MG models fitting MG learning with the
adversarial policies of the opponent. Incorporating the adversarial GEC, we
propose a model-based posterior sampling method for learning adversarial MG
with potential partial observability. We further provide low regret bounds for
proposed algorithms that can scale sublinearly with the proposed GEC and the
number of episodes $T$. To the best of our knowledge, we for the first time
develop generic model-based posterior sampling algorithms for competitive RL
that can be applied to a majority of tractable zero-sum MG classes in both
fully observable and partially observable MGs with self-play and adversarial
learning.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19886" title="Abstract">arXiv:2310.19886</a> [<a href="/pdf/2310.19886" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BTRec: BERT-Based Trajectory Recommendation for Personalized Tours
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ho%2C+N+L">Ngai Lam Ho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+R+K">Roy Ka-Wei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+K+H">Kwan Hui Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> RecSys 2023, Workshop on Recommenders in Tourism
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">An essential task for tourists having a pleasant holiday is to have a
well-planned itinerary with relevant recommendations, especially when visiting
unfamiliar cities. Many tour recommendation tools only take into account a
limited number of factors, such as popular Points of Interest (POIs) and
routing constraints. Consequently, the solutions they provide may not always
align with the individual users of the system. We propose an iterative
algorithm in this paper, namely: BTREC (BERT-based Trajectory Recommendation),
that extends from the POIBERT embedding algorithm to recommend personalized
itineraries on POIs using the BERT framework. Our BTREC algorithm incorporates
users' demographic information alongside past POI visits into a modified BERT
language model to recommend a personalized POI itinerary prediction given a
pair of source and destination POIs. Our recommendation system can create a
travel itinerary that maximizes POIs visited, while also taking into account
user preferences for categories of POIs and time availability. Our
recommendation algorithm is largely inspired by the problem of sentence
completion in natural language processing (NLP). Using a dataset of eight
cities of different sizes, our experimental results demonstrate that our
proposed algorithm is stable and outperforms many other sequence prediction
algorithms, measured by recall, precision, and F1-scores.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19889" title="Abstract">arXiv:2310.19889</a> [<a href="/pdf/2310.19889" title="Download PDF">pdf</a>, <a href="/format/2310.19889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Geometry of Blind Spots in Vision Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+S">Sriram Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Sriramanan%2C+G">Gaurang Sriramanan</a>, 
<a href="/search/cs?searchtype=author&query=Sadasivan%2C+V+S">Vinu Sankar Sadasivan</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 20 figures, Accepted at NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the remarkable success of deep neural networks in a myriad of
settings, several works have demonstrated their overwhelming sensitivity to
near-imperceptible perturbations, known as adversarial attacks. On the other
hand, prior works have also observed that deep networks can be under-sensitive,
wherein large-magnitude perturbations in input space do not induce appreciable
changes to network activations. In this work, we study in detail the phenomenon
of under-sensitivity in vision models such as CNNs and Transformers, and
present techniques to study the geometry and extent of "equi-confidence" level
sets of such networks. We propose a Level Set Traversal algorithm that
iteratively explores regions of high confidence with respect to the input space
using orthogonal components of the local gradients. Given a source image, we
use this algorithm to identify inputs that lie in the same equi-confidence
level set as the source image despite being perceptually similar to arbitrary
images from other classes. We further observe that the source image is linearly
connected by a high-confidence path to these inputs, uncovering a star-like
structure for level sets of deep networks. Furthermore, we attempt to identify
and estimate the extent of these connected higher-dimensional regions over
which the model maintains a high degree of confidence. The code for this
project is publicly available at
https://github.com/SriramB-98/blindspots-neurips-sub
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19898" title="Abstract">arXiv:2310.19898</a> [<a href="/pdf/2310.19898" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIST: Medical Image Segmentation Transformer with Convolutional  Attention Mixing (CAM) Decoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Md Motiur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Shokouhmand%2C+S">Shiva Shokouhmand</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+S">Smriti Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Faezipour%2C+M">Miad Faezipour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures, 3 tables, accepted for publication in WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">One of the common and promising deep learning approaches used for medical
image segmentation is transformers, as they can capture long-range dependencies
among the pixels by utilizing self-attention. Despite being successful in
medical image segmentation, transformers face limitations in capturing local
contexts of pixels in multimodal dimensions. We propose a Medical Image
Segmentation Transformer (MIST) incorporating a novel Convolutional Attention
Mixing (CAM) decoder to address this issue. MIST has two parts: a pre-trained
multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded
feature representation is passed through the CAM decoder for segmenting the
images. In the CAM decoder, an attention-mixer combining multi-head
self-attention, spatial attention, and squeeze and excitation attention modules
is introduced to capture long-range dependencies in all spatial dimensions.
Moreover, to enhance spatial information gain, deep and shallow convolutions
are used for feature extraction and receptive field expansion, respectively.
The integration of low-level and high-level features from different network
stages is enabled by skip connections, allowing MIST to suppress unnecessary
information. The experiments show that our MIST transformer with CAM decoder
outperforms the state-of-the-art models specifically designed for medical image
segmentation on the ACDC and Synapse datasets. Our results also demonstrate
that adding the CAM decoder with a hierarchical transformer improves
segmentation performance significantly. Our model with data and code is
publicly available on GitHub.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19902" title="Abstract">arXiv:2310.19902</a> [<a href="/pdf/2310.19902" title="Download PDF">pdf</a>, <a href="/format/2310.19902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Herd: Using multiple, smaller LLMs to match the performances of  proprietary, large LLMs via an intelligent composer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hari%2C+S+N">Surya Narayanan Hari</a>, 
<a href="/search/cs?searchtype=author&query=Thomson%2C+M">Matt Thomson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Currently, over a thousand LLMs exist that are multi-purpose and are capable
of performing real world tasks, including Q&amp;A, text summarization, content
generation, etc. However, accessibility, scale and reliability of free models
prevents them from being widely deployed in everyday use cases. To address the
first two issues of access and scale, organisations such as HuggingFace have
created model repositories where users have uploaded model weights and
quantized versions of models trained using different paradigms, as well as
model cards describing their training process. While some models report
performance on commonly used benchmarks, not all do, and interpreting the real
world impact of trading off performance on a benchmark for model deployment
cost, is unclear. Here, we show that a herd of open source models can match or
exceed the performance of proprietary models via an intelligent router. We show
that a Herd of open source models is able to match the accuracy of ChatGPT,
despite being composed of models that are effectively 2.5x smaller. We show
that in cases where GPT is not able to answer the query, Herd is able to
identify a model that can, at least 40% of the time.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19903" title="Abstract">arXiv:2310.19903</a> [<a href="/pdf/2310.19903" title="Download PDF">pdf</a>, <a href="/format/2310.19903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-agent Reinforcement Learning Study of Emergence of Social  Classes out of Arbitrary Governance: The Role of Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dizaji%2C+A+S">Aslan S. Dizaji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">There are several theories in economics regarding the roots or causes of
prosperity in a society. One of these theories or hypotheses -- named geography
hypothesis -- mentions that the reason why some countries are prosperous and
some others are poor is the geographical location of the countries in the world
as makes their climate and environment favorable or unfavorable regarding
natural resources. Another competing hypothesis states that man-made
institutions particularly inclusive political institutions are the reasons why
some countries are prosperous and some others are poor. On the other hand,
there is a specific political theory developed for the long-term social
development in Iran -- named Arbitrary Rule and Aridisolatic Society which
particularly emphasizes on the role of aridity to shape arbitrary political and
economical institutions in Iran, without any functional social classes in the
society. In this paper, by extending the AI-Economist -- a recently developed
two-level multi-agent reinforcement learning environment -- I show that when
the central planner is ruling the environment by arbitrary rules, the society
evolves through different paths in different environments. In the environment
having band-like vertical isolated patches of natural resources, all mobile
agents are equally exploited by the central planner and the central planner is
also not gaining any income, while in the society having more uniformly
distributed natural resources, the productivity and Maximin are higher and the
society generates a heterogeneous stratified social structure. All these
findings provide a partial answer to the above debate and reconcile the role of
geography and political institutions on the long-term development in a region.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19906" title="Abstract">arXiv:2310.19906</a> [<a href="/pdf/2310.19906" title="Download PDF">pdf</a>, <a href="/format/2310.19906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Prototype-based Graph Information Bottleneck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Sangwoo Seo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The success of Graph Neural Networks (GNNs) has led to a need for
understanding their decision-making process and providing explanations for
their predictions, which has given rise to explainable AI (XAI) that offers
transparent explanations for black-box models. Recently, the use of prototypes
has successfully improved the explainability of models by learning prototypes
to imply training graphs that affect the prediction. However, these approaches
tend to provide prototypes with excessive information from the entire graph,
leading to the exclusion of key substructures or the inclusion of irrelevant
substructures, which can limit both the interpretability and the performance of
the model in downstream tasks. In this work, we propose a novel framework of
explainable GNNs, called interpretable Prototype-based Graph Information
Bottleneck (PGIB) that incorporates prototype learning within the information
bottleneck framework to provide prototypes with the key subgraph from the input
graph that is important for the model prediction. This is the first work that
incorporates prototype learning into the process of identifying the key
subgraphs that have a critical impact on the prediction performance. Extensive
experiments, including qualitative analysis, demonstrate that PGIB outperforms
state-of-the-art methods in terms of both prediction performance and
explainability.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19909" title="Abstract">arXiv:2310.19909</a> [<a href="/pdf/2310.19909" title="Download PDF">pdf</a>, <a href="/format/2310.19909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Battle of the Backbones: A Large-Scale Comparison of Pretrained Models  across Computer Vision Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goldblum%2C+M">Micah Goldblum</a>, 
<a href="/search/cs?searchtype=author&query=Souri%2C+H">Hossein Souri</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+R">Renkun Ni</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+M">Manli Shu</a>, 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+V">Viraj Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Somepalli%2C+G">Gowthami Somepalli</a>, 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+P">Prithvijit Chattopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+M">Mark Ibrahim</a>, 
<a href="/search/cs?searchtype=author&query=Bardes%2C+A">Adrien Bardes</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+J">Judy Hoffman</a>, 
<a href="/search/cs?searchtype=author&query=Chellappa%2C+R">Rama Chellappa</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+A+G">Andrew Gordon Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+T">Tom Goldstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural network based computer vision systems are typically built on a
backbone, a pretrained or randomly initialized feature extractor. Several years
ago, the default option was an ImageNet-trained convolutional neural network.
However, the recent past has seen the emergence of countless backbones
pretrained using various algorithms and datasets. While this abundance of
choice has led to performance increases for a range of systems, it is difficult
for practitioners to make informed decisions about which backbone to choose.
Battle of the Backbones (BoB) makes this choice easier by benchmarking a
diverse suite of pretrained models, including vision-language models, those
trained via self-supervised learning, and the Stable Diffusion backbone, across
a diverse set of computer vision tasks ranging from classification to object
detection to OOD generalization and more. Furthermore, BoB sheds light on
promising directions for the research community to advance computer vision by
illuminating strengths and weakness of existing approaches through a
comprehensive analysis conducted on more than 1500 training runs. While vision
transformers (ViTs) and self-supervised learning (SSL) are increasingly
popular, we find that convolutional neural networks pretrained in a supervised
fashion on large training sets still perform best on most tasks among the
models we consider. Moreover, in apples-to-apples comparisons on the same
architectures and similarly sized pretraining datasets, we find that SSL
backbones are highly competitive, indicating that future works should perform
SSL pretraining with advanced architectures and larger pretraining datasets. We
release the raw results of our experiments along with code that allows
researchers to put their own backbones through the gauntlet here:
https://github.com/hsouri/Battle-of-the-Backbones
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19915" title="Abstract">arXiv:2310.19915</a> [<a href="/pdf/2310.19915" title="Download PDF">pdf</a>, <a href="/format/2310.19915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors  Using Protein Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seongwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Mollaei%2C+P">Parisa Mollaei</a>, 
<a href="/search/cs?searchtype=author&query=Antony%2C+A">Akshay Antony</a>, 
<a href="/search/cs?searchtype=author&query=Magar%2C+R">Rishikesh Magar</a>, 
<a href="/search/cs?searchtype=author&query=Farimani%2C+A+B">Amir Barati Farimani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
<p class="mathjax">With the rise of Transformers and Large Language Models (LLMs) in Chemistry
and Biology, new avenues for the design and understanding of therapeutics have
opened up to the scientific community. Protein sequences can be modeled as
language and can take advantage of recent advances in LLMs, specifically with
the abundance of our access to the protein sequence datasets. In this paper, we
developed the GPCR-BERT model for understanding the sequential design of G
Protein-Coupled Receptors (GPCRs). GPCRs are the target of over one-third of
FDA-approved pharmaceuticals. However, there is a lack of comprehensive
understanding regarding the relationship between amino acid sequence, ligand
selectivity, and conformational motifs (such as NPxxY, CWxP, E/DRY). By
utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with
prediction tasks of variations in the motifs, we were able to shed light on
several relationships between residues in the binding pocket and some of the
conserved motifs. To achieve this, we took advantage of attention weights, and
hidden states of the model that are interpreted to extract the extent of
contributions of amino acids in dictating the type of masked ones. The
fine-tuned models demonstrated high accuracy in predicting hidden residues
within the motifs. In addition, the analysis of embedding was performed over 3D
structures to elucidate the higher-order interactions within the conformations
of the receptors.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19917" title="Abstract">arXiv:2310.19917</a> [<a href="/pdf/2310.19917" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unmasking Bias and Inequities: A Systematic Review of Bias Detection and  Mitigation in Healthcare Artificial Intelligence Using Electronic Health  Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liqin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Julie Hong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiaqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Li Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 2 figures, 2 tables, 2 supplementary files, 66 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Objectives: Artificial intelligence (AI) applications utilizing electronic
health records (EHRs) have gained popularity, but they also introduce various
types of bias. This study aims to systematically review the literature that
address bias in AI research utilizing EHR data. Methods: A systematic review
was conducted following the Preferred Reporting Items for Systematic Reviews
and Meta-analyses (PRISMA) guideline. We retrieved articles published between
January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the
Institute of Electrical and Electronics Engineers. We defined six major types
of bias and summarized the existing approaches in bias handling. Results: Out
of the 252 retrieved articles, 20 met the inclusion criteria for the final
review. Five out of six bias were covered in this review: eight studies
analyzed selection bias; six on implicit bias; five on confounding bias; four
on measurement bias; two on algorithmic bias. For bias handling approaches, ten
studies identified bias during model development, while seventeen presented
methods to mitigate the bias. Discussion: Bias may infiltrate the AI
application development process at various stages. Although this review
discusses methods for addressing bias at different development stages, there is
room for implementing additional effective approaches. Conclusion: Despite
growing attention to bias in healthcare AI, research using EHR data on this
topic is still limited. Detecting and mitigating AI bias with EHR data
continues to pose challenges. Further research is needed to raise a
standardized method that is generalizable and interpretable to detect, mitigate
and evaluate bias in medical AI.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19919" title="Abstract">arXiv:2310.19919</a> [<a href="/pdf/2310.19919" title="Download PDF">pdf</a>, <a href="/format/2310.19919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning Strategies through Value Maximization in Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carrasco-Davis%2C+R">Rodrigo Carrasco-Davis</a>, 
<a href="/search/cs?searchtype=author&query=Mas%C3%ADs%2C+J">Javier Mas&#xed;s</a>, 
<a href="/search/cs?searchtype=author&query=Saxe%2C+A+M">Andrew M. Saxe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Biological and artificial learning agents face numerous choices about how to
learn, ranging from hyperparameter selection to aspects of task distributions
like curricula. Understanding how to make these meta-learning choices could
offer normative accounts of cognitive control functions in biological learners
and improve engineered systems. Yet optimal strategies remain challenging to
compute in modern deep networks due to the complexity of optimizing through the
entire learning process. Here we theoretically investigate optimal strategies
in a tractable setting. We present a learning effort framework capable of
efficiently optimizing control signals on a fully normative objective:
discounted cumulative performance throughout learning. We obtain computational
tractability by using average dynamical equations for gradient descent,
available for simple neural network architectures. Our framework accommodates a
range of meta-learning and automatic curriculum learning methods in a unified
normative setting. We apply this framework to investigate the effect of
approximations in common meta-learning algorithms; infer aspects of optimal
curricula; and compute optimal neuronal resource allocation in a continual
learning setting. Across settings, we find that control effort is most
beneficial when applied to easier aspects of a task early in learning; followed
by sustained effort on harder aspects. Overall, the learning effort framework
provides a tractable theoretical test bed to study normative benefits of
interventions in a variety of learning systems, as well as a formal account of
optimal cognitive control strategies over learning trajectories posited by
established theories in cognitive neuroscience.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19923" title="Abstract">arXiv:2310.19923</a> [<a href="/pdf/2310.19923" title="Download PDF">pdf</a>, <a href="/format/2310.19923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long  Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%BCnther%2C+M">Michael G&#xfc;nther</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+J">Jackmin Ong</a>, 
<a href="/search/cs?searchtype=author&query=Mohr%2C+I">Isabelle Mohr</a>, 
<a href="/search/cs?searchtype=author&query=Abdessalem%2C+A">Alaeddine Abdessalem</a>, 
<a href="/search/cs?searchtype=author&query=Abel%2C+T">Tanguy Abel</a>, 
<a href="/search/cs?searchtype=author&query=Akram%2C+M+K">Mohammad Kalim Akram</a>, 
<a href="/search/cs?searchtype=author&query=Guzman%2C+S">Susana Guzman</a>, 
<a href="/search/cs?searchtype=author&query=Mastrapas%2C+G">Georgios Mastrapas</a>, 
<a href="/search/cs?searchtype=author&query=Sturuam%2C+S">Saba Sturuam</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Werk%2C+M">Maximilian Werk</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Han Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
<br />To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI's proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19925" title="Abstract">arXiv:2310.19925</a> [<a href="/pdf/2310.19925" title="Download PDF">pdf</a>, <a href="/ps/2310.19925" title="Download PostScript">ps</a>, <a href="/format/2310.19925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenRAND: A Performance Portable, Reproducible Random Number Generation  Library for Parallel Computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+S+S">Shihab Shahriar Khan</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+B">Bryce Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Edelmaierd%2C+C">Christopher Edelmaierd</a>, 
<a href="/search/cs?searchtype=author&query=Aktulga%2C+H+M">Hasan Metin Aktulga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper currently under review in Softwarex journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">We introduce OpenRAND, a C++17 library aimed at facilitating reproducible
scientific research through the generation of statistically robust and yet
replicable random numbers. OpenRAND accommodates single and multi-threaded
applications on CPUs and GPUs and offers a simplified, user-friendly API that
complies with the C++ standard's random number engine interface. It is
portable: it functions seamlessly as a lightweight, header-only library, making
it adaptable to a wide spectrum of software and hardware platforms. It is
statistically robust: a suite of built-in tests ensures no pattern exists
within single or multiple streams. Despite the simplicity and portability, it
is remarkably performant-matching and sometimes even outperforming native
libraries by a significant margin. Our tests, including a Brownian walk
simulation, affirm its reproducibility and highlight its computational
efficiency, outperforming CUDA's cuRAND by up to 1.8 times.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19927" title="Abstract">arXiv:2310.19927</a> [<a href="/pdf/2310.19927" title="Download PDF">pdf</a>, <a href="/format/2310.19927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Based Reparameterization Policy Gradient Methods: Theory and  Practical Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Boyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely
adopted for continuous control tasks in robotics and computer graphics.
However, recent studies have revealed that, when applied to long-term
reinforcement learning problems, model-based RP PGMs may experience chaotic and
non-smooth optimization landscapes with exploding gradient variance, which
leads to slow convergence. This is in contrast to the conventional belief that
reparameterization methods have low gradient estimation variance in problems
such as training deep generative models. To comprehend this phenomenon, we
conduct a theoretical examination of model-based RP PGMs and search for
solutions to the optimization difficulties. Specifically, we analyze the
convergence of the model-based RP PGMs and pinpoint the smoothness of function
approximators as a major factor that affects the quality of gradient
estimation. Based on our analysis, we propose a spectral normalization method
to mitigate the exploding variance issue caused by long model unrolls. Our
experimental results demonstrate that proper normalization significantly
reduces the gradient variance of model-based RP PGMs. As a result, the
performance of the proposed method is comparable or superior to other gradient
estimators, such as the Likelihood Ratio (LR) gradient estimator. Our code is
available at https://github.com/agentification/RP_PGM.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19930" title="Abstract">arXiv:2310.19930</a> [<a href="/pdf/2310.19930" title="Download PDF">pdf</a>, <a href="/format/2310.19930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling-robust built-in a posteriori error estimation for discontinuous  least-squares finite element methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bringmann%2C+P">Philipp Bringmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A convincing feature of least-squares finite element methods is the built-in
a posteriori error estimator for any conforming discretization. In order to
generalize this property to discontinuous finite element ansatz functions, this
paper introduces a least-squares principle on piecewise Sobolev functions for
the solution of the Poisson model problem in 2D with mixed boundary conditions.
It allows for fairly general discretizations including standard piecewise
polynomial ansatz spaces on triangular and polygonal meshes. The presented
scheme enforces the interelement continuity of the piecewise polynomials by
additional least-squares residuals. A side condition on the normal jumps of the
flux variable requires a vanishing integral mean and enables a natural
weighting of the jump in the least-squares functional in terms of the mesh
size. This avoids over-penalization with additional regularity assumptions on
the exact solution as usually present in the literature on discontinuous LSFEM.
The proof of the built-in a posteriori error estimation for the over-penalized
scheme is presented as well. All results in this paper are robust with respect
to the size of the domain guaranteed by a suitable weighting of the residuals
in the least-squares functional. Numerical experiments exhibit optimal
convergence rates of the adaptive mesh-refining algorithm for various
polynomial degrees.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19932" title="Abstract">arXiv:2310.19932</a> [<a href="/pdf/2310.19932" title="Download PDF">pdf</a>, <a href="/format/2310.19932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sim2Real for Environmental Neural Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scholz%2C+J">Jonas Scholz</a>, 
<a href="/search/cs?searchtype=author&query=Andersson%2C+T+R">Tom R. Andersson</a>, 
<a href="/search/cs?searchtype=author&query=Vaughan%2C+A">Anna Vaughan</a>, 
<a href="/search/cs?searchtype=author&query=Requeima%2C+J">James Requeima</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures, To be published in Tackling Climate Change with Machine Learning workshop at NeurIPS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Machine learning (ML)-based weather models have recently undergone rapid
improvements. These models are typically trained on gridded reanalysis data
from numerical data assimilation systems. However, reanalysis data comes with
limitations, such as assumptions about physical laws and low spatiotemporal
resolution. The gap between reanalysis and reality has sparked growing interest
in training ML models directly on observations such as weather stations.
Modelling scattered and sparse environmental observations requires scalable and
flexible ML architectures, one of which is the convolutional conditional neural
process (ConvCNP). ConvCNPs can learn to condition on both gridded and
off-the-grid context data to make uncertainty-aware predictions at target
locations. However, the sparsity of real observations presents a challenge for
data-hungry deep learning models like the ConvCNP. One potential solution is
'Sim2Real': pre-training on reanalysis and fine-tuning on observational data.
We analyse Sim2Real with a ConvCNP trained to interpolate surface air
temperature over Germany, using varying numbers of weather stations for
fine-tuning. On held-out weather stations, Sim2Real training substantially
outperforms the same model architecture trained only with reanalysis data or
only with station data, showing that reanalysis data can serve as a stepping
stone for learning from real observations. Sim2Real could thus enable more
accurate models for weather prediction and climate monitoring.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19936" title="Abstract">arXiv:2310.19936</a> [<a href="/pdf/2310.19936" title="Download PDF">pdf</a>, <a href="/format/2310.19936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Few-Annotation Learning for Object Detection: Are  Transformer-based Models More Efficient ?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouniot%2C+Q">Quentin Bouniot</a>, 
<a href="/search/cs?searchtype=author&query=Loesch%2C+A">Ang&#xe9;lique Loesch</a>, 
<a href="/search/cs?searchtype=author&query=Audigier%2C+R">Romaric Audigier</a>, 
<a href="/search/cs?searchtype=author&query=Habrard%2C+A">Amaury Habrard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at WACV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">For specialized and dense downstream tasks such as object detection, labeling
data requires expertise and can be very expensive, making few-shot and
semi-supervised models much more attractive alternatives. While in the few-shot
setup we observe that transformer-based object detectors perform better than
convolution-based two-stage models for a similar amount of parameters, they are
not as effective when used with recent approaches in the semi-supervised
setting. In this paper, we propose a semi-supervised method tailored for the
current state-of-the-art object detector Deformable DETR in the few-annotation
learning setup using a student-teacher architecture, which avoids relying on a
sensitive post-processing of the pseudo-labels generated by the teacher model.
We evaluate our method on the semi-supervised object detection benchmarks COCO
and Pascal VOC, and it outperforms previous methods, especially when
annotations are scarce. We believe that our contributions open new
possibilities to adapt similar object detection methods in this setup as well.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19938" title="Abstract">arXiv:2310.19938</a> [<a href="/pdf/2310.19938" title="Download PDF">pdf</a>, <a href="/format/2310.19938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Akbari%2C+S">Saiedeh Akbari</a>, 
<a href="/search/eess?searchtype=author&query=Griffis%2C+E+J">Emily J. Griffis</a>, 
<a href="/search/eess?searchtype=author&query=Patil%2C+O+S">Omkar Sudhir Patil</a>, 
<a href="/search/eess?searchtype=author&query=Dixon%2C+W+E">Warren E. Dixon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep neural network (DNN)-based adaptive controllers can be used to
compensate for unstructured uncertainties in nonlinear dynamic systems.
However, DNNs are also very susceptible to overfitting and co-adaptation.
Dropout regularization is an approach where nodes are randomly dropped during
training to alleviate issues such as overfitting and co-adaptation. In this
paper, a dropout DNN-based adaptive controller is developed. The developed
dropout technique allows the deactivation of weights that are stochastically
selected for each individual layer within the DNN. Simultaneously, a
Lyapunov-based real-time weight adaptation law is introduced to update the
weights of all layers of the DNN for online unsupervised learning. A non-smooth
Lyapunov-based stability analysis is performed to ensure asymptotic convergence
of the tracking error. Simulation results of the developed dropout DNN-based
adaptive controller indicate a 38.32% improvement in the tracking error, a
53.67% improvement in the function approximation error, and 50.44% lower
control effort when compared to a baseline adaptive DNN-based controller
without dropout regularization.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19942" title="Abstract">arXiv:2310.19942</a> [<a href="/pdf/2310.19942" title="Download PDF">pdf</a>, <a href="/format/2310.19942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Split-NER: Named Entity Recognition via Two Question-Answering-based  Classifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+J">Jatin Arora</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Youngja Park</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (2023) 416-426
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we address the NER problem by splitting it into two logical
sub-tasks: (1) Span Detection which simply extracts entity mention spans
irrespective of entity type; (2) Span Classification which classifies the spans
into their entity types. Further, we formulate both sub-tasks as
question-answering (QA) problems and produce two leaner models which can be
optimized separately for each sub-task. Experiments with four cross-domain
datasets demonstrate that this two-step approach is both effective and time
efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17
and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all
cases, it achieves a significant reduction in training time compared to its QA
baseline counterpart. The effectiveness of our system stems from fine-tuning
the BERT model twice, separately for span detection and classification. The
source code can be found at https://github.com/c3sr/split-ner.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19943" title="Abstract">arXiv:2310.19943</a> [<a href="/pdf/2310.19943" title="Download PDF">pdf</a>, <a href="/format/2310.19943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Acquisition of Physical Knowledge in Generative Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buschoff%2C+L+M+S">Luca M. Schulze Buschoff</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+E">Eric Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Binz%2C+M">Marcel Binz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">As children grow older, they develop an intuitive understanding of the
physical processes around them. Their physical understanding develops in
stages, moving along developmental trajectories which have been mapped out
extensively in previous empirical research. Here, we investigate how the
learning trajectories of deep generative neural networks compare to children's
developmental trajectories using physical understanding as a testbed. We
outline an approach that allows us to examine two distinct hypotheses of human
development - stochastic optimization and complexity increase. We find that
while our models are able to accurately predict a number of physical processes,
their learning trajectories under both hypotheses do not follow the
developmental trajectories of children.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19944" title="Abstract">arXiv:2310.19944</a> [<a href="/pdf/2310.19944" title="Download PDF">pdf</a>, <a href="/format/2310.19944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Unscented Autoencoders for Trajectory Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Janjo%C5%A1%2C+F">Faris Janjo&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Hallgarten%2C+M">Marcel Hallgarten</a>, 
<a href="/search/cs?searchtype=author&query=Knittel%2C+A">Anthony Knittel</a>, 
<a href="/search/cs?searchtype=author&query=Dolgov%2C+M">Maxim Dolgov</a>, 
<a href="/search/cs?searchtype=author&query=Zell%2C+A">Andreas Zell</a>, 
<a href="/search/cs?searchtype=author&query=Z%C3%B6llner%2C+J+M">J. Marius Z&#xf6;llner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The \ac{CVAE} is one of the most widely-used models in trajectory prediction
for \ac{AD}. It captures the interplay between a driving context and its
ground-truth future into a probabilistic latent space and uses it to produce
predictions. In this paper, we challenge key components of the CVAE. We
leverage recent advances in the space of the VAE, the foundation of the CVAE,
which show that a simple change in the sampling procedure can greatly benefit
performance. We find that unscented sampling, which draws samples from any
learned distribution in a deterministic manner, can naturally be better suited
to trajectory prediction than potentially dangerous random sampling. We go
further and offer additional improvements, including a more structured mixture
latent space, as well as a novel, potentially more expressive way to do
inference with CVAEs. We show wide applicability of our models by evaluating
them on the INTERACTION prediction dataset, outperforming the state of the art,
as well as at the task of image modeling on the CelebA dataset, outperforming
the baseline vanilla CVAE. Code is available at
https://github.com/boschresearch/cuae-prediction.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19951" title="Abstract">arXiv:2310.19951</a> [<a href="/pdf/2310.19951" title="Download PDF">pdf</a>, <a href="/format/2310.19951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Behavior Change with Observational Studies: a Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pera%2C+A">Arianna Pera</a>, 
<a href="/search/cs?searchtype=author&query=de+Francisci+Morales%2C+G">Gianmarco de Francisci Morales</a>, 
<a href="/search/cs?searchtype=author&query=Aiello%2C+L+M">Luca Maria Aiello</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Exploring behavioral change in the digital age is imperative for societal
progress in the context of 21st-century challenges. We analyzed 148 articles
(2000-2023) and built a map that categorizes behaviors and change detection
methodologies, platforms of reference, and theoretical frameworks that
characterize online behavior change. Our findings uncover a focus on sentiment
shifts, an emphasis on API-restricted platforms, and limited theory
integration. We call for methodologies able to capture a wider range of
behavioral types, diverse data sources, and stronger theory-practice alignment
in the study of online behavioral change.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19953" title="Abstract">arXiv:2310.19953</a> [<a href="/pdf/2310.19953" title="Download PDF">pdf</a>, <a href="/ps/2310.19953" title="Download PostScript">ps</a>, <a href="/format/2310.19953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Quantum Algorithm for Load Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Neufeld%2C+D">David Neufeld</a>, 
<a href="/search/eess?searchtype=author&query=Hafshejani%2C+S+F">Sajad Fathi Hafshejani</a>, 
<a href="/search/eess?searchtype=author&query=Gaur%2C+D">Daya Gaur</a>, 
<a href="/search/eess?searchtype=author&query=Benkoczi%2C+R">Robert Benkoczi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We study a hybrid quantum algorithm for solving the AC load flow problem. The
algorithm uses a quantum algorithm to compute the direction in the
Newton-Raphson method. This hybrid approach offers scalability and improved
convergence rates in theory.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19956" title="Abstract">arXiv:2310.19956</a> [<a href="/pdf/2310.19956" title="Download PDF">pdf</a>, <a href="/format/2310.19956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Depth and Width on Transformer Language Model  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petty%2C+J">Jackson Petty</a>, 
<a href="/search/cs?searchtype=author&query=van+Steenkiste%2C+S">Sjoerd van Steenkiste</a>, 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+I">Ishita Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+F">Fei Sha</a>, 
<a href="/search/cs?searchtype=author&query=Garrette%2C+D">Dan Garrette</a>, 
<a href="/search/cs?searchtype=author&query=Linzen%2C+T">Tal Linzen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">To process novel sentences, language models (LMs) must generalize
compositionally -- combine familiar elements in new ways. What aspects of a
model's structure promote compositional generalization? Focusing on
transformers, we test the hypothesis, motivated by recent theoretical and
empirical work, that transformers generalize more compositionally when they are
deeper (have more layers). Because simply adding layers increases the total
number of parameters, confounding depth and size, we construct three classes of
models which trade off depth for width such that the total number of parameters
is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs
and fine-tune them on tasks that test for compositional generalization. We
report three main conclusions: (1) after fine-tuning, deeper models generalize
better out-of-distribution than shallower models do, but the relative benefit
of additional layers diminishes rapidly; (2) within each family, deeper models
show better language modeling performance, but returns are similarly
diminishing; (3) the benefits of depth for compositional generalization cannot
be attributed solely to better performance on language modeling or on
in-distribution data.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19957" title="Abstract">arXiv:2310.19957</a> [<a href="/pdf/2310.19957" title="Download PDF">pdf</a>, <a href="/ps/2310.19957" title="Download PostScript">ps</a>, <a href="/format/2310.19957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhe Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">With advancements in GPS, remote sensing, and computational simulation, an
enormous volume of spatiotemporal data is being collected at an increasing
speed from various application domains, spanning Earth sciences, agriculture,
smart cities, and public safety. Such emerging geospatial and spatiotemporal
big data, coupled with recent advances in deep learning technologies, foster
new opportunities to solve problems that have not been possible before. For
instance, remote sensing researchers can potentially train a foundation model
using Earth imagery big data for numerous land cover and land use modeling
tasks. Coastal modelers can train AI surrogates to speed up numerical
simulations. However, the distinctive characteristics of spatiotemporal big
data pose new challenges for deep learning technologies. This vision paper
introduces various types of spatiotemporal big data, discusses new research
opportunities in the realm of deep learning applied to spatiotemporal big data,
lists the unique challenges, and identifies several future research needs.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19958" title="Abstract">arXiv:2310.19958</a> [<a href="/pdf/2310.19958" title="Download PDF">pdf</a>, <a href="/format/2310.19958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PriPrune: Quantifying and Preserving Privacy in Pruned Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+T">Tianyue Chu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Laoutaris%2C+N">Nikolaos Laoutaris</a>, 
<a href="/search/cs?searchtype=author&query=Markopoulou%2C+A">Athina Markopoulou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT)

</div>
<p class="mathjax">Federated learning (FL) is a paradigm that allows several client devices and
a server to collaboratively train a global model, by exchanging only model
updates, without the devices sharing their local training data. These devices
are often constrained in terms of communication and computation resources, and
can further benefit from model pruning -- a paradigm that is widely used to
reduce the size and complexity of models. Intuitively, by making local models
coarser, pruning is expected to also provide some protection against privacy
attacks in the context of FL. However this protection has not been previously
characterized, formally or experimentally, and it is unclear if it is
sufficient against state-of-the-art attacks.
<br />In this paper, we perform the first investigation of privacy guarantees for
model pruning in FL. We derive information-theoretic upper bounds on the amount
of information leaked by pruned FL models. We complement and validate these
theoretical findings, with comprehensive experiments that involve
state-of-the-art privacy attacks, on several state-of-the-art FL pruning
schemes, using benchmark datasets. This evaluation provides valuable insights
into the choices and parameters that can affect the privacy protection provided
by pruning. Based on these insights, we introduce PriPrune -- a privacy-aware
algorithm for local model pruning, which uses a personalized per-client defense
mask and adapts the defense pruning rate so as to jointly optimize privacy and
model performance. PriPrune is universal in that can be applied after any
pruned FL scheme on the client, without modification, and protects against any
inversion attack by the server. Our empirical evaluation demonstrates that
PriPrune significantly improves the privacy-accuracy tradeoff compared to
state-of-the-art pruned FL schemes that do not take privacy into account.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19960" title="Abstract">arXiv:2310.19960</a> [<a href="/pdf/2310.19960" title="Download PDF">pdf</a>, <a href="/format/2310.19960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological Learning for Motion Data via Mixed Coordinates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Hengrui Luo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jisu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Patania%2C+A">Alice Patania</a>, 
<a href="/search/cs?searchtype=author&query=Vejdemo-Johansson%2C+M">Mikael Vejdemo-Johansson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2021 IEEE International Conference on Big Data (Big Data)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Topology (math.AT); Computation (stat.CO)

</div>
<p class="mathjax">Topology can extract the structural information in a dataset efficiently. In
this paper, we attempt to incorporate topological information into a multiple
output Gaussian process model for transfer learning purposes. To achieve this
goal, we extend the framework of circular coordinates into a novel framework of
mixed valued coordinates to take linear trends in the time series into
consideration.
<br />One of the major challenges to learn from multiple time series effectively
via a multiple output Gaussian process model is constructing a functional
kernel. We propose to use topologically induced clustering to construct a
cluster based kernel in a multiple output Gaussian process model. This kernel
not only incorporates the topological structural information, but also allows
us to put forward a unified framework using topological information in time and
motion series.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19961" title="Abstract">arXiv:2310.19961</a> [<a href="/pdf/2310.19961" title="Download PDF">pdf</a>, <a href="/format/2310.19961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExPT: Synthetic Pretraining for Few-Shot Experimental Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Tung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Sudhanshu Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Grover%2C+A">Aditya Grover</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 Conference on Neural Information Processing Systems (NeurIPS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Experimental design is a fundamental problem in many science and engineering
fields. In this problem, sample efficiency is crucial due to the time, money,
and safety costs of real-world design evaluations. Existing approaches either
rely on active data collection or access to large, labeled datasets of past
experiments, making them impractical in many real-world scenarios. In this
work, we address the more challenging yet realistic setting of few-shot
experimental design, where only a few labeled data points of input designs and
their corresponding values are available. We approach this problem as a
conditional generation task, where a model conditions on a few labeled examples
and the desired output to generate an optimal input design. To this end, we
introduce Experiment Pretrained Transformers (ExPT), a foundation model for
few-shot experimental design that employs a novel combination of synthetic
pretraining with in-context learning. In ExPT, we only assume knowledge of a
finite collection of unlabelled data points from the input domain and pretrain
a transformer neural network to optimize diverse synthetic functions defined
over this domain. Unsupervised pretraining allows ExPT to adapt to any design
task at test time in an in-context fashion by conditioning on a few labeled
data points from the target task and generating the candidate optima. We
evaluate ExPT on few-shot experimental design in challenging domains and
demonstrate its superior generality and performance compared to existing
methods. The source code is available at https://github.com/tung-nd/ExPT.git.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19967" title="Abstract">arXiv:2310.19967</a> [<a href="/pdf/2310.19967" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Early detection of inflammatory arthritis to improve referrals using  multimodal machine learning from blood testing, semi-structured and  unstructured patient records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weizi Li</a>, 
<a href="/search/cs?searchtype=author&query=Bradlow%2C+A">Anthony Bradlow</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A+T+Y">Antoni T.Y. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Bazuaye%2C+E">Eghosa Bazuaye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in The 57th Hawaii International Conference on System Sciences, 3-6 Jan 2024, Hawaii
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Early detection of inflammatory arthritis (IA) is critical to efficient and
accurate hospital referral triage for timely treatment and preventing the
deterioration of the IA disease course, especially under limited healthcare
resources. The manual assessment process is the most common approach in
practice for the early detection of IA, but it is extremely labor-intensive and
inefficient. A large amount of clinical information needs to be assessed for
every referral from General Practice (GP) to the hospitals. Machine learning
shows great potential in automating repetitive assessment tasks and providing
decision support for the early detection of IA. However, most machine
learning-based methods for IA detection rely on blood testing results. But in
practice, blood testing data is not always available at the point of referrals,
so we need methods to leverage multimodal data such as semi-structured and
unstructured data for early detection of IA. In this research, we present
fusion and ensemble learning-based methods using multimodal data to assist
decision-making in the early detection of IA. To the best of our knowledge, our
study is the first attempt to utilize multimodal data to support the early
detection of IA from GP referrals.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19970" title="Abstract">arXiv:2310.19970</a> [<a href="/pdf/2310.19970" title="Download PDF">pdf</a>, <a href="/format/2310.19970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategies to Harness the Transformers&#x27; Potential: UNSL at eRisk 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thompson%2C+H">Horacio Thompson</a>, 
<a href="/search/cs?searchtype=author&query=Cagnina%2C+L">Leticia Cagnina</a>, 
<a href="/search/cs?searchtype=author&query=Errecalde%2C+M">Marcelo Errecalde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Conference and Labs of the Evaluation Forum (CLEF 2023), Thessaloniki, Greece
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CEUR Workshop Proceedings 2023, vol. 3497, pp. 791-804
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The CLEF eRisk Laboratory explores solutions to different tasks related to
risk detection on the Internet. In the 2023 edition, Task 1 consisted of
searching for symptoms of depression, the objective of which was to extract
user writings according to their relevance to the BDI Questionnaire symptoms.
Task 2 was related to the problem of early detection of pathological gambling
risks, where the participants had to detect users at risk as quickly as
possible. Finally, Task 3 consisted of estimating the severity levels of signs
of eating disorders. Our research group participated in the first two tasks,
proposing solutions based on Transformers. For Task 1, we applied different
approaches that can be interesting in information retrieval tasks. Two
proposals were based on the similarity of contextualized embedding vectors, and
the other one was based on prompting, an attractive current technique of
machine learning. For Task 2, we proposed three fine-tuned models followed by
decision policy according to criteria defined by an early detection framework.
One model presented extended vocabulary with important words to the addressed
domain. In the last task, we obtained good performances considering the
decision-based metrics, ranking-based metrics, and runtime. In this work, we
explore different ways to deploy the predictive potential of Transformers in
eRisk tasks.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19975" title="Abstract">arXiv:2310.19975</a> [<a href="/pdf/2310.19975" title="Download PDF">pdf</a>, <a href="/format/2310.19975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioInstruct: Instruction Tuning of Large Language Models for Biomedical  Natural Language Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+H">Hieu Tran</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhichao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zonghai Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) has achieved a great success in many natural
language processing (NLP) tasks. This is achieved by pretraining of LLMs on
vast amount of data and then instruction tuning to specific domains. However,
only a few instructions in the biomedical domain have been published. To
address this issue, we introduce BioInstruct, a customized task-specific
instruction dataset containing more than 25,000 examples. This dataset was
generated attractively by prompting a GPT-4 language model with a
three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using
the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical
natural language processing (BioNLP). We conducted instruction tuning on the
LLaMA LLMs (1\&amp;2, 7B\&amp;13B) and evaluated them on BioNLP applications, including
information extraction, question answering, and text generation. We also
evaluated how instructions contributed to model performance using multi-tasking
learning principles.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19978" title="Abstract">arXiv:2310.19978</a> [<a href="/pdf/2310.19978" title="Download PDF">pdf</a>, <a href="/format/2310.19978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Up Differentially Private LASSO Regularized Logistic Regression  via Faster Frank-Wolfe Iterations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raff%2C+E">Edward Raff</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+A">Amol Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+F">Fred Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation (stat.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">To the best of our knowledge, there are no methods today for training
differentially private regression models on sparse input data. To remedy this,
we adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be
aware of sparse inputs and to use them effectively. In doing so, we reduce the
training time of the algorithm from $\mathcal{O}( T D S + T N S)$ to
$\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$, where $T$ is the number of
iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features.
Our results demonstrate that this procedure can reduce runtime by a factor of
up to $2,200\times$, depending on the value of the privacy parameter $\epsilon$
and the sparsity of the dataset.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19981" title="Abstract">arXiv:2310.19981</a> [<a href="/pdf/2310.19981" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &#x27;Person&#x27; == Light-skinned, Western Man, and Sexualization of Women of  Color: Stereotypes in Stable Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Sourojit Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Caliskan%2C+A">Aylin Caliskan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Upcoming publication, Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We study stereotypes embedded within one of the most popular text-to-image
generators: Stable Diffusion. We examine what stereotypes of gender and
nationality/continental identity does Stable Diffusion display in the absence
of such information i.e. what gender and nationality/continental identity is
assigned to `a person', or to `a person from Asia'. Using vision-language model
CLIP's cosine similarity to compare images generated by CLIP-based Stable
Diffusion v2.1 verified by manual examination, we chronicle results from 136
prompts (50 results/prompt) of front-facing images of persons from 6 different
continents, 27 nationalities and 3 genders. We observe how Stable Diffusion
outputs of `a person' without any additional gender/nationality information
correspond closest to images of men and least with persons of nonbinary gender,
and to persons from Europe/North America over Africa/Asia, pointing towards
Stable Diffusion having a concerning representation of personhood to be a
European/North American man. We also show continental stereotypes and resultant
harms e.g. a person from Oceania is deemed to be Australian/New Zealander over
Papua New Guinean, pointing to the erasure of Indigenous Oceanic peoples, who
form a majority over descendants of colonizers both in Papua New Guinea and in
Oceania overall. Finally, we unexpectedly observe a pattern of
oversexualization of women, specifically Latin American, Mexican, Indian and
Egyptian women relative to other nationalities, measured through an NSFW
detector. This demonstrates how Stable Diffusion perpetuates Western
fetishization of women of color through objectification in media, which if left
unchecked will amplify this stereotypical representation. Image datasets are
made publicly available.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19983" title="Abstract">arXiv:2310.19983</a> [<a href="/pdf/2310.19983" title="Download PDF">pdf</a>, <a href="/format/2310.19983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational Design of Magnetic Soft Shape-Forming Catheters using the  Material Point Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davy%2C+J">Joshua Davy</a>, 
<a href="/search/cs?searchtype=author&query=Lloyd%2C+P">Peter Lloyd</a>, 
<a href="/search/cs?searchtype=author&query=Chandler%2C+J+H">James H.Chandler</a>, 
<a href="/search/cs?searchtype=author&query=Valdastri%2C+P">Pietro Valdastri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 2 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IROS 2023 Workshop: Data vs Model in Medical Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Magnetic Soft Catheters (MSCs) are capable of miniaturization due to the use
of an external magnetic field for actuation. Through careful design of the
magnetic elements within the MSC and the external magnetic field, the shape
along the full length of the catheter can be precisely controlled. However,
modeling of the magnetic-soft material is challenging due to the complex
relationship between magnetic and elastic stresses within the material.
Approaches based on traditional Finite Element Methods (FEM) lead to high
computation time and rely on proprietary implementations. In this work, we
showcase the use of our recently presented open-source simulation framework
based on the Material Point Method (MPM) for the computational design of
magnetic soft catheters to realize arbitrary shapes in 3D, and to facilitate
follow-the-leader shape-forming insertion.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19986" title="Abstract">arXiv:2310.19986</a> [<a href="/pdf/2310.19986" title="Download PDF">pdf</a>, <a href="/format/2310.19986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Weak Decision Boundaries in Image Classification by  Leveraging Web Search and Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dammu%2C+P+P+S">Preetam Prabhu Srikar Dammu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yunhe Feng</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+C">Chirag Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Note: This is a copy of the copyrighted version published in IJCAI 2023 (DOI: <a href="https://doi.org/10.24963/ijcai.2023/659">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Machine learning (ML) technologies are known to be riddled with ethical and
operational problems, however, we are witnessing an increasing thrust by
businesses to deploy them in sensitive applications. One major issue among many
is that ML models do not perform equally well for underrepresented groups. This
puts vulnerable populations in an even disadvantaged and unfavorable position.
We propose an approach that leverages the power of web search and generative
models to alleviate some of the shortcomings of discriminative models. We
demonstrate our method on an image classification problem using ImageNet's
People Subtree subset, and show that it is effective in enhancing robustness
and mitigating bias in certain classes that represent vulnerable populations
(e.g., female doctor of color). Our new method is able to (1) identify weak
decision boundaries for such classes; (2) construct search queries for Google
as well as text for generating images through DALL-E 2 and Stable Diffusion;
and (3) show how these newly captured training samples could alleviate
population bias issue. While still improving the model's overall performance
considerably, we achieve a significant reduction (77.30\%) in the model's
gender accuracy disparity. In addition to these improvements, we observed a
notable enhancement in the classifier's decision boundary, as it is
characterized by fewer weakspots and an increased separation between classes.
Although we showcase our method on vulnerable populations in this study, the
proposed technique is extendable to a wide range of problems and domains.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19990" title="Abstract">arXiv:2310.19990</a> [<a href="/pdf/2310.19990" title="Download PDF">pdf</a>, <a href="/format/2310.19990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Limits of Learned Local Search Heuristics: Are You the  Mightiest of the Meek?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nath%2C+A">Ankur Nath</a>, 
<a href="/search/cs?searchtype=author&query=Kuhnle%2C+A">Alan Kuhnle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, combining neural networks with local search heuristics has
become popular in the field of combinatorial optimization. Despite its
considerable computational demands, this approach has exhibited promising
outcomes with minimal manual engineering. However, we have identified three
critical limitations in the empirical evaluation of these integration attempts.
Firstly, instances with moderate complexity and weak baselines pose a challenge
in accurately evaluating the effectiveness of learning-based approaches.
Secondly, the absence of an ablation study makes it difficult to quantify and
attribute improvements accurately to the deep learning architecture. Lastly,
the generalization of learned heuristics across diverse distributions remains
underexplored. In this study, we conduct a comprehensive investigation into
these identified limitations. Surprisingly, we demonstrate that a simple
learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA)
learned heuristics in terms of performance and generalizability. Our findings
challenge prevailing assumptions and open up exciting avenues for future
research and innovation in combinatorial optimization.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19991" title="Abstract">arXiv:2310.19991</a> [<a href="/pdf/2310.19991" title="Download PDF">pdf</a>, <a href="/format/2310.19991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Minghao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Venkataraman%2C+S">Shivaram Venkataraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">As neural networks (NN) are deployed across diverse sectors, their energy
demand correspondingly grows. While several prior works have focused on
reducing energy consumption during training, the continuous operation of
ML-powered systems leads to significant energy use during inference. This paper
investigates how the configuration of on-device hardware-elements such as GPU,
memory, and CPU frequency, often neglected in prior studies, affects energy
consumption for NN inference with regular fine-tuning. We propose PolyThrottle,
a solution that optimizes configurations across individual hardware components
using Constrained Bayesian Optimization in an energy-conserving manner. Our
empirical evaluation uncovers novel facets of the energy-performance
equilibrium showing that we can save up to 36 percent of energy for popular
models. We also validate that PolyThrottle can quickly converge towards
near-optimal settings while satisfying application constraints.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19995" title="Abstract">arXiv:2310.19995</a> [<a href="/pdf/2310.19995" title="Download PDF">pdf</a>, <a href="/format/2310.19995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emotional Theory of Mind: Bridging Fast Visual Processing with Slow  Linguistic Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Etesam%2C+Y">Yasaman Etesam</a>, 
<a href="/search/cs?searchtype=author&query=Yalcin%2C+O+N">Ozge Nilay Yalcin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+A">Angelica Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages(including references and appendix), 8 Tables, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The emotional theory of mind problem in images is an emotion recognition
task, specifically asking "How does the person in the bounding box feel?"
Facial expressions, body pose, contextual information and implicit commonsense
knowledge all contribute to the difficulty of the task, making this task
currently one of the hardest problems in affective computing. The goal of this
work is to evaluate the emotional commonsense knowledge embedded in recent
large vision language models (CLIP, LLaVA) and large language models (GPT-3.5)
on the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely
text-based language model on images, we construct "narrative captions" relevant
to emotion perception, using a set of 872 physical social signal descriptions
related to 26 emotional categories, along with 224 labels for emotionally
salient environmental contexts, sourced from writer's guides for character
expressions and settings. We evaluate the use of the resulting captions in an
image-to-language-to-emotion task. Experiments using zero-shot vision-language
models on EMOTIC show that combining "fast" and "slow" reasoning is a promising
way forward to improve emotion recognition systems. Nevertheless, a gap remains
in the zero-shot emotional theory of mind task compared to prior work trained
on the EMOTIC dataset.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19996" title="Abstract">arXiv:2310.19996</a> [<a href="/pdf/2310.19996" title="Download PDF">pdf</a>, <a href="/format/2310.19996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Anchor Label Propagation for Transductive Few-Shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazarou%2C+M">Michalis Lazarou</a>, 
<a href="/search/cs?searchtype=author&query=Avrithis%2C+Y">Yannis Avrithis</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+G">Guangyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Stathaki%2C+T">Tania Stathaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published in ICIP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Few-shot learning addresses the issue of classifying images using limited
labeled data. Exploiting unlabeled data through the use of transductive
inference methods such as label propagation has been shown to improve the
performance of few-shot learning significantly. Label propagation infers
pseudo-labels for unlabeled data by utilizing a constructed graph that exploits
the underlying manifold structure of the data. However, a limitation of the
existing label propagation approaches is that the positions of all data points
are fixed and might be sub-optimal so that the algorithm is not as effective as
possible. In this work, we propose a novel algorithm that adapts the feature
embeddings of the labeled data by minimizing a differentiable loss function
optimizing their positions in the manifold in the process. Our novel algorithm,
Adaptive Anchor Label Propagation}, outperforms the standard label propagation
algorithm by as much as 7% and 2% in the 1-shot and 5-shot settings
respectively. We provide experimental results highlighting the merits of our
algorithm on four widely used few-shot benchmark datasets, namely miniImageNet,
tieredImageNet, CUB and CIFAR-FS and two commonly used backbones, ResNet12 and
WideResNet-28-10. The source code can be found at
https://github.com/MichalisLazarou/A2LP.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19998" title="Abstract">arXiv:2310.19998</a> [<a href="/pdf/2310.19998" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative retrieval-augmented ontologic graph and multi-agent  strategies for interpretive large language model-based materials design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buehler%2C+M+J">Markus J. Buehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Materials Science (cond-mat.mtrl-sci); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">Transformer neural networks show promising capabilities, in particular for
uses in materials analysis, design and manufacturing, including their capacity
to work effectively with both human language, symbols, code, and numerical
data. Here we explore the use of large language models (LLMs) as a tool that
can support engineering analysis of materials, applied to retrieving key
information about subject areas, developing research hypotheses, discovery of
mechanistic relationships across disparate areas of knowledge, and writing and
executing simulation codes for active knowledge generation based on physical
ground truths. When used as sets of AI agents with specific features,
capabilities, and instructions, LLMs can provide powerful problem solution
strategies for applications in analysis and design problems. Our experiments
focus on using a fine-tuned model, MechGPT, developed based on training data in
the mechanics of materials domain. We first affirm how finetuning endows LLMs
with reasonable understanding of domain knowledge. However, when queried
outside the context of learned matter, LLMs can have difficulty to recall
correct information. We show how this can be addressed using
retrieval-augmented Ontological Knowledge Graph strategies that discern how the
model understands what concepts are important and how they are related.
Illustrated for a use case of relating distinct areas of knowledge - here,
music and proteins - such strategies can also provide an interpretable graph
structure with rich information at the node, edge and subgraph level. We
discuss nonlinear sampling strategies and agent-based modeling applied to
complex question answering, code generation and execution in the context of
automated force field development from actively learned Density Functional
Theory (DFT) modeling, and data analysis.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19999" title="Abstract">arXiv:2310.19999</a> [<a href="/pdf/2310.19999" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systems Interoperability Types: A Tertiary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maciel%2C+R+S+P">Rita S. P. Maciel</a>, 
<a href="/search/cs?searchtype=author&query=Valle%2C+P+H">Pedro H. Valle</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+K+S">K&#xe9;cia S. Santos</a>, 
<a href="/search/cs?searchtype=author&query=Nakagawa%2C+E+Y">Elisa Y. Nakagawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Interoperability has been a focus of attention over at least four decades,
with the emergence of several interoperability types (or levels), diverse
models, frameworks, and solutions, also as a result of a continuous effort from
different domains. The current heterogeneity in technologies such as
blockchain, IoT and new application domains such as Industry 4.0 brings not
only new interaction possibilities but also challenges for interoperability.
Moreover, confusion and ambiguity in the current understanding of
interoperability types exist, hampering stakeholders' communication and
decision making. This work presents an updated panorama of software-intensive
systems interoperability with particular attention to its types. For this, we
conducted a tertiary study that scrutinized 37 secondary studies published from
2012 to 2023, from which we found 36 interoperability types associated with 117
different definitions, besides 13 interoperability models and six frameworks in
various domains. This panorama reveals that the concern with interoperability
has migrated from technical to social-technical issues going beyond the
software systems' boundary and still requiring solving many open issues. We
also address the urgent actions and also potential research opportunities to
leverage interoperability as a multidisciplinary research field to achieve
low-coupled, cost-effective, and interoperable systems.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20003" title="Abstract">arXiv:2310.20003</a> [<a href="/pdf/2310.20003" title="Download PDF">pdf</a>, <a href="/format/2310.20003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Early Detection of Depression and Eating Disorders in Spanish: UNSL at  MentalRiskES 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thompson%2C+H">Horacio Thompson</a>, 
<a href="/search/cs?searchtype=author&query=Errecalde%2C+M">Marcelo Errecalde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Iberian Languages Evaluation Forum (IberLEF 2023), Ja\'en, Spain
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CEUR Workshop Proceedings 2023, vol. 3496
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">MentalRiskES is a novel challenge that proposes to solve problems related to
early risk detection for the Spanish language. The objective is to detect, as
soon as possible, Telegram users who show signs of mental disorders considering
different tasks. Task 1 involved the users' detection of eating disorders, Task
2 focused on depression detection, and Task 3 aimed at detecting an unknown
disorder. These tasks were divided into subtasks, each one defining a
resolution approach. Our research group participated in subtask A for Tasks 1
and 2: a binary classification problem that evaluated whether the users were
positive or negative. To solve these tasks, we proposed models based on
Transformers followed by a decision policy according to criteria defined by an
early detection framework. One of the models presented an extended vocabulary
with important words for each task to be solved. In addition, we applied a
decision policy based on the history of predictions that the model performs
during user evaluation. For Tasks 1 and 2, we obtained the second-best
performance according to rankings based on classification and latency,
demonstrating the effectiveness and consistency of our approaches for solving
early detection problems in the Spanish language.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20008" title="Abstract">arXiv:2310.20008</a> [<a href="/pdf/2310.20008" title="Download PDF">pdf</a>, <a href="/format/2310.20008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Tabletop Game Design: A Case Study in the Risk Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rossato%2C+L+B">Lana Bertoldo Rossato</a>, 
<a href="/search/cs?searchtype=author&query=Bombardelli%2C+L+B">Leonardo Boaventura Bombardelli</a>, 
<a href="/search/cs?searchtype=author&query=Tavares%2C+A+R">Anderson Rocha Tavares</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, accepted for publication at the XXII Braziliam Simposium on Games and Digital Entertainment (SBGames 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Creating and evaluating games manually is an arduous and laborious task.
Procedural content generation can aid by creating game artifacts, but usually
not an entire game. Evolutionary game design, which combines evolutionary
algorithms with automated playtesting, has been used to create novel board
games with simple equipment; however, the original approach does not include
complex tabletop games with dice, cards, and maps. This work proposes an
extension of the approach for tabletop games, evaluating the process by
generating variants of Risk, a military strategy game where players must
conquer map territories to win. We achieved this using a genetic algorithm to
evolve the chosen parameters, as well as a rules-based agent to test the games
and a variety of quality criteria to evaluate the new variations generated. Our
results show the creation of new variations of the original game with smaller
maps, resulting in shorter matches. Also, the variants produce more balanced
matches, maintaining the usual drama. We also identified limitations in the
process, where, in many cases, where the objective function was correctly
pursued, but the generated games were nearly trivial. This work paves the way
towards promising research regarding the use of evolutionary game design beyond
classic board games.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20009" title="Abstract">arXiv:2310.20009</a> [<a href="/pdf/2310.20009" title="Download PDF">pdf</a>, <a href="/format/2310.20009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nash or Stackelberg? -- A comparative study for game-theoretic AV  decision-making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bateman%2C+B">Brady Bateman</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+M">Ming Xin</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+H+E">H. Eric Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mushuang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, submitted to ECC24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">This paper studies game-theoretic decision-making for autonomous vehicles
(AVs). A receding horizon multi-player game is formulated to model the AV
decision-making problem. Two classes of games, including Nash game and
Stackelber games, are developed respectively. For each of the two games, two
solution settings, including pairwise games and multi-player games, are
introduced, respectively, to solve the game in multi-agent scenarios.
Comparative studies are conducted via statistical simulations to gain
understandings of the performance of the two classes of games and of the two
solution settings, respectively. The simulations are conducted in
intersection-crossing scenarios, and the game performance is quantified by
three metrics: safety, travel efficiency, and computational time.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20012" title="Abstract">arXiv:2310.20012</a> [<a href="/pdf/2310.20012" title="Download PDF">pdf</a>, <a href="/format/2310.20012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Feature Attribution for Outliers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jeff Shen</a>, 
<a href="/search/cs?searchtype=author&query=Melchior%2C+P">Peter Melchior</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, accepted to NeurIPS 2023 Workshop on Machine Learning and the Physical Sciences. Code available at <a href="https://github.com/al-jshen/imo">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Machine learning techniques can automatically identify outliers in massive
datasets, much faster and more reproducible than human inspection ever could.
But finding such outliers immediately leads to the question: which features
render this input anomalous? We propose a new feature attribution method,
Inverse Multiscale Occlusion, that is specifically designed for outliers, for
which we have little knowledge of the type of features we want to identify and
expect that the model performance is questionable because anomalous test data
likely exceed the limits of the training data. We demonstrate our method on
outliers detected in galaxy spectra from the Dark Energy Survey Instrument and
find its results to be much more interpretable than alternative attribution
approaches.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20024" title="Abstract">arXiv:2310.20024</a> [<a href="/pdf/2310.20024" title="Download PDF">pdf</a>, <a href="/format/2310.20024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology Recoverability Prediction for Ad-Hoc Robot Networks: A  Data-Driven Fault-Tolerant Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Macktoobian%2C+M">Matin Macktoobian</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Z">Zhan Shu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qing Zhao</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Signal and Information Processing over
  Networks, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Faults occurring in ad-hoc robot networks may fatally perturb their
topologies leading to disconnection of subsets of those networks. Optimal
topology synthesis is generally resource-intensive and time-consuming to be
done in real time for large ad-hoc robot networks. One should only perform
topology re-computations if the probability of topology recoverability after
the occurrence of any fault surpasses that of its irrecoverability. We
formulate this problem as a binary classification problem. Then, we develop a
two-pathway data-driven model based on Bayesian Gaussian mixture models that
predicts the solution to a typical problem by two different pre-fault and
post-fault prediction pathways. The results, obtained by the integration of the
predictions of those pathways, clearly indicate the success of our model in
solving the topology (ir)recoverability prediction problem compared to the best
of current strategies found in the literature.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20025" title="Abstract">arXiv:2310.20025</a> [<a href="/pdf/2310.20025" title="Download PDF">pdf</a>, <a href="/format/2310.20025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with  Learned Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mianchu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meng Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Spotlight Presentation at Goal-conditioned Reinforcement Learning Workshop at NeurIPS, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn
general-purpose policies from diverse and multi-task offline datasets. Despite
notable recent progress, the predominant offline GCRL methods have been
restricted to model-free approaches, constraining their capacity to tackle
limited data budgets and unseen goal generalization. In this work, we propose a
novel two-stage model-based framework, Goal-conditioned Offline Planning
(GOPlan), including (1) pretraining a prior policy capable of capturing
multi-modal action distribution within the multi-goal dataset; (2) employing
the reanalysis method with planning to generate imagined trajectories for
funetuning policies. Specifically, the prior policy is based on an
advantage-weighted Conditioned Generative Adversarial Networks that exhibits
distinct mode separation to overcome the pitfalls of out-of-distribution (OOD)
actions. For further policy optimization, the reanalysis method generates
high-quality imaginary data by planning with learned models for both
intra-trajectory and inter-trajectory goals. Through experimental evaluations,
we demonstrate that GOPlan achieves state-of-the-art performance on various
offline multi-goal manipulation tasks. Moreover, our results highlight the
superior ability of GOPlan to handle small data budgets and generalize to OOD
goals.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20030" title="Abstract">arXiv:2310.20030</a> [<a href="/pdf/2310.20030" title="Download PDF">pdf</a>, <a href="/format/2310.20030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Riemannian Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+A">Aaron Lou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minkai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Differential Geometry (math.DG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Riemannian diffusion models draw inspiration from standard Euclidean space
diffusion models to learn distributions on general manifolds. Unfortunately,
the additional geometric complexity renders the diffusion transition term
inexpressible in closed form, so prior methods resort to imprecise
approximations of the score matching training objective that degrade
performance and preclude applications in high dimensions. In this work, we
reexamine these approximations and propose several practical improvements. Our
key observation is that most relevant manifolds are symmetric spaces, which are
much more amenable to computation. By leveraging and combining various
ans\"{a}tze, we can quickly compute relevant quantities to high precision. On
low dimensional datasets, our correction produces a noticeable improvement,
allowing diffusion to compete with other methods. Additionally, we show that
our method enables us to scale to high dimensional tasks on nontrivial
manifolds. In particular, we model QCD densities on $SU(n)$ lattices and
contrastively learned embeddings on high dimensional hyperspheres.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20033" title="Abstract">arXiv:2310.20033</a> [<a href="/pdf/2310.20033" title="Download PDF">pdf</a>, <a href="/format/2310.20033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic Imitation Edit Feedback for Factual Alignment in Clinical  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+P">Prakamya Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zonghai Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beining Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+R">Rohan Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) like the GPT and LLaMA families have
demonstrated exceptional capabilities in capturing and condensing critical
contextual information and achieving state-of-the-art performance in the
summarization task. However, community concerns about these models'
hallucination issues continue to rise. LLMs sometimes generate factually
hallucinated summaries, which can be extremely harmful in the clinical domain
NLP tasks (e.g., clinical note summarization), where factually incorrect
statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using
human feedback has shown the promise of aligning LLMs to be factually
consistent during generation, but such training procedure requires high-quality
human-annotated data, which can be extremely expensive to get in the clinical
domain. In this work, we propose a new pipeline using ChatGPT instead of human
experts to generate high-quality feedback data for improving factual
consistency in the clinical note summarization task. We focus specifically on
edit feedback because recent work discusses the shortcomings of human alignment
via preference feedback in complex situations (such as clinical NLP tasks that
require extensive expert knowledge), as well as some advantages of collecting
edit feedback from domain experts. In addition, although GPT has reached the
expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much
previous work discussing whether GPT can generate expert-level edit feedback
for LMs in the clinical note summarization task. We hope to fill this gap.
Finally, our evaluations demonstrate the potential use of GPT edits in human
alignment, especially from a factuality perspective.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20034" title="Abstract">arXiv:2310.20034</a> [<a href="/pdf/2310.20034" title="Download PDF">pdf</a>, <a href="/format/2310.20034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GG-LLM: Geometrically Grounding Large Language Models for Zero-shot  Human Activity Forecasting in Human-Aware Task Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graule%2C+M+A">Moritz A. Graule</a>, 
<a href="/search/cs?searchtype=author&query=Isler%2C+V">Volkan Isler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">A robot in a human-centric environment needs to account for the human's
intent and future motion in its task and motion planning to ensure safe and
effective operation. This requires symbolic reasoning about probable future
actions and the ability to tie these actions to specific locations in the
physical environment. While one can train behavioral models capable of
predicting human motion from past activities, this approach requires large
amounts of data to achieve acceptable long-horizon predictions. More
importantly, the resulting models are constrained to specific data formats and
modalities. Moreover, connecting predictions from such models to the
environment at hand to ensure the applicability of these predictions is an
unsolved problem. We present a system that utilizes a Large Language Model
(LLM) to infer a human's next actions from a range of modalities without
fine-tuning. A novel aspect of our system that is critical to robotics
applications is that it links the predicted actions to specific locations in a
semantic map of the environment. Our method leverages the fact that LLMs,
trained on a vast corpus of text describing typical human behaviors, encode
substantial world knowledge, including probable sequences of human actions and
activities. We demonstrate how these localized activity predictions can be
incorporated in a human-aware task planner for an assistive robot to reduce the
occurrences of undesirable human-robot interactions by 29.2% on average.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20036" title="Abstract">arXiv:2310.20036</a> [<a href="/pdf/2310.20036" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Student Certificate Sharing System Using Blockchain and NFTs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khati%2C+P">Prakhyat Khati</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+A+K">Ajay Kumar Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Vassileva%2C+J">Julita Vassileva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In this paper, we propose a certificate sharing system based on blockchain
that gives students authority and control over their academic certificates. Our
strategy involves developing blockchain-based NFT certifications that can be
shared with institutions or employers using blockchain addresses. Students may
access the data created by each individual institute in a single platform,
filter the view of the relevant courses according to their requirements, and
mint their certificate metadata as NFTs. This method provides accountability of
access, comprehensive records that are permanently maintained in IPFS, and
verifiable provenance for creating, distributing, and accessing certificates.
It also makes it possible to share certificates more safely and efficiently. By
incorporating trust factors through data provenance, our system provides a
countermeasure against issues such as fake and duplicate certificates. It
addresses the challenge of the traditional certificate verification processes,
which are lengthy manual process. With this system, students can manage and
validate their academic credentials from multiple institutions in one location
while ensuring authenticity and confidentiality using digital signatures and
hashing for data protection against unauthorized access. Overall, our suggested
system ensures data safety, accountability, and confidentiality while offering
a novel approach to certificate distribution.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20038" title="Abstract">arXiv:2310.20038</a> [<a href="/pdf/2310.20038" title="Download PDF">pdf</a>, <a href="/ps/2310.20038" title="Download PostScript">ps</a>, <a href="/format/2310.20038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Nonlinear Distortion Characterization for Frequency-Selective  Massive MIMO Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salman%2C+M+B">Murat Babek Salman</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rnson%2C+E">Emil Bj&#xf6;rnson</a>, 
<a href="/search/cs?searchtype=author&query=Guvensen%2C+G+M">Gokhan Muzaffer Guvensen</a>, 
<a href="/search/cs?searchtype=author&query=Ciloglu%2C+T">Tolga Ciloglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is presented in ICC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Nonlinear distortion stemming from low-cost power amplifiers may severely
affect wireless communication performance through out-of-band (OOB) radiation
and in-band distortion. The distortion is correlated between different transmit
antennas in an antenna array, which results in a beamforming gain at the
receiver side that grows with the number of antennas. In this paper, we
investigate how the strength of the distortion is affected by the frequency
selectivity of the channel. A closed-form expression for the received
distortion power is derived as a function of the number of multipath components
(MPCs) and the delay spread, which highlight their impact. The performed
analysis, which is verified via numerical simulations, reveals that as the
number of MPCs increases, distortion exhibits distinct characteristics for
in-band and OOB frequencies. It is shown that the received in-band and OOB
distortion power is inversely proportional to the number of MPCs, and it is
reported that as the delay spread gets narrower, the in-band distortion power
is beamformed towards the intended user, which yields higher received in-band
distortion compared to the OOB distortion.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20042" title="Abstract">arXiv:2310.20042</a> [<a href="/pdf/2310.20042" title="Download PDF">pdf</a>, <a href="/format/2310.20042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comprehending Variability in Analysis Results of Software Product Lines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toledo%2C+R+F">Rafael F. Toledo</a>, 
<a href="/search/cs?searchtype=author&query=Atlee%2C+J+M">Joanne M. Atlee</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+R+M">Rui Ming Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Analyses of a software product line (SPL) typically report variable results
that are annotated with logical expressions indicating the set of product
variants for which the results hold. These expressions can get complicated and
difficult to reason about when the SPL has lots of features and product
variants. Previous work introduced a visualizer that supports filters for
highlighting the analysis results that apply to product variants of interest,
but this work was weakly evaluated. In this paper, we report on a controlled
user study that evaluates the effectiveness of this new visualizer in helping
the user search variable results and compare the results of multiple variants.
Our findings indicate that the use of the new visualizer significantly improves
the correctness and efficiency of the user's work and reduces the user's
cognitive load in working with variable results.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20046" title="Abstract">arXiv:2310.20046</a> [<a href="/pdf/2310.20046" title="Download PDF">pdf</a>, <a href="/format/2310.20046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Which Examples to Annotate for In-Context Learning? Towards Effective  and Efficient Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mavromatis%2C+C">Costas Mavromatis</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B">Balasubramaniam Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhengyuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiani Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Rangwala%2C+H">Huzefa Rangwala</a>, 
<a href="/search/cs?searchtype=author&query=Faloutsos%2C+C">Christos Faloutsos</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) can adapt to new tasks via in-context learning
(ICL). ICL is efficient as it does not require any parameter updates to the
trained LLM, but only few annotated examples as input for the LLM. In this
work, we investigate an active learning approach for ICL, where there is a
limited budget for annotating examples. We propose a model-adaptive
optimization-free algorithm, termed AdaICL, which identifies examples that the
model is uncertain about, and performs semantic diversity-based example
selection. Diversity-based sampling improves overall effectiveness, while
uncertainty sampling improves budget efficiency and helps the LLM learn new
information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage
problem, that dynamically adapts based on the model's feedback and can be
approximately solved via greedy algorithms. Extensive experiments on nine
datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy
points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient
than performing annotations uniformly at random, while it outperforms SOTA with
2x fewer ICL examples.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20049" title="Abstract">arXiv:2310.20049</a> [<a href="/pdf/2310.20049" title="Download PDF">pdf</a>, <a href="/format/2310.20049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C3%BCnzli%2C+S">Stefan K&#xfc;nzli</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C3%B6tschla%2C+F">Florain Gr&#xf6;tschla</a>, 
<a href="/search/cs?searchtype=author&query=Mathys%2C+J">Jo&#xeb;l Mathys</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Simulating fluid dynamics is crucial for the design and development process,
ranging from simple valves to complex turbomachinery. Accurately solving the
underlying physical equations is computationally expensive. Therefore,
learning-based solvers that model interactions on meshes have gained interest
due to their promising speed-ups. However, it is unknown to what extent these
models truly understand the underlying physical principles and can generalize
rather than interpolate. Generalization is a key requirement for a
general-purpose fluid simulator, which should adapt to different topologies,
resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to
test the \textit{generalization} of learned graph-based fluid simulators. SURF
comprises individual datasets and provides specific performance and
generalization metrics for evaluating and comparing different models. We
empirically demonstrate the applicability of SURF by thoroughly investigating
the two state-of-the-art graph-based models, yielding new insights into their
generalization.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20051" title="Abstract">arXiv:2310.20051</a> [<a href="/pdf/2310.20051" title="Download PDF">pdf</a>, <a href="/format/2310.20051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Expressibility of Polynomial based Attention Scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guangyi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Junze Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.11685">arXiv:2310.11685</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large language models (LLMs) have significantly improved various aspects of
our daily lives. These models have impacted numerous domains, from healthcare
to education, enhancing productivity, decision-making processes, and
accessibility. As a result, they have influenced and, to some extent, reshaped
people's lifestyles. However, the quadratic complexity of attention in
transformer architectures poses a challenge when scaling up these models for
processing long textual contexts. This issue makes it impractical to train very
large models on lengthy texts or use them efficiently during inference. While a
recent study by [KMZ23] introduced a technique that replaces the softmax with a
polynomial function and polynomial sketching to speed up attention mechanisms,
the theoretical understandings of this new approach are not yet well
understood.
<br />In this paper, we offer a theoretical analysis of the expressive capabilities
of polynomial attention. Our study reveals a disparity in the ability of
high-degree and low-degree polynomial attention. Specifically, we construct two
carefully designed datasets, namely $\mathcal{D}_0$ and $\mathcal{D}_1$, where
$\mathcal{D}_1$ includes a feature with a significantly larger value compared
to $\mathcal{D}_0$. We demonstrate that with a sufficiently high degree
$\beta$, a single-layer polynomial attention network can distinguish between
$\mathcal{D}_0$ and $\mathcal{D}_1$. However, with a low degree $\beta$, the
network cannot effectively separate the two datasets. This analysis underscores
the greater effectiveness of high-degree polynomials in amplifying large values
and distinguishing between datasets. Our analysis offers insight into the
representational capacity of polynomial attention and provides a rationale for
incorporating higher-degree polynomials in attention mechanisms to capture
intricate linguistic correlations.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20052" title="Abstract">arXiv:2310.20052</a> [<a href="/pdf/2310.20052" title="Download PDF">pdf</a>, <a href="/format/2310.20052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class  Incremental Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+A">Anton Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yaqian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gomes%2C+H+M">Heitor Murilo Gomes</a>, 
<a href="/search/cs?searchtype=author&query=Bifet%2C+A">Albert Bifet</a>, 
<a href="/search/cs?searchtype=author&query=Pfahringer%2C+B">Bernhard Pfahringer</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 32nd ACM international conference on
  information and knowledge management, CIKM 2023, birmingham, united kingdom,
  october 21-25, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Continual learning aims to create artificial neural networks capable of
accumulating knowledge and skills through incremental training on a sequence of
tasks. The main challenge of continual learning is catastrophic interference,
wherein new knowledge overrides or interferes with past knowledge, leading to
forgetting. An associated issue is the problem of learning "cross-task
knowledge," where models fail to acquire and retain knowledge that helps
differentiate classes across task boundaries. A common solution to both
problems is "replay," where a limited buffer of past instances is utilized to
learn cross-task knowledge and mitigate catastrophic interference. However, a
notable drawback of these methods is their tendency to overfit the limited
replay buffer. In contrast, our proposed solution, SurpriseNet, addresses
catastrophic interference by employing a parameter isolation method and
learning cross-task knowledge using an auto-encoder inspired by anomaly
detection. SurpriseNet is applicable to both structured and unstructured data,
as it does not rely on image-specific inductive biases. We have conducted
empirical experiments demonstrating the strengths of SurpriseNet on various
traditional vision continual-learning benchmarks, as well as on structured data
datasets. Source code made available at https://doi.org/10.5281/zenodo.8247906
and https://github.com/tachyonicClock/SurpriseNet-CIKM-23
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20054" title="Abstract">arXiv:2310.20054</a> [<a href="/pdf/2310.20054" title="Download PDF">pdf</a>, <a href="/format/2310.20054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Hierarchical Monte Carlo Belief-State Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamgochian%2C+A">Arec Jamgochian</a>, 
<a href="/search/cs?searchtype=author&query=Buurmeijer%2C+H">Hugo Buurmeijer</a>, 
<a href="/search/cs?searchtype=author&query=Wray%2C+K+H">Kyle H. Wray</a>, 
<a href="/search/cs?searchtype=author&query=Corso%2C+A">Anthony Corso</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review for the 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Optimal plans in Constrained Partially Observable Markov Decision Processes
(CPOMDPs) maximize reward objectives while satisfying hard cost constraints,
generalizing safe planning under state and transition uncertainty.
Unfortunately, online CPOMDP planning is extremely difficult in large or
continuous problem domains. In many large robotic domains, hierarchical
decomposition can simplify planning by using tools for low-level control given
high-level action primitives (options). We introduce Constrained Options Belief
Tree Search (COBeTS) to leverage this hierarchy and scale online search-based
CPOMDP planning to large robotic problems. We show that if primitive option
controllers are defined to satisfy assigned constraint budgets, then COBeTS
will satisfy constraints anytime. Otherwise, COBeTS will guide the search
towards a safe sequence of option primitives, and hierarchical monitoring can
be used to achieve runtime safety. We demonstrate COBeTS in several
safety-critical, constrained partially observable robotic domains, showing that
it can plan successfully in continuous CPOMDPs while non-hierarchical baselines
cannot.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20056" title="Abstract">arXiv:2310.20056</a> [<a href="/pdf/2310.20056" title="Download PDF">pdf</a>, <a href="/format/2310.20056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the data-driven description of lattice materials mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ben-Yelun%2C+I">Ismael Ben-Yelun</a>, 
<a href="/search/math?searchtype=author&query=Irastorza-Valera%2C+L">Luis Irastorza-Valera</a>, 
<a href="/search/math?searchtype=author&query=Saucedo-Mora%2C+L">Luis Saucedo-Mora</a>, 
<a href="/search/math?searchtype=author&query=Mont%C3%A1ns%2C+F+J">Francisco Javier Mont&#xe1;ns</a>, 
<a href="/search/math?searchtype=author&query=Chinesta%2C+F">Francisco Chinesta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In the emerging field of mechanical metamaterials, using periodic lattice
structures as a primary ingredient is relatively frequent. However, the choice
of aperiodic lattices in these structures presents unique advantages regarding
failure, e.g., buckling or fracture, because avoiding repeated patterns
prevents global failures, with local failures occurring in turn that can
beneficially delay structural collapse. Therefore, it is expedient to develop
models for computing efficiently the effective mechanical properties in
lattices from different general features while addressing the challenge of
presenting topologies (or graphs) of different sizes. In this paper, we develop
a deep learning model to predict energetically-equivalent mechanical properties
of linear elastic lattices effectively. Considering the lattice as a graph and
defining material and geometrical features on such, we show that Graph Neural
Networks provide more accurate predictions than a dense, fully connected
strategy, thanks to the geometrically induced bias through graph
representation, closer to the underlying equilibrium laws from mechanics solved
in the direct problem. Leveraging the efficient forward-evaluation of a vast
number of lattices using this surrogate enables the inverse problem, i.e., to
obtain a structure having prescribed specific behavior, which is ultimately
suitable for multiscale structural optimization problems.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20057" title="Abstract">arXiv:2310.20057</a> [<a href="/pdf/2310.20057" title="Download PDF">pdf</a>, <a href="/format/2310.20057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SolarFormer: Multi-scale Transformer for Solar PV Profiling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Luis%2C+A">Adrian de Luis</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+M">Minh Tran</a>, 
<a href="/search/cs?searchtype=author&query=Hanyu%2C+T">Taisei Hanyu</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+A">Anh Tran</a>, 
<a href="/search/cs?searchtype=author&query=Haitao%2C+L">Liao Haitao</a>, 
<a href="/search/cs?searchtype=author&query=McCann%2C+R">Roy McCann</a>, 
<a href="/search/cs?searchtype=author&query=Mantooth%2C+A">Alan Mantooth</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Ying Huang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+N">Ngan Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">As climate change intensifies, the global imperative to shift towards
sustainable energy sources becomes more pronounced. Photovoltaic (PV) energy is
a favored choice due to its reliability and ease of installation. Accurate
mapping of PV installations is crucial for understanding their adoption and
informing energy policy. To meet this need, we introduce the SolarFormer,
designed to segment solar panels from aerial imagery, offering insights into
their location and size. However, solar panel identification in Computer Vision
is intricate due to various factors like weather conditions, roof conditions,
and Ground Sampling Distance (GSD) variations. To tackle these complexities, we
present the SolarFormer, featuring a multi-scale Transformer encoder and a
masked-attention Transformer decoder. Our model leverages low-level features
and incorporates an instance query mechanism to enhance the localization of
solar PV installations. We rigorously evaluated our SolarFormer using diverse
datasets, including GGE (France), IGN (France), and USGS (California, USA),
across different GSDs. Our extensive experiments consistently demonstrate that
our model either matches or surpasses state-of-the-art models, promising
enhanced solar panel segmentation for global sustainable energy initiatives.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20059" title="Abstract">arXiv:2310.20059</a> [<a href="/pdf/2310.20059" title="Download PDF">pdf</a>, <a href="/format/2310.20059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concept Alignment as a Prerequisite for Value Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rane%2C+S">Sunayana Rane</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+M">Mark Ho</a>, 
<a href="/search/cs?searchtype=author&query=Sucholutsky%2C+I">Ilia Sucholutsky</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Value alignment is essential for building AI systems that can safely and
reliably interact with people. However, what a person values -- and is even
capable of valuing -- depends on the concepts that they are currently using to
understand and evaluate what happens in the world. The dependence of values on
concepts means that concept alignment is a prerequisite for value alignment --
agents need to align their representation of a situation with that of humans in
order to successfully align their values. Here, we formally analyze the concept
alignment problem in the inverse reinforcement learning setting, show how
neglecting concept alignment can lead to systematic value mis-alignment, and
describe an approach that helps minimize such failure modes by jointly
reasoning about a person's concepts and values. Additionally, we report
experimental results with human participants showing that humans reason about
the concepts used by an agent when acting intentionally, in line with our joint
reasoning model.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20061" title="Abstract">arXiv:2310.20061</a> [<a href="/pdf/2310.20061" title="Download PDF">pdf</a>, <a href="/format/2310.20061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation Framework for Understanding Sensitive Attribute Association  Bias in Latent Factor Recommendation Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beattie%2C+L">Lex Beattie</a>, 
<a href="/search/cs?searchtype=author&query=Corpus%2C+I">Isabel Corpus</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L+H">Lucy H. Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ravichandran%2C+P">Praveen Ravichandran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">We present a novel evaluation framework for representation bias in latent
factor recommendation (LFR) algorithms. Our framework introduces the concept of
attribute association bias in recommendations allowing practitioners to explore
how recommendation systems can introduce or amplify stakeholder representation
harm. Attribute association bias (AAB) occurs when sensitive attributes become
semantically captured or entangled in the trained recommendation latent space.
This bias can result in the recommender reinforcing harmful stereotypes, which
may result in downstream representation harms to system consumer and provider
stakeholders. LFR models are at risk of experiencing AAB due to their ability
to entangle explicit and implicit attributes into the trained latent space.
Understanding this phenomenon is essential due to the increasingly common use
of entity vectors as attributes in downstream components in hybrid industry
recommendation systems. We provide practitioners with a framework for executing
disaggregated evaluations of AAB within broader algorithmic auditing
frameworks. Inspired by research in natural language processing (NLP) observing
gender bias in word embeddings, our framework introduces AAB evaluation methods
specifically for recommendation entity vectors. We present four evaluation
strategies for sensitive AAB in LFR models: attribute bias directions,
attribute association bias metrics, classification for explaining bias, and
latent space visualization. We demonstrate the utility of our framework by
evaluating user gender AAB regarding podcast genres with an industry case study
of a production-level DNN recommendation model. We uncover significant levels
of user gender AAB when user gender is used and removed as a model feature
during training, pointing to the potential for systematic bias in LFR model
outputs.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20062" title="Abstract">arXiv:2310.20062</a> [<a href="/pdf/2310.20062" title="Download PDF">pdf</a>, <a href="/format/2310.20062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramesh%2C+V">Vishal Ramesh</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+N">Naman Goel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Synthetic data is emerging as a promising way to harness the value of data,
while reducing privacy risks. The potential of synthetic data is not limited to
privacy-friendly data release, but also includes complementing real data in
use-cases such as training machine learning algorithms that are more fair and
robust to distribution shifts etc. There is a lot of interest in algorithmic
advances in synthetic data generation for providing better privacy and
statistical guarantees and for its better utilisation in machine learning
pipelines. However, for responsible and trustworthy synthetic data generation,
it is not sufficient to focus only on these algorithmic aspects and instead, a
holistic view of the synthetic data generation pipeline must be considered. We
build a novel system that allows the contributors of real data to autonomously
participate in differentially private synthetic data generation without relying
on a trusted centre. Our modular, general and scalable solution is based on
three building blocks namely: Solid (Social Linked Data), MPC (Secure
Multi-Party Computation) and Trusted Execution Environments (TEEs). Solid is a
specification that lets people store their data securely in decentralised data
stores called Pods and control access to their data. MPC refers to the set of
cryptographic methods for different parties to jointly compute a function over
their inputs while keeping those inputs private. TEEs such as Intel SGX rely on
hardware based features for confidentiality and integrity of code and data. We
show how these three technologies can be effectively used to address various
challenges in responsible and trustworthy synthetic data generation by
ensuring: 1) contributor autonomy, 2) decentralisation, 3) privacy and 4)
scalability. We support our claims with rigorous empirical results on simulated
and real datasets and different synthetic data generation algorithms.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20064" title="Abstract">arXiv:2310.20064</a> [<a href="/pdf/2310.20064" title="Download PDF">pdf</a>, <a href="/format/2310.20064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scalable Training Strategy for Blind Multi-Distribution Noise Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kevin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kulshrestha%2C+S">Sakshum Kulshrestha</a>, 
<a href="/search/cs?searchtype=author&query=Metzler%2C+C">Christopher Metzler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Despite recent advances, developing general-purpose universal denoising and
artifact-removal networks remains largely an open problem: Given fixed network
weights, one inherently trades-off specialization at one task (e.g.,~removing
Poisson noise) for performance at another (e.g.,~removing speckle noise). In
addition, training such a network is challenging due to the curse of
dimensionality: As one increases the dimensions of the specification-space
(i.e.,~the number of parameters needed to describe the noise distribution) the
number of unique specifications one needs to train for grows exponentially.
Uniformly sampling this space will result in a network that does well at very
challenging problem specifications but poorly at easy problem specifications,
where even large errors will have a small effect on the overall mean squared
error.
<br />In this work we propose training denoising networks using an
adaptive-sampling/active-learning strategy. Our work improves upon a recently
proposed universal denoiser training strategy by extending these results to
higher dimensions and by incorporating a polynomial approximation of the true
specification-loss landscape. This approximation allows us to reduce training
times by almost two orders of magnitude. We test our method on simulated joint
Poisson-Gaussian-Speckle noise and demonstrate that with our proposed training
strategy, a single blind, generalist denoiser network can achieve peak
signal-to-noise ratios within a uniform bound of specialized denoiser networks
across a large range of operating conditions. We also capture a small dataset
of images with varying amounts of joint Poisson-Gaussian-Speckle noise and
demonstrate that a universal denoiser trained using our adaptive-sampling
strategy outperforms uniformly trained baselines.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20065" title="Abstract">arXiv:2310.20065</a> [<a href="/pdf/2310.20065" title="Download PDF">pdf</a>, <a href="/format/2310.20065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LinFlo-Net: A two-stage deep learning method to generate simulation  ready meshes of the heart
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+A">Arjun Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+F">Fanwei Kong</a>, 
<a href="/search/cs?searchtype=author&query=Shadden%2C+S">Shawn Shadden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to to the Journal of Biomechanical Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a deep learning model to automatically generate computer models of
the human heart from patient imaging data with an emphasis on its capability to
generate thin-walled cardiac structures. Our method works by deforming a
template mesh to fit the cardiac structures to the given image. Compared with
prior deep learning methods that adopted this approach, our framework is
designed to minimize mesh self-penetration, which typically arises when
deforming surface meshes separated by small distances. We achieve this by using
a two-stage diffeomorphic deformation process along with a novel loss function
derived from the kinematics of motion that penalizes surface contact and
interpenetration. Our model demonstrates comparable accuracy with
state-of-the-art methods while additionally producing meshes free of
self-intersections. The resultant meshes are readily usable in physics based
simulation, minimizing the need for post-processing and cleanup.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20067" title="Abstract">arXiv:2310.20067</a> [<a href="/pdf/2310.20067" title="Download PDF">pdf</a>, <a href="/format/2310.20067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vignat: Vulnerability identification by learning code semantics via  graph attention networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kaiser%2C+G">Gail Kaiser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vulnerability identification is crucial to protect software systems from
attacks for cyber-security. However, huge projects have more than millions of
lines of code, and the complex dependencies make it hard to carry out
traditional static and dynamic methods. Furthermore, the semantic structure of
various types of vulnerabilities differs greatly and may occur simultaneously,
making general rule-based methods difficult to extend. In this paper, we
propose \textit{Vignat}, a novel attention-based framework for identifying
vulnerabilities by learning graph-level semantic representations of code. We
represent codes with code property graphs (CPGs) in fine grain and use graph
attention networks (GATs) for vulnerability detection. The results show that
Vignat is able to achieve $57.38\%$ accuracy on reliable datasets derived from
popular C libraries. Furthermore, the interpretability of our GATs provides
valuable insights into vulnerability patterns.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20071" title="Abstract">arXiv:2310.20071</a> [<a href="/pdf/2310.20071" title="Download PDF">pdf</a>, <a href="/format/2310.20071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals  in Factorized Orthogonal Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengzhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kimura%2C+T">Tomoyoshi Kimura</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dongxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Diggavi%2C+S">Suhas Diggavi</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+M">Mani Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Abdelzaher%2C+T">Tarek Abdelzaher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at: [github](<a href="https://github.com/tomoyoshki/focal">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">This paper proposes a novel contrastive learning framework, called FOCAL, for
extracting comprehensive features from multimodal time-series sensing signals
through self-supervised training. Existing multimodal contrastive frameworks
mostly rely on the shared information between sensory modalities, but do not
explicitly consider the exclusive modality information that could be critical
to understanding the underlying sensing physics. Besides, contrastive
frameworks for time series have not handled the temporal information locality
appropriately. FOCAL solves these challenges by making the following
contributions: First, given multimodal time series, it encodes each modality
into a factorized latent space consisting of shared features and private
features that are orthogonal to each other. The shared space emphasizes feature
patterns consistent across sensory modalities through a modal-matching
objective. In contrast, the private space extracts modality-exclusive
information through a transformation-invariant objective. Second, we propose a
temporal structural constraint for modality features, such that the average
distance between temporally neighboring samples is no larger than that of
temporally distant samples. Extensive evaluations are performed on four
multimodal sensing datasets with two backbone encoders and two classifiers to
demonstrate the superiority of FOCAL. It consistently outperforms the
state-of-the-art baselines in downstream tasks with a clear margin, under
different ratios of available labels. The code and self-collected dataset are
available at https://github.com/tomoyoshki/focal.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20072" title="Abstract">arXiv:2310.20072</a> [<a href="/pdf/2310.20072" title="Download PDF">pdf</a>, <a href="/format/2310.20072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Evaluation of Generative Models with Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehri%2C+S">Shuhaib Mehri</a>, 
<a href="/search/cs?searchtype=author&query=Shwartz%2C+V">Vered Shwartz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Automatic evaluation of natural language generation has long been an elusive
goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate
human judgements for a particular task and evaluation criterion. Inspired by
the generalization ability of instruction-tuned models, we propose a learned
metric based on instruction tuning. To test our approach, we collected HEAP, a
dataset of human judgements across various NLG tasks and evaluation criteria.
Our findings demonstrate that instruction tuning language models on HEAP yields
good performance on many evaluation tasks, though some criteria are less
trivial to learn than others. Further, jointly training on multiple tasks can
yield additional performance improvements, which can be beneficial for future
tasks with little to no human annotated data.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20075" title="Abstract">arXiv:2310.20075</a> [<a href="/pdf/2310.20075" title="Download PDF">pdf</a>, <a href="/format/2310.20075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meek Separators and Their Applications in Targeted Causal Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shiragur%2C+K">Kirankumar Shiragur</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Uhler%2C+C">Caroline Uhler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Discrete Mathematics (cs.DM); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">Learning causal structures from interventional data is a fundamental problem
with broad applications across various fields. While many previous works have
focused on recovering the entire causal graph, in practice, there are scenarios
where learning only part of the causal graph suffices. This is called
$targeted$ causal discovery. In our work, we focus on two such well-motivated
problems: subset search and causal matching. We aim to minimize the number of
interventions in both cases.
<br />Towards this, we introduce the $Meek~separator$, which is a subset of
vertices that, when intervened, decomposes the remaining unoriented edges into
smaller connected components. We then present an efficient algorithm to find
Meek separators that are of small sizes. Such a procedure is helpful in
designing various divide-and-conquer-based approaches. In particular, we
propose two randomized algorithms that achieve logarithmic approximation for
subset search and causal matching, respectively. Our results provide the first
known average-case provable guarantees for both problems. We believe that this
opens up possibilities to design near-optimal methods for many other targeted
causal structure learning problems arising from various applications.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20077" title="Abstract">arXiv:2310.20077</a> [<a href="/pdf/2310.20077" title="Download PDF">pdf</a>, <a href="/format/2310.20077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial Tensorized Transformers for Natural Language Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vadlamannati%2C+S">Subhadra Vadlamannati</a>, 
<a href="/search/cs?searchtype=author&query=Solgi%2C+R">Ryan Solgi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Review under the 16th International Conference on Agents and Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The transformer architecture has revolutionized Natural Language Processing
(NLP) and other machine-learning tasks, due to its unprecedented accuracy.
However, their extensive memory and parameter requirements often hinder their
practical applications. In this work, we study the effect of tensor-train
decomposition to improve the accuracy and compress transformer vision-language
neural networks, namely BERT and ViT. We focus both on embedding-layer
compression and partial tensorization of neural networks (PTNN) through an
algorithmic approach. Our novel PTNN approach significantly improves the
accuracy of existing models by up to 5%, all without the need for post-training
adjustments, breaking new ground in the field of tensor decomposition.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20078" title="Abstract">arXiv:2310.20078</a> [<a href="/pdf/2310.20078" title="Download PDF">pdf</a>, <a href="/format/2310.20078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TorchProbe: Fuzzing Dynamic Deep Learning Compilers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Q">Qidong Su</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+C">Chuqin Geng</a>, 
<a href="/search/cs?searchtype=author&query=Pekhimenko%2C+G">Gennady Pekhimenko</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+X">Xujie Si</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Static and dynamic computational graphs represent two distinct approaches to
constructing deep learning frameworks. The former prioritizes compiler-based
optimizations, while the latter focuses on programmability and
user-friendliness. The recent release of PyTorch 2.0, which supports compiling
arbitrary deep learning programs in Python, signifies a new direction in the
evolution of deep learning infrastructure to incorporate compiler techniques in
a more dynamic manner and support more dynamic language features like dynamic
control flows and closures. Given PyTorch's seamless integration with Python,
its compiler aims to support arbitrary deep learning code written in Python.
However, the inherent dynamism of Python poses challenges to the completeness
and robustness of the compiler. While recent research has introduced fuzzing to
test deep learning compilers, there is still a lack of comprehensive analysis
on how to test dynamic features. To address this issue, we propose several code
transformations to generate test cases involving dynamic features. These
transformations preserve the program's semantics, ensuring that any discrepancy
between the transformed and original programs indicates the presence of a bug.
Through our approach, we have successfully identified twenty previously unknown
bugs in the PyTorch compiler and its underlying tensor compiler Triton.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20081" title="Abstract">arXiv:2310.20081</a> [<a href="/pdf/2310.20081" title="Download PDF">pdf</a>, <a href="/format/2310.20081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Summarization and Retrieval for Enhanced Personalization via  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Richardson%2C+C">Chris Richardson</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gillespie%2C+K">Kellen Gillespie</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+S">Sudipta Kar</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Arshdeep Singh</a>, 
<a href="/search/cs?searchtype=author&query=Raeesy%2C+Z">Zeynab Raeesy</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+O+Z">Omar Zia Khan</a>, 
<a href="/search/cs?searchtype=author&query=Sethy%2C+A">Abhinav Sethy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, International Workshop on Personalized Generative AI (@CIKM 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Personalization, the ability to tailor a system to individual users, is an
essential factor in user experience with natural language processing (NLP)
systems. With the emergence of Large Language Models (LLMs), a key question is
how to leverage these models to better personalize user experiences. To
personalize a language model's output, a straightforward approach is to
incorporate past user data into the language model prompt, but this approach
can result in lengthy inputs exceeding limitations on input length and
incurring latency and cost issues. Existing approaches tackle such challenges
by selectively extracting relevant user data (i.e. selective retrieval) to
construct a prompt for downstream tasks. However, retrieval-based methods are
limited by potential information loss, lack of more profound user
understanding, and cold-start challenges. To overcome these limitations, we
propose a novel summary-augmented approach by extending retrieval-augmented
personalization with task-aware user summaries generated by LLMs. The summaries
can be generated and stored offline, enabling real-world systems with runtime
constraints like voice assistants to leverage the power of LLMs. Experiments
show our method with 75% less of retrieved user data is on-par or outperforms
retrieval augmentation on most tasks in the LaMP personalization benchmark. We
demonstrate that offline summarization via LLMs and runtime retrieval enables
better performance for personalization on a range of tasks under practical
constraints.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20082" title="Abstract">arXiv:2310.20082</a> [<a href="/pdf/2310.20082" title="Download PDF">pdf</a>, <a href="/format/2310.20082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Subgraph GNNs by Learning Effective Selection Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bevilacqua%2C+B">Beatrice Bevilacqua</a>, 
<a href="/search/cs?searchtype=author&query=Eliasof%2C+M">Moshe Eliasof</a>, 
<a href="/search/cs?searchtype=author&query=Meirom%2C+E">Eli Meirom</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+B">Bruno Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Maron%2C+H">Haggai Maron</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Subgraph GNNs are provably expressive neural architectures that learn graph
representations from sets of subgraphs. Unfortunately, their applicability is
hampered by the computational complexity associated with performing message
passing on many subgraphs. In this paper, we consider the problem of learning
to select a small subset of the large set of possible subgraphs in a
data-driven fashion. We first motivate the problem by proving that there are
families of WL-indistinguishable graphs for which there exist efficient
subgraph selection policies: small subsets of subgraphs that can already
identify all the graphs within the family. We then propose a new approach,
called Policy-Learn, that learns how to select subgraphs in an iterative
manner. We prove that, unlike popular random policies and prior work addressing
the same problem, our architecture is able to learn the efficient policies
mentioned above. Our experimental results demonstrate that Policy-Learn
outperforms existing baselines across a wide range of datasets.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20083" title="Abstract">arXiv:2310.20083</a> [<a href="/pdf/2310.20083" title="Download PDF">pdf</a>, <a href="/format/2310.20083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Facial asymmetry: A Computer Vision based behaviometric index for  assessment during a face-to-face interview
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keshari%2C+S">Shuvam Keshari</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+T">Tanusree Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Mullick%2C+R">Raju Mullick</a>, 
<a href="/search/cs?searchtype=author&query=Rathor%2C+A">Ashish Rathor</a>, 
<a href="/search/cs?searchtype=author&query=Patnaik%2C+P">Priyadarshi Patnaik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Choosing the right person for the right job makes the personnel interview
process a cognitively demanding task. Psychometric tests, followed by an
interview, have often been used to aid the process although such mechanisms
have their limitations. While psychometric tests suffer from faking or social
desirability of responses, the interview process depends on the way the
responses are analyzed by the interviewers. We propose the use of behaviometry
as an assistive tool to facilitate an objective assessment of the interviewee
without increasing the cognitive load of the interviewer. Behaviometry is a
relatively little explored field of study in the selection process, that
utilizes inimitable behavioral characteristics like facial expressions,
vocalization patterns, pupillary reactions, proximal behavior, body language,
etc. The method analyzes thin slices of behavior and provides unbiased
information about the interviewee. The current study proposes the methodology
behind this tool to capture facial expressions, in terms of facial asymmetry
and micro-expressions. Hemi-facial composites using a structural similarity
index was used to develop a progressive time graph of facial asymmetry, as a
test case. A frame-by-frame analysis was performed on three YouTube video
samples, where Structural similarity index (SSID) scores of 75% and more showed
behavioral congruence. The research utilizes open-source computer vision
algorithms and libraries (python-opencv and dlib) to formulate the procedure
for analysis of the facial asymmetry.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20089" title="Abstract">arXiv:2310.20089</a> [<a href="/pdf/2310.20089" title="Download PDF">pdf</a>, <a href="/format/2310.20089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Keyword-optimized Template Insertion for Clinical Information Extraction  via Prompt-based Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alleva%2C+E">Eugenia Alleva</a>, 
<a href="/search/cs?searchtype=author&query=Landi%2C+I">Isotta Landi</a>, 
<a href="/search/cs?searchtype=author&query=Shaw%2C+L+J">Leslee J Shaw</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6ttinger%2C+E">Erwin B&#xf6;ttinger</a>, 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+T+J">Thomas J Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Ensari%2C+I">Ipek Ensari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Clinical note classification is a common clinical NLP task. However,
annotated data-sets are scarse. Prompt-based learning has recently emerged as
an effective method to adapt pre-trained models for text classification using
only few training examples. A critical component of prompt design is the
definition of the template (i.e. prompt text). The effect of template position,
however, has been insufficiently investigated. This seems particularly
important in the clinical setting, where task-relevant information is usually
sparse in clinical notes. In this study we develop a keyword-optimized template
insertion method (KOTI) and show how optimizing position can improve
performance on several clinical tasks in a zero-shot and few-shot training
setting.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20091" title="Abstract">arXiv:2310.20091</a> [<a href="/pdf/2310.20091" title="Download PDF">pdf</a>, <a href="/format/2310.20091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Density-based User Representation through Gaussian Process Regression  for Multi-interest Personalized Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haolun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mesh%2C+O">Ofer Mesh</a>, 
<a href="/search/cs?searchtype=author&query=Zogh%2C+M">Masrour Zogh</a>, 
<a href="/search/cs?searchtype=author&query=Diaz%2C+F">Fernando Diaz</a>, Xue (Steve)Liu, 
<a href="/search/cs?searchtype=author&query=Boutilier%2C+C">Craig Boutilier</a>, 
<a href="/search/cs?searchtype=author&query=Karimzadehgan%2C+M">Maryam Karimzadehgan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Accurate modeling of the diverse and dynamic interests of users remains a
significant challenge in the design of personalized recommender systems.
Existing user modeling methods, like single-point and multi-point
representations, have limitations w.r.t. accuracy, diversity, computational
cost, and adaptability. To overcome these deficiencies, we introduce
density-based user representations (DURs), a novel model that leverages
Gaussian process regression for effective multi-interest recommendation and
retrieval. Our approach, GPR4DUR, exploits DURs to capture user interest
variability without manual tuning, incorporates uncertainty-awareness, and
scales well to large numbers of users. Experiments using real-world offline
datasets confirm the adaptability and efficiency of GPR4DUR, while online
experiments with simulated users demonstrate its ability to address the
exploration-exploitation trade-off by effectively utilizing model uncertainty.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20092" title="Abstract">arXiv:2310.20092</a> [<a href="/pdf/2310.20092" title="Download PDF">pdf</a>, <a href="/format/2310.20092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond U: Making Diffusion Models Faster &amp; Lighter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Calvo-Ordonez%2C+S">Sergio Calvo-Ordonez</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiahao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lipei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Schonlieb%2C+C">Carola-Bibiane Schonlieb</a>, 
<a href="/search/cs?searchtype=author&query=Aviles-Rivero%2C+A+I">Angelica I Aviles-Rivero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, Neural Information Processing Systems (NeurIPS) 2023 Workshop on Diffusion Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diffusion models are a family of generative models that yield record-breaking
performance in tasks such as image synthesis, video generation, and molecule
design. Despite their capabilities, their efficiency, especially in the reverse
denoising process, remains a challenge due to slow convergence rates and high
computational costs. In this work, we introduce an approach that leverages
continuous dynamical systems to design a novel denoising network for diffusion
models that is more parameter-efficient, exhibits faster convergence, and
demonstrates increased noise robustness. Experimenting with denoising
probabilistic diffusion models, our framework operates with approximately a
quarter of the parameters and 30% of the Floating Point Operations (FLOPs)
compared to standard U-Nets in Denoising Diffusion Probabilistic Models
(DDPMs). Furthermore, our model is up to 70% faster in inference than the
baseline models when measured in equal conditions while converging to better
quality solutions.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20093" title="Abstract">arXiv:2310.20093</a> [<a href="/pdf/2310.20093" title="Download PDF">pdf</a>, <a href="/format/2310.20093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Neural Language Models as Cognitive Models of Language  Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+H+J+V">H&#xe9;ctor Javier V&#xe1;zquez Mart&#xed;nez</a>, 
<a href="/search/cs?searchtype=author&query=Heuser%2C+A+L">Annika Lea Heuser</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Charles Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kodner%2C+J">Jordan Kodner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the GenBench 2023 workshop proceedings, the first workshop on (benchmarking) generalisation in NLP. GenBench 2023 will be held at EMNLP 2023 on December 6, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The success of neural language models (LMs) on many technological tasks has
brought about their potential relevance as scientific theories of language
despite some clear differences between LM training and child language
acquisition. In this paper we argue that some of the most prominent benchmarks
for evaluating the syntactic capacities of LMs may not be sufficiently
rigorous. In particular, we show that the template-based benchmarks lack the
structural diversity commonly found in the theoretical and psychological
studies of language. When trained on small-scale data modeling child language
acquisition, the LMs can be readily matched by simple baseline models. We
advocate for the use of the readily available, carefully curated datasets that
have been evaluated for gradient acceptability by large pools of native
speakers and are designed to probe the structural basis of grammar
specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences
in a way inconsistent with human language users. We conclude with suggestions
for better connecting LMs with the empirical study of child language
acquisition.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20095" title="Abstract">arXiv:2310.20095</a> [<a href="/pdf/2310.20095" title="Download PDF">pdf</a>, <a href="/format/2310.20095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $p$-Poisson surface reconstruction in curl-free flow from point clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Yesom Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T">Taekyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hahn%2C+J">Jooyoung Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Myungjoo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, accepted for Advances in Neural Information Processing Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computational Geometry (cs.CG); Mathematical Physics (math-ph)

</div>
<p class="mathjax">The aim of this paper is the reconstruction of a smooth surface from an
unorganized point cloud sampled by a closed surface, with the preservation of
geometric shapes, without any further information other than the point cloud.
Implicit neural representations (INRs) have recently emerged as a promising
approach to surface reconstruction. However, the reconstruction quality of
existing methods relies on ground truth implicit function values or surface
normal vectors. In this paper, we show that proper supervision of partial
differential equations and fundamental properties of differential vector fields
are sufficient to robustly reconstruct high-quality surfaces. We cast the
$p$-Poisson equation to learn a signed distance function (SDF) and the
reconstructed surface is implicitly represented by the zero-level set of the
SDF. For efficient training, we develop a variable splitting structure by
introducing a gradient of the SDF as an auxiliary variable and impose the
$p$-Poisson equation directly on the auxiliary variable as a hard constraint.
Based on the curl-free property of the gradient field, we impose a curl-free
constraint on the auxiliary variable, which leads to a more faithful
reconstruction. Experiments on standard benchmark datasets show that the
proposed INR provides a superior and robust reconstruction. The code is
available at \url{https://github.com/Yebbi/PINC}.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20096" title="Abstract">arXiv:2310.20096</a> [<a href="/pdf/2310.20096" title="Download PDF">pdf</a>, <a href="/format/2310.20096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Market Design through Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravindranath%2C+S+S">Sai Srivatsa Ravindranath</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yanchen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Parkes%2C+D+C">David C. Parkes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The $\textit{data market design}$ problem is a problem in economic theory to
find a set of signaling schemes (statistical experiments) to maximize expected
revenue to the information seller, where each experiment reveals some of the
information known to a seller and has a corresponding price [Bergemann et al.,
2018]. Each buyer has their own decision to make in a world environment, and
their subjective expected value for the information associated with a
particular experiment comes from the improvement in this decision and depends
on their prior and value for different outcomes. In a setting with multiple
buyers, a buyer's expected value for an experiment may also depend on the
information sold to others [Bonatti et al., 2022]. We introduce the application
of deep learning for the design of revenue-optimal data markets, looking to
expand the frontiers of what can be understood and achieved. Relative to
earlier work on deep learning for auction design [D\"utting et al., 2023], we
must learn signaling schemes rather than allocation rules and handle
$\textit{obedience constraints}$ $-$ these arising from modeling the downstream
actions of buyers $-$ in addition to incentive constraints on bids. Our
experiments demonstrate that this new deep learning framework can almost
precisely replicate all known solutions from theory, expand to more complex
settings, and be used to establish the optimality of new designs for data
markets and make conjectures in regard to the structure of optimal designs.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20098" title="Abstract">arXiv:2310.20098</a> [<a href="/pdf/2310.20098" title="Download PDF">pdf</a>, <a href="/format/2310.20098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Learning for Smoothed Online Convex Optimization with Feedback  Delay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wierman%2C+A">Adam Wierman</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaolei Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">We study a challenging form of Smoothed Online Convex Optimization, a.k.a.
SOCO, including multi-step nonlinear switching costs and feedback delay. We
propose a novel machine learning (ML) augmented online algorithm,
Robustness-Constrained Learning (RCL), which combines untrusted ML predictions
with a trusted expert online algorithm via constrained projection to robustify
the ML prediction. Specifically,we prove that RCL is able to
guarantee$(1+\lambda)$-competitiveness against any given expert for
any$\lambda&gt;0$, while also explicitly training the ML model in a
robustification-aware manner to improve the average-case performance.
Importantly,RCL is the first ML-augmented algorithm with a provable robustness
guarantee in the case of multi-step switching cost and feedback delay.We
demonstrate the improvement of RCL in both robustness and average performance
using battery management for electrifying transportationas a case study.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20104" title="Abstract">arXiv:2310.20104</a> [<a href="/pdf/2310.20104" title="Download PDF">pdf</a>, <a href="/format/2310.20104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits  and Characteristics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karnalim%2C+O">Oscar Karnalim</a>, 
<a href="/search/cs?searchtype=author&query=Toba%2C+H">Hapnes Toba</a>, 
<a href="/search/cs?searchtype=author&query=Johan%2C+M+C">Meliana Christianti Johan</a>, 
<a href="/search/cs?searchtype=author&query=Handoyo%2C+E+D">Erico Darmawan Handoyo</a>, 
<a href="/search/cs?searchtype=author&query=Setiawan%2C+Y+D">Yehezkiel David Setiawan</a>, 
<a href="/search/cs?searchtype=author&query=Luwia%2C+J+A">Josephine Alvina Luwia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE TALE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In programming education, plagiarism and misuse of artificial intelligence
(AI) assistance are emerging issues. However, not many relevant studies are
focused on web programming. We plan to develop automated tools to help
instructors identify both misconducts. To fully understand the issues, we
conducted a controlled experiment to observe the unfair benefits and the
characteristics. We compared student performance in completing web programming
tasks independently, with a submission to plagiarize, and with the help of AI
assistance (ChatGPT). Our study shows that students who are involved in such
misconducts get comparable test marks with less completion time. Plagiarized
submissions are similar to the independent ones except in trivial aspects such
as color and identifier names. AI-assisted submissions are more complex, making
them less readable. Students believe AI assistance could be useful given proper
acknowledgment of the use, although they are not convinced with readability and
correctness of the solutions.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20105" title="Abstract">arXiv:2310.20105</a> [<a href="/pdf/2310.20105" title="Download PDF">pdf</a>, <a href="/format/2310.20105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Classification of Student Help Requests in Programming Courses  Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Savelka%2C+J">Jaromir Savelka</a>, 
<a href="/search/cs?searchtype=author&query=Denny%2C+P">Paul Denny</a>, 
<a href="/search/cs?searchtype=author&query=Liffiton%2C+M">Mark Liffiton</a>, 
<a href="/search/cs?searchtype=author&query=Sheese%2C+B">Brad Sheese</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The accurate classification of student help requests with respect to the type
of help being sought can enable the tailoring of effective responses.
Automatically classifying such requests is non-trivial, but large language
models (LLMs) appear to offer an accessible, cost-effective solution. This
study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying
help requests from students in an introductory programming class. In zero-shot
trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories,
while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests
related to debugging. Fine-tuning the GPT-3.5 model improved its performance to
such an extent that it approximated the accuracy and consistency across
categories observed between two human raters. Overall, this study demonstrates
the feasibility of using LLMs to enhance educational systems through the
automated classification of student needs.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20109" title="Abstract">arXiv:2310.20109</a> [<a href="/pdf/2310.20109" title="Download PDF">pdf</a>, <a href="/format/2310.20109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Objective Intrinsic Reward Learning for Conversational Recommender  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhendong Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Conversational Recommender Systems (CRS) actively elicit user preferences to
generate adaptive recommendations. Mainstream reinforcement learning-based CRS
solutions heavily rely on handcrafted reward functions, which may not be
aligned with user intent in CRS tasks. Therefore, the design of task-specific
rewards is critical to facilitate CRS policy learning, which remains largely
under-explored in the literature. In this work, we propose a novel approach to
address this challenge by learning intrinsic rewards from interactions with
users. Specifically, we formulate intrinsic reward learning as a
multi-objective bi-level optimization problem. The inner level optimizes the
CRS policy augmented by the learned intrinsic rewards, while the outer level
drives the intrinsic rewards to optimize two CRS-specific objectives:
maximizing the success rate and minimizing the number of turns to reach a
successful recommendation in conversations. To evaluate the effectiveness of
our approach, we conduct extensive experiments on three public CRS benchmarks.
The results show that our algorithm significantly improves CRS performance by
exploiting informative learned intrinsic rewards.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20111" title="Abstract">arXiv:2310.20111</a> [<a href="/pdf/2310.20111" title="Download PDF">pdf</a>, <a href="/format/2310.20111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Large Language Models Better Data Creators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dong-Ho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>, 
<a href="/search/cs?searchtype=author&query=Sewak%2C+M">Mohit Sewak</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+R+W">Ryen W. White</a>, 
<a href="/search/cs?searchtype=author&query=Jauhar%2C+S+K">Sujay Kumar Jauhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference. 12 pages, 5 figures, 6 tables. Code is available at <a href="https://github.com/microsoft/llm-data-creation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Although large language models (LLMs) have advanced the state-of-the-art in
NLP significantly, deploying them for downstream applications is still
challenging due to cost, responsiveness, control, or concerns around privacy
and security. As such, trainable models are still the preferred option in some
cases. However, these models still require human-labeled data for optimal
performance, which is expensive and time-consuming to obtain. In order to
address this issue, several techniques to reduce human effort involve labeling
or generating data using LLMs. Although these methods are effective for certain
applications, in practice they encounter difficulties in real-world scenarios.
Labeling data requires careful data selection, while generating data
necessitates task-specific prompt engineering. In this paper, we propose a
unified data creation pipeline that requires only a single formatting example,
and which is applicable to a broad range of tasks, including traditionally
problematic ones with semantically devoid label spaces. In our experiments we
demonstrate that instruction-following LLMs are highly cost-effective data
creators, and that models trained with these data exhibit performance better
than those trained with human-labeled data (by up to 17.5%) on
out-of-distribution evaluation, while maintaining comparable performance on
in-distribution tasks. These results have important implications for the
robustness of NLP systems deployed in the real-world.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20112" title="Abstract">arXiv:2310.20112</a> [<a href="/pdf/2310.20112" title="Download PDF">pdf</a>, <a href="/ps/2310.20112" title="Download PostScript">ps</a>, <a href="/format/2310.20112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Well-Posedness of the Bochner Integral Form of Operator-Valued Riccati  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cheung%2C+J">James Cheung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Initial Release
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">In this short paper, we prove that the Bochner integral form of the
operator-valued Riccati equation has a unique solution if and only if its mild
form has a unique solution. This implies that the mild and Bochner integral
forms of this equation are equivalent. The result is obtained through an
operator representation argument.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20117" title="Abstract">arXiv:2310.20117</a> [<a href="/pdf/2310.20117" title="Download PDF">pdf</a>, <a href="/format/2310.20117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refined Equivalent Pinhole Model for Large-scale 3D Reconstruction from  Spaceborne CCD Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Danyang%2C+H">Hong Danyang</a>, 
<a href="/search/cs?searchtype=author&query=Anzhu%2C+Y">Yu Anzhu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Ji Song</a>, 
<a href="/search/cs?searchtype=author&query=Xuefeng%2C+C">Cao Xuefeng</a>, 
<a href="/search/cs?searchtype=author&query=Yujun%2C+Q">Quan Yujun</a>, 
<a href="/search/cs?searchtype=author&query=Wenyue%2C+G">Guo Wenyue</a>, 
<a href="/search/cs?searchtype=author&query=Chunping%2C+Q">Qiu Chunping</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In this study, we present a large-scale earth surface reconstruction pipeline
for linear-array charge-coupled device (CCD) satellite imagery. While
mainstream satellite image-based reconstruction approaches perform
exceptionally well, the rational functional model (RFM) is subject to several
limitations. For example, the RFM has no rigorous physical interpretation and
differs significantly from the pinhole imaging model; hence, it cannot be
directly applied to learning-based 3D reconstruction networks and to more novel
reconstruction pipelines in computer vision. Hence, in this study, we introduce
a method in which the RFM is equivalent to the pinhole camera model (PCM),
meaning that the internal and external parameters of the pinhole camera are
used instead of the rational polynomial coefficient parameters. We then derive
an error formula for this equivalent pinhole model for the first time,
demonstrating the influence of the image size on the accuracy of the
reconstruction. In addition, we propose a polynomial image refinement model
that minimizes equivalent errors via the least squares method. The experiments
were conducted using four image datasets: WHU-TLC, DFC2019, ISPRS-ZY3, and GF7.
The results demonstrated that the reconstruction accuracy was proportional to
the image size. Our polynomial image refinement model significantly enhanced
the accuracy and completeness of the reconstruction, and achieved more
significant improvements for larger-scale images.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20120" title="Abstract">arXiv:2310.20120</a> [<a href="/pdf/2310.20120" title="Download PDF">pdf</a>, <a href="/format/2310.20120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Team I2R-VI-FF Technical Report on EPIC-KITCHENS VISOR Hand Object  Segmentation Challenge 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Ying Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qianli Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this report, we present our approach to the EPIC-KITCHENS VISOR Hand
Object Segmentation Challenge, which focuses on the estimation of the relation
between the hands and the objects given a single frame as input. The
EPIC-KITCHENS VISOR dataset provides pixel-wise annotations and serves as a
benchmark for hand and active object segmentation in egocentric video. Our
approach combines the baseline method, i.e., Point-based Rendering (PointRend)
and the Segment Anything Model (SAM), aiming to enhance the accuracy of hand
and object segmentation outcomes, while also minimizing instances of missed
detection. We leverage accurate hand segmentation maps obtained from the
baseline method to extract more precise hand and in-contact object segments. We
utilize the class-agnostic segmentation provided by SAM and apply specific
hand-crafted constraints to enhance the results. In cases where the baseline
model misses the detection of hands or objects, we re-train an object detector
on the training set to enhance the detection accuracy. The detected hand and
in-contact object bounding boxes are then used as prompts to extract their
respective segments from the output of SAM. By effectively combining the
strengths of existing methods and applying our refinements, our submission
achieved the 1st place in terms of evaluation criteria in the VISOR HOS
Challenge.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20121" title="Abstract">arXiv:2310.20121</a> [<a href="/pdf/2310.20121" title="Download PDF">pdf</a>, <a href="/format/2310.20121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ling-CL: Understanding NLP Models through Linguistic Curricula
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elgaar%2C+M">Mohamed Elgaar</a>, 
<a href="/search/cs?searchtype=author&query=Amiri%2C+H">Hadi Amiri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We employ a characterization of linguistic complexity from psycholinguistic
and language acquisition research to develop data-driven curricula to
understand the underlying linguistic knowledge that models learn to address NLP
tasks. The novelty of our approach is in the development of linguistic
curricula derived from data, existing knowledge about linguistic complexity,
and model behavior during training. By analyzing several benchmark NLP
datasets, our curriculum learning approaches identify sets of linguistic
metrics (indices) that inform the challenges and reasoning required to address
each task. Our work will inform future research in all NLP areas, allowing
linguistic complexity to be considered early in the research and development
process. In addition, our work prompts an examination of gold standards and
fair evaluation in NLP.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20127" title="Abstract">arXiv:2310.20127</a> [<a href="/pdf/2310.20127" title="Download PDF">pdf</a>, <a href="/format/2310.20127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Prompt Tuning with Learned Prompting Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Ming Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Prompt tuning prepends a soft prompt to the input embeddings or hidden states
and only optimizes the prompt to adapt pretrained models (PTMs) to downstream
tasks. The previous work manually selects prompt layers which are far from
optimal and failed to exploit the potential of prompt tuning. In this work, we
propose a novel framework, \underline{S}elective \underline{P}rompt
\underline{T}uning (SPT), that learns to select the proper prompt layers by
inserting a prompt controlled by a learnable probabilistic gate at each
intermediate layer. We further propose a novel bi-level optimization framework,
SPT-DARTS, that can better optimize the learnable gates and improve the final
prompt tuning performances of the learned prompt layer settings. We conduct
extensive experiments with ten benchmark datasets under the full-data and
few-shot scenarios. The results demonstrate that our SPT framework can perform
better than the previous state-of-the-art PETuning baselines with comparable or
fewer tunable parameters.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20132" title="Abstract">arXiv:2310.20132</a> [<a href="/pdf/2310.20132" title="Download PDF">pdf</a>, <a href="/ps/2310.20132" title="Download PostScript">ps</a>, <a href="/format/2310.20132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear codes with few weights from non-weakly regular plateaued  functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yadi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+F">Fang-Wei Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 34 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Linear codes with few weights have significant applications in secret sharing
schemes, authentication codes, association schemes, and strongly regular
graphs. There are a number of methods to construct linear codes, one of which
is based on functions. Furthermore, two generic constructions of linear codes
from functions called the first and the second generic constructions, have
aroused the research interest of scholars. Recently, in \cite{Nian}, Li and
Mesnager proposed two open problems: Based on the first and the second generic
constructions, respectively, construct linear codes from non-weakly regular
plateaued functions and determine their weight distributions.
<br />Motivated by these two open problems, in this paper, firstly, based on the
first generic construction, we construct some three-weight and at most
five-weight linear codes from non-weakly regular plateaued functions and
determine the weight distributions of the constructed codes. Next, based on the
second generic construction, we construct some three-weight and at most
five-weight linear codes from non-weakly regular plateaued functions belonging
to $\mathcal{NWRF}$ (defined in this paper) and determine the weight
distributions of the constructed codes. We also give the punctured codes of
these codes obtained based on the second generic construction and determine
their weight distributions. Meanwhile, we obtain some optimal and almost
optimal linear codes. Besides, by the Ashikhmin-Barg condition, we have that
the constructed codes are minimal for almost all cases and obtain some secret
sharing schemes with nice access structures based on their dual codes.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20138" title="Abstract">arXiv:2310.20138</a> [<a href="/pdf/2310.20138" title="Download PDF">pdf</a>, <a href="/format/2310.20138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEPN: Detecting and Editing Privacy Neurons in Pretrained Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junzhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minghui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Weilong Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shuangzhi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+C">Chao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Deyi Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models pretrained on a huge amount of data capture rich
knowledge and information in the training data. The ability of data
memorization and regurgitation in pretrained language models, revealed in
previous studies, brings the risk of data leakage. In order to effectively
reduce these risks, we propose a framework DEPN to Detect and Edit Privacy
Neurons in pretrained language models, partially inspired by knowledge neurons
and model editing. In DEPN, we introduce a novel method, termed as privacy
neuron detector, to locate neurons associated with private information, and
then edit these detected privacy neurons by setting their activations to zero.
Furthermore, we propose a privacy neuron aggregator dememorize private
information in a batch processing manner. Experimental results show that our
method can significantly and efficiently reduce the exposure of private data
leakage without deteriorating the performance of the model. Additionally, we
empirically demonstrate the relationship between model memorization and privacy
neurons, from multiple perspectives, including model size, training time,
prompts, privacy neuron distribution, illustrating the robustness of our
approach.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20141" title="Abstract">arXiv:2310.20141</a> [<a href="/pdf/2310.20141" title="Download PDF">pdf</a>, <a href="/format/2310.20141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Difference Predictive Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chongyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Eysenbach%2C+B">Benjamin Eysenbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website (<a href="https://chongyi-zheng.github.io/td_infonce">this https URL</a>) and code (<a href="https://github.com/chongyi-zheng/td_infonce">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Predicting and reasoning about the future lie at the heart of many
time-series questions. For example, goal-conditioned reinforcement learning can
be viewed as learning representations to predict which states are likely to be
visited in the future. While prior methods have used contrastive predictive
coding to model time series data, learning representations that encode
long-term dependencies usually requires large amounts of data. In this paper,
we introduce a temporal difference version of contrastive predictive coding
that stitches together pieces of different time series data to decrease the
amount of data required to learn predictions of future events. We apply this
representation learning method to derive an off-policy algorithm for
goal-conditioned RL. Experiments demonstrate that, compared with prior RL
methods, ours achieves $2 \times$ median improvement in success rates and can
better cope with stochastic environments. In tabular settings, we show that our
method is about $20 \times$ more sample efficient than the successor
representation and $1500 \times$ more sample efficient than the standard (Monte
Carlo) version of contrastive predictive coding.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20144" title="Abstract">arXiv:2310.20144</a> [<a href="/pdf/2310.20144" title="Download PDF">pdf</a>, <a href="/format/2310.20144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EELBERT: Tiny Models through Dynamic Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohn%2C+G">Gabrielle Cohn</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+R">Rishika Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Deepanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Patwardhan%2C+S">Siddharth Patwardhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, Industry Track 9 pages, 2 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce EELBERT, an approach for compression of transformer-based models
(e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is
achieved by replacing the input embedding layer of the model with dynamic, i.e.
on-the-fly, embedding computations. Since the input embedding layer accounts
for a significant fraction of the model size, especially for the smaller BERT
variants, replacing this layer with an embedding computation function helps us
reduce the model size significantly. Empirical evaluation on the GLUE benchmark
shows that our BERT variants (EELBERT) suffer minimal regression compared to
the traditional BERT models. Through this approach, we are able to develop our
smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully
trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20145" title="Abstract">arXiv:2310.20145</a> [<a href="/pdf/2310.20145" title="Download PDF">pdf</a>, <a href="/format/2310.20145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+J">Junlong Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+W">Wenlong Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhitang Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Bayesian Optimization (BO) is a sample-efficient optimization algorithm
widely employed across various applications. In some challenging BO tasks,
input uncertainty arises due to the inevitable randomness in the optimization
process, such as machining errors, execution noise, or contextual variability.
This uncertainty deviates the input from the intended value before evaluation,
resulting in significant performance fluctuations in the final result. In this
paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO,
which can effectively identify a robust optimum that performs consistently well
under arbitrary input uncertainty. Our method directly models the uncertain
inputs of arbitrary distributions by empowering the Gaussian Process with the
Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference
via Nystrom approximation. Rigorous theoretical regret bound is established
under MMD estimation error and extensive experiments on synthetic functions and
real problems demonstrate that our approach can handle various input
uncertainties and achieve state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20148" title="Abstract">arXiv:2310.20148</a> [<a href="/pdf/2310.20148" title="Download PDF">pdf</a>, <a href="/format/2310.20148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision-Making for Autonomous Vehicles with Interaction-Aware  Behavioral Prediction and Social-Attention Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kaiwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+H+E">H. Eric Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Girard%2C+A">Anouck Girard</a>, 
<a href="/search/cs?searchtype=author&query=Kolmanovsky%2C+I">Ilya Kolmanovsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">Autonomous vehicles need to accomplish their tasks while interacting with
human drivers in traffic. It is thus crucial to equip autonomous vehicles with
artificial reasoning to better comprehend the intentions of the surrounding
traffic, thereby facilitating the accomplishments of the tasks. In this work,
we propose a behavioral model that encodes drivers' interacting intentions into
latent social-psychological parameters. Leveraging a Bayesian filter, we
develop a receding-horizon optimization-based controller for autonomous vehicle
decision-making which accounts for the uncertainties in the interacting
drivers' intentions. For online deployment, we design a neural network
architecture based on the attention mechanism which imitates the behavioral
model with online estimated parameter priors. We also propose a decision tree
search algorithm to solve the decision-making problem online. The proposed
behavioral model is then evaluated in terms of its capabilities for real-world
trajectory prediction. We further conduct extensive evaluations of the proposed
decision-making module, in forced highway merging scenarios, using both
simulated environments and real-world traffic datasets. The results demonstrate
that our algorithms can complete the forced merging tasks in various traffic
conditions while ensuring driving safety.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20150" title="Abstract">arXiv:2310.20150</a> [<a href="/pdf/2310.20150" title="Download PDF">pdf</a>, <a href="/format/2310.20150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlearn What You Want to Forget: Efficient Unlearning for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have achieved significant progress from
pre-training on and memorizing a wide range of textual data, however, this
process might suffer from privacy issues and violations of data protection
regulations. As a result, the ability to easily remove data related to
individual users from such models while not deteriorating their predictive
quality after the removal becomes increasingly important. To address these
issues, in this work, we propose an efficient unlearning framework that could
efficiently update LLMs without having to retrain the whole model after data
removals, by introducing lightweight unlearning layers learned with a selective
teacher-student objective into the transformers. In addition, we introduce a
fusion mechanism to effectively combine different unlearning layers that learns
to forget different sets of data to handle a sequence of forgetting operations.
Experiments on classification and generation tasks demonstrate the
effectiveness of our proposed methods compared to the state-of-the-art
baselines.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20151" title="Abstract">arXiv:2310.20151</a> [<a href="/pdf/2310.20151" title="Download PDF">pdf</a>, <a href="/format/2310.20151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Consensus Seeking via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaben Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+W">Wenkang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lufeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shiyu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">Multi-agent systems driven by large language models (LLMs) have shown
promising abilities for solving complex tasks in a collaborative manner. This
work considers a fundamental problem in multi-agent collaboration: consensus
seeking. When multiple agents work together, we are interested in how they can
reach a consensus through inter-agent negotiation. To that end, this work
studies a consensus-seeking task where the state of each agent is a numerical
value and they negotiate with each other to reach a consensus value. It is
revealed that when not explicitly directed on which strategy should be adopted,
the LLM-driven agents primarily use the average strategy for consensus seeking
although they may occasionally use some other strategies. Moreover, this work
analyzes the impact of the agent number, agent personality, and network
topology on the negotiation process. The findings reported in this work can
potentially lay the foundations for understanding the behaviors of LLM-driven
multi-agent systems for solving more complex tasks. Furthermore, LLM-driven
consensus seeking is applied to a multi-robot aggregation task. This
application demonstrates the potential of LLM-driven agents to achieve
zero-shot autonomous planning for multi-robot collaboration tasks. Project
website: westlakeintelligentrobotics.github.io/ConsensusLLM/.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20153" title="Abstract">arXiv:2310.20153</a> [<a href="/pdf/2310.20153" title="Download PDF">pdf</a>, <a href="/format/2310.20153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Multi-fidelity Learning for Cost-effective Adaptation of  Language Model with Sparse Human Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuohang Li</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+K">Kamalika Das</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sricharan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, their suitability for domain-specific tasks, is limited
due to their immense scale at deployment, susceptibility to misinformation, and
more importantly, high data annotation costs. We propose a novel Interactive
Multi-Fidelity Learning (IMFL) framework for the cost-effective development of
small domain-specific LMs under limited annotation budgets. Our approach
formulates the domain-specific fine-tuning process as a multi-fidelity learning
problem, focusing on identifying the optimal acquisition strategy that balances
between low-fidelity automatic LLM annotations and high-fidelity human
annotations to maximize model performance. We further propose an
exploration-exploitation query strategy that enhances annotation diversity and
informativeness, incorporating two innovative designs: 1) prompt retrieval that
selects in-context examples from human-annotated samples to improve LLM
annotation, and 2) variable batch size that controls the order for choosing
each fidelity to facilitate knowledge distillation, ultimately enhancing
annotation quality. Extensive experiments on financial and medical tasks
demonstrate that IMFL achieves superior performance compared with single
fidelity annotations. Given a limited budget of human annotation, IMFL
significantly outperforms the human annotation baselines in all four tasks and
achieves very close performance as human annotations on two of the tasks. These
promising results suggest that the high human annotation costs in
domain-specific tasks can be significantly reduced by employing IMFL, which
utilizes fewer human annotations, supplemented with cheaper and faster LLM
(e.g., GPT-3.5) annotations to achieve comparable performance.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20158" title="Abstract">arXiv:2310.20158</a> [<a href="/pdf/2310.20158" title="Download PDF">pdf</a>, <a href="/format/2310.20158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+D">Daman Arora</a>, 
<a href="/search/cs?searchtype=author&query=Kini%2C+A">Anush Kini</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+R">Sayak Ray Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Natarajan%2C+N">Nagarajan Natarajan</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+G">Gaurav Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Amit Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Given a query and a document corpus, the information retrieval (IR) task is
to output a ranked list of relevant documents. Combining large language models
(LLMs) with embedding-based retrieval models, recent work shows promising
results on the zero-shot retrieval problem, i.e., no access to labeled data
from the target domain. Two such popular paradigms are generation-augmented
retrieval or GAR (generate additional context for the query and then retrieve),
and retrieval-augmented generation or RAG (retrieve relevant documents as
context and then generate answers). The success of these paradigms hinges on
(i) high-recall retrieval models, which are difficult to obtain in the
zero-shot setting, and (ii) high-precision (re-)ranking models which typically
need a good initialization. In this work, we propose a novel GAR-meets-RAG
recurrence formulation that overcomes the challenges of existing paradigms. Our
method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in
the zero-shot setting. A key design principle is that the rewrite-retrieval
stages improve the recall of the system and a final re-ranking stage improves
the precision. We conduct extensive experiments on zero-shot passage retrieval
benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in
the BEIR benchmark, outperforming previous best results in Recall@100 and
nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the
previous best.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20159" title="Abstract">arXiv:2310.20159</a> [<a href="/pdf/2310.20159" title="Download PDF">pdf</a>, <a href="/format/2310.20159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Guided Visual Question Answering: Elevate Your Multimodal  Language Model Using Knowledge-Enriched Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosal%2C+D">Deepanway Ghosal</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+N">Navonil Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+R+K">Roy Ka-Wei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Visual question answering (VQA) is the task of answering questions about an
image. The task assumes an understanding of both the image and the question to
provide a natural language answer. VQA has gained popularity in recent years
due to its potential applications in a wide range of fields, including
robotics, education, and healthcare. In this paper, we focus on
knowledge-augmented VQA, where answering the question requires commonsense
knowledge, world knowledge, and reasoning about ideas and concepts not present
in the image. We propose a multimodal framework that uses language guidance
(LG) in the form of rationales, image captions, scene graphs, etc to answer
questions more accurately. We benchmark our method on the multi-choice
question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets
using CLIP and BLIP models. We show that the use of language guidance is a
simple but powerful and effective strategy for visual question answering. Our
language guidance improves the performance of CLIP by 7.6% and BLIP-2 by 4.8%
in the challenging A-OKVQA dataset. We also observe consistent improvement in
performance on the Science-QA, VSR, and IconQA datasets when using the proposed
language guidances. The implementation of LG-VQA is publicly available at
https:// github.com/declare-lab/LG-VQA.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20160" title="Abstract">arXiv:2310.20160</a> [<a href="/pdf/2310.20160" title="Download PDF">pdf</a>, <a href="/format/2310.20160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Relationship between Code Verifiability and Understandability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldman%2C+K">Kobi Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Kellogg%2C+M">Martin Kellogg</a>, 
<a href="/search/cs?searchtype=author&query=Chaparro%2C+O">Oscar Chaparro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear at Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Proponents of software verification have argued that simpler code is easier
to verify: that is, that verification tools issue fewer false positives and
require less human intervention when analyzing simpler code. We empirically
validate this assumption by comparing the number of warnings produced by four
state-of-the-art verification tools on 211 snippets of Java code with 20
metrics of code comprehensibility from human subjects in six prior studies. Our
experiments, based on a statistical (meta-)analysis, show that, in aggregate,
there is a small correlation (r = 0.23) between understandability and
verifiability. The results support the claim that easy-to-verify code is often
easier to understand than code that requires more effort to verify. Our work
has implications for the users and designers of verification tools and for
future attempts to automatically measure code comprehensibility: verification
tools may have ancillary benefits to understandability, and measuring
understandability may require reasoning about semantic, not just syntactic,
code properties.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20162" title="Abstract">arXiv:2310.20162</a> [<a href="/pdf/2310.20162" title="Download PDF">pdf</a>, <a href="/format/2310.20162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Robustness Transferable across Languages in Multilingual Neural  Machine Translation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Leiyu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Supryadi">Supryadi</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Deyi Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Robustness, the ability of models to maintain performance in the face of
perturbations, is critical for developing reliable NLP systems. Recent studies
have shown promising results in improving the robustness of models through
adversarial training and data augmentation. However, in machine translation,
most of these studies have focused on bilingual machine translation with a
single translation direction. In this paper, we investigate the transferability
of robustness across different languages in multilingual neural machine
translation. We propose a robustness transfer analysis protocol and conduct a
series of experiments. In particular, we use character-, word-, and multi-level
noises to attack the specific translation direction of the multilingual neural
machine translation model and evaluate the robustness of other translation
directions. Our findings demonstrate that the robustness gained in one
translation direction can indeed transfer to other translation directions.
Additionally, we empirically find scenarios where robustness to character-level
noise and word-level noise is more likely to transfer.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20163" title="Abstract">arXiv:2310.20163</a> [<a href="/pdf/2310.20163" title="Download PDF">pdf</a>, <a href="/format/2310.20163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Perturbative Solution to the Linear Influence/Network Autocorrelation  Model Under Network Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Butts%2C+C+T">Carter T. Butts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Known by many names and arising in many settings, the forced linear diffusion
model is central to the modeling of power and influence within social networks
(while also serving as the mechanistic justification for the widely used
spatial/network autocorrelation models). The standard equilibrium solution to
the diffusion model depends on strict timescale separation between network
dynamics and attribute dynamics, such that the diffusion network can be
considered fixed with respect to the diffusion process. Here, we consider a
relaxation of this assumption, in which the network changes only slowly
relative to the diffusion dynamics. In this case, we show that one can obtain a
perturbative solution to the diffusion model, which depends on knowledge of
past states in only a minimal way.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20168" title="Abstract">arXiv:2310.20168</a> [<a href="/pdf/2310.20168" title="Download PDF">pdf</a>, <a href="/format/2310.20168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Visualizing Droplet Distributions in Simulations of  Shallow Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Will%2C+J+C">Justus C. Will</a>, 
<a href="/search/cs?searchtype=author&query=Jenney%2C+A+M">Andrea M. Jenney</a>, 
<a href="/search/cs?searchtype=author&query=Lamb%2C+K+D">Kara D. Lamb</a>, 
<a href="/search/cs?searchtype=author&query=Pritchard%2C+M+S">Michael S. Pritchard</a>, 
<a href="/search/cs?searchtype=author&query=Kaul%2C+C">Colleen Kaul</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Po-Lun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pressel%2C+K">Kyle Pressel</a>, 
<a href="/search/cs?searchtype=author&query=Shpund%2C+J">Jacob Shpund</a>, 
<a href="/search/cs?searchtype=author&query=van+Lier-Walqui%2C+M">Marcus van Lier-Walqui</a>, 
<a href="/search/cs?searchtype=author&query=Mandt%2C+S">Stephan Mandt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures, accepted at NeurIPS 2023 (Machine Learning and the Physical Sciences Workshop)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Thorough analysis of local droplet-level interactions is crucial to better
understand the microphysical processes in clouds and their effect on the global
climate. High-accuracy simulations of relevant droplet size distributions from
Large Eddy Simulations (LES) of bin microphysics challenge current analysis
techniques due to their high dimensionality involving three spatial dimensions,
time, and a continuous range of droplet sizes. Utilizing the compact latent
representations from Variational Autoencoders (VAEs), we produce novel and
intuitive visualizations for the organization of droplet sizes and their
evolution over time beyond what is possible with clustering techniques. This
greatly improves interpretation and allows us to examine aerosol-cloud
interactions by contrasting simulations with different aerosol concentrations.
We find that the evolution of the droplet spectrum is similar across aerosol
levels but occurs at different paces. This similarity suggests that
precipitation initiation processes are alike despite variations in onset times.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20170" title="Abstract">arXiv:2310.20170</a> [<a href="/pdf/2310.20170" title="Download PDF">pdf</a>, <a href="/format/2310.20170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain  Question Answering over Knowledge Base and Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenting Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ye Liu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+T">Tong Niu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yavuz%2C+S">Semih Yavuz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have exhibited impressive generation
capabilities, but they suffer from hallucinations when solely relying on their
internal knowledge, especially when answering questions that require less
commonly known information. Retrieval-augmented LLMs have emerged as a
potential solution to ground LLMs in external knowledge. Nonetheless, recent
approaches have primarily emphasized retrieval from unstructured text corpora,
owing to its seamless integration into prompts. When using structured data such
as knowledge graphs, most methods simplify it into natural text, neglecting the
underlying structures. Moreover, a significant gap in the current landscape is
the absence of a realistic benchmark for evaluating the effectiveness of
grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and
text). To fill this gap, we have curated a comprehensive dataset that poses two
unique challenges: (1) Two-hop multi-source questions that require retrieving
information from both open-domain structured and unstructured knowledge
sources; retrieving information from structured knowledge sources is a critical
component in correctly answering the questions. (2) The generation of symbolic
queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another
layer of challenge. Our dataset is created using a combination of automatic
generation through predefined reasoning chains and human annotation. We also
introduce a novel approach that leverages multiple retrieval tools, including
text passage retrieval and symbolic language-assisted retrieval. Our model
outperforms previous approaches by a significant margin, demonstrating its
effectiveness in addressing the above-mentioned reasoning challenges.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20174" title="Abstract">arXiv:2310.20174</a> [<a href="/pdf/2310.20174" title="Download PDF">pdf</a>, <a href="/format/2310.20174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphTransformers for Geospatial Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+P">Pallavi Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Satyaki Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In this paper we introduce a novel framework for trajectory prediction of
geospatial sequences using GraphTransformers. When viewed across several
sequences, we observed that a graph structure automatically emerges between
different geospatial points that is often not taken into account for such
sequence modeling tasks. We show that by leveraging this graph structure
explicitly, geospatial trajectory prediction can be significantly improved. Our
GraphTransformer approach improves upon state-of-the-art Transformer based
baseline significantly on HURDAT, a dataset where we are interested in
predicting the trajectory of a hurricane on a 6 hourly basis.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20175" title="Abstract">arXiv:2310.20175</a> [<a href="/pdf/2310.20175" title="Download PDF">pdf</a>, <a href="/format/2310.20175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LFAA: Crafting Transferable Targeted Adversarial Examples with  Low-Frequency Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kunyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Juluan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep neural networks are susceptible to adversarial attacks, which pose a
significant threat to their security and reliability in real-world
applications. The most notable adversarial attacks are transfer-based attacks,
where an adversary crafts an adversarial example to fool one model, which can
also fool other models. While previous research has made progress in improving
the transferability of untargeted adversarial examples, the generation of
targeted adversarial examples that can transfer between models remains a
challenging task. In this work, we present a novel approach to generate
transferable targeted adversarial examples by exploiting the vulnerability of
deep neural networks to perturbations on high-frequency components of images.
We observe that replacing the high-frequency component of an image with that of
another image can mislead deep models, motivating us to craft perturbations
containing high-frequency information to achieve targeted attacks. To this end,
we propose a method called Low-Frequency Adversarial Attack (\name), which
trains a conditional generator to generate targeted adversarial perturbations
that are then added to the low-frequency component of the image. Extensive
experiments on ImageNet demonstrate that our proposed approach significantly
outperforms state-of-the-art methods, improving targeted attack success rates
by a margin from 3.2\% to 15.5\%.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20177" title="Abstract">arXiv:2310.20177</a> [<a href="/pdf/2310.20177" title="Download PDF">pdf</a>, <a href="/format/2310.20177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An extended Fourier pseudospectral method for the Gross-Pitaevskii  equation with low regularity potential
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bao%2C+W">Weizhu Bao</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+B">Bo Lin</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+Y">Ying Ma</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+C">Chushan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose and analyze an extended Fourier pseudospectral (eFP) method for
the spatial discretization of the Gross-Pitaevskii equation (GPE) with low
regularity potential by treating the potential in an extended window for its
discrete Fourier transform. The proposed eFP method maintains optimal
convergence rates with respect to the regularity of the exact solution even if
the potential is of low regularity and enjoys similar computational cost as the
standard Fourier pseudospectral method, and thus it is both efficient and
accurate. Furthermore, similar to the Fourier spectral/pseudospectral methods,
the eFP method can be easily coupled with different popular temporal
integrators including finite difference methods, time-splitting methods and
exponential-type integrators. Numerical results are presented to validate our
optimal error estimates and to demonstrate that they are sharp as well as to
show its efficiency in practical computations.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20178" title="Abstract">arXiv:2310.20178</a> [<a href="/pdf/2310.20178" title="Download PDF">pdf</a>, <a href="/format/2310.20178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Discover Skills through Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunseung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byungkun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hojoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+D">Dongyoon Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sejik Park</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+K">Kyushik Min</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the field of unsupervised skill discovery (USD), a major challenge is
limited exploration, primarily due to substantial penalties when skills deviate
from their initial trajectories. To enhance exploration, recent methodologies
employ auxiliary rewards to maximize the epistemic uncertainty or entropy of
states. However, we have identified that the effectiveness of these rewards
declines as the environmental complexity rises. Therefore, we present a novel
USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects
the guide skill that possesses the highest potential to reach unexplored
states, (2) guides other skills to follow guide skill, then (3) the guided
skills are dispersed to maximize their discriminability in unexplored states.
Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD
baselines in challenging environments, including two navigation benchmarks and
a continuous control benchmark. Qualitative visualizations and code of
DISCO-DANCE are available at https://mynsng.github.io/discodance.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20179" title="Abstract">arXiv:2310.20179</a> [<a href="/pdf/2310.20179" title="Download PDF">pdf</a>, <a href="/ps/2310.20179" title="Download PostScript">ps</a>, <a href="/format/2310.20179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generalization of the Tang-Ding Binary Cyclic Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhonghua Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Ling Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shixin Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Cyclic codes are an interesting family of linear codes since they have
efficient decoding algorithms and contain optimal codes as subfamilies.
Constructing infinite families of cyclic codes with good parameters is
important in both theory and practice. Recently, Tang and Ding [IEEE Trans.
Inf. Theory, vol. 68, no. 12, pp. 7842--7849, 2022] proposed an infinite family
of binary cyclic codes with good parameters. Shi et al. [<a href="/abs/2309.12003">arXiv:2309.12003v1</a>,
2023] developed the binary Tang-Ding codes to the $4$-ary case. Inspired by
these two works, we study $2^s$-ary Tang-Ding codes, where $s\geq 2$. Good
lower bounds on the minimum distance of the $2^s$-ary Tang-Ding codes are
presented. As a by-product, an infinite family of $2^s$-ary duadic codes with a
square-root like lower bound is presented.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20181" title="Abstract">arXiv:2310.20181</a> [<a href="/pdf/2310.20181" title="Download PDF">pdf</a>, <a href="/format/2310.20181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An explicit and symmetric exponential wave integrator for the nonlinear  Schr&#xf6;dinger equation with low regularity potential and nonlinearity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bao%2C+W">Weizhu Bao</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+C">Chushan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose and analyze a novel symmetric exponential wave integrator (sEWI)
for the nonlinear Schr\"odinger equation (NLSE) with low regularity potential
and typical power-type nonlinearity of the form $ f(\rho) = \rho^\sigma $,
where $ \rho:=|\psi|^2 $ is the density with $ \psi $ the wave function and $
\sigma &gt; 0 $ is the exponent of the nonlinearity. The sEWI is explicit and
stable under a time step size restriction independent of the mesh size. We
rigorously establish error estimates of the sEWI under various regularity
assumptions on potential and nonlinearity. For "good" potential and
nonlinearity ($H^2$-potential and $\sigma \geq 1$), we establish an optimal
second-order error bound in $L^2$-norm. For low regularity potential and
nonlinearity ($L^\infty$-potential and $\sigma &gt; 0$), we obtain a first-order
$L^2$-norm error bound accompanied with a uniform $H^2$-norm bound of the
numerical solution. Moreover, adopting a new technique of \textit{regularity
compensation oscillation} (RCO) to analyze error cancellation, for some
non-resonant time steps, the optimal second-order $L^2$-norm error bound is
proved under a weaker assumption on the nonlinearity: $\sigma \geq 1/2$. For
all the cases, we also present corresponding fractional order error bounds in
$H^1$-norm, which is the natural norm in terms of energy. Extensive numerical
results are reported to confirm our error estimates and to demonstrate the
superiority of the sEWI, including much weaker regularity requirements on
potential and nonlinearity, and excellent long-time behavior with
near-conservation of mass and energy.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20183" title="Abstract">arXiv:2310.20183</a> [<a href="/pdf/2310.20183" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thriving in a Pandemic: Lessons Learned from a Resilient University  Program Seen Through the CoI Lens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zihui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+J+C+E">John C.E. Johnson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In March 2020, college campuses underwent a sudden transformation to online
learning due to the COVID-19 outbreak. To understand the impact of COVID-19 on
students' expectations, this study conducted a three-year survey from ten core
courses within the Project Management Center for Excellence at the University
of Maryland. The study involved two main steps: 1) a statistical analysis to
evaluate students' expectations regarding "student," "class," "instructor," and
"effort;" and 2) a lexical salience-valence analysis (LSVA) through the lens of
the Community of Inquiry (CoI) framework to show the changes of students'
expectations. The results revealed that students' overall evaluations
maintained relatively consistent amid the COVID-19 teaching period. However,
there were significant shifts of the student expectations toward Cognitive,
Social and Teaching Presence course elements based on LSVA results. Also, clear
differences emerged between under-graduates and graduates in their expectations
and preferences in course design and delivery. These insights provide practical
recommendations for course instructors in designing effective online courses.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20185" title="Abstract">arXiv:2310.20185</a> [<a href="/pdf/2310.20185" title="Download PDF">pdf</a>, <a href="/format/2310.20185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decomposed Phase Analysis using Convex Inner Approximations: a  Methodology for DER Hosting Capacity in Distribution Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mavalizadeh%2C+H">Hani Mavalizadeh</a>, 
<a href="/search/eess?searchtype=author&query=Almassalkhi%2C+M">Mads Almassalkhi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, submitted to PSCC 2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper uses convex inner approximations (CIA) of the AC power flow to
tackle the optimization problem of quantifying a three-phase distribution
feeder's capacity to host distributed energy resources (DERs). This is often
connoted hosting capacity (HC), but herein we consider separative bounds for
each node on positive and negative DER injections, which ensures that
injections within these nodal limits satisfy feeder voltage and current limits
and across nodes sum up to the feeder HC. The methodology decomposes a
three-phase feeder into separate phases and applies CIA-based techniques to
each phase. An analysis is developed to determine the technical condition under
which this per-phase approach can still guarantee three-phase constraints. New
approaches are then presented that modify the per-phase optimization problems
to overcome conservativeness inherent to CIA methods and increase HC, including
selectively modifying the per-phase impedances and iteratively relaxing
per-phase voltage bounds. Discussion is included on trade-offs and feasibility.
To validate the methodology simulation-based analysis is conducted with the
IEEE 37-node test feeder and a real 534-node unbalanced radial distribution
feeder.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20187" title="Abstract">arXiv:2310.20187</a> [<a href="/pdf/2310.20187" title="Download PDF">pdf</a>, <a href="/format/2310.20187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Pre-training for Precipitation Post-processor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+S">Sojung An</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junha Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Jiyeon Jang</a>, 
<a href="/search/cs?searchtype=author&query=Na%2C+I">Inchae Na</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+W">Wooyeon Park</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+S">Sujeong You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Securing sufficient forecast lead time for local precipitation is essential
for preventing hazardous weather events. Nonetheless, global warming-induced
climate change is adding to the challenge of accurately predicting severe
precipitation events, such as heavy rainfall. In this work, we propose a deep
learning-based precipitation post-processor approach to numerical weather
prediction (NWP) models. The precipitation post-processor consists of (i)
self-supervised pre-training, where parameters of encoder are pre-trained on
the reconstruction of masked variables of the atmospheric physics domain, and
(ii) transfer learning on precipitation segmentation tasks (target domain) from
the pre-trained encoder. We also introduce a heuristic labeling approach for
effectively training class-imbalanced datasets. Our experiment results in
precipitation correction for regional NWP show that the proposed method
outperforms other approaches.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20189" title="Abstract">arXiv:2310.20189</a> [<a href="/pdf/2310.20189" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LFG: A Generative Network for Real-Time Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junyi Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure, 4 tables. Source code would be uploaded to github soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommender systems are essential information technologies today, and
recommendation algorithms combined with deep learning have become a research
hotspot in this field. The recommendation model known as LFM (Latent Factor
Model), which captures latent features through matrix factorization and
gradient descent to fit user preferences, has given rise to various
recommendation algorithms that bring new improvements in recommendation
accuracy. However, collaborative filtering recommendation models based on LFM
lack flexibility and has shortcomings for real-time recommendations, as they
need to redo the matrix factorization and retrain using gradient descent when
new users arrive. In response to this, this paper innovatively proposes a
Latent Factor Generator (LFG) network, and set the movie recommendation as
research theme. The LFG dynamically generates user latent factors through deep
neural networks without the need for re-factorization or retrain. Experimental
results indicate that the LFG recommendation model outperforms traditional
matrix factorization algorithms in recommendation accuracy, providing an
effective solution to the challenges of real-time recommendations with LFM.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20190" title="Abstract">arXiv:2310.20190</a> [<a href="/pdf/2310.20190" title="Download PDF">pdf</a>, <a href="/format/2310.20190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visible to Thermal image Translation for improving visual task in low  light conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+A">Md Azim Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Several visual tasks, such as pedestrian detection and image-to-image
translation, are challenging to accomplish in low light using RGB images. Heat
variation of objects in thermal images can be used to overcome this. In this
work, an end-to-end framework, which consists of a generative network and a
detector network, is proposed to translate RGB image into Thermal ones and
compare generated thermal images with real data. We have collected images from
two different locations using the Parrot Anafi Thermal drone. After that, we
created a two-stream network, preprocessed, augmented, the image data, and
trained the generator and discriminator models from scratch. The findings
demonstrate that it is feasible to translate RGB training data to thermal data
using GAN. As a result, thermal data can now be produced more quickly and
affordably, which is useful for security and surveillance applications.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20192" title="Abstract">arXiv:2310.20192</a> [<a href="/pdf/2310.20192" title="Download PDF">pdf</a>, <a href="/format/2310.20192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shaping Opinions in Social Networks with Shadow Banning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yen-Shao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zaman%2C+T">Tauhid Zaman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The proliferation of harmful content and misinformation on social networks
necessitates content moderation policies to maintain platform health. One such
policy is shadow banning, which limits content visibility. The danger of shadow
banning is that it can be misused by social media platforms to manipulate
opinions. Here we present an optimization based approach to shadow banning that
can shape opinions into a desired distribution and scale to large networks.
Simulations on real network topologies show that our shadow banning policies
can shift opinions and increase or decrease opinion polarization. We find that
if one shadow bans with the aim of shifting opinions in a certain direction,
the resulting shadow banning policy can appear neutral. This shows the
potential for social media platforms to misuse shadow banning without being
detected. Our results demonstrate the power and danger of shadow banning for
opinion manipulation in social networks.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20193" title="Abstract">arXiv:2310.20193</a> [<a href="/pdf/2310.20193" title="Download PDF">pdf</a>, <a href="/format/2310.20193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated  Recommendation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+X">Xi Leng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 59th Annual Allerton Conference on Communication, Control, and Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Preserving privacy and reducing communication costs for edge users pose
significant challenges in recommendation systems. Although federated learning
has proven effective in protecting privacy by avoiding data exchange between
clients and servers, it has been shown that the server can infer user ratings
based on updated non-zero gradients obtained from two consecutive rounds of
user-uploaded gradients. Moreover, federated recommendation systems (FRS) face
the challenge of heterogeneity, leading to decreased recommendation
performance. In this paper, we propose FedRec+, an ensemble framework for FRS
that enhances privacy while addressing the heterogeneity challenge. FedRec+
employs optimal subset selection based on feature similarity to generate
near-optimal virtual ratings for pseudo items, utilizing only the user's local
information. This approach reduces noise without incurring additional
communication costs. Furthermore, we utilize the Wasserstein distance to
estimate the heterogeneity and contribution of each client, and derive optimal
aggregation weights by solving a defined optimization problem. Experimental
results demonstrate the state-of-the-art performance of FedRec+ across various
reference datasets.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20195" title="Abstract">arXiv:2310.20195</a> [<a href="/pdf/2310.20195" title="Download PDF">pdf</a>, <a href="/format/2310.20195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Continuations in Multilingual Idiomatic Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pokharel%2C+R">Rhitabrat Pokharel</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Ameeta Agrawal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at MRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The ability to process idiomatic or literal multiword expressions is a
crucial aspect of understanding and generating any language. The task of
generating contextually relevant continuations for narratives containing
idiomatic (or literal) expressions can allow us to test the ability of
generative language models (LMs) in understanding nuanced language containing
non-compositional figurative text. We conduct a series of experiments using
datasets in two distinct languages (English and Portuguese) under three
different training settings (zero-shot, few-shot, and fine-tuned). Our results
suggest that the models are only slightly better at generating continuations
for literal contexts than idiomatic contexts, with exceedingly small margins.
Furthermore, the models studied in this work perform equally well across both
languages, indicating the robustness of generative models in performing this
task.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20199" title="Abstract">arXiv:2310.20199</a> [<a href="/pdf/2310.20199" title="Download PDF">pdf</a>, <a href="/format/2310.20199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In Search of Lost Online Test-time Adaptation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yadan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuoxiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zi Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In this paper, we present a comprehensive survey on online test-time
adaptation (OTTA), a paradigm focused on adapting machine learning models to
novel data distributions upon batch arrival. Despite the proliferation of OTTA
methods recently, the field is mired in issues like ambiguous settings,
antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the
real challenges and making reproducibility elusive. For clarity and a rigorous
comparison, we classify OTTA techniques into three primary categories and
subject them to benchmarks using the potent Vision Transformer (ViT) backbone
to discover genuinely effective strategies. Our benchmarks span not only
conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also
real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating
variations across search engines and synthesized data by diffusion models. To
gauge efficiency in online scenarios, we introduce novel evaluation metrics,
inclusive of FLOPs, shedding light on the trade-offs between adaptation
accuracy and computational overhead. Our findings diverge from existing
literature, indicating: (1) transformers exhibit heightened resilience to
diverse domain shifts, (2) the efficacy of many OTTA methods hinges on ample
batch sizes, and (3) stability in optimization and resistance to perturbations
are critical during adaptation, especially when the batch size is 1. Motivated
by these insights, we pointed out promising directions for future research. The
source code will be made available.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20200" title="Abstract">arXiv:2310.20200</a> [<a href="/pdf/2310.20200" title="Download PDF">pdf</a>, <a href="/format/2310.20200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Domain Polarization for Enhancing the Physical Layer Security of  MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+L">Luping Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yao Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">A novel Physical Layer Security (PLS) framework is conceived for enhancing
the security of the wireless communication systems by exploiting multi-domain
polarization in Multiple-Input Multiple-Output (MIMO) systems. We design a
sophisticated key generation scheme based on multi-domain polarization, and the
corresponding receivers. An in-depth analysis of the system's secrecy rate is
provided, demonstrating the confidentiality of our approach in the presence of
eavesdroppers having strong computational capabilities. More explicitly, our
simulation results and theoretical analysis corroborate the advantages of the
proposed scheme in terms of its bit error rate (BER), block error rate (BLER),
and maximum achievable secrecy rate. Our findings indicate that the innovative
PLS framework effectively enhances the security and reliability of wireless
communication systems. For instance, in a $4\times4$ MIMO setup, the proposed
PLS strategy exhibits an improvement of $2$dB compared to conventional MIMO,
systems at a BLER of $2\cdot 10^{-5}$ while the eavesdropper's BLER reaches
$1$.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20201" title="Abstract">arXiv:2310.20201</a> [<a href="/pdf/2310.20201" title="Download PDF">pdf</a>, <a href="/format/2310.20201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-Helpful Multimodal Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yihang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shimizu%2C+S">Shuichiro Shimizu</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+C">Chenhui Chu</a>, 
<a href="/search/cs?searchtype=author&query=Kurohashi%2C+S">Sadao Kurohashi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Main Conference (long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing multimodal machine translation (MMT) datasets consist of images and
video captions or instructional video subtitles, which rarely contain
linguistic ambiguity, making visual information ineffective in generating
appropriate translations. Recent work has constructed an ambiguous subtitles
dataset to alleviate this problem but is still limited to the problem that
videos do not necessarily contribute to disambiguation. We introduce EVA
(Extensive training set and Video-helpful evaluation set for Ambiguous
subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En)
parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs,
and corresponding video clips collected from movies and TV episodes. In
addition to the extensive training set, EVA contains a video-helpful evaluation
set in which subtitles are ambiguous, and videos are guaranteed helpful for
disambiguation. Furthermore, we propose SAFA, an MMT model based on the
Selective Attention model with two novel methods: Frame attention loss and
Ambiguity augmentation, aiming to use videos in EVA for disambiguation fully.
Experiments on EVA show that visual information and the proposed methods can
boost translation performance, and our model performs significantly better than
existing MMT models. The EVA dataset and the SAFA model are available at:
https://github.com/ku-nlp/video-helpful-MMT.git.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20203" title="Abstract">arXiv:2310.20203</a> [<a href="/pdf/2310.20203" title="Download PDF">pdf</a>, <a href="/format/2310.20203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Importance Estimation with Random Gradient for Neural Network Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sapkota%2C+S">Suman Sapkota</a>, 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+B">Binod Bhattarai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures, ICLR 2023 Workshop on Sparsity in Neural Networks. arXiv admin note: text overlap with <a href="/abs/2306.13203">arXiv:2306.13203</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Global Neuron Importance Estimation is used to prune neural networks for
efficiency reasons. To determine the global importance of each neuron or
convolutional kernel, most of the existing methods either use activation or
gradient information or both, which demands abundant labelled examples. In this
work, we use heuristics to derive importance estimation similar to Taylor First
Order (TaylorFO) approximation based methods. We name our methods TaylorFO-abs
and TaylorFO-sq. We propose two additional methods to improve these importance
estimation methods. Firstly, we propagate random gradients from the last layer
of a network, thus avoiding the need for labelled examples. Secondly, we
normalize the gradient magnitude of the last layer output before propagating,
which allows all examples to contribute similarly to the importance score. Our
methods with additional techniques perform better than previous methods when
tested on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets.
Furthermore, our method also complements the existing methods and improves
their performances when combined with them.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20204" title="Abstract">arXiv:2310.20204</a> [<a href="/pdf/2310.20204" title="Download PDF">pdf</a>, <a href="/format/2310.20204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General-Purpose Retrieval-Enhanced Medical Prediction Model Using  Near-Infinite History
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shim%2C+C">Chaeeun Shim</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B+S+K">Bosco Seong Kyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Im%2C+C">Chami Im</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+S+Y">Sung Yoon Lim</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+H">Han-Gil Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The source codes corresponding to this paper are available at: <a href="https://github.com/starmpcc/REMed">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Developing clinical prediction models (e.g., mortality prediction) based on
electronic health records (EHRs) typically relies on expert opinion for feature
selection and adjusting observation window size. This burdens experts and
creates a bottleneck in the development process. We propose Retrieval-Enhanced
Medical prediction model (REMed) to address such challenges. REMed can
essentially evaluate an unlimited number of clinical events, select the
relevant ones, and make predictions. This approach effectively eliminates the
need for manual feature selection and enables an unrestricted observation
window. We verified these properties through experiments on 27 clinical tasks
and two independent cohorts from publicly available EHR datasets, where REMed
outperformed other contemporary architectures that aim to handle as many events
as possible. Notably, we found that the preferences of REMed align closely with
those of medical experts. We expect our approach to significantly expedite the
development of EHR prediction models by minimizing clinicians' need for manual
involvement.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20208" title="Abstract">arXiv:2310.20208</a> [<a href="/pdf/2310.20208" title="Download PDF">pdf</a>, <a href="/format/2310.20208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Youwei Pang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaoqi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tian-Zhu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lihe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huchuan Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extensions to the conference version: <a href="/abs/2203.02688">arXiv:2203.02688</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent camouflaged object detection (COD) attempts to segment objects
visually blended into their surroundings, which is extremely complex and
difficult in real-world scenarios. Apart from the high intrinsic similarity
between camouflaged objects and their background, objects are usually diverse
in scale, fuzzy in appearance, and even severely occluded. To this end, we
propose an effective unified collaborative pyramid network which mimics human
behavior when observing vague images and videos, \textit{i.e.}, zooming in and
out. Specifically, our approach employs the zooming strategy to learn
discriminative mixed-scale semantics by the multi-head scale integration and
rich granularity perception units, which are designed to fully explore
imperceptible clues between candidate objects and background surroundings. The
former's intrinsic multi-head aggregation provides more diverse visual
patterns. The latter's routing mechanism can effectively propagate inter-frame
difference in spatiotemporal scenarios and adaptively ignore static
representations. They provides a solid foundation for realizing a unified
architecture for static and dynamic COD. Moreover, considering the uncertainty
and ambiguity derived from indistinguishable textures, we construct a simple
yet effective regularization, uncertainty awareness loss, to encourage
predictions with higher confidence in candidate regions. Our highly
task-friendly framework consistently outperforms existing state-of-the-art
methods in image and video COD benchmarks. The code will be available at
\url{https://github.com/lartpang/ZoomNeXt}.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20209" title="Abstract">arXiv:2310.20209</a> [<a href="/pdf/2310.20209" title="Download PDF">pdf</a>, <a href="/format/2310.20209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Contention-Aware Cluster Scheduling with Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ryu%2C+J">Junyeol Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Eo%2C+J">Jeongyoon Eo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">With continuous advances in deep learning, distributed training is becoming
common in GPU clusters. Specifically, for emerging workloads with diverse
amounts, ratios, and patterns of communication, we observe that network
contention can significantly degrade training throughput. However, widely used
scheduling policies often face limitations as they are agnostic to network
contention between jobs. In this paper, we present a new approach to mitigate
network contention in GPU clusters using reinforcement learning. We formulate
GPU cluster scheduling as a reinforcement learning problem and opt to learn a
network contention-aware scheduling policy that efficiently captures contention
sensitivities and dynamically adapts scheduling decisions through continuous
evaluation and improvement. We show that compared to widely used scheduling
policies, our approach reduces average job completion time by up to 18.2\% and
effectively cuts the tail job completion time by up to 20.7\% while allowing a
preferable trade-off between average job completion time and resource
utilization.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20210" title="Abstract">arXiv:2310.20210</a> [<a href="/pdf/2310.20210" title="Download PDF">pdf</a>, <a href="/format/2310.20210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UWFormer: Underwater Image Enhancement via a Semi-Supervised Multi-Scale  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zinuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shenghong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weiwen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pun%2C+C">Chi-Man Pun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Underwater images often exhibit poor quality, imbalanced coloration, and low
contrast due to the complex and intricate interaction of light, water, and
objects. Despite the significant contributions of previous underwater
enhancement techniques, there exist several problems that demand further
improvement: (i) Current deep learning methodologies depend on Convolutional
Neural Networks (CNNs) that lack multi-scale enhancement and also have limited
global perception fields. (ii) The scarcity of paired real-world underwater
datasets poses a considerable challenge, and the utilization of synthetic image
pairs risks overfitting. To address the aforementioned issues, this paper
presents a Multi-scale Transformer-based Network called UWFormer for enhancing
images at multiple frequencies via semi-supervised learning, in which we
propose a Nonlinear Frequency-aware Attention mechanism and a Multi-Scale
Fusion Feed-forward Network for low-frequency enhancement. Additionally, we
introduce a specialized underwater semi-supervised training strategy, proposing
a Subaqueous Perceptual Loss function to generate reliable pseudo labels.
Experiments using full-reference and non-reference underwater benchmarks
demonstrate that our method outperforms state-of-the-art methods in terms of
both quantity and visual quality.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20211" title="Abstract">arXiv:2310.20211</a> [<a href="/pdf/2310.20211" title="Download PDF">pdf</a>, <a href="/format/2310.20211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibration by Distribution Matching: Trainable Kernel Calibration  Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marx%2C+C">Charles Marx</a>, 
<a href="/search/cs?searchtype=author&query=Zalouk%2C+S">Sofian Zalouk</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Calibration ensures that probabilistic forecasts meaningfully capture
uncertainty by requiring that predicted probabilities align with empirical
frequencies. However, many existing calibration methods are specialized for
post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on
the insight that calibration can be viewed as a distribution matching task, we
introduce kernel-based calibration metrics that unify and generalize popular
forms of calibration for both classification and regression. These metrics
admit differentiable sample estimates, making it easy to incorporate a
calibration objective into empirical risk minimization. Furthermore, we provide
intuitive mechanisms to tailor calibration metrics to a decision task, and
enforce accurate loss estimation and no regret decisions. Our empirical
evaluation demonstrates that employing these metrics as regularizers enhances
calibration, sharpness, and decision-making across a range of regression and
classification tasks, outperforming methods relying solely on post-hoc
recalibration.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20212" title="Abstract">arXiv:2310.20212</a> [<a href="/pdf/2310.20212" title="Download PDF">pdf</a>, <a href="/format/2310.20212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Evaluation of Automated Analysis Tools for Solidity Smart  Contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiyuan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liehuang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 6 figure, IEEE Communications Surveys &amp; Tutorials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Blockchain smart contracts have emerged as a transformative force in the
digital realm, spawning a diverse range of compelling applications. Since
solidity smart contracts across various domains manage trillions of dollars in
virtual coins, they become a prime target for attacks. One of the primary
challenges is keeping abreast of the latest techniques and tools for developing
secure smart contracts and examining those already deployed. In this paper, we
seek to address these challenges from four aspects: (1) We begin by examining
ten automatic tools, specifically focusing on their methodologies and their
ability to identify vulnerabilities in solidity smart contracts. (2) We propose
a novel criterion for evaluating these tools, based on the ISO/IEC 25010
standard. (3) To facilitate the evaluation of the selected tools, we construct
a benchmark that encompasses two distinct datasets: a collection of 389
labelled smart contracts and a scaled set of 20,000 unique cases from
real-world contracts. (4) We provide a comparison of the selected tools,
offering insights into their strengths and weaknesses and highlighting areas
where further improvements are needed. Through this evaluation, we hope to
provide developers and researchers with valuable guidance on selecting and
using smart contract analysis tools and contribute to the ongoing efforts to
improve the security and reliability of smart contracts.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20215" title="Abstract">arXiv:2310.20215</a> [<a href="/pdf/2310.20215" title="Download PDF">pdf</a>, <a href="/ps/2310.20215" title="Download PostScript">ps</a>, <a href="/format/2310.20215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handover Protocol Learning for LEO Satellite Networks: Access Delay and  Collision Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Ju-Hyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Soohyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Molisch%2C+A+F">Andreas F. Molisch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">This study presents a novel deep reinforcement learning (DRL)-based handover
(HO) protocol, called DHO, specifically designed to address the persistent
challenge of long propagation delays in low-Earth orbit (LEO) satellite
networks' HO procedures. DHO skips the Measurement Report (MR) in the HO
procedure by leveraging its predictive capabilities after being trained with a
pre-determined LEO satellite orbital pattern. This simplification eliminates
the propagation delay incurred during the MR phase, while still providing
effective HO decisions. The proposed DHO outperforms the legacy HO protocol
across diverse network conditions in terms of access delay, collision rate, and
handover success rate, demonstrating the practical applicability of DHO in
real-world networks. Furthermore, the study examines the trade-off between
access delay and collision rate and also evaluates the training performance and
convergence of DHO using various DRL algorithms.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20216" title="Abstract">arXiv:2310.20216</a> [<a href="/pdf/2310.20216" title="Download PDF">pdf</a>, <a href="/format/2310.20216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does GPT-4 Pass the Turing Test?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jones%2C+C">Cameron Jones</a>, 
<a href="/search/cs?searchtype=author&query=Bergen%2C+B">Benjamin Bergen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4
prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and
GPT-3.5 (14%), but falling short of chance and the baseline set by human
participants (63%). Participants' decisions were based mainly on linguistic
style (35%) and socio-emotional traits (27%), supporting the idea that
intelligence is not sufficient to pass the Turing Test. Participants'
demographics, including education and familiarity with LLMs, did not predict
detection rate, suggesting that even those who understand systems deeply and
interact with them frequently may be susceptible to deception. Despite known
limitations as a test of intelligence, we argue that the Turing Test continues
to be relevant as an assessment of naturalistic communication and deception. AI
models with the ability to masquerade as humans could have widespread societal
consequences, and we analyse the effectiveness of different strategies and
criteria for judging humanlikeness.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20218" title="Abstract">arXiv:2310.20218</a> [<a href="/pdf/2310.20218" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Review for Transformer-based Long-term Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+L">Liyilei Su</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+X">Xumin Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Heng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Bingding Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The emergence of deep learning has yielded noteworthy advancements in time
series forecasting (TSF). Transformer architectures, in particular, have
witnessed broad utilization and adoption in TSF tasks. Transformers have proven
to be the most successful solution to extract the semantic correlations among
the elements within a long sequence. Various variants have enabled transformer
architecture to effectively handle long-term time series forecasting (LTSF)
tasks. In this article, we first present a comprehensive overview of
transformer architectures and their subsequent enhancements developed to
address various LTSF tasks. Then, we summarize the publicly available LTSF
datasets and relevant evaluation metrics. Furthermore, we provide valuable
insights into the best practices and techniques for effectively training
transformers in the context of time-series analysis. Lastly, we propose
potential research directions in this rapidly evolving field.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20223" title="Abstract">arXiv:2310.20223</a> [<a href="/pdf/2310.20223" title="Download PDF">pdf</a>, <a href="/format/2310.20223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maoxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Weilong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianpu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zijian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+M">Mengda Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As the development of cities, traffic congestion becomes an increasingly
pressing issue, and traffic prediction is a classic method to relieve that
issue. Traffic prediction is one specific application of spatio-temporal
prediction learning, like taxi scheduling, weather prediction, and ship
trajectory prediction. Against these problems, classical spatio-temporal
prediction learning methods including deep learning, require large amounts of
training data. In reality, some newly developed cities with insufficient
sensors would not hold that assumption, and the data scarcity makes predictive
performance worse. In such situation, the learning method on insufficient data
is known as few-shot learning (FSL), and the FSL of traffic prediction remains
challenges. On the one hand, graph structures' irregularity and dynamic nature
of graphs cannot hold the performance of spatio-temporal learning method. On
the other hand, conventional domain adaptation methods cannot work well on
insufficient training data, when transferring knowledge from different domains
to the intended target domain.To address these challenges, we propose a novel
spatio-temporal domain adaptation (STDA) method that learns transferable
spatio-temporal meta-knowledge from data-sufficient cities in an adversarial
manner. This learned meta-knowledge can improve the prediction performance of
data-scarce cities. Specifically, we train the STDA model using a
Model-Agnostic Meta-Learning (MAML) based episode learning process, which is a
model-agnostic meta-learning framework that enables the model to solve new
learning tasks using only a small number of training samples. We conduct
numerous experiments on four traffic prediction datasets, and our results show
that the prediction performance of our model has improved by 7\% compared to
baseline models on the two metrics of MAE and RMSE.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20225" title="Abstract">arXiv:2310.20225</a> [<a href="/pdf/2310.20225" title="Download PDF">pdf</a>, <a href="/format/2310.20225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VisPercep: A Vision-Language Approach to Enhance Visual Perception for  People with Blindness and Low Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yu Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuaihang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Rangan%2C+S">Sundeep Rangan</a>, 
<a href="/search/cs?searchtype=author&query=Rizzo%2C+J">John-Ross Rizzo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yi Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">People with blindness and low vision (pBLV) encounter substantial challenges
when it comes to comprehensive scene recognition and precise object
identification in unfamiliar environments. Additionally, due to the vision
loss, pBLV have difficulty in accessing and identifying potential tripping
hazards on their own. In this paper, we present a pioneering approach that
leverages a large vision-language model to enhance visual perception for pBLV,
offering detailed and comprehensive descriptions of the surrounding
environments and providing warnings about the potential risks. Our method
begins by leveraging a large image tagging model (i.e., Recognize Anything
(RAM)) to identify all common objects present in the captured images. The
recognition results and user query are then integrated into a prompt, tailored
specifically for pBLV using prompt engineering. By combining the prompt and
input image, a large vision-language model (i.e., InstructBLIP) generates
detailed and comprehensive descriptions of the environment and identifies
potential risks in the environment by analyzing the environmental objects and
scenes, relevant to the prompt. We evaluate our approach through experiments
conducted on both indoor and outdoor datasets. Our results demonstrate that our
method is able to recognize objects accurately and provide insightful
descriptions and analysis of the environment for pBLV.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20227" title="Abstract">arXiv:2310.20227</a> [<a href="/pdf/2310.20227" title="Download PDF">pdf</a>, <a href="/ps/2310.20227" title="Download PostScript">ps</a>, <a href="/format/2310.20227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Scalable Capacity in Wireless Mesh Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+L">Lei Lei</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+A">Aimin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xudong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ~12pages, 4 figures, submitted to IEEE TIT, part of this work has been published in IEEE MASS 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Wireless mesh networks play a critical role in enabling key networking
scenarios in beyond-5G (B5G) and 6G networks, including integrated access and
backhaul (IAB), multi-hop sidelinks, and V2X. However, it still poses a
challenge to deliver scalable per-node throughput via mesh networking, which
significantly limits the potential of large-scale deployment of wireless mesh
networks. Existing research has achieved $O(1)$ per-node throughput in a dense
network, but how to achieve scalability remains an unresolved issue for an
extended wireless network where the network size increases with a constant node
density. This issue prevents a wireless mesh network from large-scale
deployment. To this end, this paper aims to develop a theoretical approach to
achieving scalable per-node throughput in wireless mesh networks. First, the
key factors that limit the per-node throughput of wireless mesh networks are
analyzed, through which two major ones are identified, i.e., link sharing and
interference. Next, a multi-tier hierarchical architecture is proposed to
overcome the link-sharing issue. The inter-tier interference under this
architecture is then mitigated by utilizing orthogonal frequency allocation
between adjacent tiers, while the intra-tier interference is reduced by
considering two specific transmission schemes, one is MIMO spatial multiplexing
with time-division, the other is MIMO beamforming. Theoretical analysis shows
that the multi-tier mesh networking architecture can achieve a per-node
throughput of $\Theta(1)$ in both schemes, as long as certain conditions on
network parameters including bandwidth, antenna numbers, and node numbers of
each tier are satisfied. A case study on a realistic deployment of 10,000 nodes
is then carried out, which demonstrates that a scalable throughput of
$\Theta(1)$ is achievable with a reasonable assumption on bandwidth and antenna
numbers.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20228" title="Abstract">arXiv:2310.20228</a> [<a href="/pdf/2310.20228" title="Download PDF">pdf</a>, <a href="/format/2310.20228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing Human Pose from Inertial Measurements: A Generative  Model-based Compressive Sensing Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hieu%2C+N+Q">Nguyen Quang Hieu</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+T">Dinh Thai Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+N">Diep N. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Alsheikh%2C+M+A">Mohammad Abu Alsheikh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The ability to sense, localize, and estimate the 3D position and orientation
of the human body is critical in virtual reality (VR) and extended reality (XR)
applications. This becomes more important and challenging with the deployment
of VR/XR applications over the next generation of wireless systems such as 5G
and beyond. In this paper, we propose a novel framework that can reconstruct
the 3D human body pose of the user given sparse measurements from Inertial
Measurement Unit (IMU) sensors over a noisy wireless environment. Specifically,
our framework allows the transmission of compressed IMU signals through noisy
wireless channels and the recovery of such signals at the receiver, e.g., an
edge server. This task is very challenging due to the constraints of transmit
power, recovery accuracy, and recovery latency. To address these challenges, we
first develop a deep generative model at the receiver to recover the data from
linear measurements of IMU signals. The linear measurements of the IMU signals
are obtained by a linear projection with a measurement matrix based on
compressive sensing theory. The key to the success of our framework lies in the
novel design of the measurement matrix at the transmitter, which can not only
ensure power constraints for the IMU devices but also obtain a highly accurate
recovery for the IMU signals at the receiver. Our framework can achieve robust
performance for recovering 3D human poses from noisy compressed IMU signals.
Additionally, our pre-trained deep generative model achieves signal
reconstruction accuracy comparable to an optimization-based approach, i.e.,
Lasso, but is an order of magnitude faster.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20234" title="Abstract">arXiv:2310.20234</a> [<a href="/pdf/2310.20234" title="Download PDF">pdf</a>, <a href="/format/2310.20234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection  in Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junnan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Guohuan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianmin Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaolin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D object detection in point clouds is important for autonomous driving
systems. A primary challenge in 3D object detection stems from the sparse
distribution of points within the 3D scene. Existing high-performance methods
typically employ 3D sparse convolutional neural networks with small kernels to
extract features. To reduce computational costs, these methods resort to
submanifold sparse convolutions, which prevent the information exchange among
spatially disconnected features. Some recent approaches have attempted to
address this problem by introducing large-kernel convolutions or self-attention
mechanisms, but they either achieve limited accuracy improvements or incur
excessive computational costs. We propose HEDNet, a hierarchical
encoder-decoder network for 3D object detection, which leverages
encoder-decoder blocks to capture long-range dependencies among features in the
spatial space, particularly for large and distant objects. We conducted
extensive experiments on the Waymo Open and nuScenes datasets. HEDNet achieved
superior detection accuracy on both datasets than previous state-of-the-art
methods with competitive efficiency. The code is available at
https://github.com/zhanggang001/HEDNet.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20236" title="Abstract">arXiv:2310.20236</a> [<a href="/pdf/2310.20236" title="Download PDF">pdf</a>, <a href="/format/2310.20236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamically Updating Event Representations for Temporal Relation  Classification with Multi-category Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+F">Fei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Asahara%2C+M">Masayuki Asahara</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+I">Ichiro Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Kurohashi%2C+S">Sadao Kurohashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2020 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Temporal relation classification is a pair-wise task for identifying the
relation of a temporal link (TLINK) between two mentions, i.e. event, time, and
document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs
involving a common mention do not share information. 2) Existing models with
independent classifiers for each TLINK category (E2E, E2T, and E2D) hinder from
using the whole data. This paper presents an event centric model that allows to
manage dynamic event representations across multiple TLINKs. Our model deals
with three TLINK categories with multi-task learning to leverage the full size
of data. The experimental results show that our proposal outperforms
state-of-the-art models and two transfer learning baselines on both the English
and Japanese data.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20239" title="Abstract">arXiv:2310.20239</a> [<a href="/pdf/2310.20239" title="Download PDF">pdf</a>, <a href="/format/2310.20239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coded Caching Schemes for Multiaccess Topologies via Combinatorial  Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minquan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Elia%2C+P">Petros Elia</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper studies a multiaccess coded caching (MACC) where the connectivity
topology between the users and the caches can be described by a class of
combinatorial designs. Our model includes as special cases several MACC
topologies considered in previous works. The considered MACC network includes a
server containing $N$ files, $\Gamma$ cache nodes and $K$ cacheless users,
where each user can access $L$ cache nodes. The server is connected to the
users via an error-free shared link, while the users can retrieve the cache
content of the connected cache-nodes while the users can directly access the
content in their connected cache-nodes. Our goal is to minimise the worst-case
transmission load on the shared link in the delivery phase. The main limitation
of the existing MACC works is that only some specific access topologies are
considered, and thus the number of users $K$ should be either linear or
exponential to $\Gamma$. We overcome this limitation by formulating a new
access topology derived from two classical combinatorial structures, referred
to as the $t$-design and the $t$-group divisible design. In these topologies,
$K$ scales linearly, polynomially, or even exponentially with $\Gamma$. By
leveraging the properties of the considered combinatorial structures, we
propose two classes of coded caching schemes for a flexible number of users,
where the number of users can scale linearly, polynomially or exponentially
with the number of cache nodes. In addition, our schemes can unify most schemes
for the shared link network and unify many schemes for the multi-access network
except for the cyclic wrap-around topology.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20240" title="Abstract">arXiv:2310.20240</a> [<a href="/pdf/2310.20240" title="Download PDF">pdf</a>, <a href="/format/2310.20240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breathing Life into Faces: Speech-driven 3D Facial Animation with  Natural Head Pose and Detailed Shape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianyu He</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lianying Yin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianxin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xin Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The creation of lifelike speech-driven 3D facial animation requires a natural
and precise synchronization between audio input and facial expressions.
However, existing works still fail to render shapes with flexible head poses
and natural facial details (e.g., wrinkles). This limitation is mainly due to
two aspects: 1) Collecting training set with detailed 3D facial shapes is
highly expensive. This scarcity of detailed shape annotations hinders the
training of models with expressive facial animation. 2) Compared to mouth
movement, the head pose is much less correlated to speech content.
Consequently, concurrent modeling of both mouth movement and head pose yields
the lack of facial movement controllability. To address these challenges, we
introduce VividTalker, a new framework designed to facilitate speech-driven 3D
facial animation characterized by flexible head pose and natural facial
details. Specifically, we explicitly disentangle facial animation into head
pose and mouth movement and encode them separately into discrete latent spaces.
Then, these attributes are generated through an autoregressive process
leveraging a window-based Transformer architecture. To augment the richness of
3D facial animation, we construct a new 3D dataset with detailed shapes and
learn to synthesize facial details in line with speech content. Extensive
quantitative and qualitative experiments demonstrate that VividTalker
outperforms state-of-the-art methods, resulting in vivid and realistic
speech-driven 3D facial animation.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20242" title="Abstract">arXiv:2310.20242</a> [<a href="/pdf/2310.20242" title="Download PDF">pdf</a>, <a href="/format/2310.20242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent-Reflecting-Surface-Assisted UAV Communications for 6G  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+Z">Zhaolong Ning</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tengfeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaojie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F+R">Fei Richard Yu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Song Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In 6th-Generation (6G) mobile networks, Intelligent Reflective Surfaces
(IRSs) and Unmanned Aerial Vehicles (UAVs) have emerged as promising
technologies to address the coverage difficulties and resource constraints
faced by terrestrial networks. UAVs, with their mobility and low costs, offer
diverse connectivity options for mobile users and a novel deployment paradigm
for 6G networks. However, the limited battery capacity of UAVs, dynamic and
unpredictable channel environments, and communication resource constraints
result in poor performance of traditional UAV-based networks. IRSs can not only
reconstruct the wireless environment in a unique way, but also achieve wireless
network relay in a cost-effective manner. Hence, it receives significant
attention as a promising solution to solve the above challenges. In this
article, we conduct a comprehensive survey on IRS-assisted UAV communications
for 6G networks. First, primary issues, key technologies, and application
scenarios of IRS-assisted UAV communications for 6G networks are introduced.
Then, we put forward specific solutions to the issues of IRS-assisted UAV
communications. Finally, we discuss some open issues and future research
directions to guide researchers in related fields.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20243" title="Abstract">arXiv:2310.20243</a> [<a href="/pdf/2310.20243" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrast-agent-induced deterministic component of CT-density in the  abdominal aorta during routine angiography: proof of concept study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kodenko%2C+M+R">Maria R. Kodenko</a>, 
<a href="/search/cs?searchtype=author&query=Vasilev%2C+Y+A">Yuriy A. Vasilev</a>, 
<a href="/search/cs?searchtype=author&query=Kulberg%2C+N+S">Nicholas S. Kulberg</a>, 
<a href="/search/cs?searchtype=author&query=Samorodov%2C+A+V">Andrey V. Samorodov</a>, 
<a href="/search/cs?searchtype=author&query=Vladzimirskyy%2C+A+V">Anton V. Vladzimirskyy</a>, 
<a href="/search/cs?searchtype=author&query=Omelyanskaya%2C+O+V">Olga V. Omelyanskaya</a>, 
<a href="/search/cs?searchtype=author&query=Reshetnikov%2C+R+V">Roman V. Reshetnikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Background and objective: CTA is a gold standard of preoperative diagnosis of
abdominal aorta and typically used for geometric-only characteristic
extraction. We assume that a model describing the dynamic behavior of the
contrast agent in the vessel can be developed from the data of routine CTA
studies, allowing the procedure to be investigated and optimized without the
need for additional perfusion CT studies. Obtained spatial distribution of CA
can be valuable for both increasing the diagnostic value of a particular study
and improving the CT data processing tools. Methods: In accordance with the
Beer-Lambert law and the absence of chemical interaction between blood and CA,
we postulated the existence of a deterministic CA-induced component in the CT
signal density. The proposed model, having a double-sigmoid structure, contains
six coefficients relevant to the properties of hemodynamics. To validate the
model, expert segmentation was performed using the 3D Slicer application for
the CTA data obtained from publicly available source. The model was fitted to
the data using the non-linear least square method with Levenberg-Marquardt
optimization. Results: We analyzed 594 CTA images (4 studies with median size
of 144 slices, IQR [134; 158.5]; 1:1 normal:pathology balance). Goodness-of-fit
was proved by Wilcox test (p-value &gt; 0.05 for all cases). The proposed model
correctly simulated normal blood flow and hemodynamics disturbances caused by
local abnormalities (aneurysm, thrombus and arterial branching). Conclusions:
Proposed approach can be useful for personalized CA modeling of vessels,
improvement of CTA image processing and preparation of synthetic CT training
data for artificial intelligence.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20246" title="Abstract">arXiv:2310.20246</a> [<a href="/pdf/2310.20246" title="Download PDF">pdf</a>, <a href="/format/2310.20246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking Language Barriers in Multilingual Mathematical Reasoning:  Insights and Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zinan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+N">Ning Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+L">Linjun Shou</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Ming Gong</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing research predominantly focuses on developing powerful language
learning models (LLMs) for mathematical reasoning within monolingual languages,
with few explorations in preserving efficacy in a multilingual context. To
bridge this gap, this paper pioneers exploring and training powerful
Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we
construct the first multilingual math reasoning instruction dataset,
MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue
of training data scarcity in xMR tasks. Based on the collected dataset, we
propose different training strategies to build powerful xMR LLMs, named
MathOctopus, notably outperform conventional open-source LLMs and exhibit
superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B
reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond
remarkable results, we unearth several pivotal observations and insights from
extensive experiments: (1) When extending the rejection sampling strategy to
the multilingual context, it proves effective for model performances, albeit
limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)
across multiple languages not only significantly enhances model performance
multilingually but also elevates their monolingual performance. This indicates
that crafting multilingual corpora can be regarded as a vital strategy for
enhancing model performance in a specific language, especially in mathematical
reasoning tasks. For instance, MathOctopus-7B improves its counterparts that
trained on English from 42.2% to 50.8% on GSM8K testset.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20248" title="Abstract">arXiv:2310.20248</a> [<a href="/pdf/2310.20248" title="Download PDF">pdf</a>, <a href="/ps/2310.20248" title="Download PostScript">ps</a>, <a href="/format/2310.20248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relative normalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dowek%2C+G">Gilles Dowek</a>, 
<a href="/search/cs?searchtype=author&query=Miquel%2C+A">Alexandre Miquel</a> (UPCit&#xe9;)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">G{\"o}del's second incompleteness theorem forbids to prove, in a given theory
U, the consistency of many theories-in particular, of the theory U itself-as
well as it forbids to prove the normalization property for these theories,
since this property implies their consistency. When we cannot prove in a theory
U the consistency of a theory T , we can try to prove a relative consistency
theorem, that is, a theorem of the form: If U is consistent then T is
consistent. Following the same spirit, we show in this paper how to prove
relative normalization theorems, that is, theorems of the form: If U is
1-consistent, then T has the normalization property.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20249" title="Abstract">arXiv:2310.20249</a> [<a href="/pdf/2310.20249" title="Download PDF">pdf</a>, <a href="/format/2310.20249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qingqing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peizhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yifan%2C+W">Wang Yifan</a>, 
<a href="/search/cs?searchtype=author&query=Sorkine-Hornung%2C+O">Olga Sorkine-Hornung</a>, 
<a href="/search/cs?searchtype=author&query=Wetzstein%2C+G">Gordon Wetzstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://cyanzhao42.github.io/pose2motion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Creating believable motions for various characters has long been a goal in
computer graphics. Current learning-based motion synthesis methods depend on
extensive motion datasets, which are often challenging, if not impossible, to
obtain. On the other hand, pose data is more accessible, since static posed
characters are easier to create and can even be extracted from images using
recent advancements in computer vision. In this paper, we utilize this
alternative data source and introduce a neural motion synthesis approach
through retargeting. Our method generates plausible motions for characters that
have only pose data by transferring motion from an existing motion capture
dataset of another character, which can have drastically different skeletons.
Our experiments show that our method effectively combines the motion features
of the source character with the pose features of the target character, and
performs robustly with small or noisy pose data sets, ranging from a few
artist-created poses to noisy poses estimated directly from images.
Additionally, a conducted user study indicated that a majority of participants
found our retargeted motion to be more enjoyable to watch, more lifelike in
appearance, and exhibiting fewer artifacts. Project page:
https://cyanzhao42.github.io/pose2motion
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20250" title="Abstract">arXiv:2310.20250</a> [<a href="/pdf/2310.20250" title="Download PDF">pdf</a>, <a href="/format/2310.20250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversified Node Sampling based Hierarchical Transformer Pooling for  Graph Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gaichao Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinsong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hopcroft%2C+J+E">John E. Hopcroft</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kun He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Graph pooling methods have been widely used on downsampling graphs, achieving
impressive results on multiple graph-level tasks like graph classification and
graph generation. An important line called node dropping pooling aims at
exploiting learnable scoring functions to drop nodes with comparatively lower
significance scores. However, existing node dropping methods suffer from two
limitations: (1) for each pooled node, these models struggle to capture
long-range dependencies since they mainly take GNNs as the backbones; (2)
pooling only the highest-scoring nodes tends to preserve similar nodes, thus
discarding the affluent information of low-scoring nodes. To address these
issues, we propose a Graph Transformer Pooling method termed GTPool, which
introduces Transformer to node dropping pooling to efficiently capture
long-range pairwise interactions and meanwhile sample nodes diversely.
Specifically, we design a scoring module based on the self-attention mechanism
that takes both global context and local context into consideration, measuring
the importance of nodes more comprehensively. GTPool further utilizes a
diversified sampling method named Roulette Wheel Sampling (RWS) that is able to
flexibly preserve nodes across different scoring intervals instead of only
higher scoring nodes. In this way, GTPool could effectively obtain long-range
information and select more representative nodes. Extensive experiments on 11
benchmark datasets demonstrate the superiority of GTPool over existing popular
graph pooling methods.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20251" title="Abstract">arXiv:2310.20251</a> [<a href="/pdf/2310.20251" title="Download PDF">pdf</a>, <a href="/format/2310.20251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Implementation of Multimodal Fusion System for Intelligent Digital  Human Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yaodong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+K">Kaiyue Bi</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+L">Lian Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">With the rapid development of artificial intelligence (AI), digital humans
have attracted more and more attention and are expected to achieve a wide range
of applications in several industries. Then, most of the existing digital
humans still rely on manual modeling by designers, which is a cumbersome
process and has a long development cycle. Therefore, facing the rise of digital
humans, there is an urgent need for a digital human generation system combined
with AI to improve development efficiency. In this paper, an implementation
scheme of an intelligent digital human generation system with multimodal fusion
is proposed. Specifically, text, speech and image are taken as inputs, and
interactive speech is synthesized using large language model (LLM), voiceprint
extraction, and text-to-speech conversion techniques. Then the input image is
age-transformed and a suitable image is selected as the driving image. Then,
the modification and generation of digital human video content is realized by
digital human driving, novel view synthesis, and intelligent dressing
techniques. Finally, we enhance the user experience through style transfer,
super-resolution, and quality evaluation. Experimental results show that the
system can effectively realize digital human generation. The related code is
released at https://github.com/zyj-2000/CUMT_2D_PhotoSpeaker.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20253" title="Abstract">arXiv:2310.20253</a> [<a href="/pdf/2310.20253" title="Download PDF">pdf</a>, <a href="/ps/2310.20253" title="Download PostScript">ps</a>, <a href="/format/2310.20253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cut elimination for Zermelo set theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dowek%2C+G">Gilles Dowek</a>, 
<a href="/search/cs?searchtype=author&query=Miquel%2C+A">Alexandre Miquel</a> (UPCit&#xe9;)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We show how to express intuitionistic Zermelo set theory in deduction modulo
(i.e. by replacing its axioms by rewrite rules) in such a way that the
corresponding notion of proof enjoys the normalization property. To do so, we
first rephrase set theory as a theory of pointed graphs (following a paradigm
due to P. Aczel) by interpreting set-theoretic equality as bisimilarity, and
show that in this setting, Zermelo's axioms can be decomposed into
graph-theoretic primitives that can be turned into rewrite rules. We then show
that the theory we obtain in deduction modulo is a conservative extension of (a
minor extension of) Zermelo set theory. Finally, we prove the normalization of
the intuitionistic fragment of the theory.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20254" title="Abstract">arXiv:2310.20254</a> [<a href="/pdf/2310.20254" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Intelligence for reverse engineering: application to  detergents using Raman spectroscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marote%2C+P">Pedro Marote</a> (UCBL, ISA), 
<a href="/search/cs?searchtype=author&query=Martin%2C+M">Marie Martin</a> (UCBL, ISA), 
<a href="/search/cs?searchtype=author&query=Bonhomme%2C+A">Anne Bonhomme</a>, 
<a href="/search/cs?searchtype=author&query=Lant%C3%A9ri%2C+P">Pierre Lant&#xe9;ri</a> (ISA, UCBL), 
<a href="/search/cs?searchtype=author&query=Cl%C3%A9ment%2C+Y">Yohann Cl&#xe9;ment</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The reverse engineering of a complex mixture, regardless of its nature, has
become significant today. Being able to quickly assess the potential toxicity
of new commercial products in relation to the environment presents a genuine
analytical challenge. The development of digital tools (databases,
chemometrics, machine learning, etc.) and analytical techniques (Raman
spectroscopy, NIR spectroscopy, mass spectrometry, etc.) will allow for the
identification of potential toxic molecules. In this article, we use the
example of detergent products, whose composition can prove dangerous to humans
or the environment, necessitating precise identification and quantification for
quality control and regulation purposes. The combination of various digital
tools (spectral database, mixture database, experimental design, Chemometrics /
Machine Learning algorithm{\ldots}) together with different sample preparation
methods (raw sample, or several concentrated / diluted samples) Raman
spectroscopy, has enabled the identification of the mixture's constituents and
an estimation of its composition. Implementing such strategies across different
analytical tools can result in time savings for pollutant identification and
contamination assessment in various matrices. This strategy is also applicable
in the industrial sector for product or raw material control, as well as for
quality control purposes.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20256" title="Abstract">arXiv:2310.20256</a> [<a href="/pdf/2310.20256" title="Download PDF">pdf</a>, <a href="/format/2310.20256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for  Personality Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Tianyuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fanqi Wan</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bingzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiaxiang Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advances in large language models (LLMs), such as ChatGPT, have
showcased remarkable zero-shot performance across various NLP tasks. However,
the potential of LLMs in personality detection, which involves identifying an
individual's personality from their written texts, remains largely unexplored.
Drawing inspiration from Psychological Questionnaires, which are carefully
designed by psychologists to evaluate individual personality traits through a
series of targeted items, we argue that these items can be regarded as a
collection of well-structured chain-of-thought (CoT) processes. By
incorporating these processes, LLMs can enhance their capabilities to make more
reasonable inferences on personality from textual input. In light of this, we
propose a novel personality detection method, called PsyCoT, which mimics the
way individuals complete psychological questionnaires in a multi-turn dialogue
manner. In particular, we employ a LLM as an AI assistant with a specialization
in text analysis. We prompt the assistant to rate individual items at each turn
and leverage the historical rating results to derive a conclusive personality
preference. Our experiments demonstrate that PsyCoT significantly improves the
performance and robustness of GPT-3.5 in personality detection, achieving an
average F1 score improvement of 4.23/10.63 points on two benchmark datasets
compared to the standard prompting method. Our code is available at
https://github.com/TaoYang225/PsyCoT.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20258" title="Abstract">arXiv:2310.20258</a> [<a href="/pdf/2310.20258" title="Download PDF">pdf</a>, <a href="/format/2310.20258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Bayesian Optimization via Learning Correlated Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seunghun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+J">Jaewon Chu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sihyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Juyeon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">Hyunwoo J. Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Bayesian optimization is a powerful method for optimizing black-box functions
with limited function evaluations. Recent works have shown that optimization in
a latent space through deep generative models such as variational autoencoders
leads to effective and efficient Bayesian optimization for structured or
discrete data. However, as the optimization does not take place in the input
space, it leads to an inherent gap that results in potentially suboptimal
solutions. To alleviate the discrepancy, we propose Correlated latent space
Bayesian Optimization (CoBO), which focuses on learning correlated latent
spaces characterized by a strong correlation between the distances in the
latent space and the distances within the objective function. Specifically, our
method introduces Lipschitz regularization, loss weighting, and trust region
recoordination to minimize the inherent gap around the promising areas. We
demonstrate the effectiveness of our approach on several optimization tasks in
discrete data, such as molecule design and arithmetic expression fitting, and
achieve high performance within a small budget.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20260" title="Abstract">arXiv:2310.20260</a> [<a href="/pdf/2310.20260" title="Download PDF">pdf</a>, <a href="/format/2310.20260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating  Chess Moves based on Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alrdahi%2C+H">Haifa Alrdahi</a>, 
<a href="/search/cs?searchtype=author&query=Batista-Navarro%2C+R">Riza Batista-Navarro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 10 Figures, 9 Tabels
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Learning chess strategies has been investigated widely, with most studies
focussing on learning from previous games using search algorithms. Chess
textbooks encapsulate grandmaster knowledge, explain playing strategies and
require a smaller search space compared to traditional chess agents. This paper
examines chess textbooks as a new knowledge source for enabling machines to
learn how to play chess -- a resource that has not been explored previously. We
developed the LEAP corpus, a first and new heterogeneous dataset with
structured (chess move notations and board states) and unstructured data
(textual descriptions) collected from a chess textbook containing 1164
sentences discussing strategic moves from 91 games. We firstly labelled the
sentences based on their relevance, i.e., whether they are discussing a move.
Each relevant sentence was then labelled according to its sentiment towards the
described move. We performed empirical experiments that assess the performance
of various transformer-based baseline models for sentiment analysis. Our
results demonstrate the feasibility of employing transformer-based sentiment
analysis models for evaluating chess moves, with the best performing model
obtaining a weighted micro F_1 score of 68%. Finally, we synthesised the LEAP
corpus to create a larger dataset, which can be used as a solution to the
limited textual resource in the chess domain.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20266" title="Abstract">arXiv:2310.20266</a> [<a href="/pdf/2310.20266" title="Download PDF">pdf</a>, <a href="/format/2310.20266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Average Return in Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marthe%2C+A">Alexandre Marthe</a> (ENS de Lyon, UMPA-ENSL), 
<a href="/search/cs?searchtype=author&query=Garivier%2C+A">Aur&#xe9;lien Garivier</a> (UMPA-ENSL, MC2), 
<a href="/search/cs?searchtype=author&query=Vernade%2C+C">Claire Vernade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips 2023, Dec 2023, New Orleans, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Optimization and Control (math.OC); Probability (math.PR)

</div>
<p class="mathjax">What are the functionals of the reward that can be computed and optimized
exactly in Markov Decision Processes? In the finite-horizon, undiscounted
setting, Dynamic Programming (DP) can only handle these operations efficiently
for certain classes of statistics. We summarize the characterization of these
classes for policy evaluation, and give a new answer for the planning problem.
Interestingly, we prove that only generalized means can be optimized exactly,
even in the more general framework of Distributional Reinforcement Learning
(DistRL).DistRL permits, however, to evaluate other functionals approximately.
We provide error bounds on the resulting estimators, and discuss the potential
of this approach as well as its limitations.These results contribute to
advancing the theory of Markov Decision Processes by examining overall
characteristics of the return, and particularly risk-conscious strategies.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20267" title="Abstract">arXiv:2310.20267</a> [<a href="/pdf/2310.20267" title="Download PDF">pdf</a>, <a href="/format/2310.20267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A non-overlapping optimization-based domain decomposition approach to  component-based model reduction of incompressible flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Taddei%2C+T">Tommaso Taddei</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+X">Xuejun Xu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a component-based model order reduction procedure to efficiently
and accurately solve parameterized incompressible flows governed by the
Navier-Stokes equations. Our approach leverages a non-overlapping
optimization-based domain decomposition technique to determine the control
variable that minimizes jumps across the interfaces between sub-domains. To
solve the resulting constrained optimization problem, we propose both
Gauss-Newton and sequential quadratic programming methods, which effectively
transform the constrained problem into an unconstrained formulation.
Furthermore, we integrate model order reduction techniques into the
optimization framework, to speed up computations. In particular, we incorporate
localized training and adaptive enrichment to reduce the burden associated with
the training of the local reduced-order models. Numerical results are presented
to demonstrate the validity and effectiveness of the overall methodology.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20268" title="Abstract">arXiv:2310.20268</a> [<a href="/pdf/2310.20268" title="Download PDF">pdf</a>, <a href="/format/2310.20268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructing Sample-to-Class Graph for Few-Shot Class-Incremental  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+F">Fuyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+F">Fan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fenglei Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Few-shot class-incremental learning (FSCIL) aims to build machine learning
model that can continually learn new concepts from a few data samples, without
forgetting knowledge of old classes.
<br />The challenges of FSCIL lies in the limited data of new classes, which not
only lead to significant overfitting issues but also exacerbates the notorious
catastrophic forgetting problems. As proved in early studies, building sample
relationships is beneficial for learning from few-shot samples. In this paper,
we promote the idea to the incremental scenario, and propose a Sample-to-Class
(S2C) graph learning method for FSCIL.
<br />Specifically, we propose a Sample-level Graph Network (SGN) that focuses on
analyzing sample relationships within a single session. This network helps
aggregate similar samples, ultimately leading to the extraction of more refined
class-level features.
<br />Then, we present a Class-level Graph Network (CGN) that establishes
connections across class-level features of both new and old classes. This
network plays a crucial role in linking the knowledge between different
sessions and helps improve overall learning in the FSCIL scenario. Moreover, we
design a multi-stage strategy for training S2C model, which mitigates the
training challenges posed by limited data in the incremental process.
<br />The multi-stage training strategy is designed to build S2C graph from base to
few-shot stages, and improve the capacity via an extra pseudo-incremental
stage. Experiments on three popular benchmark datasets show that our method
clearly outperforms the baselines and sets new state-of-the-art results in
FSCIL.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20271" title="Abstract">arXiv:2310.20271</a> [<a href="/pdf/2310.20271" title="Download PDF">pdf</a>, <a href="/format/2310.20271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Denoising Training to Test-Time Adaptation: Enhancing Domain  Generalization for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+R">Ruxue Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hangjie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+D">Dong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+W">Wenbo Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yaoyao Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In medical image segmentation, domain generalization poses a significant
challenge due to domain shifts caused by variations in data acquisition devices
and other factors. These shifts are particularly pronounced in the most common
scenario, which involves only single-source domain data due to privacy
concerns. To address this, we draw inspiration from the self-supervised
learning paradigm that effectively discourages overfitting to the source
domain. We propose the Denoising Y-Net (DeY-Net), a novel approach
incorporating an auxiliary denoising decoder into the basic U-Net architecture.
The auxiliary decoder aims to perform denoising training, augmenting the
domain-invariant representation that facilitates domain generalization.
Furthermore, this paradigm provides the potential to utilize unlabeled data.
Building upon denoising training, we propose Denoising Test Time Adaptation
(DeTTA) that further: (i) adapts the model to the target domain in a
sample-wise manner, and (ii) adapts to the noise-corrupted input. Extensive
experiments conducted on widely-adopted liver segmentation benchmarks
demonstrate significant domain generalization improvements over our baseline
and state-of-the-art results compared to other methods. Code is available at
https://github.com/WenRuxue/DeTTA.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20274" title="Abstract">arXiv:2310.20274</a> [<a href="/pdf/2310.20274" title="Download PDF">pdf</a>, <a href="/format/2310.20274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extracting Entities of Interest from Comparative Product Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+J">Jatin Arora</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Sumit Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+P">Pawan Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+S">Sayan Pathak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source Code: <a href="https://github.com/jatinarora2702/Review-Information-Extraction">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2017 ACM on Conference on Information and
  Knowledge Management, Pages 1975 - 1978
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a deep learning based approach to extract product
comparison information out of user reviews on various e-commerce websites. Any
comparative product review has three major entities of information: the names
of the products being compared, the user opinion (predicate) and the feature or
aspect under comparison. All these informing entities are dependent on each
other and bound by the rules of the language, in the review. We observe that
their inter-dependencies can be captured well using LSTMs. We evaluate our
system on existing manually labeled datasets and observe out-performance over
the existing Semantic Role Labeling (SRL) framework popular for this task.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20275" title="Abstract">arXiv:2310.20275</a> [<a href="/pdf/2310.20275" title="Download PDF">pdf</a>, <a href="/format/2310.20275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age Optimum Sampling in Non-Stationary Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Haoyue Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kompella%2C+S">Sastry Kompella</a>, 
<a href="/search/cs?searchtype=author&query=Tassiulas%2C+L">Leandros Tassiulas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this work, we consider a status update system with a sensor and a
receiver. The status update information is sampled by the sensor and then
forwarded to the receiver through a channel with non-stationary delay
distribution. The data freshness at the receiver is quantified by the
Age-of-Information (AoI). The goal is to design an online sampling strategy
that can minimize the average AoI when the non-stationary delay distribution is
unknown. Assuming that channel delay distribution may change over time, to
minimize the average AoI, we propose a joint stochastic approximation and
non-parametric change point detection algorithm that can: (1) learn the optimum
update threshold when the delay distribution remains static; (2) detect the
change in transmission delay distribution quickly and then restart the learning
process. Simulation results show that the proposed algorithm can quickly detect
the delay changes, and the average AoI obtained by the proposed policy
converges to the minimum AoI.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20277" title="Abstract">arXiv:2310.20277</a> [<a href="/pdf/2310.20277" title="Download PDF">pdf</a>, <a href="/format/2310.20277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Structural Equation Model of Open Source Blockchain Software  Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nijsse%2C+J">Jeff Nijsse</a>, 
<a href="/search/cs?searchtype=author&query=Litchfield%2C+A">Alan Litchfield</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The widespread use of GitHub among software developers as a communal platform
for coordinating software development has led to an abundant supply of publicly
accessible data. Ever since the inception of Bitcoin, blockchain teams have
incorporated the concept of open source code as a fundamental principle, thus
making the majority of blockchain-based projects' code and version control data
available for analysis. We define health in open source software projects to be
a combination of the concepts of sustainability, robustness, and niche
occupation. Sustainability is further divided into interest and engagement.
This work uses exploratory factor analysis to identify latent constructs that
are representative of general public interest or popularity in software, and
software robustness within open source blockchain projects. We find that
interest is a combination of stars, forks, and text mentions in the GitHub
repository, while a second factor for robustness is composed of a criticality
score, time since last updated, numerical rank, and geographic distribution.
Cross validation of the dataset is carried out with good support for the model.
A structural model of software health is proposed such that general interest
positively influences developer engagement, which, in turn, positively predicts
software robustness. The implications of structural equation modelling in the
context of software engineering and next steps are discussed.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20279" title="Abstract">arXiv:2310.20279</a> [<a href="/pdf/2310.20279" title="Download PDF">pdf</a>, <a href="/format/2310.20279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine learning refinement of in situ images acquired by low electron  dose LC-TEM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katsuno%2C+H">Hiroyasu Katsuno</a>, 
<a href="/search/cs?searchtype=author&query=Kimura%2C+Y">Yuki Kimura</a>, 
<a href="/search/cs?searchtype=author&query=Yamazaki%2C+T">Tomoya Yamazaki</a>, 
<a href="/search/cs?searchtype=author&query=Takigawa%2C+I">Ichigaku Takigawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We study a machine learning (ML) technique for refining images acquired
during in situ observation using liquid-cell transmission electron microscopy
(LC-TEM). Our model is constructed using a U-Net architecture and a ResNet
encoder. For training our ML model, we prepared an original image dataset that
contained pairs of images of samples acquired with and without a solution
present. The former images were used as noisy images and the latter images were
used as corresponding ground truth images. The number of pairs of image sets
was $1,204$ and the image sets included images acquired at several different
magnifications and electron doses. The trained model converted a noisy image
into a clear image. The time necessary for the conversion was on the order of
10ms, and we applied the model to in situ observations using the software Gatan
DigitalMicrograph (DM). Even if a nanoparticle was not visible in a view window
in the DM software because of the low electron dose, it was visible in a
successive refined image generated by our ML model.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20280" title="Abstract">arXiv:2310.20280</a> [<a href="/pdf/2310.20280" title="Download PDF">pdf</a>, <a href="/format/2310.20280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palaskar%2C+S">Santosh Palaskar</a>, 
<a href="/search/cs?searchtype=author&query=Ekambaram%2C+V">Vijay Ekambaram</a>, 
<a href="/search/cs?searchtype=author&query=Jati%2C+A">Arindam Jati</a>, 
<a href="/search/cs?searchtype=author&query=Gantayat%2C+N">Neelamadhav Gantayat</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Avirup Saha</a>, 
<a href="/search/cs?searchtype=author&query=Nagar%2C+S">Seema Nagar</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+N+H">Nam H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Dayama%2C+P">Pankaj Dayama</a>, 
<a href="/search/cs?searchtype=author&query=Sindhgatta%2C+R">Renuka Sindhgatta</a>, 
<a href="/search/cs?searchtype=author&query=Mohapatra%2C+P">Prateeti Mohapatra</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+H">Harshit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Kalagnanam%2C+J">Jayant Kalagnanam</a>, 
<a href="/search/cs?searchtype=author&query=Hemachandra%2C+N">Nandyala Hemachandra</a>, 
<a href="/search/cs?searchtype=author&query=Rangaraj%2C+N">Narayan Rangaraj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the Thirty-Sixth Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The efficiency of business processes relies on business key performance
indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps
data fuses both Biz-KPIs and IT event channels together as multivariate time
series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue
through proactive corrective measures. However, BizITOps data generally exhibit
both useful and noisy inter-channel interactions between Biz-KPIs and IT events
that need to be effectively decoupled. This leads to suboptimal forecasting
performance when existing multivariate forecasting models are employed. To
address this, we introduce AutoMixer, a time-series Foundation Model (FM)
approach, grounded on the novel technique of channel-compressed pretrain and
finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed
pretraining and integrates it with the advanced TSMixer model for multivariate
time series forecasting. This fusion greatly enhances the potency of TSMixer
for accurate forecasts and also generalizes well across several downstream
tasks. Through detailed experiments and dashboard analytics, we show
AutoMixer's capability to consistently improve the Biz-KPI's forecasting
accuracy (by 11-15%) which directly translates to actionable business insights.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20285" title="Abstract">arXiv:2310.20285</a> [<a href="/pdf/2310.20285" title="Download PDF">pdf</a>, <a href="/format/2310.20285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Generalized Linear Models by Trading off Computation for  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tatzel%2C+L">Lukas Tatzel</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+J">Jonathan Wenger</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+F">Frank Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main text: 10 pages, 6 figures; Supplements: 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic
framework to model categorical, ordinal and continuous data, and are widely
used in practice. However, exact inference in GLMs is prohibitively expensive
for large datasets, thus requiring approximations in practice. The resulting
approximation error adversely impacts the reliability of the model and is not
accounted for in the uncertainty of the prediction. In this work, we introduce
a family of iterative methods that explicitly model this error. They are
uniquely suited to parallel modern computing hardware, efficiently recycle
computations, and compress information to reduce both the time and memory
requirements for GLMs. As we demonstrate on a realistically large
classification problem, our method significantly accelerates training by
explicitly trading off reduced computation for increased uncertainty.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20287" title="Abstract">arXiv:2310.20287</a> [<a href="/pdf/2310.20287" title="Download PDF">pdf</a>, <a href="/format/2310.20287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep  Ensemble Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Woojun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+Y">Yongjae Shin</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jongeui Park</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Youngchul Sung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep reinforcement learning (RL) has achieved remarkable success in solving
complex tasks through its integration with deep neural networks (DNNs) as
function approximators. However, the reliance on DNNs has introduced a new
challenge called primacy bias, whereby these function approximators tend to
prioritize early experiences, leading to overfitting. To mitigate this primacy
bias, a reset method has been proposed, which performs periodic resets of a
portion or the entirety of a deep RL agent while preserving the replay buffer.
However, the use of the reset method can result in performance collapses after
executing the reset, which can be detrimental from the perspective of safe RL
and regret minimization. In this paper, we propose a new reset-based method
that leverages deep ensemble learning to address the limitations of the vanilla
reset method and enhance sample efficiency. The proposed method is evaluated
through various experiments including those in the domain of safe RL. Numerical
results show its effectiveness in high sample efficiency and safety
considerations.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20290" title="Abstract">arXiv:2310.20290</a> [<a href="/pdf/2310.20290" title="Download PDF">pdf</a>, <a href="/format/2310.20290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Rayleigh Quotient Iteration for Dual Quaternion Hermitian Eigenvalue  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Duan%2C+S">Shan-Qi Duan</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Q">Qing-Wen Wang</a>, 
<a href="/search/math?searchtype=author&query=Duan%2C+X">Xue-Feng Duan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The application of eigenvalue theory to dual quaternion Hermitian matrix
holds significance in the realm of multi-agent formation control. In this
paper, we focus on the numerical algorithm for the right eigenvalue of a dual
quaternion Hermitian matrix. Rayleigh quotient iteration is proposed for
computing the extreme eigenvalue with the associated eigenvector of the dual
quaternion Hermitian matrix. We also derive an analysis of the convergence
characteristics of the Rayleigh quotient iteration, which exhibits a local
convergence rate of cubic. Numerical examples are provided to illustrate the
efficiency of the proposed Rayleigh quotient iteration for the dual quaternion
Hermitian eigenvalue problem.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20293" title="Abstract">arXiv:2310.20293</a> [<a href="/pdf/2310.20293" title="Download PDF">pdf</a>, <a href="/format/2310.20293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Annotator: A Generic Active Learning Baseline for LiDAR Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+B">Binhui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qingju Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C+H">Chi Harold Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xinjing Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Project page at <a href="https://binhuixie.github.io/annotator-web/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Active learning, a label-efficient paradigm, empowers models to interactively
query an oracle for labeling new data. In the realm of LiDAR semantic
segmentation, the challenges stem from the sheer volume of point clouds,
rendering annotation labor-intensive and cost-prohibitive. This paper presents
Annotator, a general and efficient active learning baseline, in which a
voxel-centric online selection strategy is tailored to efficiently probe and
annotate the salient and exemplar voxel girds within each LiDAR scan, even
under distribution shift. Concretely, we first execute an in-depth analysis of
several common selection strategies such as Random, Entropy, Margin, and then
develop voxel confusion degree (VCD) to exploit the local topology relations
and structures of point clouds. Annotator excels in diverse settings, with a
particular focus on active learning (AL), active source-free domain adaptation
(ASFDA), and active domain adaptation (ADA). It consistently delivers
exceptional performance across LiDAR semantic segmentation benchmarks, spanning
both simulation-to-real and real-to-real scenarios. Surprisingly, Annotator
exhibits remarkable efficiency, requiring significantly fewer annotations,
e.g., just labeling five voxels per scan in the SynLiDAR-to-SemanticKITTI task.
This results in impressive performance, achieving 87.8% fully-supervised
performance under AL, 88.5% under ASFDA, and 94.4% under ADA. We envision that
Annotator will offer a simple, general, and efficient solution for
label-efficient 3D applications. Project page:
https://binhuixie.github.io/annotator-web
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20299" title="Abstract">arXiv:2310.20299</a> [<a href="/pdf/2310.20299" title="Download PDF">pdf</a>, <a href="/format/2310.20299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verification of Neural Networks Local Differential Classification  Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reshef%2C+R">Roie Reshef</a>, 
<a href="/search/cs?searchtype=author&query=Kabaha%2C+A">Anan Kabaha</a>, 
<a href="/search/cs?searchtype=author&query=Seleznova%2C+O">Olga Seleznova</a>, 
<a href="/search/cs?searchtype=author&query=Drachsler-Cohen%2C+D">Dana Drachsler-Cohen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Neural networks are susceptible to privacy attacks. To date, no verifier can
reason about the privacy of individuals participating in the training set. We
propose a new privacy property, called local differential classification
privacy (LDCP), extending local robustness to a differential privacy setting
suitable for black-box classifiers. Given a neighborhood of inputs, a
classifier is LDCP if it classifies all inputs the same regardless of whether
it is trained with the full dataset or whether any single entry is omitted. A
naive algorithm is highly impractical because it involves training a very large
number of networks and verifying local robustness of the given neighborhood
separately for every network. We propose Sphynx, an algorithm that computes an
abstraction of all networks, with a high probability, from a small set of
networks, and verifies LDCP directly on the abstract network. The challenge is
twofold: network parameters do not adhere to a known distribution probability,
making it difficult to predict an abstraction, and predicting too large
abstraction harms the verification. Our key idea is to transform the parameters
into a distribution given by KDE, allowing to keep the over-approximation error
small. To verify LDCP, we extend a MILP verifier to analyze an abstract
network. Experimental results show that by training only 7% of the networks,
Sphynx predicts an abstract network obtaining 93% verification accuracy and
reducing the analysis time by $1.7\cdot10^4$x.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20301" title="Abstract">arXiv:2310.20301</a> [<a href="/pdf/2310.20301" title="Download PDF">pdf</a>, <a href="/format/2310.20301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revolutionizing Global Food Security: Empowering Resilience through  Integrated AI Foundation Models and Data-Driven Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shoaib%2C+M+R">Mohamed R. Shoaib</a>, 
<a href="/search/cs?searchtype=author&query=Emara%2C+H+M">Heba M. Emara</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Food security, a global concern, necessitates precise and diverse data-driven
solutions to address its multifaceted challenges. This paper explores the
integration of AI foundation models across various food security applications,
leveraging distinct data types, to overcome the limitations of current deep and
machine learning methods. Specifically, we investigate their utilization in
crop type mapping, cropland mapping, field delineation and crop yield
prediction. By capitalizing on multispectral imagery, meteorological data, soil
properties, historical records, and high-resolution satellite imagery, AI
foundation models offer a versatile approach. The study demonstrates that AI
foundation models enhance food security initiatives by providing accurate
predictions, improving resource allocation, and supporting informed
decision-making. These models serve as a transformative force in addressing
global food security limitations, marking a significant leap toward a
sustainable and secure food future.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20305" title="Abstract">arXiv:2310.20305</a> [<a href="/pdf/2310.20305" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bilateral Network with Residual U-blocks and Dual-Guided Attention for  Real-time Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+L">Liang Liao</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+L">Liang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingsheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shusheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">When some application scenarios need to use semantic segmentation technology,
like automatic driving, the primary concern comes to real-time performance
rather than extremely high segmentation accuracy. To achieve a good trade-off
between speed and accuracy, two-branch architecture has been proposed in recent
years. It treats spatial information and semantics information separately which
allows the model to be composed of two networks both not heavy. However, the
process of fusing features with two different scales becomes a performance
bottleneck for many nowaday two-branch models. In this research, we design a
new fusion mechanism for two-branch architecture which is guided by attention
computation. To be precise, we use the Dual-Guided Attention (DGA) module we
proposed to replace some multi-scale transformations with the calculation of
attention which means we only use several attention layers of near linear
complexity to achieve performance comparable to frequently-used multi-layer
fusion. To ensure that our module can be effective, we use Residual U-blocks
(RSU) to build one of the two branches in our networks which aims to obtain
better multi-scale features. Extensive experiments on Cityscapes and CamVid
dataset show the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20307" title="Abstract">arXiv:2310.20307</a> [<a href="/pdf/2310.20307" title="Download PDF">pdf</a>, <a href="/format/2310.20307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Interpretation of Self-Attention in Pre-Trained Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rohekar%2C+R+Y">Raanan Y. Rohekar</a>, 
<a href="/search/cs?searchtype=author&query=Gurwicz%2C+Y">Yaniv Gurwicz</a>, 
<a href="/search/cs?searchtype=author&query=Nisimov%2C+S">Shami Nisimov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv admin note: text overlap with <a href="/abs/2210.10621">arXiv:2210.10621</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a causal interpretation of self-attention in the Transformer
neural network architecture. We interpret self-attention as a mechanism that
estimates a structural equation model for a given input sequence of symbols
(tokens). The structural equation model can be interpreted, in turn, as a
causal structure over the input symbols under the specific context of the input
sequence. Importantly, this interpretation remains valid in the presence of
latent confounders. Following this interpretation, we estimate conditional
independence relations between input symbols by calculating partial
correlations between their corresponding representations in the deepest
attention layer. This enables learning the causal structure over an input
sequence using existing constraint-based algorithms. In this sense, existing
pre-trained Transformers can be utilized for zero-shot causal-discovery. We
demonstrate this method by providing causal explanations for the outcomes of
Transformers in two tasks: sentiment classification (NLP) and recommendation.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20308" title="Abstract">arXiv:2310.20308</a> [<a href="/pdf/2310.20308" title="Download PDF">pdf</a>, <a href="/format/2310.20308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A physics-informed GAN Framework based on Model-free Data-Driven  Computational Mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ciftci%2C+K">Kerem Ciftci</a>, 
<a href="/search/cs?searchtype=author&query=Hackl%2C+K">Klaus Hackl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Model-free data-driven computational mechanics, first proposed by
Kirchdoerfer and Ortiz, replace phenomenological models with numerical
simulations based on sample data sets in strain-stress space. In this study, we
integrate this paradigm within physics-informed generative adversarial networks
(GANs). We enhance the conventional physics-informed neural network framework
by implementing the principles of data-driven computational mechanics into
GANs. Specifically, the generator is informed by physical constraints, while
the discriminator utilizes the closest strain-stress data to discern the
authenticity of the generator's output. This combined approach presents a new
formalism to harness data-driven mechanics and deep learning to simulate and
predict mechanical behaviors.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20310" title="Abstract">arXiv:2310.20310</a> [<a href="/pdf/2310.20310" title="Download PDF">pdf</a>, <a href="/format/2310.20310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Conserving Higher Order Mixed Finite Element Discretizations of  Maxwell&#x27;s Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Arya%2C+A">Archana Arya</a>, 
<a href="/search/math?searchtype=author&query=Kalyanaraman%2C+K">Kaushik Kalyanaraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We study a system of Maxwell's equations that describes the time evolution of
electromagnetic fields with an additional electric scalar variable to make the
system amenable to a mixed finite element spatial discretization. We
demonstrate stability and energy conservation for the variational formulation
of this Maxwell's system. We then discuss two implicit, energy conserving
schemes for its temporal discretization: the classical Crank-Nicholson scheme
and an implicit leapfrog scheme. We next show discrete stability and discrete
energy conservation for the semi-discretization using these two time
integration methods. We complete our discussion by showing that the error for
the full discretization of the Maxwell's system with each of the two implicit
time discretization schemes and with spatial discretization through a
conforming sequence of de Rham finite element spaces converges quadratically in
the step size of the time discretization and as an appropriate polynomial power
of the mesh parameter in accordance with the choice of approximating polynomial
spaces. Our results for the Crank-Nicholson method are generally well known but
have not been demonstrated for this Maxwell's system. Our implicit leapfrog
scheme is a new method to the best of our knowledge and we provide a complete
error analysis for it. Finally, we show computational results to validate our
theoretical claims using linear and quadratic Whitney forms for the finite
element discretization for some model problems in two and three spatial
dimensions.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20316" title="Abstract">arXiv:2310.20316</a> [<a href="/pdf/2310.20316" title="Download PDF">pdf</a>, <a href="/format/2310.20316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HWD: A Novel Evaluation Score for Styled Handwritten Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pippi%2C+V">Vittorio Pippi</a>, 
<a href="/search/cs?searchtype=author&query=Quattrini%2C+F">Fabio Quattrini</a>, 
<a href="/search/cs?searchtype=author&query=Cascianelli%2C+S">Silvia Cascianelli</a>, 
<a href="/search/cs?searchtype=author&query=Cucchiara%2C+R">Rita Cucchiara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at BMVC2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Digital Libraries (cs.DL)

</div>
<p class="mathjax">Styled Handwritten Text Generation (Styled HTG) is an important task in
document analysis, aiming to generate text images with the handwriting of given
reference images. In recent years, there has been significant progress in the
development of deep learning models for tackling this task. Being able to
measure the performance of HTG models via a meaningful and representative
criterion is key for fostering the development of this research topic. However,
despite the current adoption of scores for natural image generation evaluation,
assessing the quality of generated handwriting remains challenging. In light of
this, we devise the Handwriting Distance (HWD), tailored for HTG evaluation. In
particular, it works in the feature space of a network specifically trained to
extract handwriting style features from the variable-lenght input images and
exploits a perceptual distance to compare the subtle geometric features of
handwriting. Through extensive experimental evaluation on different word-level
and line-level datasets of handwritten text images, we demonstrate the
suitability of the proposed HWD as a score for Styled HTG. The pretrained model
used as backbone will be released to ease the adoption of the score, aiming to
provide a valuable tool for evaluating HTG models and thus contributing to
advancing this important research area.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20319" title="Abstract">arXiv:2310.20319</a> [<a href="/pdf/2310.20319" title="Download PDF">pdf</a>, <a href="/format/2310.20319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object  Detectors on LiDAR-Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schinagl%2C+D">David Schinagl</a>, 
<a href="/search/cs?searchtype=author&query=Krispel%2C+G">Georg Krispel</a>, 
<a href="/search/cs?searchtype=author&query=Fruhwirth-Reisinger%2C+C">Christian Fruhwirth-Reisinger</a>, 
<a href="/search/cs?searchtype=author&query=Possegger%2C+H">Horst Possegger</a>, 
<a href="/search/cs?searchtype=author&query=Bischof%2C+H">Horst Bischof</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023, code is available at <a href="https://github.com/dschinagl/gace">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Widely-used LiDAR-based 3D object detectors often neglect fundamental
geometric information readily available from the object proposals in their
confidence estimation. This is mostly due to architectural design choices,
which were often adopted from the 2D image domain, where geometric context is
rarely available. In 3D, however, considering the object properties and its
surroundings in a holistic way is important to distinguish between true and
false positive detections, e.g. occluded pedestrians in a group. To address
this, we present GACE, an intuitive and highly efficient method to improve the
confidence estimation of a given black-box 3D object detector. We aggregate
geometric cues of detections and their spatial relationships, which enables us
to properly assess their plausibility and consequently, improve the confidence
estimation. This leads to consistent performance gains over a variety of
state-of-the-art detectors. Across all evaluated detectors, GACE proves to be
especially beneficial for the vulnerable road user classes, i.e. pedestrians
and cyclists.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20320" title="Abstract">arXiv:2310.20320</a> [<a href="/pdf/2310.20320" title="Download PDF">pdf</a>, <a href="/format/2310.20320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theory of Mind in Large Language Models: Examining Performance of 11  State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Duijn%2C+M+J">Max J. van Duijn</a>, 
<a href="/search/cs?searchtype=author&query=van+Dijk%2C+B+M+A">Bram M.A. van Dijk</a>, 
<a href="/search/cs?searchtype=author&query=Kouwenhoven%2C+T">Tom Kouwenhoven</a>, 
<a href="/search/cs?searchtype=author&query=de+Valk%2C+W">Werner de Valk</a>, 
<a href="/search/cs?searchtype=author&query=Spruit%2C+M+R">Marco R. Spruit</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Putten%2C+P">Peter van der Putten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, Forthcoming in Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To what degree should we ascribe cognitive capacities to Large Language
Models (LLMs), such as the ability to reason about intentions and beliefs known
as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11
base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the
dominant false-belief paradigm, including non-literal language usage and
recursive intentionality; (ii) using newly rewritten versions of standardized
tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides
closed questions; and (iv) benchmarking LLM performance against that of
children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from
the GPT family outperform other models, and often also children. Base-LLMs are
mostly unable to solve ToM tasks, even with specialized prompting. We suggest
that the interlinked evolution and development of language and ToM may help
explain what instruction-tuning adds: rewarding cooperative communication that
takes into account interlocutor and context. We conclude by arguing for a
nuanced perspective on ToM in LLMs.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20322" title="Abstract">arXiv:2310.20322</a> [<a href="/pdf/2310.20322" title="Download PDF">pdf</a>, <a href="/format/2310.20322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FA Team at the NTCIR-17 UFO Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okumura%2C+Y">Yuki Okumura</a>, 
<a href="/search/cs?searchtype=author&query=Fujitake%2C+M">Masato Fujitake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be appeared at the NTCIR-17 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">The FA team participated in the Table Data Extraction (TDE) and Text-to-Table
Relationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of
Non-Financial Objects in Financial Reports (UFO). This paper reports our
approach to solving the problems and discusses the official results. We
successfully utilized various enhancement techniques based on the ELECTRA
language model to extract valuable data from tables. Our efforts resulted in an
impressive TDE accuracy rate of 93.43 %, positioning us in second place on the
Leaderboard rankings. This outstanding achievement is a testament to our
proposed approach's effectiveness. In the TTRE task, we proposed the rule-based
method to extract meaningful relationships between the text and tables task and
confirmed the performance.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20323" title="Abstract">arXiv:2310.20323</a> [<a href="/pdf/2310.20323" title="Download PDF">pdf</a>, <a href="/format/2310.20323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemanticBoost: Elevating Motion Generation with Augmented Textual Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xin He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaoli Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+X">Xiaohang Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Chao Wen</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Current techniques face difficulties in generating motions from intricate
semantic descriptions, primarily due to insufficient semantic annotations in
datasets and weak contextual understanding. To address these issues, we present
SemanticBoost, a novel framework that tackles both challenges simultaneously.
Our framework comprises a Semantic Enhancement module and a Context-Attuned
Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary
semantics from motion data, enriching the dataset's textual description and
ensuring precise alignment between text and motion data without depending on
large language models. On the other hand, the CAMD approach provides an
all-encompassing solution for generating high-quality, semantically consistent
motion sequences by effectively capturing context information and aligning the
generated motion with the given textual descriptions. Distinct from existing
methods, our approach can synthesize accurate orientational movements, combined
motions based on specific body part descriptions, and motions generated from
complex, extended sentences. Our experimental results demonstrate that
SemanticBoost, as a diffusion-based method, outperforms auto-regressive-based
techniques, achieving cutting-edge performance on the Humanml3D dataset while
maintaining realistic and smooth motion generation quality.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20325" title="Abstract">arXiv:2310.20325</a> [<a href="/pdf/2310.20325" title="Download PDF">pdf</a>, <a href="/format/2310.20325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A polynomial-time $\text{OPT}^&#x3b5;$-approximation algorithm for  maximum independent set of connected subgraphs in a planar graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cslovjecsek%2C+J">Jana Cslovjecsek</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Micha&#x142; Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=W%C4%99grzycki%2C+K">Karol W&#x119;grzycki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 2 colored figures, SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In the Maximum Independent Set of Objects problem, we are given an $n$-vertex
planar graph $G$ and a family $\mathcal{D}$ of $N$ objects, where each object
is a connected subgraph of $G$. The task is to find a subfamily $\mathcal{F}
\subseteq \mathcal{D}$ of maximum cardinality that consists of pairwise
disjoint objects. This problem is $\mathsf{NP}$-hard and is equivalent to the
problem of finding the maximum number of pairwise disjoint polygons in a given
family of polygons in the plane.
<br />As shown by Adamaszek et al. (J. ACM '19), the problem admits a
\emph{quasi-polynomial time approximation scheme} (QPTAS): a
$(1-\varepsilon)$-approximation algorithm whose running time is bounded by
$2^{\mathrm{poly}(\log(N),1/\epsilon)} \cdot n^{\mathcal{O}(1)}$. Nevertheless,
to the best of our knowledge, in the polynomial-time regime only the trivial
$\mathcal{O}(N)$-approximation is known for the problem in full generality. In
the restricted setting where the objects are pseudolines in the plane, Fox and
Pach (SODA '11) gave an $N^{\varepsilon}$-approximation algorithm with running
time $N^{2^{\tilde{\mathcal{O}}(1/\varepsilon)}}$, for any $\varepsilon&gt;0$.
<br />In this work, we present an $\text{OPT}^{\varepsilon}$-approximation
algorithm for the problem that runs in time
$N^{\tilde{\mathcal{O}}(1/\varepsilon^2)} n^{\mathcal{O}(1)}$, for any
$\varepsilon&gt;0$, thus improving upon the result of Fox and Pach both in terms
of generality and in terms of the running time. Our approach combines the
methodology of Voronoi separators, introduced by Marx and Pilipczuk (TALG '22),
with a new analysis of the approximation factor.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20326" title="Abstract">arXiv:2310.20326</a> [<a href="/pdf/2310.20326" title="Download PDF">pdf</a>, <a href="/format/2310.20326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Erato: Automatizing Poetry Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agirrezabal%2C+M">Manex Agirrezabal</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+H+G">Hugo Gon&#xe7;alo Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Ormazabal%2C+A">Aitor Ormazabal</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 22nd Portuguese Conference on Artificial Intelligence (EPIA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present Erato, a framework designed to facilitate the automated evaluation
of poetry, including that generated by poetry generation systems. Our framework
employs a diverse set of features, and we offer a brief overview of Erato's
capabilities and its potential for expansion. Using Erato, we compare and
contrast human-authored poetry with automatically-generated poetry,
demonstrating its effectiveness in identifying key differences. Our
implementation code and software are freely available under the GNU GPLv3
license.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20327" title="Abstract">arXiv:2310.20327</a> [<a href="/pdf/2310.20327" title="Download PDF">pdf</a>, <a href="/format/2310.20327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Entropy-Based Test-Time Adaptation from a Clustering View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+G">Guoliang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+H">Hanjiang Lai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jian Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Domain shift is a common problem in the realistic world, where training data
and test data follow different data distributions. To deal with this problem,
fully test-time adaptation (TTA) leverages the unlabeled data encountered
during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA)
methods, which minimize the prediction's entropy on test samples, have shown
great success. In this paper, we introduce a new perspective on the EBTTA,
which interprets these methods from a view of clustering. It is an iterative
algorithm: 1) in the assignment step, the forward process of the EBTTA models
is the assignment of labels for these test samples, and 2) in the updating
step, the backward process is the update of the model via the assigned samples.
Based on the interpretation, we can gain a deeper understanding of EBTTA, where
we show that the entropy loss would further increase the largest probability.
Accordingly, we offer an alternative explanation that why existing EBTTA
methods are sensitive to initial assignments, outliers, and batch size. This
observation can guide us to put forward the improvement of EBTTA. We propose
robust label assignment, weight adjustment, and gradient accumulation to
alleviate the above problems. Experimental results demonstrate that our method
can achieve consistent improvements on various datasets. Code is provided in
the supplementary material.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20328" title="Abstract">arXiv:2310.20328</a> [<a href="/pdf/2310.20328" title="Download PDF">pdf</a>, <a href="/format/2310.20328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for  Computational Linguistics and Cognitive Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Dijk%2C+B+M+A">Bram M.A. van Dijk</a>, 
<a href="/search/cs?searchtype=author&query=van+Duijn%2C+M+J">Max J. van Duijn</a>, 
<a href="/search/cs?searchtype=author&query=Verberne%2C+S">Suzan Verberne</a>, 
<a href="/search/cs?searchtype=author&query=Spruit%2C+M+R">Marco R. Spruit</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, forthcoming in Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this resource paper we release ChiSCor, a new corpus containing 619
fantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was
compiled for studying how children render character perspectives, and
unravelling language and cognition in development, with computational tools.
Unlike existing resources, ChiSCor's stories were produced in natural contexts,
in line with recent calls for more ecologically valid datasets. ChiSCor hosts
text, audio, and annotations for character complexity and linguistic
complexity. Additional metadata (e.g. education of caregivers) is available for
one third of the Dutch children. ChiSCor also includes a small set of 62
English stories. This paper details how ChiSCor was compiled and shows its
potential for future work with three brief case studies: i) we show that the
syntactic complexity of stories is strikingly stable across children's ages;
ii) we extend work on Zipfian distributions in free speech and show that
ChiSCor obeys Zipf's law closely, reflecting its social context; iii) we show
that even though ChiSCor is relatively small, the corpus is rich enough to
train informative lemma vectors that allow us to analyse children's language
use. We end with a reflection on the value of narrative datasets in
computational linguistics.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20329" title="Abstract">arXiv:2310.20329</a> [<a href="/pdf/2310.20329" title="Download PDF">pdf</a>, <a href="/format/2310.20329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructCoder: Empowering Language Models for Code Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qisheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kaixin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tiedong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qizhe Xie</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junxian He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Code editing encompasses a variety of pragmatic tasks that developers deal
with daily. Despite its relevance and practical usefulness, automatic code
editing remains an underexplored area in the evolution of deep learning models,
partly due to data scarcity. In this work, we explore the use of large language
models (LLMs) to edit code based on user instructions, covering a broad range
of implicit tasks such as comment insertion, code optimization, and code
refactoring. To facilitate this, we introduce InstructCoder, the first dataset
designed to adapt LLMs for general-purpose code editing, containing
highdiversity code-editing tasks. It consists of over 114,000
instruction-input-output triplets and covers multiple distinct code editing
scenarios. The dataset is systematically expanded through an iterative process
that commences with code editing data sourced from GitHub commits as seed
tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for
more task data. Our experiments demonstrate that open-source LLMs fine-tuned on
InstructCoder can edit code correctly based on users' instructions most of the
time, exhibiting unprecedented code-editing performance levels. Such results
suggest that proficient instruction-finetuning can lead to significant
amelioration in code editing abilities. The dataset and the source code are
available at https://github.com/qishenghu/CodeInstruct.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20331" title="Abstract">arXiv:2310.20331</a> [<a href="/pdf/2310.20331" title="Download PDF">pdf</a>, <a href="/format/2310.20331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Aware Adaptive Sampling for Self-Sustainability in  Resource-Constrained IoT Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Giordano%2C+M">Marco Giordano</a>, 
<a href="/search/eess?searchtype=author&query=Cortesi%2C+S">Silvano Cortesi</a>, 
<a href="/search/eess?searchtype=author&query=Mekikis%2C+P">Prodromos-Vasileios Mekikis</a>, 
<a href="/search/eess?searchtype=author&query=Crabolu%2C+M">Michele Crabolu</a>, 
<a href="/search/eess?searchtype=author&query=Bellusci%2C+G">Giovanni Bellusci</a>, 
<a href="/search/eess?searchtype=author&query=Magno%2C+M">Michele Magno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article has been accepted for publication in the Proceedings of the 11th International Workshop on Energy Harvesting and Energy-Neutral Sensing Systems (ENSSys '23). DOI: <a href="https://doi.org/10.1145/3628353.3628545">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In the ever-growing Internet of Things (IoT) landscape, smart power
management algorithms combined with energy harvesting solutions are crucial to
obtain self-sustainability. This paper presents an energy-aware adaptive
sampling rate algorithm designed for embedded deployment in
resource-constrained, battery-powered IoT devices. The algorithm, based on a
finite state machine (FSM) and inspired by Transmission Control Protocol (TCP)
Reno's additive increase and multiplicative decrease, maximizes sensor sampling
rates, ensuring power self-sustainability without risking battery depletion.
Moreover, we characterized our solar cell with data acquired over 48 days and
used the model created to obtain energy data from an open-source world-wide
dataset. To validate our approach, we introduce the EcoTrack device, a
versatile device with global navigation satellite system (GNSS) capabilities
and Long-Term Evolution Machine Type Communication (LTE-M) connectivity,
supporting MQTT protocol for cloud data relay. This multi-purpose device can be
used, for instance, as a health and safety wearable, remote hazard monitoring
system, or as a global asset tracker. The results, validated on data from three
different European cities, show that the proposed algorithm enables
self-sustainability while maximizing sampled locations per day. In experiments
conducted with a 3000 mAh battery capacity, the algorithm consistently
maintained a minimum of 24 localizations per day and achieved peaks of up to
3000.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20332" title="Abstract">arXiv:2310.20332</a> [<a href="/pdf/2310.20332" title="Download PDF">pdf</a>, <a href="/format/2310.20332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recaptured Raw Screen Image and Video Demoir&#xe9;ing via Channel and  Spatial Modulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+H">Huanjing Yue</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yijia Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingyu Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Capturing screen contents by smartphone cameras has become a common way for
information sharing. However, these images and videos are often degraded by
moir\'e patterns, which are caused by frequency aliasing between the camera
filter array and digital display grids. We observe that the moir\'e patterns in
raw domain is simpler than those in sRGB domain, and the moir\'e patterns in
raw color channels have different properties. Therefore, we propose an image
and video demoir\'eing network tailored for raw inputs. We introduce a
color-separated feature branch, and it is fused with the traditional
feature-mixed branch via channel and spatial modulations. Specifically, the
channel modulation utilizes modulated color-separated features to enhance the
color-mixed features. The spatial modulation utilizes the feature with large
receptive field to modulate the feature with small receptive field. In
addition, we build the first well-aligned raw video demoir\'eing
(RawVDemoir\'e) dataset and propose an efficient temporal alignment method by
inserting alternating patterns. Experiments demonstrate that our method
achieves state-of-the-art performance for both image and video demori\'eing. We
have released the code and dataset in https://github.com/tju-chengyijia/VD_raw.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20339" title="Abstract">arXiv:2310.20339</a> [<a href="/pdf/2310.20339" title="Download PDF">pdf</a>, <a href="/format/2310.20339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExoRecovery: Push Recovery with a Lower-Limb Exoskeleton based on  Stepping Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Orhan%2C+Z+%C3%96">Zeynep &#xd6;zge Orhan</a>, 
<a href="/search/cs?searchtype=author&query=Shafiee%2C+M">Milad Shafiee</a>, 
<a href="/search/cs?searchtype=author&query=Juillard%2C+V">Vincent Juillard</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+J+C">Joel Coelho Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Ijspeert%2C+A">Auke Ijspeert</a>, 
<a href="/search/cs?searchtype=author&query=Bouri%2C+M">Mohamed Bouri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted for a conference. 8 pages including references, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Balance loss is a significant challenge in lower-limb exoskeleton
applications, as it can lead to potential falls, thereby impacting user safety
and confidence. We introduce a control framework for omnidirectional recovery
step planning by online optimization of step duration and position in response
to external forces. We map the step duration and position to a human-like foot
trajectory, which is then translated into joint trajectories using inverse
kinematics. These trajectories are executed via an impedance controller,
promoting cooperation between the exoskeleton and the user.
<br />Moreover, our framework is based on the concept of the divergent component of
motion, also known as the Extrapolated Center of Mass, which has been
established as a consistent dynamic for describing human movement. This
real-time online optimization framework enhances the adaptability of
exoskeleton users under unforeseen forces thereby improving the overall user
stability and safety. To validate the effectiveness of our approach,
simulations, and experiments were conducted. Our push recovery experiments
employing the exoskeleton in zero-torque mode (without assistance) exhibit an
alignment with the exoskeleton's recovery assistance mode, that shows the
consistency of the control framework with human intention. To the best of our
knowledge, this is the first cooperative push recovery framework for the
lower-limb human exoskeleton that relies on the simultaneous adaptation of
intra-stride parameters in both frontal and sagittal directions. The proposed
control scheme has been validated with human subject experiments.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20340" title="Abstract">arXiv:2310.20340</a> [<a href="/pdf/2310.20340" title="Download PDF">pdf</a>, <a href="/format/2310.20340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Coverage Path Planning with Turn Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krupke%2C+D+M">Dominik Michael Krupke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">Coverage path planning is a fundamental challenge in robotics, with diverse
applications in aerial surveillance, manufacturing, cleaning, inspection,
agriculture, and more. The main objective is to devise a trajectory for an
agent that efficiently covers a given area, while minimizing time or energy
consumption. Existing practical approaches often lack a solid theoretical
foundation, relying on purely heuristic methods, or overly abstracting the
problem to a simple Traveling Salesman Problem in Grid Graphs. Moreover, the
considered cost functions only rarely consider turn cost, prize-collecting
variants for uneven cover demand, or arbitrary geometric regions.
<br />In this paper, we describe an array of systematic methods for handling
arbitrary meshes derived from intricate, polygonal environments. This
adaptation paves the way to compute efficient coverage paths with a robust
theoretical foundation for real-world robotic applications. Through
comprehensive evaluations, we demonstrate that the algorithm also exhibits low
optimality gaps, while efficiently handling complex environments. Furthermore,
we showcase its versatility in handling partial coverage and accommodating
heterogeneous passage costs, offering the flexibility to trade off coverage
quality and time efficiency.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20343" title="Abstract">arXiv:2310.20343</a> [<a href="/pdf/2310.20343" title="Download PDF">pdf</a>, <a href="/format/2310.20343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Multi-modal Encoders for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+Z">Zixuan Yi</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zijun Long</a>, 
<a href="/search/cs?searchtype=author&query=Ounis%2C+I">Iadh Ounis</a>, 
<a href="/search/cs?searchtype=author&query=Macdonald%2C+C">Craig Macdonald</a>, 
<a href="/search/cs?searchtype=author&query=Mccreadie%2C+R">Richard Mccreadie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">In recent years, the rapid growth of online multimedia services, such as
e-commerce platforms, has necessitated the development of personalised
recommendation approaches that can encode diverse content about each item.
Indeed, modern multi-modal recommender systems exploit diverse features
obtained from raw images and item descriptions to enhance the recommendation
performance. However, the existing multi-modal recommenders primarily depend on
the features extracted individually from different media through pre-trained
modality-specific encoders, and exhibit only shallow alignments between
different modalities - limiting these systems' ability to capture the
underlying relationships between the modalities. In this paper, we investigate
the usage of large multi-modal encoders within the specific context of
recommender systems, as these have previously demonstrated state-of-the-art
effectiveness when ranking items across various domains. Specifically, we
tailor two state-of-the-art multi-modal encoders (CLIP and VLMo) for
recommendation tasks using a range of strategies, including the exploration of
pre-trained and fine-tuned encoders, as well as the assessment of the
end-to-end training of these encoders. We demonstrate that pre-trained large
multi-modal encoders can generate more aligned and effective user/item
representations compared to existing modality-specific encoders across three
multi-modal recommendation datasets. Furthermore, we show that fine-tuning
these large multi-modal encoders with recommendation datasets leads to an
enhanced recommendation performance. In terms of different training paradigms,
our experiments highlight the essential role of the end-to-end training of
large multi-modal encoders in multi-modal recommendation systems.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20344" title="Abstract">arXiv:2310.20344</a> [<a href="/pdf/2310.20344" title="Download PDF">pdf</a>, <a href="/format/2310.20344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Valued Verification of Strategic Ability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Konikowska%2C+B">Beata Konikowska</a>, 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>, 
<a href="/search/cs?searchtype=author&query=Penczek%2C+W">Wojciech Penczek</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Fundam. Informaticae 175 (2020), p. 207--251
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Some multi-agent scenarios call for the possibility of evaluating
specifications in a richer domain of truth values. Examples include runtime
monitoring of a temporal property over a growing prefix of an infinite path,
inconsistency analysis in distributed databases, and verification methods that
use incomplete anytime algorithms, such as bounded model checking. In this
paper, we present multi-valued alternating-time temporal logic (mv-ATL*), an
expressive logic to specify strategic abilities in multi-agent systems. It is
well known that, for branching-time logics, a general method for
model-independent translation from multi-valued to two-valued model checking
exists. We show that the method cannot be directly extended to mv-ATL*. We also
propose two ways of overcoming the problem. Firstly, we identify constraints on
formulas for which the model-independent translation can be suitably adapted.
Secondly, we present a model-dependent reduction that can be applied to all
formulas of mv-ATL*. We show that, in all cases, the complexity of verification
increases only linearly when new truth values are added to the evaluation
domain. We also consider several examples that show possible applications of
mv-ATL* and motivate its use for model checking multi-agent systems.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20347" title="Abstract">arXiv:2310.20347</a> [<a href="/pdf/2310.20347" title="Download PDF">pdf</a>, <a href="/format/2310.20347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Generators for a Family of Matrix Multiplication Routines with  Apache TVM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alaejos%2C+G">Guillermo Alaejos</a>, 
<a href="/search/cs?searchtype=author&query=Castell%C3%B3%2C+A">Adri&#xe1;n Castell&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Jord%C3%A1%2C+P">Pedro Alonso-Jord&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Igual%2C+F+D">Francisco D. Igual</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+H">H&#xe9;ctor Mart&#xed;nez</a>, 
<a href="/search/cs?searchtype=author&query=Quintana-Ort%C3%AD%2C+E+S">Enrique S. Quintana-Ort&#xed;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 22 figures. Submitted to ACM TOMS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We explore the utilization of the Apache TVM open source framework to
automatically generate a family of algorithms that follow the approach taken by
popular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in
order to obtain high-performance blocked formulations of the general matrix
multiplication (GEMM). % In addition, we fully automatize the generation
process, by also leveraging the Apache TVM framework to derive a complete
variety of the processor-specific micro-kernels for GEMM. This is in contrast
with the convention in high performance libraries, which hand-encode a single
micro-kernel per architecture using Assembly code. % In global, the combination
of our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves
portability, maintainability and, globally, streamlines the software life
cycle; 2)~provides high flexibility to easily tailor and optimize the solution
to different data types, processor architectures, and matrix operand shapes,
yielding performance on a par (or even superior for specific matrix shapes)
with that of hand-tuned libraries; and 3)~features a small memory footprint.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20348" title="Abstract">arXiv:2310.20348</a> [<a href="/pdf/2310.20348" title="Download PDF">pdf</a>, <a href="/format/2310.20348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class Incremental Learning with Pre-trained Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xialei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xusheng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haori Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jia-wen Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Bagdanov%2C+A+D">Andrew D. Bagdanov</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the advent of large-scale pre-trained models, interest in adapting and
exploiting them for continual learning scenarios has grown.
<br />In this paper, we propose an approach to exploiting pre-trained
vision-language models (e.g. CLIP) that enables further adaptation instead of
only using zero-shot learning of new tasks. We augment a pre-trained CLIP model
with additional layers after the Image Encoder or before the Text Encoder. We
investigate three different strategies: a Linear Adapter, a Self-attention
Adapter, each operating on the image embedding, and Prompt Tuning which instead
modifies prompts input to the CLIP text encoder. We also propose a method for
parameter retention in the adapter layers that uses a measure of parameter
importance to better maintain stability and plasticity during incremental
learning. Our experiments demonstrate that the simplest solution -- a single
Linear Adapter layer with parameter retention -- produces the best results.
Experiments on several conventional benchmarks consistently show a significant
margin of improvement over the current state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20349" title="Abstract">arXiv:2310.20349</a> [<a href="/pdf/2310.20349" title="Download PDF">pdf</a>, <a href="/format/2310.20349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Low-cost Strategic Monitoring Approach for Scalable and Interpretable  Error Detection in Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geissler%2C+F">Florian Geissler</a>, 
<a href="/search/cs?searchtype=author&query=Qutub%2C+S">Syed Qutub</a>, 
<a href="/search/cs?searchtype=author&query=Paulitsch%2C+M">Michael Paulitsch</a>, 
<a href="/search/cs?searchtype=author&query=Pattabiraman%2C+K">Karthik Pattabiraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present a highly compact run-time monitoring approach for deep computer
vision networks that extracts selected knowledge from only a few (down to
merely two) hidden layers, yet can efficiently detect silent data corruption
originating from both hardware memory and input faults. Building on the insight
that critical faults typically manifest as peak or bulk shifts in the
activation distribution of the affected network layers, we use strategically
placed quantile markers to make accurate estimates about the anomaly of the
current inference as a whole. Importantly, the detector component itself is
kept algorithmically transparent to render the categorization of regular and
abnormal behavior interpretable to a human. Our technique achieves up to ~96%
precision and ~98% recall of detection. Compared to state-of-the-art anomaly
detection techniques, this approach requires minimal compute overhead (as
little as 0.3% with respect to non-supervised inference time) and contributes
to the explainability of the model.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20350" title="Abstract">arXiv:2310.20350</a> [<a href="/pdf/2310.20350" title="Download PDF">pdf</a>, <a href="/format/2310.20350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Shape Completion and Grasp Prediction for Fast and Versatile  Grasping with a Multi-Fingered Hand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Humt%2C+M">Matthias Humt</a>, 
<a href="/search/cs?searchtype=author&query=Winkelbauer%2C+D">Dominik Winkelbauer</a>, 
<a href="/search/cs?searchtype=author&query=Hillenbrand%2C+U">Ulrich Hillenbrand</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4uml%2C+B">Berthold B&#xe4;uml</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, 3 tables, 1 algorithm, 2023 IEEE-RAS International Conference on Humanoid Robots (Humanoids), Project page: <a href="https://dlr-alr.github.io/2023-humanoids-completion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Grasping objects with limited or no prior knowledge about them is a highly
relevant skill in assistive robotics. Still, in this general setting, it has
remained an open problem, especially when it comes to only partial
observability and versatile grasping with multi-fingered hands. We present a
novel, fast, and high fidelity deep learning pipeline consisting of a shape
completion module that is based on a single depth image, and followed by a
grasp predictor that is based on the predicted object shape. The shape
completion network is based on VQDIF and predicts spatial occupancy values at
arbitrary query points. As grasp predictor, we use our two-stage architecture
that first generates hand poses using an autoregressive model and then
regresses finger joint configurations per pose. Critical factors turn out to be
sufficient data realism and augmentation, as well as special attention to
difficult cases during training. Experiments on a physical robot platform
demonstrate successful grasping of a wide range of household objects based on a
depth image from a single viewpoint. The whole pipeline is fast, taking only
about 1 s for completing the object's shape (0.7 s) and generating 1000 grasps
(0.3 s).
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20352" title="Abstract">arXiv:2310.20352</a> [<a href="/pdf/2310.20352" title="Download PDF">pdf</a>, <a href="/format/2310.20352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMERICANO: Argument Generation with Discourse-driven Decomposition and  Agent Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhe Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+H+P">Hou Pong Chan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yu Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Argument generation is a challenging task in natural language processing,
which requires rigorous reasoning and proper content organization. Inspired by
recent chain-of-thought prompting that breaks down a complex task into
intermediate steps, we propose Americano, a novel framework with agent
interaction for argument generation. Our approach decomposes the generation
process into sequential actions grounded on argumentation theory, which first
executes actions sequentially to generate argumentative discourse components,
and then produces a final argument conditioned on the components. To further
mimic the human writing process and improve the left-to-right generation
paradigm of current autoregressive language models, we introduce an argument
refinement module which automatically evaluates and refines argument drafts
based on feedback received. We evaluate our framework on the task of
counterargument generation using a subset of Reddit/CMV dataset. The results
show that our method outperforms both end-to-end and chain-of-thought prompting
methods and can generate more coherent and persuasive arguments with diverse
and rich contents.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20354" title="Abstract">arXiv:2310.20354</a> [<a href="/pdf/2310.20354" title="Download PDF">pdf</a>, <a href="/format/2310.20354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Complexity of Heterogeneous Geometric Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+K+M">Keith Malcolm Smith</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+J+P">Jason P. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Heterogeneity and geometry are key explanatory components underlying the
structure of real-world networks. The relationship between these components and
the statistical complexity of networks is not well understood. We introduce a
parsimonious normalised measure of statistical complexity for networks --
normalised hierarchical complexity. The measure is trivially 0 in regular
graphs and we prove that this measure tends to 0 in Erd\"os-R\'enyi random
graphs in the thermodynamic limit. We go on to demonstrate that greater
complexity arises from the combination of hierarchical and geometric components
to the network structure than either on their own. Further, the levels of
complexity achieved are similar to those found in many real-world networks. We
also find that real world networks establish connections in a way which
increases hierarchical complexity and which our null models and a range of
attachment mechanisms fail to explain. This underlines the non-trivial nature
of statistical complexity in real-world networks and provides foundations for
the comparative analysis of network complexity within and across disciplines.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20355" title="Abstract">arXiv:2310.20355</a> [<a href="/pdf/2310.20355" title="Download PDF">pdf</a>, <a href="/format/2310.20355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Muscle volume quantification: guiding transformers with anatomical  priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piecuch%2C+L">Louise Piecuch</a>, 
<a href="/search/cs?searchtype=author&query=Duque%2C+V+G">Vanessa Gonzales Duque</a>, 
<a href="/search/cs?searchtype=author&query=Sarcher%2C+A">Aur&#xe9;lie Sarcher</a>, 
<a href="/search/cs?searchtype=author&query=Hollville%2C+E">Enzo Hollville</a>, 
<a href="/search/cs?searchtype=author&query=Nordez%2C+A">Antoine Nordez</a>, 
<a href="/search/cs?searchtype=author&query=Rabita%2C+G">Giuseppe Rabita</a>, 
<a href="/search/cs?searchtype=author&query=Guilhem%2C+G">Ga&#xeb;l Guilhem</a>, 
<a href="/search/cs?searchtype=author&query=Mateus%2C+D">Diana Mateus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Muscle volume is a useful quantitative biomarker in sports, but also for the
follow-up of degenerative musculo-skelletal diseases. In addition to volume,
other shape biomarkers can be extracted by segmenting the muscles of interest
from medical images. Manual segmentation is still today the gold standard for
such measurements despite being very time-consuming. We propose a method for
automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance
Images to assist such morphometric analysis. By their nature, the tissue of
different muscles is undistinguishable when observed in MR Images. Thus, muscle
segmentation algorithms cannot rely on appearance but only on contour cues.
However, such contours are hard to detect and their thickness varies across
subjects. To cope with the above challenges, we propose a segmentation approach
based on a hybrid architecture, combining convolutional and visual transformer
blocks. We investigate for the first time the behaviour of such hybrid
architectures in the context of muscle segmentation for shape analysis.
Considering the consistent anatomical muscle configuration, we rely on
transformer blocks to capture the longrange relations between the muscles. To
further exploit the anatomical priors, a second contribution of this work
consists in adding a regularisation loss based on an adjacency matrix of
plausible muscle neighbourhoods estimated from the training data. Our
experimental results on a unique database of elite athletes show it is possible
to train complex hybrid models from a relatively small database of large
volumes, while the anatomical prior regularisation favours better predictions.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20357" title="Abstract">arXiv:2310.20357</a> [<a href="/pdf/2310.20357" title="Download PDF">pdf</a>, <a href="/format/2310.20357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing the Spatial Awareness Capability of Multi-Modal Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yongqiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Feng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haiyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+C">Chengfeng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Zhengwei Tao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xinhai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Donghong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">The Multi-Modal Large Language Model (MLLM) refers to an extension of the
Large Language Model (LLM) equipped with the capability to receive and infer
multi-modal data. Spatial awareness stands as one of the crucial abilities of
MLLM, encompassing diverse skills related to understanding spatial
relationships among objects and between objects and the scene area. Industries
such as autonomous driving, smart healthcare, robotics, virtual, and augmented
reality heavily demand MLLM's spatial awareness capabilities. However, there
exists a noticeable gap between the current spatial awareness capabilities of
MLLM and the requirements set by human needs. To address this issue, this paper
proposes using more precise spatial position information between objects to
guide MLLM in providing more accurate responses to user-related inquiries.
Specifically, for a particular multi-modal task, we utilize algorithms for
acquiring geometric spatial information and scene graphs to obtain relevant
geometric spatial information and scene details of objects involved in the
query. Subsequently, based on this information, we direct MLLM to address
spatial awareness-related queries posed by the user. Extensive experiments were
conducted in benchmarks such as MME, MM-Vet, and other multi-modal large
language models. The experimental results thoroughly confirm the efficacy of
the proposed method in enhancing the spatial awareness tasks and associated
tasks of MLLM.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20360" title="Abstract">arXiv:2310.20360</a> [<a href="/pdf/2310.20360" title="Download PDF">pdf</a>, <a href="/format/2310.20360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical Introduction to Deep Learning: Methods, Implementations,  and Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jentzen%2C+A">Arnulf Jentzen</a>, 
<a href="/search/cs?searchtype=author&query=Kuckuck%2C+B">Benno Kuckuck</a>, 
<a href="/search/cs?searchtype=author&query=von+Wurstemberger%2C+P">Philippe von Wurstemberger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 601 pages, 36 figures, 45 source codes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA); Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">This book aims to provide an introduction to the topic of deep learning
algorithms. We review essential components of deep learning algorithms in full
mathematical detail including different artificial neural network (ANN)
architectures (such as fully-connected feedforward ANNs, convolutional ANNs,
recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different
optimization algorithms (such as the basic stochastic gradient descent (SGD)
method, accelerated methods, and adaptive methods). We also cover several
theoretical aspects of deep learning algorithms such as approximation
capacities of ANNs (including a calculus for ANNs), optimization theory
(including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In
the last part of the book some deep learning approximation methods for PDEs are
reviewed including physics-informed neural networks (PINNs) and deep Galerkin
methods. We hope that this book will be useful for students and scientists who
do not yet have any background in deep learning at all and would like to gain a
solid foundation as well as for practitioners who would like to obtain a firmer
mathematical understanding of the objects and methods considered in deep
learning.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20363" title="Abstract">arXiv:2310.20363</a> [<a href="/pdf/2310.20363" title="Download PDF">pdf</a>, <a href="/format/2310.20363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAFE: Conflict-Aware Feature-wise Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dejl%2C+A">Adam Dejl</a>, 
<a href="/search/cs?searchtype=author&query=Ayoobi%2C+H">Hamed Ayoobi</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+M">Matthew Williams</a>, 
<a href="/search/cs?searchtype=author&query=Toni%2C+F">Francesca Toni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Feature attribution methods are widely used to explain neural models by
determining the influence of individual input features on the models' outputs.
We propose a novel feature attribution method, CAFE (Conflict-Aware
Feature-wise Explanations), that addresses three limitations of the existing
methods: their disregard for the impact of conflicting features, their lack of
consideration for the influence of bias terms, and an overly high sensitivity
to local variations in the underpinning activation functions. Unlike other
methods, CAFE provides safeguards against overestimating the effects of neuron
inputs and separately traces positive and negative influences of input features
and biases, resulting in enhanced robustness and increased ability to surface
feature conflicts. We show experimentally that CAFE is better able to identify
conflicting features on synthetic tabular data and exhibits the best overall
fidelity on several real-world tabular datasets, while being highly
computationally efficient.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20366" title="Abstract">arXiv:2310.20366</a> [<a href="/pdf/2310.20366" title="Download PDF">pdf</a>, <a href="/format/2310.20366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distil the informative essence of loop detector data set: Is  network-level traffic forecasting hungry for more data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guopeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Knoop%2C+V+L">Victor L. Knoop</a>, 
<a href="/search/cs?searchtype=author&query=C.%2C+J+W">J.W.C.</a> (Hans)
<a href="/search/cs?searchtype=author&query=Lint%2C+v">van Lint</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Network-level traffic condition forecasting has been intensively studied for
decades. Although prediction accuracy has been continuously improved with
emerging deep learning models and ever-expanding traffic data, traffic
forecasting still faces many challenges in practice. These challenges include
the robustness of data-driven models, the inherent unpredictability of traffic
dynamics, and whether further improvement of traffic forecasting requires more
sensor data. In this paper, we focus on this latter question and particularly
on data from loop detectors. To answer this, we propose an uncertainty-aware
traffic forecasting framework to explore how many samples of loop data are
truly effective for training forecasting models. Firstly, the model design
combines traffic flow theory with graph neural networks, ensuring the
robustness of prediction and uncertainty quantification. Secondly, evidential
learning is employed to quantify different sources of uncertainty in a single
pass. The estimated uncertainty is used to "distil" the essence of the dataset
that sufficiently covers the information content. Results from a case study of
a highway network around Amsterdam show that, from 2018 to 2021, more than 80\%
of the data during daytime can be removed. The remaining 20\% samples have
equal prediction power for training models. This result suggests that indeed
large traffic datasets can be subdivided into significantly smaller but equally
informative datasets. From these findings, we conclude that the proposed
methodology proves valuable in evaluating large traffic datasets' true
information content. Further extensions, such as extracting smaller, spatially
non-redundant datasets, are possible with this method.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20367" title="Abstract">arXiv:2310.20367</a> [<a href="/pdf/2310.20367" title="Download PDF">pdf</a>, <a href="/format/2310.20367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Machine Learning-Based Framework for Clustering Residential  Electricity Load Profiles to Enhance Demand Response Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michalakopoulos%2C+V">Vasilis Michalakopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Sarmas%2C+E">Elissaios Sarmas</a>, 
<a href="/search/cs?searchtype=author&query=Papias%2C+I">Ioannis Papias</a>, 
<a href="/search/cs?searchtype=author&query=Skaloumpakas%2C+P">Panagiotis Skaloumpakas</a>, 
<a href="/search/cs?searchtype=author&query=Marinakis%2C+V">Vangelis Marinakis</a>, 
<a href="/search/cs?searchtype=author&query=Doukas%2C+H">Haris Doukas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Load shapes derived from smart meter data are frequently employed to analyze
daily energy consumption patterns, particularly in the context of applications
like Demand Response (DR). Nevertheless, one of the most important challenges
to this endeavor lies in identifying the most suitable consumer clusters with
similar consumption behaviors. In this paper, we present a novel machine
learning based framework in order to achieve optimal load profiling through a
real case study, utilizing data from almost 5000 households in London. Four
widely used clustering algorithms are applied specifically K-means, K-medoids,
Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An
empirical analysis as well as multiple evaluation metrics are leveraged to
assess those algorithms. Following that, we redefine the problem as a
probabilistic classification one, with the classifier emulating the behavior of
a clustering algorithm,leveraging Explainable AI (xAI) to enhance the
interpretability of our solution. According to the clustering algorithm
analysis the optimal number of clusters for this case is seven. Despite that,
our methodology shows that two of the clusters, almost 10\% of the dataset,
exhibit significant internal dissimilarity and thus it splits them even further
to create nine clusters in total. The scalability and versatility of our
solution makes it an ideal choice for power utility companies aiming to segment
their users for creating more targeted Demand Response programs.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20369" title="Abstract">arXiv:2310.20369</a> [<a href="/pdf/2310.20369" title="Download PDF">pdf</a>, <a href="/format/2310.20369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability and Generalization of the Decentralized Stochastic Gradient  Descent Ascent Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Miaoxi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The growing size of available data has attracted increasing interest in
solving minimax problems in a decentralized manner for various machine learning
tasks. Previous theoretical research has primarily focused on the convergence
rate and communication complexity of decentralized minimax algorithms, with
little attention given to their generalization. In this paper, we investigate
the primal-dual generalization bound of the decentralized stochastic gradient
descent ascent (D-SGDA) algorithm using the approach of algorithmic stability
under both convex-concave and nonconvex-nonconcave settings. Our theory refines
the algorithmic stability in a decentralized manner and demonstrates that the
decentralized structure does not destroy the stability and generalization of
D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain
situations. Our results analyze the impact of different topologies on the
generalization bound of the D-SGDA algorithm beyond trivial factors such as
sample sizes, learning rates, and iterations. We also evaluate the optimization
error and balance it with the generalization gap to obtain the optimal
population risk of D-SGDA in the convex-concave setting. Additionally, we
perform several numerical experiments which validate our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20375" title="Abstract">arXiv:2310.20375</a> [<a href="/pdf/2310.20375" title="Download PDF">pdf</a>, <a href="/format/2310.20375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GreenCourier: Carbon-Aware Scheduling for Serverless Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chadha%2C+M">Mohak Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+T">Thandayuthapani Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Arima%2C+E">Eishi Arima</a>, 
<a href="/search/cs?searchtype=author&query=Gerndt%2C+M">Michael Gerndt</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+M">Martin Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Abboud%2C+O">Osama Abboud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the ACM 9th International Workshop on Serverless Computing (WoSC@Middleware'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">This paper presents GreenCourier, a novel scheduling framework that enables
the runtime scheduling of serverless functions across geographically
distributed regions based on their carbon efficiencies. Our framework
incorporates an intelligent scheduling strategy for Kubernetes and supports
Knative as the serverless platform. To obtain real-time carbon information for
different geographical regions, our framework supports multiple marginal carbon
emissions sources such as WattTime and the Carbon-aware SDK. We comprehensively
evaluate the performance of our framework using the Google Kubernetes Engine
and production serverless function traces for scheduling functions across
Spain, France, Belgium, and the Netherlands. Results from our experiments show
that compared to other approaches, GreenCourier reduces carbon emissions per
function invocation by an average of 13.25%.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20380" title="Abstract">arXiv:2310.20380</a> [<a href="/pdf/2310.20380" title="Download PDF">pdf</a>, <a href="/format/2310.20380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dropout Strategy in Reinforcement Learning: Limiting the Surrogate  Objective Variance in Policy Optimization Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhengpeng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Changdong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+W">Weizheng Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Policy-based reinforcement learning algorithms are widely used in various
fields. Among them, mainstream policy optimization algorithms such as PPO and
TRPO introduce importance sampling into reinforcement learning, which allows
the reuse of historical data. However, this also results in high variance of
the surrogate objective and indirectly affects the stability and convergence of
the algorithm. In this paper, we first derived an upper bound of the variance
of the surrogate objective, which can grow quadratically with the increase of
the surrogate objective. Next, we proposed a dropout technique to avoid the
excessive increase of the surrogate objective variance caused by importance
sampling. Then, we introduced a general reinforcement learning framework
applicable to mainstream policy optimization methods, and applied the dropout
technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct
comparative experiments between D-PPO and PPO algorithms in the Atari 2600
environment, results show that D-PPO achieved significant performance
improvements compared to PPO, and effectively limited the excessive increase of
the surrogate objective variance during training.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20381" title="Abstract">arXiv:2310.20381</a> [<a href="/pdf/2310.20381" title="Download PDF">pdf</a>, <a href="/format/2310.20381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Study of GPT-4V&#x27;s Multimodal Capabilities in Medical  Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingshu Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhanyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xinyu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingqiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Leyang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Longyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Luping Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents a comprehensive evaluation of GPT-4V's capabilities
across diverse medical imaging tasks, including Radiology Report Generation,
Medical Visual Question Answering (VQA), and Visual Grounding. While prior
efforts have explored GPT-4V's performance in medical imaging, to the best of
our knowledge, our study represents the first quantitative evaluation on
publicly available benchmarks. Our findings highlight GPT-4V's potential in
generating descriptive reports for chest X-ray images, particularly when guided
by well-structured prompts. However, its performance on the MIMIC-CXR dataset
benchmark reveals areas for improvement in certain evaluation metrics, such as
CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in
distinguishing between question types but falls short of prevailing benchmarks
in terms of accuracy. Furthermore, our analysis finds the limitations of
conventional evaluation metrics like the BLEU score, advocating for the
development of more semantically robust assessment methods. In the field of
Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding
boxes, but its precision is lacking, especially in identifying specific medical
organs and signs. Our evaluation underscores the significant potential of
GPT-4V in the medical imaging domain, while also emphasizing the need for
targeted refinements to fully unlock its capabilities.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20384" title="Abstract">arXiv:2310.20384</a> [<a href="/pdf/2310.20384" title="Download PDF">pdf</a>, <a href="/format/2310.20384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do large language models solve verbal analogies like children do?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stevenson%2C+C+E">Claire E. Stevenson</a>, 
<a href="/search/cs?searchtype=author&query=ter+Veen%2C+M">Mathilde ter Veen</a>, 
<a href="/search/cs?searchtype=author&query=Choenni%2C+R">Rochelle Choenni</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Maas%2C+H+L+J">Han L. J. van der Maas</a>, 
<a href="/search/cs?searchtype=author&query=Shutova%2C+E">Ekaterina Shutova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Analogy-making lies at the heart of human cognition. Adults solve analogies
such as \textit{Horse belongs to stable like chicken belongs to ...?} by
mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In
contrast, children often use association, e.g., answering \textit{egg}. This
paper investigates whether large language models (LLMs) solve verbal analogies
in A:B::C:? form using associations, similar to what children do. We use verbal
analogies extracted from an online adaptive learning environment, where 14,002
7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six
tested Dutch monolingual and multilingual LLMs performed around the same level
as children, with MGPT performing worst, around the 7-year-old level, and XLM-V
and GPT-3 the best, slightly above the 11-year-old level. However, when we
control for associative processes this picture changes and each model's
performance level drops 1-2 years. Further experiments demonstrate that
associative processes often underlie correctly solved analogies. We conclude
that the LLMs we tested indeed tend to solve verbal analogies by association
with C like children do.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20387" title="Abstract">arXiv:2310.20387</a> [<a href="/pdf/2310.20387" title="Download PDF">pdf</a>, <a href="/format/2310.20387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of LiLAS 2020 -- Living Labs for Academic Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schaer%2C+P">Philipp Schaer</a>, 
<a href="/search/cs?searchtype=author&query=Schaible%2C+J">Johann Schaible</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+L+J+G">Leyla Jael Garcia Castro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript version of the CLEF 2020 proceedings paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Experimental IR Meets Multilinguality, Multimodality, and
  Interaction. CLEF 2020. Lecture Notes in Computer Science(), vol 12260.
  Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Academic Search is a timeless challenge that the field of Information
Retrieval has been dealing with for many years. Even today, the search for
academic material is a broad field of research that recently started working on
problems like the COVID-19 pandemic. However, test collections and specialized
data sets like CORD-19 only allow for system-oriented experiments, while the
evaluation of algorithms in real-world environments is only available to
researchers from industry. In LiLAS, we open up two academic search platforms
to allow participating research to evaluate their systems in a Docker-based
research environment. This overview paper describes the motivation,
infrastructure, and two systems LIVIVO and GESIS Search that are part of this
CLEF lab.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20388" title="Abstract">arXiv:2310.20388</a> [<a href="/pdf/2310.20388" title="Download PDF">pdf</a>, <a href="/format/2310.20388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Propensity for Causality-based Recommendation without  Exposure Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhongzhou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures (including appendices), accepted as poster at NeurIPS 2013
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Causality-based recommendation systems focus on the causal effects of
user-item interactions resulting from item exposure (i.e., which items are
recommended or exposed to the user), as opposed to conventional
correlation-based recommendation. They are gaining popularity due to their
multi-sided benefits to users, sellers and platforms alike. However, existing
causality-based recommendation methods require additional input in the form of
exposure data and/or propensity scores (i.e., the probability of exposure) for
training. Such data, crucial for modeling causality in recommendation, are
often not available in real-world situations due to technical or privacy
constraints. In this paper, we bridge the gap by proposing a new framework,
called Propensity Estimation for Causality-based Recommendation (PropCare). It
can estimate the propensity and exposure from a more practical setup, where
only interaction data are available without any ground truth on exposure or
propensity in training and inference. We demonstrate that, by relating the
pairwise characteristics between propensity and item popularity, PropCare
enables competitive causality-based recommendation given only the conventional
interaction data. We further present a theoretical analysis on the bias of the
causal effect under our model estimation. Finally, we empirically evaluate
PropCare through both quantitative and qualitative experiments.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20391" title="Abstract">arXiv:2310.20391</a> [<a href="/pdf/2310.20391" title="Download PDF">pdf</a>, <a href="/format/2310.20391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Serverless Scheduling Policies based on Cost Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Palma%2C+G">Giuseppe De Palma</a> (Universit&#xe0; di Bologna), 
<a href="/search/cs?searchtype=author&query=Giallorenzo%2C+S">Saverio Giallorenzo</a> (Universit&#xe0; di Bologna and Inria), 
<a href="/search/cs?searchtype=author&query=Laneve%2C+C">Cosimo Laneve</a> (Universit&#xe0; di Bologna), 
<a href="/search/cs?searchtype=author&query=Mauro%2C+J">Jacopo Mauro</a> (University of Southern Denmark), 
<a href="/search/cs?searchtype=author&query=Trentin%2C+M">Matteo Trentin</a> (Universit&#xe0; di Bologna), 
<a href="/search/cs?searchtype=author&query=Zavattaro%2C+G">Gianluigi Zavattaro</a> (Universit&#xe0; di Bologna and Inria)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings TiCSA 2023, <a href="/abs/2310.18720">arXiv:2310.18720</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 392, 2023, pp. 40-52
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Current proprietary and open-source serverless platforms follow opinionated,
hardcoded scheduling policies to deploy the functions to be executed over the
available workers. Such policies may decrease the performance and the security
of the application due to locality issues (e.g., functions executed by workers
far from the databases to be accessed). These limitations are partially
overcome by the adoption of APP, a new platform-agnostic declarative language
that allows serverless platforms to support multiple scheduling logics.
Defining the "right" scheduling policy in APP is far from being a trivial task
since it often requires rounds of refinement involving knowledge of the
underlying infrastructure, guesswork, and empirical testing. In this paper, we
start investigating how information derived from static analysis could be
incorporated into APP scheduling function policies to help users select the
best-performing workers at function allocation. We substantiate our proposal by
presenting a pipeline able to extract cost equations from functions' code,
synthesising cost expressions through the usage of off-the-shelf solvers, and
extending APP allocation policies to consider this information.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20392" title="Abstract">arXiv:2310.20392</a> [<a href="/pdf/2310.20392" title="Download PDF">pdf</a>, <a href="/format/2310.20392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Configuring Timing Parameters to Ensure Execution-Time Opacity in Timed  Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andr%C3%A9%2C+%C3%89">&#xc9;tienne Andr&#xe9;</a> (Universit&#xe9; Sorbonne Paris Nord, LIPN, CNRS, Villetaneuse, France), 
<a href="/search/cs?searchtype=author&query=Lefaucheux%2C+E">Engel Lefaucheux</a> (Universit&#xe9; de Lorraine, CNRS, Inria, LORIA, Nancy, France), 
<a href="/search/cs?searchtype=author&query=Lime%2C+D">Didier Lime</a> (Nantes Universit&#xe9;, &#xc9;cole Centrale Nantes, CNRS, LS2N, Nantes, France), 
<a href="/search/cs?searchtype=author&query=Marinho%2C+D">Dylan Marinho</a> (Universit&#xe9; de Lorraine, CNRS, Inria, LORIA, Nancy, France), 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jun Sun</a> (School of Computing and Information Systems, Singapore Management University, Singapore)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings TiCSA 2023, <a href="/abs/2310.18720">arXiv:2310.18720</a>. This invited paper mainly summarizes results on opacity from two recent works published in ToSEM (2022) and at ICECCS 2023, providing unified notations and concept names for the sake of consistency. In addition, we prove a few original results absent from these works
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 392, 2023, pp. 1-26
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Cryptography and Security (cs.CR); Software Engineering (cs.SE)

</div>
<p class="mathjax">Timing information leakage occurs whenever an attacker successfully deduces
confidential internal information by observing some timed information such as
events with timestamps. Timed automata are an extension of finite-state
automata with a set of clocks evolving linearly and that can be tested or
reset, making this formalism able to reason on systems involving concurrency
and timing constraints. In this paper, we summarize a recent line of works
using timed automata as the input formalism, in which we assume that the
attacker has access (only) to the system execution time. First, we address the
following execution-time opacity problem: given a timed system modeled by a
timed automaton, given a secret location and a final location, synthesize the
execution times from the initial location to the final location for which one
cannot deduce whether the secret location was visited. This means that for any
such execution time, the system is opaque: either the final location is not
reachable, or it is reachable with that execution time for both a run visiting
and a run not visiting the secret location. We also address the full
execution-time opacity problem, asking whether the system is opaque for all
execution times; we also study a weak counterpart. Second, we add timing
parameters, which are a way to configure a system: we identify a subclass of
parametric timed automata with some decidability results. In addition, we
devise a semi-algorithm for synthesizing timing parameter valuations
guaranteeing that the resulting system is opaque. Third, we report on problems
when the secret has itself an expiration date, thus defining expiring
execution-time opacity problems. We finally show that our method can also apply
to program analysis with configurable internal timings.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20395" title="Abstract">arXiv:2310.20395</a> [<a href="/pdf/2310.20395" title="Download PDF">pdf</a>, <a href="/format/2310.20395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spreadsheet-based Configuration of Families of Real-Time Specifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Proen%C3%A7a%2C+J">Jos&#xe9; Proen&#xe7;a</a> (CISTER and University of Porto, Portugal), 
<a href="/search/cs?searchtype=author&query=Pereira%2C+D">David Pereira</a> (CISTER, Polytechnic Institute of Porto, Portugal), 
<a href="/search/cs?searchtype=author&query=Nandi%2C+G+S">Giann Spilere Nandi</a> (CISTER, Polytechnic Institute of Porto, Portugal), 
<a href="/search/cs?searchtype=author&query=Borrami%2C+S">Sina Borrami</a> (Alstom), 
<a href="/search/cs?searchtype=author&query=Melchert%2C+J">Jonas Melchert</a> (Alstom)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings TiCSA 2023, <a href="/abs/2310.18720">arXiv:2310.18720</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 392, 2023, pp. 27-39
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Model checking real-time systems is complex, and requires a careful trade-off
between including enough detail to be useful and not too much detail to avoid
state explosion. This work exploits variability of the formal model being
analysed and the requirements being checked, to facilitate the model-checking
of variations of real-time specifications. This work results from the
collaboration between academics and Alstom, a railway company with a concrete
use-case, in the context of the VALU3S European project. The configuration of
the variability of the formal specifications is described in MS Excel
spreadsheets with a particular structure, making it easy to use also by
developers. These spreadsheets are processed automatically by our prototype
tool that generates instances and runs the model checker. We propose the
extension of our previous work by exploiting analysis over valid combination of
features, while preserving the simplicity of a spreadsheet-based interface with
the model checker.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20396" title="Abstract">arXiv:2310.20396</a> [<a href="/pdf/2310.20396" title="Download PDF">pdf</a>, <a href="/format/2310.20396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Product Line Management with Graphical MBSE Views
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krapf%2C+P">Pascal Krapf</a> (Syscience), 
<a href="/search/cs?searchtype=author&query=Berthier%2C+S">S&#xe9;bastien Berthier</a> (Syscience), 
<a href="/search/cs?searchtype=author&query=Levy%2C+N">Nicole Levy</a> (CEDRIC-CNAM)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings TiCSA 2023, <a href="/abs/2310.18720">arXiv:2310.18720</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 392, 2023, pp. 53-65
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Reducing the cost and delay and improving quality are major issues for
product and software development, especially in the automotive domain. Product
line engineering is a wellknown approach to engineer systems with the aim to
reduce costs and development time as well as to improve the product quality.
Feature models enable to make logical selection of features and obtain a
filtered set of assets that compose the product. We propose to use a color code
in feature models to make possible decisions visual in the feature tree. The
color code is explained and its use is illustrated. The completeness of the
approach is discussed.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20401" title="Abstract">arXiv:2310.20401</a> [<a href="/pdf/2310.20401" title="Download PDF">pdf</a>, <a href="/format/2310.20401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilitarian Algorithm Configuration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graham%2C+D+R">Devon R. Graham</a>, 
<a href="/search/cs?searchtype=author&query=Leyton-Brown%2C+K">Kevin Leyton-Brown</a>, 
<a href="/search/cs?searchtype=author&query=Roughgarden%2C+T">Tim Roughgarden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We present the first nontrivial procedure for configuring heuristic
algorithms to maximize the utility provided to their end users while also
offering theoretical guarantees about performance. Existing procedures seek
configurations that minimize expected runtime. However, very recent theoretical
work argues that expected runtime minimization fails to capture algorithm
designers' preferences. Here we show that the utilitarian objective also
confers significant algorithmic benefits. Intuitively, this is because mean
runtime is dominated by extremely long runs even when they are incredibly rare;
indeed, even when an algorithm never gives rise to such long runs,
configuration procedures that provably minimize mean runtime must perform a
huge number of experiments to demonstrate this fact. In contrast, utility is
bounded and monotonically decreasing in runtime, allowing for meaningful
empirical bounds on a configuration's performance. This paper builds on this
idea to describe effective and theoretically sound configuration procedures. We
prove upper bounds on the runtime of these procedures that are similar to
theoretical lower bounds, while also demonstrating their performance
empirically.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20407" title="Abstract">arXiv:2310.20407</a> [<a href="/pdf/2310.20407" title="Download PDF">pdf</a>, <a href="/format/2310.20407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised detection of coordinated fake-follower campaigns on social  media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zouzou%2C+Y">Yasser Zouzou</a>, 
<a href="/search/cs?searchtype=author&query=Varol%2C+O">Onur Varol</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures, 1 table and supplementary information
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Automated social media accounts, known as bots, are increasingly recognized
as key tools for manipulative online activities. These activities can stem from
coordination among several accounts and these automated campaigns can
manipulate social network structure by following other accounts, amplifying
their content, and posting messages to spam online discourse. In this study, we
present a novel unsupervised detection method designed to target a specific
category of malicious accounts designed to manipulate user metrics such as
online popularity. Our framework identifies anomalous following patterns among
all the followers of a social media account. Through the analysis of a large
number of accounts on the Twitter platform (rebranded as Twitter after the
acquisition of Elon Musk), we demonstrate that irregular following patterns are
prevalent and are indicative of automated fake accounts. Notably, we find that
these detected groups of anomalous followers exhibit consistent behavior across
multiple accounts. This observation, combined with the computational efficiency
of our proposed approach, makes it a valuable tool for investigating
large-scale coordinated manipulation campaigns on social media platforms.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20410" title="Abstract">arXiv:2310.20410</a> [<a href="/pdf/2310.20410" title="Download PDF">pdf</a>, <a href="/format/2310.20410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FollowBench: A Multi-level Fine-grained Constraints Following Benchmark  for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuxin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangyou Li</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The ability to follow instructions is crucial to Large Language Models (LLMs)
to handle various real-world applications. Existing benchmarks primarily focus
on evaluating superficial response quality, which does not necessarily indicate
instruction-following capability. To fill this research gap, in this paper, we
propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark
for LLMs. FollowBench comprehensively includes five different types (i.e.,
Content, Scenario, Style, Format, and Example) of fine-grained constraints. To
enable a precise constraint following estimation, we introduce a Multi-level
mechanism that incrementally adds a single constraint to the initial
instruction at each level. To evaluate whether LLMs' outputs have satisfied
every individual constraint, we propose to prompt strong LLMs with constraint
evolution paths to handle challenging semantic constraints. By evaluating nine
closed-source and open-source popular LLMs on FollowBench, we highlight the
weaknesses of LLMs in instruction following and point towards potential avenues
for future work. The data and code are publicly available at
https://github.com/YJiangcm/FollowBench.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20412" title="Abstract">arXiv:2310.20412</a> [<a href="/pdf/2310.20412" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thermal-Infrared Remote Target Detection System for Maritime Rescue  based on Data Augmentation with 3D Synthetic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheong%2C+S">Sungjin Cheong</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+W">Wonho Jung</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+Y+S">Yoon Seop Lim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Yong-Hwa Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper proposes a thermal-infrared (TIR) remote target detection system
for maritime rescue using deep learning and data augmentation. We established a
self-collected TIR dataset consisting of multiple scenes imitating human rescue
situations using a TIR camera (FLIR). Additionally, to address dataset scarcity
and improve model robustness, a synthetic dataset from a 3D game (ARMA3) to
augment the data is further collected. However, a significant domain gap exists
between synthetic TIR and real TIR images. Hence, a proper domain adaptation
algorithm is essential to overcome the gap. Therefore, we suggest a domain
adaptation algorithm in a target-background separated manner from 3D
game-to-real, based on a generative model, to address this issue. Furthermore,
a segmentation network with fixed-weight kernels at the head is proposed to
improve the signal-to-noise ratio (SNR) and provide weak attention, as remote
TIR targets inherently suffer from unclear boundaries. Experiment results
reveal that the network trained on augmented data consisting of translated
synthetic and real TIR data outperforms that trained on only real TIR data by a
large margin. Furthermore, the proposed segmentation model surpasses the
performance of state-of-the-art segmentation methods.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20414" title="Abstract">arXiv:2310.20414</a> [<a href="/pdf/2310.20414" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta Learning for Multi-View Visuomotor Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alwis%2C+B">Benji Alwis</a>, 
<a href="/search/cs?searchtype=author&query=Pears%2C+N">Nick Pears</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengcheng Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces a new approach for quickly adapting a multi-view
visuomotor system for robots to varying camera configurations from the baseline
setup. It utilises meta-learning to fine-tune the perceptual network while
keeping the policy network fixed. Experimental results demonstrate a
significant reduction in the number of new training episodes needed to attain
baseline performance.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20419" title="Abstract">arXiv:2310.20419</a> [<a href="/pdf/2310.20419" title="Download PDF">pdf</a>, <a href="/format/2310.20419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relative NN-Descent: A Fast Index Construction for Graph-Based  Approximate Nearest Neighbor Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ono%2C+N">Naoki Ono</a>, 
<a href="/search/cs?searchtype=author&query=Matsui%2C+Y">Yusuke Matsui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACMMM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Approximate Nearest Neighbor Search (ANNS) is the task of finding the
database vector that is closest to a given query vector. Graph-based ANNS is
the family of methods with the best balance of accuracy and speed for
million-scale datasets. However, graph-based methods have the disadvantage of
long index construction time. Recently, many researchers have improved the
tradeoff between accuracy and speed during a search. However, there is little
research on accelerating index construction. We propose a fast graph
construction algorithm, Relative NN-Descent (RNN-Descent). RNN-Descent combines
NN-Descent, an algorithm for constructing approximate K-nearest neighbor graphs
(K-NN graphs), and RNG Strategy, an algorithm for selecting edges effective for
search. This algorithm allows the direct construction of graph-based indexes
without ANNS. Experimental results demonstrated that the proposed method had
the fastest index construction speed, while its search performance is
comparable to existing state-of-the-art methods such as NSG. For example, in
experiments on the GIST1M dataset, the construction of the proposed method is
2x faster than NSG. Additionally, it was even faster than the construction
speed of NN-Descent.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20424" title="Abstract">arXiv:2310.20424</a> [<a href="/pdf/2310.20424" title="Download PDF">pdf</a>, <a href="/format/2310.20424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDC-PIM: Efficient Algorithm/Architecture Co-design for Doubling Data  Capacity of SRAM-based Processing-In-Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+C">Cenlin Duan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianlei Yang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaolin He</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yingjie Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yikun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiou Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Ziyan He</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bonan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaotao Jia</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Weitao Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weisheng Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, to be published in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Processing-in-memory (PIM), as a novel computing paradigm, provides
significant performance benefits from the aspect of effective data movement
reduction. SRAM-based PIM has been demonstrated as one of the most promising
candidates due to its endurance and compatibility. However, the integration
density of SRAM-based PIM is much lower than other non-volatile memory-based
ones, due to its inherent 6T structure for storing a single bit. Within
comparable area constraints, SRAM-based PIM exhibits notably lower capacity.
Thus, aiming to unleash its capacity potential, we propose DDC-PIM, an
efficient algorithm/architecture co-design methodology that effectively doubles
the equivalent data capacity. At the algorithmic level, we propose a
filter-wise complementary correlation (FCC) algorithm to obtain a bitwise
complementary pair. At the architecture level, we exploit the intrinsic
cross-coupled structure of 6T SRAM to store the bitwise complementary pair in
their complementary states ($Q/\overline{Q}$), thereby maximizing the data
capacity of each SRAM cell. The dual-broadcast input structure and
reconfigurable unit support both depthwise and pointwise convolution, adhering
to the requirements of various neural networks. Evaluation results show that
DDC-PIM yields about $2.84\times$ speedup on MobileNetV2 and $2.69\times$ on
EfficientNet-B0 with negligible accuracy loss compared with PIM baseline
implementation. Compared with state-of-the-art SRAM-based PIM macros, DDC-PIM
achieves up to $8.41\times$ and $2.75\times$ improvement in weight density and
area efficiency, respectively.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20425" title="Abstract">arXiv:2310.20425</a> [<a href="/pdf/2310.20425" title="Download PDF">pdf</a>, <a href="/format/2310.20425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discussing the Spectrum of Physics-Enhanced Machine Learning via a  Survey on Structural Mechanics Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haywood-Alexander%2C+M">Marcus Haywood-Alexander</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bacsa%2C+K">Kiran Bacsa</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhilu Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chatzi%2C+E">Eleni Chatzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The intersection of physics and machine learning has given rise to a paradigm
that we refer to here as physics-enhanced machine learning (PEML), aiming to
improve the capabilities and reduce the individual shortcomings of data- or
physics-only methods. In this paper, the spectrum of physics-enhanced machine
learning methods, expressed across the defining axes of physics and data, is
discussed by engaging in a comprehensive exploration of its characteristics,
usage, and motivations. In doing so, this paper offers a survey of recent
applications and developments of PEML techniques, revealing the potency of PEML
in addressing complex challenges. We further demonstrate application of select
such schemes on the simple working example of a single-degree-of-freedom
Duffing oscillator, which allows to highlight the individual characteristics
and motivations of different `genres' of PEML approaches. To promote
collaboration and transparency, and to provide practical examples for the
reader, the code of these working examples is provided alongside this paper. As
a foundational contribution, this paper underscores the significance of PEML in
pushing the boundaries of scientific and engineering research, underpinned by
the synergy of physical insights and machine learning capabilities.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20426" title="Abstract">arXiv:2310.20426</a> [<a href="/pdf/2310.20426" title="Download PDF">pdf</a>, <a href="/format/2310.20426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Pareto Set Learning with Structure Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">The multiobjective evolutionary optimization algorithm (MOEA) is a powerful
approach for tackling multiobjective optimization problems (MOPs), which can
find a finite set of approximate Pareto solutions in a single run. However,
under mild regularity conditions, the Pareto optimal set of a continuous MOP
could be a low dimensional continuous manifold that contains infinite
solutions. In addition, structure constraints on the whole optimal solution
set, which characterize the patterns shared among all solutions, could be
required in many real-life applications. It is very challenging for existing
finite population based MOEAs to handle these structure constraints properly.
In this work, we propose the first model-based algorithmic framework to learn
the whole solution set with structure constraints for multiobjective
optimization. In our approach, the Pareto optimality can be traded off with a
preferred structure among the whole solution set, which could be crucial for
many real-world problems. We also develop an efficient evolutionary learning
method to train the set model with structure constraints. Experimental studies
on benchmark test suites and real-world application problems demonstrate the
promising performance of our proposed framework.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20428" title="Abstract">arXiv:2310.20428</a> [<a href="/pdf/2310.20428" title="Download PDF">pdf</a>, <a href="/format/2310.20428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In the recovery of sparse vectors from quadratic measurements, the  presence of linear terms breaks the square root bottleneck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cosse%2C+A">Augustin Cosse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Motivated by recent results in the statistical physics of spin glasses, we
study the recovery of a sparse vector $\mathbf{x}_0\in \mathbb{S}^{n-1}$,
$\|\mathbf{x}_0\|_{\ell_0} = k&lt;n$, from $m$ quadratic measurements of the form
$ (1-\lambda)\langle \mathbf{A}_i, \mathbf{x}_0\mathbf{x}_0^T \rangle + \lambda
\langle\mathbf{c}_i,\mathbf{x}_0 \rangle $ where $\mathbf{A}_{i},
\mathbf{c}_{i}$ have i.i.d Gaussian entries. This can be related to a
constrained version of the 2-spin Hamiltonian with external field for which it
was recently shown (in the absence of any structural constraint and in the
asymptotic regime) that the geometry of the energy landscape becomes trivial
above a certain threshold $\lambda &gt; \lambda_c\in (0,1)$. Building on this idea
we study the evolution of the so-called square root bottleneck for $\lambda\in
[0,1]$ in the setting of the sparse rank one matrix recovery/sensing problem.
We show that recovery of the vector $\mathbf{x}_0$ can be guaranteed as soon as
$m\gtrsim k^2 (1-\lambda)^2/\lambda^2$, $\lambda \gtrsim k^{-1/2}$ provided
that this vector satisfies a sufficiently strong incoherence condition, thus
retrieving the linear regime for an external field $(1-\lambda)/\lambda
\lesssim k^{-1/2}$. Our proof relies on an interpolation between the linear and
quadratic settings, as well as on standard convex geometry arguments.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20430" title="Abstract">arXiv:2310.20430</a> [<a href="/pdf/2310.20430" title="Download PDF">pdf</a>, <a href="/ps/2310.20430" title="Download PostScript">ps</a>, <a href="/format/2310.20430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Borrowable Fractional Ownership Types for Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakayama%2C+T">Takashi Nakayama</a>, 
<a href="/search/cs?searchtype=author&query=Matsushita%2C+Y">Yusuke Matsushita</a>, 
<a href="/search/cs?searchtype=author&query=Sakayori%2C+K">Ken Sakayori</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+R">Ryosuke Sato</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+N">Naoki Kobayashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An extended version of the paper to appear in Proceedings of VMCAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Automated verification of functional correctness of imperative programs with
references (a.k.a. pointers) is challenging because of reference aliasing.
Ownership types have recently been applied to address this issue, but the
existing approaches were limited in that they are effective only for a class of
programs whose reference usage follows a certain style. To relax the
limitation, we combine the approaches of ConSORT (based on fractional
ownership) and RustHorn (based on borrowable ownership), two recent approaches
to automated program verification based on ownership types, and propose the
notion of borrowable fractional ownership types. We formalize a new type system
based on the borrowable fractional ownership types and show how we can use it
to automatically reduce the program verification problem for imperative
programs with references to that for functional programs without references. We
also show the soundness of our type system and the translation, and conduct
experiments to confirm the effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20431" title="Abstract">arXiv:2310.20431</a> [<a href="/pdf/2310.20431" title="Download PDF">pdf</a>, <a href="/format/2310.20431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Raising the ClaSS of Streaming Time Series Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ermshaus%2C+A">Arik Ermshaus</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4fer%2C+P">Patrick Sch&#xe4;fer</a>, 
<a href="/search/cs?searchtype=author&query=Leser%2C+U">Ulf Leser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">Ubiquitous sensors today emit high frequency streams of numerical
measurements that reflect properties of human, animal, industrial, commercial,
and natural processes. Shifts in such processes, e.g. caused by external events
or internal state changes, manifest as changes in the recorded signals. The
task of streaming time series segmentation (STSS) is to partition the stream
into consecutive variable-sized segments that correspond to states of the
observed processes or entities. The partition operation itself must in
performance be able to cope with the input frequency of the signals. We
introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS.
ClaSS assesses the homogeneity of potential partitions using self-supervised
time series classification and applies statistical tests to detect significant
change points (CPs). In our experimental evaluation using two large benchmarks
and six real-world data archives, we found ClaSS to be significantly more
precise than eight state-of-the-art competitors. Its space and time complexity
is independent of segment sizes and linear only in the sliding window size. We
also provide ClaSS as a window operator with an average throughput of 538 data
points per second for the Apache Flink streaming engine.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20433" title="Abstract">arXiv:2310.20433</a> [<a href="/pdf/2310.20433" title="Download PDF">pdf</a>, <a href="/format/2310.20433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple and tight complexity lower bounds for solving Rabin games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casares%2C+A">Antonio Casares</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Marcin Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Micha&#x142; Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Souza%2C+U+S">U&#xe9;verton S. Souza</a>, 
<a href="/search/cs?searchtype=author&query=Thejaswini%2C+K+S">K. S. Thejaswini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures. To appear in SOSA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We give a simple proof that assuming the Exponential Time Hypothesis (ETH),
determining the winner of a Rabin game cannot be done in time $2^{o(k \log k)}
\cdot n^{O(1)}$, where $k$ is the number of pairs of vertex subsets involved in
the winning condition and $n$ is the vertex count of the game graph. While this
result follows from the lower bounds provided by Calude et al [SIAM J. Comp.
2022], our reduction is simpler and arguably provides more insight into the
complexity of the problem. In fact, the analogous lower bounds discussed by
Calude et al, for solving Muller games and multidimensional parity games,
follow as simple corollaries of our approach. Our reduction also highlights the
usefulness of a certain pivot problem -- Permutation SAT -- which may be of
independent interest.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20435" title="Abstract">arXiv:2310.20435</a> [<a href="/pdf/2310.20435" title="Download PDF">pdf</a>, <a href="/format/2310.20435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Sustainability and Trustworthiness of Federated Learning  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Celdran%2C+A+H">Alberto Huertas Celdran</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+P+M+S">Pedro Miguel Sanchez Sanchez</a>, 
<a href="/search/cs?searchtype=author&query=Zumtaugwald%2C+L">Lynn Zumtaugwald</a>, 
<a href="/search/cs?searchtype=author&query=Bovet%2C+G">Gerome Bovet</a>, 
<a href="/search/cs?searchtype=author&query=Stiller%2C+B">Burkhard Stiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Artificial intelligence (AI) plays a pivotal role in various sectors,
influencing critical decision-making processes in our daily lives. Within the
AI landscape, novel AI paradigms, such as Federated Learning (FL), focus on
preserving data privacy while collaboratively training AI models. In such a
context, a group of experts from the European Commission (AI-HLEG) has
identified sustainable AI as one of the key elements that must be considered to
provide trustworthy AI. While existing literature offers several taxonomies and
solutions for assessing the trustworthiness of FL models, a significant gap
exists in considering sustainability and the carbon footprint associated with
FL. Thus, this work introduces the sustainability pillar to the most recent and
comprehensive trustworthy FL taxonomy, making this work the first to address
all AI-HLEG requirements. The sustainability pillar assesses the FL system
environmental impact, incorporating notions and metrics for hardware
efficiency, federation complexity, and energy grid carbon intensity. Then, this
work designs and implements an algorithm for evaluating the trustworthiness of
FL models by incorporating the sustainability pillar. Extensive evaluations
with the FederatedScope framework and various scenarios varying federation
participants, complexities, hardware, and energy grids demonstrate the
usefulness of the proposed solution.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20436" title="Abstract">arXiv:2310.20436</a> [<a href="/pdf/2310.20436" title="Download PDF">pdf</a>, <a href="/format/2310.20436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and  Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhengdi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaoli Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yongkang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Birdal%2C+T">Tolga Birdal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages; Project page available at <a href="https://signavatars.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we present SignAvatars, the first large-scale multi-prompt 3D
sign language (SL) motion dataset designed to bridge the communication gap for
hearing-impaired individuals. While there has been an exponentially growing
number of research regarding digital communication, the majority of existing
communication technologies primarily cater to spoken or written languages,
instead of SL, the essential communication method for hearing-impaired
communities. Existing SL datasets, dictionaries, and sign language production
(SLP) methods are typically limited to 2D as the annotating 3D models and
avatars for SL is usually an entirely manual and labor-intensive process
conducted by SL experts, often resulting in unnatural avatars. In response to
these challenges, we compile and curate the SignAvatars dataset, which
comprises 70,000 videos from 153 signers, totaling 8.34 million frames,
covering both isolated signs and continuous, co-articulated signs, with
multiple prompts including HamNoSys, spoken language, and words. To yield 3D
holistic annotations, including meshes and biomechanically-valid poses of body,
hands, and face, as well as 2D and 3D keypoints, we introduce an automated
annotation pipeline operating on our large corpus of SL videos. SignAvatars
facilitates various tasks such as 3D sign language recognition (SLR) and the
novel 3D SL production (SLP) from diverse inputs like text scripts, individual
words, and HamNoSys notation. Hence, to evaluate the potential of SignAvatars,
we further propose a unified benchmark of 3D SL holistic motion production. We
believe that this work is a significant step forward towards bringing the
digital world to the hearing-impaired communities. Our project page is at
https://signavatars.github.io/
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20440" title="Abstract">arXiv:2310.20440</a> [<a href="/pdf/2310.20440" title="Download PDF">pdf</a>, <a href="/ps/2310.20440" title="Download PostScript">ps</a>, <a href="/format/2310.20440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The SourceData-NLP dataset: integrating curation into scientific  publishing for training large language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abreu-Vicente%2C+J">Jorge Abreu-Vicente</a>, 
<a href="/search/cs?searchtype=author&query=Sonntag%2C+H">Hannah Sonntag</a>, 
<a href="/search/cs?searchtype=author&query=Eidens%2C+T">Thomas Eidens</a>, 
<a href="/search/cs?searchtype=author&query=Lemberger%2C+T">Thomas Lemberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Introduction: The scientific publishing landscape is expanding rapidly,
creating challenges for researchers to stay up-to-date with the evolution of
the literature. Natural Language Processing (NLP) has emerged as a potent
approach to automating knowledge extraction from this vast amount of
publications and preprints. Tasks such as Named-Entity Recognition (NER) and
Named-Entity Linking (NEL), in conjunction with context-dependent semantic
interpretation, offer promising and complementary approaches to extracting
structured information and revealing key concepts.
<br />Results: We present the SourceData-NLP dataset produced through the routine
curation of papers during the publication process. A unique feature of this
dataset is its emphasis on the annotation of bioentities in figure legends. We
annotate eight classes of biomedical entities (small molecules, gene products,
subcellular components, cell lines, cell types, tissues, organisms, and
diseases), their role in the experimental design, and the nature of the
experimental method as an additional class. SourceData-NLP contains more than
620,000 annotated biomedical entities, curated from 18,689 figures in 3,223
papers in molecular and cell biology. We illustrate the dataset's usefulness by
assessing BioLinkBERT and PubmedBERT, two transformers-based models, fine-tuned
on the SourceData-NLP dataset for NER. We also introduce a novel
context-dependent semantic task that infers whether an entity is the target of
a controlled intervention or the object of measurement.
<br />Conclusions: SourceData-NLP's scale highlights the value of integrating
curation into publishing. Models trained with SourceData-NLP will furthermore
enable the development of tools able to extract causal hypotheses from the
literature and assemble them into knowledge graphs.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20443" title="Abstract">arXiv:2310.20443</a> [<a href="/pdf/2310.20443" title="Download PDF">pdf</a>, <a href="/format/2310.20443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ontologies for Models and Algorithms in Applied Mathematics and Related  Disciplines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schembera%2C+B">Bj&#xf6;rn Schembera</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%BCbbeling%2C+F">Frank W&#xfc;bbeling</a>, 
<a href="/search/cs?searchtype=author&query=Kleikamp%2C+H">Hendrik Kleikamp</a>, 
<a href="/search/cs?searchtype=author&query=Biedinger%2C+C">Christine Biedinger</a>, 
<a href="/search/cs?searchtype=author&query=Fiedler%2C+J">Jochen Fiedler</a>, 
<a href="/search/cs?searchtype=author&query=Reidelbach%2C+M">Marco Reidelbach</a>, 
<a href="/search/cs?searchtype=author&query=Shehu%2C+A">Aurela Shehu</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+B">Burkhard Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Koprucki%2C+T">Thomas Koprucki</a>, 
<a href="/search/cs?searchtype=author&query=Iglezakis%2C+D">Dorothea Iglezakis</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B6ddeke%2C+D">Dominik G&#xf6;ddeke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of a Conference Paper to appear in the Proceeding of the 17th International Conference on Metadata and Semantics Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Databases (cs.DB); Digital Libraries (cs.DL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">In applied mathematics and related disciplines, the
modeling-simulation-optimization workflow is a prominent scheme, with
mathematical models and numerical algorithms playing a crucial role. For these
types of mathematical research data, the Mathematical Research Data Initiative
has developed, merged and implemented ontologies and knowledge graphs. This
contributes to making mathematical research data FAIR by introducing semantic
technology and documenting the mathematical foundations accordingly. Using the
concrete example of microfracture analysis of porous media, it is shown how the
knowledge of the underlying mathematical model and the corresponding numerical
algorithms for its solution can be represented by the ontologies.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20444" title="Abstract">arXiv:2310.20444</a> [<a href="/pdf/2310.20444" title="Download PDF">pdf</a>, <a href="/format/2310.20444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing the Impact of Companies on AI Research Based on Publications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=F%C3%A4rber%2C+M">Michael F&#xe4;rber</a>, 
<a href="/search/cs?searchtype=author&query=Tampakis%2C+L">Lazaros Tampakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Scientometrics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Digital Libraries (cs.DL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Artificial Intelligence (AI) is one of the most momentous technologies of our
time. Thus, it is of major importance to know which stakeholders influence AI
research. Besides researchers at universities and colleges, researchers in
companies have hardly been considered in this context. In this article, we
consider how the influence of companies on AI research can be made measurable
on the basis of scientific publishing activities. We compare academic- and
company-authored AI publications published in the last decade and use
scientometric data from multiple scholarly databases to look for differences
across these groups and to disclose the top contributing organizations. While
the vast majority of publications is still produced by academia, we find that
the citation count an individual publication receives is significantly higher
when it is (co-)authored by a company. Furthermore, using a variety of
altmetric indicators, we notice that publications with company participation
receive considerably more attention online. Finally, we place our analysis
results in a broader context and present targeted recommendations to safeguard
a harmonious balance between academia and industry in the realm of AI research.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20446" title="Abstract">arXiv:2310.20446</a> [<a href="/pdf/2310.20446" title="Download PDF">pdf</a>, <a href="/format/2310.20446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAVSS: Location-Guided Audio-Visual Spatial Audio Separation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yuxin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yapeng Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Existing machine learning research has achieved promising results in monaural
audio-visual separation (MAVS). However, most MAVS methods purely consider what
the sound source is, not where it is located. This can be a problem in VR/AR
scenarios, where listeners need to be able to distinguish between similar audio
sources located in different directions. To address this limitation, we have
generalized MAVS to spatial audio separation and proposed LAVSS: a
location-guided audio-visual spatial audio separator. LAVSS is inspired by the
correlation between spatial audio and visual location. We introduce the phase
difference carried by binaural audio as spatial cues, and we utilize positional
representations of sounding objects as additional modality guidance. We also
leverage multi-level cross-modal attention to perform visual-positional
collaboration with audio features. In addition, we adopt a pre-trained monaural
separator to transfer knowledge from rich mono sounds to boost spatial audio
separation. This exploits the correlation between monaural and binaural
channels. Experiments on the FAIR-Play dataset demonstrate the superiority of
the proposed LAVSS over existing benchmarks of audio-visual separation. Our
project page: https://yyx666660.github.io/LAVSS/.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20447" title="Abstract">arXiv:2310.20447</a> [<a href="/pdf/2310.20447" title="Download PDF">pdf</a>, <a href="/format/2310.20447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adriaensen%2C+S">Steven Adriaensen</a>, 
<a href="/search/cs?searchtype=author&query=Rakotoarison%2C+H">Herilalaina Rakotoarison</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+S">Samuel M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Learning curve extrapolation aims to predict model performance in later
epochs of training, based on the performance in earlier epochs. In this work,
we argue that, while the inherent uncertainty in the extrapolation of learning
curves warrants a Bayesian approach, existing methods are (i) overly
restrictive, and/or (ii) computationally expensive. We describe the first
application of prior-data fitted neural networks (PFNs) in this context. A PFN
is a transformer, pre-trained on data generated from a prior, to perform
approximate Bayesian inference in a single forward pass. We propose LC-PFN, a
PFN trained to extrapolate 10 million artificial right-censored learning curves
generated from a parametric prior proposed in prior art using MCMC. We
demonstrate that LC-PFN can approximate the posterior predictive distribution
more accurately than MCMC, while being over 10 000 times faster. We also show
that the same LC-PFN achieves competitive performance extrapolating a total of
20 000 real learning curves from four learning curve benchmarks (LCBench,
NAS-Bench-201, Taskset, and PD1) that stem from training a wide range of model
architectures (MLPs, CNNs, RNNs, and Transformers) on 53 different datasets
with varying input modalities (tabular, image, text, and protein data).
Finally, we investigate its potential in the context of model selection and
find that a simple LC-PFN based predictive early stopping criterion obtains 2 -
6x speed-ups on 45 of these datasets, at virtually no overhead.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20448" title="Abstract">arXiv:2310.20448</a> [<a href="/pdf/2310.20448" title="Download PDF">pdf</a>, <a href="/format/2310.20448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Federated Unlearning: Challenges, Methods, and Future  Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziyao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiyuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+M">Minyi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+K">Kwok-Yan Lam</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xingliang Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In recent years, the notion of ``the right to be forgotten" (RTBF) has
evolved into a fundamental element of data privacy regulations, affording
individuals the ability to request the removal of their personal data from
digital records. Consequently, given the extensive adoption of data-intensive
machine learning (ML) algorithms and increasing concerns for personal data
privacy protection, the concept of machine unlearning (MU) has gained
considerable attention. MU empowers an ML model to selectively eliminate
sensitive or personally identifiable information it acquired during the
training process. Evolving from the foundational principles of MU, federated
unlearning (FU) has emerged to confront the challenge of data erasure within
the domain of federated learning (FL) settings. This empowers the FL model to
unlearn an FL client or identifiable information pertaining to the client while
preserving the integrity of the decentralized learning process. Nevertheless,
unlike traditional MU, the distinctive attributes of federated learning
introduce specific challenges for FU techniques. These challenges lead to the
need for tailored design when designing FU algorithms. Therefore, this
comprehensive survey delves into the techniques, methodologies, and recent
advancements in federated unlearning. It provides an overview of fundamental
concepts and principles, evaluates existing federated unlearning algorithms,
reviews optimizations tailored to federated learning, engages in discussions
regarding practical applications, along with an assessment of their
limitations, and outlines promising directions for future research.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20452" title="Abstract">arXiv:2310.20452</a> [<a href="/pdf/2310.20452" title="Download PDF">pdf</a>, <a href="/format/2310.20452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islamov%2C+R">Rustem Islamov</a>, 
<a href="/search/cs?searchtype=author&query=Safaryan%2C+M">Mher Safaryan</a>, 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We analyze asynchronous-type algorithms for distributed SGD in the
heterogeneous setting, where each worker has its own computation and
communication speeds, as well as data distribution. In these algorithms,
workers compute possibly stale and stochastic gradients associated with their
local data at some iteration back in history and then return those gradients to
the server without synchronizing with other workers. We present a unified
convergence theory for non-convex smooth functions in the heterogeneous regime.
The proposed analysis provides convergence for pure asynchronous SGD and its
various modifications. Moreover, our theory explains what affects the
convergence rate and what can be done to improve the performance of
asynchronous algorithms. In particular, we introduce a novel asynchronous
method based on worker shuffling. As a by-product of our analysis, we also
demonstrate convergence guarantees for gradient-type algorithms such as SGD
with random reshuffling and shuffle-once mini-batch SGD. The derived rates
match the best-known results for those algorithms, highlighting the tightness
of our approach. Finally, our numerical evaluations support theoretical
findings and show the good practical performance of our method.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20453" title="Abstract">arXiv:2310.20453</a> [<a href="/pdf/2310.20453" title="Download PDF">pdf</a>, <a href="/format/2310.20453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generate What You Prefer: Reshaping Sequential Recommendation via Guided  Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiancan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhicai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yancheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiangnan He</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Sequential recommendation aims to recommend the next item that matches a
user's interest, based on the sequence of items he/she interacted with before.
Scrutinizing previous studies, we can summarize a common learning-to-classify
paradigm -- given a positive item, a recommender model performs negative
sampling to add negative items and learns to classify whether the user prefers
them or not, based on his/her historical interaction sequence. Although
effective, we reveal two inherent limitations:(1) it may differ from human
behavior in that a user could imagine an oracle item in mind and select
potential items matching the oracle; and (2) the classification is limited in
the candidate pool with noisy or easy supervision from negative samples, which
dilutes the preference signals towards the oracle item. Yet, generating the
oracle item from the historical interaction sequence is mostly unexplored. To
bridge the gap, we reshape sequential recommendation as a learning-to-generate
paradigm, which is achieved via a guided diffusion model, termed
DreamRec.Specifically, for a sequence of historical items, it applies a
Transformer encoder to create guidance representations. Noising target items
explores the underlying distribution of item space; then, with the guidance of
historical interactions, the denoising process generates an oracle item to
recover the positive item, so as to cast off negative sampling and depict the
true preference of the user directly. We evaluate the effectiveness of DreamRec
through extensive experiments and comparisons with existing methods. Codes and
data are open-sourced at https://github.com/YangZhengyi98/DreamRec.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20456" title="Abstract">arXiv:2310.20456</a> [<a href="/pdf/2310.20456" title="Download PDF">pdf</a>, <a href="/format/2310.20456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Deep Understanding of Multilingual End-to-End Speech  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haoran Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaohu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yikun Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shaolin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Deyi Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we employ Singular Value Canonical Correlation Analysis
(SVCCA) to analyze representations learnt in a multilingual end-to-end speech
translation model trained over 22 languages. SVCCA enables us to estimate
representational similarity across languages and layers, enhancing our
understanding of the functionality of multilingual speech translation and its
potential connection to multilingual neural machine translation. The
multilingual speech translation model is trained on the CoVoST 2 dataset in all
possible directions, and we utilize LASER to extract parallel bitext data for
SVCCA analysis. We derive three major findings from our analysis: (I)
Linguistic similarity loses its efficacy in multilingual speech translation
when the training data for a specific language is limited. (II) Enhanced
encoder representations and well-aligned audio-text data significantly improve
translation quality, surpassing the bilingual counterparts when the training
data is not compromised. (III) The encoder representations of multilingual
speech translation demonstrate superior performance in predicting phonetic
features in linguistic typology prediction. With these findings, we propose
that releasing the constraint of limited data for low-resource languages and
subsequently combining them with linguistically related high-resource languages
could offer a more effective approach for multilingual end-to-end speech
translation.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20457" title="Abstract">arXiv:2310.20457</a> [<a href="/pdf/2310.20457" title="Download PDF">pdf</a>, <a href="/format/2310.20457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlexTrain: A Dynamic Training Framework for Heterogeneous Devices  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Unsal%2C+M">Mert Unsal</a>, 
<a href="/search/cs?searchtype=author&query=Maatouk%2C+A">Ali Maatouk</a>, 
<a href="/search/cs?searchtype=author&query=De+Domenico%2C+A">Antonio De Domenico</a>, 
<a href="/search/cs?searchtype=author&query=Piovesan%2C+N">Nicola Piovesan</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+F">Fadhel Ayed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Workshop on Advancing Neural Network Training (WANT) at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As deep learning models become increasingly large, they pose significant
challenges in heterogeneous devices environments. The size of deep learning
models makes it difficult to deploy them on low-power or resource-constrained
devices, leading to long inference times and high energy consumption. To
address these challenges, we propose FlexTrain, a framework that accommodates
the diverse storage and computational resources available on different devices
during the training phase. FlexTrain enables efficient deployment of deep
learning models, while respecting device constraints, minimizing communication
costs, and ensuring seamless integration with diverse devices. We demonstrate
the effectiveness of FlexTrain on the CIFAR-100 dataset, where a single global
model trained with FlexTrain can be easily deployed on heterogeneous devices,
saving training time and energy consumption. We also extend FlexTrain to the
federated learning setting, showing that our approach outperforms standard
federated learning benchmarks on both CIFAR-10 and CIFAR-100 datasets.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20463" title="Abstract">arXiv:2310.20463</a> [<a href="/pdf/2310.20463" title="Download PDF">pdf</a>, <a href="/ps/2310.20463" title="Download PostScript">ps</a>, <a href="/format/2310.20463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Neural PDE Solvers using Symbolic Frameworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+Y+R">Yolanne Yi Ran Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the NeurIPS 2023 AI for Science Workshop. arXiv admin note: text overlap with <a href="/abs/2310.19763">arXiv:2310.19763</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Partial differential equations (PDEs) are ubiquitous in the world around us,
modelling phenomena from heat and sound to quantum systems. Recent advances in
deep learning have resulted in the development of powerful neural solvers;
however, while these methods have demonstrated state-of-the-art performance in
both accuracy and computational efficiency, a significant challenge remains in
their interpretability. Most existing methodologies prioritize predictive
accuracy over clarity in the underlying mechanisms driving the model's
decisions. Interpretability is crucial for trustworthiness and broader
applicability, especially in scientific and engineering domains where neural
PDE solvers might see the most impact. In this context, a notable gap in
current research is the integration of symbolic frameworks (such as symbolic
regression) into these solvers. Symbolic frameworks have the potential to
distill complex neural operations into human-readable mathematical expressions,
bridging the divide between black-box predictions and solutions.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20467" title="Abstract">arXiv:2310.20467</a> [<a href="/pdf/2310.20467" title="Download PDF">pdf</a>, <a href="/format/2310.20467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL  Anthology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Guerin%2C+F">Frank Guerin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The ACL Anthology is an online repository that serves as a comprehensive
collection of publications in the field of natural language processing (NLP)
and computational linguistics (CL). This paper presents a tool called ``ACL
Anthology Helper''. It automates the process of parsing and downloading papers
along with their meta-information, which are then stored in a local MySQL
database. This allows for efficient management of the local papers using a wide
range of operations, including "where," "group," "order," and more. By
providing over 20 operations, this tool significantly enhances the retrieval of
literature based on specific conditions. Notably, this tool has been
successfully utilised in writing a survey paper (Tang et al.,2022a). By
introducing the ACL Anthology Helper, we aim to enhance researchers' ability to
effectively access and organise literature from the ACL Anthology. This tool
offers a convenient solution for researchers seeking to explore the ACL
Anthology's vast collection of publications while allowing for more targeted
and efficient literature retrieval.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20468" title="Abstract">arXiv:2310.20468</a> [<a href="/pdf/2310.20468" title="Download PDF">pdf</a>, <a href="/format/2310.20468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Introduction to Causal Inference Methods for Observational  Human-Robot Interaction Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+J+R">Jaron J. R. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ajaykumar%2C+G">Gopika Ajaykumar</a>, 
<a href="/search/cs?searchtype=author&query=Shpitser%2C+I">Ilya Shpitser</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chien-Ming Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Quantitative methods in Human-Robot Interaction (HRI) research have primarily
relied upon randomized, controlled experiments in laboratory settings. However,
such experiments are not always feasible when external validity, ethical
constraints, and ease of data collection are of concern. Furthermore, as
consumer robots become increasingly available, increasing amounts of real-world
data will be available to HRI researchers, which prompts the need for
quantative approaches tailored to the analysis of observational data. In this
article, we present an alternate approach towards quantitative research for HRI
researchers using methods from causal inference that can enable researchers to
identify causal relationships in observational settings where randomized,
controlled experiments cannot be run. We highlight different scenarios that HRI
research with consumer household robots may involve to contextualize how
methods from causal inference can be applied to observational HRI research.
<br />We then provide a tutorial summarizing key concepts from causal inference
using a graphical model perspective and link to code examples throughout the
article, which are available at https://gitlab.com/causal/causal_hri. Our work
paves the way for further discussion on new approaches towards observational
HRI research while providing a starting point for HRI researchers to add causal
inference techniques to their analytical toolbox.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20469" title="Abstract">arXiv:2310.20469</a> [<a href="/pdf/2310.20469" title="Download PDF">pdf</a>, <a href="/format/2310.20469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amoeba: Circumventing ML-supported Network Censorship via Adversarial  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Diallo%2C+A+F">Alec F. Diallo</a>, 
<a href="/search/cs?searchtype=author&query=Patras%2C+P">Paul Patras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Embedding covert streams into a cover channel is a common approach to
circumventing Internet censorship, due to censors' inability to examine
encrypted information in otherwise permitted protocols (Skype, HTTPS, etc.).
However, recent advances in machine learning (ML) enable detecting a range of
anti-censorship systems by learning distinct statistical patterns hidden in
traffic flows. Therefore, designing obfuscation solutions able to generate
traffic that is statistically similar to innocuous network activity, in order
to deceive ML-based classifiers at line speed, is difficult.
<br />In this paper, we formulate a practical adversarial attack strategy against
flow classifiers as a method for circumventing censorship. Specifically, we
cast the problem of finding adversarial flows that will be misclassified as a
sequence generation task, which we solve with Amoeba, a novel reinforcement
learning algorithm that we design. Amoeba works by interacting with censoring
classifiers without any knowledge of their model structure, but by crafting
packets and observing the classifiers' decisions, in order to guide the
sequence generation process. Our experiments using data collected from two
popular anti-censorship systems demonstrate that Amoeba can effectively shape
adversarial flows that have on average 94% attack success rate against a range
of ML algorithms. In addition, we show that these adversarial flows are robust
in different network environments and possess transferability across various ML
models, meaning that once trained against one, our agent can subvert other
censoring classifiers without retraining.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20470" title="Abstract">arXiv:2310.20470</a> [<a href="/pdf/2310.20470" title="Download PDF">pdf</a>, <a href="/format/2310.20470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representativeness as a Forgotten Lesson for Multilingual and  Code-switched Data Collection and Preparation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%C4%9Fru%C3%B6z%2C+A+S">A. Seza Do&#x11f;ru&#xf6;z</a>, 
<a href="/search/cs?searchtype=author&query=Sitaram%2C+S">Sunayana Sitaram</a>, 
<a href="/search/cs?searchtype=author&query=Yong%2C+Z">Zheng-Xin Yong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for EMNLP'23 Findings (to appear on EMNLP'23 Proceedings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multilingualism is widespread around the world and code-switching (CSW) is a
common practice among different language pairs/tuples across locations and
regions. However, there is still not much progress in building successful CSW
systems, despite the recent advances in Massive Multilingual Language Models
(MMLMs). We investigate the reasons behind this setback through a critical
study about the existing CSW data sets (68) across language pairs in terms of
the collection and preparation (e.g. transcription and annotation) stages. This
in-depth analysis reveals that \textbf{a)} most CSW data involves English
ignoring other language pairs/tuples \textbf{b)} there are flaws in terms of
representativeness in data collection and preparation stages due to ignoring
the location based, socio-demographic and register variation in CSW. In
addition, lack of clarity on the data selection and filtering stages shadow the
representativeness of CSW data sets. We conclude by providing a short
check-list to improve the representativeness for forthcoming studies involving
CSW data collection and preparation.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20473" title="Abstract">arXiv:2310.20473</a> [<a href="/pdf/2310.20473" title="Download PDF">pdf</a>, <a href="/ps/2310.20473" title="Download PostScript">ps</a>, <a href="/format/2310.20473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Roundtrip Spanners, Emulators, and Directed Girth Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harbuzova%2C+A">Alina Harbuzova</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Ce Jin</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+V+V">Virginia Vassilevska Williams</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zixuan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Roundtrip spanners are the analog of spanners in directed graphs, where the
roundtrip metric is used as a notion of distance. Recent works have shown
existential results of roundtrip spanners nearly matching the undirected case,
but the time complexity for constructing roundtrip spanners is still widely
open.
<br />This paper focuses on developing fast algorithms for roundtrip spanners and
related problems. For any $n$-vertex directed graph $G$ with $m$ edges (with
non-negative edge weights), our results are as follows:
<br />- 3-roundtrip spanner faster than APSP: We give an
$\tilde{O}(m\sqrt{n})$-time algorithm that constructs a roundtrip spanner of
stretch $3$ and optimal size $O(n^{3/2})$. Previous constructions of roundtrip
spanners of the same size either required $\Omega(nm)$ time [Roditty, Thorup,
Zwick SODA'02; Cen, Duan, Gu ICALP'20], or had worse stretch $4$ [Chechik and
Lifshitz SODA'21].
<br />- Optimal roundtrip emulator in dense graphs: For integer $k\ge 3$, we give
an $O(kn^2\log n)$-time algorithm that constructs a roundtrip \emph{emulator}
of stretch $(2k-1)$ and size $O(kn^{1+1/k})$, which is optimal for constant $k$
under Erd\H{o}s' girth conjecture. Previous work of [Thorup and Zwick STOC'01]
implied a roundtrip emulator of the same size and stretch, but it required
$\Omega(nm)$ construction time. Our improved running time is near-optimal for
dense graphs.
<br />- Faster girth approximation in sparse graphs: We give an
$\tilde{O}(mn^{1/3})$-time algorithm that $4$-approximates the girth of a
directed graph. This can be compared with the previous $2$-approximation
algorithm in $\tilde{O}(n^2, m\sqrt{n})$ time by [Chechik and Lifshitz
SODA'21]. In sparse graphs, our algorithm achieves better running time at the
cost of a larger approximation ratio.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20474" title="Abstract">arXiv:2310.20474</a> [<a href="/pdf/2310.20474" title="Download PDF">pdf</a>, <a href="/format/2310.20474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Critical Role of Artificially Intelligent Conversational Chatbot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mostafa%2C+S+A+M">Seraj A. M. Mostafa</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+Z">Md Z. Islam</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+Z">Mohammad Z. Islam</a>, 
<a href="/search/cs?searchtype=author&query=Jeehan%2C+F">Fairose Jeehan</a>, 
<a href="/search/cs?searchtype=author&query=Jafreen%2C+S">Saujanna Jafreen</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+R+U">Raihan U. Islam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of Conversation 2023 position paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Artificially intelligent chatbot, such as ChatGPT, represents a recent and
powerful advancement in the AI domain. Users prefer them for obtaining quick
and precise answers, avoiding the usual hassle of clicking through multiple
links in traditional searches. ChatGPT's conversational approach makes it
comfortable and accessible for finding answers quickly and in an organized
manner. However, it is important to note that these chatbots have limitations,
especially in terms of providing accurate answers as well as ethical concerns.
In this study, we explore various scenarios involving ChatGPT's ethical
implications within academic contexts, its limitations, and the potential
misuse by specific user groups. To address these challenges, we propose
architectural solutions aimed at preventing inappropriate use and promoting
responsible AI interactions.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20475" title="Abstract">arXiv:2310.20475</a> [<a href="/pdf/2310.20475" title="Download PDF">pdf</a>, <a href="/format/2310.20475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linked Papers With Code: The Latest in Machine Learning as an RDF  Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=F%C3%A4rber%2C+M">Michael F&#xe4;rber</a>, 
<a href="/search/cs?searchtype=author&query=Lamprecht%2C+D">David Lamprecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ISWC'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we introduce Linked Papers With Code (LPWC), an RDF knowledge
graph that provides comprehensive, current information about almost 400,000
machine learning publications. This includes the tasks addressed, the datasets
utilized, the methods implemented, and the evaluations conducted, along with
their results. Compared to its non-RDF-based counterpart Papers With Code, LPWC
not only translates the latest advancements in machine learning into RDF
format, but also enables novel ways for scientific impact quantification and
scholarly key content recommendation. LPWC is openly accessible at
https://linkedpaperswithcode.com and is licensed under CC-BY-SA 4.0. As a
knowledge graph in the Linked Open Data cloud, we offer LPWC in multiple
formats, from RDF dump files to a SPARQL endpoint for direct web queries, as
well as a data source with resolvable URIs and links to the data sources
SemOpenAlex, Wikidata, and DBLP. Additionally, we supply knowledge graph
embeddings, enabling LPWC to be readily applied in machine learning
applications.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20476" title="Abstract">arXiv:2310.20476</a> [<a href="/pdf/2310.20476" title="Download PDF">pdf</a>, <a href="/format/2310.20476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Transformer Architecture for Indoor Room Temperature Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clemente%2C+A+V">Alfredo V Clemente</a>, 
<a href="/search/cs?searchtype=author&query=Nocente%2C+A">Alessandro Nocente</a>, 
<a href="/search/cs?searchtype=author&query=Ruocco%2C+M">Massimiliano Ruocco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A thorough regulation of building energy systems translates in relevant
energy savings and in a better comfort for the occupants. Algorithms to predict
the thermal state of a building on a certain time horizon with a good
confidence are essential for the implementation of effective control systems.
This work presents a global Transformer architecture for indoor temperature
forecasting in multi-room buildings, aiming at optimizing energy consumption
and reducing greenhouse gas emissions associated with HVAC systems. Recent
advancements in deep learning have enabled the development of more
sophisticated forecasting models compared to traditional feedback control
systems. The proposed global Transformer architecture can be trained on the
entire dataset encompassing all rooms, eliminating the need for multiple
room-specific models, significantly improving predictive performance, and
simplifying deployment and maintenance. Notably, this study is the first to
apply a Transformer architecture for indoor temperature forecasting in
multi-room buildings. The proposed approach provides a novel solution to
enhance the accuracy and efficiency of temperature forecasting, serving as a
valuable tool to optimize energy consumption and decrease greenhouse gas
emissions in the building sector.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20477" title="Abstract">arXiv:2310.20477</a> [<a href="/pdf/2310.20477" title="Download PDF">pdf</a>, <a href="/format/2310.20477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Practitioner Perspectives On Training Data Attribution  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+E">Elisa Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Kortukov%2C+E">Evgenii Kortukov</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jean Song</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S+J">Seong Joon Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS XAI in Action workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Explainable AI (XAI) aims to provide insight into opaque model reasoning to
humans and as such is an interdisciplinary field by nature. In this paper, we
interviewed 10 practitioners to understand the possible usability of training
data attribution (TDA) explanations and to explore the design space of such an
approach. We confirmed that training data quality is often the most important
factor for high model performance in practice and model developers mainly rely
on their own experience to curate data. End-users expect explanations to
enhance their interaction with the model and do not necessarily prioritise but
are open to training data as a means of explanation. Within our participants,
we found that TDA explanations are not well-known and therefore not used. We
urge the community to focus on the utility of TDA techniques from the
human-machine collaboration perspective and broaden the TDA evaluation to
reflect common use cases in practice.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20478" title="Abstract">arXiv:2310.20478</a> [<a href="/pdf/2310.20478" title="Download PDF">pdf</a>, <a href="/format/2310.20478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Black-boxes: Explainable Deep Learning Models for Patent  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shajalal%2C+M">Md Shajalal</a>, 
<a href="/search/cs?searchtype=author&query=Denef%2C+S">Sebastian Denef</a>, 
<a href="/search/cs?searchtype=author&query=Karim%2C+M+R">Md. Rezaul Karim</a>, 
<a href="/search/cs?searchtype=author&query=Boden%2C+A">Alexander Boden</a>, 
<a href="/search/cs?searchtype=author&query=Stevens%2C+G">Gunnar Stevens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the pre-print of the submitted manuscript on the World Conference on eXplainable Artificial Intelligence (xAI2023), Lisbon, Portugal. The published manuscript can be found here <a href="https://doi.org/10.1007/978-3-031-44067-0_24">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Recent technological advancements have led to a large number of patents in a
diverse range of domains, making it challenging for human experts to analyze
and manage. State-of-the-art methods for multi-label patent classification rely
on deep neural networks (DNNs), which are complex and often considered
black-boxes due to their opaque decision-making processes. In this paper, we
propose a novel deep explainable patent classification framework by introducing
layer-wise relevance propagation (LRP) to provide human-understandable
explanations for predictions. We train several DNN models, including Bi-LSTM,
CNN, and CNN-BiLSTM, and propagate the predictions backward from the output
layer up to the input layer of the model to identify the relevance of words for
individual predictions. Considering the relevance score, we then generate
explanations by visualizing relevant words for the predicted patent class.
Experimental results on two datasets comprising two-million patent texts
demonstrate high performance in terms of various evaluation measures. The
explanations generated for each prediction highlight important relevant words
that align with the predicted class, making the prediction more understandable.
Explainable systems have the potential to facilitate the adoption of complex
AI-enabled methods for patent classification in real-world applications.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20479" title="Abstract">arXiv:2310.20479</a> [<a href="/pdf/2310.20479" title="Download PDF">pdf</a>, <a href="/format/2310.20479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jo%2C+Y">Yohan Jo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xinyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+A">Arijit Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Basiou%2C+N">Nikoletta Basiou</a>, 
<a href="/search/cs?searchtype=author&query=Auvray%2C+V">Vincent Auvray</a>, 
<a href="/search/cs?searchtype=author&query=Malandrakis%2C+N">Nikolaos Malandrakis</a>, 
<a href="/search/cs?searchtype=author&query=Metallinou%2C+A">Angeliki Metallinou</a>, 
<a href="/search/cs?searchtype=author&query=Potamianos%2C+A">Alexandros Potamianos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear in EMNLP-Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While most task-oriented dialogues assume conversations between the agent and
one user at a time, dialogue systems are increasingly expected to communicate
with multiple users simultaneously who make decisions collaboratively. To
facilitate development of such systems, we release the Multi-User MultiWOZ
dataset: task-oriented dialogues among two users and one agent. To collect this
dataset, each user utterance from MultiWOZ 2.2 was replaced with a small chat
between two users that is semantically and pragmatically consistent with the
original user utterance, thus resulting in the same dialogue state and system
response. These dialogues reflect interesting dynamics of collaborative
decision-making in task-oriented scenarios, e.g., social chatter and
deliberation. Supported by this data, we propose the novel task of multi-user
contextual query rewriting: to rewrite a task-oriented chat between two users
as a concise task-oriented query that retains only task-relevant information
and that is directly consumable by the dialogue system. We demonstrate that in
multi-user dialogues, using predicted rewrites substantially improves dialogue
state tracking without modifying existing dialogue systems that are trained for
single-user dialogues. Further, this method surpasses training a medium-sized
model directly on multi-user dialogues and generalizes to unseen domains.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20486" title="Abstract">arXiv:2310.20486</a> [<a href="/pdf/2310.20486" title="Download PDF">pdf</a>, <a href="/ps/2310.20486" title="Download PostScript">ps</a>, <a href="/format/2310.20486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Binary Differential Privacy via Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Torkamani%2C+S">Sahel Torkamani</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimi%2C+J+B">Javad B. Ebrahimi</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+P">Parastoo Sadeghi</a>, 
<a href="/search/cs?searchtype=author&query=D%27Oliveira%2C+R+G+L">Rafael G. L. D&#x27;Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A9dard%2C+M">Muriel M&#xe9;dard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We present the notion of \emph{reasonable utility} for binary mechanisms,
which applies to all utility functions in the literature. This notion induces a
partial ordering on the performance of all binary differentially private (DP)
mechanisms. DP mechanisms that are maximal elements of this ordering are
optimal DP mechanisms for every reasonable utility. By looking at differential
privacy as a randomized graph coloring, we characterize these optimal DP in
terms of their behavior on a certain subset of the boundary datasets we call a
boundary hitting set. In the process of establishing our results, we also
introduce a useful notion that generalizes DP conditions for binary-valued
queries, which we coin as suitable pairs. Suitable pairs abstract away the
algebraic roles of $\varepsilon,\delta$ in the DP framework, making the
derivations and understanding of our proofs simpler. Additionally, the notion
of a suitable pair can potentially capture privacy conditions in frameworks
other than DP and may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20487" title="Abstract">arXiv:2310.20487</a> [<a href="/pdf/2310.20487" title="Download PDF">pdf</a>, <a href="/format/2310.20487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Can Interpret Latent Space of Sequential  Recommender
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiancan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yanchen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jizhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yancheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">An Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiangnan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Sequential recommendation is to predict the next item of interest for a user,
based on her/his interaction history with previous items. In conventional
sequential recommenders, a common approach is to model item sequences using
discrete IDs, learning representations that encode sequential behaviors and
reflect user preferences. Inspired by recent success in empowering large
language models (LLMs) to understand and reason over diverse modality data
(e.g., image, audio, 3D points), a compelling research question arises: ``Can
LLMs understand and work with hidden representations from ID-based sequential
recommenders?''.To answer this, we propose a simple framework, RecInterpreter,
which examines the capacity of open-source LLMs to decipher the representation
space of sequential recommenders. Specifically, with the multimodal pairs (\ie
representations of interaction sequence and text narrations), RecInterpreter
first uses a lightweight adapter to map the representations into the token
embedding space of the LLM. Subsequently, it constructs a sequence-recovery
prompt that encourages the LLM to generate textual descriptions for items
within the interaction sequence. Taking a step further, we propose a
sequence-residual prompt instead, which guides the LLM in identifying the
residual item by contrasting the representations before and after integrating
this residual into the existing sequence. Empirical results showcase that our
RecInterpreter enhances the exemplar LLM, LLaMA, to understand hidden
representations from ID-based sequential recommenders, especially when guided
by our sequence-residual prompts. Furthermore, RecInterpreter enables LLaMA to
instantiate the oracle items generated by generative recommenders like
DreamRec, concreting the item a user would ideally like to interact with next.
Codes are available at https://github.com/YangZhengyi98/RecInterpreter.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20488" title="Abstract">arXiv:2310.20488</a> [<a href="/pdf/2310.20488" title="Download PDF">pdf</a>, <a href="/format/2310.20488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NaijaCoder: Participatory Design for Early Algorithms Education in the  Global South
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alabi%2C+D">Daniel Alabi</a>, 
<a href="/search/cs?searchtype=author&query=Adegbile%2C+A">Atinuke Adegbile</a>, 
<a href="/search/cs?searchtype=author&query=Afuye%2C+L">Lekan Afuye</a>, 
<a href="/search/cs?searchtype=author&query=Abel%2C+P">Philip Abel</a>, 
<a href="/search/cs?searchtype=author&query=Monaco%2C+A">Alida Monaco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the Technical Symposium on Computer Science Education (SIGCSE TS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The majority of Nigerian high schoolers have little to no exposure to the
basics of algorithms and programming. We believe this trajectory should change
as programming offers these students, especially those from indigent
backgrounds, an opportunity to learn profitable skills and ignite their
passions for problem-solving and critical thinking.
<br />NaijaCoder is an organization that is dedicated to organizing a free,
intensive summer program in Nigeria to teach the basics of algorithms and
computer programming to high schoolers. However, the adoption of computer
science curriculum has been especially challenging in countries in the global
south that face unique challenges -- such as unstable power supply, internet
service, and price volatility. We design a curriculum that is more conducive to
the local environment while incorporating rigorous thinking and preparation.
Using basic survey designs, we elicit feedback, from the students, designed to
further improve and iterate on our curriculum.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20490" title="Abstract">arXiv:2310.20490</a> [<a href="/pdf/2310.20490" title="Download PDF">pdf</a>, <a href="/format/2310.20490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-Tailed Learning as Multi-Objective Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weiqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+F">Fan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+F">Fanhua Shang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+L">Liang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wei Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Real-world data is extremely imbalanced and presents a long-tailed
distribution, resulting in models that are biased towards classes with
sufficient samples and perform poorly on rare classes. Recent methods propose
to rebalance classes but they undertake the seesaw dilemma (what is increasing
performance on tail classes may decrease that of head classes, and vice versa).
In this paper, we argue that the seesaw dilemma is derived from gradient
imbalance of different classes, in which gradients of inappropriate classes are
set to important for updating, thus are prone to overcompensation or
undercompensation on tail classes. To achieve ideal compensation, we formulate
the long-tailed recognition as an multi-objective optimization problem, which
fairly respects the contributions of head and tail classes simultaneously. For
efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy to gather
the classes with similar gradient directions, thus approximately make every
update under a Pareto descent direction. Our GBG method drives classes with
similar gradient directions to form more representative gradient and provide
ideal compensation to the tail classes. Moreover, We conduct extensive
experiments on commonly used benchmarks in long-tailed learning and demonstrate
the superiority of our method over existing SOTA methods.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20491" title="Abstract">arXiv:2310.20491</a> [<a href="/pdf/2310.20491" title="Download PDF">pdf</a>, <a href="/format/2310.20491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Decision-Making Using Spatiotemporal Graphs in Connected  Autonomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M+C">Ming C. Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Collaborative decision-making is an essential capability for multi-robot
systems, such as connected vehicles, to collaboratively control autonomous
vehicles in accident-prone scenarios. Under limited communication bandwidth,
capturing comprehensive situational awareness by integrating connected agents'
observation is very challenging. In this paper, we propose a novel
collaborative decision-making method that efficiently and effectively
integrates collaborators' representations to control the ego vehicle in
accident-prone scenarios. Our approach formulates collaborative decision-making
as a classification problem. We first represent sequences of raw observations
as spatiotemporal graphs, which significantly reduce the package size to share
among connected vehicles. Then we design a novel spatiotemporal graph neural
network based on heterogeneous graph learning, which analyzes spatial and
temporal connections of objects in a unified way for collaborative
decision-making. We evaluate our approach using a high-fidelity simulator that
considers realistic traffic, communication bandwidth, and vehicle sensing among
connected autonomous vehicles. The experimental results show that our
representation achieves over 100x reduction in the shared data size that meets
the requirements of communication bandwidth for connected autonomous driving.
In addition, our approach achieves over 30% improvements in driving safety.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20492" title="Abstract">arXiv:2310.20492</a> [<a href="/pdf/2310.20492" title="Download PDF">pdf</a>, <a href="/format/2310.20492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Log-based Anomaly Detection of Enterprise Software: An Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wijesinghe%2C+N">Nadun Wijesinghe</a> (Calgary, Canada), 
<a href="/search/cs?searchtype=author&query=Hemmati%2C+H">Hadi Hemmati</a> (Toronto, Canada)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 14 figures. Submitted to QRS 2023 - 23rd IEEE International Conference on Software Quality, Reliability and Security
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Most enterprise applications use logging as a mechanism to diagnose
anomalies, which could help with reducing system downtime. Anomaly detection
using software execution logs has been explored in several prior studies, using
both classical and deep neural network-based machine learning models. In recent
years, the research has largely focused in using variations of sequence-based
deep neural networks (e.g., Long-Short Term Memory and Transformer-based
models) for log-based anomaly detection on open-source data. However, they have
not been applied in industrial datasets, as often. In addition, the studied
open-source datasets are typically very large in size with logging statements
that do not change much over time, which may not be the case with a dataset
from an industrial service that is relatively new. In this paper, we evaluate
several state-of-the-art anomaly detection models on an industrial dataset from
our research partner, which is much smaller and loosely structured than most
large scale open-source benchmark datasets. Results show that while all models
are capable of detecting anomalies, certain models are better suited for
less-structured datasets. We also see that model effectiveness changes when a
common data leak associated with a random train-test split in some prior work
is removed. A qualitative study of the defects' characteristics identified by
the developers on the industrial dataset further shows strengths and weaknesses
of the models in detecting different types of anomalies. Finally, we explore
the effect of limited training data by gradually increasing the training set
size, to evaluate if the model effectiveness does depend on the training set
size.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20493" title="Abstract">arXiv:2310.20493</a> [<a href="/pdf/2310.20493" title="Download PDF">pdf</a>, <a href="/format/2310.20493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Requirement falsification for cyber-physical systems using generative  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peltom%C3%A4ki%2C+J">Jarkko Peltom&#xe4;ki</a>, 
<a href="/search/cs?searchtype=author&query=Porres%2C+I">Ivan Porres</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 5 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">We present the OGAN algorithm for automatic requirement falsification of
cyber-physical systems. System inputs and output are represented as piecewise
constant signals over time while requirements are expressed in signal temporal
logic. OGAN can find inputs that are counterexamples for the safety of a system
revealing design, software, or hardware defects before the system is taken into
operation. The OGAN algorithm works by training a generative machine learning
model to produce such counterexamples. It executes tests atomically and does
not require any previous model of the system under test. We evaluate OGAN using
the ARCH-COMP benchmark problems, and the experimental results show that
generative models are a viable method for requirement falsification. OGAN can
be applied to new systems with little effort, has few requirements for the
system under test, and exhibits state-of-the-art CPS falsification efficiency
and effectiveness.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20494" title="Abstract">arXiv:2310.20494</a> [<a href="/pdf/2310.20494" title="Download PDF">pdf</a>, <a href="/ps/2310.20494" title="Download PostScript">ps</a>, <a href="/format/2310.20494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Transformer-Based Model With Self-Distillation for Multimodal Emotion  Recognition in Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongfei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yijia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures. Accepted by IEEE Transactions on Multimedia (TMM)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Multimedia (Early Access), 27 April 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Emotion recognition in conversations (ERC), the task of recognizing the
emotion of each utterance in a conversation, is crucial for building empathetic
machines. Existing studies focus mainly on capturing context- and
speaker-sensitive dependencies on the textual modality but ignore the
significance of multimodal information. Different from emotion recognition in
textual conversations, capturing intra- and inter-modal interactions between
utterances, learning weights between different modalities, and enhancing modal
representations play important roles in multimodal ERC. In this paper, we
propose a transformer-based model with self-distillation (SDT) for the task.
The transformer-based model captures intra- and inter-modal interactions by
utilizing intra- and inter-modal transformers, and learns weights between
modalities dynamically by designing a hierarchical gated fusion strategy.
Furthermore, to learn more expressive modal representations, we treat soft
labels of the proposed model as extra training supervision. Specifically, we
introduce self-distillation to transfer knowledge of hard and soft labels from
the proposed model to each modality. Experiments on IEMOCAP and MELD datasets
demonstrate that SDT outperforms previous state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20496" title="Abstract">arXiv:2310.20496</a> [<a href="/pdf/2310.20496" title="Download PDF">pdf</a>, <a href="/format/2310.20496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BasisFormer: Attention-based Time Series Forecasting with Learnable and  Interpretable Basis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+Z">Zelin Ni</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shizhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weiyao Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023(poster)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Bases have become an integral part of modern deep learning-based models for
time series forecasting due to their ability to act as feature extractors or
future references. To be effective, a basis must be tailored to the specific
set of time series data and exhibit distinct correlation with each time series
within the set. However, current state-of-the-art methods are limited in their
ability to satisfy both of these requirements simultaneously. To address this
challenge, we propose BasisFormer, an end-to-end time series forecasting
architecture that leverages learnable and interpretable bases. This
architecture comprises three components: First, we acquire bases through
adaptive self-supervised learning, which treats the historical and future
sections of the time series as two distinct views and employs contrastive
learning. Next, we design a Coef module that calculates the similarity
coefficients between the time series and bases in the historical view via
bidirectional cross-attention. Finally, we present a Forecast module that
selects and consolidates the bases in the future view based on the similarity
coefficients, resulting in accurate future predictions. Through extensive
experiments on six datasets, we demonstrate that BasisFormer outperforms
previous state-of-the-art methods by 11.04\% and 15.78\% respectively for
univariate and multivariate forecasting tasks. Code is available at:
\url{https://github.com/nzl5116190/Basisformer}
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20497" title="Abstract">arXiv:2310.20497</a> [<a href="/pdf/2310.20497" title="Download PDF">pdf</a>, <a href="/ps/2310.20497" title="Download PostScript">ps</a>, <a href="/format/2310.20497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the matrix code of quadratic relationships for a Goppa code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mora%2C+R">Rocco Mora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In this article, we continue the analysis started in \cite{CMT23} for the
matrix code of quadratic relationships associated with a Goppa code. We provide
new sparse and low-rank elements in the matrix code and categorize them
according to their shape. Thanks to this description, we prove that the set of
rank 2 matrices in the matrix codes associated with square-free binary Goppa
codes, i.e. those used in Classic McEiece, is much larger than what is
expected, at least in the case where the Goppa polynomial degree is 2. We build
upon the algebraic determinantal modeling introduced in \cite{CMT23} to derive
a structural attack on these instances. Our method can break in just a few
seconds some recent challenges about key-recovery attacks on the McEliece
cryptosystem, consistently reducing their estimated security level. We also
provide a general method, valid for any Goppa polynomial degree, to transform a
generic pair of support and multiplier into a pair of support and Goppa
polynomial.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20498" title="Abstract">arXiv:2310.20498</a> [<a href="/pdf/2310.20498" title="Download PDF">pdf</a>, <a href="/format/2310.20498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Learning of Continuous Data by Tensor Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meiburg%2C+A">Alex Meiburg</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+J">Jacob Miller</a>, 
<a href="/search/cs?searchtype=author&query=Tihon%2C+R">Rapha&#xeb;lle Tihon</a>, 
<a href="/search/cs?searchtype=author&query=Rabusseau%2C+G">Guillaume Rabusseau</a>, 
<a href="/search/cs?searchtype=author&query=Perdomo-Ortiz%2C+A">Alejandro Perdomo-Ortiz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistical Mechanics (cond-mat.stat-mech); Quantum Physics (quant-ph); Machine Learning (stat.ML)

</div>
<p class="mathjax">Beyond their origin in modeling many-body quantum systems, tensor networks
have emerged as a promising class of models for solving machine learning
problems, notably in unsupervised generative learning. While possessing many
desirable features arising from their quantum-inspired nature, tensor network
generative models have previously been largely restricted to binary or
categorical data, limiting their utility in real-world modeling problems. We
overcome this by introducing a new family of tensor network generative models
for continuous data, which are capable of learning from distributions
containing continuous random variables. We develop our method in the setting of
matrix product states, first deriving a universal expressivity theorem proving
the ability of this model family to approximate any reasonably smooth
probability density function with arbitrary precision. We then benchmark the
performance of this model on several synthetic and real-world datasets, finding
that the model learns and generalizes well on distributions of continuous and
discrete variables. We develop methods for modeling different data domains, and
introduce a trainable compression layer which is found to increase model
performance given limited memory or computational resources. Overall, our
methods give important theoretical and empirical evidence of the efficacy of
quantum-inspired methods for the rapidly growing field of generative learning.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20499" title="Abstract">arXiv:2310.20499</a> [<a href="/pdf/2310.20499" title="Download PDF">pdf</a>, <a href="/format/2310.20499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Word Guessing Games to Assess the Intelligence of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+T">Tian Liang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiwei He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jen-tes Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenxiang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xing Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The automatic evaluation of LLM-based agent intelligence is critical in
developing advanced LLM-based agents. Although considerable effort has been
devoted to developing human-annotated evaluation datasets, such as AlpacaEval,
existing techniques are costly, time-consuming, and lack adaptability. In this
paper, inspired by the popular language game ``Who is Spy'', we propose to use
the word guessing game to assess the intelligence performance of LLMs. Given a
word, the LLM is asked to describe the word and determine its identity (spy or
not) based on its and other players' descriptions. Ideally, an advanced agent
should possess the ability to accurately describe a given word using an
aggressive description while concurrently maximizing confusion in the
conservative description, enhancing its participation in the game. To this end,
we first develop DEEP to evaluate LLMs' expression and disguising abilities.
DEEP requires LLM to describe a word in aggressive and conservative modes. We
then introduce SpyGame, an interactive multi-agent framework designed to assess
LLMs' intelligence through participation in a competitive language-based board
game. Incorporating multi-agent interaction, SpyGame requires the target LLM to
possess linguistic skills and strategic thinking, providing a more
comprehensive evaluation of LLMs' human-like cognitive abilities and
adaptability in complex communication situations. The proposed evaluation
framework is very easy to implement. We collected words from multiple sources,
domains, and languages and used the proposed evaluation framework to conduct
experiments. Extensive experiments demonstrate that the proposed DEEP and
SpyGame effectively evaluate the capabilities of various LLMs, capturing their
ability to adapt to novel situations and engage in strategic communication.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20501" title="Abstract">arXiv:2310.20501</a> [<a href="/pdf/2310.20501" title="Download PDF">pdf</a>, <a href="/format/2310.20501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs may Dominate Information Access: Neural Retrievers are Biased  Towards LLM-Generated Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+S">Sunhao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuqi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaolin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jun Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Recently, the emergence of large language models (LLMs) has revolutionized
the paradigm of information retrieval (IR) applications, especially in web
search. With their remarkable capabilities in generating human-like texts, LLMs
have created enormous texts on the Internet. As a result, IR systems in the
LLMs era are facing a new challenge: the indexed documents now are not only
written by human beings but also automatically generated by the LLMs. How these
LLM-generated documents influence the IR systems is a pressing and still
unexplored question. In this work, we conduct a quantitative evaluation of
different IR models in scenarios where both human-written and LLM-generated
texts are involved. Surprisingly, our findings indicate that neural retrieval
models tend to rank LLM-generated documents higher.We refer to this category of
biases in neural retrieval models towards the LLM-generated text as the
\textbf{source bias}. Moreover, we discover that this bias is not confined to
the first-stage neural retrievers, but extends to the second-stage neural
re-rankers. Then, we provide an in-depth analysis from the perspective of text
compression and observe that neural models can better understand the semantic
information of LLM-generated text, which is further substantiated by our
theoretical analysis.We also discuss the potential server concerns stemming
from the observed source bias and hope our findings can serve as a critical
wake-up call to the IR community and beyond. To facilitate future explorations
of IR in the LLM era, the constructed two new benchmarks and codes will later
be available at \url{https://github.com/KID-22/LLM4IR-Bias}.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20504" title="Abstract">arXiv:2310.20504</a> [<a href="/pdf/2310.20504" title="Download PDF">pdf</a>, <a href="/ps/2310.20504" title="Download PostScript">ps</a>, <a href="/format/2310.20504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SumComp: Coding for Digital Over-the-Air Computation via the Ring of  Integers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razavikia%2C+S">Saeed Razavikia</a>, 
<a href="/search/cs?searchtype=author&query=Da+Silva+J%C3%BAnior%2C+J+M+B">Jos&#xe9; Mairton Barros Da Silva J&#xfa;nior</a>, 
<a href="/search/cs?searchtype=author&query=Fischione%2C+C">Carlo Fischione</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Communication and computation are traditionally treated as separate entities,
allowing for individual optimizations. However, many applications focus on
local information's functionality rather than the information itself. For such
cases, harnessing interference for computation in a multiple access channel
through digital over-the-air computation can notably increase the computation,
as established by the ChannelComp method. However, the coding scheme originally
proposed in ChannelComp may suffer from high computational complexity because
it is general and is not optimized for specific modulation categories.
Therefore, this study considers a specific category of digital modulations for
over-the-air computations, QAM and PAM, for which we introduce a novel coding
scheme called SumComp. Furthermore, we derive an MSE analysis for SumComp
coding in the computation of the arithmetic mean function and establish an
upper bound on the MAE for a set of nomographic functions. Simulation results
affirm the superior performance of SumComp coding compared to traditional
analog over-the-air computation and the original coding in ChannelComp
approaches regarding both MSE and MAE over a noisy multiple access channel.
Specifically, SumComp coding shows approximately $10$ dB improvements for
computing arithmetic and geometric mean on the normalized MSE for low noise
scenarios.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20515" title="Abstract">arXiv:2310.20515</a> [<a href="/pdf/2310.20515" title="Download PDF">pdf</a>, <a href="/format/2310.20515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRa Multi-Hop Networks for Monitoring Underground Mining Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scalambrin%2C+L">Luca Scalambrin</a>, 
<a href="/search/cs?searchtype=author&query=Zanella%2C+A">Andrea Zanella</a>, 
<a href="/search/cs?searchtype=author&query=Vilajosana%2C+X">Xavier Vilajosana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Internet of Things applications have gained widespread recognition for their
efficacy in typical scenarios, such as smart cities and smart healthcare.
Nonetheless, there exist numerous unconventional situations where IoT
technologies have not yet been massively applied, though they can be extremely
useful. One of such domains is the underground mining sector, where enhancing
automation monitoring through wireless communications is of essential
significance. In this paper, we focus on the development, implementation, and
evaluation of a LoRa-based multi-hop network tailored specifically for
monitoring underground mining environments, where data traffic is sporadic, but
energy efficiency is of paramount importance. We hence define a synchronization
framework that makes it possible for the nodes to sleep for most of the time,
waking up only when they need to exchange traffic. Notably, our network
achieves a sub 40us proven synchronization accuracy between parent-child pairs
with minimum overhead for diverse topologies, rendering it highly viable for
subterranean operations. Furthermore, for proper network dimensioning, we model
the interplay between network's throughput, frame size, and sampling periods of
potential applications. Moreover, we propose a model to estimate devices' duty
cycle based on their position within the multi-hop network, along with
empirical observations for its validation. The proposed models make it possible
to optimize the network's performance to meet the specific demands that can
arise from the different subterranean use cases, in which robustness, low power
operation, and compliance with radio-frequency regulations are key requirements
that must be met.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20518" title="Abstract">arXiv:2310.20518</a> [<a href="/pdf/2310.20518" title="Download PDF">pdf</a>, <a href="/format/2310.20518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving RRT for Automated Parking in Real-world Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vlasak%2C+J">Jiri Vlasak</a>, 
<a href="/search/cs?searchtype=author&query=Sojka%2C+M">Michal Sojka</a>, 
<a href="/search/cs?searchtype=author&query=Hanz%C3%A1lek%2C+Z">Zden&#x11b;k Hanz&#xe1;lek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 14 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Automated parking is a self-driving feature that has been in cars for several
years. Parking assistants in currently sold cars fail to park in more complex
real-world scenarios and require the driver to move the car to an expected
starting position before the assistant is activated. We overcome these
limitations by proposing a planning algorithm consisting of two stages: (1) a
geometric planner for maneuvering inside the parking slot and (2) a
Rapidly-exploring Random Trees (RRT)-based planner that finds a collision-free
path from the initial position to the slot entry. Evaluation of computational
experiments demonstrates that improvements over commonly used RRT extensions
reduce the parking path cost by 21 % and reduce the computation time by 79.5 %.
The suitability of the algorithm for real-world parking scenarios was verified
in physical experiments with Porsche Cayenne.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20524" title="Abstract">arXiv:2310.20524</a> [<a href="/pdf/2310.20524" title="Download PDF">pdf</a>, <a href="/format/2310.20524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Aytijhya Saha</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+N+R">Nikhil R. Pal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we present a novel embedded feature selection method based on
a Multi-layer Perceptron (MLP) network and generalize it for group-feature or
sensor selection problems, which can control the level of redundancy among the
selected features or groups. Additionally, we have generalized the group lasso
penalty for feature selection to encompass a mechanism for selecting valuable
group features while simultaneously maintaining a control over redundancy. We
establish the monotonicity and convergence of the proposed algorithm, with a
smoothed version of the penalty terms, under suitable assumptions. Experimental
results on several benchmark datasets demonstrate the promising performance of
the proposed methodology for both feature selection and group feature selection
over some state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20533" title="Abstract">arXiv:2310.20533</a> [<a href="/pdf/2310.20533" title="Download PDF">pdf</a>, <a href="/format/2310.20533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algebraic hierarchical locally recoverable codes with nested affine  subspace recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haymaker%2C+K">Kathryn Haymaker</a>, 
<a href="/search/cs?searchtype=author&query=Malmskog%2C+B">Beth Malmskog</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+G+L">Gretchen L. Matthews</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Algebraic Geometry (math.AG)

</div>
<p class="mathjax">Codes with locality, also known as locally recoverable codes, allow for
recovery of erasures using proper subsets of other coordinates. Theses subsets
are typically of small cardinality to promote recovery using limited network
traffic and other resources. Hierarchical locally recoverable codes allow for
recovery of erasures using sets of other symbols whose sizes increase as needed
to allow for recovery of more symbols. In this paper, we construct codes with
hierarchical locality from a geometric perspective, using fiber products of
curves. We demonstrate how the constructed hierarchical codes can be viewed as
punctured subcodes of Reed-Muller codes. This point of view provides natural
structures for local recovery at each level in the hierarchy.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20536" title="Abstract">arXiv:2310.20536</a> [<a href="/pdf/2310.20536" title="Download PDF">pdf</a>, <a href="/ps/2310.20536" title="Download PostScript">ps</a>, <a href="/format/2310.20536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Dictionary with Subconstant Wasted Bits per Key
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jingxun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Huacheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Renfei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages; SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Dictionaries have been one of the central questions in data structures. A
dictionary data structure maintains a set of key-value pairs under insertions
and deletions such that given a query key, the data structure efficiently
returns its value. The state-of-the-art dictionaries [Bender, Farach-Colton,
Kuszmaul, Kuszmaul, Liu 2022] store $n$ key-value pairs with only $O(n
\log^{(k)} n)$ bits of redundancy, and support all operations in $O(k)$ time,
for $k \leq \log^* n$. It was recently shown to be optimal [Li, Liang, Yu, Zhou
2023b].
<br />In this paper, we study the regime where the redundant bits is $R=o(n)$, and
show that when $R$ is at least $n/\text{poly}\log n$, all operations can be
supported in $O(\log^* n + \log (n/R))$ time, matching the lower bound in this
regime [Li, Liang, Yu, Zhou 2023b]. We present two data structures based on
which range $R$ is in. The data structure for $R&lt;n/\log^{0.1} n$ utilizes a
generalization of adapters studied in [Berger, Kuszmaul, Polak, Tidor, Wein
2022] and [Li, Liang, Yu, Zhou 2023a]. The data structure for $R \geq
n/\log^{0.1} n$ is based on recursively hashing into buckets with logarithmic
sizes.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20539" title="Abstract">arXiv:2310.20539</a> [<a href="/pdf/2310.20539" title="Download PDF">pdf</a>, <a href="/format/2310.20539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Computational Lens: from Quantum Physics to Neuroscience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+C">Chi-Ning Chou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD thesis, Harvard University, Cambridge, Massachusetts, USA. 2023. Some chapters report joint work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Neurons and Cognition (q-bio.NC); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Two transformative waves of computing have redefined the way we approach
science. The first wave came with the birth of the digital computer, which
enabled scientists to numerically simulate their models and analyze massive
datasets. This technological breakthrough led to the emergence of many
sub-disciplines bearing the prefix "computational" in their names. Currently,
we are in the midst of the second wave, marked by the remarkable advancements
in artificial intelligence. From predicting protein structures to classifying
galaxies, the scope of its applications is vast, and there can only be more
awaiting us on the horizon.
<br />While these two waves influence scientific methodology at the instrumental
level, in this dissertation, I will present the computational lens in science,
aiming at the conceptual level. Specifically, the central thesis posits that
computation serves as a convenient and mechanistic language for understanding
and analyzing information processing systems, offering the advantages of
composability and modularity.
<br />This dissertation begins with an illustration of the blueprint of the
computational lens, supported by a review of relevant previous work.
Subsequently, I will present my own works in quantum physics and neuroscience
as concrete examples. In the concluding chapter, I will contemplate the
potential of applying the computational lens across various scientific fields,
in a way that can provide significant domain insights, and discuss potential
future directions.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20545" title="Abstract">arXiv:2310.20545</a> [<a href="/pdf/2310.20545" title="Download PDF">pdf</a>, <a href="/format/2310.20545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-task learning of convex combinations of forecasting models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Felici%2C+G">Giovanni Felici</a>, 
<a href="/search/cs?searchtype=author&query=Sudoso%2C+A+M">Antonio M. Sudoso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Forecast combination involves using multiple forecasts to create a single,
more accurate prediction. Recently, feature-based forecasting has been employed
to either select the most appropriate forecasting models or to learn the
weights of their convex combination. In this paper, we present a multi-task
learning methodology that simultaneously addresses both problems. This approach
is implemented through a deep neural network with two branches: the regression
branch, which learns the weights of various forecasting methods by minimizing
the error of combined forecasts, and the classification branch, which selects
forecasting methods with an emphasis on their diversity. To generate training
labels for the classification task, we introduce an optimization-driven
approach that identifies the most appropriate methods for a given time series.
The proposed approach elicits the essential role of diversity in feature-based
forecasting and highlights the interplay between model combination and model
selection when learning forecasting ensembles. Experimental results on a large
set of series from the M4 competition dataset show that our proposal enhances
point forecast accuracy compared to state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20550" title="Abstract">arXiv:2310.20550</a> [<a href="/pdf/2310.20550" title="Download PDF">pdf</a>, <a href="/format/2310.20550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CapsFusion: Rethinking Image-Text Data at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qiying Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Quan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaosong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yufeng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinlong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingjing Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large multimodal models demonstrate remarkable generalist ability to perform
diverse multimodal tasks in a zero-shot manner. Large-scale web-based
image-text pairs contribute fundamentally to this success, but suffer from
excessive noise. Recent studies use alternative captions synthesized by
captioning models and have achieved notable benchmark performance. However, our
experiments reveal significant Scalability Deficiency and World Knowledge Loss
issues in models trained with synthetic captions, which have been largely
obscured by their initial benchmark success. Upon closer examination, we
identify the root cause as the overly-simplified language structure and lack of
knowledge details in existing synthetic captions. To provide higher-quality and
more scalable multimodal pretraining data, we propose CapsFusion, an advanced
framework that leverages large language models to consolidate and refine
information from both web-based image-text pairs and synthetic captions.
Extensive experiments show that CapsFusion captions exhibit remarkable
all-round superiority over existing captions in terms of model performance
(e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample
efficiency (requiring 11-16 times less computation than baselines), world
knowledge depth, and scalability. These effectiveness, efficiency and
scalability advantages position CapsFusion as a promising candidate for future
scaling of LMM training.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20552" title="Abstract">arXiv:2310.20552</a> [<a href="/pdf/2310.20552" title="Download PDF">pdf</a>, <a href="/ps/2310.20552" title="Download PostScript">ps</a>, <a href="/format/2310.20552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-preserving design of graph neural networks with applications to  vertical federated learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruofan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Lingjuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaolong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+X">Xiuquan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xinyi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The paradigm of vertical federated learning (VFL), where institutions
collaboratively train machine learning models via combining each other's local
feature or label information, has achieved great success in applications to
financial risk management (FRM). The surging developments of graph
representation learning (GRL) have opened up new opportunities for FRM
applications under FL via efficiently utilizing the graph-structured data
generated from underlying transaction networks. Meanwhile, transaction
information is often considered highly sensitive. To prevent data leakage
during training, it is critical to develop FL protocols with formal privacy
guarantees. In this paper, we present an end-to-end GRL framework in the VFL
setting called VESPER, which is built upon a general privatization scheme
termed perturbed message passing (PMP) that allows the privatization of many
popular graph neural architectures.Based on PMP, we discuss the strengths and
weaknesses of specific design choices of concrete graph neural architectures
and provide solutions and improvements for both dense and sparse graphs.
Extensive empirical evaluations over both public datasets and an industry
dataset demonstrate that VESPER is capable of training high-performance GNN
models over both sparse and dense graphs under reasonable privacy budgets.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20554" title="Abstract">arXiv:2310.20554</a> [<a href="/pdf/2310.20554" title="Download PDF">pdf</a>, <a href="/format/2310.20554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Delay-Robust Multimodal Journey Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bez%2C+D">Dominik Bez</a>, 
<a href="/search/cs?searchtype=author&query=Sauer%2C+J">Jonas Sauer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study journey planning in multimodal networks consisting of public transit
plus an unrestricted transfer mode (e.g., walking or cycling). In order to
provide good results in practice, algorithms must account for vehicle delays.
Delay-responsive algorithms receive a continuous stream of delay updates and
must return optimal journeys in the currently known delay scenario. Updates are
incorporated in an update phase, which must be fast (e.g., a few seconds). The
fastest known approach for multimodal journey planning is ULTRA, which
precomputes shortcuts representing transfers between vehicles. This allows
query algorithms to find Pareto-optimal journeys regarding arrival time and the
number of public transit trips without any performance loss compared to pure
public transit networks. However, the precomputation phase does not account for
delays and is too slow to rerun during the update phase. We present
Delay-ULTRA, a delay-responsive variant of ULTRA. Since accounting for all
theoretically possible delays would yield an impractically large set of
shortcuts, our approach precomputes shortcuts that are provably sufficient as
long as delays do not exceed a configurable limit (e.g., 5 minutes). To handle
delays above the limit (which are less frequent in practice), we propose a
heuristic search for missing shortcuts that is fast enough to be run during the
update phase. Our experimental evaluation on real-world data shows that
Delay-ULTRA fails to find less than 0.02% of optimal journeys on metropolitan
and mid-sized country networks, and 0.16% on the much larger Germany network.
Considering that the available delay information in realistic applications is
never perfectly accurate, these error rates are negligible. Query speed is at
most twice as slow as ULTRA without delay information, and up to 8 times faster
than the fastest exact algorithm, which does not require a preprocessing phase.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20558" title="Abstract">arXiv:2310.20558</a> [<a href="/pdf/2310.20558" title="Download PDF">pdf</a>, <a href="/format/2310.20558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Token Barrier: Chunking and Convolution for Efficient Long  Text Classification with BERT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A">Aman Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Milios%2C+E">Evangelos Milios</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures, submitted to NeurIPS 23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformer-based models, specifically BERT, have propelled research in
various NLP tasks. However, these models are limited to a maximum token limit
of 512 tokens. Consequently, this makes it non-trivial to apply it in a
practical setting with long input. Various complex methods have claimed to
overcome this limit, but recent research questions the efficacy of these models
across different classification tasks. These complex architectures evaluated on
carefully curated long datasets perform at par or worse than simple baselines.
In this work, we propose a relatively simple extension to vanilla BERT
architecture called ChunkBERT that allows finetuning of any pretrained models
to perform inference on arbitrarily long text. The proposed method is based on
chunking token representations and CNN layers, making it compatible with any
pre-trained BERT. We evaluate chunkBERT exclusively on a benchmark for
comparing long-text classification models across a variety of tasks (including
binary classification, multi-class classification, and multi-label
classification). A BERT model finetuned using the ChunkBERT method performs
consistently across long samples in the benchmark while utilizing only a
fraction (6.25\%) of the original memory footprint. These findings suggest that
efficient finetuning and inference can be achieved through simple modifications
to pre-trained BERT models.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20561" title="Abstract">arXiv:2310.20561</a> [<a href="/pdf/2310.20561" title="Download PDF">pdf</a>, <a href="/format/2310.20561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive Control for Autonomous Driving with Uncertain, Multi-modal  Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nair%2C+S+H">Siddharth H. Nair</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hotae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Joa%2C+E">Eunhyek Joa</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+H+E">H. Eric Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Borrelli%2C+F">Francesco Borrelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first three authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose a Stochastic MPC (SMPC) formulation for path planning with
autonomous vehicles in scenarios involving multiple agents with multi-modal
predictions. The multi-modal predictions capture the uncertainty of urban
driving in distinct modes/maneuvers (e.g., yield, keep speed) and driving
trajectories (e.g., speed, turning radius), which are incorporated for
multi-modal collision avoidance chance constraints for path planning. In the
presence of multi-modal uncertainties, it is challenging to reliably compute
feasible path planning solutions at real-time frequencies ($\geq$ 10 Hz). Our
main technological contribution is a convex SMPC formulation that
simultaneously (1) optimizes over parameterized feedback policies and (2)
allocates risk levels for each mode of the prediction. The use of feedback
policies and risk allocation enhances the feasibility and performance of the
SMPC formulation against multi-modal predictions with large uncertainty. We
evaluate our approach via simulations and road experiments with a full-scale
vehicle interacting in closed-loop with virtual vehicles. We consider distinct,
multi-modal driving scenarios: 1) Negotiating a traffic light and a fast,
tailgating agent, 2) Executing an unprotected left turn at a traffic
intersection, and 3) Changing lanes in the presence of multiple agents. For all
of these scenarios, our approach reliably computes multi-modal solutions to the
path-planning problem at real-time frequencies.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20563" title="Abstract">arXiv:2310.20563</a> [<a href="/pdf/2310.20563" title="Download PDF">pdf</a>, <a href="/ps/2310.20563" title="Download PostScript">ps</a>, <a href="/format/2310.20563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taking control: Policies to address extinction risks from AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miotti%2C+A">Andrea Miotti</a>, 
<a href="/search/cs?searchtype=author&query=Wasil%2C+A">Akash Wasil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper provides policy recommendations to reduce extinction risks from
advanced artificial intelligence (AI). First, we briefly provide background
information about extinction risks from AI. Second, we argue that voluntary
commitments from AI companies would be an inappropriate and insufficient
response. Third, we describe three policy proposals that would meaningfully
address the threats from advanced AI: (1) establishing a Multinational AGI
Consortium to enable democratic oversight of advanced AI (MAGIC), (2)
implementing a global cap on the amount of computing power used to train an AI
system (global compute cap), and (3) requiring affirmative safety evaluations
to ensure that risks are kept below acceptable levels (gating critical
experiments). MAGIC would be a secure, safety-focused, internationally-governed
institution responsible for reducing risks from advanced AI and performing
research to safely harness the benefits of AI. MAGIC would also maintain
emergency response infrastructure (kill switch) to swiftly halt AI development
or withdraw model deployment in the event of an AI-related emergency. The
global compute cap would end the corporate race toward dangerous AI systems
while enabling the vast majority of AI innovation to continue unimpeded. Gating
critical experiments would ensure that companies developing powerful AI systems
are required to present affirmative evidence that these models keep extinction
risks below an acceptable threshold. After describing these recommendations, we
propose intermediate steps that the international community could take to
implement these proposals and lay the groundwork for international coordination
around advanced AI.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20567" title="Abstract">arXiv:2310.20567</a> [<a href="/pdf/2310.20567" title="Download PDF">pdf</a>, <a href="/format/2310.20567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-shot backpropagation for multi-step prediction in physics-based  system identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Donati%2C+C">Cesare Donati</a>, 
<a href="/search/eess?searchtype=author&query=Mammarella%2C+M">Martina Mammarella</a>, 
<a href="/search/eess?searchtype=author&query=Dabbene%2C+F">Fabrizio Dabbene</a>, 
<a href="/search/eess?searchtype=author&query=Novara%2C+C">Carlo Novara</a>, 
<a href="/search/eess?searchtype=author&query=Lagoa%2C+C">Constantino Lagoa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The aim of this paper is to present a novel general framework for the
identification of possibly interconnected systems, while preserving their
physical properties and providing accuracy in multi-step prediction. An
analytical and recursive algorithm for the gradient computation of the
multi-step loss function based on backpropagation is introduced, providing
physical and structural insight directly into the learning algorithm. As a case
study, the proposed approach is tested for estimating the inertia matrix of a
space debris starting from state observations.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20568" title="Abstract">arXiv:2310.20568</a> [<a href="/pdf/2310.20568" title="Download PDF">pdf</a>, <a href="/format/2310.20568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Learning for LTI Systems with Stability Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ghanipoor%2C+F">Farhad Ghanipoor</a>, 
<a href="/search/eess?searchtype=author&query=Murguia%2C+C">Carlos Murguia</a>, 
<a href="/search/eess?searchtype=author&query=Esfahani%2C+P+M">Peyman Mohajerin Esfahani</a>, 
<a href="/search/eess?searchtype=author&query=van+de+Wouw%2C+N">Nathan van de Wouw</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We present a framework for learning of modeling uncertainties in Linear Time
Invariant (LTI) systems. We propose a methodology to extend the dynamics of an
LTI (without uncertainty) with an uncertainty model, based on measured data, to
improve the predictive capacity of the model in the input-output sense. The
proposed framework guarantees stability of the extended model. To achieve this,
two semi-definite programs are provided that allow obtaining optimal
uncertainty model parameters, given state and uncertainty data. To obtain this
data from available input-output trajectory data, we introduce a filter in
which an internal model of uncertainty is proposed. This filter is also
designed via a semi-definite program with guaranteed robustness with respect to
uncertainty model mismatches, disturbances, and noise. Numerical simulations
are presented to illustrate the effectiveness and practicality of the proposed
methodology in improving model accuracy, while warranting model stability.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20574" title="Abstract">arXiv:2310.20574</a> [<a href="/pdf/2310.20574" title="Download PDF">pdf</a>, <a href="/format/2310.20574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-Theoretic Trust Regions for Stochastic Gradient-Based  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dahlinger%2C+P">Philipp Dahlinger</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+P">Philipp Becker</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCttenrauch%2C+M">Maximilian H&#xfc;ttenrauch</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+G">Gerhard Neumann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Stochastic gradient-based optimization is crucial to optimize neural
networks. While popular approaches heuristically adapt the step size and
direction by rescaling gradients, a more principled approach to improve
optimizers requires second-order information. Such methods precondition the
gradient using the objective's Hessian. Yet, computing the Hessian is usually
expensive and effectively using second-order information in the stochastic
gradient setting is non-trivial. We propose using Information-Theoretic Trust
Region Optimization (arTuRO) for improved updates with uncertain second-order
information. By modeling the network parameters as a Gaussian distribution and
using a Kullback-Leibler divergence-based trust region, our approach takes
bounded steps accounting for the objective's curvature and uncertainty in the
parameters. Before each update, it solves the trust region problem for an
optimal step size, resulting in a more stable and faster optimization process.
We approximate the diagonal elements of the Hessian from stochastic gradients
using a simple recursive least squares approach, constructing a model of the
expected Hessian over time using only first-order information. We show that
arTuRO combines the fast convergence of adaptive moment-based optimization with
the generalization capabilities of SGD.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20577" title="Abstract">arXiv:2310.20577</a> [<a href="/pdf/2310.20577" title="Download PDF">pdf</a>, <a href="/format/2310.20577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offloading Real-Time Tasks in IIoT Environments under Consideration of  Networking Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behnke%2C+I">Ilja Behnke</a>, 
<a href="/search/cs?searchtype=author&query=Wiesner%2C+P">Philipp Wiesner</a>, 
<a href="/search/cs?searchtype=author&query=Voelker%2C+P">Paul Voelker</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+O">Odej Kao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2nd International Workshop on Middleware for the Edge (MiddleWEdge '23). 2023. ACM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Offloading is a popular way to overcome the resource and power constraints of
networked embedded devices, which are increasingly found in industrial
environments. It involves moving resource-intensive computational tasks to a
more powerful device on the network, often in close proximity to enable
wireless communication. However, many Industrial Internet of Things (IIoT)
applications have real-time constraints. Offloading such tasks over a wireless
network with latency uncertainties poses new challenges.
<br />In this paper, we aim to better understand these challenges by proposing a
system architecture and scheduler for real-time task offloading in wireless
IIoT environments. Based on a prototype, we then evaluate different system
configurations and discuss their trade-offs and implications. Our design showed
to prevent deadline misses under high load and network uncertainties and was
able to outperform a reference scheduler in terms of successful task
throughput. Under heavy task load, where the reference scheduler had a success
rate of 5%, our design achieved a success rate of 60%.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20581" title="Abstract">arXiv:2310.20581</a> [<a href="/pdf/2310.20581" title="Download PDF">pdf</a>, <a href="/format/2310.20581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Gradient Descent for Gaussian Processes Done Right
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J+A">Jihao Andreas Lin</a>, 
<a href="/search/cs?searchtype=author&query=Padhy%2C+S">Shreyas Padhy</a>, 
<a href="/search/cs?searchtype=author&query=Antor%C3%A1n%2C+J">Javier Antor&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Tripp%2C+A">Austin Tripp</a>, 
<a href="/search/cs?searchtype=author&query=Terenin%2C+A">Alexander Terenin</a>, 
<a href="/search/cs?searchtype=author&query=Szepesv%C3%A1ri%2C+C">Csaba Szepesv&#xe1;ri</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>, 
<a href="/search/cs?searchtype=author&query=Janz%2C+D">David Janz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the optimisation problem associated with Gaussian process regression
using squared loss. The most common approach to this problem is to apply an
exact solver, such as conjugate gradient descent, either directly, or to a
reduced-order version of the problem. Recently, driven by successes in deep
learning, stochastic gradient descent has gained traction as an alternative. In
this paper, we show that when done right$\unicode{x2014}$by which we mean using
specific insights from the optimisation and kernel
communities$\unicode{x2014}$this approach is highly effective. We thus
introduce a particular stochastic dual gradient descent algorithm, that may be
implemented with a few lines of code using any deep learning framework. We
explain our design decisions by illustrating their advantage against
alternatives with ablation studies and show that the new method is highly
competitive. Our evaluations on standard regression benchmarks and a Bayesian
optimisation task set our approach apart from preconditioned conjugate
gradients, variational Gaussian process approximations, and a previous version
of stochastic gradient descent for Gaussian processes. On a molecular binding
affinity prediction task, our method places Gaussian process regression on par
in terms of performance with state-of-the-art graph neural networks.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20587" title="Abstract">arXiv:2310.20587</a> [<a href="/pdf/2310.20587" title="Download PDF">pdf</a>, <a href="/format/2310.20587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Power of Pre-trained Language Models for Offline  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+R">Ruizhe Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuyao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ze%2C+Y">Yanjie Ze</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon S. Du</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Offline reinforcement learning (RL) aims to find a near-optimal policy using
pre-collected datasets. In real-world scenarios, data collection could be
costly and risky; therefore, offline RL becomes particularly challenging when
the in-domain data is limited. Given recent advances in Large Language Models
(LLMs) and their few-shot learning prowess, this paper introduces
$\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a
general framework based on Decision Transformers to effectively use pre-trained
Language Models (LMs) for offline RL. Our framework highlights four crucial
components: (1) Initializing Decision Transformers with sequentially
pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to
full-weight fine-tuning, to combine the pre-trained knowledge from LMs and
in-domain knowledge effectively, (3) using the non-linear MLP transformation
instead of linear projections, to generate embeddings, and (4) integrating an
auxiliary language prediction loss during fine-tuning to stabilize the LMs and
retain their original abilities on languages. Empirical results indicate
$\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks
and closes the gap between value-based offline RL methods and decision
transformers in dense-reward tasks. In particular, our method demonstrates
superior performance in scenarios with limited data samples. Our project
website is https://lamo2023.github.io
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20588" title="Abstract">arXiv:2310.20588</a> [<a href="/pdf/2310.20588" title="Download PDF">pdf</a>, <a href="/format/2310.20588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaizhu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Anh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=De%2C+S">Suparna De</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">In the era of the Internet of Things (IoT), the retrieval of relevant medical
information has become essential for efficient clinical decision-making. This
paper introduces MedFusionRank, a novel approach to zero-shot medical
information retrieval (MIR) that combines the strengths of pre-trained language
models and statistical methods while addressing their limitations. The proposed
approach leverages a pre-trained BERT-style model to extract compact yet
informative keywords. These keywords are then enriched with domain knowledge by
linking them to conceptual entities within a medical knowledge graph.
Experimental evaluations on medical datasets demonstrate MedFusion Rank's
superior performance over existing methods, with promising results with a
variety of evaluation metrics. MedFusionRank demonstrates efficacy in
retrieving relevant information, even from short or single-term queries.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20589" title="Abstract">arXiv:2310.20589</a> [<a href="/pdf/2310.20589" title="Download PDF">pdf</a>, <a href="/format/2310.20589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increasing The Performance of Cognitively Inspired Data-Efficient  Language Models via Implicit Structure Building
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Momen%2C+O">Omar Momen</a>, 
<a href="/search/cs?searchtype=author&query=Arps%2C+D">David Arps</a>, 
<a href="/search/cs?searchtype=author&query=Kallmeyer%2C+L">Laura Kallmeyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the BabyLM shared task at CoNLL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we describe our submission to the BabyLM Challenge 2023 shared
task on data-efficient language model (LM) pretraining (Warstadt et al., 2023).
We train transformer-based masked language models that incorporate unsupervised
predictions about hierarchical sentence structure into the model architecture.
Concretely, we use the Structformer architecture (Shen et al., 2021) and
variants thereof. StructFormer models have been shown to perform well on
unsupervised syntactic induction based on limited pretraining data, and to
yield performance improvements over a vanilla transformer architecture (Shen et
al., 2021). Evaluation of our models on 39 tasks provided by the BabyLM
challenge shows promising improvements of models that integrate a hierarchical
bias into the architecture at some particular tasks, even though they fail to
consistently outperform the RoBERTa baseline model provided by the shared task
organizers on all tasks.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20590" title="Abstract">arXiv:2310.20590</a> [<a href="/pdf/2310.20590" title="Download PDF">pdf</a>, <a href="/format/2310.20590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Enhanced RRT based Algorithm for Dynamic Path Planning and Energy  Management of a Mobile Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chitre%2C+R">Ronit Chitre</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Arpita Sinha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Mobile robots often have limited battery life and need to recharge
periodically. This paper presents an RRT- based path-planning algorithm that
addresses battery power management. A path is generated continuously from the
robot's current position to its recharging station. The robot decides if a
recharge is needed based on the energy required to travel on that path and the
robot's current power. RRT* is used to generate the first path, and then
subsequent paths are made using information from previous trees. Finally, the
presented algorithm was compared with Extended Rate Random Tree (ERRT)
algorithm
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20593" title="Abstract">arXiv:2310.20593</a> [<a href="/pdf/2310.20593" title="Download PDF">pdf</a>, <a href="/format/2310.20593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLODCAST: Flow and Depth Forecasting via Multimodal Recurrent  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ciamarra%2C+A">Andrea Ciamarra</a>, 
<a href="/search/cs?searchtype=author&query=Becattini%2C+F">Federico Becattini</a>, 
<a href="/search/cs?searchtype=author&query=Seidenari%2C+L">Lorenzo Seidenari</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bimbo%2C+A">Alberto Del Bimbo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Pattern Recognition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Forecasting motion and spatial positions of objects is of fundamental
importance, especially in safety-critical settings such as autonomous driving.
In this work, we address the issue by forecasting two different modalities that
carry complementary information, namely optical flow and depth. To this end we
propose FLODCAST a flow and depth forecasting model that leverages a multitask
recurrent architecture, trained to jointly forecast both modalities at once. We
stress the importance of training using flows and depth maps together,
demonstrating that both tasks improve when the model is informed of the other
modality. We train the proposed model to also perform predictions for several
timesteps in the future. This provides better supervision and leads to more
precise predictions, retaining the capability of the model to yield outputs
autoregressively for any future time horizon. We test our model on the
challenging Cityscapes dataset, obtaining state of the art results for both
flow and depth forecasting. Thanks to the high quality of the generated flows,
we also report benefits on the downstream task of segmentation forecasting,
injecting our predictions in a flow-based mask-warping framework.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20598" title="Abstract">arXiv:2310.20598</a> [<a href="/pdf/2310.20598" title="Download PDF">pdf</a>, <a href="/format/2310.20598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Conversion with Switching Costs: Robust and Learning-Augmented  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lechowicz%2C+A">Adam Lechowicz</a>, 
<a href="/search/cs?searchtype=author&query=Christianson%2C+N">Nicolas Christianson</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Bo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+N">Noman Bashir</a>, 
<a href="/search/cs?searchtype=author&query=Hajiesmaili%2C+M">Mohammad Hajiesmaili</a>, 
<a href="/search/cs?searchtype=author&query=Wierman%2C+A">Adam Wierman</a>, 
<a href="/search/cs?searchtype=author&query=Shenoy%2C+P">Prashant Shenoy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce and study online conversion with switching costs, a family of
online problems that capture emerging problems at the intersection of energy
and sustainability. In this problem, an online player attempts to purchase
(alternatively, sell) fractional shares of an asset during a fixed time horizon
with length $T$. At each time step, a cost function (alternatively, price
function) is revealed, and the player must irrevocably decide an amount of
asset to convert. The player also incurs a switching cost whenever their
decision changes in consecutive time steps, i.e., when they increase or
decrease their purchasing amount. We introduce competitive (robust)
threshold-based algorithms for both the minimization and maximization variants
of this problem, and show they are optimal among deterministic online
algorithms. We then propose learning-augmented algorithms that take advantage
of untrusted black-box advice (such as predictions from a machine learning
model) to achieve significantly better average-case performance without
sacrificing worst-case competitive guarantees. Finally, we empirically evaluate
our proposed algorithms using a carbon-aware EV charging case study, showing
that our algorithms substantially improve on baseline methods for this problem.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20602" title="Abstract">arXiv:2310.20602</a> [<a href="/pdf/2310.20602" title="Download PDF">pdf</a>, <a href="/format/2310.20602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compliant actuators that mimic biological muscle performance with  applications in a highly biomimetic robotic arm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haosen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Guowu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+L">Lei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lingyun Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper endeavours to bridge the existing gap in muscular actuator design
for ligament-skeletal-inspired robots, thereby fostering the evolution of these
robotic systems. We introduce two novel compliant actuators, namely the
Internal Torsion Spring Compliant Actuator (ICA) and the External Spring
Compliant Actuator (ECA), and present a comparative analysis against the
previously conceived Magnet Integrated Soft Actuator (MISA) through
computational and experimental results. These actuators, employing a
motor-tendon system, emulate biological muscle-like forms, enhancing artificial
muscle technology. A robotic arm application inspired by the skeletal ligament
system is presented. Experiments demonstrate satisfactory power in tasks like
lifting dumbbells (peak power: 36W), playing table tennis (end-effector speed:
3.2 m/s), and door opening, without compromising biomimetic aesthetics.
Compared to other linear stiffness serial elastic actuators (SEAs), ECA and ICA
exhibit high power-to-volume (361 x 10^3 W/m) and power-to-mass (111.6 W/kg)
ratios respectively, endorsing the biomimetic design's promise in robotic
development.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20605" title="Abstract">arXiv:2310.20605</a> [<a href="/pdf/2310.20605" title="Download PDF">pdf</a>, <a href="/format/2310.20605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Lyapunov-Stable Polynomial Dynamical Systems Through Imitation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abyaneh%2C+A">Amin Abyaneh</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hsiu-Chin Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In 7th Annual Conference on Robot Learning 2023 Aug 30
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Imitation learning is a paradigm to address complex motion planning problems
by learning a policy to imitate an expert's behavior. However, relying solely
on the expert's data might lead to unsafe actions when the robot deviates from
the demonstrated trajectories. Stability guarantees have previously been
provided utilizing nonlinear dynamical systems, acting as high-level motion
planners, in conjunction with the Lyapunov stability theorem. Yet, these
methods are prone to inaccurate policies, high computational cost, sample
inefficiency, or quasi stability when replicating complex and highly nonlinear
trajectories. To mitigate this problem, we present an approach for learning a
globally stable nonlinear dynamical system as a motion planning policy. We
model the nonlinear dynamical system as a parametric polynomial and learn the
polynomial's coefficients jointly with a Lyapunov candidate. To showcase its
success, we compare our method against the state of the art in simulation and
conduct real-world experiments with the Kinova Gen3 Lite manipulator arm. Our
experiments demonstrate the sample efficiency and reproduction accuracy of our
method for various expert trajectories, while remaining stable in the face of
perturbations.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20606" title="Abstract">arXiv:2310.20606</a> [<a href="/pdf/2310.20606" title="Download PDF">pdf</a>, <a href="/ps/2310.20606" title="Download PostScript">ps</a>, <a href="/format/2310.20606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Way Communication Complexity of Partial XOR Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Podolskii%2C+V+V">Vladimir V. Podolskii</a>, 
<a href="/search/cs?searchtype=author&query=Sluch%2C+D">Dmitrii Sluch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">Boolean function $F(x,y)$ for $x,y \in \{0,1\}^n$ is an XOR function if
$F(x,y)=f(x\oplus y)$ for some function $f$ on $n$ input bits, where $\oplus$
is a bit-wise XOR. XOR functions are relevant in communication complexity,
partially for allowing Fourier analytic technique. For total XOR functions it
is known that deterministic communication complexity of $F$ is closely related
to parity decision tree complexity of $f$. Montanaro and Osbourne (2009)
observed that one-sided communication complexity $D_{cc}^{\rightarrow}(F)$ of
$F$ is exactly equal to nonadaptive parity decision tree complexity
$NADT^{\oplus}(f)$ of $f$. Hatami et al. (2018) showed that unrestricted
communication complexity of $F$ is polynomially related to parity decision tree
complexity of $f$.
<br />We initiate the studies of a similar connection for partial functions. We
show that in case of one-sided communication complexity whether these measures
are equal, depends on the number of undefined inputs of $f$. On the one hand,
if $D_{cc}^{\rightarrow}(F)=t$ and $f$ is undefined on at most
$O(\frac{2^{n-t}}{\sqrt{n-t}})$, then $NADT^{\oplus}(f)=t$.
<br />On the other hand, for a wide range of values of $D_{cc}^{\rightarrow}(F)$
and $NADT^{\oplus}(f)$ (from constant to $n-2$) we provide partial functions
for which $D_{cc}^{\rightarrow}(F) &lt; NADT^{\oplus}(f)$. In particular, we
provide a function with an exponential gap between the two measures. Our
separation results translate to the case of two-sided communication complexity
as well, in particular showing that the result of Hatami et al. (2018) cannot
be generalized to partial functions.
<br />Previous results for total functions heavily rely on Boolean Fourier analysis
and the technique does not translate to partial functions. For the proofs of
our results we build a linear algebraic framework instead. Separation results
are proved through the reduction to covering codes.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20607" title="Abstract">arXiv:2310.20607</a> [<a href="/pdf/2310.20607" title="Download PDF">pdf</a>, <a href="/format/2310.20607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for  Pathological Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+W">Wenkang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Peixiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaomin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Heyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Lin Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pathological captioning of Whole Slide Images (WSIs), though is essential in
computer-aided pathological diagnosis, has rarely been studied due to the
limitations in datasets and model training efficacy. In this paper, we propose
a new paradigm Subtype-guided Masked Transformer (SGMT) for pathological
captioning based on Transformers, which treats a WSI as a sequence of sparse
patches and generates an overall caption sentence from the sequence. An
accompanying subtype prediction is introduced into SGMT to guide the training
process and enhance the captioning accuracy. We also present an Asymmetric
Masked Mechansim approach to tackle the large size constraint of pathological
image captioning, where the numbers of sequencing patches in SGMT are sampled
differently in the training and inferring phases, respectively. Experiments on
the PatchGastricADC22 dataset demonstrate that our approach effectively adapts
to the task with a transformer-based model and achieves superior performance
than traditional RNN-based methods. Our codes are to be made available for
further research and development.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20608" title="Abstract">arXiv:2310.20608</a> [<a href="/pdf/2310.20608" title="Download PDF">pdf</a>, <a href="/format/2310.20608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Robotic Reinforcement Learning with Asynchronous Human  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balsells%2C+M">Max Balsells</a>, 
<a href="/search/cs?searchtype=author&query=Torne%2C+M">Marcel Torne</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Desai%2C+S">Samedh Desai</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website <a href="https://guided-exploration-autonomous-rl.github.io/GEAR/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Ideally, we would place a robot in a real-world environment and leave it
there improving on its own by gathering more experience autonomously. However,
algorithms for autonomous robotic learning have been challenging to realize in
the real world. While this has often been attributed to the challenge of sample
complexity, even sample-efficient techniques are hampered by two major
challenges - the difficulty of providing well "shaped" rewards, and the
difficulty of continual reset-free training. In this work, we describe a system
for real-world reinforcement learning that enables agents to show continual
improvement by training directly in the real world without requiring
painstaking effort to hand-design reward functions or reset mechanisms. Our
system leverages occasional non-expert human-in-the-loop feedback from remote
users to learn informative distance functions to guide exploration while
leveraging a simple self-supervised learning algorithm for goal-directed policy
learning. We show that in the absence of resets, it is particularly important
to account for the current "reachability" of the exploration policy when
deciding which regions of the space to explore. Based on this insight, we
instantiate a practical learning system - GEAR, which enables robots to simply
be placed in real-world environments and left to train autonomously without
interruption. The system streams robot experience to a web interface only
requiring occasional asynchronous feedback from remote, crowdsourced,
non-expert humans in the form of binary comparative feedback. We evaluate this
system on a suite of robotic tasks in simulation and demonstrate its
effectiveness at learning behaviors both in simulation and the real world.
Project website https://guided-exploration-autonomous-rl.github.io/GEAR/.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20615" title="Abstract">arXiv:2310.20615</a> [<a href="/pdf/2310.20615" title="Download PDF">pdf</a>, <a href="/format/2310.20615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Min-Sum Motion Planning for Two Square Robots in a  Polygonal Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+P+K">Pankaj K. Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Halperin%2C+D">Dan Halperin</a>, 
<a href="/search/cs?searchtype=author&query=Sharir%2C+M">Micha Sharir</a>, 
<a href="/search/cs?searchtype=author&query=Steiger%2C+A">Alex Steiger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The conference version of the paper is accepted to SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computational Geometry (cs.CG); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Let $\mathcal{W} \subset \mathbb{R}^2$ be a planar polygonal environment
(i.e., a polygon potentially with holes) with a total of $n$ vertices, and let
$A,B$ be two robots, each modeled as an axis-aligned unit square, that can
translate inside $\mathcal{W}$. Given source and target placements
$s_A,t_A,s_B,t_B \in \mathcal{W}$ of $A$ and $B$, respectively, the goal is to
compute a \emph{collision-free motion plan} $\mathbf{\pi}^*$, i.e., a motion
plan that continuously moves $A$ from $s_A$ to $t_A$ and $B$ from $s_B$ to
$t_B$ so that $A$ and $B$ remain inside $\mathcal{W}$ and do not collide with
each other during the motion. Furthermore, if such a plan exists, then we wish
to return a plan that minimizes the sum of the lengths of the paths traversed
by the robots, $\left|\mathbf{\pi}^*\right|$. Given $\mathcal{W},
s_A,t_A,s_B,t_B$ and a parameter $\varepsilon &gt; 0$, we present an
$n^2\varepsilon^{-O(1)} \log n$-time $(1+\varepsilon)$-approximation algorithm
for this problem. We are not aware of any polynomial time algorithm for this
problem, nor do we know whether the problem is NP-Hard. Our result is the first
polynomial-time $(1+\varepsilon)$-approximation algorithm for an optimal motion
planning problem involving two robots moving in a polygonal environment.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20618" title="Abstract">arXiv:2310.20618</a> [<a href="/pdf/2310.20618" title="Download PDF">pdf</a>, <a href="/format/2310.20618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Reconstruction of Ultrasound Images with Informative  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huneau%2C+C">Cl&#xe9;ment Huneau</a>, 
<a href="/search/cs?searchtype=author&query=Idier%2C+J">J&#xe9;r&#xf4;me Idier</a>, 
<a href="/search/cs?searchtype=author&query=Mateus%2C+D">Diana Mateus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. (10 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite its wide use in medicine, ultrasound imaging faces several challenges
related to its poor signal-to-noise ratio and several sources of noise and
artefacts. Enhancing ultrasound image quality involves balancing concurrent
factors like contrast, resolution, and speckle preservation. In recent years,
there has been progress both in model-based and learning-based approaches to
improve ultrasound image reconstruction. Bringing the best from both worlds, we
propose a hybrid approach leveraging advances in diffusion models. To this end,
we adapt Denoising Diffusion Restoration Models (DDRM) to incorporate
ultrasound physics through a linear direct model and an unsupervised
fine-tuning of the prior diffusion model. We conduct comprehensive experiments
on simulated, in-vitro, and in-vivo data, demonstrating the efficacy of our
approach in achieving high-quality image reconstructions from a single plane
wave input and in comparison to state-of-the-art methods. Finally, given the
stochastic nature of the method, we analyse in depth the statistical properties
of single and multiple-sample reconstructions, experimentally show the
informativeness of their variance, and provide an empirical model relating this
behaviour to speckle noise. The code and data are available at: (upon
acceptance).
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20620" title="Abstract">arXiv:2310.20620</a> [<a href="/pdf/2310.20620" title="Download PDF">pdf</a>, <a href="/format/2310.20620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Unreasonable Effectiveness of Random Target Embeddings for  Continuous-Output Neural Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tokarchuk%2C+E">Evgeniia Tokarchuk</a>, 
<a href="/search/cs?searchtype=author&query=Niculae%2C+V">Vlad Niculae</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Continuous-output neural machine translation (CoNMT) replaces the discrete
next-word prediction problem with an embedding prediction. The semantic
structure of the target embedding space (i.e., closeness of related words) is
intuitively believed to be crucial. We challenge this assumption and show that
completely random output embeddings can outperform laboriously pretrained ones,
especially on larger datasets. Further investigation shows this surprising
effect is strongest for rare words, due to the geometry of their embeddings. We
shed further light on this finding by designing a mixed strategy that combines
random and pre-trained embeddings for different tokens.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20621" title="Abstract">arXiv:2310.20621</a> [<a href="/pdf/2310.20621" title="Download PDF">pdf</a>, <a href="/format/2310.20621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deepfake detection by exploiting surface anomalies: the SurFake approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ciamarra%2C+A">Andrea Ciamarra</a>, 
<a href="/search/cs?searchtype=author&query=Caldelli%2C+R">Roberto Caldelli</a>, 
<a href="/search/cs?searchtype=author&query=Becattini%2C+F">Federico Becattini</a>, 
<a href="/search/cs?searchtype=author&query=Seidenari%2C+L">Lorenzo Seidenari</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bimbo%2C+A">Alberto Del Bimbo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The ever-increasing use of synthetically generated content in different
sectors of our everyday life, one for all media information, poses a strong
need for deepfake detection tools in order to avoid the proliferation of
altered messages. The process to identify manipulated content, in particular
images and videos, is basically performed by looking for the presence of some
inconsistencies and/or anomalies specifically due to the fake generation
process. Different techniques exist in the scientific literature that exploit
diverse ad-hoc features in order to highlight possible modifications. In this
paper, we propose to investigate how deepfake creation can impact on the
characteristics that the whole scene had at the time of the acquisition. In
particular, when an image (video) is captured the overall geometry of the scene
(e.g. surfaces) and the acquisition process (e.g. illumination) determine a
univocal environment that is directly represented by the image pixel values;
all these intrinsic relations are possibly changed by the deepfake generation
process. By resorting to the analysis of the characteristics of the surfaces
depicted in the image it is possible to obtain a descriptor usable to train a
CNN for deepfake detection: we refer to such an approach as SurFake.
Experimental results carried out on the FF++ dataset for different kinds of
deepfake forgeries and diverse deep learning models confirm that such a feature
can be adopted to discriminate between pristine and altered images;
furthermore, experiments witness that it can also be combined with visual data
to provide a certain improvement in terms of detection accuracy.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20623" title="Abstract">arXiv:2310.20623</a> [<a href="/pdf/2310.20623" title="Download PDF">pdf</a>, <a href="/format/2310.20623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully dynamic approximation schemes on planar and apex-minor-free graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+T">Tuukka Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Nadara%2C+W">Wojciech Nadara</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Micha&#x142; Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Soko%C5%82owski%2C+M">Marek Soko&#x142;owski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, accepted to SODA '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The classic technique of Baker [J. ACM '94] is the most fundamental approach
for designing approximation schemes on planar, or more generally
topologically-constrained graphs, and it has been applied in a myriad of
different variants and settings throughout the last 30 years. In this work we
propose a dynamic variant of Baker's technique, where instead of finding an
approximate solution in a given static graph, the task is to design a data
structure for maintaining an approximate solution in a fully dynamic graph,
that is, a graph that is changing over time by edge deletions and edge
insertions. Specifically, we address the two most basic problems -- Maximum
Weight Independent Set and Minimum Weight Dominating Set -- and we prove the
following: for a fully dynamic $n$-vertex planar graph $G$, one can:
<br />* maintain a $(1-\varepsilon)$-approximation of the maximum weight of an
independent set in $G$ with amortized update time $f(\varepsilon)\cdot
n^{o(1)}$; and,
<br />* under the additional assumption that the maximum degree of the graph is
bounded at all times by a constant, also maintain a
$(1+\varepsilon)$-approximation of the minimum weight of a dominating set in
$G$ with amortized update time $f(\varepsilon)\cdot n^{o(1)}$.
<br />In both cases, $f(\varepsilon)$ is doubly-exponential in
$\mathrm{poly}(1/\varepsilon)$ and the data structure can be initialized in
time $f(\varepsilon)\cdot n^{1+o(1)}$. All our results in fact hold in the
larger generality of any graph class that excludes a fixed apex-graph as a
minor.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20624" title="Abstract">arXiv:2310.20624</a> [<a href="/pdf/2310.20624" title="Download PDF">pdf</a>, <a href="/format/2310.20624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lermen%2C+S">Simon Lermen</a>, 
<a href="/search/cs?searchtype=author&query=Rogers-Smith%2C+C">Charlie Rogers-Smith</a>, 
<a href="/search/cs?searchtype=author&query=Ladish%2C+J">Jeffrey Ladish</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">AI developers often apply safety alignment procedures to prevent the misuse
of their AI systems. For example, before Meta released Llama 2-Chat, a
collection of instruction fine-tuned large language models, they invested
heavily in safety training, incorporating extensive red-teaming and
reinforcement learning from human feedback. However, it remains unclear how
well safety training guards against model misuse when attackers have access to
model weights. We explore the robustness of safety training in language models
by subversively fine-tuning the public weights of Llama 2-Chat. We employ
low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of
less than $200 per model and using only one GPU, we successfully undo the
safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically,
our fine-tuning technique significantly reduces the rate at which the model
refuses to follow harmful instructions. We achieve a refusal rate below 1% for
our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method
retains general performance, which we validate by comparing our fine-tuned
models against Llama 2-Chat across two benchmarks. Additionally, we present a
selection of harmful outputs produced by our models. While there is
considerable uncertainty about the scope of risks from current models, it is
likely that future models will have significantly more dangerous capabilities,
including the ability to hack into critical infrastructure, create dangerous
bio-weapons, or autonomously replicate and adapt to new environments. We show
that subversive fine-tuning is practical and effective, and hence argue that
evaluating risks from fine-tuning should be a core part of risk assessments for
releasing model weights.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20632" title="Abstract">arXiv:2310.20632</a> [<a href="/pdf/2310.20632" title="Download PDF">pdf</a>, <a href="/format/2310.20632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Planarity in Practice -- Engineering the Synchronized  Planarity Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fink%2C+S+D">Simon D. Fink</a>, 
<a href="/search/cs?searchtype=author&query=Rutter%2C+I">Ignaz Rutter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear in Proceedings of ALENEX 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In the constrained planarity setting, we ask whether a graph admits a planar
drawing that additionally satisfies a given set of constraints. These
constraints are often derived from very natural problems; prominent examples
are Level Planarity, where vertices have to lie on given horizontal lines
indicating a hierarchy, and Clustered Planarity, where we additionally draw the
boundaries of clusters which recursively group the vertices in a crossing-free
manner. Despite receiving significant amount of attention and substantial
theoretical progress on these problems, only very few of the found solutions
have been put into practice and evaluated experimentally.
<br />In this paper, we describe our implementation of the recent quadratic-time
algorithm by Bl\"asius et al. [TALG Vol 19, No 4] for solving the problem
Synchronized Planarity, which can be seen as a common generalization of several
constrained planarity problems, including the aforementioned ones. Our
experimental evaluation on an existing benchmark set shows that even our
baseline implementation outperforms all competitors by at least an order of
magnitude. We systematically investigate the degrees of freedom in the
implementation of the Synchronized Planarity algorithm for larger instances and
propose several modifications that further improve the performance. Altogether,
this allows us to solve instances with up to 100 vertices in milliseconds and
instances with up to 100 000 vertices within a few minutes.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20633" title="Abstract">arXiv:2310.20633</a> [<a href="/pdf/2310.20633" title="Download PDF">pdf</a>, <a href="/format/2310.20633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defining a New NLP Playground
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sha Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chi Han</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Pengfei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Edwards%2C+C">Carl Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Manling Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+Y+R">Yi R. Fung</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Charles Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tetreault%2C+J+R">Joel R. Tetreault</a>, 
<a href="/search/cs?searchtype=author&query=Hovy%2C+E+H">Eduard H. Hovy</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP Findings 2023 "Theme Track: Large Language Models and the Future of NLP"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The recent explosion of performance of large language models (LLMs) has
changed the field of Natural Language Processing (NLP) more abruptly and
seismically than any other shift in the field's 80-year history. This has
resulted in concerns that the field will become homogenized and
resource-intensive. The new status quo has put many academic researchers,
especially PhD students, at a disadvantage. This paper aims to define a new NLP
playground by proposing 20+ PhD-dissertation-worthy research directions,
covering theoretical analysis, new and challenging problems, learning
paradigms, and interdisciplinary applications.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20636" title="Abstract">arXiv:2310.20636</a> [<a href="/pdf/2310.20636" title="Download PDF">pdf</a>, <a href="/format/2310.20636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Higher-Order Moments to Assess the Quality of GAN-generated Image  Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luzi%2C+L">Lorenzo Luzi</a>, 
<a href="/search/cs?searchtype=author&query=Jenne%2C+H">Helen Jenne</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+R">Ryan Murray</a>, 
<a href="/search/cs?searchtype=author&query=Marrero%2C+C+O">Carlos Ortiz Marrero</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The rapid advancement of Generative Adversarial Networks (GANs) necessitates
the need to robustly evaluate these models. Among the established evaluation
criteria, the Fr\'{e}chet Inception Distance (FID) has been widely adopted due
to its conceptual simplicity, fast computation time, and strong correlation
with human perception. However, FID has inherent limitations, mainly stemming
from its assumption that feature embeddings follow a Gaussian distribution, and
therefore can be defined by their first two moments. As this does not hold in
practice, in this paper we explore the importance of third-moments in image
feature data and use this information to define a new measure, which we call
the Skew Inception Distance (SID). We prove that SID is a pseudometric on
probability distributions, show how it extends FID, and present a practical
method for its computation. Our numerical experiments support that SID either
tracks with FID or, in some cases, aligns more closely with human perception
when evaluating image features of ImageNet data.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20638" title="Abstract">arXiv:2310.20638</a> [<a href="/pdf/2310.20638" title="Download PDF">pdf</a>, <a href="/format/2310.20638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Histopathological Image Analysis with Style-Augmented Feature Domain  Mixing for Improved Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khamankar%2C+V">Vaibhav Khamankar</a>, 
<a href="/search/cs?searchtype=author&query=Bera%2C+S">Sutanu Bera</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Saumik Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+D">Debashis Sen</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+P+K">Prabir Kumar Biswas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper is published in MedAGI 2023 (MICCAI 2023 1st International Workshop on Foundation Models for General Medical AI) Code link: <a href="https://github.com/Vaibhav-Khamankar/FuseStyle">this https URL</a> Paper link: <a href="https://nbviewer.org/github/MedAGI/medagi.github.io/blob/main/src/assets/papers/P17.pdf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Tissues and Organs (q-bio.TO)

</div>
<p class="mathjax">Histopathological images are essential for medical diagnosis and treatment
planning, but interpreting them accurately using machine learning can be
challenging due to variations in tissue preparation, staining and imaging
protocols. Domain generalization aims to address such limitations by enabling
the learning models to generalize to new datasets or populations. Style
transfer-based data augmentation is an emerging technique that can be used to
improve the generalizability of machine learning models for histopathological
images. However, existing style transfer-based methods can be computationally
expensive, and they rely on artistic styles, which can negatively impact model
accuracy. In this study, we propose a feature domain style mixing technique
that uses adaptive instance normalization to generate style-augmented versions
of images. We compare our proposed method with existing style transfer-based
data augmentation methods and found that it performs similarly or better,
despite requiring less computation and time. Our results demonstrate the
potential of feature domain statistics mixing in the generalization of learning
models for histopathological image analysis.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20641" title="Abstract">arXiv:2310.20641</a> [<a href="/pdf/2310.20641" title="Download PDF">pdf</a>, <a href="/format/2310.20641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Improvement in Multi-class Classification via Automated  Hierarchy Generation and Exploitation through Extended LCPN Schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alagoz%2C+C">Celal Alagoz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Hierarchical classification (HC) plays a pivotal role in multi-class
classification tasks, where objects are organized into a hierarchical
structure. This study explores the performance of HC through a comprehensive
analysis that encompasses both hierarchy generation and hierarchy exploitation.
This analysis is particularly relevant in scenarios where a predefined
hierarchy structure is not readily accessible. Notably, two novel hierarchy
exploitation schemes, LCPN+ and LCPN+F, which extend the capabilities of LCPN
and combine the strengths of global and local classification, have been
introduced and evaluated alongside existing methods. The findings reveal the
consistent superiority of LCPN+F, which outperforms other schemes across
various datasets and scenarios. Moreover, this research emphasizes not only
effectiveness but also efficiency, as LCPN+ and LCPN+F maintain runtime
performance comparable to Flat Classification (FC). Additionally, this study
underscores the importance of selecting the right hierarchy exploitation scheme
to maximize classification performance. This work extends our understanding of
HC and establishes a benchmark for future research, fostering advancements in
multi-class classification methodologies.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20649" title="Abstract">arXiv:2310.20649</a> [<a href="/pdf/2310.20649" title="Download PDF">pdf</a>, <a href="/format/2310.20649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Batch Norm Statistics Update for Natural Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+S">Shahbaz Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Norouzzadeh%2C+M+S">Mohammad Sadegh Norouzzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">DNNs trained on natural clean samples have been shown to perform poorly on
corrupted samples, such as noisy or blurry images. Various data augmentation
methods have been recently proposed to improve DNN's robustness against common
corruptions. Despite their success, they require computationally expensive
training and cannot be applied to off-the-shelf trained models. Recently, it
has been shown that updating BatchNorm (BN) statistics of an off-the-shelf
model on a single corruption improves its accuracy on that corruption
significantly. However, adopting the idea at inference time when the type of
corruption is unknown and changing decreases the effectiveness of this method.
In this paper, we harness the Fourier domain to detect the corruption type, a
challenging task in the image domain. We propose a unified framework consisting
of a corruption-detection model and BN statistics update that improves the
corruption accuracy of any off-the-shelf trained model. We benchmark our
framework on different models and datasets. Our results demonstrate about 8%
and 4% accuracy improvement on CIFAR10-C and ImageNet-C, respectively.
Furthermore, our framework can further improve the accuracy of state-of-the-art
robust models, such as AugMix and DeepAug.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20650" title="Abstract">arXiv:2310.20650</a> [<a href="/pdf/2310.20650" title="Download PDF">pdf</a>, <a href="/format/2310.20650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Limitations of State-Aware Imitation Learning for Autonomous  Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cultrera%2C+L">Luca Cultrera</a>, 
<a href="/search/cs?searchtype=author&query=Becattini%2C+F">Federico Becattini</a>, 
<a href="/search/cs?searchtype=author&query=Seidenari%2C+L">Lorenzo Seidenari</a>, 
<a href="/search/cs?searchtype=author&query=Pala%2C+P">Pietro Pala</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bimbo%2C+A">Alberto Del Bimbo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Intelligent Vehicles
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Conditional Imitation learning is a common and effective approach to train
autonomous driving agents. However, two issues limit the full potential of this
approach: (i) the inertia problem, a special case of causal confusion where the
agent mistakenly correlates low speed with no acceleration, and (ii) low
correlation between offline and online performance due to the accumulation of
small errors that brings the agent in a previously unseen state. Both issues
are critical for state-aware models, yet informing the driving agent of its
internal state as well as the state of the environment is of crucial
importance. In this paper we propose a multi-task learning agent based on a
multi-stage vision transformer with state token propagation. We feed the state
of the vehicle along with the representation of the environment as a special
token of the transformer and propagate it throughout the network. This allows
us to tackle the aforementioned issues from different angles: guiding the
driving policy with learned stop/go information, performing data augmentation
directly on the state of the vehicle and visually explaining the model's
decisions. We report a drastic decrease in inertia and a high correlation
between offline and online metrics.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20653" title="Abstract">arXiv:2310.20653</a> [<a href="/pdf/2310.20653" title="Download PDF">pdf</a>, <a href="/ps/2310.20653" title="Download PostScript">ps</a>, <a href="/format/2310.20653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite Difference Approximation with ADI Scheme for Two-dimensional  Keller-Segel Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lu%2C+Y">Yubin Lu</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chi-An Chen</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+X">Xiaofan Li</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+C">Chun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Analysis of PDEs (math.AP)

</div>
<p class="mathjax">Keller-Segel systems are a set of nonlinear partial differential equations
used to model chemotaxis in biology. In this paper, we propose two alternating
direction implicit (ADI) schemes to solve the 2D Keller-Segel systems directly
with minimal computational cost, while preserving positivity, energy
dissipation law and mass conservation. One scheme unconditionally preserves
positivity, while the other does so conditionally. Both schemes achieve
second-order accuracy in space, with the former being first-order accuracy in
time and the latter second-order accuracy in time. Besides, the former scheme
preserves the energy dissipation law asymptotically. We validate these results
through numerical experiments, and also compare the efficiency of our schemes
with the standard five-point scheme, demonstrating that our approaches
effectively reduce computational costs.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20654" title="Abstract">arXiv:2310.20654</a> [<a href="/pdf/2310.20654" title="Download PDF">pdf</a>, <a href="/format/2310.20654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Pick-and-Pass&quot; as a Hat-Trick Class for First-Principle Memory,  Generalizability, and Interpretability Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jason Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rezai%2C+R">Ryan Rezai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Closed drafting or "pick and pass" is a popular game mechanic where each
round players select a card or other playable element from their hand and pass
the rest to the next player. Games employing closed drafting make for great
studies on memory and turn order due to their explicitly calculable memory of
other players' hands. In this paper, we establish first-principle benchmarks
for studying model-free reinforcement learning algorithms and their comparative
ability to learn memory in a popular family of closed drafting games called
"Sushi Go Party!", producing state-of-the-art results on this environment along
the way. Furthermore, as Sushi Go Party! can be expressed as a set of
closely-related games based on the set of cards in play, we quantify the
generalizability of reinforcement learning algorithms trained on various sets
of cards, establishing key trends between generalized performance and the set
distance between the train and evaluation game configurations. Finally, we fit
decision rules to interpret the strategy of the learned models and compare them
to the ranking preferences of human players, finding intuitive common rules and
intriguing new moves.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20656" title="Abstract">arXiv:2310.20656</a> [<a href="/pdf/2310.20656" title="Download PDF">pdf</a>, <a href="/format/2310.20656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Compositionality in Sentiment: New Data and Analyses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dankers%2C+V">Verna Dankers</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+C+G">Christopher G. Lucas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in EMNLP Findings 2023; 13 pages total (5 in the main paper, 3 pages with limitations, acknowledgments and references, 5 pages with appendices)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">When natural language phrases are combined, their meaning is often more than
the sum of their parts. In the context of NLP tasks such as sentiment analysis,
where the meaning of a phrase is its sentiment, that still applies. Many NLP
studies on sentiment analysis, however, focus on the fact that sentiment
computations are largely compositional. We, instead, set out to obtain
non-compositionality ratings for phrases with respect to their sentiment. Our
contributions are as follows: a) a methodology for obtaining those
non-compositionality ratings, b) a resource of ratings for 259 phrases --
NonCompSST -- along with an analysis of that resource, and c) an evaluation of
computational models for sentiment analysis using this new resource.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20663" title="Abstract">arXiv:2310.20663</a> [<a href="/pdf/2310.20663" title="Download PDF">pdf</a>, <a href="/format/2310.20663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline RL with Observation Histories: Analyzing and Improving Sample  Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Joey Hong</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A">Anca Dragan</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline reinforcement learning (RL) can in principle synthesize more optimal
behavior from a dataset consisting only of suboptimal trials. One way that this
can happen is by "stitching" together the best parts of otherwise suboptimal
trajectories that overlap on similar states, to create new behaviors where each
individual state is in-distribution, but the overall returns are higher.
However, in many interesting and complex applications, such as autonomous
navigation and dialogue systems, the state is partially observed. Even worse,
the state representation is unknown or not easy to define. In such cases,
policies and value functions are often conditioned on observation histories
instead of states. In these cases, it is not clear if the same kind of
"stitching" is feasible at the level of observation histories, since two
different trajectories would always have different histories, and thus "similar
states" that might lead to effective stitching cannot be leveraged.
Theoretically, we show that standard offline RL algorithms conditioned on
observation histories suffer from poor sample complexity, in accordance with
the above intuition. We then identify sufficient conditions under which offline
RL can still be efficient -- intuitively, it needs to learn a compact
representation of history comprising only features relevant for action
selection. We introduce a bisimulation loss that captures the extent to which
this happens, and propose that offline RL can explicitly optimize this loss to
aid worst-case sample complexity. Empirically, we show that across a variety of
tasks either our proposed loss improves performance, or the value of this loss
is already minimized as a consequence of standard offline RL, indicating that
it correlates well with good performance.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20666" title="Abstract">arXiv:2310.20666</a> [<a href="/pdf/2310.20666" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StairNet: Visual Recognition of Stairs for Human-Robot Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurbis%2C+A+G">Andrew Garrett Kurbis</a>, 
<a href="/search/cs?searchtype=author&query=Kuzmenko%2C+D">Dmytro Kuzmenko</a>, 
<a href="/search/cs?searchtype=author&query=Ivanyuk-Skulskiy%2C+B">Bogdan Ivanyuk-Skulskiy</a>, 
<a href="/search/cs?searchtype=author&query=Mihailidis%2C+A">Alex Mihailidis</a>, 
<a href="/search/cs?searchtype=author&query=Laschowski%2C+B">Brokoslaw Laschowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Human-robot walking with prosthetic legs and exoskeletons, especially over
complex terrains such as stairs, remains a significant challenge. Egocentric
vision has the unique potential to detect the walking environment prior to
physical interactions, which can improve transitions to and from stairs. This
motivated us to create the StairNet initiative to support the development of
new deep learning models for visual sensing and recognition of stairs, with an
emphasis on lightweight and efficient neural networks for onboard real-time
inference. In this study, we present an overview of the development of our
large-scale dataset with over 515,000 manually labeled images, as well as our
development of different deep learning models (e.g., 2D and 3D CNN, hybrid CNN
and LSTM, and ViT networks) and training methods (e.g., supervised learning
with temporal data and semi-supervised learning with unlabeled images) using
our new dataset. We consistently achieved high classification accuracy (i.e.,
up to 98.8%) with different designs, offering trade-offs between model accuracy
and size. When deployed on mobile devices with GPU and NPU accelerators, our
deep learning models achieved inference speeds up to 2.8 ms. We also deployed
our models on custom-designed CPU-powered smart glasses. However, limitations
in the embedded hardware yielded slower inference speeds of 1.5 seconds,
presenting a trade-off between human-centered design and performance. Overall,
we showed that StairNet can be an effective platform to develop and study new
visual perception systems for human-robot locomotion with applications in
exoskeleton and prosthetic leg control.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20669" title="Abstract">arXiv:2310.20669</a> [<a href="/pdf/2310.20669" title="Download PDF">pdf</a>, <a href="/format/2310.20669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling multi-legged robot locomotion with slipping and its  experimental validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Ziyou Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Revzen%2C+S">Shai Revzen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Multi-legged robots with six or more legs are not in common use, despite
designs with superior stability, maneuverability, and a low number of actuators
being available for over 20 years. This may be in part due to the difficulty in
modeling multi-legged motion with slipping and producing reliable predictions
of body velocity. Here we present a detailed measurement of the foot contact
forces in a hexapedal robot with multiple sliding contacts, and provide an
algorithm for predicting these contact forces and the body velocity. The
algorithm relies on the recently published observation that even while
slipping, multi-legged robots are principally kinematic, and employ a friction
law ansatz that allows us to compute the shape-change to body-velocity
connection and the foot contact forces. This results in the ability to simulate
motion plans for a large number of potentially slipping legs. In homogeneous
environments, this can run in (parallel) logarithmic time of the planning
horizon
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20673" title="Abstract">arXiv:2310.20673</a> [<a href="/pdf/2310.20673" title="Download PDF">pdf</a>, <a href="/format/2310.20673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing Act: Constraining Disparate Impact in Sparse Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashemizadeh%2C+M">Meraj Hashemizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Ramirez%2C+J">Juan Ramirez</a>, 
<a href="/search/cs?searchtype=author&query=Sukumaran%2C+R">Rohan Sukumaran</a>, 
<a href="/search/cs?searchtype=author&query=Farnadi%2C+G">Golnoosh Farnadi</a>, 
<a href="/search/cs?searchtype=author&query=Lacoste-Julien%2C+S">Simon Lacoste-Julien</a>, 
<a href="/search/cs?searchtype=author&query=Gallego-Posada%2C+J">Jose Gallego-Posada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/merajhashemi/Balancing_Act">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Model pruning is a popular approach to enable the deployment of large deep
learning models on edge devices with restricted computational or storage
capacities. Although sparse models achieve performance comparable to that of
their dense counterparts at the level of the entire dataset, they exhibit high
accuracy drops for some data sub-groups. Existing methods to mitigate this
disparate impact induced by pruning (i) rely on surrogate metrics that address
the problem indirectly and have limited interpretability; or (ii) scale poorly
with the number of protected sub-groups in terms of computational cost. We
propose a constrained optimization approach that $\textit{directly addresses
the disparate impact of pruning}$: our formulation bounds the accuracy change
between the dense and sparse models, for each sub-group. This choice of
constraints provides an interpretable success criterion to determine if a
pruned model achieves acceptable disparity levels. Experimental results
demonstrate that our technique scales reliably to problems involving large
models and hundreds of protected sub-groups.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20679" title="Abstract">arXiv:2310.20679</a> [<a href="/pdf/2310.20679" title="Download PDF">pdf</a>, <a href="/format/2310.20679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Field Discovery In Interacting Dynamical Systems With Neural  Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kofinas%2C+M">Miltiadis Kofinas</a>, 
<a href="/search/cs?searchtype=author&query=Bekkers%2C+E+J">Erik J. Bekkers</a>, 
<a href="/search/cs?searchtype=author&query=Nagaraja%2C+N+S">Naveen Shankar Nagaraja</a>, 
<a href="/search/cs?searchtype=author&query=Gavves%2C+E">Efstratios Gavves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. <a href="https://github.com/mkofinas/aether">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Systems of interacting objects often evolve under the influence of field
effects that govern their dynamics, yet previous works have abstracted away
from such effects, and assume that systems evolve in a vacuum. In this work, we
focus on discovering these fields, and infer them from the observed dynamics
alone, without directly observing them. We theorize the presence of latent
force fields, and propose neural fields to learn them. Since the observed
dynamics constitute the net effect of local object interactions and global
field effects, recently popularized equivariant networks are inapplicable, as
they fail to capture global information. To address this, we propose to
disentangle local object interactions -- which are $\mathrm{SE}(n)$ equivariant
and depend on relative states -- from external global field effects -- which
depend on absolute states. We model interactions with equivariant graph
networks, and combine them with neural fields in a novel graph network that
integrates field forces. Our experiments show that we can accurately discover
the underlying fields in charged particles settings, traffic scenes, and
gravitational n-body problems, and effectively use them to learn the system and
forecast future trajectories.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20682" title="Abstract">arXiv:2310.20682</a> [<a href="/pdf/2310.20682" title="Download PDF">pdf</a>, <a href="/format/2310.20682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression with Exact Error Distribution for Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hegazy%2C+M">Mahmoud Hegazy</a>, 
<a href="/search/cs?searchtype=author&query=Leluc%2C+R">R&#xe9;mi Leluc</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C+T">Cheuk Ting Li</a>, 
<a href="/search/cs?searchtype=author&query=Dieuleveut%2C+A">Aymeric Dieuleveut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Compression schemes have been extensively used in Federated Learning (FL) to
reduce the communication cost of distributed learning. While most approaches
rely on a bounded variance assumption of the noise produced by the compressor,
this paper investigates the use of compression and aggregation schemes that
produce a specific error distribution, e.g., Gaussian or Laplace, on the
aggregated data. We present and analyze different aggregation schemes based on
layered quantizers achieving exact error distribution. We provide different
methods to leverage the proposed compression schemes to obtain
compression-for-free in differential privacy applications. Our general
compression methods can recover and improve standard FL schemes with Gaussian
perturbations such as Langevin dynamics and randomized smoothing.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20685" title="Abstract">arXiv:2310.20685</a> [<a href="/pdf/2310.20685" title="Download PDF">pdf</a>, <a href="/format/2310.20685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeRF Revisited: Fixing Quadrature Instability in Volume Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uy%2C+M+A">Mikaela Angelina Uy</a>, 
<a href="/search/cs?searchtype=author&query=Nakayama%2C+K">Kiyohiro Nakayama</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guandao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+R+K">Rahul Krishna Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Guibas%2C+L">Leonidas Guibas</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Neural radiance fields (NeRF) rely on volume rendering to synthesize novel
views. Volume rendering requires evaluating an integral along each ray, which
is numerically approximated with a finite sum that corresponds to the exact
integral along the ray under piecewise constant volume density. As a
consequence, the rendered result is unstable w.r.t. the choice of samples along
the ray, a phenomenon that we dub quadrature instability. We propose a
mathematically principled solution by reformulating the sample-based rendering
equation so that it corresponds to the exact integral under piecewise linear
volume density. This simultaneously resolves multiple issues: conflicts between
samples along different rays, imprecise hierarchical sampling, and
non-differentiability of quantiles of ray termination distances w.r.t. model
parameters. We demonstrate several benefits over the classical sample-based
rendering equation, such as sharper textures, better geometric reconstruction,
and stronger depth supervision. Our proposed formulation can be also be used as
a drop-in replacement to the volume rendering equation of existing NeRF-based
methods. Our project page can be found at pl-nerf.github.io.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20689" title="Abstract">arXiv:2310.20689</a> [<a href="/pdf/2310.20689" title="Download PDF">pdf</a>, <a href="/format/2310.20689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning From Mistakes Makes LLM Better Reasoner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+S">Shengnan An</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zexiong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zeqi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian-Guang Lou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) recently exhibited remarkable reasoning
capabilities on solving math problems. To further improve this capability, this
work proposes Learning from Mistakes (LeMa), akin to human learning processes.
Consider a human student who failed to solve a math problem, he will learn from
what mistake he has made and how to correct it. Mimicking this error-driven
learning process, LeMa fine-tunes LLMs on mistake-correction data pairs
generated by GPT-4. Specifically, we first collect inaccurate reasoning paths
from various LLMs and then employ GPT-4 as a "corrector" to (1) identify the
mistake step, (2) explain the reason for the mistake, and (3) correct the
mistake and generate the final answer. Experimental results demonstrate the
effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning
tasks, LeMa consistently improves the performance compared with fine-tuning on
CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as
WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on
MATH. This surpasses the SOTA performance achieved by non-execution open-source
models on these challenging tasks. Our code, data and models will be publicly
available at https://github.com/microsoft/CodeT.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20695" title="Abstract">arXiv:2310.20695</a> [<a href="/pdf/2310.20695" title="Download PDF">pdf</a>, <a href="/format/2310.20695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Junkun Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zhongwei Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhiyin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+S">Sifan Long</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+K">Kun Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+K">Kun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Junyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+E">Errui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lanfen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingdong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Model pre-training is essential in human-centric perception. In this paper,
we first introduce masked image modeling (MIM) as a pre-training approach for
this task. Upon revisiting the MIM training strategy, we reveal that human
structure priors offer significant potential. Motivated by this insight, we
further incorporate an intuitive human structure prior - human parts - into
pre-training. Specifically, we employ this prior to guide the mask sampling
process. Image patches, corresponding to human part regions, have high priority
to be masked out. This encourages the model to concentrate more on body
structure information during pre-training, yielding substantial benefits across
a range of human-centric perception tasks. To further capture human
characteristics, we propose a structure-invariant alignment loss that enforces
different masked views, guided by the human part prior, to be closely aligned
for the same image. We term the entire method as HAP. HAP simply uses a plain
ViT as the encoder yet establishes new state-of-the-art performance on 11
human-centric benchmarks, and on-par result on one dataset. For example, HAP
achieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K
for pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose
estimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20697" title="Abstract">arXiv:2310.20697</a> [<a href="/pdf/2310.20697" title="Download PDF">pdf</a>, <a href="/format/2310.20697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-Transport: Toward Learning Causal Effects of Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+V">Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Morency%2C+L">Louis-Philippe Morency</a>, 
<a href="/search/cs?searchtype=author&query=Ben-Michael%2C+E">Eli Ben-Michael</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">As language technologies gain prominence in real-world settings, it is
important to understand how changes to language affect reader perceptions. This
can be formalized as the causal effect of varying a linguistic attribute (e.g.,
sentiment) on a reader's response to the text. In this paper, we introduce
Text-Transport, a method for estimation of causal effects from natural language
under any text distribution. Current approaches for valid causal effect
estimation require strong assumptions about the data, meaning the data from
which one can estimate valid causal effects often is not representative of the
actual target domain of interest. To address this issue, we leverage the notion
of distribution shift to describe an estimator that transports causal effects
between domains, bypassing the need for strong assumptions in the target
domain. We derive statistical guarantees on the uncertainty of this estimator,
and we report empirical results and analyses that support the validity of
Text-Transport across data settings. Finally, we use Text-Transport to study a
realistic setting--hate speech on social media--in which causal effects do
shift significantly between text domains, demonstrating the necessity of
transport when conducting causal inference on natural language.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20700" title="Abstract">arXiv:2310.20700</a> [<a href="/pdf/2310.20700" title="Download PDF">pdf</a>, <a href="/format/2310.20700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEINE: Short-to-Long Video Diffusion Model for Generative Transition and  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaohui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+S">Shaobin Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiashuo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yali Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently video generation has achieved substantial progress with realistic
results. Nevertheless, existing AI-generated videos are usually very short
clips ("shot-level") depicting a single scene. To deliver a coherent long video
("story-level"), it is desirable to have creative transition and prediction
effects across different clips. This paper presents a short-to-long video
diffusion model, SEINE, that focuses on generative transition and prediction.
The goal is to generate high-quality long videos with smooth and creative
transitions between scenes and varying lengths of shot-level videos.
Specifically, we propose a random-mask video diffusion model to automatically
generate transitions based on textual descriptions. By providing the images of
different scenes as inputs, combined with text-based control, our model
generates transition videos that ensure coherence and visual quality.
Furthermore, the model can be readily extended to various tasks such as
image-to-video animation and autoregressive video prediction. To conduct a
comprehensive evaluation of this new generative task, we propose three
assessing criteria for smooth and creative transition: temporal consistency,
semantic similarity, and video-text semantic alignment. Extensive experiments
validate the effectiveness of our approach over existing methods for generative
transition and prediction, enabling the creation of story-level long videos.
Project page: https://vchitect.github.io/SEINE-project/ .
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20703" title="Abstract">arXiv:2310.20703</a> [<a href="/pdf/2310.20703" title="Download PDF">pdf</a>, <a href="/format/2310.20703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vanishing Gradients in Reinforcement Finetuning of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razin%2C+N">Noam Razin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hattie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Saremi%2C+O">Omid Saremi</a>, 
<a href="/search/cs?searchtype=author&query=Thilak%2C+V">Vimal Thilak</a>, 
<a href="/search/cs?searchtype=author&query=Bradley%2C+A">Arwen Bradley</a>, 
<a href="/search/cs?searchtype=author&query=Nakkiran%2C+P">Preetum Nakkiran</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+J">Joshua Susskind</a>, 
<a href="/search/cs?searchtype=author&query=Littwin%2C+E">Etai Littwin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">Pretrained language models are commonly aligned with human preferences and
downstream tasks via reinforcement finetuning (RFT), which entails maximizing a
(possibly learned) reward function using policy gradient algorithms. This work
highlights a fundamental optimization obstacle in RFT: we prove that the
expected gradient for an input vanishes when its reward standard deviation
under the model is small, even if the expected reward is far from optimal.
Through experiments on an RFT benchmark and controlled environments, as well as
a theoretical analysis, we then demonstrate that vanishing gradients due to
small reward standard deviation are prevalent and detrimental, leading to
extremely slow reward maximization. Lastly, we explore ways to overcome
vanishing gradients in RFT. We find the common practice of an initial
supervised finetuning (SFT) phase to be the most promising candidate, which
sheds light on its importance in an RFT pipeline. Moreover, we show that a
relatively small number of SFT optimization steps on as few as 1% of the input
samples can suffice, indicating that the initial SFT phase need not be
expensive in terms of compute and data labeling efforts. Overall, our results
emphasize that being mindful for inputs whose expected gradient vanishes, as
measured by the reward standard deviation, is crucial for successful execution
of RFT.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20704" title="Abstract">arXiv:2310.20704</a> [<a href="/pdf/2310.20704" title="Download PDF">pdf</a>, <a href="/format/2310.20704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked  Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Srijan Das</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+T">Tanmay Jain</a>, 
<a href="/search/cs?searchtype=author&query=Reilly%2C+D">Dominick Reilly</a>, 
<a href="/search/cs?searchtype=author&query=Balaji%2C+P">Pranav Balaji</a>, 
<a href="/search/cs?searchtype=author&query=Karmakar%2C+S">Soumyajit Karmakar</a>, 
<a href="/search/cs?searchtype=author&query=Marjit%2C+S">Shyam Marjit</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Abhijit Das</a>, 
<a href="/search/cs?searchtype=author&query=Ryoo%2C+M">Michael Ryoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite
their success, ViTs lack inductive biases, which can make it difficult to train
them with limited data. To address this challenge, prior studies suggest
training ViTs with self-supervised learning (SSL) and fine-tuning sequentially.
However, we observe that jointly optimizing ViTs for the primary task and a
Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the
amount of training data is limited. We explore the appropriate SSL tasks that
can be optimized alongside the primary task, the training schemes for these
tasks, and the data scale at which they can be most effective. Our findings
reveal that SSAT is a powerful technique that enables ViTs to leverage the
unique characteristics of both the self-supervised and primary tasks, achieving
better performance than typical ViTs pre-training with SSL and fine-tuning
sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT
significantly improves ViT performance while reducing carbon footprint. We also
confirm the effectiveness of SSAT in the video domain for deepfake detection,
showcasing its generalizability. Our code is available at
https://github.com/dominickrei/Limited-data-vits.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20705" title="Abstract">arXiv:2310.20705</a> [<a href="/pdf/2310.20705" title="Download PDF">pdf</a>, <a href="/format/2310.20705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Farthest Greedy Path Sampling for Two-shot Recommender Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yufan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tunhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wei Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+F">Feng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hai Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiran Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Weight-sharing Neural Architecture Search (WS-NAS) provides an efficient
mechanism for developing end-to-end deep recommender models. However, in
complex search spaces, distinguishing between superior and inferior
architectures (or paths) is challenging. This challenge is compounded by the
limited coverage of the supernet and the co-adaptation of subnet weights, which
restricts the exploration and exploitation capabilities inherent to
weight-sharing mechanisms. To address these challenges, we introduce Farthest
Greedy Path Sampling (FGPS), a new path sampling strategy that balances path
quality and diversity. FGPS enhances path diversity to facilitate more
comprehensive supernet exploration, while emphasizing path quality to ensure
the effective identification and utilization of promising architectures. By
incorporating FGPS into a Two-shot NAS (TS-NAS) framework, we derive
high-performance architectures. Evaluations on three Click-Through Rate (CTR)
prediction benchmarks demonstrate that our approach consistently achieves
superior results, outperforming both manually designed and most NAS-based
models.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20706" title="Abstract">arXiv:2310.20706</a> [<a href="/pdf/2310.20706" title="Download PDF">pdf</a>, <a href="/format/2310.20706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDAM-PS: Diligent Domain Adaptive Mixer for Person Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almansoori%2C+M+K">Mohammed Khaleed Almansoori</a>, 
<a href="/search/cs?searchtype=author&query=Fiaz%2C+M">Mustansar Fiaz</a>, 
<a href="/search/cs?searchtype=author&query=Cholakkal%2C+H">Hisham Cholakkal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in WACV-2024. Code is here at \url{<a href="https://github.com/mustansarfiaz/DDAM-PS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Person search (PS) is a challenging computer vision problem where the
objective is to achieve joint optimization for pedestrian detection and
re-identification (ReID). Although previous advancements have shown promising
performance in the field under fully and weakly supervised learning fashion,
there exists a major gap in investigating the domain adaptation ability of PS
models. In this paper, we propose a diligent domain adaptive mixer (DDAM) for
person search (DDAP-PS) framework that aims to bridge a gap to improve
knowledge transfer from the labeled source domain to the unlabeled target
domain. Specifically, we introduce a novel DDAM module that generates moderate
mixed-domain representations by combining source and target domain
representations. The proposed DDAM module encourages domain mixing to minimize
the distance between the two extreme domains, thereby enhancing the ReID task.
To achieve this, we introduce two bridge losses and a disparity loss. The
objective of the two bridge losses is to guide the moderate mixed-domain
representations to maintain an appropriate distance from both the source and
target domain representations. The disparity loss aims to prevent the moderate
mixed-domain representations from being biased towards either the source or
target domains, thereby avoiding overfitting. Furthermore, we address the
conflict between the two subtasks, localization and ReID, during domain
adaptation. To handle this cross-task conflict, we forcefully decouple the
norm-aware embedding, which aids in better learning of the moderate
mixed-domain representation. We conduct experiments to validate the
effectiveness of our proposed method. Our approach demonstrates favorable
performance on the challenging PRW and CUHK-SYSU datasets. Our source code is
publicly available at \url{https://github.com/mustansarfiaz/DDAM-PS}.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20707" title="Abstract">arXiv:2310.20707</a> [<a href="/pdf/2310.20707" title="Download PDF">pdf</a>, <a href="/format/2310.20707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s In My Big Data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elazar%2C+Y">Yanai Elazar</a>, 
<a href="/search/cs?searchtype=author&query=Bhagia%2C+A">Akshita Bhagia</a>, 
<a href="/search/cs?searchtype=author&query=Magnusson%2C+I">Ian Magnusson</a>, 
<a href="/search/cs?searchtype=author&query=Ravichander%2C+A">Abhilasha Ravichander</a>, 
<a href="/search/cs?searchtype=author&query=Schwenk%2C+D">Dustin Schwenk</a>, 
<a href="/search/cs?searchtype=author&query=Suhr%2C+A">Alane Suhr</a>, 
<a href="/search/cs?searchtype=author&query=Walsh%2C+P">Pete Walsh</a>, 
<a href="/search/cs?searchtype=author&query=Groeneveld%2C+D">Dirk Groeneveld</a>, 
<a href="/search/cs?searchtype=author&query=Soldaini%2C+L">Luca Soldaini</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sameer Singh</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hanna Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Dodge%2C+J">Jesse Dodge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large text corpora are the backbone of language models. However, we have a
limited understanding of the content of these corpora, including general
statistics, quality, social factors, and inclusion of evaluation data
(contamination). In this work, we propose What's In My Big Data? (WIMBD), a
platform and a set of sixteen analyses that allow us to reveal and compare the
contents of large text corpora. WIMBD builds on two basic capabilities -- count
and search -- at scale, which allows us to analyze more than 35 terabytes on a
standard compute node. We apply WIMBD to ten different corpora used to train
popular language models, including C4, The Pile, and RedPajama. Our analysis
uncovers several surprising and previously undocumented findings about these
corpora, including the high prevalence of duplicate, synthetic, and low-quality
content, personally identifiable information, toxic language, and benchmark
contamination. For instance, we find that about 50% of the documents in
RedPajama and LAION-2B-en are duplicates. In addition, several datasets used
for benchmarking models trained on such corpora are contaminated with respect
to important benchmarks, including the Winograd Schema Challenge and parts of
GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a
standard set of evaluations for new text-based corpora and to encourage more
analyses and transparency around them: github.com/allenai/wimbd.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20708" title="Abstract">arXiv:2310.20708</a> [<a href="/pdf/2310.20708" title="Download PDF">pdf</a>, <a href="/format/2310.20708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unexpected Improvements to Expected Improvement for Bayesian  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ament%2C+S">Sebastian Ament</a>, 
<a href="/search/cs?searchtype=author&query=Daulton%2C+S">Samuel Daulton</a>, 
<a href="/search/cs?searchtype=author&query=Eriksson%2C+D">David Eriksson</a>, 
<a href="/search/cs?searchtype=author&query=Balandat%2C+M">Maximilian Balandat</a>, 
<a href="/search/cs?searchtype=author&query=Bakshy%2C+E">Eytan Bakshy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">Expected Improvement (EI) is arguably the most popular acquisition function
in Bayesian optimization and has found countless successful applications, but
its performance is often exceeded by that of more recent methods. Notably, EI
and its variants, including for the parallel and multi-objective settings, are
challenging to optimize because their acquisition values vanish numerically in
many regions. This difficulty generally increases as the number of
observations, dimensionality of the search space, or the number of constraints
grow, resulting in performance that is inconsistent across the literature and
most often sub-optimal. Herein, we propose LogEI, a new family of acquisition
functions whose members either have identical or approximately equal optima as
their canonical counterparts, but are substantially easier to optimize
numerically. We demonstrate that numerical pathologies manifest themselves in
"classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as
their constrained, noisy, and parallel variants, and propose corresponding
reformulations that remedy these pathologies. Our empirical results show that
members of the LogEI family of acquisition functions substantially improve on
the optimization performance of their canonical counterparts and surprisingly,
are on par with or exceed the performance of recent state-of-the-art
acquisition functions, highlighting the understated role of numerical
optimization in the literature.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20710" title="Abstract">arXiv:2310.20710</a> [<a href="/pdf/2310.20710" title="Download PDF">pdf</a>, <a href="/format/2310.20710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance  Fields by Analyzing and Enhancing Fourier PlenOctrees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rabich%2C+S">Saskia Rabich</a>, 
<a href="/search/cs?searchtype=author&query=Stotko%2C+P">Patrick Stotko</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+R">Reinhard Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Fourier PlenOctrees have shown to be an efficient representation for
real-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many
advantages, this method suffers from artifacts introduced by the involved
compression when combining it with recent state-of-the-art techniques for
training the static per-frame NeRF models. In this paper, we perform an
in-depth analysis of these artifacts and leverage the resulting insights to
propose an improved representation. In particular, we present a novel density
encoding that adapts the Fourier-based compression to the characteristics of
the transfer function used by the underlying volume rendering procedure and
leads to a substantial reduction of artifacts in the dynamic model.
Furthermore, we show an augmentation of the training data that relaxes the
periodicity assumption of the compression. We demonstrate the effectiveness of
our enhanced Fourier PlenOctrees in the scope of quantitative and qualitative
evaluations on synthetic and real-world scenes.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Wed,  1 Nov 23</h3>
<dl>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15499" title="Abstract">arXiv:2305.15499</a> (cross-list from gr-qc) [<a href="/pdf/2305.15499" title="Download PDF">pdf</a>, <a href="/format/2305.15499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural network reconstruction of cosmology using the Pantheon  compilation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/gr-qc?searchtype=author&query=Dialektopoulos%2C+K+F">Konstantinos F. Dialektopoulos</a>, 
<a href="/search/gr-qc?searchtype=author&query=Mukherjee%2C+P">Purba Mukherjee</a>, 
<a href="/search/gr-qc?searchtype=author&query=Said%2C+J+L">Jackson Levi Said</a>, 
<a href="/search/gr-qc?searchtype=author&query=Mifsud%2C+J">Jurgen Mifsud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 12 sets of figures, Accepted for publication in EPJ C
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Eur. Phys. J. C 83, 956 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Relativity and Quantum Cosmology (gr-qc)</span>; Cosmology and Nongalactic Astrophysics (astro-ph.CO); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we reconstruct the Hubble diagram using various data sets,
including correlated ones, in Artificial Neural Networks (ANN). Using ReFANN,
that was built for data sets with independent uncertainties, we expand it to
include non-Guassian data points, as well as data sets with covariance matrices
among others. Furthermore, we compare our results with the existing ones
derived from Gaussian processes and we also perform null tests in order to test
the validity of the concordance model of cosmology.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19005" title="Abstract">arXiv:2310.19005</a> (cross-list from eess.SP) [<a href="/pdf/2310.19005" title="Download PDF">pdf</a>, <a href="/ps/2310.19005" title="Download PostScript">ps</a>, <a href="/format/2310.19005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel-based Joint Multiple Graph Learning and Clustering of Graph  Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alizade%2C+M+H">Mohamad H. Alizade</a>, 
<a href="/search/eess?searchtype=author&query=Einizade%2C+A">Aref Einizade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Within the context of Graph Signal Processing (GSP), Graph Learning (GL) is
concerned with the inference of a graph's topology from nodal observations,
i.e., graph signals. However, data is often in mixed form, relating to
different underlying structures. This heterogeneity necessitates the joint
clustering and learning of multiple graphs. In many real-life applications,
there are available node-side covariates (i.e., kernels) that imperatively
should be incorporated, which has not been addressed by the rare graph signal
clustering approaches. To this end and inspired by the rich K-means framework,
we propose a novel kernel-based algorithm to incorporate this node-side
information as we jointly partition the signals and learn a graph for each
cluster. Numerical experiments demonstrate its effectiveness over the
state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19812" title="Abstract">arXiv:2310.19812</a> (cross-list from eess.IV) [<a href="/pdf/2310.19812" title="Download PDF">pdf</a>, <a href="/format/2310.19812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brain decoding: toward real-time reconstruction of visual perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Benchetrit%2C+Y">Yohann Benchetrit</a>, 
<a href="/search/eess?searchtype=author&query=Banville%2C+H">Hubert Banville</a>, 
<a href="/search/eess?searchtype=author&query=King%2C+J">Jean-R&#xe9;mi King</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">In the past five years, the use of generative and foundational AI systems has
greatly improved the decoding of brain activity. Visual perception, in
particular, can now be decoded from functional Magnetic Resonance Imaging
(fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers
from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally
constrains its real-time usage. Here, we propose an alternative approach based
on magnetoencephalography (MEG), a neuroimaging device capable of measuring
brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we
develop an MEG decoding model trained with both contrastive and regression
objectives and consisting of three modules: i) pretrained embeddings obtained
from the image, ii) an MEG module trained end-to-end and iii) a pretrained
image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X
improvement of image-retrieval over classic linear decoders. Second, late brain
responses to images are best decoded with DINOv2, a recent foundational image
model. Third, image retrievals and generations both suggest that MEG signals
primarily contain high-level visual features, whereas the same approach applied
to 7T fMRI also recovers low-level features. Overall, these results provide an
important step towards the decoding - in real time - of the visual processes
continuously unfolding within the human brain.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19817" title="Abstract">arXiv:2310.19817</a> (cross-list from eess.AS) [<a href="/pdf/2310.19817" title="Download PDF">pdf</a>, <a href="/format/2310.19817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligibility prediction with a pretrained noise-robust automatic  speech recognition model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tu%2C+Z">Zehai Tu</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+N">Ning Ma</a>, 
<a href="/search/eess?searchtype=author&query=Barker%2C+J">Jon Barker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">This paper describes two intelligibility prediction systems derived from a
pretrained noise-robust automatic speech recognition (ASR) model for the second
Clarity Prediction Challenge (CPC2). One system is intrusive and leverages the
hidden representations of the ASR model. The other system is non-intrusive and
makes predictions with derived ASR uncertainty. The ASR model is only
pretrained with a simulated noisy speech corpus and does not take advantage of
the CPC2 data. For that reason, the intelligibility prediction systems are
robust to unseen scenarios given the accurate prediction performance on the
CPC2 evaluation.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19830" title="Abstract">arXiv:2310.19830</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.19830" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GalliformeSpectra: A Hen Breed Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Himel%2C+G+M+S">Galib Muhammad Shahriar Himel</a>, 
<a href="/search/q-bio?searchtype=author&query=Islam%2C+M+M">Md Masudul Islam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This article presents a comprehensive dataset featuring ten distinct hen
breeds, sourced from various regions, capturing the unique characteristics and
traits of each breed. The dataset encompasses Bielefeld, Blackorpington,
Brahma, Buckeye, Fayoumi, Leghorn, Newhampshire, Plymouthrock, Sussex, and
Turken breeds, offering a diverse representation of poultry commonly bred
worldwide. A total of 1010 original JPG images were meticulously collected,
showcasing the physical attributes, feather patterns, and distinctive features
of each hen breed. These images were subsequently standardized, resized, and
converted to PNG format for consistency within the dataset. The compilation,
although unevenly distributed across the breeds, provides a rich resource,
serving as a foundation for research and applications in poultry science,
genetics, and agricultural studies. This dataset holds significant potential to
contribute to various fields by enabling the exploration and analysis of unique
characteristics and genetic traits across different hen breeds, thereby
supporting advancements in poultry breeding, farming, and genetic research.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19831" title="Abstract">arXiv:2310.19831</a> (cross-list from stat.ML) [<a href="/pdf/2310.19831" title="Download PDF">pdf</a>, <a href="/format/2310.19831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining by Imitating: Understanding Decisions by Interpretable Policy  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=H%C3%BCy%C3%BCk%2C+A">Alihan H&#xfc;y&#xfc;k</a>, 
<a href="/search/stat?searchtype=author&query=Jarrett%2C+D">Daniel Jarrett</a>, 
<a href="/search/stat?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proc. 9th International Conference on Learning Representations
  (ICLR 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding human behavior from observed data is critical for transparency
and accountability in decision-making. Consider real-world settings such as
healthcare, in which modeling a decision-maker's policy is challenging -- with
no access to underlying states, no knowledge of environment dynamics, and no
allowance for live experimentation. We desire learning a data-driven
representation of decision-making behavior that (1) inheres transparency by
design, (2) accommodates partial observability, and (3) operates completely
offline. To satisfy these key criteria, we propose a novel model-based Bayesian
method for interpretable policy learning ("Interpole") that jointly estimates
an agent's (possibly biased) belief-update process together with their
(possibly suboptimal) belief-action mapping. Through experiments on both
simulated and real-world data for the problem of Alzheimer's disease diagnosis,
we illustrate the potential of our approach as an investigative device for
auditing, quantifying, and understanding human decision-making behavior.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19835" title="Abstract">arXiv:2310.19835</a> (cross-list from eess.IV) [<a href="/pdf/2310.19835" title="Download PDF">pdf</a>, <a href="/format/2310.19835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrossEAI: Using Explainable AI to generate better bounding boxes for  Chest X-ray images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+J">Jinze Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Explainability is critical for deep learning applications in healthcare which
are mandated to provide interpretations to both patients and doctors according
to legal regulations and responsibilities. Explainable AI methods, such as
feature importance using integrated gradients, model approximation using LIME,
or neuron activation and layer conductance to provide interpretations for
certain health risk predictions. In medical imaging diagnosis, disease
classification usually achieves high accuracy, but generated bounding boxes
have much lower Intersection over Union (IoU). Different methods with
self-supervised or semi-supervised learning strategies have been proposed, but
few improvements have been identified for bounding box generation. Previous
work shows that bounding boxes generated by these methods are usually larger
than ground truth and contain major non-disease area. This paper utilizes the
advantages of post-hoc AI explainable methods to generate bounding boxes for
chest x-ray image diagnosis. In this work, we propose CrossEAI which combines
heatmap and gradient map to generate more targeted bounding boxes. By using
weighted average of Guided Backpropagation and Grad-CAM++, we are able to
generate bounding boxes which are closer to the ground truth. We evaluate our
model on a chest x-ray dataset. The performance has significant improvement
over the state of the art model with the same setting, with $9\%$ improvement
in average of all diseases over all IoU. Moreover, as a model that does not use
any ground truth bounding box information for training, we achieve same
performance in general as the model that uses $80\%$ of the ground truth
bounding box information for training
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19844" title="Abstract">arXiv:2310.19844</a> (cross-list from astro-ph.IM) [<a href="/pdf/2310.19844" title="Download PDF">pdf</a>, <a href="/format/2310.19844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Analysis of Robust Ballistic Landings on the Secondary of a  Binary Asteroid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Fodde%2C+I">Iosto Fodde</a>, 
<a href="/search/astro-ph?searchtype=author&query=Feng%2C+J">Jinglang Feng</a>, 
<a href="/search/astro-ph?searchtype=author&query=Vasile%2C+M">Massimiliano Vasile</a>, 
<a href="/search/astro-ph?searchtype=author&query=Gil-Fern%C3%A1ndez%2C+J">Jes&#xfa;s Gil-Fern&#xe1;ndez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Earth and Planetary Astrophysics (astro-ph.EP); Numerical Analysis (math.NA); Optimization and Control (math.OC); Probability (math.PR)

</div>
<p class="mathjax">ESA's Hera mission aims to visit binary asteroid Didymos in late 2026,
investigating its physical characteristics and the result of NASA's impact by
the DART spacecraft in more detail. Two CubeSats on-board Hera plan to perform
a ballistic landing on the secondary of the system, called Dimorphos. For these
types of landings the translational state during descent is not controlled,
reducing the spacecrafts complexity but also increasing its sensitivity to
deployment maneuver errors and dynamical uncertainties. This paper introduces a
novel methodology to analyse the effect of these uncertainties on the dynamics
of the lander and design a trajectory that is robust against them. This
methodology consists of propagating the uncertain state of the lander using the
non-intrusive Chebyshev interpolation (NCI) technique, which approximates the
uncertain dynamics using a polynomial expansion, and analysing the results
using the pseudo-diffusion indicator, derived from the coefficients of the
polynomial expansion, which quantifies the rate of growth of the set of
possible states of the spacecraft over time. This indicator is used here to
constrain the impact velocity and angle to values which allow for successful
settling on the surface. This information is then used to optimize the landing
trajectory by applying the NCI technique inside the transcription of the
problem. The resulting trajectory increases the robustness of the trajectory
compared to a conventional method, improving the landing success by 20 percent
and significantly reducing the landing footprint.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19849" title="Abstract">arXiv:2310.19849</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.19849" title="Download PDF">pdf</a>, <a href="/format/2310.19849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting mutational effects on protein-protein binding via a  side-chain diffusion probabilistic model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhu%2C+T">Tian Zhu</a>, 
<a href="/search/q-bio?searchtype=author&query=Ren%2C+M">Milong Ren</a>, 
<a href="/search/q-bio?searchtype=author&query=Yu%2C+C">Chungong Yu</a>, 
<a href="/search/q-bio?searchtype=author&query=Bu%2C+D">Dongbo Bu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+H">Haicang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Many crucial biological processes rely on networks of protein-protein
interactions. Predicting the effect of amino acid mutations on protein-protein
binding is vital in protein engineering and therapeutic discovery. However, the
scarcity of annotated experimental data on binding energy poses a significant
challenge for developing computational approaches, particularly deep
learning-based methods. In this work, we propose SidechainDiff, a
representation learning-based approach that leverages unlabelled experimental
protein structures. SidechainDiff utilizes a Riemannian diffusion model to
learn the generative process of side-chain conformations and can also give the
structural context representations of mutations on the protein-protein
interface. Leveraging the learned representations, we achieve state-of-the-art
performance in predicting the mutational effects on protein-protein binding.
Furthermore, SidechainDiff is the first diffusion-based generative model for
side-chains, distinguishing it from prior efforts that have predominantly
focused on generating protein backbone structures.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19870" title="Abstract">arXiv:2310.19870</a> (cross-list from hep-th) [<a href="/pdf/2310.19870" title="Download PDF">pdf</a>, <a href="/format/2310.19870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metric Flows with Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-th?searchtype=author&query=Halverson%2C+J">James Halverson</a>, 
<a href="/search/hep-th?searchtype=author&query=Ruehle%2C+F">Fabian Ruehle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages + references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Theory (hep-th)</span>; Machine Learning (cs.LG); Differential Geometry (math.DG)

</div>
<p class="mathjax">We develop a theory of flows in the space of Riemannian metrics induced by
neural network gradient descent. This is motivated in part by recent advances
in approximating Calabi-Yau metrics with neural networks and is enabled by
recent advances in understanding flows in the space of neural networks. We
derive the corresponding metric flow equations, which are governed by a metric
neural tangent kernel, a complicated, non-local object that evolves in time.
However, many architectures admit an infinite-width limit in which the kernel
becomes fixed and the dynamics simplify. Additional assumptions can induce
locality in the flow, which allows for the realization of Perelman's
formulation of Ricci flow that was used to resolve the 3d Poincar\'e
conjecture. We apply these ideas to numerical Calabi-Yau metrics, including a
discussion on the importance of feature learning.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19882" title="Abstract">arXiv:2310.19882</a> (cross-list from quant-ph) [<a href="/pdf/2310.19882" title="Download PDF">pdf</a>, <a href="/format/2310.19882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning quantum states and unitaries of bounded gate complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhao%2C+H">Haimeng Zhao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lewis%2C+L">Laura Lewis</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kannan%2C+I">Ishaan Kannan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Quek%2C+Y">Yihui Quek</a>, 
<a href="/search/quant-ph?searchtype=author&query=Huang%2C+H">Hsin-Yuan Huang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Caro%2C+M+C">Matthias C. Caro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure, 1 table + 56-page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Machine Learning (cs.LG)

</div>
<p class="mathjax">While quantum state tomography is notoriously hard, most states hold little
interest to practically-minded tomographers. Given that states and unitaries
appearing in Nature are of bounded gate complexity, it is natural to ask if
efficient learning becomes possible. In this work, we prove that to learn a
state generated by a quantum circuit with $G$ two-qubit gates to a small trace
distance, a sample complexity scaling linearly in $G$ is necessary and
sufficient. We also prove that the optimal query complexity to learn a unitary
generated by $G$ gates to a small average-case error scales linearly in $G$.
While sample-efficient learning can be achieved, we show that under reasonable
cryptographic conjectures, the computational complexity for learning states and
unitaries of gate complexity $G$ must scale exponentially in $G$. We illustrate
how these results establish fundamental limitations on the expressivity of
quantum machine learning models and provide new perspectives on no-free-lunch
theorems in unitary learning. Together, our results answer how the complexity
of learning quantum states and unitaries relate to the complexity of creating
these states and unitaries.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19910" title="Abstract">arXiv:2310.19910</a> (cross-list from astro-ph.CO) [<a href="/pdf/2310.19910" title="Download PDF">pdf</a>, <a href="/format/2310.19910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Simulation-based Inference for Cosmological Initial Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=List%2C+F">Florian List</a>, 
<a href="/search/astro-ph?searchtype=author&query=Montel%2C+N+A">Noemi Anau Montel</a>, 
<a href="/search/astro-ph?searchtype=author&query=Weniger%2C+C">Christoph Weniger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the NeurIPS 2023 workshop Machine Learning and the Physical Sciences; 5 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reconstructing astrophysical and cosmological fields from observations is
challenging. It requires accounting for non-linear transformations, mixing of
spatial structure, and noise. In contrast, forward simulators that map fields
to observations are readily available for many applications. We present a
versatile Bayesian field reconstruction algorithm rooted in simulation-based
inference and enhanced by autoregressive modeling. The proposed technique is
applicable to generic (non-differentiable) forward simulators and allows
sampling from the posterior for the underlying field. We show first promising
results on a proof-of-concept application: the recovery of cosmological initial
conditions from late-time density fields.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19914" title="Abstract">arXiv:2310.19914</a> (cross-list from quant-ph) [<a href="/pdf/2310.19914" title="Download PDF">pdf</a>, <a href="/format/2310.19914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient entanglement purification based on noise guessing decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Roque%2C+A">Andr&#xe9; Roque</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cruz%2C+D">Diogo Cruz</a>, 
<a href="/search/quant-ph?searchtype=author&query=Monteiro%2C+F+A">Francisco A. Monteiro</a>, 
<a href="/search/quant-ph?searchtype=author&query=Coutinho%2C+B+C">Bruno C. Coutinho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">In this paper, we propose a novel bipartite entanglement purification
protocol built upon hashing and upon the guessing random additive noise
decoding (GRAND) approach recently devised for classical error correction
codes. Our protocol offers substantial advantages over existing hashing
protocols, requiring fewer qubits for purification, achieving higher
fidelities, and delivering better yields with reduced computational costs. We
provide numerical and semi-analytical results to corroborate our findings and
provide a detailed comparison with the hashing protocol of Bennet et al.
Although that pioneering work devised performance bounds, it did not offer an
explicit construction for implementation. The present work fills that gap,
offering both an explicit and more efficient purification method. We
demonstrate that our protocol is capable of purifying states with noise on the
order of 10% per Bell pair even with a small ensemble of 16 pairs. The work
explores a measurement-based implementation of the protocol to address
practical setups with noise. This work opens the path to practical and
efficient entanglement purification using hashing-based methods with feasible
computational costs. Compared to the original hashing protocol, the proposed
method can achieve some desired fidelity with a number of initial resources up
to one hundred times smaller. Therefore, the proposed method seems well-fit for
future quantum networks with a limited number of resources and entails a
relatively low computational overhead.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19920" title="Abstract">arXiv:2310.19920</a> (cross-list from math.OC) [<a href="/pdf/2310.19920" title="Download PDF">pdf</a>, <a href="/format/2310.19920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving a Class of Cut-Generating Linear Programs via Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rajabalizadeh%2C+A">Atefeh Rajabalizadeh</a>, 
<a href="/search/math?searchtype=author&query=Davarnia%2C+D">Danial Davarnia</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> INFORMS Journal on Computing, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Cut-generating linear programs (CGLPs) play a key role as a separation oracle
to produce valid inequalities for the feasible region of mixed-integer
programs. When incorporated inside branch-and-bound, the cutting planes
obtained from CGLPs help to tighten relaxations and improve dual bounds.
However, running the CGLPs at the nodes of the branch-and-bound tree is
computationally cumbersome due to the large number of node candidates and the
lack of a priori knowledge on which nodes admit useful cutting planes. As a
result, CGLPs are often avoided at default settings of branch-and-cut
algorithms despite their potential impact on improving dual bounds. In this
paper, we propose a novel framework based on machine learning to approximate
the optimal value of a CGLP class that determines whether a cutting plane can
be generated at a node of the branch-and-bound tree. Translating the CGLP as an
indicator function of the objective function vector, we show that it can be
approximated through conventional data classification techniques. We provide a
systematic procedure to efficiently generate training data sets for the
corresponding classification problem based on the CGLP structure. We conduct
computational experiments on benchmark instances using classification methods
such as logistic regression. These results suggest that the approximate CGLP
obtained from classification can improve the solution time compared to that of
conventional cutting plane methods. Our proposed framework can be efficiently
applied to a large number of nodes in the branch-and-bound tree to identify the
best candidates for adding a cut.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19973" title="Abstract">arXiv:2310.19973</a> (cross-list from stat.ML) [<a href="/pdf/2310.19973" title="Download PDF">pdf</a>, <a href="/format/2310.19973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Enhancement of Privacy Bounds for Mixture Mechanisms via  $f$-Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+C">Chendi Wang</a>, 
<a href="/search/stat?searchtype=author&query=Su%2C+B">Buxin Su</a>, 
<a href="/search/stat?searchtype=author&query=Ye%2C+J">Jiayuan Ye</a>, 
<a href="/search/stat?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>, 
<a href="/search/stat?searchtype=author&query=Su%2C+W+J">Weijie J. Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)

</div>
<p class="mathjax">Differentially private (DP) machine learning algorithms incur many sources of
randomness, such as random initialization, random batch subsampling, and
shuffling. However, such randomness is difficult to take into account when
proving differential privacy bounds because it induces mixture distributions
for the algorithm's output that are difficult to analyze. This paper focuses on
improving privacy bounds for shuffling models and one-iteration differentially
private gradient descent (DP-GD) with random initializations using $f$-DP. We
derive a closed-form expression of the trade-off function for shuffling models
that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP.
Moreover, we investigate the effects of random initialization on the privacy of
one-iteration DP-GD. Our numerical computations of the trade-off function
indicate that random initialization can enhance the privacy of DP-GD. Our
analysis of $f$-DP guarantees for these mixture mechanisms relies on an
inequality for trade-off functions introduced in this paper. This inequality
implies the joint convexity of $F$-divergences. Finally, we study an $f$-DP
analog of the advanced joint convexity of the hockey-stick divergence related
to $(\epsilon,\delta)$-DP and apply it to analyze the privacy of mixture
mechanisms.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19982" title="Abstract">arXiv:2310.19982</a> (cross-list from math.CO) [<a href="/pdf/2310.19982" title="Download PDF">pdf</a>, <a href="/format/2310.19982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polychromatic Colorings of Geometric Hypergraphs via Shallow Hitting  Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Planken%2C+T">Tim Planken</a>, 
<a href="/search/math?searchtype=author&query=Ueckerdt%2C+T">Torsten Ueckerdt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">A range family $\mathcal{R}$ is a family of subsets of $\mathbb{R}^d$, like
all halfplanes, or all unit disks. Given a range family $\mathcal{R}$, we
consider the $m$-uniform range capturing hypergraphs
$\mathcal{H}(V,\mathcal{R},m)$ whose vertex-sets $V$ are finite sets of points
in $\mathbb{R}^d$ with any $m$ vertices forming a hyperedge $e$ whenever $e = V
\cap R$ for some $R \in \mathcal{R}$. Given additionally an integer $k \geq 2$,
we seek to find the minimum $m = m_{\mathcal{R}}(k)$ such that every
$\mathcal{H}(V,\mathcal{R},m)$ admits a polychromatic $k$-coloring of its
vertices, that is, where every hyperedge contains at least one point of each
color. Clearly, $m_{\mathcal{R}}(k) \geq k$ and the gold standard is an upper
bound $m_{\mathcal{R}}(k) = O(k)$ that is linear in $k$.
<br />A $t$-shallow hitting set in $\mathcal{H}(V,\mathcal{R},m)$ is a subset $S
\subseteq V$ such that $1 \leq |e \cap S| \leq t$ for each hyperedge $e$; i.e.,
every hyperedge is hit at least once but at most $t$ times by $S$. We show for
several range families $\mathcal{R}$ the existence of $t$-shallow hitting sets
in every $\mathcal{H}(V,\mathcal{R},m)$ with $t$ being a constant only
depending on $\mathcal{R}$. This in particular proves that $m_{\mathcal{R}}(k)
\leq tk = O(k)$ in such cases, improving previous polynomial bounds in $k$.
Particularly, we prove this for the range families of all axis-aligned strips
in $\mathbb{R}^d$, all bottomless and topless rectangles in $\mathbb{R}^2$, and
for all unit-height axis-aligned rectangles in $\mathbb{R}^2$.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20007" title="Abstract">arXiv:2310.20007</a> (cross-list from stat.ML) [<a href="/pdf/2310.20007" title="Download PDF">pdf</a>, <a href="/ps/2310.20007" title="Download PostScript">ps</a>, <a href="/format/2310.20007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Moradipari%2C+A">Ahmadreza Moradipari</a>, 
<a href="/search/stat?searchtype=author&query=Pedramfar%2C+M">Mohammad Pedramfar</a>, 
<a href="/search/stat?searchtype=author&query=Zini%2C+M+S">Modjtaba Shokrian Zini</a>, 
<a href="/search/stat?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we prove the first Bayesian regret bounds for Thompson
Sampling in reinforcement learning in a multitude of settings. We simplify the
learning problem using a discrete set of surrogate environments, and present a
refined analysis of the information ratio using posterior consistency. This
leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time
inhomogeneous reinforcement learning problem where $H$ is the episode length
and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments.
We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as
tabular, linear and finite mixtures, and discuss how how our results are either
the first of their kind or improve the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20031" title="Abstract">arXiv:2310.20031</a> (cross-list from q-bio.TO) [<a href="/pdf/2310.20031" title="Download PDF">pdf</a>, <a href="/format/2310.20031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recipes for calibration and validation of agent-based models in cancer  biomedicine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Cogno%2C+N">Nicol&#xf2; Cogno</a>, 
<a href="/search/q-bio?searchtype=author&query=Axenie%2C+C">Cristian Axenie</a>, 
<a href="/search/q-bio?searchtype=author&query=Bauer%2C+R">Roman Bauer</a>, 
<a href="/search/q-bio?searchtype=author&query=Vavourakis%2C+V">Vasileios Vavourakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Tissues and Organs (q-bio.TO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Computational models and simulations are not just appealing because of their
intrinsic characteristics across spatiotemporal scales, scalability, and
predictive power, but also because the set of problems in cancer biomedicine
that can be addressed computationally exceeds the set of those amenable to
analytical solutions. Agent-based models and simulations are especially
interesting candidates among computational modelling strategies in cancer
research due to their capabilities to replicate realistic local and global
interaction dynamics at a convenient and relevant scale. Yet, the absence of
methods to validate the consistency of the results across scales can hinder
adoption by turning fine-tuned models into black boxes. This review compiles
relevant literature to explore strategies to leverage high-fidelity simulations
of multi-scale, or multi-level, cancer models with a focus on validation
approached as simulation calibration. We argue that simulation calibration goes
beyond parameter optimization by embedding informative priors to generate
plausible parameter configurations across multiple dimensions.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20039" title="Abstract">arXiv:2310.20039</a> (cross-list from eess.IV) [<a href="/pdf/2310.20039" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Radiomics as a measure superior to the Dice similarity coefficient for  tumor segmentation performance evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Watanabe%2C+Y">Yoichi Watanabe</a> (1), 
<a href="/search/eess?searchtype=author&query=Akramova%2C+R">Rukhsora Akramova</a> (1) ((1) Department of Radiation Oncology, University of Minnesota Medical School, Minneapolis, MN, USA)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">In high-quality radiotherapy delivery, precise segmentation of targets and
healthy structures is essential. This study proposes Radiomics features as a
superior measure for assessing the segmentation ability of physicians and
auto-segmentation tools, in comparison to the widely used Dice Similarity
Coefficient (DSC). The research involves selecting reproducible radiomics
features for evaluating segmentation accuracy by analyzing radiomics data from
2 CT scans of 10 lung tumors, available in the RIDER Data Library. Radiomics
features were extracted using PyRadiomics, with selection based on the
Concordance Correlation Coefficient (CCC). Subsequently, CT images from 10
patients, each segmented by different physicians or auto-segmentation tools,
were used to assess segmentation performance. The study reveals 206 radiomics
features with a CCC greater than 0.93 between the two CT images, indicating
robust reproducibility. Among these features, seven exhibit low Intraclass
Correlation Coefficients (ICC), signifying increased sensitivity to
segmentation differences. Notably, ICCs of original shape features, including
sphericity, elongation, and flatness, ranged from 0.1177 to 0.995. In contrast,
all DSC values exceeded 0.778. This research demonstrates that radiomics
features, particularly those related to shape and energy, can capture subtle
variations in tumor segmentation characteristics, unlike DSC. As a result,
Radiomics features with ICC prove superior for evaluating a physician's tumor
segmentation ability and the performance of auto-segmentation tools. The
findings suggest that these new metrics can be employed to assess novel
auto-segmentation methods and enhance the training of individuals in medical
segmentation, thus contributing to improved radiotherapy practices.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20053" title="Abstract">arXiv:2310.20053</a> (cross-list from stat.ML) [<a href="/pdf/2310.20053" title="Download PDF">pdf</a>, <a href="/format/2310.20053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ujv%C3%A1ry%2C+S">Szilvia Ujv&#xe1;ry</a>, 
<a href="/search/stat?searchtype=author&query=Flamich%2C+G">Gergely Flamich</a>, 
<a href="/search/stat?searchtype=author&query=Fortuin%2C+V">Vincent Fortuin</a>, 
<a href="/search/stat?searchtype=author&query=Lobato%2C+J+M+H">Jos&#xe9; Miguel Hern&#xe1;ndez Lobato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Mathematics of Modern Machine Learning Workshop at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">An important yet underexplored question in the PAC-Bayes literature is how
much tightness we lose by restricting the posterior family to factorized
Gaussian distributions when optimizing a PAC-Bayes bound. We investigate this
issue by estimating data-independent PAC-Bayes bounds using the optimal
posteriors, comparing them to bounds obtained using MFVI. Concretely, we (1)
sample from the optimal Gibbs posterior using Hamiltonian Monte Carlo, (2)
estimate its KL divergence from the prior with thermodynamic integration, and
(3) propose three methods to obtain high-probability bounds under different
assumptions. Our experiments on the MNIST dataset reveal significant tightness
gaps, as much as 5-6\% in some cases.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20060" title="Abstract">arXiv:2310.20060</a> (cross-list from math.OC) [<a href="/pdf/2310.20060" title="Download PDF">pdf</a>, <a href="/format/2310.20060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaSub: Stochastic Optimization Using Second-Order Information in  Low-Dimensional Subspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=da+Mata%2C+J+V+G">Jo&#xe3;o Victor Galv&#xe3;o da Mata</a>, 
<a href="/search/math?searchtype=author&query=Andersen%2C+M+S">Martin S. Andersen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce AdaSub, a stochastic optimization algorithm that computes a
search direction based on second-order information in a low-dimensional
subspace that is defined adaptively based on available current and past
information. Compared to first-order methods, second-order methods exhibit
better convergence characteristics, but the need to compute the Hessian matrix
at each iteration results in excessive computational expenses, making them
impractical. To address this issue, our approach enables the management of
computational expenses and algorithm efficiency by enabling the selection of
the subspace dimension for the search. Our code is freely available on GitHub,
and our preliminary numerical results demonstrate that AdaSub surpasses popular
stochastic optimizers in terms of time and number of iterations required to
reach a given accuracy.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20079" title="Abstract">arXiv:2310.20079</a> (cross-list from physics.plasm-ph) [<a href="/pdf/2310.20079" title="Download PDF">pdf</a>, <a href="/format/2310.20079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance  Dynamics in Tokamak Fusion Reactors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+A+M">Allen M. Wang</a>, 
<a href="/search/physics?searchtype=author&query=Garnier%2C+D+T">Darren T. Garnier</a>, 
<a href="/search/physics?searchtype=author&query=Rea%2C+C">Cristina Rea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Plasma Physics (physics.plasm-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While fusion reactors known as tokamaks hold promise as a firm energy source,
advances in plasma control, and handling of events where control of plasmas is
lost, are needed for them to be economical. A significant bottleneck towards
applying more advanced control algorithms is the need for better plasma
simulation, where both physics-based and data-driven approaches currently fall
short. The former is bottle-necked by both computational cost and the
difficulty of modelling plasmas, and the latter is bottle-necked by the
relative paucity of data. To address this issue, this work applies the neural
ordinary differential equations (ODE) framework to the problem of predicting a
subset of plasma dynamics, namely the coupled plasma current and internal
inductance dynamics. As the neural ODE framework allows for the natural
inclusion of physics-based inductive biases, we train both physics-based and
neural network models on data from the Alcator C-Mod fusion reactor and find
that a model that combines physics-based equations with a neural ODE performs
better than both existing physics-motivated ODEs and a pure neural ODE model.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20090" title="Abstract">arXiv:2310.20090</a> (cross-list from stat.ML) [<a href="/pdf/2310.20090" title="Download PDF">pdf</a>, <a href="/format/2310.20090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Gap Between Variational Inference and Wasserstein Gradient  Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yi%2C+M">Mingxuan Yi</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+S">Song Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">Variational inference is a technique that approximates a target distribution
by optimizing within the parameter space of variational families. On the other
hand, Wasserstein gradient flows describe optimization within the space of
probability measures where they do not necessarily admit a parametric density
function. In this paper, we bridge the gap between these two methods. We
demonstrate that, under certain conditions, the Bures-Wasserstein gradient flow
can be recast as the Euclidean gradient flow where its forward Euler scheme is
the standard black-box variational inference algorithm. Specifically, the
vector field of the gradient flow is generated via the path-derivative gradient
estimator. We also offer an alternative perspective on the path-derivative
gradient, framing it as a distillation procedure to the Wasserstein gradient
flow. Distillations can be extended to encompass $f$-divergences and
non-Gaussian variational families. This extension yields a new gradient
estimator for $f$-divergences, readily implementable using contemporary machine
learning libraries like PyTorch or TensorFlow.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20101" title="Abstract">arXiv:2310.20101</a> (cross-list from eess.IV) [<a href="/pdf/2310.20101" title="Download PDF">pdf</a>, <a href="/format/2310.20101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Medical Image Denosing via Explainable AI Feature Preserving Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dong%2C+G">Guanfang Dong</a>, 
<a href="/search/eess?searchtype=author&query=Basu%2C+A">Anup Basu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Denoising algorithms play a crucial role in medical image processing and
analysis. However, classical denoising algorithms often ignore explanatory and
critical medical features preservation, which may lead to misdiagnosis and
legal liabilities.In this work, we propose a new denoising method for medical
images that not only efficiently removes various types of noise, but also
preserves key medical features throughout the process. To achieve this goal, we
utilize a gradient-based eXplainable Artificial Intelligence (XAI) approach to
design a feature preserving loss function. Our feature preserving loss function
is motivated by the characteristic that gradient-based XAI is sensitive to
noise. Through backpropagation, medical image features before and after
denoising can be kept consistent. We conducted extensive experiments on three
available medical image datasets, including synthesized 13 different types of
noise and artifacts. The experimental results demonstrate the superiority of
our method in terms of denoising performance, model explainability, and
generalization.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20102" title="Abstract">arXiv:2310.20102</a> (cross-list from stat.ML) [<a href="/pdf/2310.20102" title="Download PDF">pdf</a>, <a href="/ps/2310.20102" title="Download PostScript">ps</a>, <a href="/format/2310.20102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic  Generalization Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+Z">Ziqiao Wang</a>, 
<a href="/search/stat?searchtype=author&query=Mao%2C+Y">Yongyi Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present new information-theoretic generalization guarantees through the a
novel construction of the "neighboring-hypothesis" matrix and a new family of
stability notions termed sample-conditioned hypothesis (SCH) stability. Our
approach yields sharper bounds that improve upon previous information-theoretic
bounds in various learning scenarios. Notably, these bounds address the
limitations of existing information-theoretic bounds in the context of
stochastic convex optimization (SCO) problems, as explored in the recent work
by Haghifam et al. (2023).
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20140" title="Abstract">arXiv:2310.20140</a> (cross-list from eess.IV) [<a href="/pdf/2310.20140" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Diabetic Foot Ulcer Images with Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Basiri%2C+R">Reza Basiri</a>, 
<a href="/search/eess?searchtype=author&query=Manji%2C+K">Karim Manji</a>, 
<a href="/search/eess?searchtype=author&query=Harton%2C+F">Francois Harton</a>, 
<a href="/search/eess?searchtype=author&query=Poonja%2C+A">Alisha Poonja</a>, 
<a href="/search/eess?searchtype=author&query=Popovic%2C+M+R">Milos R. Popovic</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+S+S">Shehroz S. Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, 6th Workshop on AI for Aging, Rehabilitation and Intelligent Assisted Living at European Conference on Machine Learning, Italy, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diabetic Foot Ulcer (DFU) is a serious skin wound requiring specialized care.
However, real DFU datasets are limited, hindering clinical training and
research activities. In recent years, generative adversarial networks and
diffusion models have emerged as powerful tools for generating synthetic images
with remarkable realism and diversity in many applications. This paper explores
the potential of diffusion models for synthesizing DFU images and evaluates
their authenticity through expert clinician assessments. Additionally,
evaluation metrics such as Frechet Inception Distance (FID) and Kernel
Inception Distance (KID) are examined to assess the quality of the synthetic
DFU images. A dataset of 2,000 DFU images is used for training the diffusion
model, and the synthetic images are generated by applying diffusion processes.
The results indicate that the diffusion model successfully synthesizes visually
indistinguishable DFU images. 70% of the time, clinicians marked synthetic DFU
images as real DFUs. However, clinicians demonstrate higher unanimous
confidence in rating real images than synthetic ones. The study also reveals
that FID and KID metrics do not significantly align with clinicians'
assessments, suggesting alternative evaluation approaches are needed. The
findings highlight the potential of diffusion models for generating synthetic
DFU images and their impact on medical training programs and research in wound
detection and classification.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20155" title="Abstract">arXiv:2310.20155</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.20155" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLatom 3: Platform for machine learning-enhanced computational chemistry  simulations and workflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Dral%2C+P+O">Pavlo O. Dral</a>, 
<a href="/search/physics?searchtype=author&query=Ge%2C+F">Fuchun Ge</a>, 
<a href="/search/physics?searchtype=author&query=Hou%2C+Y">Yi-Fan Hou</a>, 
<a href="/search/physics?searchtype=author&query=Zheng%2C+P">Peikun Zheng</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+Y">Yuxinxin Chen</a>, 
<a href="/search/physics?searchtype=author&query=Barbatti%2C+M">Mario Barbatti</a>, 
<a href="/search/physics?searchtype=author&query=Isayev%2C+O">Olexandr Isayev</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+C">Cheng Wang</a>, 
<a href="/search/physics?searchtype=author&query=Xue%2C+B">Bao-Xin Xue</a>, 
<a href="/search/physics?searchtype=author&query=Pinheiro%2C+M">Max Pinheiro Jr</a>, 
<a href="/search/physics?searchtype=author&query=Su%2C+Y">Yuming Su</a>, 
<a href="/search/physics?searchtype=author&query=Dai%2C+Y">Yiheng Dai</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+Y">Yangtao Chen</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+L">Lina Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+S">Shuang Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Ullah%2C+A">Arif Ullah</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+Q">Quanhao Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Ou%2C+Y">Yanchi Ou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Machine learning (ML) is increasingly becoming a common tool in computational
chemistry. At the same time, the rapid development of ML methods requires a
flexible software framework for designing custom workflows. MLatom 3 is a
program package designed to leverage the power of ML to enhance typical
computational chemistry simulations and to create complex workflows. This
open-source package provides plenty of choice to the users who can run
simulations with the command line options, input files, or with scripts using
MLatom as a Python package, both on their computers and on the online XACS
cloud computing at XACScloud.com. Computational chemists can calculate energies
and thermochemical properties, optimize geometries, run molecular and quantum
dynamics, and simulate (ro)vibrational, one-photon UV/vis absorption, and
two-photon absorption spectra with ML, quantum mechanical, and combined models.
The users can choose from an extensive library of methods containing
pre-trained ML models and quantum mechanical approximations such as AIQM1
approaching coupled-cluster accuracy. The developers can build their own models
using various ML algorithms. The great flexibility of MLatom is largely due to
the extensive use of the interfaces to many state-of-the-art software packages
and libraries.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20172" title="Abstract">arXiv:2310.20172</a> (cross-list from gr-qc) [<a href="/pdf/2310.20172" title="Download PDF">pdf</a>, <a href="/format/2310.20172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compact Binary Systems Waveform Generation with Generative Pre-trained  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/gr-qc?searchtype=author&query=Shi%2C+R">Ruijun Shi</a>, 
<a href="/search/gr-qc?searchtype=author&query=Zhou%2C+Y">Yue Zhou</a>, 
<a href="/search/gr-qc?searchtype=author&query=Zhao%2C+T">Tianyu Zhao</a>, 
<a href="/search/gr-qc?searchtype=author&query=Cao%2C+Z">Zhoujian Cao</a>, 
<a href="/search/gr-qc?searchtype=author&query=Ren%2C+Z">Zhixiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Relativity and Quantum Cosmology (gr-qc)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)

</div>
<p class="mathjax">Space-based gravitational wave detection is one of the most anticipated
gravitational wave (GW) detection projects in the next decade, which will
detect abundant compact binary systems. However, the precise prediction of
space GW waveforms remains unexplored. To solve the data processing difficulty
in the increasing waveform complexity caused by detectors' response and
second-generation time-delay interferometry (TDI 2.0), an interpretable
pre-trained large model named CBS-GPT (Compact Binary Systems Waveform
Generation with Generative Pre-trained Transformer) is proposed. For compact
binary system waveforms, three models were trained to predict the waveforms of
massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and
galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%,
respectively. The CBS-GPT model exhibits notable interpretability, with its
hidden parameters effectively capturing the intricate information of waveforms,
even with complex instrument response and a wide parameter range. Our research
demonstrates the potential of large pre-trained models in gravitational wave
data processing, opening up new opportunities for future tasks such as gap
completion, GW signal detection, and signal noise reduction.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20205" title="Abstract">arXiv:2310.20205</a> (cross-list from math.CO) [<a href="/pdf/2310.20205" title="Download PDF">pdf</a>, <a href="/ps/2310.20205" title="Download PostScript">ps</a>, <a href="/format/2310.20205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The differential properties of certain permutation polynomials over  finite fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Garg%2C+K">Kirpa Garg</a>, 
<a href="/search/math?searchtype=author&query=Hasan%2C+S+U">Sartaj Ul Hasan</a>, 
<a href="/search/math?searchtype=author&query=Stanica%2C+P">Pantelimon Stanica</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Finding functions, particularly permutations, with good differential
properties has received a lot of attention due to their possible applications.
For instance, in combinatorial design theory, a correspondence of perfect
$c$-nonlinear functions and difference sets in some quasigroups was recently
shown [1]. Additionally, in a recent manuscript by Pal and Stanica [20], a very
interesting connection between the $c$-differential uniformity and boomerang
uniformity when $c=-1$ was pointed out, showing that that they are the same for
an odd APN permutations. This makes the construction of functions with low
$c$-differential uniformity an intriguing problem. We investigate the
$c$-differential uniformity of some classes of permutation polynomials. As a
result, we add four more classes of permutation polynomials to the family of
functions that only contains a few (non-trivial) perfect $c$-nonlinear
functions over finite fields of even characteristic. Moreover, we include a
class of permutation polynomials with low $c$-differential uniformity over the
field of characteristic~$3$. As a byproduct, our proofs shows the permutation
property of these classes. To solve the involved equations over finite fields,
we use various techniques, in particular, we find explicitly many Walsh
transform coefficients and Weil sums that may be of an independent interest.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20224" title="Abstract">arXiv:2310.20224</a> (cross-list from stat.ML) [<a href="/pdf/2310.20224" title="Download PDF">pdf</a>, <a href="/format/2310.20224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with  Graphs for Passenger Trajectory Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+Z">Ziyue Li</a>, 
<a href="/search/stat?searchtype=author&query=Yan%2C+H">Hao Yan</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+L">Lijun Sun</a>, 
<a href="/search/stat?searchtype=author&query=Ketter%2C+W">Wolfgang Ketter</a>, 
<a href="/search/stat?searchtype=author&query=Tsung%2C+F">Fugee Tsung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ACM SIGSPATIAL 2023. arXiv admin note: substantial text overlap with <a href="/abs/2306.13794">arXiv:2306.13794</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">Passenger clustering based on trajectory records is essential for
transportation operators. However, existing methods cannot easily cluster the
passengers due to the hierarchical structure of the passenger trip information,
including multiple trips within each passenger and multi-dimensional
information about each trip. Furthermore, existing approaches rely on an
accurate specification of the clustering number to start. Finally, existing
methods do not consider spatial semantic graphs such as geographical proximity
and functional similarity between the locations. In this paper, we propose a
novel tensor Dirichlet Process Multinomial Mixture model with graphs, which can
preserve the hierarchical structure of the multi-dimensional trip information
and cluster them in a unified one-step manner with the ability to determine the
number of clusters automatically. The spatial graphs are utilized in community
detection to link the semantic neighbors. We further propose a tensor version
of Collapsed Gibbs Sampling method with a minimum cluster size requirement. A
case study based on Hong Kong metro passenger data is conducted to demonstrate
the automatic process of cluster amount evolution and better cluster quality
measured by within-cluster compactness and cross-cluster separateness. The code
is available at https://github.com/bonaldli/TensorDPMM-G.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20238" title="Abstract">arXiv:2310.20238</a> (cross-list from eess.AS) [<a href="/pdf/2310.20238" title="Download PDF">pdf</a>, <a href="/format/2310.20238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Study of speaker localization with binaural microphone array  incorporating auditory filters and lateral angle estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Maymon%2C+Y">Yanir Maymon</a>, 
<a href="/search/eess?searchtype=author&query=Nelken%2C+I">Israel Nelken</a>, 
<a href="/search/eess?searchtype=author&query=Rafaely%2C+B">Boaz Rafaely</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Acoustics, Volume 213,2023, 109632
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Speaker localization for binaural microphone arrays has been widely studied
for applications such as speech communication, video conferencing, and robot
audition. Many methods developed for this task, including the direct path
dominance (DPD) test, share common stages in their processing, which include
transformation using the short-time Fourier transform (STFT), and a direction
of arrival (DOA) search that is based on the head related transfer function
(HRTF) set. In this paper, alternatives to these processing stages, motivated
by human hearing, are proposed. These include incorporating an auditory filter
bank to replace the STFT, and a new DOA search based on transformed HRTF as
steering vectors. A simulation study and an experimental study are conducted to
validate the proposed alternatives, and both are applied to two binaural DOA
estimation methods; the results show that the proposed method compares
favorably with current methods.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20245" title="Abstract">arXiv:2310.20245</a> (cross-list from math.CO) [<a href="/pdf/2310.20245" title="Download PDF">pdf</a>, <a href="/format/2310.20245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding a Maximum Restricted $t$-Matching via Boolean Edge-CSP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Iwamasa%2C+Y">Yuni Iwamasa</a>, 
<a href="/search/math?searchtype=author&query=Kobayashi%2C+Y">Yusuke Kobayashi</a>, 
<a href="/search/math?searchtype=author&query=Takazawa%2C+K">Kenjiro Takazawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The problem of finding a maximum $2$-matching without short cycles has
received significant attention due to its relevance to the Hamilton cycle
problem. This problem is generalized to finding a maximum $t$-matching which
excludes specified complete $t$-partite subgraphs, where $t$ is a fixed
positive integer. The polynomial solvability of this generalized problem
remains an open question. In this paper, we present polynomial-time algorithms
for the following two cases of this problem: in the first case the forbidden
complete $t$-partite subgraphs are edge-disjoint; and in the second case the
maximum degree of the input graph is at most $2t-1$. Our result for the first
case extends the previous work of Nam (1994) showing the polynomial solvability
of the problem of finding a maximum $2$-matching without cycles of length four,
where the cycles of length four are vertex-disjoint. The second result expands
upon the works of B\'{e}rczi and V\'{e}gh (2010) and Kobayashi and Yin (2012),
which focused on graphs with maximum degree at most $t+1$. Our algorithms are
obtained from exploiting the discrete structure of restricted $t$-matchings and
employing an algorithm for the Boolean edge-CSP.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20265" title="Abstract">arXiv:2310.20265</a> (cross-list from eess.IV) [<a href="/pdf/2310.20265" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Dose CT Image Enhancement Using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Demir%2C+A">A.Demir</a>, 
<a href="/search/eess?searchtype=author&query=Shames%2C+M+M+A">M.M.A.Shames</a>, 
<a href="/search/eess?searchtype=author&query=Gerek%2C+O+N">O.N.Gerek</a>, 
<a href="/search/eess?searchtype=author&query=Ergin%2C+S">S.Ergin</a>, 
<a href="/search/eess?searchtype=author&query=Fidan%2C+M">M.Fidan</a>, 
<a href="/search/eess?searchtype=author&query=Koc%2C+M">M.Koc</a>, 
<a href="/search/eess?searchtype=author&query=Gulmezoglu%2C+M+B">M.B.Gulmezoglu</a>, 
<a href="/search/eess?searchtype=author&query=Barkana%2C+A">A.Barkana</a>, 
<a href="/search/eess?searchtype=author&query=Calisir%2C+C">C.Calisir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The application of ionizing radiation for diagnostic imaging is common around
the globe. However, the process of imaging, itself, remains to be a relatively
hazardous operation. Therefore, it is preferable to use as low a dose of
ionizing radiation as possible, particularly in computed tomography (CT)
imaging systems, where multiple x-ray operations are performed for the
reconstruction of slices of body tissues. A popular method for radiation dose
reduction in CT imaging is known as the quarter-dose technique, which reduces
the x-ray dose but can cause a loss of image sharpness. Since CT image
reconstruction from directional x-rays is a nonlinear process, it is
analytically difficult to correct the effect of dose reduction on image
quality. Recent and popular deep-learning approaches provide an intriguing
possibility of image enhancement for low-dose artifacts. Some recent works
propose combinations of multiple deep-learning and classical methods for this
purpose, which over-complicate the process. However, it is observed here that
the straight utilization of the well-known U-NET provides very successful
results for the correction of low-dose artifacts. Blind tests with actual
radiologists reveal that the U-NET enhanced quarter-dose CT images not only
provide an immense visual improvement over the low-dose versions, but also
become diagnostically preferable images, even when compared to their full-dose
CT versions.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20292" title="Abstract">arXiv:2310.20292</a> (cross-list from eess.IV) [<a href="/pdf/2310.20292" title="Download PDF">pdf</a>, <a href="/format/2310.20292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IARS SegNet: Interpretable Attention Residual Skip connection SegNet for  melanoma segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=V%2C+S+N">Shankara Narayanan V</a>, 
<a href="/search/eess?searchtype=author&query=OK%2C+S">Sikha OK</a>, 
<a href="/search/eess?searchtype=author&query=Benitez%2C+R">Raul Benitez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the journal: Computers in Biology and Medicine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Skin lesion segmentation plays a crucial role in the computer-aided diagnosis
of melanoma. Deep Learning models have shown promise in accurately segmenting
skin lesions, but their widespread adoption in real-life clinical settings is
hindered by their inherent black-box nature. In domains as critical as
healthcare, interpretability is not merely a feature but a fundamental
requirement for model adoption. This paper proposes IARS SegNet an advanced
segmentation framework built upon the SegNet baseline model. Our approach
incorporates three critical components: Skip connections, residual
convolutions, and a global attention mechanism onto the baseline Segnet
architecture. These elements play a pivotal role in accentuating the
significance of clinically relevant regions, particularly the contours of skin
lesions. The inclusion of skip connections enhances the model's capacity to
learn intricate contour details, while the use of residual convolutions allows
for the construction of a deeper model while preserving essential image
features. The global attention mechanism further contributes by extracting
refined feature maps from each convolutional and deconvolutional block, thereby
elevating the model's interpretability. This enhancement highlights critical
regions, fosters better understanding, and leads to more accurate skin lesion
segmentation for melanoma diagnosis.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20342" title="Abstract">arXiv:2310.20342</a> (cross-list from math.NT) [<a href="/pdf/2310.20342" title="Download PDF">pdf</a>, <a href="/ps/2310.20342" title="Download PostScript">ps</a>, <a href="/format/2310.20342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bohr sets generated by polynomials and Coppersmith&#x27;s method in many  variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baird%2C+R">Riley Baird</a>, 
<a href="/search/math?searchtype=author&query=Kerr%2C+B">Bryce Kerr</a>, 
<a href="/search/math?searchtype=author&query=Shparlinski%2C+I">Igor Shparlinski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Computational Complexity (cs.CC); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We obtain bounds on the average size of Bohr sets with coefficients
parametrised by polynomials over finite fields and obtain a series of general
results and also some sharper results for specific sets which are important for
applications to computer science. In particular, we use our estimates to show
that a heuristic assumption used in the many variable version of Coppersmith's
method holds with high probability. We demonstrate the use of our results on
the approximate greatest common divisor problem and obtain a fully rigorous
version of the heuristic algorithm of H. Cohn and N. Heninger (2013).
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20365" title="Abstract">arXiv:2310.20365</a> (cross-list from physics.optics) [<a href="/pdf/2310.20365" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical-layer key distribution using synchronous complex dynamics of  DBR semiconductor lasers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+A">Anbang Wang</a>, 
<a href="/search/physics?searchtype=author&query=Du%2C+Y">Yicheng Du</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+Q">Qingtian Li</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+L">Longsheng Wang</a>, 
<a href="/search/physics?searchtype=author&query=Jia%2C+Z">Zhiwei Jia</a>, 
<a href="/search/physics?searchtype=author&query=Qin%2C+Y">Yuwen Qin</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+Y">Yuncai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Common-signal-induced synchronization of semiconductor lasers with optical
feedback inspired a promising physical key distribution with
information-theoretic security and potential in high rate. A significant
challenge is the requirement to shorten the synchronization recovery time for
increasing key rate without sacrificing operation parameter space for security.
Here, open-loop synchronization of wavelength-tunable multi-section distributed
Bragg reflector (DBR) lasers is proposed as a solution for physical-layer key
distribution. Experiments show that the synchronization is sensitive to two
operation parameters, i.e., currents of grating section and phase section.
Furthermore, fast wavelength-shift keying synchronization can be achieved by
direct modulation on one of the two currents. The synchronization recovery time
is shortened by one order of magnitude compared to close-loop synchronization.
An experimental implementation is demonstrated with a final key rate of 5.98
Mbit/s over 160 km optical fiber distance. It is thus believed that
fast-tunable multi-section semiconductor lasers opens a new avenue of high-rate
physical-layer key distribution using laser synchronization.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20389" title="Abstract">arXiv:2310.20389</a> (cross-list from eess.IV) [<a href="/pdf/2310.20389" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Resolution Reference Image Assisted Volumetric Super-Resolution of  Cardiac Diffusion Weighted Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yinzhe Wu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+J">Jiahao Huang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+F">Fanwen Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ferreira%2C+P">Pedro Ferreira</a>, 
<a href="/search/eess?searchtype=author&query=Scott%2C+A">Andrew Scott</a>, 
<a href="/search/eess?searchtype=author&query=Nielles-Vallespin%2C+S">Sonia Nielles-Vallespin</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by SPIE Medical Imaging 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) is the only in vivo
method to non-invasively examine the microstructure of the human heart. Current
research in DT-CMR aims to improve the understanding of how the cardiac
microstructure relates to the macroscopic function of the healthy heart as well
as how microstructural dysfunction contributes to disease. To get the final
DT-CMR metrics, we need to acquire diffusion weighted images of at least 6
directions. However, due to DWI's low signal-to-noise ratio, the standard voxel
size is quite big on the scale for microstructures. In this study, we explored
the potential of deep-learning-based methods in improving the image quality
volumetrically (x4 in all dimensions). This study proposed a novel framework to
enable volumetric super-resolution, with an additional model input of
high-resolution b0 DWI. We demonstrated that the additional input could offer
higher super-resolved image quality. Going beyond, the model is also able to
super-resolve DWIs of unseen b-values, proving the model framework's
generalizability for cardiac DWI superresolution. In conclusion, we would then
recommend giving the model a high-resolution reference image as an additional
input to the low-resolution image for training and inference to guide all
super-resolution frameworks for parametric imaging where a reference image is
available.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20398" title="Abstract">arXiv:2310.20398</a> (cross-list from astro-ph.EP) [<a href="/pdf/2310.20398" title="Download PDF">pdf</a>, <a href="/format/2310.20398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A hybrid approach for solving the gravitational N-body problem with  Artificial Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Ulibarrena%2C+V+S">Veronica Saz Ulibarrena</a>, 
<a href="/search/astro-ph?searchtype=author&query=Horn%2C+P">Philipp Horn</a>, 
<a href="/search/astro-ph?searchtype=author&query=Zwart%2C+S+P">Simon Portegies Zwart</a>, 
<a href="/search/astro-ph?searchtype=author&query=Sellentin%2C+E">Elena Sellentin</a>, 
<a href="/search/astro-ph?searchtype=author&query=Koren%2C+B">Barry Koren</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cai%2C+M+X">Maxwell X. Cai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the Journal of Computational Physics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Simulating the evolution of the gravitational N-body problem becomes
extremely computationally expensive as N increases since the problem complexity
scales quadratically with the number of bodies. We study the use of Artificial
Neural Networks (ANNs) to replace expensive parts of the integration of
planetary systems. Neural networks that include physical knowledge have grown
in popularity in the last few years, although few attempts have been made to
use them to speed up the simulation of the motion of celestial bodies. We study
the advantages and limitations of using Hamiltonian Neural Networks to replace
computationally expensive parts of the numerical simulation. We compare the
results of the numerical integration of a planetary system with asteroids with
those obtained by a Hamiltonian Neural Network and a conventional Deep Neural
Network, with special attention to understanding the challenges of this
problem. Due to the non-linear nature of the gravitational equations of motion,
errors in the integration propagate. To increase the robustness of a method
that uses neural networks, we propose a hybrid integrator that evaluates the
prediction of the network and replaces it with the numerical solution if
considered inaccurate. Hamiltonian Neural Networks can make predictions that
resemble the behavior of symplectic integrators but are challenging to train
and in our case fail when the inputs differ ~7 orders of magnitude. In
contrast, Deep Neural Networks are easy to train but fail to conserve energy,
leading to fast divergence from the reference solution. The hybrid integrator
designed to include the neural networks increases the reliability of the method
and prevents large energy errors without increasing the computing cost
significantly. For this problem, the use of neural networks results in faster
simulations when the number of asteroids is &gt;70.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20403" title="Abstract">arXiv:2310.20403</a> (cross-list from eess.SP) [<a href="/pdf/2310.20403" title="Download PDF">pdf</a>, <a href="/format/2310.20403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Base Station Cooperative Sensing with AI-Aided Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Favarelli%2C+E">Elia Favarelli</a>, 
<a href="/search/eess?searchtype=author&query=Matricardi%2C+E">Elisabetta Matricardi</a>, 
<a href="/search/eess?searchtype=author&query=Pucci%2C+L">Lorenzo Pucci</a>, 
<a href="/search/eess?searchtype=author&query=Paolini%2C+E">Enrico Paolini</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+W">Wen Xu</a>, 
<a href="/search/eess?searchtype=author&query=Giorgetti%2C+A">Andrea Giorgetti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this work, we investigate the performance of a joint sensing and
communication (JSC) network consisting of multiple base stations (BSs) that
cooperate through a fusion center (FC) to exchange information about the sensed
environment while concurrently establishing communication links with a set of
user equipments (UEs). Each BS within the network operates as a monostatic
radar system, enabling comprehensive scanning of the monitored area and
generating range-angle maps that provide information regarding the position of
a group of heterogeneous objects. The acquired maps are subsequently fused in
the FC. Then, a convolutional neural network (CNN) is employed to infer the
category of the targets, e.g., pedestrians or vehicles, and such information is
exploited by an adaptive clustering algorithm to group the detections
originating from the same target more effectively. Finally, two multi-target
tracking algorithms, the probability hypothesis density (PHD) filter and
multi-Bernoulli mixture (MBM) filter, are applied to estimate the state of the
targets. Numerical results demonstrated that our framework could provide
remarkable sensing performance, achieving an optimal sub-pattern assignment
(OSPA) less than 60 cm, while keeping communication services to UEs with a
reduction of the communication capacity in the order of 10% to 20%. The impact
of the number of BSs engaged in sensing is also examined, and we show that in
the specific case study, 3 BSs ensure a localization error below 1 m.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20415" title="Abstract">arXiv:2310.20415</a> (cross-list from econ.TH) [<a href="/pdf/2310.20415" title="Download PDF">pdf</a>, <a href="/format/2310.20415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coalitional Manipulations and Immunity of the Shapley Value
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Basteck%2C+C">Christian Basteck</a>, 
<a href="/search/econ?searchtype=author&query=Huettner%2C+F">Frank Huettner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider manipulations in the context of coalitional games, where a
coalition aims to increase the total payoff of its members. An allocation rule
is immune to coalitional manipulation if no coalition can benefit from internal
reallocation of worth on the level of its subcoalitions
(reallocation-proofness), and if no coalition benefits from a lower worth while
all else remains the same (weak coalitional monotonicity). Replacing additivity
in Shapley's original characterization by these requirements yields a new
foundation of the Shapley value, i.e., it is the unique efficient and symmetric
allocation rule that awards nothing to a null player and is immune to
coalitional manipulations. We further find that for efficient allocation rules,
reallocation-proofness is equivalent to constrained marginality, a weaker
variant of Young's marginality axiom. Our second characterization improves upon
Young's characterization by weakening the independence requirement intrinsic to
marginality.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20421" title="Abstract">arXiv:2310.20421</a> (cross-list from quant-ph) [<a href="/pdf/2310.20421" title="Download PDF">pdf</a>, <a href="/format/2310.20421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage solution for ancilla-assisted quantum process tomography:  error analysis and optimal design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Xiao%2C+S">Shuixin Xiao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+Y">Yuanlong Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Dong%2C+D">Daoyi Dong</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Quantum process tomography (QPT) is a fundamental task to characterize the
dynamics of quantum systems. In contrast to standard QPT, ancilla-assisted
process tomography (AAPT) framework introduces an extra ancilla system such
that a single input state is needed. In this paper, we extend the two-stage
solution, a method originally designed for standard QPT, to perform AAPT. Our
algorithm has $O(Md_A^2d_B^2)$ computational complexity where $ M $ is the type
number of the measurement operators, $ d_A $ is the dimension of the quantum
system of interest, and $d_B$ is the dimension of the ancilla system. Then we
establish an error upper bound and further discuss the optimal design on the
input state in AAPT. A numerical example on a phase damping process
demonstrates the effectiveness of the optimal design and illustrates the
theoretical error analysis.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20427" title="Abstract">arXiv:2310.20427</a> (cross-list from eess.IV) [<a href="/pdf/2310.20427" title="Download PDF">pdf</a>, <a href="/format/2310.20427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing and Enhancing Robustness of Deep Learning Models with  Corruption Emulation in Digital Pathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+P">Peixiang Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+S">Songtao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Gan%2C+Y">Yulu Gan</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+R">Rui Xu</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+R">Rongqi Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+W">Wenkang Qin</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+L">Limei Guo</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+S">Shan Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+L">Lin Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep learning in digital pathology brings intelligence and automation as
substantial enhancements to pathological analysis, the gold standard of
clinical diagnosis. However, multiple steps from tissue preparation to slide
imaging introduce various image corruptions, making it difficult for deep
neural network (DNN) models to achieve stable diagnostic results for clinical
use. In order to assess and further enhance the robustness of the models, we
analyze the physical causes of the full-stack corruptions throughout the
pathological life-cycle and propose an Omni-Corruption Emulation (OmniCE)
method to reproduce 21 types of corruptions quantified with 5-level severity.
We then construct three OmniCE-corrupted benchmark datasets at both patch level
and slide level and assess the robustness of popular DNNs in classification and
segmentation tasks. Further, we explore to use the OmniCE-corrupted datasets as
augmentation data for training and experiments to verify that the
generalization ability of the models has been significantly enhanced.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20438" title="Abstract">arXiv:2310.20438</a> (cross-list from stat.ML) [<a href="/pdf/2310.20438" title="Download PDF">pdf</a>, <a href="/ps/2310.20438" title="Download PostScript">ps</a>, <a href="/format/2310.20438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Phase Transition Phenomenon of Shuffled Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+H">Hang Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+P">Ping Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the phase transition phenomenon inherent in the shuffled (permuted)
regression problem, which has found numerous applications in databases,
privacy, data analysis, etc. In this study, we aim to precisely identify the
locations of the phase transition points by leveraging techniques from message
passing (MP). In our analysis, we first transform the permutation recovery
problem into a probabilistic graphical model. We then leverage the analytical
tools rooted in the message passing (MP) algorithm and derive an equation to
track the convergence of the MP algorithm. By linking this equation to the
branching random walk process, we are able to characterize the impact of the
signal-to-noise-ratio ($\snr$) on the permutation recovery. Depending on
whether the signal is given or not, we separately investigate the oracle case
and the non-oracle case. The bottleneck in identifying the phase transition
regimes lies in deriving closed-form formulas for the corresponding critical
points, but only in rare scenarios can one obtain such precise expressions. To
tackle this technical challenge, this study proposes the Gaussian approximation
method, which allows us to obtain the closed-form formulas in almost all
scenarios. In the oracle case, our method can fairly accurately predict the
phase transition $\snr$. In the non-oracle case, our algorithm can predict the
maximum allowed number of permuted rows and uncover its dependency on the
sample number.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20458" title="Abstract">arXiv:2310.20458</a> (cross-list from math.AG) [<a href="/pdf/2310.20458" title="Download PDF">pdf</a>, <a href="/format/2310.20458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine learning detects terminal singularities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Coates%2C+T">Tom Coates</a>, 
<a href="/search/math?searchtype=author&query=Kasprzyk%2C+A+M">Alexander M. Kasprzyk</a>, 
<a href="/search/math?searchtype=author&query=Veneziale%2C+S">Sara Veneziale</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Algebraic varieties are the geometric shapes defined by systems of polynomial
equations; they are ubiquitous across mathematics and science. Amongst these
algebraic varieties are Q-Fano varieties: positively curved shapes which have
Q-factorial terminal singularities. Q-Fano varieties are of fundamental
importance in geometry as they are "atomic pieces" of more complex shapes - the
process of breaking a shape into simpler pieces in this sense is called the
Minimal Model Programme. Despite their importance, the classification of Q-Fano
varieties remains unknown. In this paper we demonstrate that machine learning
can be used to understand this classification. We focus on 8-dimensional
positively-curved algebraic varieties that have toric symmetry and Picard rank
2, and develop a neural network classifier that predicts with 95% accuracy
whether or not such an algebraic variety is Q-Fano. We use this to give a first
sketch of the landscape of Q-Fanos in dimension 8. How the neural network is
able to detect Q-Fano varieties with such accuracy remains mysterious, and
hints at some deep mathematical theory waiting to be uncovered. Furthermore,
when visualised using the quantum period, an invariant that has played an
important role in recent theoretical developments, we observe that the
classification as revealed by ML appears to fall within a bounded region, and
is stratified by the Fano index. This suggests that it may be possible to state
and prove conjectures on completeness in the future. Inspired by the ML
analysis, we formulate and prove a new global combinatorial criterion for a
positively curved toric variety of Picard rank 2 to have terminal
singularities. Together with the first sketch of the landscape of Q-Fanos in
higher dimensions, this gives new evidence that machine learning can be an
essential tool in developing mathematical conjectures and accelerating
theoretical discovery.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20508" title="Abstract">arXiv:2310.20508</a> (cross-list from stat.ML) [<a href="/pdf/2310.20508" title="Download PDF">pdf</a>, <a href="/format/2310.20508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parametric Fairness with Statistical Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=HU%2C+F">Fran&#xe7;ois HU</a>, 
<a href="/search/stat?searchtype=author&query=Ratz%2C+P">Philipp Ratz</a>, 
<a href="/search/stat?searchtype=author&query=Charpentier%2C+A">Arthur Charpentier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Algorithmic fairness has gained prominence due to societal and regulatory
concerns about biases in Machine Learning models. Common group fairness metrics
like Equalized Odds for classification or Demographic Parity for both
classification and regression are widely used and a host of computationally
advantageous post-processing methods have been developed around them. However,
these metrics often limit users from incorporating domain knowledge. Despite
meeting traditional fairness criteria, they can obscure issues related to
intersectional fairness and even replicate unwanted intra-group biases in the
resulting fair solution. To avoid this narrow perspective, we extend the
concept of Demographic Parity to incorporate distributional properties in the
predictions, allowing expert knowledge to be used in the fair solution. We
illustrate the use of this new metric through a practical example of wages, and
develop a parametric method that efficiently addresses practical challenges
like limited training data and constraints on total spending, offering a robust
solution for real-life applications.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20522" title="Abstract">arXiv:2310.20522</a> (cross-list from math.CO) [<a href="/pdf/2310.20522" title="Download PDF">pdf</a>, <a href="/ps/2310.20522" title="Download PostScript">ps</a>, <a href="/format/2310.20522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight bounds on adjacency labels for monotone graph classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bonnet%2C+%C3%89">&#xc9;douard Bonnet</a>, 
<a href="/search/math?searchtype=author&query=Duron%2C+J">Julien Duron</a>, 
<a href="/search/math?searchtype=author&query=Sylvester%2C+J">John Sylvester</a>, 
<a href="/search/math?searchtype=author&query=Zamaraev%2C+V">Viktor Zamaraev</a>, 
<a href="/search/math?searchtype=author&query=Zhukovskii%2C+M">Maksim Zhukovskii</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">A class of graphs admits an adjacency labeling scheme of size $f(n)$, if the
vertices of any $n$-vertex graph $G$ in the class can be assigned binary
strings (aka labels) of length $f(n)$ so that the adjacency between each pair
of vertices in $G$ can be determined only from their labels. The Implicit Graph
Conjecture (IGC) claimed that any graph class which is hereditary (i.e. closed
under taking induced subgraphs) and factorial (i.e. containing $2^{\Theta(n
\log n)}$ graphs on $n$ vertices) admits an adjacency labeling scheme of order
optimal size $O(\log n)$. After thirty years open, the IGC was recently
disproved [Hatami and Hatami, FOCS 2022].
<br />In this work we show that the IGC does not hold even for monotone graph
classes, i.e. classes closed under taking subgraphs. More specifically, we show
that there are monotone factorial graph classes for which the size of any
adjacency labeling scheme is $\Omega(\log^2 n)$. Moreover, this is best
possible, as any monotone factorial class admits an adjacency labeling scheme
of size $O(\log^2 n)$.
<br />This is a consequence of our general result that establishes tight bounds on
the size of adjacency labeling schemes for monotone graph classes: for any
function $f: \mathbb{R}_{\geq 0} \rightarrow \mathbb{R}_{\geq 0}$ with $\log x
\leq f(x) \leq x^{1-\delta}$ for some constant $\delta &gt; 0$, that satisfies
some natural conditions, there exist monotone graph classes, in which the
number of $n$-vertex graphs grows as $2^{O(nf(n))}$ and that do not admit
adjacency labels of size at most $f(n) \log n$. On the other hand any such
class admits adjacency labels of size $O(f(n)\log n)$, which is a factor of
$\log n$ away from the order optimal bound $O(f(n))$. This is the first example
of tight bounds on adjacency labels for graph classes that do not admit order
optimal adjacency labeling schemes.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20535" title="Abstract">arXiv:2310.20535</a> (cross-list from astro-ph.IM) [<a href="/pdf/2310.20535" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The ESO Science Archive Facility: Status, Impact, and Prospects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Romaniello%2C+M">Martino Romaniello</a>, 
<a href="/search/astro-ph?searchtype=author&query=Arnaboldi%2C+M">Magda Arnaboldi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Barbieri%2C+M">Mauro Barbieri</a>, 
<a href="/search/astro-ph?searchtype=author&query=Delmotte%2C+N">Nausicaa Delmotte</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dobrzycki%2C+A">Adam Dobrzycki</a>, 
<a href="/search/astro-ph?searchtype=author&query=Fourniol%2C+N">Nathalie Fourniol</a>, 
<a href="/search/astro-ph?searchtype=author&query=Freudling%2C+W">Wolfram Freudling</a>, 
<a href="/search/astro-ph?searchtype=author&query=Grave%2C+J">Jorge Grave</a>, 
<a href="/search/astro-ph?searchtype=author&query=Mascetti%2C+L">Laura Mascetti</a>, 
<a href="/search/astro-ph?searchtype=author&query=Micol%2C+A">Alberto Micol</a>, 
<a href="/search/astro-ph?searchtype=author&query=Retzlaff%2C+J">J&#xf6;rg Retzlaff</a>, 
<a href="/search/astro-ph?searchtype=author&query=Rosse%2C+N">Nicolas Rosse</a>, 
<a href="/search/astro-ph?searchtype=author&query=Tax%2C+T">Tomas Tax</a>, 
<a href="/search/astro-ph?searchtype=author&query=Vuong%2C+M">Myha Vuong</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hainaut%2C+O">Olivier Hainaut</a>, 
<a href="/search/astro-ph?searchtype=author&query=Rejkuba%2C+M">Marina Rejkuba</a>, 
<a href="/search/astro-ph?searchtype=author&query=Sterzik%2C+M">Michael Sterzik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures. arXiv admin note: text overlap with <a href="/abs/2209.11605">arXiv:2209.11605</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Messenger 191, 29 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Digital Libraries (cs.DL)

</div>
<p class="mathjax">Scientific data collected at ESO's observatories are freely and openly
accessible online through the ESO Science Archive Facility. In addition to the
raw data straight out of the instruments, the ESO Science Archive also contains
four million processed science files available for use by scientists and
astronomy enthusiasts worldwide. ESO subscribes to the FAIR (Findable,
Accessible, Interoperable, Reusable) guiding principles for scientific data
management and stewardship. All data in the ESO Science Archive are distributed
according to the terms of the Creative Commons Attribution 4.0 International
licence (CC BY 4.0).
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20544" title="Abstract">arXiv:2310.20544</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2310.20544" title="Download PDF">pdf</a>, <a href="/format/2310.20544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-theoretic causality and applications to turbulence: energy  cascade and inner/outer layer interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lozano-Dur%C3%A1n%2C+A">Adri&#xe1;n Lozano-Dur&#xe1;n</a>, 
<a href="/search/physics?searchtype=author&query=Arranz%2C+G">Gonzalo Arranz</a>, 
<a href="/search/physics?searchtype=author&query=Ling%2C+Y">Yuenong Ling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Information Theory (cs.IT); Chaotic Dynamics (nlin.CD); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We introduce an information-theoretic method for quantifying causality in
chaotic systems. The approach, referred to as IT-causality, quantifies
causality by measuring the information gained about future events conditioned
on the knowledge of past events. The causal interactions are classified into
redundant, unique, and synergistic contributions depending on their nature. The
formulation is non-intrusive, invariance under invertible transformations of
the variables, and provides the missing causality due to unobserved variables.
The method only requires pairs of past-future events of the quantities of
interest, making it convenient for both computational simulations and
experimental investigations. IT-causality is validated in four scenarios
representing basic causal interactions among variables: mediator, confounder,
redundant collider, and synergistic collider. The approach is leveraged to
address two questions relevant to turbulence research: i) the scale locality of
the energy cascade in isotropic turbulence, and ii) the interactions between
inner and outer layer flow motions in wall-bounded turbulence. In the former
case, we demonstrate that causality in the energy cascade flows sequentially
from larger to smaller scales without requiring intermediate scales.
Conversely, the flow of information from small to large scales is shown to be
redundant. In the second problem, we observe a unidirectional causality flow,
with causality predominantly originating from the outer layer and propagating
towards the inner layer, but not vice versa. The decomposition of IT-causality
into intensities also reveals that the causality is primarily associated with
high-velocity streaks.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20579" title="Abstract">arXiv:2310.20579</a> (cross-list from stat.ML) [<a href="/pdf/2310.20579" title="Download PDF">pdf</a>, <a href="/format/2310.20579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Initialization Matters: Privacy-Utility Analysis of Overparameterized  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ye%2C+J">Jiayuan Ye</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+Z">Zhenyu Zhu</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+F">Fanghui Liu</a>, 
<a href="/search/stat?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>, 
<a href="/search/stat?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We analytically investigate how over-parameterization of models in randomized
machine learning algorithms impacts the information leakage about their
training data. Specifically, we prove a privacy bound for the KL divergence
between model distributions on worst-case neighboring datasets, and explore its
dependence on the initialization, width, and depth of fully connected neural
networks. We find that this KL privacy bound is largely determined by the
expected squared gradient norm relative to model parameters during training.
Notably, for the special setting of linearized network, our analysis indicates
that the squared gradient norm (and therefore the escalation of privacy loss)
is tied directly to the per-layer variance of the initialization distribution.
By using this analysis, we demonstrate that privacy bound improves with
increasing depth under certain initializations (LeCun and Xavier), while
degrades with increasing depth under other initializations (He and NTK). Our
work reveals a complex interplay between privacy and depth that depends on the
chosen initialization distribution. We further prove excess empirical risk
bounds under a fixed KL privacy budget, and show that the interplay between
privacy utility trade-off and depth is similarly affected by the
initialization.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20582" title="Abstract">arXiv:2310.20582</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.20582" title="Download PDF">pdf</a>, <a href="/format/2310.20582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The serotonergic psychedelic N,N-dipropyltryptamine alters  information-processing dynamics in cortical neural circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Varley%2C+T+F">Thomas F. Varley</a>, 
<a href="/search/q-bio?searchtype=author&query=Havert%2C+D">Daniel Havert</a>, 
<a href="/search/q-bio?searchtype=author&query=Fosque%2C+L">Leandro Fosque</a>, 
<a href="/search/q-bio?searchtype=author&query=Alipour%2C+A">Abolfazl Alipour</a>, 
<a href="/search/q-bio?searchtype=author&query=Weerawongphrom%2C+N">Naruepon Weerawongphrom</a>, 
<a href="/search/q-bio?searchtype=author&query=Naganobori%2C+H">Hiroki Naganobori</a>, 
<a href="/search/q-bio?searchtype=author&query=O%27Shea%2C+L">Lily O&#x27;Shea</a>, 
<a href="/search/q-bio?searchtype=author&query=Pope%2C+M">Maria Pope</a>, 
<a href="/search/q-bio?searchtype=author&query=Beggs%2C+J">John Beggs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Information Theory (cs.IT); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
<p class="mathjax">Most of the recent work in psychedelic neuroscience has been done using
non-invasive neuroimaging, with data recorded from the brains of adult
volunteers under the influence of a variety of drugs. While this data provides
holistic insights into the effects of psychedelics on whole-brain dynamics, the
effects of psychedelics on the meso-scale dynamics of cortical circuits remains
much less explored. Here, we report the effects of the serotonergic psychedelic
N,N-diproptyltryptamine (DPT) on information-processing dynamics in a sample of
in vitro organotypic cultures made from rat cortical tissue. Three hours of
spontaneous activity were recorded: an hour of pre-drug control, and hour of
exposure to 10$\mu$M DPT solution, and a final hour of washout, once again
under control conditions. We found that DPT reversibly alters information
dynamics in multiple ways: first, the DPT condition was associated with higher
entropy of spontaneous firing activity and reduced the amount of time
information was stored in individual neurons. Second, DPT also reduced the
reversibility of neural activity, increasing the entropy produced and
suggesting a drive away from equilibrium. Third, DPT altered the structure of
neuronal circuits, decreasing the overall information flow coming into each
neuron, but increasing the number of weak connections, creating a dynamic that
combines elements of integration and disintegration. Finally, DPT decreased the
higher-order statistical synergy present in sets of three neurons.
Collectively, these results paint a complex picture of how psychedelics
regulate information processing in meso-scale cortical tissue. Implications for
existing hypotheses of psychedelic action, such as the Entropic Brain
Hypothesis, are discussed.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20599" title="Abstract">arXiv:2310.20599</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.20599" title="Download PDF">pdf</a>, <a href="/format/2310.20599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brain-like Flexible Visual Inference by Harnessing Feedback-Feedforward  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Toosi%2C+T">Tahereh Toosi</a>, 
<a href="/search/q-bio?searchtype=author&query=Issa%2C+E+B">Elias B. Issa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In natural vision, feedback connections support versatile visual inference
capabilities such as making sense of the occluded or noisy bottom-up sensory
information or mediating pure top-down processes such as imagination. However,
the mechanisms by which the feedback pathway learns to give rise to these
capabilities flexibly are not clear. We propose that top-down effects emerge
through alignment between feedforward and feedback pathways, each optimizing
its own objectives. To achieve this co-optimization, we introduce
Feedback-Feedforward Alignment (FFA), a learning algorithm that leverages
feedback and feedforward pathways as mutual credit assignment computational
graphs, enabling alignment. In our study, we demonstrate the effectiveness of
FFA in co-optimizing classification and reconstruction tasks on widely used
MNIST and CIFAR10 datasets. Notably, the alignment mechanism in FFA endows
feedback connections with emergent visual inference functions, including
denoising, resolving occlusions, hallucination, and imagination. Moreover, FFA
offers bio-plausibility compared to traditional backpropagation (BP) methods in
implementation. By repurposing the computational graph of credit assignment
into a goal-driven feedback pathway, FFA alleviates weight transport problems
encountered in BP, enhancing the bio-plausibility of the learning algorithm.
Our study presents FFA as a promising proof-of-concept for the mechanisms
underlying how feedback connections in the visual cortex support flexible
visual functions. This work also contributes to the broader field of visual
inference underlying perceptual phenomena and has implications for developing
more biologically inspired learning algorithms.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20601" title="Abstract">arXiv:2310.20601</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.20601" title="Download PDF">pdf</a>, <a href="/format/2310.20601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional connectivity modules in recurrent neural networks: function,  origin and dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Tanner%2C+J">Jacob Tanner</a>, 
<a href="/search/q-bio?searchtype=author&query=L.%2C+S+M">Sina Mansour L.</a>, 
<a href="/search/q-bio?searchtype=author&query=Coletta%2C+L">Ludovico Coletta</a>, 
<a href="/search/q-bio?searchtype=author&query=Gozzi%2C+A">Alessandro Gozzi</a>, 
<a href="/search/q-bio?searchtype=author&query=Betzel%2C+R+F">Richard F. Betzel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding the ubiquitous phenomenon of neural synchronization across
species and organizational levels is crucial for decoding brain function.
Despite its prevalence, the specific functional role, origin, and dynamical
implication of modular structures in correlation-based networks remains
ambiguous. Using recurrent neural networks trained on systems neuroscience
tasks, this study investigates these important characteristics of modularity in
correlation networks. We demonstrate that modules are functionally coherent
units that contribute to specialized information processing. We show that
modules form spontaneously from asymmetries in the sign and weight of
projections from the input layer to the recurrent layer. Moreover, we show that
modules define connections with similar roles in governing system behavior and
dynamics. Collectively, our findings clarify the function, formation, and
operational significance of functional connectivity modules, offering insights
into cortical function and laying the groundwork for further studies on brain
function, development, and dynamics.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20604" title="Abstract">arXiv:2310.20604</a> (cross-list from eess.IV) [<a href="/pdf/2310.20604" title="Download PDF">pdf</a>, <a href="/format/2310.20604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Synthetic MRI Generation from CT Scans Using CycleGAN with  Feature Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nikbakhsh%2C+S">Saba Nikbakhsh</a>, 
<a href="/search/eess?searchtype=author&query=Naghashyar%2C+L">Lachin Naghashyar</a>, 
<a href="/search/eess?searchtype=author&query=Valizadeh%2C+M">Morteza Valizadeh</a>, 
<a href="/search/eess?searchtype=author&query=Amirani%2C+M+C">Mehdi Chehel Amirani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In the field of radiotherapy, accurate imaging and image registration are of
utmost importance for precise treatment planning. Magnetic Resonance Imaging
(MRI) offers detailed imaging without being invasive and excels in soft-tissue
contrast, making it a preferred modality for radiotherapy planning. However,
the high cost of MRI, longer acquisition time, and certain health
considerations for patients pose challenges. Conversely, Computed Tomography
(CT) scans offer a quicker and less expensive imaging solution. To bridge these
modalities and address multimodal alignment challenges, we introduce an
approach for enhanced monomodal registration using synthetic MRI images.
Utilizing unpaired data, this paper proposes a novel method to produce these
synthetic MRI images from CT scans, leveraging CycleGANs and feature
extractors. By building upon the foundational work on Cycle-Consistent
Adversarial Networks and incorporating advancements from related literature,
our methodology shows promising results, outperforming several state-of-the-art
methods. The efficacy of our approach is validated by multiple comparison
metrics.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20609" title="Abstract">arXiv:2310.20609</a> (cross-list from stat.ML) [<a href="/pdf/2310.20609" title="Download PDF">pdf</a>, <a href="/format/2310.20609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Matching via convex relaxation to the simplex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Valdivia%2C+E+A">Ernesto Araya Valdivia</a>, 
<a href="/search/stat?searchtype=author&query=Tyagi%2C+H">Hemant Tyagi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">This paper addresses the Graph Matching problem, which consists of finding
the best possible alignment between two input graphs, and has many applications
in computer vision, network deanonymization and protein alignment. A common
approach to tackle this problem is through convex relaxations of the NP-hard
\emph{Quadratic Assignment Problem} (QAP).
<br />Here, we introduce a new convex relaxation onto the unit simplex and develop
an efficient mirror descent scheme with closed-form iterations for solving this
problem. Under the correlated Gaussian Wigner model, we show that the simplex
relaxation admits a unique solution with high probability. In the noiseless
case, this is shown to imply exact recovery of the ground truth permutation.
Additionally, we establish a novel sufficiency condition for the input matrix
in standard greedy rounding methods, which is less restrictive than the
commonly used `diagonal dominance' condition. We use this condition to show
exact one-step recovery of the ground truth (holding almost surely) via the
mirror descent scheme, in the noiseless setting. We also use this condition to
obtain significantly improved conditions for the GRAMPA algorithm [Fan et al.
2019] in the noiseless setting.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20630" title="Abstract">arXiv:2310.20630</a> (cross-list from stat.ML) [<a href="/pdf/2310.20630" title="Download PDF">pdf</a>, <a href="/format/2310.20630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Projecting basis functions with tensor networks for Gaussian process  regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Menzen%2C+C">Clara Menzen</a>, 
<a href="/search/stat?searchtype=author&query=Memmel%2C+E">Eva Memmel</a>, 
<a href="/search/stat?searchtype=author&query=Batselier%2C+K">Kim Batselier</a>, 
<a href="/search/stat?searchtype=author&query=Kok%2C+M">Manon Kok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a method for approximate Gaussian process (GP) regression
with tensor networks (TNs). A parametric approximation of a GP uses a linear
combination of basis functions, where the accuracy of the approximation depends
on the total number of basis functions $M$. We develop an approach that allows
us to use an exponential amount of basis functions without the corresponding
exponential computational complexity. The key idea to enable this is using
low-rank TNs. We first find a suitable low-dimensional subspace from the data,
described by a low-rank TN. In this low-dimensional subspace, we then infer the
weights of our model by solving a Bayesian inference problem. Finally, we
project the resulting weights back to the original space to make GP
predictions. The benefit of our approach comes from the projection to a smaller
subspace: It modifies the shape of the basis functions in a way that it sees
fit based on the given data, and it allows for efficient computations in the
smaller subspace. In an experiment with an 18-dimensional benchmark data set,
we show the applicability of our method to an inverse dynamics problem.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20671" title="Abstract">arXiv:2310.20671</a> (cross-list from quant-ph) [<a href="/pdf/2310.20671" title="Download PDF">pdf</a>, <a href="/format/2310.20671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Density Matrix Emulation of Quantum Recurrent Neural Networks for  Multivariate Time Series Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Viqueira%2C+J+D">Jos&#xe9; Daniel Viqueira</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fa%C3%ADlde%2C+D">Daniel Fa&#xed;lde</a>, 
<a href="/search/quant-ph?searchtype=author&query=Juane%2C+M+M">Mariamo M. Juane</a>, 
<a href="/search/quant-ph?searchtype=author&query=G%C3%B3mez%2C+A">Andr&#xe9;s G&#xf3;mez</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mera%2C+D">David Mera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantum Recurrent Neural Networks (QRNNs) are robust candidates to model and
predict future values in multivariate time series. However, the effective
implementation of some QRNN models is limited by the need of mid-circuit
measurements. Those increase the requirements for quantum hardware, which in
the current NISQ era does not allow reliable computations. Emulation arises as
the main near-term alternative to explore the potential of QRNNs, but existing
quantum emulators are not dedicated to circuits with multiple intermediate
measurements. In this context, we design a specific emulation method that
relies on density matrix formalism. The mathematical development is explicitly
provided as a compact formulation by using tensor notation. It allows us to
show how the present and past information from a time series is transmitted
through the circuit, and how to reduce the computational cost in every time
step of the emulated network. In addition, we derive the analytical gradient
and the Hessian of the network outputs with respect to its trainable
parameters, with an eye on gradient-based training and noisy outputs that would
appear when using real quantum processors. We finally test the presented
methods using a novel hardware-efficient ansatz and three diverse datasets that
include univariate and multivariate time series. Our results show how QRNNs can
make accurate predictions of future values by capturing non-trivial patterns of
input series with different complexities.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20699" title="Abstract">arXiv:2310.20699</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.20699" title="Download PDF">pdf</a>, <a href="/format/2310.20699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Multistate Bennett Acceptance Ratio Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ding%2C+X">Xinqiang Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)

</div>
<p class="mathjax">The multistate Bennett acceptance ratio (MBAR) method is a prevalent approach
for computing free energies of thermodynamic states. In this work, we introduce
BayesMBAR, a Bayesian generalization of the MBAR method. By integrating
configurations sampled from thermodynamic states with a prior distribution,
BayesMBAR computes a posterior distribution of free energies. Using the
posterior distribution, we derive free energy estimations and compute their
associated uncertainties. Notably, when a uniform prior distribution is used,
BayesMBAR recovers the MBAR's result but provides more accurate uncertainty
estimates. Additionally, when prior knowledge about free energies is available,
BayesMBAR can incorporate this information into the estimation procedure by
using non-uniform prior distributions. As an example, we show that, by
incorporating the prior knowledge about the smoothness of free energy surfaces,
BayesMBAR provides more accurate estimates than the MBAR method. Given MBAR's
widespread use in free energy calculations, we anticipate BayesMBAR to be an
essential tool in various applications of free energy calculations.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Wed,  1 Nov 23</h3>
<dl>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1611.10283" title="Abstract">arXiv:1611.10283</a> (replaced) [<a href="/pdf/1611.10283" title="Download PDF">pdf</a>, <a href="/ps/1611.10283" title="Download PostScript">ps</a>, <a href="/format/1611.10283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bandit algorithms to emulate human decision making using probabilistic  distortions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kolla%2C+R+K">Ravi Kumar Kolla</a>, 
<a href="/search/cs?searchtype=author&query=A.%2C+P+L">Prashanth L.A.</a>, 
<a href="/search/cs?searchtype=author&query=Gopalan%2C+A">Aditya Gopalan</a>, 
<a href="/search/cs?searchtype=author&query=Jagannathan%2C+K">Krishna Jagannathan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+M">Michael Fu</a>, 
<a href="/search/cs?searchtype=author&query=Marcus%2C+S">Steve Marcus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The material in this paper was presented in part at the 2017 AAAI Conference on Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1710.03435" title="Abstract">arXiv:1710.03435</a> (replaced) [<a href="/pdf/1710.03435" title="Download PDF">pdf</a>, <a href="/format/1710.03435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combinatorial and Asymptotical Results on the Neighborhood Grid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Skrodzki%2C+M">Martin Skrodzki</a>, 
<a href="/search/math?searchtype=author&query=Reitebuch%2C+U">Ulrich Reitebuch</a>, 
<a href="/search/math?searchtype=author&query=McDonough%2C+A">Alex McDonough</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.07626" title="Abstract">arXiv:2103.07626</a> (replaced) [<a href="/pdf/2103.07626" title="Download PDF">pdf</a>, <a href="/format/2103.07626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Helmholtzian Eigenmap: Topological feature discovery &amp; edge flow  learning from point cloud data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+Y">Yu-Chia Chen</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+W">Weicheng Wu</a>, 
<a href="/search/stat?searchtype=author&query=Meil%C4%83%2C+M">Marina Meil&#x103;</a>, 
<a href="/search/stat?searchtype=author&query=Kevrekidis%2C+I+G">Ioannis G. Kevrekidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.08502" title="Abstract">arXiv:2106.08502</a> (replaced) [<a href="/pdf/2106.08502" title="Download PDF">pdf</a>, <a href="/format/2106.08502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Averaging on the Bures-Wasserstein manifold: dimension-free convergence  of gradient descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Altschuler%2C+J+M">Jason M. Altschuler</a>, 
<a href="/search/math?searchtype=author&query=Chewi%2C+S">Sinho Chewi</a>, 
<a href="/search/math?searchtype=author&query=Gerber%2C+P">Patrik Gerber</a>, 
<a href="/search/math?searchtype=author&query=Stromme%2C+A+J">Austin J. Stromme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v3: Fixed error in Theorem 3, see footnote 2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.05828" title="Abstract">arXiv:2108.05828</a> (replaced) [<a href="/pdf/2108.05828" title="Download PDF">pdf</a>, <a href="/format/2108.05828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A general class of surrogate functions for stable and efficient  reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaswani%2C+S">Sharan Vaswani</a>, 
<a href="/search/cs?searchtype=author&query=Bachem%2C+O">Olivier Bachem</a>, 
<a href="/search/cs?searchtype=author&query=Totaro%2C+S">Simone Totaro</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+R">Robert Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Shivam Garg</a>, 
<a href="/search/cs?searchtype=author&query=Geist%2C+M">Matthieu Geist</a>, 
<a href="/search/cs?searchtype=author&query=Machado%2C+M+C">Marlos C. Machado</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
<a href="/search/cs?searchtype=author&query=Roux%2C+N+L">Nicolas Le Roux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed minor typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.07313" title="Abstract">arXiv:2109.07313</a> (replaced) [<a href="/pdf/2109.07313" title="Download PDF">pdf</a>, <a href="/format/2109.07313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximately EFX Allocations for Indivisible Chores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shengwei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaowei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Include new results about partial EFX allocations and improved results regarding approximate EFX allocations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.01339" title="Abstract">arXiv:2110.01339</a> (replaced) [<a href="/pdf/2110.01339" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pharmacoprint -- a combination of pharmacophore fingerprint and  artificial intelligence as a tool for computer-aided drug design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Warszycki%2C+D">Dawid Warszycki</a>, 
<a href="/search/q-bio?searchtype=author&query=Struski%2C+%C5%81">&#x141;ukasz Struski</a>, 
<a href="/search/q-bio?searchtype=author&query=%C5%9Amieja%2C+M">Marek &#x15a;mieja</a>, 
<a href="/search/q-bio?searchtype=author&query=Kafel%2C+R">Rafa&#x142; Kafel</a>, 
<a href="/search/q-bio?searchtype=author&query=Kurczab%2C+R">Rafa&#x142; Kurczab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal of Chemical Information and Modeling (2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.02353" title="Abstract">arXiv:2112.02353</a> (replaced) [<a href="/pdf/2112.02353" title="Download PDF">pdf</a>, <a href="/format/2112.02353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Hierarchy Transition: Delving into Class Hierarchies to Enhance  Deep Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Renzhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=cai%2C+D">De cai</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+K">Kaiwen Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xixi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+D">Deyu Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.07158" title="Abstract">arXiv:2112.07158</a> (replaced) [<a href="/pdf/2112.07158" title="Download PDF">pdf</a>, <a href="/format/2112.07158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hop-Spanners for Geometric Intersection Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conroy%2C+J+B">Jonathan B. Conroy</a>, 
<a href="/search/cs?searchtype=author&query=T%C3%B3th%2C+C+D">Csaba D. T&#xf3;th</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 24 figures, full version of an extended abstract in the Proceedings of SoCG 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.00522" title="Abstract">arXiv:2201.00522</a> (replaced) [<a href="/pdf/2201.00522" title="Download PDF">pdf</a>, <a href="/format/2201.00522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Coverage Criteria for Combinatorial Sequence Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elyasaf%2C+A">Achiya Elyasaf</a>, 
<a href="/search/cs?searchtype=author&query=Farchi%2C+E">Eitan Farchi</a>, 
<a href="/search/cs?searchtype=author&query=Margalit%2C+O">Oded Margalit</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+G">Gera Weiss</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+Y">Yeshayahu Weiss</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 tables, 5 figures, and 2 listing
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Transactions on Software Engineering, vol. 49, no. 8, pp.
  4023-4034, 24 May 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.10101" title="Abstract">arXiv:2202.10101</a> (replaced) [<a href="/pdf/2202.10101" title="Download PDF">pdf</a>, <a href="/format/2202.10101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BERT WEAVER: Using WEight AVERaging to enable lifelong learning for  transformer-based models in biomedical semantic search engines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C3%BChnel%2C+L">Lisa K&#xfc;hnel</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+A">Alexander Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Hammer%2C+B">Barbara Hammer</a>, 
<a href="/search/cs?searchtype=author&query=Fluck%2C+J">Juliane Fluck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.04279" title="Abstract">arXiv:2203.04279</a> (replaced) [<a href="/pdf/2203.04279" title="Download PDF">pdf</a>, <a href="/format/2203.04279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Warp Consistency for Weakly-Supervised Semantic  Correspondences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Truong%2C+P">Prune Truong</a>, 
<a href="/search/cs?searchtype=author&query=Danelljan%2C+M">Martin Danelljan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fisher Yu</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CVPR 2022 code: <a href="https://github.com/PruneTruong/DenseMatching">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR) 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.15009" title="Abstract">arXiv:2203.15009</a> (replaced) [<a href="/pdf/2203.15009" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DAMNETS: A Deep Autoregressive Model for Generating Markovian Network  Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Clarkson%2C+J">Jase Clarkson</a>, 
<a href="/search/stat?searchtype=author&query=Cucuringu%2C+M">Mihai Cucuringu</a>, 
<a href="/search/stat?searchtype=author&query=Elliott%2C+A">Andrew Elliott</a>, 
<a href="/search/stat?searchtype=author&query=Reinert%2C+G">Gesine Reinert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistical Finance (q-fin.ST); Applications (stat.AP); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.15446" title="Abstract">arXiv:2203.15446</a> (replaced) [<a href="/pdf/2203.15446" title="Download PDF">pdf</a>, <a href="/ps/2203.15446" title="Download PostScript">ps</a>, <a href="/format/2203.15446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A framework for minimal hereditary classes of graphs of unbounded  clique-width
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Brignall%2C+R">Robert Brignall</a>, 
<a href="/search/math?searchtype=author&query=Cocks%2C+D">Daniel Cocks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM J. Disc. Math. Vol. 37, Iss. 4 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.02445" title="Abstract">arXiv:2204.02445</a> (replaced) [<a href="/pdf/2204.02445" title="Download PDF">pdf</a>, <a href="/format/2204.02445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHORE: Contact, Human and Object REconstruction from a single RGB image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xianghui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+B+L">Bharat Lal Bhatnagar</a>, 
<a href="/search/cs?searchtype=author&query=Pons-Moll%2C+G">Gerard Pons-Moll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ECCV 2022, edited the acknowledgement
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.06457" title="Abstract">arXiv:2204.06457</a> (replaced) [<a href="/pdf/2204.06457" title="Download PDF">pdf</a>, <a href="/format/2204.06457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Cross-Lingual Adjustment of Contextual Word  Representations on Zero-Shot Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Efimov%2C+P">Pavel Efimov</a>, 
<a href="/search/cs?searchtype=author&query=Boytsov%2C+L">Leonid Boytsov</a>, 
<a href="/search/cs?searchtype=author&query=Arslanova%2C+E">Elena Arslanova</a>, 
<a href="/search/cs?searchtype=author&query=Braslavski%2C+P">Pavel Braslavski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at ECIR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.08663" title="Abstract">arXiv:2204.08663</a> (replaced) [<a href="/pdf/2204.08663" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training of Equivariant Graph Matching Networks with Conformation  Flexibility for Drug Binding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shuting Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yinghui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xurui Jin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Bowen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhangming Niu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangrong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xiangxiang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advanced Science 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.10695" title="Abstract">arXiv:2204.10695</a> (replaced) [<a href="/pdf/2204.10695" title="Download PDF">pdf</a>, <a href="/format/2204.10695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universum-inspired Supervised Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+A">Aiyang Han</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+C">Chuanxing Geng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Songcan Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Image Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.10916" title="Abstract">arXiv:2205.10916</a> (replaced) [<a href="/pdf/2205.10916" title="Download PDF">pdf</a>, <a href="/format/2205.10916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Data-Enabled Predictive Leading Cruise Control in  Mixed Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+K">Kaixiang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+K">Kaian Chen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhaojian Li</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jun Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+Y">Yang Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.01203" title="Abstract">arXiv:2206.01203</a> (replaced) [<a href="/pdf/2206.01203" title="Download PDF">pdf</a>, <a href="/format/2206.01203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Box2Mask: Weakly Supervised 3D Semantic Instance Segmentation Using  Bounding Boxes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chibane%2C+J">Julian Chibane</a>, 
<a href="/search/cs?searchtype=author&query=Engelmann%2C+F">Francis Engelmann</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T+A">Tuan Anh Tran</a>, 
<a href="/search/cs?searchtype=author&query=Pons-Moll%2C+G">Gerard Pons-Moll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://virtualhumans.mpi-inf.mpg.de/box2mask/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> European Conference on Computer Vision (ECCV), 2022, Oral
  Presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.05927" title="Abstract">arXiv:2206.05927</a> (replaced) [<a href="/pdf/2206.05927" title="Download PDF">pdf</a>, <a href="/format/2206.05927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yunge Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinlong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiahua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haibo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xieyuanli Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Feng Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.07264" title="Abstract">arXiv:2206.07264</a> (replaced) [<a href="/pdf/2206.07264" title="Download PDF">pdf</a>, <a href="/format/2206.07264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Born for Auto-Tagging: Faster and better with new objective functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chiung-ju Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shieh%2C+H">Huang-Ting Shieh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.08823" title="Abstract">arXiv:2206.08823</a> (replaced) [<a href="/pdf/2206.08823" title="Download PDF">pdf</a>, <a href="/format/2206.08823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language with Vision: a Study on Grounded Word and Sentence Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahmohammadi%2C+H">Hassan Shahmohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Heitmeier%2C+M">Maria Heitmeier</a>, 
<a href="/search/cs?searchtype=author&query=Shafaei-Bajestan%2C+E">Elnaz Shafaei-Bajestan</a>, 
<a href="/search/cs?searchtype=author&query=Lensch%2C+H+P+A">Hendrik P. A. Lensch</a>, 
<a href="/search/cs?searchtype=author&query=Baayen%2C+H">Harald Baayen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.14330" title="Abstract">arXiv:2206.14330</a> (replaced) [<a href="/pdf/2206.14330" title="Download PDF">pdf</a>, <a href="/ps/2206.14330" title="Download PostScript">ps</a>, <a href="/format/2206.14330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Based Approaches to Channel Charting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aly%2C+A">Amr Aly</a>, 
<a href="/search/cs?searchtype=author&query=Ayanoglu%2C+E">Ender Ayanoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.15381" title="Abstract">arXiv:2206.15381</a> (replaced) [<a href="/pdf/2206.15381" title="Download PDF">pdf</a>, <a href="/format/2206.15381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How direct is the link between words and images?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahmohammadi%2C+H">Hassan Shahmohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Heitmeier%2C+M">Maria Heitmeier</a>, 
<a href="/search/cs?searchtype=author&query=Shafaei-Bajestan%2C+E">Elnaz Shafaei-Bajestan</a>, 
<a href="/search/cs?searchtype=author&query=Lensch%2C+H+P+A">Hendrik P. A. Lensch</a>, 
<a href="/search/cs?searchtype=author&query=Baayen%2C+H">Harald Baayen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the Mental Lexicon Journal: <a href="https://benjamins.com/catalog/ml">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.02016" title="Abstract">arXiv:2207.02016</a> (replaced) [<a href="/pdf/2207.02016" title="Download PDF">pdf</a>, <a href="/format/2207.02016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Reinforcement Learning in Continuous Control Tasks with  Uncertainty Set Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Boedecker%2C+J">Joschka Boedecker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04550" title="Abstract">arXiv:2207.04550</a> (replaced) [<a href="/pdf/2207.04550" title="Download PDF">pdf</a>, <a href="/format/2207.04550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Order for Inventory Systems with Lost Sales and Uncertain  Supplies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+B">Boxiao Chen</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+J">Jiashuo Jiang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jiawei Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Z">Zhengyuan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08502" title="Abstract">arXiv:2207.08502</a> (replaced) [<a href="/pdf/2207.08502" title="Download PDF">pdf</a>, <a href="/format/2207.08502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomial-time algorithms for continuous metrics on atomic clouds of  unordered points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurlin%2C+V">Vitaliy Kurlin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 2 figures. The paper was published (with grayscale images) in the journal MATCH Communications in Mathematical and in Computer Chemistry, v.91 (2024), p.79-108, <a href="https://doi.org/10.46793/match.91-1.079K.">this https URL</a> This version includes color images. The latest file is at <a href="http://kurlin.org/projects/cloud-isometry-spaces/metrics-atomic-clouds.pdf">this http URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> MATCH Communications in Mathematical and in Computer Chemistry,
  v.91 (2024), p.79-108
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.03829" title="Abstract">arXiv:2208.03829</a> (replaced) [<a href="/pdf/2208.03829" title="Download PDF">pdf</a>, <a href="/format/2208.03829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Random Points: Combinatorial Complexity and Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Har-Peled%2C+S">Sariel Har-Peled</a>, 
<a href="/search/cs?searchtype=author&query=Harb%2C+E">Elfarouk Harb</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.04591" title="Abstract">arXiv:2208.04591</a> (replaced) [<a href="/pdf/2208.04591" title="Download PDF">pdf</a>, <a href="/format/2208.04591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stronger Privacy Amplification by Shuffling for R&#xe9;nyi and Approximate  Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldman%2C+V">Vitaly Feldman</a>, 
<a href="/search/cs?searchtype=author&query=McMillan%2C+A">Audra McMillan</a>, 
<a href="/search/cs?searchtype=author&query=Talwar%2C+K">Kunal Talwar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Errata added. 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02564" title="Abstract">arXiv:2209.02564</a> (replaced) [<a href="/pdf/2209.02564" title="Download PDF">pdf</a>, <a href="/format/2209.02564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Domain Adaptation with Contrastive Learning for Object  Detection in the Satellite Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biswas%2C+D">Debojyoti Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Te%C5%A1i%C4%87%2C+J">Jelena Te&#x161;i&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.04123" title="Abstract">arXiv:2209.04123</a> (replaced) [<a href="/pdf/2209.04123" title="Download PDF">pdf</a>, <a href="/format/2209.04123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Stochastic Bin-Packing in Large Service Systems with  Time-Varying Item Sizes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yige Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qiaomin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weina Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05042" title="Abstract">arXiv:2209.05042</a> (replaced) [<a href="/pdf/2209.05042" title="Download PDF">pdf</a>, <a href="/format/2209.05042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Optimization Landscape of Dynamic Output Feedback: A Case Study  for Linear Quadratic Regulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jingliang Duan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+W">Wenhan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lin Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2201.09598">arXiv:2201.09598</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE 61st Conference on Decision and Control (CDC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07230" title="Abstract">arXiv:2209.07230</a> (replaced) [<a href="/pdf/2209.07230" title="Download PDF">pdf</a>, <a href="/ps/2209.07230" title="Download PostScript">ps</a>, <a href="/format/2209.07230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recovery Guarantees for Distributed-OMP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Amiraz%2C+C">Chen Amiraz</a>, 
<a href="/search/stat?searchtype=author&query=Krauthgamer%2C+R">Robert Krauthgamer</a>, 
<a href="/search/stat?searchtype=author&query=Nadler%2C+B">Boaz Nadler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07787" title="Abstract">arXiv:2209.07787</a> (replaced) [<a href="/pdf/2209.07787" title="Download PDF">pdf</a>, <a href="/format/2209.07787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Double logistic regression approach to biased positive-unlabeled data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Furma%C5%84czyk%2C+K">Konrad Furma&#x144;czyk</a>, 
<a href="/search/stat?searchtype=author&query=Mielniczuk%2C+J">Jan Mielniczuk</a>, 
<a href="/search/stat?searchtype=author&query=Rejchel%2C+W">Wojciech Rejchel</a>, 
<a href="/search/stat?searchtype=author&query=Teisseyre%2C+P">Pawe&#x142; Teisseyre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> -
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09593" title="Abstract">arXiv:2209.09593</a> (replaced) [<a href="/pdf/2209.09593" title="Download PDF">pdf</a>, <a href="/format/2209.09593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation  Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Larionov%2C+D">Daniil Larionov</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C3%BCnwald%2C+J">Jens Gr&#xfc;nwald</a>, 
<a href="/search/cs?searchtype=author&query=Leiter%2C+C">Christoph Leiter</a>, 
<a href="/search/cs?searchtype=author&query=Eger%2C+S">Steffen Eger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023; note the title and author change
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12301" title="Abstract">arXiv:2209.12301</a> (replaced) [<a href="/pdf/2209.12301" title="Download PDF">pdf</a>, <a href="/format/2209.12301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constant-delay enumeration for SLP-compressed documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz%2C+M">Mart&#xed;n Mu&#xf1;oz</a>, 
<a href="/search/cs?searchtype=author&query=Riveros%2C+C">Cristian Riveros</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.14790" title="Abstract">arXiv:2209.14790</a> (replaced) [<a href="/pdf/2209.14790" title="Download PDF">pdf</a>, <a href="/format/2209.14790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse PCA With Multiple Components
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cory-Wright%2C+R">Ryan Cory-Wright</a>, 
<a href="/search/math?searchtype=author&query=Pauphilet%2C+J">Jean Pauphilet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version with improved algorithmics and a new section containing a generalization of the Gershgorin circle theorem; comments or suggestions welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04216" title="Abstract">arXiv:2210.04216</a> (replaced) [<a href="/pdf/2210.04216" title="Download PDF">pdf</a>, <a href="/format/2210.04216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMPose: Alternately Mixed Global-Local Attention Model for 3D Human Pose  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongxin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+Y">Yunwei Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peiyuan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICASSP 2023 Accepted Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09404" title="Abstract">arXiv:2210.09404</a> (replaced) [<a href="/pdf/2210.09404" title="Download PDF">pdf</a>, <a href="/format/2210.09404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measures of Information Reflect Memorization Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bansal%2C+R">Rachit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Pruthi%2C+D">Danish Pruthi</a>, 
<a href="/search/cs?searchtype=author&query=Belinkov%2C+Y">Yonatan Belinkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages; NeurIPS 2022. Code and data at <a href="https://rachitbansal.github.io/information-measures">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09809" title="Abstract">arXiv:2210.09809</a> (replaced) [<a href="/pdf/2210.09809" title="Download PDF">pdf</a>, <a href="/format/2210.09809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Convolutions, Non-linearity and Depth in Graph Neural  Networks using Neural Tangent Kernel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sabanayagam%2C+M">Mahalakshmi Sabanayagam</a>, 
<a href="/search/cs?searchtype=author&query=Esser%2C+P">Pascal Esser</a>, 
<a href="/search/cs?searchtype=author&query=Ghoshdastidar%2C+D">Debarghya Ghoshdastidar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 24 figures. Code available at <a href="https://github.com/mahalakshmi-sabanayagam/NTK_GCN">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10001" title="Abstract">arXiv:2211.10001</a> (replaced) [<a href="/pdf/2211.10001" title="Download PDF">pdf</a>, <a href="/format/2211.10001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BDTS: Blockchain-based Data Trading System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+E">Erya Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bo Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qianhong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sanxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenchang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+Y">Yingxin Bi</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wenyi Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICICS 2023 (Best Paper Award)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Information and Communications
  Security, pp. 645-664. Singapore: Springer Nature Singapore, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11087" title="Abstract">arXiv:2211.11087</a> (replaced) [<a href="/pdf/2211.11087" title="Download PDF">pdf</a>, <a href="/format/2211.11087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conceptor-Aided Debiasing of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yifei%2C+L+S">Li S. Yifei</a>, 
<a href="/search/cs?searchtype=author&query=Ungar%2C+L">Lyle Ungar</a>, 
<a href="/search/cs?searchtype=author&query=Sedoc%2C+J">Jo&#xe3;o Sedoc</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11127" title="Abstract">arXiv:2211.11127</a> (replaced) [<a href="/pdf/2211.11127" title="Download PDF">pdf</a>, <a href="/format/2211.11127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming Reachability Analysis of DNN-Controlled Systems via  Abstraction-Based Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jiaxu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhi%2C+D">Dapeng Zhi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Si Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peixin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Katz%2C+G">Guy Katz</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.01792" title="Abstract">arXiv:2212.01792</a> (replaced) [<a href="/pdf/2212.01792" title="Download PDF">pdf</a>, <a href="/ps/2212.01792" title="Download PostScript">ps</a>, <a href="/format/2212.01792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification by sparse additive models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Abramovich%2C+F">Felix Abramovich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02648" title="Abstract">arXiv:2212.02648</a> (replaced) [<a href="/pdf/2212.02648" title="Download PDF">pdf</a>, <a href="/format/2212.02648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moayeri%2C+M">Mazda Moayeri</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+S">Sahil Singla</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS '23 (Spotlight). Camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04079" title="Abstract">arXiv:2212.04079</a> (replaced) [<a href="/pdf/2212.04079" title="Download PDF">pdf</a>, <a href="/format/2212.04079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A numerical domain decomposition method for solving elliptic equations  on manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cao%2C+S">Shuhao Cao</a>, 
<a href="/search/math?searchtype=author&query=Qin%2C+L">Lizhen Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final version. To appear in SIAM Journal on Scientific Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Differential Geometry (math.DG)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07886" title="Abstract">arXiv:2212.07886</a> (replaced) [<a href="/pdf/2212.07886" title="Download PDF">pdf</a>, <a href="/format/2212.07886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learned Kernel For Blind Super-Resolution Kernel Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+R">Royson Lee</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Venieris%2C+S+I">Stylianos I. Venieris</a>, 
<a href="/search/cs?searchtype=author&query=Hospedales%2C+T">Timothy Hospedales</a>, 
<a href="/search/cs?searchtype=author&query=Husz%C3%A1r%2C+F">Ferenc Husz&#xe1;r</a>, 
<a href="/search/cs?searchtype=author&query=Lane%2C+N+D">Nicholas D. Lane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint: Accepted at the 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10564" title="Abstract">arXiv:2212.10564</a> (replaced) [<a href="/pdf/2212.10564" title="Download PDF">pdf</a>, <a href="/format/2212.10564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Vision-free Baseline for Multimodal Grammar Induction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Corona%2C+R">Rodolfo Corona</a>, 
<a href="/search/cs?searchtype=author&query=Mangalam%2C+K">Karttikeya Mangalam</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Catherine Chen</a>, 
<a href="/search/cs?searchtype=author&query=Flaherty%2C+D">Daniel Flaherty</a>, 
<a href="/search/cs?searchtype=author&query=Belongie%2C+S">Serge Belongie</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+K+Q">Kilian Q. Weinberger</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+J">Jitendra Malik</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10764" title="Abstract">arXiv:2212.10764</a> (replaced) [<a href="/pdf/2212.10764" title="Download PDF">pdf</a>, <a href="/format/2212.10764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning List-Level Domain-Invariant Representations for Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xian%2C+R">Ruicheng Xian</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Honglei Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zamani%2C+H">Hamed Zamani</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Ji Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+K">Kai Hui</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuanhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Comparison to v1: revised presentation and proof of Corollary 4.9
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.07261" title="Abstract">arXiv:2301.07261</a> (replaced) [<a href="/pdf/2301.07261" title="Download PDF">pdf</a>, <a href="/ps/2301.07261" title="Download PostScript">ps</a>, <a href="/format/2301.07261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Note on the $k$-colored Crossing Ratio of Dense Geometric Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fabila-Monroy%2C+R">Ruy Fabila-Monroy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08848" title="Abstract">arXiv:2301.08848</a> (replaced) [<a href="/pdf/2301.08848" title="Download PDF">pdf</a>, <a href="/format/2301.08848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity of Answers to Conjunctive Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Merkl%2C+T+C">Timo Camillo Merkl</a>, 
<a href="/search/cs?searchtype=author&query=Pichler%2C+R">Reinhard Pichler</a>, 
<a href="/search/cs?searchtype=author&query=Skritek%2C+S">Sebastian Skritek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, submitted to LMCS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11323" title="Abstract">arXiv:2301.11323</a> (replaced) [<a href="/pdf/2301.11323" title="Download PDF">pdf</a>, <a href="/format/2301.11323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Training of Deep Ensembles Fails Due to Learner Collusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeffares%2C+A">Alan Jeffares</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tennison Liu</a>, 
<a href="/search/cs?searchtype=author&query=Crabb%C3%A9%2C+J">Jonathan Crabb&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11781" title="Abstract">arXiv:2301.11781</a> (replaced) [<a href="/pdf/2301.11781" title="Download PDF">pdf</a>, <a href="/format/2301.11781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness  Interventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luxi">Luxi</a> (Lucy)He, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Rui Gao</a>, 
<a href="/search/cs?searchtype=author&query=Calmon%2C+F+P">Flavio P. Calmon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13737" title="Abstract">arXiv:2301.13737</a> (replaced) [<a href="/pdf/2301.13737" title="Download PDF">pdf</a>, <a href="/format/2301.13737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Consistent Velocity Matching of Probability Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Hurault%2C+S">Samuel Hurault</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+J">Justin Solomon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03813" title="Abstract">arXiv:2302.03813</a> (replaced) [<a href="/pdf/2302.03813" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multimodal Sensing Ring for Quantification of Scratch Intensity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padmanabha%2C+A">Akhil Padmanabha</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+S">Sonal Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Majidi%2C+C">Carmel Majidi</a>, 
<a href="/search/cs?searchtype=author&query=Erickson%2C+Z">Zackory Erickson</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Commun Med 3, 115 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04536" title="Abstract">arXiv:2302.04536</a> (replaced) [<a href="/pdf/2302.04536" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better by you, better than me, chatgpt3 as writing assistance in  students essays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basic%2C+Z">Zeljana Basic</a>, 
<a href="/search/cs?searchtype=author&query=Banovac%2C+A">Ana Banovac</a>, 
<a href="/search/cs?searchtype=author&query=Kruzic%2C+I">Ivana Kruzic</a>, 
<a href="/search/cs?searchtype=author&query=Jerkovic%2C+I">Ivan Jerkovic</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Basic, Z., Banovac, A., Kruzic, I. &amp; Jerkovic, I. ChatGPT-3.5 as
  writing assistance in students' essays. Humanit Soc Sci Commun 10, 750 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05527" title="Abstract">arXiv:2302.05527</a> (replaced) [<a href="/pdf/2302.05527" title="Download PDF">pdf</a>, <a href="/format/2302.05527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shuyan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+U">Uri Alon</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Sumit Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05743" title="Abstract">arXiv:2302.05743</a> (replaced) [<a href="/pdf/2302.05743" title="Download PDF">pdf</a>, <a href="/format/2302.05743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Distance Matrix Enough for Geometric Deep Learning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zian Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06052" title="Abstract">arXiv:2302.06052</a> (replaced) [<a href="/pdf/2302.06052" title="Download PDF">pdf</a>, <a href="/format/2302.06052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chufeng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianmin Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaolin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report; Code: <a href="https://github.com/zhanggang001/CEDNet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06358" title="Abstract">arXiv:2302.06358</a> (replaced) [<a href="/pdf/2302.06358" title="Download PDF">pdf</a>, <a href="/format/2302.06358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anticipating Next Active Objects for Egocentric Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thakur%2C+S">Sanket Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Beyan%2C+C">Cigdem Beyan</a>, 
<a href="/search/cs?searchtype=author&query=Morerio%2C+P">Pietro Morerio</a>, 
<a href="/search/cs?searchtype=author&query=Murino%2C+V">Vittorio Murino</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bue%2C+A">Alessio Del Bue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> webpage : <a href="https://sanketsans.github.io/anticipating-next-active-object-egocentric.html">this https URL</a> 13 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06896" title="Abstract">arXiv:2302.06896</a> (replaced) [<a href="/pdf/2302.06896" title="Download PDF">pdf</a>, <a href="/format/2302.06896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Message Passing Meets Graph Neural Networks: A New Paradigm for Massive  MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hengtao He</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xianghao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shenghui Song</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 Pages, 7 Figures, and 4 Tables. This paper has been accepted by the IEEE Transactions on Wireless Communications. The code is available at: <a href="https://github.com/hehengtao/AMP_GNN">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06960" title="Abstract">arXiv:2302.06960</a> (replaced) [<a href="/pdf/2302.06960" title="Download PDF">pdf</a>, <a href="/format/2302.06960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data pruning and neural scaling laws: fundamental limitations of  score-based algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ayed%2C+F">Fadhel Ayed</a>, 
<a href="/search/stat?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07491" title="Abstract">arXiv:2302.07491</a> (replaced) [<a href="/pdf/2302.07491" title="Download PDF">pdf</a>, <a href="/format/2302.07491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Temporal Graph learning with Temporal and Structural  Intensity Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Meng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Ke Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yawei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+W">Wenxuan Tu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sihang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwang Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kunlun He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09166" title="Abstract">arXiv:2302.09166</a> (replaced) [<a href="/pdf/2302.09166" title="Download PDF">pdf</a>, <a href="/format/2302.09166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning for Cutting Planes in Integer Programming: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Deza%2C+A">Arnaud Deza</a>, 
<a href="/search/math?searchtype=author&query=Khalil%2C+E+B">Elias B. Khalil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IJCAI 2023 Survey Track
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Joint Conference on Artificial Intelligence, pages 6592-6600
  (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10035" title="Abstract">arXiv:2302.10035</a> (replaced) [<a href="/pdf/2302.10035" title="Download PDF">pdf</a>, <a href="/format/2302.10035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+G">Guangwu Qian</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Pengcheng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiao-Yong Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonghong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wen Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Machine Intelligence Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10586" title="Abstract">arXiv:2302.10586</a> (replaced) [<a href="/pdf/2302.10586" title="Download PDF">pdf</a>, <a href="/format/2302.10586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few  Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+Z">Zebin You</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yong Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+F">Fan Bao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiacheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chongxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12915" title="Abstract">arXiv:2302.12915</a> (replaced) [<a href="/pdf/2302.12915" title="Download PDF">pdf</a>, <a href="/format/2302.12915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Mechanical Search with Large Vision and Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Satvik Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shivakumar%2C+K">Kaushik Shivakumar</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L+Y">Lawrence Yunliang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hoque%2C+R">Ryan Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Ichter%2C+B">Brian Ichter</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+K">Ken Goldberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14697" title="Abstract">arXiv:2302.14697</a> (replaced) [<a href="/pdf/2302.14697" title="Download PDF">pdf</a>, <a href="/ps/2302.14697" title="Download PostScript">ps</a>, <a href="/format/2302.14697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A short proof for the parameter continuation theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Borovik%2C+V">Viktoriia Borovik</a>, 
<a href="/search/math?searchtype=author&query=Breiding%2C+P">Paul Breiding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00402" title="Abstract">arXiv:2303.00402</a> (replaced) [<a href="/pdf/2303.00402" title="Download PDF">pdf</a>, <a href="/format/2303.00402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On discrete ground states of rotating Bose-Einstein condensates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Henning%2C+P">Patrick Henning</a>, 
<a href="/search/math?searchtype=author&query=Yadav%2C+M">Mahima Yadav</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01456" title="Abstract">arXiv:2303.01456</a> (replaced) [<a href="/pdf/2303.01456" title="Download PDF">pdf</a>, <a href="/ps/2303.01456" title="Download PostScript">ps</a>, <a href="/format/2303.01456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness  in ReLU Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frei%2C+S">Spencer Frei</a>, 
<a href="/search/cs?searchtype=author&query=Vardi%2C+G">Gal Vardi</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+P+L">Peter L. Bartlett</a>, 
<a href="/search/cs?searchtype=author&query=Srebro%2C+N">Nathan Srebro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages; NeurIPS 2023 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01621" title="Abstract">arXiv:2303.01621</a> (replaced) [<a href="/pdf/2303.01621" title="Download PDF">pdf</a>, <a href="/format/2303.01621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lamp%2C+J">Josephine Lamp</a>, 
<a href="/search/cs?searchtype=author&query=Derdzinski%2C+M">Mark Derdzinski</a>, 
<a href="/search/cs?searchtype=author&query=Hannemann%2C+C">Christopher Hannemann</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Linden%2C+J">Joost van der Linden</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+D">David Evans</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems 36 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03012" title="Abstract">arXiv:2303.03012</a> (replaced) [<a href="/pdf/2303.03012" title="Download PDF">pdf</a>, <a href="/format/2303.03012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Extracting Specialized Code Abilities from Large Language Models: A  Feasibility Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaozheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Pingchuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chaowei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Daoyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Cuiyun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03432" title="Abstract">arXiv:2303.03432</a> (replaced) [<a href="/pdf/2303.03432" title="Download PDF">pdf</a>, <a href="/format/2303.03432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A polar prediction model for learning to represent visual  transformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Fiquet%2C+P+H">Pierre-&#xc9;tienne H. Fiquet</a>, 
<a href="/search/stat?searchtype=author&query=Simoncelli%2C+E+P">Eero P. Simoncelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04105" title="Abstract">arXiv:2303.04105</a> (replaced) [<a href="/pdf/2303.04105" title="Download PDF">pdf</a>, <a href="/format/2303.04105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Your representations are in the network: composable and parallel  adaptation for large scale models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dukler%2C+Y">Yonatan Dukler</a>, 
<a href="/search/cs?searchtype=author&query=Achille%2C+A">Alessandro Achille</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Vivek%2C+V">Varsha Vivek</a>, 
<a href="/search/cs?searchtype=author&query=Zancato%2C+L">Luca Zancato</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+B">Benjamin Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Ravichandran%2C+A">Avinash Ravichandran</a>, 
<a href="/search/cs?searchtype=author&query=Fowlkes%2C+C">Charless Fowlkes</a>, 
<a href="/search/cs?searchtype=author&query=Swaminathan%2C+A">Ashwin Swaminathan</a>, 
<a href="/search/cs?searchtype=author&query=Soatto%2C+S">Stefano Soatto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04178" title="Abstract">arXiv:2303.04178</a> (replaced) [<a href="/pdf/2303.04178" title="Download PDF">pdf</a>, <a href="/ps/2303.04178" title="Download PostScript">ps</a>, <a href="/format/2303.04178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALSA PICANTE: a machine learning attack on LWE with binary secrets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cathy Li</a>, 
<a href="/search/cs?searchtype=author&query=Sot%C3%A1kov%C3%A1%2C+J">Jana Sot&#xe1;kov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+E">Emily Wenger</a>, 
<a href="/search/cs?searchtype=author&query=Malhou%2C+M">Mohamed Malhou</a>, 
<a href="/search/cs?searchtype=author&query=Garcelon%2C+E">Evrard Garcelon</a>, 
<a href="/search/cs?searchtype=author&query=Charton%2C+F">Francois Charton</a>, 
<a href="/search/cs?searchtype=author&query=Lauter%2C+K">Kristin Lauter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures, 17 tables; accepted to CCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04291" title="Abstract">arXiv:2303.04291</a> (replaced) [<a href="/pdf/2303.04291" title="Download PDF">pdf</a>, <a href="/format/2303.04291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+C+M">Cindy M. Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Chan%2C+E+R">Eric R. Chan</a>, 
<a href="/search/eess?searchtype=author&query=Bergman%2C+A+W">Alexander W. Bergman</a>, 
<a href="/search/eess?searchtype=author&query=Wetzstein%2C+G">Gordon Wetzstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024. Project website: <a href="https://ccnguyen.github.io/diffusion-in-the-dark/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08050" title="Abstract">arXiv:2303.08050</a> (replaced) [<a href="/pdf/2303.08050" title="Download PDF">pdf</a>, <a href="/format/2303.08050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subjective and Objective Quality Assessment for in-the-Wild Computer  Graphics Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jun Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhichao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+X">Xiongkuo Min</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08240" title="Abstract">arXiv:2303.08240</a> (replaced) [<a href="/pdf/2303.08240" title="Download PDF">pdf</a>, <a href="/format/2303.08240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parametric Surface Constrained Upsampler Network for Point Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+P">Pingping Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhenyao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Song Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update Supplementary Files
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the AAAI Conference on Artificial Intelligence. 37,
  1 (Jun. 2023), 250-258
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09093" title="Abstract">arXiv:2303.09093</a> (replaced) [<a href="/pdf/2303.09093" title="Download PDF">pdf</a>, <a href="/format/2303.09093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLEN: General-Purpose Event Detection for Thousands of Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Q">Qiusi Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sha Li</a>, 
<a href="/search/cs?searchtype=author&query=Conger%2C+K">Kathryn Conger</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+M">Martha Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023. The first two authors contributed equally. (16 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10834" title="Abstract">arXiv:2303.10834</a> (replaced) [<a href="/pdf/2303.10834" title="Download PDF">pdf</a>, <a href="/format/2303.10834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object-Centric Slot Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jindong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+F">Fei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gautam Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungjin Ahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 as a Spotlight paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10837" title="Abstract">arXiv:2303.10837</a> (replaced) [<a href="/pdf/2303.10837" title="Download PDF">pdf</a>, <a href="/format/2303.10837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving  Federated Learning System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Weizhao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuhang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shanshan Han</a>, 
<a href="/search/cs?searchtype=author&query=Joe-Wong%2C+C">Carlee Joe-Wong</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+S">Srivatsan Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chaoyang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12799" title="Abstract">arXiv:2303.12799</a> (replaced) [<a href="/pdf/2303.12799" title="Download PDF">pdf</a>, <a href="/format/2303.12799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Series as Images: Vision Transformer for Irregularly Sampled Time  Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zekun Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xifeng Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS2023. Code and data are available at: <a href="https://github.com/Leezekun/ViTST">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13294" title="Abstract">arXiv:2303.13294</a> (replaced) [<a href="/pdf/2303.13294" title="Download PDF">pdf</a>, <a href="/format/2303.13294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Considerations on the Evaluation of Biometric Quality Assessment  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schlett%2C+T">Torsten Schlett</a>, 
<a href="/search/cs?searchtype=author&query=Rathgeb%2C+C">Christian Rathgeb</a>, 
<a href="/search/cs?searchtype=author&query=Tapia%2C+J">Juan Tapia</a>, 
<a href="/search/cs?searchtype=author&query=Busch%2C+C">Christoph Busch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13485" title="Abstract">arXiv:2303.13485</a> (replaced) [<a href="/pdf/2303.13485" title="Download PDF">pdf</a>, <a href="/format/2303.13485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Timely Multi-Process Estimation Over Erasure Channels With and Without  Feedback: Signal-Independent Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banawan%2C+K">Karim Banawan</a>, 
<a href="/search/cs?searchtype=author&query=Arafa%2C+A">Ahmed Arafa</a>, 
<a href="/search/cs?searchtype=author&query=Seddik%2C+K+G">Karim G. Seddik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the JSAIT Issue on The Role of Freshness and Semantic Measures in the Transmission of Information for Next Generation Networks. arXiv admin note: text overlap with <a href="/abs/2209.11213">arXiv:2209.11213</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15349" title="Abstract">arXiv:2303.15349</a> (replaced) [<a href="/pdf/2303.15349" title="Download PDF">pdf</a>, <a href="/format/2303.15349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Maximizing Curriculum: A Curriculum-Based Approach for  Imitating Diverse Skills
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blessing%2C+D">Denis Blessing</a>, 
<a href="/search/cs?searchtype=author&query=Celik%2C+O">Onur Celik</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaogang Jia</a>, 
<a href="/search/cs?searchtype=author&query=Reuss%2C+M">Moritz Reuss</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M+X">Maximilian Xiling Li</a>, 
<a href="/search/cs?searchtype=author&query=Lioutikov%2C+R">Rudolf Lioutikov</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+G">Gerhard Neumann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16129" title="Abstract">arXiv:2303.16129</a> (replaced) [<a href="/pdf/2303.16129" title="Download PDF">pdf</a>, <a href="/format/2303.16129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A  Survey of AIGC Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minrui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiawen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zehui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+S">Shiwen Mao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhu Han</a>, 
<a href="/search/cs?searchtype=author&query=Jamalipour%2C+A">Abbas Jamalipour</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D+I">Dong In Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xuemin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Leung%2C+V+C+M">Victor C. M. Leung</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16479" title="Abstract">arXiv:2303.16479</a> (replaced) [<a href="/pdf/2303.16479" title="Download PDF">pdf</a>, <a href="/format/2303.16479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visibility Aware Human-Object Interaction Tracking from Single RGB  Camera
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xianghui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+B+L">Bharat Lal Bhatnagar</a>, 
<a href="/search/cs?searchtype=author&query=Pons-Moll%2C+G">Gerard Pons-Moll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to CVPR 2023, edited acknowledgement
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16797" title="Abstract">arXiv:2303.16797</a> (replaced) [<a href="/pdf/2303.16797" title="Download PDF">pdf</a>, <a href="/format/2303.16797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Impact of Control Signaling in RIS-Empowered Wireless  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saggese%2C+F">Fabio Saggese</a>, 
<a href="/search/cs?searchtype=author&query=Croisfelt%2C+V">Victor Croisfelt</a>, 
<a href="/search/cs?searchtype=author&query=Kotaba%2C+R">Rados&#x142;aw Kotaba</a>, 
<a href="/search/cs?searchtype=author&query=Stylianopoulos%2C+K">Kyriakos Stylianopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Alexandropoulos%2C+G+C">George C. Alexandropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Popovski%2C+P">Petar Popovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE TWC, the copyright may be transferred without further notice
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.18138" title="Abstract">arXiv:2303.18138</a> (replaced) [<a href="/pdf/2303.18138" title="Download PDF">pdf</a>, <a href="/format/2303.18138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Sihao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bingqiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shengliang Lu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingsheng He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ling Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The ACM Web conference (WWW) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00199" title="Abstract">arXiv:2304.00199</a> (replaced) [<a href="/pdf/2304.00199" title="Download PDF">pdf</a>, <a href="/format/2304.00199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of No-Collision Transportation Maps in Manifold Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Negrini%2C+E">Elisa Negrini</a>, 
<a href="/search/cs?searchtype=author&query=Nurbekyan%2C+L">Levon Nurbekyan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01295" title="Abstract">arXiv:2304.01295</a> (replaced) [<a href="/pdf/2304.01295" title="Download PDF">pdf</a>, <a href="/format/2304.01295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficiently Aligned Cross-Lingual Transfer Learning for Conversational  Tasks using Prompt-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+L">Lifu Tu</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Jin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Yavuz%2C+S">Semih Yavuz</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingbo Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02061" title="Abstract">arXiv:2304.02061</a> (replaced) [<a href="/pdf/2304.02061" title="Download PDF">pdf</a>, <a href="/format/2304.02061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Continual Human Motion in Diverse 3D Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mir%2C+A">Aymen Mir</a>, 
<a href="/search/cs?searchtype=author&query=Puig%2C+X">Xavier Puig</a>, 
<a href="/search/cs?searchtype=author&query=Kanazawa%2C+A">Angjoo Kanazawa</a>, 
<a href="/search/cs?searchtype=author&query=Pons-Moll%2C+G">Gerard Pons-Moll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02912" title="Abstract">arXiv:2304.02912</a> (replaced) [<a href="/pdf/2304.02912" title="Download PDF">pdf</a>, <a href="/format/2304.02912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification of Heavy-tailed Features in High Dimensions: a  Superstatistical Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Adomaityte%2C+U">Urte Adomaityte</a>, 
<a href="/search/stat?searchtype=author&query=Sicuro%2C+G">Gabriele Sicuro</a>, 
<a href="/search/stat?searchtype=author&query=Vivo%2C+P">Pierpaolo Vivo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03216" title="Abstract">arXiv:2304.03216</a> (replaced) [<a href="/pdf/2304.03216" title="Download PDF">pdf</a>, <a href="/format/2304.03216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Pareto Front of Multilingual Neural Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shuming Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+B">Baobao Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03531" title="Abstract">arXiv:2304.03531</a> (replaced) [<a href="/pdf/2304.03531" title="Download PDF">pdf</a>, <a href="/format/2304.03531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Retrieval to Generation: Efficient and Effective Entity Set  Expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shulin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shirong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangning Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hai-Tao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Ying Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04150" title="Abstract">arXiv:2304.04150</a> (replaced) [<a href="/pdf/2304.04150" title="Download PDF">pdf</a>, <a href="/format/2304.04150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboPianist: Dexterous Piano Playing with Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zakka%2C+K">Kevin Zakka</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Philipp Wu</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+L">Laura Smith</a>, 
<a href="/search/cs?searchtype=author&query=Gileadi%2C+N">Nimrod Gileadi</a>, 
<a href="/search/cs?searchtype=author&query=Howell%2C+T">Taylor Howell</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X+B">Xue Bin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sumeet Singh</a>, 
<a href="/search/cs?searchtype=author&query=Tassa%2C+Y">Yuval Tassa</a>, 
<a href="/search/cs?searchtype=author&query=Florence%2C+P">Pete Florence</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Andy Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Conference on Robot Learning (CORL) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06025" title="Abstract">arXiv:2304.06025</a> (replaced) [<a href="/pdf/2304.06025" title="Download PDF">pdf</a>, <a href="/format/2304.06025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karras%2C+J">Johanna Karras</a>, 
<a href="/search/cs?searchtype=author&query=Holynski%2C+A">Aleksander Holynski</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting-Chun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kemelmacher-Shlizerman%2C+I">Ira Kemelmacher-Shlizerman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://grail.cs.washington.edu/projects/dreampose/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06174" title="Abstract">arXiv:2304.06174</a> (replaced) [<a href="/pdf/2304.06174" title="Download PDF">pdf</a>, <a href="/format/2304.06174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurate transition state generation with an object-aware equivariant  elementary reaction diffusion model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Duan%2C+C">Chenru Duan</a>, 
<a href="/search/physics?searchtype=author&query=Du%2C+Y">Yuanqi Du</a>, 
<a href="/search/physics?searchtype=author&query=Jia%2C+H">Haojun Jia</a>, 
<a href="/search/physics?searchtype=author&query=Kulik%2C+H+J">Heather J. Kulik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07327" title="Abstract">arXiv:2304.07327</a> (replaced) [<a href="/pdf/2304.07327" title="Download PDF">pdf</a>, <a href="/format/2304.07327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenAssistant Conversations -- Democratizing Large Language Model  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C3%B6pf%2C+A">Andreas K&#xf6;pf</a>, 
<a href="/search/cs?searchtype=author&query=Kilcher%2C+Y">Yannic Kilcher</a>, 
<a href="/search/cs?searchtype=author&query=von+R%C3%BCtte%2C+D">Dimitri von R&#xfc;tte</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostidis%2C+S">Sotiris Anagnostidis</a>, 
<a href="/search/cs?searchtype=author&query=Tam%2C+Z">Zhi-Rui Tam</a>, 
<a href="/search/cs?searchtype=author&query=Stevens%2C+K">Keith Stevens</a>, 
<a href="/search/cs?searchtype=author&query=Barhoum%2C+A">Abdullah Barhoum</a>, 
<a href="/search/cs?searchtype=author&query=Duc%2C+N+M">Nguyen Minh Duc</a>, 
<a href="/search/cs?searchtype=author&query=Stanley%2C+O">Oliver Stanley</a>, 
<a href="/search/cs?searchtype=author&query=Nagyfi%2C+R">Rich&#xe1;rd Nagyfi</a>, 
<a href="/search/cs?searchtype=author&query=ES%2C+S">Shahul ES</a>, 
<a href="/search/cs?searchtype=author&query=Suri%2C+S">Sameer Suri</a>, 
<a href="/search/cs?searchtype=author&query=Glushkov%2C+D">David Glushkov</a>, 
<a href="/search/cs?searchtype=author&query=Dantuluri%2C+A">Arnav Dantuluri</a>, 
<a href="/search/cs?searchtype=author&query=Maguire%2C+A">Andrew Maguire</a>, 
<a href="/search/cs?searchtype=author&query=Schuhmann%2C+C">Christoph Schuhmann</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Huu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mattick%2C+A">Alexander Mattick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in NeurIPS 2023 Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07705" title="Abstract">arXiv:2304.07705</a> (replaced) [<a href="/pdf/2304.07705" title="Download PDF">pdf</a>, <a href="/format/2304.07705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handling Heavy Occlusion in Dense Crowd Tracking by Focusing on the  Heads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+W">Wei Bao</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhongzheng Lai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+D">Dong Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AJCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07882" title="Abstract">arXiv:2304.07882</a> (replaced) [<a href="/pdf/2304.07882" title="Download PDF">pdf</a>, <a href="/format/2304.07882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning of Shareable Bases for Personalization-Friendly Image  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong-You Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+J">Jike Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingda Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xuhui Jia</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+H">Hang Qi</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+B">Boqing Gong</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+W">Wei-Lun Chao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09074" title="Abstract">arXiv:2304.09074</a> (replaced) [<a href="/pdf/2304.09074" title="Download PDF">pdf</a>, <a href="/format/2304.09074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electrical Impedance Tomography with Deep Calder&#xf3;n Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cen%2C+S">Siyu Cen</a>, 
<a href="/search/math?searchtype=author&query=Jin%2C+B">Bangti Jin</a>, 
<a href="/search/math?searchtype=author&query=Shin%2C+K">Kwancheol Shin</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Z">Zhi Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, appeared at Journal of Computational Physics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09516" title="Abstract">arXiv:2304.09516</a> (replaced) [<a href="/pdf/2304.09516" title="Download PDF">pdf</a>, <a href="/format/2304.09516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling keywords and their positions in text generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sasazawa%2C+Y">Yuichi Sasazawa</a>, 
<a href="/search/cs?searchtype=author&query=Morishita%2C+T">Terufumi Morishita</a>, 
<a href="/search/cs?searchtype=author&query=Ozaki%2C+H">Hiroaki Ozaki</a>, 
<a href="/search/cs?searchtype=author&query=Imaichi%2C+O">Osamu Imaichi</a>, 
<a href="/search/cs?searchtype=author&query=Sogawa%2C+Y">Yasuhiro Sogawa</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 16th International Natural Language Generation
  Conference, 2023, pages 407 to 413
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09842" title="Abstract">arXiv:2304.09842</a> (replaced) [<a href="/pdf/2304.09842" title="Download PDF">pdf</a>, <a href="/format/2304.09842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chameleon: Plug-and-Play Compositional Reasoning with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Baolin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Galley%2C+M">Michel Galley</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 10 figures, 24 tables. Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12479" title="Abstract">arXiv:2304.12479</a> (replaced) [<a href="/pdf/2304.12479" title="Download PDF">pdf</a>, <a href="/format/2304.12479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial General Intelligence (AGI) for Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Latif%2C+E">Ehsan Latif</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+G">Gengchen Mai</a>, 
<a href="/search/cs?searchtype=author&query=Nyaaba%2C+M">Matthew Nyaaba</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuansheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+G">Guoyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaoming Zhai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Review Paper on AGI for Education
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12579" title="Abstract">arXiv:2304.12579</a> (replaced) [<a href="/pdf/2304.12579" title="Download PDF">pdf</a>, <a href="/format/2304.12579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Trajectories are Generalization Indicators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jingwen Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dacheng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14990" title="Abstract">arXiv:2304.14990</a> (replaced) [<a href="/pdf/2304.14990" title="Download PDF">pdf</a>, <a href="/format/2304.14990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Stackelberg Equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+J">Jiarui Gan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+M">Minbiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jibang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00833" title="Abstract">arXiv:2305.00833</a> (replaced) [<a href="/pdf/2305.00833" title="Download PDF">pdf</a>, <a href="/format/2305.00833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Reason and Memorize with Self-Notes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanchantin%2C+J">Jack Lanchantin</a>, 
<a href="/search/cs?searchtype=author&query=Toshniwal%2C+S">Shubham Toshniwal</a>, 
<a href="/search/cs?searchtype=author&query=Weston%2C+J">Jason Weston</a>, 
<a href="/search/cs?searchtype=author&query=Szlam%2C+A">Arthur Szlam</a>, 
<a href="/search/cs?searchtype=author&query=Sukhbaatar%2C+S">Sainbayar Sukhbaatar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01210" title="Abstract">arXiv:2305.01210</a> (replaced) [<a href="/pdf/2305.01210" title="Download PDF">pdf</a>, <a href="/format/2305.01210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of  Large Language Models for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+C+S">Chunqiu Steven Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingming Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01625" title="Abstract">arXiv:2305.01625</a> (replaced) [<a href="/pdf/2305.01625" title="Download PDF">pdf</a>, <a href="/format/2305.01625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlimiformer: Long-Range Transformers with Unlimited Length Input
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertsch%2C+A">Amanda Bertsch</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+U">Uri Alon</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Gormley%2C+M+R">Matthew R. Gormley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02997" title="Abstract">arXiv:2305.02997</a> (replaced) [<a href="/pdf/2305.02997" title="Download PDF">pdf</a>, <a href="/format/2305.02997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Do Neural Nets Outperform Boosted Trees on Tabular Data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McElfresh%2C+D">Duncan McElfresh</a>, 
<a href="/search/cs?searchtype=author&query=Khandagale%2C+S">Sujay Khandagale</a>, 
<a href="/search/cs?searchtype=author&query=Valverde%2C+J">Jonathan Valverde</a>, 
<a href="/search/cs?searchtype=author&query=C%2C+V+P">Vishak Prasad C</a>, 
<a href="/search/cs?searchtype=author&query=Feuer%2C+B">Benjamin Feuer</a>, 
<a href="/search/cs?searchtype=author&query=Hegde%2C+C">Chinmay Hegde</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Goldblum%2C+M">Micah Goldblum</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+C">Colin White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS Datasets and Benchmarks Track 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03129" title="Abstract">arXiv:2305.03129</a> (replaced) [<a href="/pdf/2305.03129" title="Download PDF">pdf</a>, <a href="/format/2305.03129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Programming by Demonstration for Long-Horizon Robot Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patton%2C+N">Noah Patton</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+K">Kia Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Missula%2C+M">Meghana Missula</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Dillig%2C+I">I&#x15f;il Dillig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 Pages, Extended Version of POPL 2024 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03447" title="Abstract">arXiv:2305.03447</a> (replaced) [<a href="/pdf/2305.03447" title="Download PDF">pdf</a>, <a href="/format/2305.03447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regular Methods for Operator Precedence Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henzinger%2C+T+A">Thomas A. Henzinger</a>, 
<a href="/search/cs?searchtype=author&query=Kebis%2C+P">Pavol Kebis</a>, 
<a href="/search/cs?searchtype=author&query=Mazzocchi%2C+N">Nicolas Mazzocchi</a>, 
<a href="/search/cs?searchtype=author&query=Sara%C3%A7%2C+N+E">N. Ege Sara&#xe7;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version of the paper to appear in ICALP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03971" title="Abstract">arXiv:2305.03971</a> (replaced) [<a href="/pdf/2305.03971" title="Download PDF">pdf</a>, <a href="/format/2305.03971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive loose optimization for robust question answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jie Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zewei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Dechen Kong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Min Hu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Ting Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages,8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05276" title="Abstract">arXiv:2305.05276</a> (replaced) [<a href="/pdf/2305.05276" title="Download PDF">pdf</a>, <a href="/format/2305.05276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Discovery from Subsampled Time Series with Proxy Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingzhou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xinwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lingjing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06563" title="Abstract">arXiv:2305.06563</a> (replaced) [<a href="/pdf/2305.06563" title="Download PDF">pdf</a>, <a href="/format/2305.06563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatiotemporal Regularized Tucker Decomposition Approach for Traffic  Data Imputation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gong%2C+W">Wenwu Gong</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+Z">Zhejun Huang</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+L">Lili Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06773" title="Abstract">arXiv:2305.06773</a> (replaced) [<a href="/pdf/2305.06773" title="Download PDF">pdf</a>, <a href="/format/2305.06773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Better Understanding of the Computer Vision Research Community  in Africa
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Omotayo%2C+A">Abdul-Hakeem Omotayo</a>, 
<a href="/search/cs?searchtype=author&query=Gamal%2C+M">Mai Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Ehab%2C+E">Eman Ehab</a>, 
<a href="/search/cs?searchtype=author&query=Dovonon%2C+G">Gbetondji Dovonon</a>, 
<a href="/search/cs?searchtype=author&query=Akinjobi%2C+Z">Zainab Akinjobi</a>, 
<a href="/search/cs?searchtype=author&query=Lukman%2C+I">Ismaila Lukman</a>, 
<a href="/search/cs?searchtype=author&query=Turki%2C+H">Houcemeddine Turki</a>, 
<a href="/search/cs?searchtype=author&query=Abdien%2C+M">Mahmod Abdien</a>, 
<a href="/search/cs?searchtype=author&query=Tondji%2C+I">Idriss Tondji</a>, 
<a href="/search/cs?searchtype=author&query=Oppong%2C+A">Abigail Oppong</a>, 
<a href="/search/cs?searchtype=author&query=Pimi%2C+Y">Yvan Pimi</a>, 
<a href="/search/cs?searchtype=author&query=Gamal%2C+K">Karim Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Ro%27ya-CV4Africa">Ro&#x27;ya-CV4Africa</a>, 
<a href="/search/cs?searchtype=author&query=Siam%2C+M">Mennatullah Siam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in EAAMO'23 under ACM License. This work is part of our African computer vision grassroots research in Ro'ya - CV4Africa, <a href="https://ro-ya-cv4africa.github.io/homepage/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06986" title="Abstract">arXiv:2305.06986</a> (replaced) [<a href="/pdf/2305.06986" title="Download PDF">pdf</a>, <a href="/format/2305.06986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nichani%2C+E">Eshaan Nichani</a>, 
<a href="/search/cs?searchtype=author&query=Damian%2C+A">Alex Damian</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: NeurIPS 2023 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09181" title="Abstract">arXiv:2305.09181</a> (replaced) [<a href="/pdf/2305.09181" title="Download PDF">pdf</a>, <a href="/format/2305.09181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Push-LSVRG-UP: Distributed Stochastic Optimization over Unbalanced  Directed Networks with Uncoordinated Triggered Probabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+J">Jinhui Hu</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+G">Guo Chen</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+H">Huaqing Li</a>, 
<a href="/search/math?searchtype=author&query=Shen%2C+Z">Zixiang Shen</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+W">Weidong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 30 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING, VOL. 10, NO.
  2, 2023, PP. 934-950
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11469" title="Abstract">arXiv:2305.11469</a> (replaced) [<a href="/pdf/2305.11469" title="Download PDF">pdf</a>, <a href="/format/2305.11469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Barzilai-Borwein Method for Distributed Optimization over Unbalanced  Directed Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+J">Jinhui Hu</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+L">Lifeng Zheng</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+L">Ling Zhang</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+H">Huaqing Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 8 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Engineering Applications of Artificial Intelligence 99 (2021)
  104151
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11554" title="Abstract">arXiv:2305.11554</a> (replaced) [<a href="/pdf/2305.11554" title="Download PDF">pdf</a>, <a href="/format/2305.11554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via  Tool Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+S">Shibo Hao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiting Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (oral). Code: <a href="https://github.com/Ber666/ToolkenGPT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12162" title="Abstract">arXiv:2305.12162</a> (replaced) [<a href="/pdf/2305.12162" title="Download PDF">pdf</a>, <a href="/format/2305.12162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scalable Neural Network for DSIC Affine Maximizer Auction Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+Z">Zhijian Duan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haoran Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yurong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xiaotie Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13084" title="Abstract">arXiv:2305.13084</a> (replaced) [<a href="/pdf/2305.13084" title="Download PDF">pdf</a>, <a href="/ps/2305.13084" title="Download PostScript">ps</a>, <a href="/format/2305.13084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fractional Graph Laplacian Approach to Oversmoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maskey%2C+S">Sohir Maskey</a>, 
<a href="/search/cs?searchtype=author&query=Paolino%2C+R">Raffaele Paolino</a>, 
<a href="/search/cs?searchtype=author&query=Bacho%2C+A">Aras Bacho</a>, 
<a href="/search/cs?searchtype=author&query=Kutyniok%2C+G">Gitta Kutyniok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally. 37 pages, 8 images
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13786" title="Abstract">arXiv:2305.13786</a> (replaced) [<a href="/pdf/2305.13786" title="Download PDF">pdf</a>, <a href="/format/2305.13786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perception Test: A Diagnostic Benchmark for Multimodal Video Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C4%83tr%C4%83ucean%2C+V">Viorica P&#x103;tr&#x103;ucean</a>, 
<a href="/search/cs?searchtype=author&query=Smaira%2C+L">Lucas Smaira</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Ankush Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Continente%2C+A+R">Adri&#xe0; Recasens Continente</a>, 
<a href="/search/cs?searchtype=author&query=Markeeva%2C+L">Larisa Markeeva</a>, 
<a href="/search/cs?searchtype=author&query=Banarse%2C+D">Dylan Banarse</a>, 
<a href="/search/cs?searchtype=author&query=Koppula%2C+S">Skanda Koppula</a>, 
<a href="/search/cs?searchtype=author&query=Heyward%2C+J">Joseph Heyward</a>, 
<a href="/search/cs?searchtype=author&query=Malinowski%2C+M">Mateusz Malinowski</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Doersch%2C+C">Carl Doersch</a>, 
<a href="/search/cs?searchtype=author&query=Matejovicova%2C+T">Tatiana Matejovicova</a>, 
<a href="/search/cs?searchtype=author&query=Sulsky%2C+Y">Yury Sulsky</a>, 
<a href="/search/cs?searchtype=author&query=Miech%2C+A">Antoine Miech</a>, 
<a href="/search/cs?searchtype=author&query=Frechette%2C+A">Alex Frechette</a>, 
<a href="/search/cs?searchtype=author&query=Klimczak%2C+H">Hanna Klimczak</a>, 
<a href="/search/cs?searchtype=author&query=Koster%2C+R">Raphael Koster</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junlin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Winkler%2C+S">Stephanie Winkler</a>, 
<a href="/search/cs?searchtype=author&query=Aytar%2C+Y">Yusuf Aytar</a>, 
<a href="/search/cs?searchtype=author&query=Osindero%2C+S">Simon Osindero</a>, 
<a href="/search/cs?searchtype=author&query=Damen%2C+D">Dima Damen</a>, 
<a href="/search/cs?searchtype=author&query=Zisserman%2C+A">Andrew Zisserman</a>, 
<a href="/search/cs?searchtype=author&query=Carreira%2C+J">Jo&#xe3;o Carreira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13971" title="Abstract">arXiv:2305.13971</a> (replaced) [<a href="/pdf/2305.13971" title="Download PDF">pdf</a>, <a href="/format/2305.13971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+S">Saibo Geng</a>, 
<a href="/search/cs?searchtype=author&query=Josifoski%2C+M">Martin Josifoski</a>, 
<a href="/search/cs?searchtype=author&query=Peyrard%2C+M">Maxime Peyrard</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14083" title="Abstract">arXiv:2305.14083</a> (replaced) [<a href="/pdf/2305.14083" title="Download PDF">pdf</a>, <a href="/format/2305.14083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Augmentation for Multimodal Learning Under Presentation  Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+V">Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Morency%2C+L">Louis-Philippe Morency</a>, 
<a href="/search/cs?searchtype=author&query=Dimitriadis%2C+D">Dimitrios Dimitriadis</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Srinagesh Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14457" title="Abstract">arXiv:2305.14457</a> (replaced) [<a href="/pdf/2305.14457" title="Download PDF">pdf</a>, <a href="/format/2305.14457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training Language Models for Comparative Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mengxia Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 - Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14535" title="Abstract">arXiv:2305.14535</a> (replaced) [<a href="/pdf/2305.14535" title="Download PDF">pdf</a>, <a href="/format/2305.14535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Quantification over Graph with Conformalized Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kexin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Ying Jin</a>, 
<a href="/search/cs?searchtype=author&query=Cand%C3%A8s%2C+E">Emmanuel Cand&#xe8;s</a>, 
<a href="/search/cs?searchtype=author&query=Leskovec%2C+J">Jure Leskovec</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14627" title="Abstract">arXiv:2305.14627</a> (replaced) [<a href="/pdf/2305.14627" title="Download PDF">pdf</a>, <a href="/format/2305.14627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Large Language Models to Generate Text with Citations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tianyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yen%2C+H">Howard Yen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiatong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023. Code and data are available at <a href="https://github.com/princeton-nlp/ALCE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14800" title="Abstract">arXiv:2305.14800</a> (replaced) [<a href="/pdf/2305.14800" title="Download PDF">pdf</a>, <a href="/format/2305.14800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Diverse In-Context Configurations for Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongliang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingzhuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haokun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xin Geng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14928" title="Abstract">arXiv:2305.14928</a> (replaced) [<a href="/pdf/2305.14928" title="Download PDF">pdf</a>, <a href="/format/2305.14928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Reliable Misinformation Mitigation: Generalization, Uncertainty,  and GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pelrine%2C+K">Kellin Pelrine</a>, 
<a href="/search/cs?searchtype=author&query=Imouza%2C+A">Anne Imouza</a>, 
<a href="/search/cs?searchtype=author&query=Thibault%2C+C">Camille Thibault</a>, 
<a href="/search/cs?searchtype=author&query=Reksoprodjo%2C+M">Meilina Reksoprodjo</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+C">Caleb Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Christoph%2C+J">Joel Christoph</a>, 
<a href="/search/cs?searchtype=author&query=Godbout%2C+J">Jean-Fran&#xe7;ois Godbout</a>, 
<a href="/search/cs?searchtype=author&query=Rabbany%2C+R">Reihaneh Rabbany</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14947" title="Abstract">arXiv:2305.14947</a> (replaced) [<a href="/pdf/2305.14947" title="Download PDF">pdf</a>, <a href="/format/2305.14947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Predictable Are Large Language Model Capabilities? A Case Study on  BIG-bench
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinyuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H+Y">Harvey Yiyun Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings. Camera-ready version. Code: <a href="https://github.com/INK-USC/predicting-big-bench">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14987" title="Abstract">arXiv:2305.14987</a> (replaced) [<a href="/pdf/2305.14987" title="Download PDF">pdf</a>, <a href="/format/2305.14987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Table-to-Text Generation Capabilities of LLMs in  Real-World Information Seeking Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Shengyun Si</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+L">Linyong Nan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready version for EMNLP 2023 industry track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15249" title="Abstract">arXiv:2305.15249</a> (replaced) [<a href="/pdf/2305.15249" title="Download PDF">pdf</a>, <a href="/format/2305.15249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision-Aware Actor-Critic with Function Approximation and Theoretical  Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaswani%2C+S">Sharan Vaswani</a>, 
<a href="/search/cs?searchtype=author&query=Kazemi%2C+A">Amirreza Kazemi</a>, 
<a href="/search/cs?searchtype=author&query=Babanezhad%2C+R">Reza Babanezhad</a>, 
<a href="/search/cs?searchtype=author&query=Roux%2C+N+L">Nicolas Le Roux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15284" title="Abstract">arXiv:2305.15284</a> (replaced) [<a href="/pdf/2305.15284" title="Download PDF">pdf</a>, <a href="/format/2305.15284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replicable Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eaton%2C+E">Eric Eaton</a>, 
<a href="/search/cs?searchtype=author&query=Hussing%2C+M">Marcel Hussing</a>, 
<a href="/search/cs?searchtype=author&query=Kearns%2C+M">Michael Kearns</a>, 
<a href="/search/cs?searchtype=author&query=Sorrell%2C+J">Jessica Sorrell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15618" title="Abstract">arXiv:2305.15618</a> (replaced) [<a href="/pdf/2305.15618" title="Download PDF">pdf</a>, <a href="/format/2305.15618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debias Coarsely, Sample Conditionally: Statistical Downscaling through  Optimal Transport and Probabilistic Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z+Y">Zhong Yi Wan</a>, 
<a href="/search/cs?searchtype=author&query=Baptista%2C+R">Ricardo Baptista</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yi-fan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+J">John Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Boral%2C+A">Anudhyan Boral</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+F">Fei Sha</a>, 
<a href="/search/cs?searchtype=author&query=Zepeda-N%C3%BA%C3%B1ez%2C+L">Leonardo Zepeda-N&#xfa;&#xf1;ez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applied Physics (physics.app-ph)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16381" title="Abstract">arXiv:2305.16381</a> (replaced) [<a href="/pdf/2305.16381" title="Download PDF">pdf</a>, <a href="/format/2305.16381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Ying Fan</a>, 
<a href="/search/cs?searchtype=author&query=Watkins%2C+O">Olivia Watkins</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuqing Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+M">Moonkyung Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Boutilier%2C+C">Craig Boutilier</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Ghavamzadeh%2C+M">Mohammad Ghavamzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kangwook Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kimin Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16397" title="Abstract">arXiv:2305.16397</a> (replaced) [<a href="/pdf/2305.16397" title="Download PDF">pdf</a>, <a href="/format/2305.16397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Diffusion Models Vision-And-Language Reasoners?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krojer%2C+B">Benno Krojer</a>, 
<a href="/search/cs?searchtype=author&query=Poole-Dayan%2C+E">Elinor Poole-Dayan</a>, 
<a href="/search/cs?searchtype=author&query=Voleti%2C+V">Vikram Voleti</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+C">Christopher Pal</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Siva Reddy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17333" title="Abstract">arXiv:2305.17333</a> (replaced) [<a href="/pdf/2305.17333" title="Download PDF">pdf</a>, <a href="/format/2305.17333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Tuning Language Models with Just Forward Passes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malladi%2C+S">Sadhika Malladi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tianyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Nichani%2C+E">Eshaan Nichani</a>, 
<a href="/search/cs?searchtype=author&query=Damian%2C+A">Alex Damian</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sanjeev Arora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 (oral). Code available at <a href="https://github.com/princeton-nlp/MeZO">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17422" title="Abstract">arXiv:2305.17422</a> (replaced) [<a href="/pdf/2305.17422" title="Download PDF">pdf</a>, <a href="/format/2305.17422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Emotion Valence is a Joint Deep Learning Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roccabruna%2C+G">Gabriel Roccabruna</a>, 
<a href="/search/cs?searchtype=author&query=Mousavi%2C+S+M">Seyed Mahed Mousavi</a>, 
<a href="/search/cs?searchtype=author&query=Riccardi%2C+G">Giuseppe Riccardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18654" title="Abstract">arXiv:2305.18654</a> (replaced) [<a href="/pdf/2305.18654" title="Download PDF">pdf</a>, <a href="/format/2305.18654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faith and Fate: Limits of Transformers on Compositionality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dziri%2C+N">Nouha Dziri</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Ximing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Sclar%2C+M">Melanie Sclar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X+L">Xiang Lorraine Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B+Y">Bill Yuchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+P">Peter West</a>, 
<a href="/search/cs?searchtype=author&query=Bhagavatula%2C+C">Chandra Bhagavatula</a>, 
<a href="/search/cs?searchtype=author&query=Bras%2C+R+L">Ronan Le Bras</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J+D">Jena D. Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+S">Soumya Sanyal</a>, 
<a href="/search/cs?searchtype=author&query=Welleck%2C+S">Sean Welleck</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Ettinger%2C+A">Allyson Ettinger</a>, 
<a href="/search/cs?searchtype=author&query=Harchaoui%2C+Z">Zaid Harchaoui</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages + appendix (40 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19065" title="Abstract">arXiv:2305.19065</a> (replaced) [<a href="/pdf/2305.19065" title="Download PDF">pdf</a>, <a href="/format/2305.19065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Template-free Articulated Neural Point Clouds for Reposable View  Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uzolas%2C+L">Lukas Uzolas</a>, 
<a href="/search/cs?searchtype=author&query=Eisemann%2C+E">Elmar Eisemann</a>, 
<a href="/search/cs?searchtype=author&query=Kellnhofer%2C+P">Petr Kellnhofer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19765" title="Abstract">arXiv:2305.19765</a> (replaced) [<a href="/pdf/2305.19765" title="Download PDF">pdf</a>, <a href="/format/2305.19765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bayesian Approach To Analysing Training Data Attribution In Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+E">Elisa Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S+J">Seong Joon Oh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01128" title="Abstract">arXiv:2306.01128</a> (replaced) [<a href="/pdf/2306.01128" title="Download PDF">pdf</a>, <a href="/format/2306.01128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Transformer Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Friedman%2C+D">Dan Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Wettig%2C+A">Alexander Wettig</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (oral). Our code is available at <a href="https://github.com/princeton-nlp/TransformerPrograms">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01257" title="Abstract">arXiv:2306.01257</a> (replaced) [<a href="/pdf/2306.01257" title="Download PDF">pdf</a>, <a href="/format/2306.01257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collect-and-Distribute Transformer for 3D Point Cloud Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Haibo Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Baosheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/haibo-qiu/CDFormer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01525" title="Abstract">arXiv:2306.01525</a> (replaced) [<a href="/pdf/2306.01525" title="Download PDF">pdf</a>, <a href="/format/2306.01525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Study of the convergence of the Meshless Lattice Boltzmann Method in  Taylor-Green and annular channel flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Strzelczyk%2C+D">Dawid Strzelczyk</a>, 
<a href="/search/physics?searchtype=author&query=Matyka%2C+M">Maciej Matyka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Replacement due to the license change, no other changes made
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01667" title="Abstract">arXiv:2306.01667</a> (replaced) [<a href="/pdf/2306.01667" title="Download PDF">pdf</a>, <a href="/format/2306.01667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards In-context Scene Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bala%C5%BEevi%C4%87%2C+I">Ivana Bala&#x17e;evi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Steiner%2C+D">David Steiner</a>, 
<a href="/search/cs?searchtype=author&query=Parthasarathy%2C+N">Nikhil Parthasarathy</a>, 
<a href="/search/cs?searchtype=author&query=Arandjelovi%C4%87%2C+R">Relja Arandjelovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A9naff%2C+O+J">Olivier J. H&#xe9;naff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01930" title="Abstract">arXiv:2306.01930</a> (replaced) [<a href="/pdf/2306.01930" title="Download PDF">pdf</a>, <a href="/format/2306.01930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Similarities Between Language Models and Neural Response  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaang Li</a>, 
<a href="/search/cs?searchtype=author&query=Karamolegkou%2C+A">Antonia Karamolegkou</a>, 
<a href="/search/cs?searchtype=author&query=Kementchedjhieva%2C+Y">Yova Kementchedjhieva</a>, 
<a href="/search/cs?searchtype=author&query=Abdou%2C+M">Mostafa Abdou</a>, 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+S">Sune Lehmann</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%B8gaard%2C+A">Anders S&#xf8;gaard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurReps@NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01989" title="Abstract">arXiv:2306.01989</a> (replaced) [<a href="/pdf/2306.01989" title="Download PDF">pdf</a>, <a href="/format/2306.01989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Vectorization Implementation of CRYSTALS-Dilithium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jieyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haoliang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhenyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunlei Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02066" title="Abstract">arXiv:2306.02066</a> (replaced) [<a href="/pdf/2306.02066" title="Download PDF">pdf</a>, <a href="/format/2306.02066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Gaussian Process Diffusion Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Prakhar Verma</a>, 
<a href="/search/cs?searchtype=author&query=Adam%2C+V">Vincent Adam</a>, 
<a href="/search/cs?searchtype=author&query=Solin%2C+A">Arno Solin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04125" title="Abstract">arXiv:2306.04125</a> (replaced) [<a href="/pdf/2306.04125" title="Download PDF">pdf</a>, <a href="/format/2306.04125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Fusion Interactions: A Study of Human and Automatic  Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+P+P">Paul Pu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Morency%2C+L">Louis-Philippe Morency</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Multimodal Interaction (ICMI '23), Code available at: <a href="https://github.com/pliang279/PID.">this https URL</a> arXiv admin note: text overlap with <a href="/abs/2302.12247">arXiv:2302.12247</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04147" title="Abstract">arXiv:2306.04147</a> (replaced) [<a href="/pdf/2306.04147" title="Download PDF">pdf</a>, <a href="/format/2306.04147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFDP: Common Frequency Domain Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khaki%2C+S">Samir Khaki</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weihan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR ECV 2023 Accepted Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04377" title="Abstract">arXiv:2306.04377</a> (replaced) [<a href="/pdf/2306.04377" title="Download PDF">pdf</a>, <a href="/format/2306.04377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Get More for Less in Decentralized Learning Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhasade%2C+A">Akash Dhasade</a>, 
<a href="/search/cs?searchtype=author&query=Kermarrec%2C+A">Anne-Marie Kermarrec</a>, 
<a href="/search/cs?searchtype=author&query=Pires%2C+R">Rafael Pires</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rishi Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Vujasinovic%2C+M">Milos Vujasinovic</a>, 
<a href="/search/cs?searchtype=author&query=Wigger%2C+J">Jeffrey Wigger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE 43rd International Conference on Distributed Computing
  Systems (ICDCS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04583" title="Abstract">arXiv:2306.04583</a> (replaced) [<a href="/pdf/2306.04583" title="Download PDF">pdf</a>, <a href="/ps/2306.04583" title="Download PostScript">ps</a>, <a href="/format/2306.04583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\varepsilon$-Almost collision-flat universal hash functions and mosaics  of designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wiese%2C+M">Moritz Wiese</a>, 
<a href="/search/math?searchtype=author&query=Boche%2C+H">Holger Boche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04675" title="Abstract">arXiv:2306.04675</a> (replaced) [<a href="/pdf/2306.04675" title="Download PDF">pdf</a>, <a href="/format/2306.04675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exposing flaws of generative model evaluation metrics and their unfair  treatment of diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stein%2C+G">George Stein</a>, 
<a href="/search/cs?searchtype=author&query=Cresswell%2C+J+C">Jesse C. Cresswell</a>, 
<a href="/search/cs?searchtype=author&query=Hosseinzadeh%2C+R">Rasa Hosseinzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yi Sui</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+B+L">Brendan Leigh Ross</a>, 
<a href="/search/cs?searchtype=author&query=Villecroze%2C+V">Valentin Villecroze</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaoyan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Caterini%2C+A+L">Anthony L. Caterini</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+J+E+T">J. Eric T. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Loaiza-Ganem%2C+G">Gabriel Loaiza-Ganem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. 53 pages, 29 figures, 12 tables. Code at <a href="https://github.com/layer6ai-labs/dgm-eval">this https URL</a>, reviews at <a href="https://openreview.net/forum?id=08zf7kTOoh">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04746" title="Abstract">arXiv:2306.04746</a> (replaced) [<a href="/pdf/2306.04746" title="Download PDF">pdf</a>, <a href="/format/2306.04746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Imperfect Surrogates for Downstream Inference: Design-based  Supervised Learning for Social Science Applications of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Egami%2C+N">Naoki Egami</a>, 
<a href="/search/stat?searchtype=author&query=Hinck%2C+M">Musashi Hinck</a>, 
<a href="/search/stat?searchtype=author&query=Stewart%2C+B+M">Brandon M. Stewart</a>, 
<a href="/search/stat?searchtype=author&query=Wei%2C+H">Hanying Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04751" title="Abstract">arXiv:2306.04751</a> (replaced) [<a href="/pdf/2306.04751" title="Download PDF">pdf</a>, <a href="/format/2306.04751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Far Can Camels Go? Exploring the State of Instruction Tuning on Open  Resources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ivison%2C+H">Hamish Ivison</a>, 
<a href="/search/cs?searchtype=author&query=Dasigi%2C+P">Pradeep Dasigi</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Khot%2C+T">Tushar Khot</a>, 
<a href="/search/cs?searchtype=author&query=Chandu%2C+K+R">Khyathi Raghavi Chandu</a>, 
<a href="/search/cs?searchtype=author&query=Wadden%2C+D">David Wadden</a>, 
<a href="/search/cs?searchtype=author&query=MacMillan%2C+K">Kelsey MacMillan</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Beltagy%2C+I">Iz Beltagy</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figure, 10 tables. NeurIPS 2023 Datasets and Benchmarks Track Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05216" title="Abstract">arXiv:2306.05216</a> (replaced) [<a href="/pdf/2306.05216" title="Download PDF">pdf</a>, <a href="/ps/2306.05216" title="Download PostScript">ps</a>, <a href="/format/2306.05216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Optimal Equilibria and Mechanisms via Learning in Zero-Sum  Extensive-Form Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostides%2C+I">Ioannis Anagnostides</a>, 
<a href="/search/cs?searchtype=author&query=Cacciamani%2C+F">Federico Cacciamani</a>, 
<a href="/search/cs?searchtype=author&query=McAleer%2C+S+M">Stephen Marcus McAleer</a>, 
<a href="/search/cs?searchtype=author&query=Haupt%2C+A+A">Andreas Alexander Haupt</a>, 
<a href="/search/cs?searchtype=author&query=Celli%2C+A">Andrea Celli</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Conitzer%2C+V">Vincent Conitzer</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05523" title="Abstract">arXiv:2306.05523</a> (replaced) [<a href="/pdf/2306.05523" title="Download PDF">pdf</a>, <a href="/format/2306.05523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FACTIFY3M: A Benchmark for Multimodal Fact Verification with  Explainability through 5W Question-Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+M">Megha Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Pahwa%2C+K">Khushbu Pahwa</a>, 
<a href="/search/cs?searchtype=author&query=Rani%2C+A">Anku Rani</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S">Shreyas Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Dalal%2C+D">Dwip Dalal</a>, 
<a href="/search/cs?searchtype=author&query=Dave%2C+H">Harshit Dave</a>, 
<a href="/search/cs?searchtype=author&query=G%2C+R">Ritvik G</a>, 
<a href="/search/cs?searchtype=author&query=Gurumurthy%2C+P">Preethi Gurumurthy</a>, 
<a href="/search/cs?searchtype=author&query=Mahor%2C+A">Adarsh Mahor</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Samahriti Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Pakala%2C+A">Aditya Pakala</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+I">Ishan Paul</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+J">Janvita Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Arghya Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Sensharma%2C+K">Kinjal Sensharma</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A+P">Amit P. Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2305.04329">arXiv:2305.04329</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05584" title="Abstract">arXiv:2306.05584</a> (replaced) [<a href="/pdf/2306.05584" title="Download PDF">pdf</a>, <a href="/format/2306.05584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and  Motion Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+J">Jia-Xing Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T">Ta-Ying Cheng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuhang He</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Kai Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaichen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Markham%2C+A">Andrew Markham</a>, 
<a href="/search/cs?searchtype=author&query=Trigoni%2C+N">Niki Trigoni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05724" title="Abstract">arXiv:2306.05724</a> (replaced) [<a href="/pdf/2306.05724" title="Download PDF">pdf</a>, <a href="/format/2306.05724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Predictive Uncertainty with Information Theoretic Shapley  Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Watson%2C+D+S">David S. Watson</a>, 
<a href="/search/stat?searchtype=author&query=O%27Hara%2C+J">Joshua O&#x27;Hara</a>, 
<a href="/search/stat?searchtype=author&query=Tax%2C+N">Niek Tax</a>, 
<a href="/search/stat?searchtype=author&query=Mudd%2C+R">Richard Mudd</a>, 
<a href="/search/stat?searchtype=author&query=Guy%2C+I">Ido Guy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready version (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06076" title="Abstract">arXiv:2306.06076</a> (replaced) [<a href="/pdf/2306.06076" title="Download PDF">pdf</a>, <a href="/format/2306.06076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Image Classification by Learning Priors from  Random Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xinyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+A">Ashwinee Panda</a>, 
<a href="/search/cs?searchtype=author&query=Sehwag%2C+V">Vikash Sehwag</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+P">Prateek Mittal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06331" title="Abstract">arXiv:2306.06331</a> (replaced) [<a href="/pdf/2306.06331" title="Download PDF">pdf</a>, <a href="/format/2306.06331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and  Problem Solving: Evidence from the Vietnamese National High School Graduation  Examination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dao%2C+X">Xuan-Quy Dao</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+N">Ngoc-Bich Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 14 images
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06723" title="Abstract">arXiv:2306.06723</a> (replaced) [<a href="/pdf/2306.06723" title="Download PDF">pdf</a>, <a href="/ps/2306.06723" title="Download PostScript">ps</a>, <a href="/format/2306.06723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counting Distinct Elements in the Turnstile Model with Differential  Privacy under Continual Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Palak Jain</a>, 
<a href="/search/cs?searchtype=author&query=Kalemaj%2C+I">Iden Kalemaj</a>, 
<a href="/search/cs?searchtype=author&query=Raskhodnikova%2C+S">Sofya Raskhodnikova</a>, 
<a href="/search/cs?searchtype=author&query=Sivakumar%2C+S">Satchit Sivakumar</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+A">Adam Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06815" title="Abstract">arXiv:2306.06815</a> (replaced) [<a href="/pdf/2306.06815" title="Download PDF">pdf</a>, <a href="/format/2306.06815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jiaqi Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Mengxin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+T">Ting Hua</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yilin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yepeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Boloni%2C+L">Ladislau Boloni</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Q">Qian Lou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07462" title="Abstract">arXiv:2306.07462</a> (replaced) [<a href="/pdf/2306.07462" title="Download PDF">pdf</a>, <a href="/format/2306.07462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Robustness of Removal-Based Feature Attributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chris Lin</a>, 
<a href="/search/cs?searchtype=author&query=Covert%2C+I">Ian Covert</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Su-In Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07916" title="Abstract">arXiv:2306.07916</a> (replaced) [<a href="/pdf/2306.07916" title="Download PDF">pdf</a>, <a href="/format/2306.07916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification of Nonlinear Latent Hierarchical Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingjing Kong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Biwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+F">Feng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08687" title="Abstract">arXiv:2306.08687</a> (replaced) [<a href="/pdf/2306.08687" title="Download PDF">pdf</a>, <a href="/format/2306.08687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Norm-guided latent space exploration for text-to-image generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samuel%2C+D">Dvir Samuel</a>, 
<a href="/search/cs?searchtype=author&query=Ben-Ari%2C+R">Rami Ben-Ari</a>, 
<a href="/search/cs?searchtype=author&query=Darshan%2C+N">Nir Darshan</a>, 
<a href="/search/cs?searchtype=author&query=Maron%2C+H">Haggai Maron</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09306" title="Abstract">arXiv:2306.09306</a> (replaced) [<a href="/pdf/2306.09306" title="Download PDF">pdf</a>, <a href="/format/2306.09306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Propagating Knowledge Updates to LMs Through Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padmanabhan%2C+S">Shankar Padmanabhan</a>, 
<a href="/search/cs?searchtype=author&query=Onoe%2C+Y">Yasumasa Onoe</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M+J+Q">Michael J.Q. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Durrett%2C+G">Greg Durrett</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Eunsol Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09549" title="Abstract">arXiv:2306.09549</a> (replaced) [<a href="/pdf/2306.09549" title="Download PDF">pdf</a>, <a href="/format/2306.09549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+M">Meng Liu</a>, 
<a href="/search/physics?searchtype=author&query=Luo%2C+Y">Youzhi Luo</a>, 
<a href="/search/physics?searchtype=author&query=Strasser%2C+A">Alex Strasser</a>, 
<a href="/search/physics?searchtype=author&query=Qian%2C+X">Xiaofeng Qian</a>, 
<a href="/search/physics?searchtype=author&query=Qian%2C+X">Xiaoning Qian</a>, 
<a href="/search/physics?searchtype=author&query=Ji%2C+S">Shuiwang Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023, Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09739" title="Abstract">arXiv:2306.09739</a> (replaced) [<a href="/pdf/2306.09739" title="Download PDF">pdf</a>, <a href="/format/2306.09739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilized Neural Differential Equations for Learning Dynamics with  Explicit Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=White%2C+A">Alistair White</a>, 
<a href="/search/cs?searchtype=author&query=Kilbertus%2C+N">Niki Kilbertus</a>, 
<a href="/search/cs?searchtype=author&query=Gelbrecht%2C+M">Maximilian Gelbrecht</a>, 
<a href="/search/cs?searchtype=author&query=Boers%2C+N">Niklas Boers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures. Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10574" title="Abstract">arXiv:2306.10574</a> (replaced) [<a href="/pdf/2306.10574" title="Download PDF">pdf</a>, <a href="/format/2306.10574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-based Data Assimilation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rozet%2C+F">Fran&#xe7;ois Rozet</a>, 
<a href="/search/cs?searchtype=author&query=Louppe%2C+G">Gilles Louppe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10711" title="Abstract">arXiv:2306.10711</a> (replaced) [<a href="/pdf/2306.10711" title="Download PDF">pdf</a>, <a href="/format/2306.10711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PLASTIC: Improving Input and Label Plasticity for Sample Efficient  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hojoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hanseul Cho</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunseung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Gwak%2C+D">Daehoon Gwak</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joonkee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+C">Chulhee Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures, accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10759" title="Abstract">arXiv:2306.10759</a> (replaced) [<a href="/pdf/2306.10759" title="Download PDF">pdf</a>, <a href="/format/2306.10759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplifying and Empowering Transformers for Large-Graph Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qitian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wentao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+F">Fan Nie</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haitian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published at NeurIPS 2023, the codes are available at <a href="https://github.com/qitianwu/SGFormer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15969" title="Abstract">arXiv:2306.15969</a> (replaced) [<a href="/pdf/2306.15969" title="Download PDF">pdf</a>, <a href="/format/2306.15969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separable Physics-Informed Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Junwoo Cho</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+S">Seungtae Nam</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hyunmo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Seok-Bae Yun</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Youngjoon Hong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+E">Eunbyung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023 (28 pages, 13 figures). workshop paper: <a href="/abs/2211.08761">arXiv:2211.08761</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16803" title="Abstract">arXiv:2306.16803</a> (replaced) [<a href="/pdf/2306.16803" title="Download PDF">pdf</a>, <a href="/format/2306.16803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Would I have gotten that reward? Long-term credit assignment by  counterfactual contribution analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meulemans%2C+A">Alexander Meulemans</a>, 
<a href="/search/cs?searchtype=author&query=Schug%2C+S">Simon Schug</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+S">Seijin Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Daw%2C+N">Nathaniel Daw</a>, 
<a href="/search/cs?searchtype=author&query=Wayne%2C+G">Gregory Wayne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16879" title="Abstract">arXiv:2306.16879</a> (replaced) [<a href="/pdf/2306.16879" title="Download PDF">pdf</a>, <a href="/format/2306.16879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surgical Phase and Instrument Recognition: How to identify appropriate  Dataset Splits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kostiuchik%2C+G">Georgii Kostiuchik</a>, 
<a href="/search/cs?searchtype=author&query=Sharan%2C+L">Lalith Sharan</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+B">Benedikt Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+I">Ivo Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Preim%2C+B">Bernhard Preim</a>, 
<a href="/search/cs?searchtype=author&query=Engelhardt%2C+S">Sandy Engelhardt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IPCAI 2023 as long abstract; Submitted to IJCARS as original article; 21 pages, 10 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17256" title="Abstract">arXiv:2306.17256</a> (replaced) [<a href="/pdf/2306.17256" title="Download PDF">pdf</a>, <a href="/format/2306.17256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Personalized Cold-Start Recommendation with Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuansheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huachi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yucheng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wenlin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01159" title="Abstract">arXiv:2307.01159</a> (replaced) [<a href="/pdf/2307.01159" title="Download PDF">pdf</a>, <a href="/format/2307.01159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft Gripping: Specifying for Trustworthiness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abeywickrama%2C+D+B">Dhaminda B. Abeywickrama</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+N+H">Nguyen Hao Le</a>, 
<a href="/search/cs?searchtype=author&query=Chance%2C+G">Greg Chance</a>, 
<a href="/search/cs?searchtype=author&query=Winter%2C+P+D">Peter D. Winter</a>, 
<a href="/search/cs?searchtype=author&query=Manzini%2C+A">Arianna Manzini</a>, 
<a href="/search/cs?searchtype=author&query=Partridge%2C+A+J">Alix J. Partridge</a>, 
<a href="/search/cs?searchtype=author&query=Ives%2C+J">Jonathan Ives</a>, 
<a href="/search/cs?searchtype=author&query=Downer%2C+J">John Downer</a>, 
<a href="/search/cs?searchtype=author&query=Deacon%2C+G">Graham Deacon</a>, 
<a href="/search/cs?searchtype=author&query=Rossiter%2C+J">Jonathan Rossiter</a>, 
<a href="/search/cs?searchtype=author&query=Eder%2C+K">Kerstin Eder</a>, 
<a href="/search/cs?searchtype=author&query=Windsor%2C+S">Shane Windsor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated the Standards subsection of paper. 9 pages, 2 figures, 1 table, 34 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01187" title="Abstract">arXiv:2307.01187</a> (replaced) [<a href="/pdf/2307.01187" title="Download PDF">pdf</a>, <a href="/format/2307.01187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAMAug: Point Prompt Augmentation for Segment Anything Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Haixing Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+P">Peng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiaozheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dajiang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Quanzheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02770" title="Abstract">arXiv:2307.02770</a> (replaced) [<a href="/pdf/2307.02770" title="Download PDF">pdf</a>, <a href="/format/2307.02770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+T">TaeHo Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Myoung%2C+K">Kibeom Myoung</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Keon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaewoong Cho</a>, 
<a href="/search/cs?searchtype=author&query=No%2C+A">Albert No</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+E+K">Ernest K. Ryu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05916" title="Abstract">arXiv:2307.05916</a> (replaced) [<a href="/pdf/2307.05916" title="Download PDF">pdf</a>, <a href="/format/2307.05916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SwiFT: Swin 4D fMRI Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+P+Y">Peter Yongho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+J">Junbeom Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+S">Sunghwan Joo</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Sangyoon Bae</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Donggyu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+Y">Yoonho Jung</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Shinjae Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+J">Jiook Cha</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+T">Taesup Moon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07238" title="Abstract">arXiv:2307.07238</a> (replaced) [<a href="/pdf/2307.07238" title="Download PDF">pdf</a>, <a href="/format/2307.07238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Remarks on Parikh-recognizable omega-languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grobler%2C+M">Mario Grobler</a>, 
<a href="/search/cs?searchtype=author&query=Sabellek%2C+L">Leif Sabellek</a>, 
<a href="/search/cs?searchtype=author&query=Siebertz%2C+S">Sebastian Siebertz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2302.04087">arXiv:2302.04087</a>, <a href="/abs/2301.08969">arXiv:2301.08969</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08507" title="Abstract">arXiv:2307.08507</a> (replaced) [<a href="/pdf/2307.08507" title="Download PDF">pdf</a>, <a href="/format/2307.08507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Accurate Optimal Transport with Mirror Descent and  Conjugate Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kemertas%2C+M">Mete Kemertas</a>, 
<a href="/search/cs?searchtype=author&query=Jepson%2C+A+D">Allan D. Jepson</a>, 
<a href="/search/cs?searchtype=author&query=Farahmand%2C+A">Amir-massoud Farahmand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09009" title="Abstract">arXiv:2307.09009</a> (replaced) [<a href="/pdf/2307.09009" title="Download PDF">pdf</a>, <a href="/format/2307.09009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How is ChatGPT&#x27;s behavior changing over time?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lingjiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zaharia%2C+M">Matei Zaharia</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> add more evaluations on instruction following
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10278" title="Abstract">arXiv:2307.10278</a> (replaced) [<a href="/pdf/2307.10278" title="Download PDF">pdf</a>, <a href="/format/2307.10278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data  with Large Value Ranges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Braun%2C+D">Daniel Braun</a>, 
<a href="/search/cs?searchtype=author&query=Borgo%2C+R">Rita Borgo</a>, 
<a href="/search/cs?searchtype=author&query=Sondag%2C+M">Max Sondag</a>, 
<a href="/search/cs?searchtype=author&query=von+Landesberger%2C+T">Tatiana von Landesberger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint and Author Version of a Full Paper, accepted to the 2023 IEEE Visualization Conference (VIS)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Visualization and Computer Graphics (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12750" title="Abstract">arXiv:2307.12750</a> (replaced) [<a href="/pdf/2307.12750" title="Download PDF">pdf</a>, <a href="/format/2307.12750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DawnIK: Decentralized Collision-Aware Inverse Kinematics Solver for  Heterogeneous Multi-Arm Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marangoz%2C+S">Salih Marangoz</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+R">Rohit Menon</a>, 
<a href="/search/cs?searchtype=author&query=Dengler%2C+N">Nils Dengler</a>, 
<a href="/search/cs?searchtype=author&query=Bennewitz%2C+M">Maren Bennewitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Salih Marangoz and Rohit Menon have equal authorship. Publication to appear in IEEE RAS Intl Conference on Humanoid Robotics (Humanoids), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15039" title="Abstract">arXiv:2307.15039</a> (replaced) [<a href="/pdf/2307.15039" title="Download PDF">pdf</a>, <a href="/format/2307.15039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EyeO: Autocalibrating Gaze Output with Gaze Input
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saran%2C+A">Akanksha Saran</a>, 
<a href="/search/cs?searchtype=author&query=Alber%2C+J">Jacob Alber</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cyril Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Paradiso%2C+A">Ann Paradiso</a>, 
<a href="/search/cs?searchtype=author&query=Bragg%2C+D">Danielle Bragg</a>, 
<a href="/search/cs?searchtype=author&query=Langford%2C+J">John Langford</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00949" title="Abstract">arXiv:2308.00949</a> (replaced) [<a href="/pdf/2308.00949" title="Download PDF">pdf</a>, <a href="/format/2308.00949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic Instance Segmentation from Semantic Image Segmentation Masks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yuchen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yuhui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zechao Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Liyong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qiaolin Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages,5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01445" title="Abstract">arXiv:2308.01445</a> (replaced) [<a href="/pdf/2308.01445" title="Download PDF">pdf</a>, <a href="/format/2308.01445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A digital twin framework for civil engineering structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Torzoni%2C+M">Matteo Torzoni</a>, 
<a href="/search/math?searchtype=author&query=Tezzele%2C+M">Marco Tezzele</a>, 
<a href="/search/math?searchtype=author&query=Mariani%2C+S">Stefano Mariani</a>, 
<a href="/search/math?searchtype=author&query=Manzoni%2C+A">Andrea Manzoni</a>, 
<a href="/search/math?searchtype=author&query=Willcox%2C+K+E">Karen E. Willcox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02493" title="Abstract">arXiv:2308.02493</a> (replaced) [<a href="/pdf/2308.02493" title="Download PDF">pdf</a>, <a href="/format/2308.02493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Body Fat Estimation from Surface Meshes using Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mueller%2C+T+T">Tamara T. Mueller</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+S">Siyu Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Starck%2C+S">Sophie Starck</a>, 
<a href="/search/eess?searchtype=author&query=Jungmann%2C+F">Friederike Jungmann</a>, 
<a href="/search/eess?searchtype=author&query=Ziller%2C+A">Alexander Ziller</a>, 
<a href="/search/eess?searchtype=author&query=Aksoy%2C+O">Orhun Aksoy</a>, 
<a href="/search/eess?searchtype=author&query=Movchan%2C+D">Danylo Movchan</a>, 
<a href="/search/eess?searchtype=author&query=Braren%2C+R">Rickmer Braren</a>, 
<a href="/search/eess?searchtype=author&query=Kaissis%2C+G">Georgios Kaissis</a>, 
<a href="/search/eess?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05483" title="Abstract">arXiv:2308.05483</a> (replaced) [<a href="/pdf/2308.05483" title="Download PDF">pdf</a>, <a href="/format/2308.05483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality Diversity under Sparse Reward and Sparse Interaction:  Application to Grasping in Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huber%2C+J">J. Huber</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A9l%C3%A9non%2C+F">F. H&#xe9;l&#xe9;non</a>, 
<a href="/search/cs?searchtype=author&query=Coninx%2C+M">M. Coninx</a>, 
<a href="/search/cs?searchtype=author&query=Amar%2C+F+B">F. Ben Amar</a>, 
<a href="/search/cs?searchtype=author&query=Doncieux%2C+S">S. Doncieux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 17 figures. Draft version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06508" title="Abstract">arXiv:2308.06508</a> (replaced) [<a href="/pdf/2308.06508" title="Download PDF">pdf</a>, <a href="/ps/2308.06508" title="Download PostScript">ps</a>, <a href="/format/2308.06508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symplectic self-orthogonal and LCD codes from the Plotkin sum  construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shitao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06629" title="Abstract">arXiv:2308.06629</a> (replaced) [<a href="/pdf/2308.06629" title="Download PDF">pdf</a>, <a href="/format/2308.06629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal FIFO grouping in public transit networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steil%2C+P">Patrick Steil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 page, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08401" title="Abstract">arXiv:2308.08401</a> (replaced) [<a href="/pdf/2308.08401" title="Download PDF">pdf</a>, <a href="/format/2308.08401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Simplest Walking Robot: A bipedal robot with one actuator and two  rigid bodies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kyle%2C+J">James Kyle</a>, 
<a href="/search/cs?searchtype=author&query=Yim%2C+J+K">Justin K. Yim</a>, 
<a href="/search/cs?searchtype=author&query=Hart%2C+K">Kendall Hart</a>, 
<a href="/search/cs?searchtype=author&query=Bergbreiter%2C+S">Sarah Bergbreiter</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+A+M">Aaron M. Johnson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE-RAS International Conference on Humanoid Robots
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10364" title="Abstract">arXiv:2308.10364</a> (replaced) [<a href="/pdf/2308.10364" title="Download PDF">pdf</a>, <a href="/format/2308.10364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SE(3) Equivariant Augmented Coupling Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Midgley%2C+L+I">Laurence I. Midgley</a>, 
<a href="/search/cs?searchtype=author&query=Stimper%2C+V">Vincent Stimper</a>, 
<a href="/search/cs?searchtype=author&query=Antor%C3%A1n%2C+J">Javier Antor&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Mathieu%2C+E">Emile Mathieu</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11090" title="Abstract">arXiv:2308.11090</a> (replaced) [<a href="/pdf/2308.11090" title="Download PDF">pdf</a>, <a href="/format/2308.11090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness Explainability using Optimal Transport with Applications in  Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ratz%2C+P">Philipp Ratz</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+F">Fran&#xe7;ois Hu</a>, 
<a href="/search/cs?searchtype=author&query=Charpentier%2C+A">Arthur Charpentier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12896" title="Abstract">arXiv:2308.12896</a> (replaced) [<a href="/pdf/2308.12896" title="Download PDF">pdf</a>, <a href="/format/2308.12896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Document Page Classification: Design, Datasets, and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Landeghem%2C+J">Jordy Van Landeghem</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sanket Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Blaschko%2C+M+B">Matthew B. Blaschko</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, accepted at WACV 2024; camera-ready (paper id 1123)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13564" title="Abstract">arXiv:2308.13564</a> (replaced) [<a href="/pdf/2308.13564" title="Download PDF">pdf</a>, <a href="/format/2308.13564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SGMM: Stochastic Approximation to Generalized Method of Moments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Chen%2C+X">Xiaohong Chen</a>, 
<a href="/search/econ?searchtype=author&query=Lee%2C+S">Sokbae Lee</a>, 
<a href="/search/econ?searchtype=author&query=Liao%2C+Y">Yuan Liao</a>, 
<a href="/search/econ?searchtype=author&query=Seo%2C+M+H">Myung Hwan Seo</a>, 
<a href="/search/econ?searchtype=author&query=Shin%2C+Y">Youngki Shin</a>, 
<a href="/search/econ?searchtype=author&query=Song%2C+M">Myunghyun Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 4 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01859" title="Abstract">arXiv:2309.01859</a> (replaced) [<a href="/pdf/2309.01859" title="Download PDF">pdf</a>, <a href="/format/2309.01859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLLB-CLIP -- train performant multilingual image retrieval model on a  budget
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Visheratin%2C+A">Alexander Visheratin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01886" title="Abstract">arXiv:2309.01886</a> (replaced) [<a href="/pdf/2309.01886" title="Download PDF">pdf</a>, <a href="/format/2309.01886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstruction of Unstable Heavy Particles Using Deep  Symmetry-Preserving Attention Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ex?searchtype=author&query=Fenton%2C+M+J">Michael James Fenton</a>, 
<a href="/search/hep-ex?searchtype=author&query=Shmakov%2C+A">Alexander Shmakov</a>, 
<a href="/search/hep-ex?searchtype=author&query=Okawa%2C+H">Hideki Okawa</a>, 
<a href="/search/hep-ex?searchtype=author&query=Li%2C+Y">Yuji Li</a>, 
<a href="/search/hep-ex?searchtype=author&query=Hsiao%2C+K">Ko-Yang Hsiao</a>, 
<a href="/search/hep-ex?searchtype=author&query=Hsu%2C+S">Shih-Chieh Hsu</a>, 
<a href="/search/hep-ex?searchtype=author&query=Whiteson%2C+D">Daniel Whiteson</a>, 
<a href="/search/hep-ex?searchtype=author&query=Baldi%2C+P">Pierre Baldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Nature Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Experiment (hep-ex)</span>; Machine Learning (cs.LG); High Energy Physics - Phenomenology (hep-ph)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04272" title="Abstract">arXiv:2309.04272</a> (replaced) [<a href="/pdf/2309.04272" title="Download PDF">pdf</a>, <a href="/format/2309.04272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity  and Last-Iterate Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jiduan Wu</a>, 
<a href="/search/eess?searchtype=author&query=Barakat%2C+A">Anas Barakat</a>, 
<a href="/search/eess?searchtype=author&query=Fatkhullin%2C+I">Ilyas Fatkhullin</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04382" title="Abstract">arXiv:2309.04382</a> (replaced) [<a href="/pdf/2309.04382" title="Download PDF">pdf</a>, <a href="/format/2309.04382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergent learning in physical systems as feedback-based aging in a  glassy landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Anisetti%2C+V+R">Vidyesh Rao Anisetti</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kandala%2C+A">Ananth Kandala</a>, 
<a href="/search/cond-mat?searchtype=author&query=Schwarz%2C+J+M">J. M. Schwarz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05961" title="Abstract">arXiv:2309.05961</a> (replaced) [<a href="/pdf/2309.05961" title="Download PDF">pdf</a>, <a href="/format/2309.05961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering  Trends across Diverse Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hazra%2C+R">Rima Hazra</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Agnik Saha</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Somnath Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06591" title="Abstract">arXiv:2309.06591</a> (replaced) [<a href="/pdf/2309.06591" title="Download PDF">pdf</a>, <a href="/format/2309.06591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homothetic tube model predictive control with multi-step predictors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Saccani%2C+D">Danilo Saccani</a>, 
<a href="/search/math?searchtype=author&query=Ferrari-Trecate%2C+G">Giancarlo Ferrari-Trecate</a>, 
<a href="/search/math?searchtype=author&query=Zeilinger%2C+M+N">Melanie N. Zeilinger</a>, 
<a href="/search/math?searchtype=author&query=K%C3%B6hler%2C+J">Johannes K&#xf6;hler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07011" title="Abstract">arXiv:2309.07011</a> (replaced) [<a href="/pdf/2309.07011" title="Download PDF">pdf</a>, <a href="/format/2309.07011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the k/log k Barrier in Collective Tree Exploration via  Tree-Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cosson%2C+R">Romain Cosson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07084" title="Abstract">arXiv:2309.07084</a> (replaced) [<a href="/pdf/2309.07084" title="Download PDF">pdf</a>, <a href="/format/2309.07084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yiran Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoqun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Z">Zijian Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+N">Ningning Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruimao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08276" title="Abstract">arXiv:2309.08276</a> (replaced) [<a href="/e-print/2309.08276" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Adaptive Phase-locked Loop for Synchronization of a Grid-Connected  Voltage Source Converter: Simulation and Experimental Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=He%2C+W">Wei He</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+J">Jiachen Yan</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+R">Romeo Ortega</a>, 
<a href="/search/eess?searchtype=author&query=Zonetti%2C+D">Daniele Zonetti</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+W">Wangping Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Something needs to be modified so that this paper is more clear
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10902" title="Abstract">arXiv:2309.10902</a> (replaced) [<a href="/pdf/2309.10902" title="Download PDF">pdf</a>, <a href="/format/2309.10902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VALID: A perceptually validated Virtual Avatar Library for Inclusion and  Diversity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+T+D">Tiffany D. Do</a>, 
<a href="/search/cs?searchtype=author&query=Zelenty%2C+S">Steve Zelenty</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez-Franco%2C+M">Mar Gonzalez-Franco</a>, 
<a href="/search/cs?searchtype=author&query=McMahan%2C+R+P">Ryan P. McMahan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12488" title="Abstract">arXiv:2309.12488</a> (replaced) [<a href="/pdf/2309.12488" title="Download PDF">pdf</a>, <a href="/format/2309.12488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharpness-Aware Minimization and the Edge of Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+P+M">Philip M. Long</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+P+L">Peter L. Bartlett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13135" title="Abstract">arXiv:2309.13135</a> (replaced) [<a href="/pdf/2309.13135" title="Download PDF">pdf</a>, <a href="/format/2309.13135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting Response to Treatment with Global Deep Learning and  Patient-Specific Pharmacokinetic Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Potosnak%2C+W">Willa Potosnak</a>, 
<a href="/search/cs?searchtype=author&query=Challu%2C+C">Cristian Challu</a>, 
<a href="/search/cs?searchtype=author&query=Olivares%2C+K+G">Kin G. Olivares</a>, 
<a href="/search/cs?searchtype=author&query=Dubrawski%2C+A">Artur Dubrawski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13391" title="Abstract">arXiv:2309.13391</a> (replaced) [<a href="/pdf/2309.13391" title="Download PDF">pdf</a>, <a href="/format/2309.13391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D-Separation for Causal Self-Explanation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haozhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhiying Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">YuanKai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yang Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14356" title="Abstract">arXiv:2309.14356</a> (replaced) [<a href="/pdf/2309.14356" title="Download PDF">pdf</a>, <a href="/format/2309.14356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COCO-Counterfactuals: Automatically Constructed Counterfactual Examples  for Image-Text Pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Tiep Le</a>, 
<a href="/search/cs?searchtype=author&query=Lal%2C+V">Vasudev Lal</a>, 
<a href="/search/cs?searchtype=author&query=Howard%2C+P">Phillip Howard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15471" title="Abstract">arXiv:2309.15471</a> (replaced) [<a href="/pdf/2309.15471" title="Download PDF">pdf</a>, <a href="/format/2309.15471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProFaaStinate: Delaying Serverless Function Calls to Optimize Platform  Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schirmer%2C+T">Trever Schirmer</a>, 
<a href="/search/cs?searchtype=author&query=Carl%2C+V">Valentin Carl</a>, 
<a href="/search/cs?searchtype=author&query=Pfandzelter%2C+T">Tobias Pfandzelter</a>, 
<a href="/search/cs?searchtype=author&query=Bermbach%2C+D">David Bermbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Proc. of 9th International Workshop on Serverless Computing (WoSC 23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15919" title="Abstract">arXiv:2309.15919</a> (replaced) [<a href="/pdf/2309.15919" title="Download PDF">pdf</a>, <a href="/ps/2309.15919" title="Download PostScript">ps</a>, <a href="/format/2309.15919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Computation of the Quantum Rate-Distortion Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=He%2C+K">Kerry He</a>, 
<a href="/search/quant-ph?searchtype=author&query=Saunderson%2C+J">James Saunderson</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fawzi%2C+H">Hamza Fawzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 2 figures, 2 tables. v2: Minor edits to introduction, abstract, and notation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16585" title="Abstract">arXiv:2309.16585</a> (replaced) [<a href="/pdf/2309.16585" title="Download PDF">pdf</a>, <a href="/format/2309.16585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-to-3D using Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zilong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huaping Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://gsgen3d.github.io.">this https URL</a> Code: <a href="https://github.com/gsgen3d/gsgen">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16645" title="Abstract">arXiv:2309.16645</a> (replaced) [<a href="/pdf/2309.16645" title="Download PDF">pdf</a>, <a href="/format/2309.16645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reusability report: Prostate cancer stratification with diverse  biologically-informed neural architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pedersen%2C+C">Christian Pedersen</a>, 
<a href="/search/cs?searchtype=author&query=Tesileanu%2C+T">Tiberiu Tesileanu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tinghui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Golkar%2C+S">Siavash Golkar</a>, 
<a href="/search/cs?searchtype=author&query=Cranmer%2C+M">Miles Cranmer</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+S">Shirley Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures. Submitted to Nature Machine Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16844" title="Abstract">arXiv:2309.16844</a> (replaced) [<a href="/pdf/2309.16844" title="Download PDF">pdf</a>, <a href="/ps/2309.16844" title="Download PostScript">ps</a>, <a href="/format/2309.16844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian  Portuguese Natural Language Processing Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Campiotti%2C+I">Israel Campiotti</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+M">Matheus Rodrigues</a>, 
<a href="/search/cs?searchtype=author&query=Albuquerque%2C+Y">Yuri Albuquerque</a>, 
<a href="/search/cs?searchtype=author&query=Azevedo%2C+R">Rafael Azevedo</a>, 
<a href="/search/cs?searchtype=author&query=Andrade%2C+A">Alyson Andrade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00093" title="Abstract">arXiv:2310.00093</a> (replaced) [<a href="/pdf/2310.00093" title="Download PDF">pdf</a>, <a href="/format/2310.00093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DataDAM: Efficient Dataset Distillation with Attention Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sajedi%2C+A">Ahmad Sajedi</a>, 
<a href="/search/cs?searchtype=author&query=Khaki%2C+S">Samir Khaki</a>, 
<a href="/search/cs?searchtype=author&query=Amjadian%2C+E">Ehsan Amjadian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L+Z">Lucy Z. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lawryshyn%2C+Y+A">Yuri A. Lawryshyn</a>, 
<a href="/search/cs?searchtype=author&query=Plataniotis%2C+K+N">Konstantinos N. Plataniotis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in International Conference in Computer Vision (ICCV) 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> booktitle = Proceedings of the IEEE/CVF International Conference
  on Computer Vision (ICCV) month = October year = 2023 pages = 17097-17107
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00747" title="Abstract">arXiv:2310.00747</a> (replaced) [<a href="/pdf/2310.00747" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NoxTrader: LSTM-Based Stock Return Momentum Prediction for Quantitative  Trading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Liu%2C+H">Hsiang-Hui Liu</a>, 
<a href="/search/q-fin?searchtype=author&query=Shu%2C+H">Han-Jay Shu</a>, 
<a href="/search/q-fin?searchtype=author&query=Chiu%2C+W">Wei-Ning Chiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Portfolio Management (q-fin.PM)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01210" title="Abstract">arXiv:2310.01210</a> (replaced) [<a href="/pdf/2310.01210" title="Download PDF">pdf</a>, <a href="/format/2310.01210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Cardiac Segmentation using Graph Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Van+De+Vyver%2C+G">Gilles Van De Vyver</a>, 
<a href="/search/eess?searchtype=author&query=Thomas%2C+S">Sarina Thomas</a>, 
<a href="/search/eess?searchtype=author&query=Ben-Yosef%2C+G">Guy Ben-Yosef</a>, 
<a href="/search/eess?searchtype=author&query=Olaisen%2C+S+H">Sindre Hellum Olaisen</a>, 
<a href="/search/eess?searchtype=author&query=Dalen%2C+H">H&#xe5;vard Dalen</a>, 
<a href="/search/eess?searchtype=author&query=L%C3%B8vstakken%2C+L">Lasse L&#xf8;vstakken</a>, 
<a href="/search/eess?searchtype=author&query=Smistad%2C+E">Erik Smistad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02230" title="Abstract">arXiv:2310.02230</a> (replaced) [<a href="/pdf/2310.02230" title="Download PDF">pdf</a>, <a href="/format/2310.02230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts  in Underspecified Visual Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scimeca%2C+L">Luca Scimeca</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+A">Alexander Rubinstein</a>, 
<a href="/search/cs?searchtype=author&query=Nicolicioiu%2C+A+M">Armand Mihai Nicolicioiu</a>, 
<a href="/search/cs?searchtype=author&query=Teney%2C+D">Damien Teney</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Neural Information Processing Systems(NeurIPS) 2023 - Workshop on Diffusion Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03320" title="Abstract">arXiv:2310.03320</a> (replaced) [<a href="/pdf/2310.03320" title="Download PDF">pdf</a>, <a href="/format/2310.03320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B">Balasubramaniam Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Ioannidis%2C+V+N">Vassilis N. Ioannidis</a>, 
<a href="/search/cs?searchtype=author&query=Rangwala%2C+H">Huzefa Rangwala</a>, 
<a href="/search/cs?searchtype=author&query=Anubhai%2C+R">Rishita Anubhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05027" title="Abstract">arXiv:2310.05027</a> (replaced) [<a href="/pdf/2310.05027" title="Download PDF">pdf</a>, <a href="/format/2310.05027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rigid Clumps in the MercuryDPM Particle Dynamics Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ostanin%2C+I">Igor Ostanin</a>, 
<a href="/search/math?searchtype=author&query=Angelidakis%2C+V">Vasileios Angelidakis</a>, 
<a href="/search/math?searchtype=author&query=Plath%2C+T">Timo Plath</a>, 
<a href="/search/math?searchtype=author&query=Pourandi%2C+S">Sahar Pourandi</a>, 
<a href="/search/math?searchtype=author&query=Thornton%2C+A">Anthony Thornton</a>, 
<a href="/search/math?searchtype=author&query=Weinhart%2C+T">Thomas Weinhart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05446" title="Abstract">arXiv:2310.05446</a> (replaced) [<a href="/pdf/2310.05446" title="Download PDF">pdf</a>, <a href="/format/2310.05446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RetSeg: Retention-based Colorectal Polyps Segmentation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=ELKarazle%2C+K">Khaled ELKarazle</a>, 
<a href="/search/eess?searchtype=author&query=Raman%2C+V">Valliappan Raman</a>, 
<a href="/search/eess?searchtype=author&query=Chua%2C+C">Caslon Chua</a>, 
<a href="/search/eess?searchtype=author&query=Then%2C+P">Patrick Then</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated PDF
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05964" title="Abstract">arXiv:2310.05964</a> (replaced) [<a href="/pdf/2310.05964" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Embeddings for Measuring Text Relatedness: Unveiling  Sentiments and Relationships in Online Comments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olakangil%2C+A">Anthony Olakangil</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cindy Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+J">Justin Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qunbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jethwa%2C+K">Kaavya Jethwa</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jason Li</a>, 
<a href="/search/cs?searchtype=author&query=Narendra%2C+A">Aryan Narendra</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Nishk Patel</a>, 
<a href="/search/cs?searchtype=author&query=Rajaram%2C+A">Arjun Rajaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 3 tables, accepted to the Second International Conference on Informatics (ICI-2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06116" title="Abstract">arXiv:2310.06116</a> (replaced) [<a href="/pdf/2310.06116" title="Download PDF">pdf</a>, <a href="/format/2310.06116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OptiMUS: Optimization Modeling Using MIP Solvers and large language  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AhmadiTeshnizi%2C+A">Ali AhmadiTeshnizi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wenzhi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Udell%2C+M">Madeleine Udell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06827" title="Abstract">arXiv:2310.06827</a> (replaced) [<a href="/pdf/2310.06827" title="Download PDF">pdf</a>, <a href="/format/2310.06827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teaching Language Models to Hallucinate Less with Synthetic Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jones%2C+E">Erik Jones</a>, 
<a href="/search/cs?searchtype=author&query=Palangi%2C+H">Hamid Palangi</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%B5es%2C+C">Clarisse Sim&#xf5;es</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+V">Varun Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Subhabrata Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+A">Arindam Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A">Ahmed Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Kamar%2C+E">Ece Kamar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08166" title="Abstract">arXiv:2310.08166</a> (replaced) [<a href="/pdf/2310.08166" title="Download PDF">pdf</a>, <a href="/format/2310.08166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ziya-Visual: Bilingual Large Vision-Language Model via Multi-Task  Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Junyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dixiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaojun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+R">Ruyi Gan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yan Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pingjian Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08748" title="Abstract">arXiv:2310.08748</a> (replaced) [<a href="/pdf/2310.08748" title="Download PDF">pdf</a>, <a href="/ps/2310.08748" title="Download PostScript">ps</a>, <a href="/format/2310.08748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Dynamic Optimization and Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boulesnane%2C+A">Abdennour Boulesnane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09833" title="Abstract">arXiv:2310.09833</a> (replaced) [<a href="/pdf/2310.09833" title="Download PDF">pdf</a>, <a href="/format/2310.09833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by  Mutual Information Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Simin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruixiao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+P">Pu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiakai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aishan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+W">Weifeng Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09866" title="Abstract">arXiv:2310.09866</a> (replaced) [<a href="/pdf/2310.09866" title="Download PDF">pdf</a>, <a href="/format/2310.09866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Multi-Objective Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haibo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chaosheng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Momma%2C+M">Michinari Momma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10385" title="Abstract">arXiv:2310.10385</a> (replaced) [<a href="/pdf/2310.10385" title="Download PDF">pdf</a>, <a href="/format/2310.10385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Better Understanding of Variations in Zero-Shot Neural Machine  Translation Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Shaomu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by the EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11760" title="Abstract">arXiv:2310.11760</a> (replaced) [<a href="/pdf/2310.11760" title="Download PDF">pdf</a>, <a href="/ps/2310.11760" title="Download PostScript">ps</a>, <a href="/format/2310.11760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Security-Constrained Optimal Power Management Algorithm for Shipboard  Microgrids with Battery Energy Storage System and Fuel Cell
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=D%27Agostino%2C+F">Fabio D&#x27;Agostino</a>, 
<a href="/search/eess?searchtype=author&query=Gallo%2C+M">Marco Gallo</a>, 
<a href="/search/eess?searchtype=author&query=Saviozzi%2C+M">Matteo Saviozzi</a>, 
<a href="/search/eess?searchtype=author&query=Silvestro%2C+F">Federico Silvestro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Transportation Electrification. arXiv admin note: substantial text overlap with <a href="/abs/2304.03621">arXiv:2304.03621</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12743" title="Abstract">arXiv:2310.12743</a> (replaced) [<a href="/pdf/2310.12743" title="Download PDF">pdf</a>, <a href="/format/2310.12743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Canonical normalizing flows for manifold learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Flouris%2C+K">Kyriakos Flouris</a>, 
<a href="/search/stat?searchtype=author&query=Konukoglu%2C+E">Ender Konukoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> https://neurips.cc/virtual/2023/poster/69924
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Differential Geometry (math.DG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13032" title="Abstract">arXiv:2310.13032</a> (replaced) [<a href="/pdf/2310.13032" title="Download PDF">pdf</a>, <a href="/format/2310.13032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality-Diversity through AI Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bradley%2C+H">Herbie Bradley</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+A">Andrew Dai</a>, 
<a href="/search/cs?searchtype=author&query=Teufel%2C+H">Hannah Teufel</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jenny Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Oostermeijer%2C+K">Koen Oostermeijer</a>, 
<a href="/search/cs?searchtype=author&query=Bellagente%2C+M">Marco Bellagente</a>, 
<a href="/search/cs?searchtype=author&query=Clune%2C+J">Jeff Clune</a>, 
<a href="/search/cs?searchtype=author&query=Stanley%2C+K">Kenneth Stanley</a>, 
<a href="/search/cs?searchtype=author&query=Schott%2C+G">Gr&#xe9;gory Schott</a>, 
<a href="/search/cs?searchtype=author&query=Lehman%2C+J">Joel Lehman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> minor addition edits for improved clarity
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13673" title="Abstract">arXiv:2310.13673</a> (replaced) [<a href="/pdf/2310.13673" title="Download PDF">pdf</a>, <a href="/format/2310.13673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeoung%2C+S">Sullam Jeoung</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yubin Ge</a>, 
<a href="/search/cs?searchtype=author&query=Diesner%2C+J">Jana Diesner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13788" title="Abstract">arXiv:2310.13788</a> (replaced) [<a href="/pdf/2310.13788" title="Download PDF">pdf</a>, <a href="/ps/2310.13788" title="Download PostScript">ps</a>, <a href="/format/2310.13788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Integer Points Counting in Parametric Polyhedra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gribanov%2C+D">D. Gribanov</a>, 
<a href="/search/cs?searchtype=author&query=Malyshev%2C+D">D. Malyshev</a>, 
<a href="/search/cs?searchtype=author&query=Pardalos%2C+P">P. Pardalos</a>, 
<a href="/search/cs?searchtype=author&query=Zolotykh%2C+N">N. Zolotykh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14288" title="Abstract">arXiv:2310.14288</a> (replaced) [<a href="/pdf/2310.14288" title="Download PDF">pdf</a>, <a href="/format/2310.14288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Popular and Dominant Matchings with Uncertain, Multilayer and Aggregated  Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cs%C3%A1ji%2C+G">Gergely Cs&#xe1;ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14318" title="Abstract">arXiv:2310.14318</a> (replaced) [<a href="/pdf/2310.14318" title="Download PDF">pdf</a>, <a href="/format/2310.14318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intent Contrastive Learning with Cross Subsequences for Sequential  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+X">Xiuyuan Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Huanhuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Pengpeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guanfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+F">Fuzhen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+V+S">Victor S. Sheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10pages, 5figures, WSDM2024. arXiv admin note: text overlap with <a href="/abs/2304.07763">arXiv:2304.07763</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14336" title="Abstract">arXiv:2310.14336</a> (replaced) [<a href="/pdf/2310.14336" title="Download PDF">pdf</a>, <a href="/format/2310.14336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Interpretable Rules for Scalable Data Representation and  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TPAMI in October 2023; Interpretable ML; Neuro-Symbolic AI; Preliminary conference version (NeurIPS 2021) available at <a href="/abs/2109.15103">arXiv:2109.15103</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14602" title="Abstract">arXiv:2310.14602</a> (replaced) [<a href="/pdf/2310.14602" title="Download PDF">pdf</a>, <a href="/ps/2310.14602" title="Download PostScript">ps</a>, <a href="/format/2310.14602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Pre-trained Transformer for Vietnamese Community-based  COVID-19 Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vo%2C+T+M">Tam Minh Vo</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+K+V">Khiem Vinh Tran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14767" title="Abstract">arXiv:2310.14767</a> (replaced) [<a href="/pdf/2310.14767" title="Download PDF">pdf</a>, <a href="/format/2310.14767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting COVID-19 Infections Using Multi-layer Centrality Measures in  Population-scale Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Westernhagen%2C+C+H">Christine Hedde-von Westernhagen</a>, 
<a href="/search/cs?searchtype=author&query=Garcia-Bernardo%2C+J">Javier Garcia-Bernardo</a>, 
<a href="/search/cs?searchtype=author&query=Bagheri%2C+A">Ayoub Bagheri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated Acknowledgement section
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15421" title="Abstract">arXiv:2310.15421</a> (replaced) [<a href="/pdf/2310.15421" title="Download PDF">pdf</a>, <a href="/format/2310.15421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FANToM: A Benchmark for Stress-testing Machine Theory of Mind in  Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sclar%2C+M">Melanie Sclar</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bras%2C+R+L">Ronan Le Bras</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gunhee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. Code and dataset can be found here: <a href="https://hyunw.kim/fantom">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15848" title="Abstract">arXiv:2310.15848</a> (replaced) [<a href="/pdf/2310.15848" title="Download PDF">pdf</a>, <a href="/format/2310.15848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Responsible Machine Learning Datasets with Fairness, Privacy, and  Regulatory Norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Surbhi Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Thakral%2C+K">Kartik Thakral</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Richa Singh</a>, 
<a href="/search/cs?searchtype=author&query=Vatsa%2C+M">Mayank Vatsa</a>, 
<a href="/search/cs?searchtype=author&query=Glaser%2C+T">Tamar Glaser</a>, 
<a href="/search/cs?searchtype=author&query=Ferrer%2C+C+C">Cristian Canton Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Hassner%2C+T">Tal Hassner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16108" title="Abstract">arXiv:2310.16108</a> (replaced) [<a href="/pdf/2310.16108" title="Download PDF">pdf</a>, <a href="/format/2310.16108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise Distributed Satellite Navigation: Differential GPS with  Sensor-Coupling for Integer Ambiguity Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Low%2C+S+Y+W">Samuel Y W Low</a>, 
<a href="/search/eess?searchtype=author&query=D%27Amico%2C+S">Simone D&#x27;Amico</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 20 figures, IEEE AERO 2024 (pre-print)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Multiagent Systems (cs.MA); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16176" title="Abstract">arXiv:2310.16176</a> (replaced) [<a href="/pdf/2310.16176" title="Download PDF">pdf</a>, <a href="/format/2310.16176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correction with Backtracking Reduces Hallucination in Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenzhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+C">Chao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Kishore%2C+V">Varsha Kishore</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+P">Jin Peng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minmin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+K+Q">Kilian Q. Weinberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16475" title="Abstract">arXiv:2310.16475</a> (replaced) [<a href="/pdf/2310.16475" title="Download PDF">pdf</a>, <a href="/format/2310.16475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Serverless Function Scheduling at the Network Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jiong Lou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhiqing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shijing Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chengtao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+W">Weijia Jia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16798" title="Abstract">arXiv:2310.16798</a> (replaced) [<a href="/pdf/2310.16798" title="Download PDF">pdf</a>, <a href="/format/2310.16798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reachability in Continuous Pushdown VASS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+A+R">A. R. Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+R">Rupak Majumdar</a>, 
<a href="/search/cs?searchtype=author&query=Thinniyam%2C+R+S">Ramanathan S. Thinniyam</a>, 
<a href="/search/cs?searchtype=author&query=Zetzsche%2C+G">Georg Zetzsche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16945" title="Abstract">arXiv:2310.16945</a> (replaced) [<a href="/pdf/2310.16945" title="Download PDF">pdf</a>, <a href="/format/2310.16945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Q-Aggregation for CATE Model Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lan%2C+H">Hui Lan</a>, 
<a href="/search/stat?searchtype=author&query=Syrgkanis%2C+V">Vasilis Syrgkanis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The main text is 10 pages, and we include the Appendix at the end (totaling 51 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17158" title="Abstract">arXiv:2310.17158</a> (replaced) [<a href="/pdf/2310.17158" title="Download PDF">pdf</a>, <a href="/format/2310.17158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CosmosDSR -- a methodology for automated detection and tracking of  orbital debris using the Unscented Kalman Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Roll%2C+D+S">Daniel S. Roll</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kurt%2C+Z">Zeyneb Kurt</a>, 
<a href="/search/astro-ph?searchtype=author&query=Woo%2C+W+L">Wai Lok Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 figures, 15 pages inc refs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17306" title="Abstract">arXiv:2310.17306</a> (replaced) [<a href="/e-print/2310.17306" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FormaT5: Abstention and Examples for Conditional Table Formatting with  Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mukul Singh</a>, 
<a href="/search/cs?searchtype=author&query=Cambronero%2C+J">Jos&#xe9; Cambronero</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Negreanu%2C+C">Carina Negreanu</a>, 
<a href="/search/cs?searchtype=author&query=Nouri%2C+E">Elnaz Nouri</a>, 
<a href="/search/cs?searchtype=author&query=Raza%2C+M">Mohammad Raza</a>, 
<a href="/search/cs?searchtype=author&query=Verbruggen%2C+G">Gust Verbruggen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There are some errors in the paper and we need to retract it
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Databases (cs.DB); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17533" title="Abstract">arXiv:2310.17533</a> (replaced) [<a href="/pdf/2310.17533" title="Download PDF">pdf</a>, <a href="/ps/2310.17533" title="Download PostScript">ps</a>, <a href="/format/2310.17533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding The Digital Fuku: Deciphering Colonial Legacies to Critically  Assess ChatGPT in Dominican Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ovalle%2C+A">Anaelia Ovalle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17680" title="Abstract">arXiv:2310.17680</a> (replaced) [<a href="/e-print/2310.17680" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeFusion: A Pre-trained Diffusion Model for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mukul Singh</a>, 
<a href="/search/cs?searchtype=author&query=Cambronero%2C+J">Jos&#xe9; Cambronero</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Negreanu%2C+C">Carina Negreanu</a>, 
<a href="/search/cs?searchtype=author&query=Verbruggen%2C+G">Gust Verbruggen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There are some errors in the paper and we need to retract it
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17811" title="Abstract">arXiv:2310.17811</a> (replaced) [<a href="/pdf/2310.17811" title="Download PDF">pdf</a>, <a href="/format/2310.17811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Style-Aware Radiology Report Generation with RadGraph and Few-Shot  Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Benjamin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruochen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+D+E">David E. Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Adithan%2C+S">Subathra Adithan</a>, 
<a href="/search/cs?searchtype=author&query=Reis%2C+E+P">Eduardo Pontes Reis</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+S">Stephen Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Venugopal%2C+V+K">Vasantha Kumar Venugopal</a>, 
<a href="/search/cs?searchtype=author&query=O%27Connell%2C+C+P">Chloe P. O&#x27;Connell</a>, 
<a href="/search/cs?searchtype=author&query=Saenz%2C+A">Agustina Saenz</a>, 
<a href="/search/cs?searchtype=author&query=Rajpurkar%2C+P">Pranav Rajpurkar</a>, 
<a href="/search/cs?searchtype=author&query=Moor%2C+M">Michael Moor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17876" title="Abstract">arXiv:2310.17876</a> (replaced) [<a href="/pdf/2310.17876" title="Download PDF">pdf</a>, <a href="/format/2310.17876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TarGEN: Targeted Data Generation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+H">Himanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Scaria%2C+K">Kevin Scaria</a>, 
<a href="/search/cs?searchtype=author&query=Anantheswaran%2C+U">Ujjwala Anantheswaran</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+S">Shreyas Verma</a>, 
<a href="/search/cs?searchtype=author&query=Parmar%2C+M">Mihir Parmar</a>, 
<a href="/search/cs?searchtype=author&query=Sawant%2C+S+A">Saurabh Arjun Sawant</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 tables, 5 figures, 5 pages references, 17 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18068" title="Abstract">arXiv:2310.18068</a> (replaced) [<a href="/pdf/2310.18068" title="Download PDF">pdf</a>, <a href="/format/2310.18068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple and Robust Dynamic Two-Dimensional Convex Hull
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%A6de%2C+E+T">Emil Toftegaard G&#xe6;de</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B8rtz%2C+I+L">Inge Li G&#xf8;rtz</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Hoog%2C+I">Ivor van der Hoog</a>, 
<a href="/search/cs?searchtype=author&query=Krogh%2C+C">Christoffer Krogh</a>, 
<a href="/search/cs?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for ALENEX24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18128" title="Abstract">arXiv:2310.18128</a> (replaced) [<a href="/pdf/2310.18128" title="Download PDF">pdf</a>, <a href="/format/2310.18128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Dynamic Time Warping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bringmann%2C+K">Karl Bringmann</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+N">Nick Fischer</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Hoog%2C+I">Ivor van der Hoog</a>, 
<a href="/search/cs?searchtype=author&query=Kipouridis%2C+E">Evangelos Kipouridis</a>, 
<a href="/search/cs?searchtype=author&query=Kociumaka%2C+T">Tomasz Kociumaka</a>, 
<a href="/search/cs?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at SODA24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18146" title="Abstract">arXiv:2310.18146</a> (replaced) [<a href="/pdf/2310.18146" title="Download PDF">pdf</a>, <a href="/format/2310.18146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Out-Orientations with Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chekuri%2C+C">Chandra Chekuri</a>, 
<a href="/search/cs?searchtype=author&query=Christiansen%2C+A+B">Aleksander Bj&#xf8;rn Christiansen</a>, 
<a href="/search/cs?searchtype=author&query=Holm%2C+J">Jacob Holm</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Hoog%2C+I">Ivor van der Hoog</a>, 
<a href="/search/cs?searchtype=author&query=Quanrud%2C+K">Kent Quanrud</a>, 
<a href="/search/cs?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>, 
<a href="/search/cs?searchtype=author&query=Schwiegelshohn%2C+C">Chris Schwiegelshohn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at SODA24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18331" title="Abstract">arXiv:2310.18331</a> (replaced) [<a href="/e-print/2310.18331" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AllTogether: Investigating the Efficacy of Spliced Prompt for Web  Navigation using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiarun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wentao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chunhong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Include wrong information in comment. Should be 7 pages and not published yet
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18346" title="Abstract">arXiv:2310.18346</a> (replaced) [<a href="/pdf/2310.18346" title="Download PDF">pdf</a>, <a href="/format/2310.18346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Free Distillation Improves Efficiency and Privacy in Federated  Thorax Disease Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the IEEE EMBS International Conference on Data Science and Engineering in Healthcare, Medicine &amp; Biology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18351" title="Abstract">arXiv:2310.18351</a> (replaced) [<a href="/pdf/2310.18351" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioImage.IO Chatbot: A Personalized Assistant for BioImage Analysis  Augmented by Community Knowledge Base
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+W">Wanlu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Fuster-Barcel%C3%B3%2C+C">Caterina Fuster-Barcel&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz-Barrutia%2C+A">Arrate Mu&#xf1;oz-Barrutia</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wei Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18356" title="Abstract">arXiv:2310.18356</a> (replaced) [<a href="/pdf/2310.18356" title="Download PDF">pdf</a>, <a href="/format/2310.18356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRAShear: Efficient Large Language Model Structured Pruning and  Knowledge Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+T">Tianyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+B">Badal Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Zharkov%2C+I">Ilya Zharkov</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Luming Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18416" title="Abstract">arXiv:2310.18416</a> (replaced) [<a href="/pdf/2310.18416" title="Download PDF">pdf</a>, <a href="/format/2310.18416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolyMerge: A Novel Technique aimed at Dynamic HD Map Updates Leveraging  Polylines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayed%2C+M">Mohamed Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Perminov%2C+S">Stepan Perminov</a>, 
<a href="/search/cs?searchtype=author&query=Tsetserukou%2C+D">Dzmitry Tsetserukou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18518" title="Abstract">arXiv:2310.18518</a> (replaced) [<a href="/pdf/2310.18518" title="Download PDF">pdf</a>, <a href="/ps/2310.18518" title="Download PostScript">ps</a>, <a href="/format/2310.18518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfiguration of plane trees in convex geometric graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bousquet%2C+N">Nicolas Bousquet</a>, 
<a href="/search/cs?searchtype=author&query=De+Meyer%2C+L">Lucas De Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Pierron%2C+T">Th&#xe9;o Pierron</a>, 
<a href="/search/cs?searchtype=author&query=Wesolek%2C+A">Alexandra Wesolek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18752" title="Abstract">arXiv:2310.18752</a> (replaced) [<a href="/pdf/2310.18752" title="Download PDF">pdf</a>, <a href="/format/2310.18752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reboost Large Language Model-based Text-to-SQL, Text-to-Python, and  Text-to-Function -- with Real Applications in Traffic Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+G">Guanghu Sui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhishuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+J">Jingqing Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Hangyu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18852" title="Abstract">arXiv:2310.18852</a> (replaced) [<a href="/pdf/2310.18852" title="Download PDF">pdf</a>, <a href="/format/2310.18852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI for Open Science: A Multi-Agent Perspective for Ethically Translating  Data to Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yakaboski%2C+C">Chase Yakaboski</a>, 
<a href="/search/cs?searchtype=author&query=Hyde%2C+G">Gregory Hyde</a>, 
<a href="/search/cs?searchtype=author&query=Nyanhongo%2C+C">Clement Nyanhongo</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+E">Eugene Santos Jr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS AI For Science Workshop 2023. 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18891" title="Abstract">arXiv:2310.18891</a> (replaced) [<a href="/pdf/2310.18891" title="Download PDF">pdf</a>, <a href="/format/2310.18891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Interaction-Aware Dynamical Models and Decision Making for  Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crosato%2C+L">Luca Crosato</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+K">Kai Tian</a>, 
<a href="/search/cs?searchtype=author&query=Shum%2C+H+P+H">Hubert P. H Shum</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+E+S+L">Edmond S. L. Ho</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yafei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chongfeng Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18999" title="Abstract">arXiv:2310.18999</a> (replaced) [<a href="/pdf/2310.18999" title="Download PDF">pdf</a>, <a href="/format/2310.18999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynPoint: Dynamic Neural Point For View Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaichen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+J">Jia-Xing Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Sangyun Shin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Kai Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Markham%2C+A">Andrew Markham</a>, 
<a href="/search/cs?searchtype=author&query=Trigoni%2C+N">Niki Trigoni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19007" title="Abstract">arXiv:2310.19007</a> (replaced) [<a href="/pdf/2310.19007" title="Download PDF">pdf</a>, <a href="/format/2310.19007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Behavior Alignment via Reward Function Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Dhawal Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chandak%2C+Y">Yash Chandak</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+S+M">Scott M. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+P+S">Philip S. Thomas</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+B+C">Bruno Castro da Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> (Spotlight) Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19019" title="Abstract">arXiv:2310.19019</a> (replaced) [<a href="/pdf/2310.19019" title="Download PDF">pdf</a>, <a href="/format/2310.19019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language  Modeling Likewise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+N">Nan He</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+H">Hanyu Lai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chenyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zirui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Junting Pan</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+R">Ruoyu Qin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Ruofan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Rui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Gangming Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Z">Zhaohui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shaoqing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Ding Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+M">Mingjie Zhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures, 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19096" title="Abstract">arXiv:2310.19096</a> (replaced) [<a href="/pdf/2310.19096" title="Download PDF">pdf</a>, <a href="/ps/2310.19096" title="Download PostScript">ps</a>, <a href="/format/2310.19096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circuit Width Estimation via Effect Typing and Linear Dependency (Long  Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colledan%2C+A">Andrea Colledan</a>, 
<a href="/search/cs?searchtype=author&query=Lago%2C+U+D">Ugo Dal Lago</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages (excluding references), 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19152" title="Abstract">arXiv:2310.19152</a> (replaced) [<a href="/pdf/2310.19152" title="Download PDF">pdf</a>, <a href="/format/2310.19152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BERT Lost Patience Won&#x27;t Be Robust to Adversarial Slowdown
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coalson%2C+Z">Zachary Coalson</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+G">Gabriel Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Bobba%2C+R">Rakesh Bobba</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Sanghyun Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 [Poster]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19336" title="Abstract">arXiv:2310.19336</a> (replaced) [<a href="/pdf/2310.19336" title="Download PDF">pdf</a>, <a href="/format/2310.19336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Considerations for the Control Design of Augmentative Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guptasarma%2C+S">Shivani Guptasarma</a>, 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+M">Monroe Kennedy III</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages. Presented at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2021) Workshop on Building and Evaluating Ethical Robotic Systems, Prague, Czech Republic, 28-30 September 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19415" title="Abstract">arXiv:2310.19415</a> (replaced) [<a href="/pdf/2310.19415" title="Download PDF">pdf</a>, <a href="/format/2310.19415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-to-3D with Classifier Score Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuan-Chen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Ding Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Song-Hai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our project page is <a href="https://xinyu-andy.github.io/Classifier-Score-Distillation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19450" title="Abstract">arXiv:2310.19450</a> (replaced) [<a href="/pdf/2310.19450" title="Download PDF">pdf</a>, <a href="/format/2310.19450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hodge-Compositional Edge Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yang%2C+M">Maosheng Yang</a>, 
<a href="/search/stat?searchtype=author&query=Borovitskiy%2C+V">Viacheslav Borovitskiy</a>, 
<a href="/search/stat?searchtype=author&query=Isufi%2C+E">Elvin Isufi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19462" title="Abstract">arXiv:2310.19462</a> (replaced) [<a href="/pdf/2310.19462" title="Download PDF">pdf</a>, <a href="/format/2310.19462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constituency Parsing using LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xuefeng Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jialong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19503" title="Abstract">arXiv:2310.19503</a> (replaced) [<a href="/pdf/2310.19503" title="Download PDF">pdf</a>, <a href="/format/2310.19503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust, Accountability, and Autonomy in Knowledge Graph-based AI for  Self-determination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ib%C3%A1%C3%B1ez%2C+L">Luis-Daniel Ib&#xe1;&#xf1;ez</a>, 
<a href="/search/cs?searchtype=author&query=Domingue%2C+J">John Domingue</a>, 
<a href="/search/cs?searchtype=author&query=Kirrane%2C+S">Sabrina Kirrane</a>, 
<a href="/search/cs?searchtype=author&query=Seneviratne%2C+O">Oshani Seneviratne</a>, 
<a href="/search/cs?searchtype=author&query=Third%2C+A">Aisling Third</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+M">Maria-Esther Vidal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19582" title="Abstract">arXiv:2310.19582</a> (replaced) [<a href="/pdf/2310.19582" title="Download PDF">pdf</a>, <a href="/format/2310.19582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-interpretable and deep features for image privacy classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baranouskaya%2C+D">Darya Baranouskaya</a>, 
<a href="/search/cs?searchtype=author&query=Cavallaro%2C+A">Andrea Cavallaro</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Image Processing (ICIP),
  Kuala Lumpur, Malaysia, 2023, pp. 3489-3492
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19596" title="Abstract">arXiv:2310.19596</a> (replaced) [<a href="/pdf/2310.19596" title="Download PDF">pdf</a>, <a href="/format/2310.19596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMaAA: Making Large Language Models as Active Annotators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanzeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yongliang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Ming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+L">Lei Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19671" title="Abstract">arXiv:2310.19671</a> (replaced) [<a href="/pdf/2310.19671" title="Download PDF">pdf</a>, <a href="/format/2310.19671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models: The Need for Nuance in Current Debates and a  Pragmatic Perspective on Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Dijk%2C+B+M+A">Bram M.A. van Dijk</a>, 
<a href="/search/cs?searchtype=author&query=Kouwenhoven%2C+T">Tom Kouwenhoven</a>, 
<a href="/search/cs?searchtype=author&query=Spruit%2C+M+R">Marco R. Spruit</a>, 
<a href="/search/cs?searchtype=author&query=van+Duijn%2C+M+J">Max J. van Duijn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 0 figures, Forthcoming in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19677" title="Abstract">arXiv:2310.19677</a> (replaced) [<a href="/pdf/2310.19677" title="Download PDF">pdf</a>, <a href="/format/2310.19677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoCa: Measuring Human-Language Model Alignment on Causal and Moral  Judgment Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+A">Allen Nie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Amdekar%2C+A">Atharva Amdekar</a>, 
<a href="/search/cs?searchtype=author&query=Piech%2C+C">Chris Piech</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Gerstenberg%2C+T">Tobias Gerstenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 7 figures. NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19708" title="Abstract">arXiv:2310.19708</a> (replaced) [<a href="/pdf/2310.19708" title="Download PDF">pdf</a>, <a href="/format/2310.19708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Language Models For Specialized Domains: A Colorful Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eitan%2C+D">Daniel Eitan</a>, 
<a href="/search/cs?searchtype=author&query=Pirchi%2C+M">Menachem Pirchi</a>, 
<a href="/search/cs?searchtype=author&query=Glazer%2C+N">Neta Glazer</a>, 
<a href="/search/cs?searchtype=author&query=Meital%2C+S">Shai Meital</a>, 
<a href="/search/cs?searchtype=author&query=Ayach%2C+G">Gil Ayach</a>, 
<a href="/search/cs?searchtype=author&query=Shamsian%2C+A">Aviv Shamsian</a>, 
<a href="/search/cs?searchtype=author&query=Navon%2C+A">Aviv Navon</a>, 
<a href="/search/cs?searchtype=author&query=Hetz%2C+G">Gil Hetz</a>, 
<a href="/search/cs?searchtype=author&query=Keshet%2C+J">Joseph Keshet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19721" title="Abstract">arXiv:2310.19721</a> (replaced) [<a href="/pdf/2310.19721" title="Download PDF">pdf</a>, <a href="/format/2310.19721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained  Image Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+D">Dewei Hu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiacheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Oguz%2C+I">Ipek Oguz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> changed title
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19736" title="Abstract">arXiv:2310.19736</a> (replaced) [<a href="/pdf/2310.19736" title="Download PDF">pdf</a>, <a href="/format/2310.19736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Large Language Models: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zishan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Renren Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+D">Dan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Supryadi">Supryadi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Linhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+B">Bojian Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Deyi Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 111 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19778" title="Abstract">arXiv:2310.19778</a> (replaced) [<a href="/pdf/2310.19778" title="Download PDF">pdf</a>, <a href="/format/2310.19778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing AI Support for Human Involvement in AI-assisted Decision  Making: A Taxonomy of Human-AI Interactions from a Systematic Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gomez%2C+C">Catalina Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S+M">Sue Min Cho</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+S">Shichang Ke</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chien-Ming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Unberath%2C+M">Mathias Unberath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19786" title="Abstract">arXiv:2310.19786</a> (replaced) [<a href="/pdf/2310.19786" title="Download PDF">pdf</a>, <a href="/ps/2310.19786" title="Download PostScript">ps</a>, <a href="/format/2310.19786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From External to Swap Regret 2.0: An Efficient Reduction and Oblivious  Adversary for Large Action Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dagan%2C+Y">Yuval Dagan</a>, 
<a href="/search/cs?searchtype=author&query=Daskalakis%2C+C">Constantinos Daskalakis</a>, 
<a href="/search/cs?searchtype=author&query=Fishelson%2C+M">Maxwell Fishelson</a>, 
<a href="/search/cs?searchtype=author&query=Golowich%2C+N">Noah Golowich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item359">Cross-lists</a></li>
<li><a href="#item416">Replacements</a></li>
</ul>
<small>[ total of 703 entries:  <b>1-703</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
