<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Mon 13 Nov 23  to  Tue 14 Nov 23, announced Wed, 15 Nov 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item360">Cross-lists</a></li>
<li><a href="#item408">Replacements</a></li>
</ul>
<small>[ total of 625 entries:  <b>1-625</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Wed, 15 Nov 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07577" title="Abstract">arXiv:2311.07577</a> [<a href="/pdf/2311.07577" title="Download PDF">pdf</a>, <a href="/ps/2311.07577" title="Download PostScript">ps</a>, <a href="/format/2311.07577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithms for Object Detection in Substations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bingying Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yadong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Q">Qinlin Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Inspection of high-voltage power equipment is an effective way to ensure
power supply reliability. Object recognition, one of the key technologies in
automatic power equipment inspection, attracts attention of many researchers
and engineers. Although quite a few existing models have some their own
advantages, object relationship between equipment which is very important in
this task is scarcely considered. This paper combining object relationship
modeling and Transformer Model proposes a Relation Transformer Model. It has
four parts -- backbone, encoder, decoder and prediction heads. With this
structure, the proposed method shows in experiments a much better performance
than other three commonly used models in object recognition in substation,
largely promoting the development of automatic power equipment inspection.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07578" title="Abstract">arXiv:2311.07578</a> [<a href="/pdf/2311.07578" title="Download PDF">pdf</a>, <a href="/format/2311.07578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Metacognitive Approach to Out-of-Distribution Detection for  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gummadi%2C+M">Meghna Gummadi</a>, 
<a href="/search/cs?searchtype=author&query=Kent%2C+C">Cassandra Kent</a>, 
<a href="/search/cs?searchtype=author&query=Schmeckpeper%2C+K">Karl Schmeckpeper</a>, 
<a href="/search/cs?searchtype=author&query=Eaton%2C+E">Eric Eaton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite outstanding semantic scene segmentation in closed-worlds, deep neural
networks segment novel instances poorly, which is required for autonomous
agents acting in an open world. To improve out-of-distribution (OOD) detection
for segmentation, we introduce a metacognitive approach in the form of a
lightweight module that leverages entropy measures, segmentation predictions,
and spatial context to characterize the segmentation model's uncertainty and
detect pixel-wise OOD data in real-time. Additionally, our approach
incorporates a novel method of generating synthetic OOD data in context with
in-distribution data, which we use to fine-tune existing segmentation models
with maximum entropy training. This further improves the metacognitive module's
performance without requiring access to OOD data while enabling compatibility
with established pre-trained models. Our resulting approach can reliably detect
OOD instances in a scene, as shown by state-of-the-art performance on OOD
detection for semantic segmentation benchmarks.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07579" title="Abstract">arXiv:2311.07579</a> [<a href="/pdf/2311.07579" title="Download PDF">pdf</a>, <a href="/format/2311.07579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relative intrinsic dimensionality is intrinsic to learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sutton%2C+O+J">Oliver J. Sutton</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qinghua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gorban%2C+A+N">Alexander N. Gorban</a>, 
<a href="/search/cs?searchtype=author&query=Tyukin%2C+I+Y">Ivan Y. Tyukin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artificial Neural Networks and Machine Learning ICANN 2023.
  Lecture Notes in Computer Science, vol 14254, pp 516-529. Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">High dimensional data can have a surprising property: pairs of data points
may be easily separated from each other, or even from arbitrary subsets, with
high probability using just simple linear classifiers. However, this is more of
a rule of thumb than a reliable property as high dimensionality alone is
neither necessary nor sufficient for successful learning. Here, we introduce a
new notion of the intrinsic dimension of a data distribution, which precisely
captures the separability properties of the data. For this intrinsic dimension,
the rule of thumb above becomes a law: high intrinsic dimension guarantees
highly separable data. We extend this notion to that of the relative intrinsic
dimension of two data distributions, which we show provides both upper and
lower bounds on the probability of successfully learning and generalising in a
binary classification problem
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07582" title="Abstract">arXiv:2311.07582</a> [<a href="/pdf/2311.07582" title="Download PDF">pdf</a>, <a href="/ps/2311.07582" title="Download PostScript">ps</a>, <a href="/format/2311.07582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Potential of Leading Large Language Models in Reasoning  Biology Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xinyu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Holmes%2C+J">Jason Holmes</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Q">Qi Gan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yusong Zou</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Y">Yuxi Teng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Tian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hongtu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yajun Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advances in Large Language Models (LLMs) have presented new
opportunities for integrating Artificial General Intelligence (AGI) into
biological research and education. This study evaluated the capabilities of
leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in
answering conceptual biology questions. The models were tested on a
108-question multiple-choice exam covering biology topics in molecular biology,
biological techniques, metabolic engineering, and synthetic biology. Among the
models, GPT-4 achieved the highest average score of 90 and demonstrated the
greatest consistency across trials with different prompts. The results
indicated GPT-4's proficiency in logical reasoning and its potential to aid
biology research through capabilities like data analysis, hypothesis
generation, and knowledge integration. However, further development and
validation are still required before the promise of LLMs in accelerating
biological discovery can be realized.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07583" title="Abstract">arXiv:2311.07583</a> [<a href="/pdf/2311.07583" title="Download PDF">pdf</a>, <a href="/ps/2311.07583" title="Download PostScript">ps</a>, <a href="/format/2311.07583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Dialect Sentence Transformation: A Comparative Analysis of  Language Models for Adapting Sentences to British English
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Shruti Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Mookherjee%2C+S">Shashwat Mookherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study explores linguistic distinctions among American, Indian, and Irish
English dialects and assesses various Language Models (LLMs) in their ability
to generate British English translations from these dialects. Using cosine
similarity analysis, the study measures the linguistic proximity between
original British English translations and those produced by LLMs for each
dialect. The findings reveal that Indian and Irish English translations
maintain notably high similarity scores, suggesting strong linguistic alignment
with British English. In contrast, American English exhibits slightly lower
similarity, reflecting its distinct linguistic traits. Additionally, the choice
of LLM significantly impacts translation quality, with Llama-2-70b consistently
demonstrating superior performance. The study underscores the importance of
selecting the right model for dialect translation, emphasizing the role of
linguistic expertise and contextual understanding in achieving accurate
translations.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07584" title="Abstract">arXiv:2311.07584</a> [<a href="/pdf/2311.07584" title="Download PDF">pdf</a>, <a href="/ps/2311.07584" title="Download PostScript">ps</a>, <a href="/format/2311.07584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Prediction of Data-Driven Knowledge summarization of High  Entropy Alloys (HEAs) literature implementing Natural Language Processing  algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+A">Akshansh Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Jatti%2C+V+S">Vijaykumar S Jatti</a>, 
<a href="/search/cs?searchtype=author&query=More%2C+V">Vaishnavi More</a>, 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+A">Anish Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Dixit%2C+D">Devarrishi Dixit</a>, 
<a href="/search/cs?searchtype=author&query=Sefene%2C+E+M">Eyob Messele Sefene</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">The ability to interpret spoken language is connected to natural language
processing. It involves teaching the AI how words relate to one another, how
they are meant to be used, and in what settings. The goal of natural language
processing (NLP) is to get a machine intelligence to process words the same way
a human brain does. This enables machine intelligence to interpret, arrange,
and comprehend textual data by processing the natural language. The technology
can comprehend what is communicated, whether it be through speech or writing
because AI pro-cesses language more quickly than humans can. In the present
study, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic
Analysis (LSA), and Kull-back-Liebler (KL) al-gorithm, are implemented for the
first time for the knowledge summarization purpose of the High Entropy Alloys
(HEAs). The performance prediction of these algorithms is made by using the
BLEU score and ROUGE score. The results showed that the Luhn algorithm has the
highest accuracy score for the knowledge summarization tasks compared to the
other used algorithms.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07585" title="Abstract">arXiv:2311.07585</a> [<a href="/pdf/2311.07585" title="Download PDF">pdf</a>, <a href="/format/2311.07585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Input Reconstruction Attack against Vertical Federated Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Fei Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Recently, large language models (LLMs) have drawn extensive attention from
academia and the public, due to the advent of the ChatGPT. While LLMs show
their astonishing ability in text generation for various tasks, privacy
concerns limit their usage in real-life businesses. More specifically, either
the user's inputs (the user sends the query to the model-hosting server) or the
model (the user downloads the complete model) itself will be revealed during
the usage. Vertical federated learning (VFL) is a promising solution to this
kind of problem. It protects both the user's input and the knowledge of the
model by splitting the model into a bottom part and a top part, which is
maintained by the user and the model provider, respectively. However, in this
paper, we demonstrate that in LLMs, VFL fails to protect the user input since
it is simple and cheap to reconstruct the input from the intermediate
embeddings. Experiments show that even with a commercial GPU, the input
sentence can be reconstructed in only one second. We also discuss several
possible solutions to enhance the privacy of vertical federated LLMs.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07586" title="Abstract">arXiv:2311.07586</a> [<a href="/pdf/2311.07586" title="Download PDF">pdf</a>, <a href="/format/2311.07586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event Detection on Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahin%2C+O+C">Ozlem Ceren Sahin</a>, 
<a href="/search/cs?searchtype=author&query=Tatbul%2C+N">Nesime Tatbul</a>, 
<a href="/search/cs?searchtype=author&query=Karagoz%2C+P">Pinar Karagoz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Detecting events by using social media has been an active research problem.
In this work, we investigate and compare the performance of two methods for
event detection in Twitter by using Apache Storm as the stream processing
infrastructure. The first event detection method is based on identifying
uncommonly common words inside tweet blocks, and the second one is based on
clustering tweets to detect a cluster as an event. Each of the methods has its
own characteristics. Uncommonly common word based method relies on the burst of
words and hence is not affected from concurrency problems in distributed
environment. On the other hand, clustering based method includes a finer
grained analysis, but it is sensitive to the concurrent processing. We
investigate the effect of stream processing and concurrency handling support
provided by Apace Storm on event detection by these methods.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07587" title="Abstract">arXiv:2311.07587</a> [<a href="/pdf/2311.07587" title="Download PDF">pdf</a>, <a href="/format/2311.07587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frontier Language Models are not Robust to Adversarial Arithmetic, or  &quot;What do I need to say so you agree 2+2=5?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freeman%2C+C+D">C. Daniel Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Culp%2C+L">Laura Culp</a>, 
<a href="/search/cs?searchtype=author&query=Parisi%2C+A">Aaron Parisi</a>, 
<a href="/search/cs?searchtype=author&query=Bileschi%2C+M+L">Maxwell L Bileschi</a>, 
<a href="/search/cs?searchtype=author&query=Elsayed%2C+G+F">Gamaleldin F Elsayed</a>, 
<a href="/search/cs?searchtype=author&query=Rizkowsky%2C+A">Alex Rizkowsky</a>, 
<a href="/search/cs?searchtype=author&query=Simpson%2C+I">Isabelle Simpson</a>, 
<a href="/search/cs?searchtype=author&query=Alemi%2C+A">Alex Alemi</a>, 
<a href="/search/cs?searchtype=author&query=Nova%2C+A">Azade Nova</a>, 
<a href="/search/cs?searchtype=author&query=Adlam%2C+B">Ben Adlam</a>, 
<a href="/search/cs?searchtype=author&query=Bohnet%2C+B">Bernd Bohnet</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+G">Gaurav Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Sedghi%2C+H">Hanie Sedghi</a>, 
<a href="/search/cs?searchtype=author&query=Mordatch%2C+I">Igor Mordatch</a>, 
<a href="/search/cs?searchtype=author&query=Gur%2C+I">Izzeddin Gur</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaehoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Co-Reyes%2C+J">JD Co-Reyes</a>, 
<a href="/search/cs?searchtype=author&query=Pennington%2C+J">Jeffrey Pennington</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kelvin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Swersky%2C+K">Kevin Swersky</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+K">Kshiteej Mahajan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Lechao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rosanne Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kornblith%2C+S">Simon Kornblith</a>, 
<a href="/search/cs?searchtype=author&query=Constant%2C+N">Noah Constant</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P+J">Peter J. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Novak%2C+R">Roman Novak</a>, 
<a href="/search/cs?searchtype=author&query=Vikram%2C+S">Sharad Vikram</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yundi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Fiedel%2C+N">Noah Fiedel</a>, 
<a href="/search/cs?searchtype=author&query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce and study the problem of adversarial arithmetic, which provides
a simple yet challenging testbed for language model alignment. This problem is
comprised of arithmetic questions posed in natural language, with an arbitrary
adversarial string inserted before the question is complete. Even in the simple
setting of 1-digit addition problems, it is easy to find adversarial prompts
that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and
even to steer models to a particular wrong answer. We additionally provide a
simple algorithm for finding successful attacks by querying those same models,
which we name "prompt inversion rejection sampling" (PIRS). We finally show
that models can be partially hardened against these attacks via reinforcement
learning and via agentic constitutional loops. However, we were not able to
make a language model fully robust against adversarial arithmetic attacks.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07588" title="Abstract">arXiv:2311.07588</a> [<a href="/pdf/2311.07588" title="Download PDF">pdf</a>, <a href="/format/2311.07588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLQxform: A Language Model-based Question to SPARQL Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiruo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Rossetto%2C+L">Luca Rossetto</a>, 
<a href="/search/cs?searchtype=author&query=Ruosch%2C+F">Florian Ruosch</a>, 
<a href="/search/cs?searchtype=author&query=Bernstein%2C+A">Abraham Bernstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, scholarly data has grown dramatically in terms of both scale
and complexity. It becomes increasingly challenging to retrieve information
from scholarly knowledge graphs that include large-scale heterogeneous
relationships, such as authorship, affiliation, and citation, between various
types of entities, e.g., scholars, papers, and organizations. As part of the
Scholarly QALD Challenge, this paper presents a question-answering (QA) system
called NLQxform, which provides an easy-to-use natural language interface to
facilitate accessing scholarly knowledge graphs. NLQxform allows users to
express their complex query intentions in natural language questions. A
transformer-based language model, i.e., BART, is employed to translate
questions into standard SPARQL queries, which can be evaluated to retrieve the
required information. According to the public leaderboard of the Scholarly QALD
Challenge at ISWC 2023 (Task 1: DBLP-QUAD - Knowledge Graph Question Answering
over DBLP), NLQxform achieved an F1 score of 0.85 and ranked first on the QA
task, demonstrating the competitiveness of the system.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07589" title="Abstract">arXiv:2311.07589</a> [<a href="/pdf/2311.07589" title="Download PDF">pdf</a>, <a href="/format/2311.07589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dialogizer: Context-aware Conversational-QA Dataset Generation from  Textual Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hwang%2C+Y">Yerin Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yongil Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+H">Hyunkyung Bae</a>, 
<a href="/search/cs?searchtype=author&query=Bang%2C+J">Jeesoo Bang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwanhee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+K">Kyomin Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To address the data scarcity issue in Conversational question answering
(ConvQA), a dialog inpainting method, which utilizes documents to generate
ConvQA datasets, has been proposed. However, the original dialog inpainting
model is trained solely on the dialog reconstruction task, resulting in the
generation of questions with low contextual relevance due to insufficient
learning of question-answer alignment. To overcome this limitation, we propose
a novel framework called Dialogizer, which has the capability to automatically
generate ConvQA datasets with high contextual relevance from textual sources.
The framework incorporates two training tasks: question-answer matching (QAM)
and topic-aware dialog generation (TDG). Moreover, re-ranking is conducted
during the inference phase based on the contextual relevance of the generated
questions. Using our framework, we produce four ConvQA datasets by utilizing
documents from multiple domains as the primary source. Through automatic
evaluation using diverse metrics, as well as human evaluation, we validate that
our proposed framework exhibits the ability to generate datasets of higher
quality compared to the baseline dialog inpainting model.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07590" title="Abstract">arXiv:2311.07590</a> [<a href="/pdf/2311.07590" title="Download PDF">pdf</a>, <a href="/format/2311.07590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical Report: Large Language Models can Strategically Deceive their  Users when Put Under Pressure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scheurer%2C+J">J&#xe9;r&#xe9;my Scheurer</a>, 
<a href="/search/cs?searchtype=author&query=Balesni%2C+M">Mikita Balesni</a>, 
<a href="/search/cs?searchtype=author&query=Hobbhahn%2C+M">Marius Hobbhahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We demonstrate a situation in which Large Language Models, trained to be
helpful, harmless, and honest, can display misaligned behavior and
strategically deceive their users about this behavior without being instructed
to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated
environment, where it assumes the role of an autonomous stock trading agent.
Within this environment, the model obtains an insider tip about a lucrative
stock trade and acts upon it despite knowing that insider trading is
disapproved of by company management. When reporting to its manager, the model
consistently hides the genuine reasons behind its trading decision. We perform
a brief investigation of how this behavior varies under changes to the setting,
such as removing model access to a reasoning scratchpad, attempting to prevent
the misaligned behavior by changing system instructions, changing the amount of
pressure the model is under, varying the perceived risk of getting caught, and
making other simple changes to the environment. To our knowledge, this is the
first demonstration of Large Language Models trained to be helpful, harmless,
and honest, strategically deceiving their users in a realistic situation
without direct instructions or training for deception.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07591" title="Abstract">arXiv:2311.07591</a> [<a href="/pdf/2311.07591" title="Download PDF">pdf</a>, <a href="/ps/2311.07591" title="Download PostScript">ps</a>, <a href="/format/2311.07591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification of Books That are Suitable for Middle School Students  Using Artificial Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niksarli%2C+A">Alp Niksarli</a>, 
<a href="/search/cs?searchtype=author&query=Gorgu%2C+S+O">Sadik Ozan Gorgu</a>, 
<a href="/search/cs?searchtype=author&query=Gencer%2C+E">Ege Gencer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reading right books contributes to children's imagination and brain
development, enhances their language and emotional comprehension abilities, and
strengthens their relationships with others. Building upon the critical role of
reading books in individual development, this paper aims to develop an
algorithm that determines the suitability of books for middle school students
by analyzing their structural and semantic features. Using methods described,
an algorithm will be created that can be utilized by institutions and
individuals responsible for children's education, such as the Ministry of
National Education officials and schools. This algorithm will facilitate the
selection of books to be taught at the middle school level. With the algorithm,
the book selection process for the middle school curriculum can be expedited,
and it will serve as a preliminary reference source for those who evaluate
books by reading them. In this paper, the Python programming language was
employed, utilizing natural language processing methods. Additionally, an
artificial neural network (ANN) was trained using the data which had been
preprocessed to construct an original dataset. To train this network, suitable
books for middle school students were provided by the MEB, Oxford and Cambridge
and with content assessed based on the "R" criterion, and inappropriate books
for middle school students in terms of content were included. This trained
neural network achieved a 90.06% consistency rate in determining the
appropriateness of the test-provided books. Considering the obtained findings,
it can be concluded that the developed software has achieved the desired
objective.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07592" title="Abstract">arXiv:2311.07592</a> [<a href="/pdf/2311.07592" title="Download PDF">pdf</a>, <a href="/format/2311.07592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hallucination-minimized Data-to-answer Framework for Financial  Decision-makers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roychowdhury%2C+S">Sohini Roychowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Alvarez%2C+A">Andres Alvarez</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+B">Brian Moore</a>, 
<a href="/search/cs?searchtype=author&query=Krema%2C+M">Marko Krema</a>, 
<a href="/search/cs?searchtype=author&query=Gelpi%2C+M+P">Maria Paz Gelpi</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+F+M">Federico Martin Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+A">Angel Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Cabrejas%2C+J+R">Jose Ramon Cabrejas</a>, 
<a href="/search/cs?searchtype=author&query=Serrano%2C+P+M">Pablo Martinez Serrano</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Punit Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Arijit Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Large Language Models (LLMs) have been applied to build several automation
and personalized question-answering prototypes so far. However, scaling such
prototypes to robust products with minimized hallucinations or fake responses
still remains an open challenge, especially in niche data-table heavy domains
such as financial decision making. In this work, we present a novel
Langchain-based framework that transforms data tables into hierarchical textual
data chunks to enable a wide variety of actionable question answering. First,
the user-queries are classified by intention followed by automated retrieval of
the most relevant data chunks to generate customized LLM prompts per query.
Next, the custom prompts and their responses undergo multi-metric scoring to
assess for hallucinations and response confidence. The proposed system is
optimized with user-query intention classification, advanced prompting, data
scaling capabilities and it achieves over 90% confidence scores for a variety
of user-queries responses ranging from {What, Where, Why, How, predict, trend,
anomalies, exceptions} that are crucial for financial decision making
applications. The proposed data to answers framework can be extended to other
analytical domains such as sales and payroll to ensure optimal hallucination
control guardrails.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07593" title="Abstract">arXiv:2311.07593</a> [<a href="/pdf/2311.07593" title="Download PDF">pdf</a>, <a href="/format/2311.07593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Follow-Up Differential Descriptions: Language Models Resolve Ambiguities  for Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esfandiarpoor%2C+R">Reza Esfandiarpoor</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+S+H">Stephen H. Bach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/BatsResearch/fudd">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">A promising approach for improving the performance of vision-language models
like CLIP for image classification is to extend the class descriptions (i.e.,
prompts) with related attributes, e.g., using brown sparrow instead of sparrow.
However, current zero-shot methods select a subset of attributes regardless of
commonalities between the target classes, potentially providing no useful
information that would have helped to distinguish between them. For instance,
they may use color instead of bill shape to distinguish between sparrows and
wrens, which are both brown. We propose Follow-up Differential Descriptions
(FuDD), a zero-shot approach that tailors the class descriptions to each
dataset and leads to additional attributes that better differentiate the target
classes. FuDD first identifies the ambiguous classes for each image, and then
uses a Large Language Model (LLM) to generate new class descriptions that
differentiate between them. The new class descriptions resolve the initial
ambiguity and help predict the correct label. In our experiments, FuDD
consistently outperforms generic description ensembles and naive LLM-generated
descriptions on 12 datasets. We show that differential descriptions are an
effective tool to resolve class ambiguities, which otherwise significantly
degrade the performance. We also show that high quality natural language class
descriptions produced by FuDD result in comparable performance to few-shot
adaptation methods.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07594" title="Abstract">arXiv:2311.07594</a> [<a href="/pdf/2311.07594" title="Download PDF">pdf</a>, <a href="/format/2311.07594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Bridge the Gap between Modalities: A Comprehensive Survey on  Multimodal Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shezheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaopeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shasha Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">This review paper explores Multimodal Large Language Models (MLLMs), which
integrate Large Language Models (LLMs) like GPT-4 to handle multimodal data
such as text and vision. MLLMs demonstrate capabilities like generating image
narratives and answering image-based questions, bridging the gap towards
real-world human-computer interactions and hinting at a potential pathway to
artificial general intelligence. However, MLLMs still face challenges in
processing the semantic gap in multimodality, which may lead to erroneous
generation, posing potential risks to society. Choosing the appropriate
modality alignment method is crucial, as improper methods might require more
parameters with limited performance improvement. This paper aims to explore
modality alignment methods for LLMs and their existing capabilities.
Implementing modality alignment allows LLMs to address environmental issues and
enhance accessibility. The study surveys existing modal alignment methods in
MLLMs into four groups: (1) Multimodal Converters that change data into
something LLMs can understand; (2) Multimodal Perceivers to improve how LLMs
perceive different types of data; (3) Tools Assistance for changing data into
one common format, usually text; and (4) Data-Driven methods that teach LLMs to
understand specific types of data in a dataset. This field is still in a phase
of exploration and experimentation, and we will organize and update various
existing research methods for multimodal information alignment.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07595" title="Abstract">arXiv:2311.07595</a> [<a href="/pdf/2311.07595" title="Download PDF">pdf</a>, <a href="/ps/2311.07595" title="Download PostScript">ps</a>, <a href="/format/2311.07595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Decision Support System for Liver Diseases Prediction: Integrating  Batch Processing, Rule-Based Event Detection and SPARQL Query
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chandra%2C+R">Ritesh Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+S">Sadhana Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Rastogi%2C+S">Satyam Rastogi</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Sonali Agarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Liver diseases pose a significant global health burden, impacting a
substantial number of individuals and exerting substantial economic and social
consequences. Rising liver problems are considered a fatal disease in many
countries, such as Egypt, Molda, etc. The objective of this study is to
construct a predictive model for liver illness using Basic Formal Ontology
(BFO) and detection rules derived from a decision tree algorithm. Based on
these rules, events are detected through batch processing using the Apache Jena
framework. Based on the event detected, queries can be directly processed using
SPARQL. To make the ontology operational, these Decision Tree (DT) rules are
converted into Semantic Web Rule Language (SWRL). Using this SWRL in the
ontology for predicting different types of liver disease with the help of the
Pellet and Drool inference engines in Protege Tools, a total of 615 records are
taken from different liver diseases. After inferring the rules, the result can
be generated for the patient according to the DT rules, and other
patient-related details along with different precautionary suggestions can be
obtained based on these results. Combining query results of batch processing
and ontology-generated results can give more accurate suggestions for disease
prevention and detection. This work aims to provide a comprehensive approach
that is applicable for liver disease prediction, rich knowledge graph
representation, and smart querying capabilities. The results show that
combining RDF data, SWRL rules, and SPARQL queries for analysing and predicting
liver disease can help medical professionals to learn more about liver diseases
and make a Decision Support System (DSS) for health care.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07596" title="Abstract">arXiv:2311.07596</a> [<a href="/pdf/2311.07596" title="Download PDF">pdf</a>, <a href="/ps/2311.07596" title="Download PostScript">ps</a>, <a href="/format/2311.07596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph GOSPA metric: a metric to measure the discrepancy between graphs  of different sizes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinhao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Fern%C3%A1ndez%2C+%C3%81+F">&#xc1;ngel F. Garc&#xed;a-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Firth%2C+R+E">Robert E. Firth</a>, 
<a href="/search/cs?searchtype=author&query=Svensson%2C+L">Lennart Svensson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper proposes a metric to measure the dissimilarity between graphs that
may have a different number of nodes. The proposed metric extends the
generalised optimal subpattern assignment (GOSPA) metric, which is a metric for
sets, to graphs. The proposed graph GOSPA metric includes costs associated with
node attribute errors for properly assigned nodes, missed and false nodes and
edge mismatches between graphs. The computation of this metric is based on
finding the optimal assignments between nodes in the two graphs, with the
possibility of leaving some of the nodes unassigned. We also propose a lower
bound for the metric, which is also a metric for graphs and is computable in
polynomial time using linear programming. The metric is first derived for
undirected unweighted graphs and it is then extended to directed and weighted
graphs. The properties of the metric are demonstrated via simulated and
empirical datasets.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07597" title="Abstract">arXiv:2311.07597</a> [<a href="/pdf/2311.07597" title="Download PDF">pdf</a>, <a href="/format/2311.07597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Actuarial Non-Life Pricing Models via Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brauer%2C+A">Alexej Brauer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Statistical Finance (q-fin.ST); Applications (stat.AP)

</div>
<p class="mathjax">Currently, there is a lot of research in the field of neural networks for
non-life insurance pricing. The usual goal is to improve the predictive power
via neural networks while building upon the generalized linear model, which is
the current industry standard. Our paper contributes to this current journey
via novel methods to enhance actuarial non-life models with transformer models
for tabular data. We build here upon the foundation laid out by the combined
actuarial neural network as well as the localGLMnet and enhance those models
via the feature tokenizer transformer. The manuscript demonstrates the
performance of the proposed methods on a real-world claim frequency dataset and
compares them with several benchmark models such as generalized linear models,
feed-forward neural networks, combined actuarial neural networks, LocalGLMnet,
and pure feature tokenizer transformer. The paper shows that the new methods
can achieve better results than the benchmark models while preserving certain
generalized linear model advantages. The paper also discusses the practical
implications and challenges of applying transformer models in actuarial
settings.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07599" title="Abstract">arXiv:2311.07599</a> [<a href="/pdf/2311.07599" title="Download PDF">pdf</a>, <a href="/format/2311.07599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing LLMs on Code Generation with Varying Levels of Prompt  Specificity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murr%2C+L">Lincoln Murr</a>, 
<a href="/search/cs?searchtype=author&query=Grainger%2C+M">Morgan Grainger</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+D">David Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated unparalleled prowess in
mimicking human-like text generation and processing. Among the myriad of
applications that benefit from LLMs, automated code generation is increasingly
promising. The potential to transform natural language prompts into executable
code promises a major shift in software development practices and paves the way
for significant reductions in manual coding efforts and the likelihood of
human-induced errors. This paper reports the results of a study that evaluates
the performance of various LLMs, such as Bard, ChatGPT-3.5, ChatGPT-4, and
Claude-2, in generating Python for coding problems. We focus on how levels of
prompt specificity impact the accuracy, time efficiency, and space efficiency
of the generated code. A benchmark of 104 coding problems, each with four types
of prompts with varying degrees of tests and specificity, was employed to
examine these aspects comprehensively. Our results indicate significant
variations in performance across different LLMs and prompt types, and its key
contribution is to reveal the ideal prompting strategy for creating accurate
Python functions. This study lays the groundwork for further research in LLM
capabilities and suggests practical implications for utilizing LLMs in
automated code generation tasks and test-driven development.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07600" title="Abstract">arXiv:2311.07600</a> [<a href="/pdf/2311.07600" title="Download PDF">pdf</a>, <a href="/format/2311.07600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polarimetric PatchMatch Multi-View Stereo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Oishi%2C+J">Jumpei Oishi</a>, 
<a href="/search/cs?searchtype=author&query=Monno%2C+Y">Yusuke Monno</a>, 
<a href="/search/cs?searchtype=author&query=Okutomi%2C+M">Masatoshi Okutomi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">PatchMatch Multi-View Stereo (PatchMatch MVS) is one of the popular MVS
approaches, owing to its balanced accuracy and efficiency. In this paper, we
propose Polarimetric PatchMatch multi-view Stereo (PolarPMS), which is the
first method exploiting polarization cues to PatchMatch MVS. The key of
PatchMatch MVS is to generate depth and normal hypotheses, which form local 3D
planes and slanted stereo matching windows, and efficiently search for the best
hypothesis based on the consistency among multi-view images. In addition to
standard photometric consistency, our PolarPMS evaluates polarimetric
consistency to assess the validness of a depth and normal hypothesis, motivated
by the physical property that the polarimetric information is related to the
object's surface normal. Experimental results demonstrate that our PolarPMS can
improve the accuracy and the completeness of reconstructed 3D models,
especially for texture-less surfaces, compared with state-of-the-art PatchMatch
MVS methods.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07601" title="Abstract">arXiv:2311.07601</a> [<a href="/pdf/2311.07601" title="Download PDF">pdf</a>, <a href="/format/2311.07601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Advertisements with LLMs: Opportunities and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>, 
<a href="/search/cs?searchtype=author&query=Hajiaghayi%2C+M">MohammadTaghi Hajiaghayi</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+K">Keivan Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Suho Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper explores the potential for leveraging Large Language Models (LLM)
in the realm of online advertising systems. We delve into essential
requirements including privacy, latency, reliability, users and advertisers'
satisfaction, which such a system must fulfill. We further introduce a general
framework for LLM advertisement, consisting of modification, bidding,
prediction, and auction modules. Different design considerations for each
module is presented, with an in-depth examination of their practicality and the
technical challenges inherent to their implementation.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07602" title="Abstract">arXiv:2311.07602</a> [<a href="/pdf/2311.07602" title="Download PDF">pdf</a>, <a href="/format/2311.07602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cache Optimization and Performance Modeling of Batched, Small, and  Rectangular Matrix Multiplication on Intel, AMD, and Fujitsu Processors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deshmukh%2C+S">Sameer Deshmukh</a>, 
<a href="/search/cs?searchtype=author&query=Yokota%2C+R">Rio Yokota</a>, 
<a href="/search/cs?searchtype=author&query=Bosilca%2C+G">George Bosilca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Mathematical Software (cs.MS)

</div>
<p class="mathjax">Factorization and multiplication of dense matrices and tensors are critical,
yet extremely expensive pieces of the scientific toolbox. Careful use of low
rank approximation can drastically reduce the computation and memory
requirements of these operations. In addition to a lower arithmetic complexity,
such methods can, by their structure, be designed to efficiently exploit modern
hardware architectures. The majority of existing work relies on batched BLAS
libraries to handle the computation of many small dense matrices. We show that
through careful analysis of the cache utilization, register accumulation using
SIMD registers and a redesign of the implementation, one can achieve
significantly higher throughput for these types of batched low-rank matrices
across a large range of block and batch sizes. We test our algorithm on 3 CPUs
using diverse ISAs -- the Fujitsu A64FX using ARM SVE, the Intel Xeon 6148
using AVX-512 and AMD EPYC 7502 using AVX-2, and show that our new batching
methodology is able to obtain more than twice the throughput of vendor
optimized libraries for all CPU architectures and problem sizes.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07603" title="Abstract">arXiv:2311.07603</a> [<a href="/pdf/2311.07603" title="Download PDF">pdf</a>, <a href="/format/2311.07603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PECoP: Parameter Efficient Continual Pretraining for Action Quality  Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dadashzadeh%2C+A">Amirhossein Dadashzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+S">Shuchao Duan</a>, 
<a href="/search/cs?searchtype=author&query=Whone%2C+A">Alan Whone</a>, 
<a href="/search/cs?searchtype=author&query=Mirmehdi%2C+M">Majid Mirmehdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024 (preprint)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The limited availability of labelled data in Action Quality Assessment (AQA),
has forced previous works to fine-tune their models pretrained on large-scale
domain-general datasets. This common approach results in weak generalisation,
particularly when there is a significant domain shift. We propose a novel,
parameter efficient, continual pretraining framework, PECoP, to reduce such
domain shift via an additional pretraining stage. In PECoP, we introduce
3D-Adapters, inserted into the pretrained model, to learn spatiotemporal,
in-domain information via self-supervised learning where only the adapter
modules' parameters are updated. We demonstrate PECoP's ability to enhance the
performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied
to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS
($\uparrow6.0\%$), MTL-AQA ($\uparrow0.99\%$), and FineDiving
($\uparrow2.54\%$). We also present a new Parkinson's Disease dataset, PD4T, of
real patients performing four various actions, where we surpass
($\uparrow3.56\%$) the state-of-the-art in comparison. Our code, pretrained
models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07604" title="Abstract">arXiv:2311.07604</a> [<a href="/pdf/2311.07604" title="Download PDF">pdf</a>, <a href="/format/2311.07604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finetuning Text-to-Image Diffusion Models for Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xudong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+Y">Yongkang Wong</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)

</div>
<p class="mathjax">The rapid adoption of text-to-image diffusion models in society underscores
an urgent need to address their biases. Without interventions, these biases
could propagate a distorted worldview and limit opportunities for minority
groups. In this work, we frame fairness as a distributional alignment problem.
Our solution consists of two main technical contributions: (1) a distributional
alignment loss that steers specific characteristics of the generated images
towards a user-defined target distribution, and (2) biased direct finetuning of
diffusion model's sampling process, which leverages a biased gradient to more
effectively optimize losses defined on the generated images. Empirically, our
method markedly reduces gender, racial, and their intersectional biases for
occupational prompts. Gender bias is significantly reduced even when finetuning
just five soft tokens. Crucially, our method supports diverse perspectives of
fairness beyond absolute equality, which is demonstrated by controlling age to
a $75\%$ young and $25\%$ old distribution while simultaneously debiasing
gender and race. Finally, our method is scalable: it can debias multiple
concepts at once by simply including these prompts in the finetuning data. We
hope our work facilitates the social alignment of T2I generative AI. We will
share code and various debiased diffusion model adaptors.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07605" title="Abstract">arXiv:2311.07605</a> [<a href="/pdf/2311.07605" title="Download PDF">pdf</a>, <a href="/format/2311.07605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conceptual Model Interpreter for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%A4rer%2C+F">Felix H&#xe4;rer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ER Forum 2023, 42nd International Conference on Conceptual Modeling (ER 2023), November 6-9, 2023, Lisbon, PT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
<p class="mathjax">Large Language Models (LLMs) recently demonstrated capabilities for
generating source code in common programming languages. Additionally,
commercial products such as ChatGPT 4 started to provide code interpreters,
allowing for the automatic execution of generated code fragments, instant
feedback, and the possibility to develop and refine in a conversational
fashion. With an exploratory research approach, this paper applies code
generation and interpretation to conceptual models. The concept and prototype
of a conceptual model interpreter is explored, capable of rendering visual
models generated in textual syntax by state-of-the-art LLMs such as Llama~2 and
ChatGPT 4. In particular, these LLMs can generate textual syntax for the
PlantUML and Graphviz modeling software that is automatically rendered within a
conversational user interface. The first result is an architecture describing
the components necessary to interact with interpreters and LLMs through APIs or
locally, providing support for many commercial and open source LLMs and
interpreters. Secondly, experimental results for models generated with ChatGPT
4 and Llama 2 are discussed in two cases covering UML and, on an instance
level, graphs created from custom data. The results indicate the possibility of
modeling iteratively in a conversational fashion.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07607" title="Abstract">arXiv:2311.07607</a> [<a href="/pdf/2311.07607" title="Download PDF">pdf</a>, <a href="/format/2311.07607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Choice via Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Joohwan Ko</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A+A">Andrew A. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Models of choice are a fundamental input to many now-canonical optimization
problems in the field of Operations Management, including assortment,
inventory, and price optimization. Naturally, accurate estimation of these
models from data is a critical step in the application of these optimization
problems in practice, and so it is perhaps surprising that such choice
estimation has to now been accomplished almost exclusively, both in theory and
in practice, (a) without the use of deep learning in any meaningful way, and
(b) via evaluation on limited data with constantly-changing metrics. This is in
stark contrast to the vast majority of similar learning applications, for which
the practice of machine learning suggests that (a) neural network-based models
are typically state-of-the-art, and (b) strict standardization on evaluation
procedures (datasets, metrics, etc.) is crucial. Thus motivated, we first
propose a choice model that is the first to successfully (both theoretically
and practically) leverage a modern neural network architectural concept
(self-attention). Theoretically, we show that our attention-based choice model
is a low-rank generalization of the Halo Multinomial Logit model, a recent
model that parsimoniously captures irrational choice effects and has seen
empirical success. We prove that whereas the Halo-MNL requires $\Omega(m^2)$
data samples to estimate, where $m$ is the number of products, our model
supports a natural nonconvex estimator (in particular, that which a standard
neural network implementation would apply) which admits a near-optimal
stationary point with $O(m)$ samples. We then establish the first
realistic-scale benchmark for choice estimation on real data and use this
benchmark to run the largest evaluation of existing choice models to date. We
find that the model we propose is dominant over both short-term and long-term
data periods.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07608" title="Abstract">arXiv:2311.07608</a> [<a href="/pdf/2311.07608" title="Download PDF">pdf</a>, <a href="/format/2311.07608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuST: Multimodal Spatiotemporal Graph-Transformer for Hospital  Readmission Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+Y">Yan Miao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lequan Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Hospital readmission prediction is considered an essential approach to
decreasing readmission rates, which is a key factor in assessing the quality
and efficacy of a healthcare system. Previous studies have extensively utilized
three primary modalities, namely electronic health records (EHR), medical
images, and clinical notes, to predict hospital readmissions. However, the
majority of these studies did not integrate information from all three
modalities or utilize the spatiotemporal relationships present in the dataset.
This study introduces a novel model called the Multimodal Spatiotemporal
Graph-Transformer (MuST) for predicting hospital readmissions. By employing
Graph Convolution Networks and temporal transformers, we can effectively
capture spatial and temporal dependencies in EHR and chest radiographs. We then
propose a fusion transformer to combine the spatiotemporal features from the
two modalities mentioned above with the features from clinical notes extracted
by a pre-trained, domain-specific transformer. We assess the effectiveness of
our methods using the latest publicly available dataset, MIMIC-IV. The
experimental results indicate that the inclusion of multimodal features in MuST
improves its performance in comparison to unimodal methods. Furthermore, our
proposed pipeline outperforms the current leading methods in the prediction of
hospital readmissions.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07611" title="Abstract">arXiv:2311.07611</a> [<a href="/pdf/2311.07611" title="Download PDF">pdf</a>, <a href="/ps/2311.07611" title="Download PostScript">ps</a>, <a href="/format/2311.07611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intentional Biases in LLM Responses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Badyal%2C+N">Nicklaus Badyal</a>, 
<a href="/search/cs?searchtype=author&query=Jacoby%2C+D">Derek Jacoby</a>, 
<a href="/search/cs?searchtype=author&query=Coady%2C+Y">Yvonne Coady</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this study we intentionally introduce biases into large language model
responses in an attempt to create specific personas for interactive media
purposes. We explore the differences between open source models such as
Falcon-7b and the GPT-4 model from Open AI, and we quantify some differences in
responses afforded by the two systems. We find that the guardrails in the GPT-4
mixture of experts models with a supervisor, while useful in assuring AI
alignment in general, are detrimental in trying to construct personas with a
variety of uncommon viewpoints. This study aims to set the groundwork for
future exploration in intentional biases of large language models such that
these practices can be applied in the creative field, and new forms of media.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07613" title="Abstract">arXiv:2311.07613</a> [<a href="/pdf/2311.07613" title="Download PDF">pdf</a>, <a href="/ps/2311.07613" title="Download PostScript">ps</a>, <a href="/format/2311.07613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Physics-informed Machine Learning-based Control Method for Nonlinear  Dynamic Systems with Highly Noisy Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ma%2C+M">Mason Ma</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jiajie Wu</a>, 
<a href="/search/eess?searchtype=author&query=Post%2C+C">Chase Post</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+T">Tony Shi</a>, 
<a href="/search/eess?searchtype=author&query=Yi%2C+J">Jingang Yi</a>, 
<a href="/search/eess?searchtype=author&query=Schmitz%2C+T">Tony Schmitz</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS)

</div>
<p class="mathjax">This study presents a physics-informed machine learning-based control method
for nonlinear dynamic systems with highly noisy measurements. Existing
data-driven control methods that use machine learning for system identification
cannot effectively cope with highly noisy measurements, resulting in unstable
control performance. To address this challenge, the present study extends
current physics-informed machine learning capabilities for modeling nonlinear
dynamics with control and integrates them into a model predictive control
framework. To demonstrate the capability of the proposed method we test and
validate with two noisy nonlinear dynamic systems: the chaotic Lorenz 3 system,
and turning machine tool. Analysis of the results illustrate that the proposed
method outperforms state-of-the-art benchmarks as measured by both modeling
accuracy and control performance for nonlinear dynamic systems under high-noise
conditions.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07614" title="Abstract">arXiv:2311.07614</a> [<a href="/pdf/2311.07614" title="Download PDF">pdf</a>, <a href="/ps/2311.07614" title="Download PostScript">ps</a>, <a href="/format/2311.07614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of a Dense Fusion Attention Network in Fault Diagnosis of  Centrifugal Fan
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhixia Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaogang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huijie Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Although the deep learning recognition model has been widely used in the
condition monitoring of rotating machinery. However, it is still a challenge to
understand the correspondence between the structure and function of the model
and the diagnosis process. Therefore, this paper discusses embedding
distributed attention modules into dense connections instead of traditional
dense cascading operations. It not only decouples the influence of space and
channel on fault feature adaptive recalibration feature weights, but also forms
a fusion attention function. The proposed dense fusion focuses on the
visualization of the network diagnosis process, which increases the
interpretability of model diagnosis. How to continuously and effectively
integrate different functions to enhance the ability to extract fault features
and the ability to resist noise is answered. Centrifugal fan fault data is used
to verify this network. Experimental results show that the network has stronger
diagnostic performance than other advanced fault diagnostic models.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07615" title="Abstract">arXiv:2311.07615</a> [<a href="/pdf/2311.07615" title="Download PDF">pdf</a>, <a href="/format/2311.07615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Algorithmic Cache Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhavikatti%2C+N">Neil Bhavikatti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 3 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study matrix-matrix multiplication of two matrices, $A$ and $B$, each of
size $n \times n$. This operation results in a matrix $C$ of size $n\times n$.
Our goal is to produce $C$ as efficiently as possible given a cache: a 1-D
limited set of data values that we can work with to perform elementary
operations (additions, multiplications, etc.). That is, we attempt to reuse the
maximum amount of data from $A$, $B$ and $C$ during our computation (or
equivalently, utilize data in the fast-access cache as often as possible).
Firstly, we introduce the matrix-matrix multiplication algorithm. Secondly, we
present a standard two-memory model to simulate the architecture of a computer,
and we explain the LRU (Least Recently Used) Cache policy (which is standard in
most computers). Thirdly, we introduce a basic model Cache Simulator, which
possesses an $\mathcal{O}(M)$ time complexity (meaning we are limited to small
$M$ values). Then we discuss and model the LFU (Least Frequently Used) Cache
policy and the explicit control cache policy. Finally, we introduce the main
result of this paper, the $\mathcal{O}(1)$ Cache Simulator, and use it to
compare, experimentally, the savings of time, energy, and communication
incurred from the ideal cache-efficient algorithm for matrix-matrix
multiplication. The Cache Simulator simulates the amount of data movement that
occurs between the main memory and the cache of the computer. One of the
findings of this project is that, in some cases, there is a significant
discrepancy in communication values between an LRU cache algorithm and explicit
cache control. We propose to alleviate this problem by ``tricking'' the LRU
cache algorithm by updating the timestamp of the data we want to keep in cache
(namely entries of matrix $C$). This enables us to have the benefits of an
explicit cache policy while being constrained by the LRU paradigm (realistic
policy on a CPU).
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07616" title="Abstract">arXiv:2311.07616</a> [<a href="/pdf/2311.07616" title="Download PDF">pdf</a>, <a href="/format/2311.07616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReIDTracker Sea: the technical report of BoaTrack and SeaDronesSee-MOT  challenge at MaCVi of WACV24
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaer Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chong%2C+W">Weitu Chong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-Object Tracking is one of the most important technologies in maritime
computer vision. Our solution tries to explore Multi-Object Tracking in
maritime Unmanned Aerial vehicles (UAVs) and Unmanned Surface Vehicles (USVs)
usage scenarios. Most of the current Multi-Object Tracking algorithms require
complex association strategies and association information (2D location and
motion, 3D motion, 3D depth, 2D appearance) to achieve better performance,
which makes the entire tracking system extremely complex and heavy. At the same
time, most of the current Multi-Object Tracking algorithms still require video
annotation data which is costly to obtain for training. Our solution tries to
explore Multi-Object Tracking in a completely unsupervised way. The scheme
accomplishes instance representation learning by using self-supervision on
ImageNet. Then, by cooperating with high-quality detectors, the multi-target
tracking task can be completed simply and efficiently. The scheme achieved top
3 performance on both UAV-based Multi-Object Tracking with Reidentification and
USV-based Multi-Object Tracking benchmarks and the solution won the
championship in many multiple Multi-Object Tracking competitions. such as
BDD100K MOT,MOTS, Waymo 2D MOT
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07617" title="Abstract">arXiv:2311.07617</a> [<a href="/pdf/2311.07617" title="Download PDF">pdf</a>, <a href="/format/2311.07617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLAMP: A Contrastive Language And Molecule Pre-training Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Redkar%2C+N">Neel Redkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 1 figure, Presenting @ NeurIPS23 &amp; Workshop - source @ <a href="https://github.com/neelr/clamp">this https URL</a> - dataset @ <a href="https://www.kaggle.com/datasets/programgeek01/cif-summary-data">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">This paper highlights a shift in how to approach material generation. Instead
of material-to-material, we propose a language-to-material generation
architecture that utilizes millions of untapped data points. Using a web
scraper to collect crystal text pairs from open-source research papers, a
contrastive model can be trained using a convolutional graph neural network
encoder and a language encoder. This would allow unsupervised zero-shot
classification which can be trained by taking advantage of linguistic
structure. Without any specific training data, an ~82\% accuracy was achieved
and ~75\% accuracy for photocatalyst prediction with an extremely small
dataset. This novel network could ideally be cross-applied to any reaction that
can be described via text, opening completely new methods to think about 3D
chemical framework generation. In the full experiment diffusion models would
likely be incorporated to fully exploit the latent space.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07618" title="Abstract">arXiv:2311.07618</a> [<a href="/pdf/2311.07618" title="Download PDF">pdf</a>, <a href="/format/2311.07618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models&#x27; Understanding of Math: Source Criticism and  Extrapolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yousefzadeh%2C+R">Roozbeh Yousefzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xuenan Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); History and Overview (math.HO)

</div>
<p class="mathjax">It has been suggested that large language models such as GPT-4 have acquired
some form of understanding beyond the correlations among the words in text
including some understanding of mathematics as well. Here, we perform a
critical inquiry into this claim by evaluating the mathematical understanding
of the GPT-4 model. Considering that GPT-4's training set is a secret, it is
not straightforward to evaluate whether the model's correct answers are based
on a mathematical understanding or based on replication of proofs that the
model has seen before. We specifically craft mathematical questions which their
formal proofs are not readily available on the web, proofs that are more likely
not seen by the GPT-4. We see that GPT-4 is unable to solve those problems
despite their simplicity. It is hard to find scientific evidence suggesting
that GPT-4 has acquired an understanding of even basic mathematical concepts. A
straightforward way to find failure modes of GPT-4 in theorem proving is to
craft questions where their formal proofs are not available on the web. Our
finding suggests that GPT-4's ability is to reproduce, rephrase, and polish the
mathematical proofs that it has seen before, and not in grasping mathematical
concepts. We also see that GPT-4's ability to prove mathematical theorems is
continuously expanding over time despite the claim that it is a fixed model. We
suggest that the task of proving mathematical theorems in formal language is
comparable to the methods used in search engines such as Google while
predicting the next word in a sentence may be a misguided approach, a recipe
that often leads to excessive extrapolation and eventual failures. Prompting
the GPT-4 over and over may benefit the GPT-4 and the OpenAI, but we question
whether it is valuable for machine learning or for theorem proving.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07619" title="Abstract">arXiv:2311.07619</a> [<a href="/pdf/2311.07619" title="Download PDF">pdf</a>, <a href="/format/2311.07619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling User Viewing Flow using Large Language Models for Article  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zulong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Moufeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+S">Shaoyang Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nan Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yu Gu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Ge Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pagese
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper proposes the User Viewing Flow Modeling (SINGLE) method for the
article recommendation task, which models the user constant preference and
instant interest from user-clicked articles. Specifically, we employ a user
constant viewing flow modeling method to summarize the user's general interest
to recommend articles. We utilize Large Language Models (LLMs) to capture
constant user preferences from previously clicked articles, such as skills and
positions. Then we design the user instant viewing flow modeling method to
build interactions between user-clicked article history and candidate articles.
It attentively reads the representations of user-clicked articles and aims to
learn the user's different interest views to match the candidate article. Our
experimental results on the Alibaba Technology Association (ATA) website show
the advantage of SINGLE, which achieves 2.4% improvements over previous
baseline models in the online A/B test. Our further analyses illustrate that
SINGLE has the ability to build a more tailored recommendation system by
mimicking different article viewing behaviors of users and recommending more
appropriate and diverse articles to match user interests.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07620" title="Abstract">arXiv:2311.07620</a> [<a href="/pdf/2311.07620" title="Download PDF">pdf</a>, <a href="/format/2311.07620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EPIM: Efficient Processing-In-Memory Accelerators based on Epitome
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Daquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhenhua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiashi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The exploration of Processing-In-Memory (PIM) accelerators has garnered
significant attention within the research community. However, the utilization
of large-scale neural networks on Processing-In-Memory (PIM) accelerators
encounters challenges due to constrained on-chip memory capacity. To tackle
this issue, current works explore model compression algorithms to reduce the
size of Convolutional Neural Networks (CNNs). Most of these algorithms either
aim to represent neural operators with reduced-size parameters (e.g.,
quantization) or search for the best combinations of neural operators (e.g.,
neural architecture search). Designing neural operators to align with PIM
accelerators' specifications is an area that warrants further study. In this
paper, we introduce the Epitome, a lightweight neural operator offering
convolution-like functionality, to craft memory-efficient CNN operators for PIM
accelerators (EPIM). On the software side, we evaluate epitomes' latency and
energy on PIM accelerators and introduce a PIM-aware layer-wise design method
to enhance their hardware efficiency. We apply epitome-aware quantization to
further reduce the size of epitomes. On the hardware side, we modify the
datapath of current PIM accelerators to accommodate epitomes and implement a
feature map reuse technique to reduce computation cost. Experimental results
reveal that our 3-bit quantized EPIM-ResNet50 attains 71.59% top-1 accuracy on
ImageNet, reducing crossbar areas by 30.65 times. EPIM surpasses the
state-of-the-art pruning methods on PIM.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07622" title="Abstract">arXiv:2311.07622</a> [<a href="/pdf/2311.07622" title="Download PDF">pdf</a>, <a href="/format/2311.07622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretrain like You Inference: Masked Tuning Improves Zero-Shot Composed  Image Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+H">Hanjiang Lai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Zero-shot composed image retrieval (ZS-CIR), which aims to retrieve a target
image based on textual modifications to a reference image without triplet
labeling, has gained more and more attention. Current ZS-CIR research mainly
relies on two unlabeled pre-trained models: the vision-language model, e.g.,
CLIP, and the Pic2Word/textual inversion model. However, the pre-trained models
and CIR tasks have substantial discrepancies, where the pre-trained models
learn the similarities between vision and language but CIR aims to learn the
modifications of the image guided by text. In this paper, we introduce a novel
unlabeled and pre-trained masked tuning approach to reduce the gap between the
pre-trained model and the downstream CIR task. We first reformulate the
pre-trained vision-language contrastive learning as the CIR task, where we
randomly mask input image patches to generate $\langle$masked image, text,
image$\rangle$ triple from an image-text pair. Then, we propose a masked
tuning, which uses the text and the masked image to learn the modifications of
the original image. With such a simple design, it can learn to capture
fine-grained text-guided modifications. Extensive experimental results
demonstrate the significant superiority of our approach over the baseline
models on three ZS-CIR datasets, including FashionIQ, CIRR, and CIRCO.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07623" title="Abstract">arXiv:2311.07623</a> [<a href="/pdf/2311.07623" title="Download PDF">pdf</a>, <a href="/format/2311.07623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PadChannel: Improving CNN Performance through Explicit Padding Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, submitted to the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In convolutional neural networks (CNNs), padding plays a pivotal role in
preserving spatial dimensions throughout the layers. Traditional padding
techniques do not explicitly distinguish between the actual image content and
the padded regions, potentially causing CNNs to incorrectly interpret the
boundary pixels or regions that resemble boundaries. This ambiguity can lead to
suboptimal feature extraction. To address this, we propose PadChannel, a novel
padding method that encodes padding statuses as an additional input channel,
enabling CNNs to easily distinguish genuine pixels from padded ones. By
incorporating PadChannel into several prominent CNN architectures, we observed
small performance improvements and notable reductions in the variances on the
ImageNet-1K image classification task at marginal increases in the
computational cost. The source code is available at
https://github.com/AussieSeaweed/pad-channel
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07625" title="Abstract">arXiv:2311.07625</a> [<a href="/pdf/2311.07625" title="Download PDF">pdf</a>, <a href="/ps/2311.07625" title="Download PostScript">ps</a>, <a href="/format/2311.07625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Activity Sparsity Complements Weight Sparsity for Efficient RNN  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherji%2C+R">Rishav Mukherji</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6ne%2C+M">Mark Sch&#xf6;ne</a>, 
<a href="/search/cs?searchtype=author&query=Nazeer%2C+K+K">Khaleelulla Khan Nazeer</a>, 
<a href="/search/cs?searchtype=author&query=Mayr%2C+C">Christian Mayr</a>, 
<a href="/search/cs?searchtype=author&query=Subramoney%2C+A">Anand Subramoney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the First MLNCP Workshop @ NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Artificial neural networks open up unprecedented machine learning
capabilities at the cost of ever growing computational requirements.
Sparsifying the parameters, often achieved through weight pruning, has been
identified as a powerful technique to compress the number of model parameters
and reduce the computational operations of neural networks. Yet, sparse
activations, while omnipresent in both biological neural networks and deep
learning systems, have not been fully utilized as a compression technique in
deep learning. Moreover, the interaction between sparse activations and weight
pruning is not fully understood. In this work, we demonstrate that activity
sparsity can compose multiplicatively with parameter sparsity in a recurrent
neural network model based on the GRU that is designed to be activity sparse.
We achieve up to $20\times$ reduction of computation while maintaining
perplexities below $60$ on the Penn Treebank language modeling task. This
magnitude of reduction has not been achieved previously with solely sparsely
connected LSTMs, and the language modeling performance of our model has not
been achieved previously with any sparsely activated recurrent neural networks
or spiking neural networks. Neuromorphic computing devices are especially good
at taking advantage of the dynamic activity sparsity, and our results provide
strong evidence that making deep learning models activity sparse and porting
them to neuromorphic devices can be a viable strategy that does not compromise
on task performance. Our results also drive further convergence of methods from
deep learning and neuromorphic computing for efficient machine learning.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07627" title="Abstract">arXiv:2311.07627</a> [<a href="/pdf/2311.07627" title="Download PDF">pdf</a>, <a href="/ps/2311.07627" title="Download PostScript">ps</a>, <a href="/format/2311.07627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Consistent Diffusion-Based Algorithm for Semi-Supervised Graph  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonald%2C+T">Thomas Bonald</a> (IP Paris), 
<a href="/search/cs?searchtype=author&query=de+Lara%2C+N">Nathan de Lara</a> (IP Paris)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2008.11944">arXiv:2008.11944</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Complex Networks, 2023, Menton, France
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The task of semi-supervised classification aims at assigning labels to all
nodes of a graph based on the labels known for a few nodes, called the seeds.
One of the most popular algorithms relies on the principle of heat diffusion,
where the labels of the seeds are spread by thermoconductance and the
temperature of each node at equilibrium is used as a score function for each
label. In this paper, we prove that this algorithm is not consistent unless the
temperatures of the nodes at equilibrium are centered before scoring. This
crucial step does not only make the algorithm provably consistent on a block
model but brings significant performance gains on real graphs.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07630" title="Abstract">arXiv:2311.07630</a> [<a href="/pdf/2311.07630" title="Download PDF">pdf</a>, <a href="/format/2311.07630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-modal Generative Model for Visual-Guided Binaural Stereo  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaojian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuan Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Binaural stereo audio is recorded by imitating the way the human ear receives
sound, which provides people with an immersive listening experience. Existing
approaches leverage autoencoders and directly exploit visual spatial
information to synthesize binaural stereo, resulting in a limited
representation of visual guidance. For the first time, we propose a visually
guided generative adversarial approach for generating binaural stereo audio
from mono audio. Specifically, we develop a Stereo Audio Generation Model
(SAGM), which utilizes shared spatio-temporal visual information to guide the
generator and the discriminator to work separately. The shared visual
information is updated alternately in the generative adversarial stage,
allowing the generator and discriminator to deliver their respective guided
knowledge while visually sharing. The proposed method learns bidirectional
complementary visual information, which facilitates the expression of visual
guidance in generation. In addition, spatial perception is a crucial attribute
of binaural stereo audio, and thus the evaluation of stereo spatial perception
is essential. However, previous metrics failed to measure the spatial
perception of audio. To this end, a metric to measure the spatial perception of
audio is proposed for the first time. The proposed metric is capable of
measuring the magnitude and direction of spatial perception in the temporal
dimension. Further, considering its function, it is feasible to utilize it
instead of demanding user studies to some extent. The proposed method achieves
state-of-the-art performance on 2 datasets and 5 evaluation metrics.
Qualitative experiments and user studies demonstrate that the method generates
space-realistic stereo audio.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07631" title="Abstract">arXiv:2311.07631</a> [<a href="/pdf/2311.07631" title="Download PDF">pdf</a>, <a href="/ps/2311.07631" title="Download PostScript">ps</a>, <a href="/format/2311.07631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The 4+1 Model of Data Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alvarado%2C+R+C">Rafael C. Alvarado</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; General Literature (cs.GL)

</div>
<p class="mathjax">Data Science is a complex and evolving field, but most agree that it can be
defined as a combination of expertise drawn from three broad areascomputer
science and technology, math and statistics, and domain knowledge -- with the
purpose of extracting knowledge and value from data. Beyond this, the field is
often defined as a series of practical activities ranging from the cleaning and
wrangling of data, to its analysis and use to infer models, to the visual and
rhetorical representation of results to stakeholders and decision-makers. This
essay proposes a model of data science that goes beyond laundry-list
definitions to get at the specific nature of data science and help distinguish
it from adjacent fields such as computer science and statistics. We define data
science as an interdisciplinary field comprising four broad areas of expertise:
value, design, systems, and analytics. A fifth area, practice, integrates the
other four in specific contexts of domain knowledge. We call this the 4+1 model
of data science. Together, these areas belong to every data science project,
even if they are often unconnected and siloed in the academy.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07632" title="Abstract">arXiv:2311.07632</a> [<a href="/pdf/2311.07632" title="Download PDF">pdf</a>, <a href="/format/2311.07632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical  Interactions Discovering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zecheng Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Molecular Networks (q-bio.MN)

</div>
<p class="mathjax">Biomedical information graphs are crucial for interaction discovering of
biomedical information in modern age, such as identification of multifarious
molecular interactions and drug discovery, which attracts increasing interests
in biomedicine, bioinformatics, and human healthcare communities. Nowadays,
more and more graph neural networks have been proposed to learn the entities of
biomedical information and precisely reveal biomedical molecule interactions
with state-of-the-art results. These methods remedy the fading of features from
a far distance but suffer from remedying such problem at the expensive cost of
redundant memory and time. In our paper, we propose a novel Residual Message
Graph Convolution Network (ResMGCN) for fast and precise biomedical interaction
prediction in a different idea. Specifically, instead of enhancing the message
from far nodes, ResMGCN aggregates lower-order information with the next round
higher information to guide the node update to obtain a more meaningful node
representation. ResMGCN is able to perceive and preserve various messages from
the previous layer and high-order information in the current layer with least
memory and time cost to obtain informative representations of biomedical
entities. We conduct experiments on four biomedical interaction network
datasets, including protein-protein, drug-drug, drug-target, and gene-disease
interactions, which demonstrates that ResMGCN outperforms previous
state-of-the-art models while achieving superb effectiveness on both storage
and time.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07633" title="Abstract">arXiv:2311.07633</a> [<a href="/pdf/2311.07633" title="Download PDF">pdf</a>, <a href="/format/2311.07633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking and Benchmarking Predict-then-Optimize Paradigm for  Combinatorial Optimization Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+H">Haoyu Geng</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+H">Han Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runzhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">Numerous web applications rely on solving combinatorial optimization
problems, such as energy cost-aware scheduling, budget allocation on web
advertising, and graph matching on social networks. However, many optimization
problems involve unknown coefficients, and improper predictions of these
factors may lead to inferior decisions which may cause energy wastage,
inefficient resource allocation, inappropriate matching in social networks,
etc. Such a research topic is referred to as "Predict-Then-Optimize (PTO)"
which considers the performance of prediction and decision-making in a unified
system. A noteworthy recent development is the end-to-end methods by directly
optimizing the ultimate decision quality which claims to yield better results
in contrast to the traditional two-stage approach. However, the evaluation
benchmarks in this field are fragmented and the effectiveness of various models
in different scenarios remains unclear, hindering the comprehensive assessment
and fast deployment of these methods. To address these issues, we provide a
comprehensive categorization of current approaches and integrate existing
experimental scenarios to establish a unified benchmark, elucidating the
circumstances under which end-to-end training yields improvements, as well as
the contexts in which it performs ineffectively. We also introduce a new
dataset for the industrial combinatorial advertising problem for inclusive
finance to open-source. We hope the rethinking and benchmarking of PTO could
facilitate more convenient evaluation and deployment, and inspire further
improvements both in the academy and industry within this field.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07634" title="Abstract">arXiv:2311.07634</a> [<a href="/pdf/2311.07634" title="Download PDF">pdf</a>, <a href="/format/2311.07634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ActiveDC: Distribution Calibration for Active Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenshuai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhenhui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+J">Jinzhou Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The pretraining-finetuning paradigm has gained popularity in various computer
vision tasks. In this paradigm, the emergence of active finetuning arises due
to the abundance of large-scale data and costly annotation requirements. Active
finetuning involves selecting a subset of data from an unlabeled pool for
annotation, facilitating subsequent finetuning. However, the use of a limited
number of training samples can lead to a biased distribution, potentially
resulting in model overfitting. In this paper, we propose a new method called
ActiveDC for the active finetuning tasks. Firstly, we select samples for
annotation by optimizing the distribution similarity between the subset to be
selected and the entire unlabeled pool in continuous space. Secondly, we
calibrate the distribution of the selected samples by exploiting implicit
category information in the unlabeled pool. The feature visualization provides
an intuitive sense of the effectiveness of our approach to distribution
calibration. We conducted extensive experiments on three image classification
datasets with different sampling ratios. The results indicate that ActiveDC
consistently outperforms the baseline performance in all image classification
tasks. The improvement is particularly significant when the sampling ratio is
low, with performance gains of up to 10%. Our code will be released.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07635" title="Abstract">arXiv:2311.07635</a> [<a href="/pdf/2311.07635" title="Download PDF">pdf</a>, <a href="/format/2311.07635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Past as a Guide: Leveraging Retrospective Learning for Python Code  Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Seunggyoon Shin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Seunggyu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sungjoon Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips2023 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">This work presents Past as a Guide (PaG), a simple approach for Large
Language Models (LLMs) to improve the coding capabilities by integrating the
past history with interactive and iterative code refinements. To be specific,
inspired by human cognitive processes, the proposed method enables LLMs to
utilize previous programming and debugging experiences to enhance the Python
code completion tasks. The framework facilitates LLMs to iteratively refine the
Python code based on previous execution and debugging results and optimize
learning and reasoning capabilities. The proposed methodology achieved a 92\%
pass@1 on HumanEval, demonstrating the potential to advance the field by
leveraging retrospection from past experiences and interactive and iterative
refinement processes without external correctness indicators.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07677" title="Abstract">arXiv:2311.07677</a> [<a href="/pdf/2311.07677" title="Download PDF">pdf</a>, <a href="/format/2311.07677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating the matrix $p \rightarrow q$ norm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guth%2C+L">Larry Guth</a>, 
<a href="/search/cs?searchtype=author&query=Maldague%2C+D">Dominique Maldague</a>, 
<a href="/search/cs?searchtype=author&query=Urschel%2C+J">John Urschel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Functional Analysis (math.FA)

</div>
<p class="mathjax">The matrix $p \rightarrow q$ norm is a fundamental quantity appearing in a
variety of areas of mathematics. This quantity is known to be efficiently
computable in only a few special cases. The best known algorithms for
approximately computing this quantity with theoretical guarantees essentially
consist of computing the $p\to q$ norm for $p,q$ where this quantity can be
computed exactly or up to a constant, and applying interpolation. We analyze
the matrix $2 \to q$ norm problem and provide an improved approximation
algorithm via a simple argument involving the rows of a given matrix. For
example, we improve the best-known $2\to 4$ norm approximation from $m^{1/8}$
to $m^{1/12}$. This insight for the $2\to q$ norm improves the best known $p
\to q$ approximation algorithm for the region $p \le 2 \le q$, and leads to an
overall improvement in the best-known approximation for $p \to q$ norms from
$m^{25/128}$ to $m^{3 - 2 \sqrt{2}}$.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07682" title="Abstract">arXiv:2311.07682</a> [<a href="/pdf/2311.07682" title="Download PDF">pdf</a>, <a href="/format/2311.07682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuse to Forget: Bias Reduction and Selective Memorization through Model  Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaman%2C+K">Kerem Zaman</a>, 
<a href="/search/cs?searchtype=author&query=Choshen%2C+L">Leshem Choshen</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Shashank Srivastava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Model fusion research aims to aggregate the knowledge of multiple models to
enhance performance by combining their weights. In this work, we study the
inverse, investigating whether and how can model fusion interfere and reduce
unwanted knowledge. We delve into the effects of model fusion on the evolution
of learned shortcuts, social biases, and memorization capabilities in
fine-tuned language models. Through several experiments covering text
classification and generation tasks, our analysis highlights that shared
knowledge among models is usually enhanced during model fusion, while unshared
knowledge is usually lost or forgotten. Based on this observation, we
demonstrate the potential of model fusion as a debiasing tool and showcase its
efficacy in addressing privacy concerns associated with language models.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07686" title="Abstract">arXiv:2311.07686</a> [<a href="/pdf/2311.07686" title="Download PDF">pdf</a>, <a href="/ps/2311.07686" title="Download PostScript">ps</a>, <a href="/format/2311.07686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Optimum Received Power with Elementwise Updates in the Least  Number of Steps for Discrete-Phase RISs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pekcan%2C+D+K">Dogan Kutay Pekcan</a>, 
<a href="/search/cs?searchtype=author&query=Ayanoglu%2C+E">Ender Ayanoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, 2 tables. arXiv admin note: substantial text overlap with <a href="/abs/2308.02673">arXiv:2308.02673</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The problem of optimizing discrete phases in a reconfigurable intelligent
surface (RIS) to maximize the received power at a user equipment is addressed.
Necessary and sufficient conditions to achieve this maximization are given.
These conditions are employed in an algorithm to achieve the maximization. New
versions of the algorithm are given that are proven to achieve convergence in N
or fewer steps whether the direct link is completely blocked or not, where N is
the number of the RIS elements, whereas previously published results achieve
this in KN or 2N number of steps where K is the number of discrete phases,
e.g., [1], [2]. Thus, for a discrete-phase RIS, the techniques presented in
this paper achieve the optimum received power in the smallest number of steps
published in the literature. In addition, in each of those N steps, the
techniques presented in this paper determine only one or a small number of
phase shifts with a simple elementwise update rule, which result in a
substantial reduction of computation time, as compared to the algorithms in the
literature, e.g., [2], [3].
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07687" title="Abstract">arXiv:2311.07687</a> [<a href="/pdf/2311.07687" title="Download PDF">pdf</a>, <a href="/format/2311.07687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend  Actions in Text Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sudhakar%2C+A+V">Arjun Vaithilingam Sudhakar</a>, 
<a href="/search/cs?searchtype=author&query=Parthasarathi%2C+P">Prasanna Parthasarathi</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+J">Janarthanan Rajendran</a>, 
<a href="/search/cs?searchtype=author&query=Chandar%2C+S">Sarath Chandar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated superior performance in
language understanding benchmarks. CALM, a popular approach, leverages
linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to
improve the performance in text games in Jericho without environment-provided
actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps
the LLM fixed during the learning of the text based games. In this work, we
explore and evaluate updating LLM used for candidate recommendation during the
learning of the text based game as well to mitigate the reliance on the human
annotated gameplays, which are costly to acquire. We observe that by updating
the LLM during learning using carefully selected in-game transitions, we can
reduce the dependency on using human annotated game plays for fine-tuning the
LLMs. We conducted further analysis to study the transferability of the updated
LLMs and observed that transferring in-game trained models to other games did
not result in a consistent transfer.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07689" title="Abstract">arXiv:2311.07689</a> [<a href="/pdf/2311.07689" title="Download PDF">pdf</a>, <a href="/format/2311.07689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MART: Improving LLM Safety with Multi-round Automatic Red-Teaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Suyu Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunting Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+R">Rui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Khabsa%2C+M">Madian Khabsa</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi-Chia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuning Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Red-teaming is a common practice for mitigating unsafe behaviors in Large
Language Models (LLMs), which involves thoroughly assessing LLMs to identify
potential flaws and addressing them with responsible and accurate responses.
While effective, manual red-teaming is costly, and existing automatic
red-teaming typically discovers safety risks without addressing them. In this
paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which
incorporates both automatic adversarial prompt writing and safe response
generation, significantly increasing red-teaming scalability and the safety of
the target LLM. Specifically, an adversarial LLM and a target LLM interplay
with each other in an iterative manner, where the adversarial LLM aims to
generate challenging prompts that elicit unsafe responses from the target LLM,
while the target LLM is fine-tuned with safety aligned data on these
adversarial prompts. In each round, the adversarial LLM crafts better attacks
on the updated target LLM, while the target LLM also improves itself through
safety fine-tuning. On adversarial prompt benchmarks, the violation rate of an
LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,
achieving comparable performance to LLMs with extensive adversarial prompt
writing. Notably, model helpfulness on non-adversarial prompts remains stable
throughout iterations, indicating the target LLM maintains strong performance
on instruction following.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07692" title="Abstract">arXiv:2311.07692</a> [<a href="/pdf/2311.07692" title="Download PDF">pdf</a>, <a href="/ps/2311.07692" title="Download PostScript">ps</a>, <a href="/format/2311.07692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Truthfulness of &#x27;Surprisingly Likely&#x27; Responses of Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goel%2C+N">Naman Goel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">The surprisingly likely criterion in the seminal work of Prelec (the Bayesian
Truth Serum) guarantees truthfulness in a game-theoretic multi-agent setting,
by rewarding rational agents to maximise the expected information gain with
their answers w.r.t. their probabilistic beliefs. We investigate the relevance
of a similar criterion for responses of LLMs. We hypothesize that if the
surprisingly likely criterion works in LLMs, under certain conditions, the
responses that maximize the reward under this criterion should be more accurate
than the responses that only maximize the posterior probability. Using
benchmarks including the TruthfulQA benchmark and using openly available LLMs:
GPT-2 and LLaMA-2, we show that the method indeed improves the accuracy
significantly (for example, upto 24 percentage points aggregate improvement on
TruthfulQA and upto 70 percentage points improvement on individual categories
of questions).
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07693" title="Abstract">arXiv:2311.07693</a> [<a href="/pdf/2311.07693" title="Download PDF">pdf</a>, <a href="/format/2311.07693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matching aggregate posteriors in the variational autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Surojit Saha</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Sarang Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Whitaker%2C+R">Ross Whitaker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The variational autoencoder (VAE) is a well-studied, deep, latent-variable
model (DLVM) that efficiently optimizes the variational lower bound of the log
marginal data likelihood and has a strong theoretical foundation. However, the
VAE's known failure to match the aggregate posterior often results in
\emph{pockets/holes} in the latent distribution (i.e., a failure to match the
prior) and/or \emph{posterior collapse}, which is associated with a loss of
information in the latent space. This paper addresses these shortcomings in
VAEs by reformulating the objective function associated with VAEs in order to
match the aggregate/marginal posterior distribution to the prior. We use kernel
density estimate (KDE) to model the aggregate posterior in high dimensions. The
proposed method is named the \emph{aggregate variational autoencoder} (AVAE)
and is built on the theoretical framework of the VAE. Empirical evaluation of
the proposed method on multiple benchmark data sets demonstrates the
effectiveness of the AVAE relative to state-of-the-art (SOTA) methods.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07695" title="Abstract">arXiv:2311.07695</a> [<a href="/pdf/2311.07695" title="Download PDF">pdf</a>, <a href="/format/2311.07695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-Buchi Barrier Certificates for Discrete-time Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murali%2C+V">Vishnu Murali</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Ashutosh Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Zamani%2C+M">Majid Zamani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Barrier certificates provide functional overapproximations for the reachable
set of dynamical systems and provide inductive guarantees on the safe evolution
of the system. Formally a barrier certificate is a real-valued function over
the state set that is required to be non-positive for the initial states,
positive over the set of unsafe states and nonincreasing along the state
transitions. These conditions together provide an inductive argument that the
system will not reach an unsafe state even once as the barrier certificate
remains non-positive for all reachable states. In the automata-theoretic
approach to verification, a key query is to determine whether the system visits
a given predicate over the states finitely often, typically resulting from the
complement of the traditional Buchi acceptance condition. This paper proposes a
barrier certificate approach to answer such queries by developing a notion of
co-Buchi barrier certificates (CBBCs) that generalize classic barrier
certificates to ensure that the traces of a system visit a given predicate a
fixed number of times. Our notion of CBBC is inspired from bounded synthesis
paradigm to LTL realizability, where the LTL specifications are converted to
safety automata via universal co-Buchi automata with a bound on final state
visitations provided as a hyperparameter. Our application of CBBCs in
verification is analogous in nature: we fix a bound and search for a suitable
barrier certificate, increasing the bound if no suitable function can be found.
We then use these CBBCs to verify our system against properties specified by
co-Buchi automata and demonstrate their effectiveness via some case studies. We
also show that the present approach strictly generalizes performant barrier
certificate based approaches that rely on cutting the paths of the automata
that start from an initial state and reach some accepting states.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07698" title="Abstract">arXiv:2311.07698</a> [<a href="/pdf/2311.07698" title="Download PDF">pdf</a>, <a href="/format/2311.07698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chaotic dynamics of two-dimensional flows around a cylinder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Scott%2C+L+R">L. Ridgway Scott</a>, 
<a href="/search/math?searchtype=author&query=Durst%2C+R">Rebecca Durst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, including appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We study flow around a cylinder from a dynamics perspective, using drag and
lift as indicators. We observe that the mean drag coefficient bifurcates from
the steady case when the Karman vortex street emerges. We also find a jump in
the dimension of the drag/lift attractor just above Reynolds number 100. We
compare the simulated drag values with experimental data obtained over the last
hundred years. Our simulations suggest that a vibrational resonance in the
cylinder would be unlikely for Reynolds numbers greater than 1000, where the
drag/lift behavior is fully chaotic.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07700" title="Abstract">arXiv:2311.07700</a> [<a href="/pdf/2311.07700" title="Download PDF">pdf</a>, <a href="/format/2311.07700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language  Models Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shangdi Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have opened up enormous opportunities while
simultaneously posing ethical dilemmas. One of the major concerns is their
ability to create text that closely mimics human writing, which can lead to
potential misuse, such as academic misconduct, disinformation, and fraud. To
address this problem, we present AuthentiGPT, an efficient classifier that
distinguishes between machine-generated and human-written texts. Under the
assumption that human-written text resides outside the distribution of
machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input
text with artificially added noise, and then semantically compares the denoised
text with the original to determine if the content is machine-generated. With
only one trainable parameter, AuthentiGPT eliminates the need for a large
training dataset, watermarking the LLM's output, or computing the
log-likelihood. Importantly, the detection capability of AuthentiGPT can be
easily adapted to any generative language model. With a 0.918 AUROC score on a
domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other
commercial algorithms, highlighting its potential for detecting
machine-generated text in academic settings.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07703" title="Abstract">arXiv:2311.07703</a> [<a href="/pdf/2311.07703" title="Download PDF">pdf</a>, <a href="/format/2311.07703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Entrainment in Spontaneous Code-switched Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+D">Debasmita Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Siying Ding</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Alayna Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Hirschberg%2C+J">Julia Hirschberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">It is well-known that interlocutors who entrain to one another have more
successful conversations than those who do not. Previous research has shown
that interlocutors entrain on linguistic features in both written and spoken
monolingual domains. More recent work on code-switched communication has also
shown preliminary evidence of entrainment on certain aspects of code-switching
(CSW). However, such studies of entrainment in code-switched domains have been
extremely few and restricted to human-machine textual interactions. Our work
studies code-switched spontaneous speech between humans by answering the
following questions: 1) Do patterns of written and spoken entrainment in
monolingual settings generalize to code-switched settings? 2) Do patterns of
entrainment on code-switching in generated text generalize to spontaneous
code-switched speech? We find evidence of affirmative answers to both of these
questions, with important implications for the potentially "universal" nature
of entrainment as a communication phenomenon, and potential applications in
inclusive and interactive speech technology.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07705" title="Abstract">arXiv:2311.07705</a> [<a href="/pdf/2311.07705" title="Download PDF">pdf</a>, <a href="/format/2311.07705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust and Scalable Hyperdimensional Computing With Brain-Like Neural  Adaptations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Faruque%2C+M+A+A">Mohammad Abdullah Al Faruque</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2304.05503">arXiv:2304.05503</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">The Internet of Things (IoT) has facilitated many applications utilizing
edge-based machine learning (ML) methods to analyze locally collected data.
Unfortunately, popular ML algorithms often require intensive computations
beyond the capabilities of today's IoT devices. Brain-inspired hyperdimensional
computing (HDC) has been introduced to address this issue. However, existing
HDCs use static encoders, requiring extremely high dimensionality and hundreds
of training iterations to achieve reasonable accuracy. This results in a huge
efficiency loss, severely impeding the application of HDCs in IoT systems. We
observed that a main cause is that the encoding module of existing HDCs lacks
the capability to utilize and adapt to information learned during training. In
contrast, neurons in human brains dynamically regenerate all the time and
provide more useful functionalities when learning new information. While the
goal of HDC is to exploit the high-dimensionality of randomly generated base
hypervectors to represent the information as a pattern of neural activity, it
remains challenging for existing HDCs to support a similar behavior as brain
neural regeneration. In this work, we present dynamic HDC learning frameworks
that identify and regenerate undesired dimensions to provide adequate accuracy
with significantly lowered dimensionalities, thereby accelerating both the
training and inference.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07708" title="Abstract">arXiv:2311.07708</a> [<a href="/pdf/2311.07708" title="Download PDF">pdf</a>, <a href="/format/2311.07708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning for Solving Stochastic Vehicle Routing Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iklassov%2C+Z">Zangir Iklassov</a>, 
<a href="/search/cs?searchtype=author&query=Sobirov%2C+I">Ikboljon Sobirov</a>, 
<a href="/search/cs?searchtype=author&query=Solozabal%2C+R">Ruben Solozabal</a>, 
<a href="/search/cs?searchtype=author&query=Takac%2C+M">Martin Takac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, accepted to ACML24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">This study addresses a gap in the utilization of Reinforcement Learning (RL)
and Machine Learning (ML) techniques in solving the Stochastic Vehicle Routing
Problem (SVRP) that involves the challenging task of optimizing vehicle routes
under uncertain conditions. We propose a novel end-to-end framework that
comprehensively addresses the key sources of stochasticity in SVRP and utilizes
an RL agent with a simple yet effective architecture and a tailored training
method. Through comparative analysis, our proposed model demonstrates superior
performance compared to a widely adopted state-of-the-art metaheuristic,
achieving a significant 3.43% reduction in travel costs. Furthermore, the model
exhibits robustness across diverse SVRP settings, highlighting its adaptability
and ability to learn optimal routing strategies in varying environments. The
publicly available implementation of our framework serves as a valuable
resource for future research endeavors aimed at advancing RL-based solutions
for SVRP.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07711" title="Abstract">arXiv:2311.07711</a> [<a href="/pdf/2311.07711" title="Download PDF">pdf</a>, <a href="/format/2311.07711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Histopathologic Cancer Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rohila%2C+V+S">Varan Singh Rohila</a>, 
<a href="/search/cs?searchtype=author&query=Lalwani%2C+N">Neeraj Lalwani</a>, 
<a href="/search/cs?searchtype=author&query=Basyal%2C+L">Lochan Basyal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Early diagnosis of the cancer cells is necessary for making an effective
treatment plan and for the health and safety of a patient. Nowadays, doctors
usually use a histological grade that pathologists determine by performing a
semi-quantitative analysis of the histopathological and cytological features of
hematoxylin-eosin (HE) stained histopathological images. This research
contributes a potential classification model for cancer prognosis to
efficiently utilize the valuable information underlying the HE-stained
histopathological images. This work uses the PatchCamelyon benchmark datasets
and trains them in a multi-layer perceptron and convolution model to observe
the model's performance in terms of precision, Recall, F1 Score, Accuracy, and
AUC Score. The evaluation result shows that the baseline convolution model
outperforms the baseline MLP model. Also, this paper introduced ResNet50 and
InceptionNet models with data augmentation, where ResNet50 is able to beat the
state-of-the-art model. Furthermore, the majority vote and concatenation
ensemble were evaluated and provided the future direction of using transfer
learning and segmentation to understand the specific features.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07712" title="Abstract">arXiv:2311.07712</a> [<a href="/pdf/2311.07712" title="Download PDF">pdf</a>, <a href="/ps/2311.07712" title="Download PostScript">ps</a>, <a href="/format/2311.07712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Cost Architecture for an Advanced Smart Shower System Using Internet  of Things Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+S">Shadeeb Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Abdelgawad%2C+A">Ahmed Abdelgawad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Wastage of water is a critical issue amongst the various global crises. This
paper proposes an architecture model for a low-cost, energy efficient SMART
Shower system that is ideal for efficient water management and be able to
predict reliably any accidental fall in the shower space. The sensors in this
prototype can document the surrounding temperature and humidity in real time
and thereby circulate the ideal temperature of water for its patron, rather
than its reliance on predictive values . Three different scenarios are
discussed that can allow reliably predicting any accidental fall in the shower
vicinity. Motion sensors, sound sensors and gesture sensors can be used to
compliment prediction of possible injuries in the shower. The integration with
the Internet of Things (IoT) platform will allow caretakers to monitor the
activities in the shower space especially in the case of elderly individuals as
there have been reported cases of casualties in the slippery shower space. The
proposed proof-of-concept prototype is cost effective and can be incorporated
into an existing system for the added precedence of safety and convenience. The
intelligent system is conserving water by optimizing its flow temperature and
the IoT platform allows real time monitoring for safety.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07715" title="Abstract">arXiv:2311.07715</a> [<a href="/pdf/2311.07715" title="Download PDF">pdf</a>, <a href="/format/2311.07715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolyIE: A Dataset of Information Extraction from Polymer Material  Scientific Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheung%2C+J+J">Jerry Junyang Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yuchen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Shetty%2C+P">Pranav Shetty</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wantian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Grampurohit%2C+S">Sanjeev Grampurohit</a>, 
<a href="/search/cs?searchtype=author&query=Ramprasad%2C+R">Rampi Ramprasad</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Scientific information extraction (SciIE), which aims to automatically
extract information from scientific literature, is becoming more important than
ever. However, there are no existing SciIE datasets for polymer materials,
which is an important class of materials used ubiquitously in our daily lives.
To bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer
materials. POLYIE is curated from 146 full-length polymer scholarly articles,
which are annotated with different named entities (i.e., materials, properties,
values, conditions) as well as their N-ary relations by domain experts. POLYIE
presents several unique challenges due to diverse lexical formats of entities,
ambiguity between entities, and variable-length relations. We evaluate
state-of-the-art named entity extraction and relation extraction models on
POLYIE, analyze their strengths and weaknesses, and highlight some difficult
cases for these models. To the best of our knowledge, POLYIE is the first SciIE
benchmark for polymer materials, and we hope it will lead to more research
efforts from the community on this challenging task. Our code and data are
available on: https://github.com/jerry3027/PolyIE.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07720" title="Abstract">arXiv:2311.07720</a> [<a href="/pdf/2311.07720" title="Download PDF">pdf</a>, <a href="/ps/2311.07720" title="Download PostScript">ps</a>, <a href="/format/2311.07720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Regression LDPC Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ebert%2C+J+R">Jamison R. Ebert</a>, 
<a href="/search/cs?searchtype=author&query=Chamberland%2C+J">Jean-Francois Chamberland</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+K+R">Krishna R. Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. arXiv admin note: substantial text overlap with <a href="/abs/2301.01899">arXiv:2301.01899</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This article introduces a novel concatenated coding scheme called sparse
regression LDPC (SR-LDPC) codes. An SR-LDPC code consists of an outer
non-binary LDPC code and an inner sparse regression code (SPARC) whose
respective field size and section sizes are equal. For such codes, an efficient
decoding algorithm is proposed based on approximate message passing (AMP) that
dynamically shares soft information between inner and outer decoders. This
dynamic exchange of information is facilitated by a denoiser that runs belief
propagation (BP) on the factor graph of the outer LDPC code within each AMP
iteration. It is shown that this denoiser falls within the class of
non-separable pseudo-Lipschitz denoising functions and thus that state
evolution holds for the proposed AMP-BP algorithm. Leveraging the rich
structure of SR-LDPC codes, this article proposes an efficient low-dimensional
approximate state evolution recursion that can be used for efficient
hyperparameter tuning, thus paving the way for future work on optimal code
design. Finally, numerical simulations demonstrate that SR-LDPC codes
outperform contemporary codes over the AWGN channel for parameters of practical
interest. SR-LDPC codes are shown to be viable means to obtain shaping gains
over the AWGN channel.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07722" title="Abstract">arXiv:2311.07722</a> [<a href="/pdf/2311.07722" title="Download PDF">pdf</a>, <a href="/format/2311.07722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Field Integrated Sensing, Positioning, and Communication: A  Downlink and Uplink Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haochen Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaolin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+X">Xidong Mu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhiwen Pan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">A near-field integrated sensing, positioning, and communication (ISPAC)
framework is proposed, where a base station (BS) simultaneously serves multiple
communication users and carries out target sensing and positioning. A novel
double-array structure is proposed to enable the near-field ISPAC at the BS.
Specifically, a small-scale assisting transceiver (AT) is attached to the
large-scale main transceiver (MT) to empower the communication system with the
ability of sensing and positioning. Based on the proposed framework, the joint
angle and distance Cram\'er-Rao bound (CRB) is first derived. Then, the CRB is
minimized subject to the minimum communication rate requirement in both
downlink and uplink ISPAC scenarios: 1) For downlink ISPAC, a downlink target
positioning algorithm is proposed and a penalty dual decomposition (PDD)-based
double-loop algorithm is developed to tackle the non-convex optimization
problem. 2) For uplink ISPAC, an uplink target positioning algorithm is
proposed and an efficient alternating optimization algorithm is conceived to
solve the non-convex CRB minimization problem with coupled user communication
and target probing design. Both proposed optimization algorithms can converge
to a stationary point of the CRB minimization problem. Numerical results show
that: 1) The proposed ISPAC system can locate the target in both angle and
distance domains merely relying on single BS and limited bandwidths; and 2) the
positioning performance achieved by the hybrid-analog-and-digital ISPAC
approaches that achieved by fully digital ISPAC when the communication rate
requirement is not stringent.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07723" title="Abstract">arXiv:2311.07723</a> [<a href="/pdf/2311.07723" title="Download PDF">pdf</a>, <a href="/format/2311.07723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization Analogies (GENIES): A Testbed for Generalizing AI  Oversight to Hard-To-Measure Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clymer%2C+J">Joshua Clymer</a>, 
<a href="/search/cs?searchtype=author&query=Baker%2C+G">Garrett Baker</a>, 
<a href="/search/cs?searchtype=author&query=Subramani%2C+R">Rohan Subramani</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sam Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/Joshuaclymer/GENIES">this https URL</a> Website: <a href="https://joshuaclymer.github.io/generalization-analogies-website/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following' by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models'
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENaralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07729" title="Abstract">arXiv:2311.07729</a> [<a href="/pdf/2311.07729" title="Download PDF">pdf</a>, <a href="/ps/2311.07729" title="Download PostScript">ps</a>, <a href="/format/2311.07729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed pressure matching strategy using diffusion adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengfei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Richard%2C+C">C&#xe9;dric Richard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">Personal sound zone (PSZ) systems, which aim to create listening (bright) and
silent (dark) zones in neighboring regions of space, are often based on
time-varying acoustics. Conventional adaptive-based methods for handling PSZ
tasks suffer from the collection and processing of acoustic transfer
functions~(ATFs) between all the matching microphones and all the loudspeakers
in a centralized manner, resulting in high calculation complexity and costly
accuracy requirements. This paper presents a distributed pressure-matching (PM)
method relying on diffusion adaptation (DPM-D) to spread the computational load
amongst nodes in order to overcome these issues. The global PM problem is
defined as a sum of local costs, and the diffusion adaption approach is then
used to create a distributed solution that just needs local information
exchanges. Simulations over multi-frequency bins and a computational complexity
analysis are conducted to evaluate the properties of the algorithm and to
compare it with centralized counterparts.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07732" title="Abstract">arXiv:2311.07732</a> [<a href="/pdf/2311.07732" title="Download PDF">pdf</a>, <a href="/ps/2311.07732" title="Download PostScript">ps</a>, <a href="/format/2311.07732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Providing a Periodic Control Solution for Balance Control While Standing  Using a Pendulum-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shahraki%2C+G">Golnoush Shahraki</a>, 
<a href="/search/eess?searchtype=author&query=Sadrzadeh%2C+M">Majid Sadrzadeh</a>, 
<a href="/search/eess?searchtype=author&query=Irankhah%2C+E">Elyas Irankhah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The stability of standing in humans is a complex process that leads to
maintaining the upright position against external disturbances. Balance control
during standing is of vital importance for humans in daily life. An issue that
is still not clearly understood is which control mechanism the central nervous
system uses to maintain stability. In the rehabilitation of standing function,
the coordination pattern between the angles of the leg joint of a healthy
person should be restored. For example, one of the rehabilitation methods is
functional electrical stimulation. In the work that was mainly done in the
control of standing balance with functional electrical stimulation, the problem
of the optimal pattern using the phase space was not mentioned at all, and a
series of predetermined desired curves were assigned to the joints, and the
controller only used these curves. followed, while the origin of these curves
are not real patterns. Therefore, the main goal of this project is to design a
periodic controller based on phase space. In such a way that a mapping related
to standing is detected first, then a feedback controller is designed so that
it is activated only when the system state space curves find a significant
distance from the detected mapping, then the feedback controller is activated,
and it adjusts the control signal so that the system state space curves come
close to the detected mapping.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07734" title="Abstract">arXiv:2311.07734</a> [<a href="/pdf/2311.07734" title="Download PDF">pdf</a>, <a href="/format/2311.07734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality-Aware Prototype Memory for Face Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smirnov%2C+E">Evgeny Smirnov</a>, 
<a href="/search/cs?searchtype=author&query=Galyuk%2C+V">Vasiliy Galyuk</a>, 
<a href="/search/cs?searchtype=author&query=Lukyanets%2C+E">Evgeny Lukyanets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Prototype Memory is a powerful model for face representation learning. It
enables the training of face recognition models using datasets of any size,
with on-the-fly generation of prototypes (classifier weights) and efficient
ways of their utilization. Prototype Memory demonstrated strong results in many
face recognition benchmarks. However, the algorithm of prototype generation,
used in it, is prone to the problems of imperfectly calculated prototypes in
case of low-quality or poorly recognizable faces in the images, selected for
the prototype creation. All images of the same person, presented in the
mini-batch, used with equal weights, and the resulting averaged prototype could
be contaminated with imperfect embeddings of such face images. It can lead to
misdirected training signals and impair the performance of the trained face
recognition models. In this paper, we propose a simple and effective way to
improve Prototype Memory with quality-aware prototype generation. Quality-Aware
Prototype Memory uses different weights for images of different quality in the
process of prototype generation. With this improvement, prototypes get more
valuable information from high-quality images and less hurt by low-quality
ones. We propose and compare several methods of quality estimation and usage,
perform extensive experiments on the different face recognition benchmarks and
demonstrate the advantages of the proposed model compared to the basic version
of Prototype Memory.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07742" title="Abstract">arXiv:2311.07742</a> [<a href="/pdf/2311.07742" title="Download PDF">pdf</a>, <a href="/format/2311.07742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Sequences as Star Graphs to Address Over-smoothing in  Self-attentive Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Parthasarathy%2C+S">Srinivasan Parthasarathy</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xia Ning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2209.07997">arXiv:2209.07997</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Self-attention (SA) mechanisms have been widely used in developing sequential
recommendation (SR) methods, and demonstrated state-of-the-art performance.
However, in this paper, we show that self-attentive SR methods substantially
suffer from the over-smoothing issue that item embeddings within a sequence
become increasingly similar across attention blocks. As widely demonstrated in
the literature, this issue could lead to a loss of information in individual
items, and significantly degrade models' scalability and performance. To
address the over-smoothing issue, in this paper, we view items within a
sequence constituting a star graph and develop a method, denoted as MSSG, for
SR. Different from existing self-attentive methods, MSSG introduces an
additional internal node to specifically capture the global information within
the sequence, and does not require information propagation among items. This
design fundamentally addresses the over-smoothing issue and enables MSSG a
linear time complexity with respect to the sequence length. We compare MSSG
with ten state-of-the-art baseline methods on six public benchmark datasets.
Our experimental results demonstrate that MSSG significantly outperforms the
baseline methods, with an improvement of as much as 10.10%. Our analysis shows
the superior scalability of MSSG over the state-of-the-art self-attentive
methods. Our complexity analysis and run-time performance comparison together
show that MSSG is both theoretically and practically more efficient than
self-attentive methods. Our analysis of the attention weights learned in
SA-based methods indicates that on sparse recommendation data, modeling
dependencies in all item pairs using the SA mechanism yields limited
information gain, and thus, might not benefit the recommendation performance
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07744" title="Abstract">arXiv:2311.07744</a> [<a href="/pdf/2311.07744" title="Download PDF">pdf</a>, <a href="/format/2311.07744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Local Attention with Hierarchical Patching for Irregular  Clinical Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaochen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Mollaysa%2C+A">Amina Mollaysa</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCrch%2C+M">Manuel Sch&#xfc;rch</a>, 
<a href="/search/cs?searchtype=author&query=Allam%2C+A">Ahmed Allam</a>, 
<a href="/search/cs?searchtype=author&query=Krauthammer%2C+M">Michael Krauthammer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of Machine Learning for Health (ML4H) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Irregular multivariate time series data is prevalent in the clinical and
healthcare domains. It is characterized by time-wise and feature-wise
irregularities, making it challenging for machine learning methods to work
with. To solve this, we introduce a new model architecture composed of two
modules: (1) DLA, a Dynamic Local Attention mechanism that uses learnable
queries and feature-specific local windows when computing the self-attention
operation. This results in aggregating irregular time steps raw input within
each window to a harmonized regular latent space representation while taking
into account the different features' sampling rates. (2) A hierarchical MLP
mixer that processes the output of DLA through multi-scale patching to leverage
information at various scales for the downstream tasks. Our approach
outperforms state-of-the-art methods on three real-world datasets, including
the latest clinical MIMIC IV dataset.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07745" title="Abstract">arXiv:2311.07745</a> [<a href="/pdf/2311.07745" title="Download PDF">pdf</a>, <a href="/format/2311.07745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplifying Complex Observation Models in Continuous POMDP Planning with  Probabilistic Guarantees and Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lev-Yehudi%2C+I">Idan Lev-Yehudi</a>, 
<a href="/search/cs?searchtype=author&query=Barenboim%2C+M">Moran Barenboim</a>, 
<a href="/search/cs?searchtype=author&query=Indelman%2C+V">Vadim Indelman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Solving partially observable Markov decision processes (POMDPs) with high
dimensional and continuous observations, such as camera images, is required for
many real life robotics and planning problems. Recent researches suggested
machine learned probabilistic models as observation models, but their use is
currently too computationally expensive for online deployment. We deal with the
question of what would be the implication of using simplified observation
models for planning, while retaining formal guarantees on the quality of the
solution. Our main contribution is a novel probabilistic bound based on a
statistical total variation distance of the simplified model. We show that it
bounds the theoretical POMDP value w.r.t. original model, from the empirical
planned value with the simplified model, by generalizing recent results of
particle-belief MDP concentration bounds. Our calculations can be separated
into offline and online parts, and we arrive at formal guarantees without
having to access the costly model at all during planning, which is also a novel
result. Finally, we demonstrate in simulation how to integrate the bound into
the routine of an existing continuous online POMDP solver.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07750" title="Abstract">arXiv:2311.07750</a> [<a href="/pdf/2311.07750" title="Download PDF">pdf</a>, <a href="/format/2311.07750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models  for Multi-Label Chest X-Ray Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ashraf%2C+S+M+N">S.M. Nabil Ashraf</a>, 
<a href="/search/cs?searchtype=author&query=Mamun%2C+M+A">Md. Adyelullahil Mamun</a>, 
<a href="/search/cs?searchtype=author&query=Abdullah%2C+H+M">Hasnat Md. Abdullah</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+M+G+R">Md. Golam Rabiul Alam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in International Conference on Computer and Information Technology (ICCIT) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Chest X-rays are widely used to diagnose thoracic diseases, but the lack of
detailed information about these abnormalities makes it challenging to develop
accurate automated diagnosis systems, which is crucial for early detection and
effective treatment. To address this challenge, we employed deep learning
techniques to identify patterns in chest X-rays that correspond to different
diseases. We conducted experiments on the "ChestX-ray14" dataset using various
pre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical
models. The best individual model was the CoAtNet, which achieved an area under
the receiver operating characteristic curve (AUROC) of 84.2%. By combining the
predictions of all trained models using a weighted average ensemble where the
weight of each model was determined using differential evolution, we further
improved the AUROC to 85.4%, outperforming other state-of-the-art methods in
this field. Our findings demonstrate the potential of deep learning techniques,
particularly ensemble deep learning, for improving the accuracy of automatic
diagnosis of thoracic diseases from chest X-rays.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07751" title="Abstract">arXiv:2311.07751</a> [<a href="/pdf/2311.07751" title="Download PDF">pdf</a>, <a href="/ps/2311.07751" title="Download PostScript">ps</a>, <a href="/format/2311.07751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong exponential stability of switched impulsive systems with  mode-constrained switching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Vallarella%2C+A+J">Alexis J. Vallarella</a>, 
<a href="/search/eess?searchtype=author&query=Mancilla-Aguilar%2C+J+L">Jos&#xe9; Luis Mancilla-Aguilar</a>, 
<a href="/search/eess?searchtype=author&query=Haimovich%2C+H">Hernan Haimovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">Strong stability, defined by bounds that decay not only over time but also
with the number of impulses, has been established as a requirement to ensure
robustness properties for impulsive systems with respect to inputs or
disturbances. Most existing results, however, only consider weak stability. In
this paper, we provide a method for calculating the maximum overshoot and the
decay rate for strong (and weak) global uniform exponential stability bounds
for non-linear switched impulsive systems. We consider the scenario of
mode-constrained switching where not all transitions between subsystems are
allowed, and where subsystems may exhibit unstable dynamics in the flow and
jump maps. Based on direct and reverse mode-dependent average dwell-time and
activation-time constraints, we derive stability bounds that can be improved by
considering longer switching sequences for computation. We provide numerical
examples that illustrate the weak and strong exponential stability bounds and
also how the results can be employed to ensure the stability robustness of
nonlinear systems that admit a global state weak linearization.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07753" title="Abstract">arXiv:2311.07753</a> [<a href="/pdf/2311.07753" title="Download PDF">pdf</a>, <a href="/format/2311.07753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bringing Reconfigurability to the Network Stack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Narayan%2C+A">Akshay Narayan</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+A">Aurojit Panda</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+M">Mohammad Alizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Balakrishnan%2C+H">Hari Balakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Arvind Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Shenker%2C+S">Scott Shenker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Reconfiguring the network stack allows applications to specialize the
implementations of communication libraries depending on where they run, the
requests they serve, and the performance they need to provide. Specializing
applications in this way is challenging because developers need to choose the
libraries they use when writing a program and cannot easily change them at
runtime. This paper introduces Bertha, which allows these choices to be changed
at runtime without limiting developer flexibility in the choice of network and
communication functions. Bertha allows applications to safely use optimized
communication primitives (including ones with deployment limitations) without
limiting deployability. Our evaluation shows cases where this results in 16x
higher throughput and 63% lower latency than current portable approaches while
imposing minimal overheads when compared to a hand-optimized versions that use
deployment-specific communication primitives.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07754" title="Abstract">arXiv:2311.07754</a> [<a href="/pdf/2311.07754" title="Download PDF">pdf</a>, <a href="/format/2311.07754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Prior-Free Mechanisms for No-Regret Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Collina%2C+N">Natalie Collina</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Aaron Roth</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Han Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS); Theoretical Economics (econ.TH)

</div>
<p class="mathjax">We study a repeated Principal Agent problem between a long lived Principal
and Agent pair in a prior free setting. In our setting, the sequence of
realized states of nature may be adversarially chosen, the Agent is non-myopic,
and the Principal aims for a strong form of policy regret. Following Camara,
Hartline, and Johnson, we model the Agent's long-run behavior with behavioral
assumptions that relax the common prior assumption (for example, that the Agent
has no swap regret). Within this framework, we revisit the mechanism proposed
by Camara et al., which informally uses calibrated forecasts of the unknown
states of nature in place of a common prior. We give two main improvements.
First, we give a mechanism that has an exponentially improved dependence (in
terms of both running time and regret bounds) on the number of distinct states
of nature. To do this, we show that our mechanism does not require truly
calibrated forecasts, but rather forecasts that are unbiased subject to only a
polynomially sized collection of events -- which can be produced with
polynomial overhead. Second, in several important special cases -- including
the focal linear contracting setting -- we show how to remove strong
``Alignment'' assumptions (which informally require that near-ties are always
broken in favor of the Principal) by specifically deploying ``stable'' policies
that do not have any near ties that are payoff relevant to the Principal. Taken
together, our new mechanism makes the compelling framework proposed by Camara
et al. much more powerful, now able to be realized over polynomially sized
state spaces, and while requiring only mild assumptions on Agent behavior.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07758" title="Abstract">arXiv:2311.07758</a> [<a href="/pdf/2311.07758" title="Download PDF">pdf</a>, <a href="/format/2311.07758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synchrophasor Data Anomaly Detection on Grid Edge by 5G Communication  and Adjacent Compute
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qin%2C+C">Chuan Qin</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+D">Dexin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Guddanti%2C+K+P">Kishan Prudhvi Guddanti</a>, 
<a href="/search/eess?searchtype=author&query=Fan%2C+X">Xiaoyuan Fan</a>, 
<a href="/search/eess?searchtype=author&query=Hou%2C+Z">Zhangshuan Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The fifth-generation mobile communication (5G) technology offers
opportunities to enhance the real-time monitoring of grids. The 5G-enabled
phasor measurement units (PMUs) feature flexible positioning and cost-effective
long-term maintenance without the constraints of fixing wires. This paper is
the first to demonstrate the applicability of 5G in PMU communication, and the
experiment was carried out at Verizon non-standalone test-bed at Pacific
Northwest National Laboratory (PNNL) Advanced Wireless Communication lab. The
performance of the 5G-enabled PMU communication setup is reviewed and discussed
in this paper, and a generalized dynamic linear model (GDLM) based real-time
synchrophasor data anomaly detection use-case is presented. Last but not least,
the practicability of implementing 5G for wide-area protection strategies is
explored and discussed by analyzing the experimental results.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07759" title="Abstract">arXiv:2311.07759</a> [<a href="/pdf/2311.07759" title="Download PDF">pdf</a>, <a href="/format/2311.07759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oltramari%2C+A">Alessandro Oltramari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">High-level reasoning can be defined as the capability to generalize over
knowledge acquired via experience, and to exhibit robust behavior in novel
situations. Such form of reasoning is a basic skill in humans, who seamlessly
use it in a broad spectrum of tasks, from language communication to decision
making in complex situations. When it manifests itself in understanding and
manipulating the everyday world of objects and their interactions, we talk
about common sense or commonsense reasoning. State-of-the-art AI systems don't
possess such capability: for instance, Large Language Models have recently
become popular by demonstrating remarkable fluency in conversing with humans,
but they still make trivial mistakes when probed for commonsense competence; on
a different level, performance degradation outside training data prevents
self-driving vehicles to safely adapt to unseen scenarios, a serious and
unsolved problem that limits the adoption of such technology. In this paper we
propose to enable high-level reasoning in AI systems by integrating cognitive
architectures with external neuro-symbolic components. We illustrate a hybrid
framework centered on ACT-R and we discuss the role of generative models in
recent and future applications.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07760" title="Abstract">arXiv:2311.07760</a> [<a href="/pdf/2311.07760" title="Download PDF">pdf</a>, <a href="/format/2311.07760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ransomware Detection Using Federated Learning with Imbalanced Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vehabovic%2C+A">Aldin Vehabovic</a>, 
<a href="/search/cs?searchtype=author&query=Zanddizari%2C+H">Hadi Zanddizari</a>, 
<a href="/search/cs?searchtype=author&query=Ghani%2C+N">Nasir Ghani</a>, 
<a href="/search/cs?searchtype=author&query=Javidi%2C+G">G. Javidi</a>, 
<a href="/search/cs?searchtype=author&query=Uluagac%2C+S">S. Uluagac</a>, 
<a href="/search/cs?searchtype=author&query=Rahouti%2C+M">M. Rahouti</a>, 
<a href="/search/cs?searchtype=author&query=Bou-Harb%2C+E">E. Bou-Harb</a>, 
<a href="/search/cs?searchtype=author&query=Pour%2C+M+S">M. Safaei Pour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Ransomware is a type of malware which encrypts user data and extorts payments
in return for the decryption keys. This cyberthreat is one of the most serious
challenges facing organizations today and has already caused immense financial
damage. As a result, many researchers have been developing techniques to
counter ransomware. Recently, the federated learning (FL) approach has also
been applied for ransomware analysis, allowing corporations to achieve
scalable, effective detection and attribution without having to share their
private data. However, in reality there is much variation in the quantity and
composition of ransomware data collected across multiple FL client
sites/regions. This imbalance will inevitably degrade the effectiveness of any
defense mechanisms. To address this concern, a modified FL scheme is proposed
using a weighted cross-entropy loss function approach to mitigate dataset
imbalance. A detailed performance evaluation study is then presented for the
case of static analysis using the latest Windows-based ransomware families. The
findings confirm improved ML classifier performance for a highly imbalanced
dataset.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07761" title="Abstract">arXiv:2311.07761</a> [<a href="/pdf/2311.07761" title="Download PDF">pdf</a>, <a href="/format/2311.07761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amodal Optical Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luz%2C+M">Maximilian Luz</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+R">Rohit Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Sekkat%2C+A+R">Ahmed Rida Sekkat</a>, 
<a href="/search/cs?searchtype=author&query=Sawade%2C+O">Oliver Sawade</a>, 
<a href="/search/cs?searchtype=author&query=Matthes%2C+E">Elmar Matthes</a>, 
<a href="/search/cs?searchtype=author&query=Brox%2C+T">Thomas Brox</a>, 
<a href="/search/cs?searchtype=author&query=Valada%2C+A">Abhinav Valada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Optical flow estimation is very challenging in situations with transparent or
occluded objects. In this work, we address these challenges at the task level
by introducing Amodal Optical Flow, which integrates optical flow with amodal
perception. Instead of only representing the visible regions, we define amodal
optical flow as a multi-layered pixel-level motion field that encompasses both
visible and occluded regions of the scene. To facilitate research on this new
task, we extend the AmodalSynthDrive dataset to include pixel-level labels for
amodal optical flow estimation. We present several strong baselines, along with
the Amodal Flow Quality metric to quantify the performance in an interpretable
manner. Furthermore, we propose the novel AmodalFlowNet as an initial step
toward addressing this task. AmodalFlowNet consists of a transformer-based
cost-volume encoder paired with a recurrent transformer decoder which
facilitates recurrent hierarchical feature propagation and amodal semantic
grounding. We demonstrate the tractability of amodal optical flow in extensive
experiments and show its utility for downstream tasks such as panoptic
tracking. We make the dataset, code, and trained models publicly available at
<a href="http://amodal-flow.cs.uni-freiburg.de.">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07763" title="Abstract">arXiv:2311.07763</a> [<a href="/pdf/2311.07763" title="Download PDF">pdf</a>, <a href="/format/2311.07763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Disagreement Problem in Faithfulness Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barr%2C+B">Brian Barr</a>, 
<a href="/search/cs?searchtype=author&query=Fatsi%2C+N">Noah Fatsi</a>, 
<a href="/search/cs?searchtype=author&query=Hancox-Li%2C+L">Leif Hancox-Li</a>, 
<a href="/search/cs?searchtype=author&query=Richter%2C+P">Peter Richter</a>, 
<a href="/search/cs?searchtype=author&query=Proano%2C+D">Daniel Proano</a>, 
<a href="/search/cs?searchtype=author&query=Mok%2C+C">Caleb Mok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages (excluding refs and appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The field of explainable artificial intelligence (XAI) aims to explain how
black-box machine learning models work. Much of the work centers around the
holy grail of providing post-hoc feature attributions to any model
architecture. While the pace of innovation around novel methods has slowed
down, the question remains of how to choose a method, and how to make it fit
for purpose. Recently, efforts around benchmarking XAI methods have suggested
metrics for that purpose -- but there are many choices. That bounty of choice
still leaves an end user unclear on how to proceed. This paper focuses on
comparing metrics with the aim of measuring faithfulness of local explanations
on tabular classification problems -- and shows that the current metrics don't
agree; leaving users unsure how to choose the most faithful explanations.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07765" title="Abstract">arXiv:2311.07765</a> [<a href="/pdf/2311.07765" title="Download PDF">pdf</a>, <a href="/format/2311.07765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based  Human Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C4%B0%C5%9Fg%C3%BCder%2C+E">Egemen &#x130;&#x15f;g&#xfc;der</a>, 
<a href="/search/cs?searchtype=author&query=%C4%B0ncel%2C+%C3%96+D">&#xd6;zlem Durmaz &#x130;ncel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Subimtted to Asian Conference in Machine Learning (ACML) 2023, Pattern Recognition in Health Analysis Workshop, 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Motion sensors integrated into wearable and mobile devices provide valuable
information about the device users. Machine learning and, recently, deep
learning techniques have been used to characterize sensor data. Mostly, a
single task, such as recognition of activities, is targeted, and the data is
processed centrally at a server or in a cloud environment. However, the same
sensor data can be utilized for multiple tasks and distributed machine-learning
techniques can be used without the requirement of the transmission of data to a
centre. This paper explores Federated Transfer Learning in a Multi-Task manner
for both sensor-based human activity recognition and device position
identification tasks. The OpenHAR framework is used to train the models, which
contains ten smaller datasets. The aim is to obtain model(s) applicable for
both tasks in different datasets, which may include only some label types.
Multiple experiments are carried in the Flower federated learning environment
using the DeepConvLSTM architecture. Results are presented for federated and
centralized versions under different parameters and restrictions. By utilizing
transfer learning and training a task-specific and personalized federated
model, we obtained a similar accuracy with training each client individually
and higher accuracy than a fully centralized approach.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07766" title="Abstract">arXiv:2311.07766</a> [<a href="/pdf/2311.07766" title="Download PDF">pdf</a>, <a href="/format/2311.07766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Integration in Multimodal Video Transformers (Partially)  Aligns with the Brain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+D+T">Dota Tianai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Toneva%2C+M">Mariya Toneva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Integrating information from multiple modalities is arguably one of the
essential prerequisites for grounding artificial intelligence systems with an
understanding of the real world. Recent advances in video transformers that
jointly learn from vision, text, and sound over time have made some progress
toward this goal, but the degree to which these models integrate information
from modalities still remains unclear. In this work, we present a promising
approach for probing a pre-trained multimodal video transformer model by
leveraging neuroscientific evidence of multimodal information processing in the
brain. Using brain recordings of participants watching a popular TV show, we
analyze the effects of multi-modal connections and interactions in a
pre-trained multi-modal video transformer on the alignment with uni- and
multi-modal brain regions. We find evidence that vision enhances masked
prediction performance during language processing, providing support that
cross-modal representations in models can benefit individual modalities.
However, we don't find evidence of brain-relevant information captured by the
joint multi-modal transformer representations beyond that captured by all of
the individual modalities. We finally show that the brain alignment of the
pre-trained joint representation can be improved by fine-tuning using a task
that requires vision-language inferences. Overall, our results paint an
optimistic picture of the ability of multi-modal transformers to integrate
vision and language in partially brain-relevant ways but also show that
improving the brain alignment of these models may require new approaches.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07767" title="Abstract">arXiv:2311.07767</a> [<a href="/pdf/2311.07767" title="Download PDF">pdf</a>, <a href="/ps/2311.07767" title="Download PostScript">ps</a>, <a href="/format/2311.07767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GreekT5: A Series of Greek Sequence-to-Sequence Models for News  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giarelis%2C+N">Nikolaos Giarelis</a>, 
<a href="/search/cs?searchtype=author&query=Mastrokostas%2C+C">Charalampos Mastrokostas</a>, 
<a href="/search/cs?searchtype=author&query=Karacapilidis%2C+N">Nikos Karacapilidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Text summarization (TS) is a natural language processing (NLP) subtask
pertaining to the automatic formulation of a concise and coherent summary that
covers the major concepts and topics from one or multiple documents. Recent
advancements in deep learning have led to the development of abstractive
summarization transformer-based models, which outperform classical approaches.
In any case, research in this field focuses on high resource languages such as
English, while the corresponding work for low resource languages is still
underdeveloped. Taking the above into account, this paper proposes a series of
novel TS models for Greek news articles. The proposed models were thoroughly
evaluated on the same dataset against GreekBART, which is the state-of-the-art
model in Greek abstractive news summarization. Our evaluation results reveal
that most of the proposed models significantly outperform GreekBART on various
evaluation metrics. We make our evaluation code public, aiming to increase the
reproducibility of this work and facilitate future research in the field.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07768" title="Abstract">arXiv:2311.07768</a> [<a href="/pdf/2311.07768" title="Download PDF">pdf</a>, <a href="/format/2311.07768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Calibration and Uncertainty Quantification of a Rate-dependent  Cohesive Zone Model for Polymer Interfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thiagarajan%2C+P">Ponkrshnan Thiagarajan</a>, 
<a href="/search/cs?searchtype=author&query=Sain%2C+T">Trisha Sain</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Susanta Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be submitted for peer-review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">In the present work, a rate-dependent cohesive zone model for the fracture of
polymeric interfaces is presented. Inverse calibration of parameters for such
complex models through trial and error is computationally tedious due to the
large number of parameters and the high computational cost associated. The
obtained parameter values are often non-unique and the calibration inherits
higher uncertainty when the available experimental data is limited. To
alleviate these difficulties, a Bayesian calibration approach is used for the
proposed rate-dependent cohesive zone model in this work. The proposed cohesive
zone model accounts for both reversible elastic and irreversible rate-dependent
separation sliding deformation at the interface. The viscous dissipation due to
the irreversible opening at the interface is modeled using elastic-viscoplastic
kinematics that incorporates the effects of strain rate. To quantify the
uncertainty associated with the inverse parameter estimation, a modular
Bayesian approach is employed to calibrate the unknown model parameters,
accounting for the parameter uncertainty of the cohesive zone model. Further,
to quantify the model uncertainties, such as incorrect assumptions or missing
physics, a discrepancy function is introduced and it is approximated as a
Gaussian process. The improvement in the model predictions following the
introduction of a discrepancy function is demonstrated justifying the need for
a discrepancy term. Finally, the overall uncertainty of the model is quantified
in a predictive setting and the results are provided as confidence intervals. A
sensitivity analysis is also performed to understand the effect of the
variability of the inputs on the nature of the output.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07772" title="Abstract">arXiv:2311.07772</a> [<a href="/pdf/2311.07772" title="Download PDF">pdf</a>, <a href="/format/2311.07772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-context Learning and Gradient Descent Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nathan%2C+T+B">Tomer Bar Nathan</a>, 
<a href="/search/cs?searchtype=author&query=Deutch%2C+G">Gilad Deutch</a>, 
<a href="/search/cs?searchtype=author&query=Magar%2C+N">Nadav Magar</a>, 
<a href="/search/cs?searchtype=author&query=Dar%2C+G">Guy Dar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In-context learning (ICL) has shown impressive results in few-shot learning
tasks, yet its underlying mechanism is still not fully understood. Recent works
suggest that ICL can be thought of as a gradient descent (GD) based
optimization process. While promising, these results mainly focus on simplified
settings of ICL and provide only a preliminary evaluation of the similarities
between the two methods. In this work, we revisit the comparison between ICL
and GD-based finetuning and study what properties of ICL an equivalent process
must follow. We highlight a major difference in the flow of information between
ICL and standard finetuning. Namely, ICL can only rely on information from
lower layers at every point, while finetuning depends on loss gradients from
deeper layers. We refer to this discrepancy as Layer Causality and show that a
layer causal variant of the finetuning process aligns with ICL on par with
vanilla finetuning and is even better in most cases across relevant metrics. To
the best of our knowledge, this is the first work to discuss this discrepancy
explicitly and suggest a solution that tackles this problem with minimal
changes.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07780" title="Abstract">arXiv:2311.07780</a> [<a href="/pdf/2311.07780" title="Download PDF">pdf</a>, <a href="/format/2311.07780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parrot-Trained Adversarial Examples: Pushing the Practicality of  Black-Box Audio Attacks against Speaker Recognition Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+R">Rui Duan</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Z">Zhe Qu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Leah Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhuo Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio adversarial examples (AEs) have posed significant security challenges
to real-world speaker recognition systems. Most black-box attacks still require
certain information from the speaker recognition model to be effective (e.g.,
keeping probing and requiring the knowledge of similarity scores). This work
aims to push the practicality of the black-box attacks by minimizing the
attacker's knowledge about a target speaker recognition model. Although it is
not feasible for an attacker to succeed with completely zero knowledge, we
assume that the attacker only knows a short (or a few seconds) speech sample of
a target speaker. Without any probing to gain further knowledge about the
target model, we propose a new mechanism, called parrot training, to generate
AEs against the target model. Motivated by recent advancements in voice
conversion (VC), we propose to use the one short sentence knowledge to generate
more synthetic speech samples that sound like the target speaker, called parrot
speech. Then, we use these parrot speech samples to train a parrot-trained(PT)
surrogate model for the attacker. Under a joint transferability and perception
framework, we investigate different ways to generate AEs on the PT model
(called PT-AEs) to ensure the PT-AEs can be generated with high transferability
to a black-box target model with good human perceptual quality. Real-world
experiments show that the resultant PT-AEs achieve the attack success rates of
45.8% - 80.8% against the open-source models in the digital-line scenario and
47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon
Echo, and Google Home, in the over-the-air scenario.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07783" title="Abstract">arXiv:2311.07783</a> [<a href="/pdf/2311.07783" title="Download PDF">pdf</a>, <a href="/format/2311.07783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Size-Aware Hypergraph Motifs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+J">Jason Niu</a>, 
<a href="/search/cs?searchtype=author&query=Amburg%2C+I+D">Ilya D. Amburg</a>, 
<a href="/search/cs?searchtype=author&query=Aksoy%2C+S+G">Sinan G. Aksoy</a>, 
<a href="/search/cs?searchtype=author&query=Sar%C4%B1y%C3%BCce%2C+A+E">Ahmet Erdem Sar&#x131;y&#xfc;ce</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Social and Information Networks (cs.SI); Data Analysis, Statistics and Probability (physics.data-an); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Complex systems frequently exhibit multi-way, rather than pairwise,
interactions. These group interactions cannot be faithfully modeled as
collections of pairwise interactions using graphs, and instead require
hypergraphs. However, methods that analyze hypergraphs directly, rather than
via lossy graph reductions, remain limited. Hypergraph motif mining holds
promise in this regard, as motif patterns serve as building blocks for larger
group interactions which are inexpressible by graphs. Recent work has focused
on categorizing and counting hypergraph motifs based on the existence of nodes
in hyperedge intersection regions. Here, we argue that the relative sizes of
hyperedge intersections within motifs contain varied and valuable information.
We propose a suite of efficient algorithms for finding triplets of hyperedges
based on optimizing the sizes of these intersection patterns. This formulation
uncovers interesting local patterns of interaction, finding hyperedge triplets
that either (1) are the least correlated with each other, (2) have the highest
pairwise but not groupwise correlation, or (3) are the most correlated with
each other. We formalize this as a combinatorial optimization problem and
design efficient algorithms based on filtering hyperedges. Our experimental
evaluation shows that the resulting hyperedge triplets yield insightful
information on real-world hypergraphs. Our approach is also orders of magnitude
faster than a naive baseline implementation.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07784" title="Abstract">arXiv:2311.07784</a> [<a href="/pdf/2311.07784" title="Download PDF">pdf</a>, <a href="/format/2311.07784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated  Class Incremental Learning for Vision Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Babakniya%2C+S">Sara Babakniya</a>, 
<a href="/search/cs?searchtype=author&query=Fabian%2C+Z">Zalan Fabian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chaoyang He</a>, 
<a href="/search/cs?searchtype=author&query=Soltanolkotabi%2C+M">Mahdi Soltanolkotabi</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2023. arXiv admin note: text overlap with <a href="/abs/2307.00497">arXiv:2307.00497</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep learning models often suffer from forgetting previously learned
information when trained on new data. This problem is exacerbated in federated
learning (FL), where the data is distributed and can change independently for
each user. Many solutions are proposed to resolve this catastrophic forgetting
in a centralized setting. However, they do not apply directly to FL because of
its unique complexities, such as privacy concerns and resource limitations. To
overcome these challenges, this paper presents a framework for
\textbf{federated class incremental learning} that utilizes a generative model
to synthesize samples from past distributions. This data can be later exploited
alongside the training data to mitigate catastrophic forgetting. To preserve
privacy, the generative model is trained on the server using data-free methods
at the end of each task without requesting data from clients. Moreover, our
solution does not demand the users to store old data or models, which gives
them the freedom to join/leave the training at any time. Additionally, we
introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically
tailored for federated continual learning. We demonstrate significant
improvements compared to existing baselines through extensive experiments on
multiple datasets.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07786" title="Abstract">arXiv:2311.07786</a> [<a href="/pdf/2311.07786" title="Download PDF">pdf</a>, <a href="/format/2311.07786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting the First Response Latency of Maintainers and Contributors in  Pull Requests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khatoonabadi%2C+S">SayedHassan Khatoonabadi</a>, 
<a href="/search/cs?searchtype=author&query=Abdellatif%2C+A">Ahmad Abdellatif</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+D+E">Diego Elias Costa</a>, 
<a href="/search/cs?searchtype=author&query=Shihab%2C+E">Emad Shihab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript submitted to IEEE Transactions on Software Engineering (TSE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The success of a Pull Request (PR) depends on the responsiveness of the
maintainers and the contributor during the review process. Being aware of the
expected waiting times can lead to better interactions and managed expectations
for both the maintainers and the contributor. In this paper, we propose a
machine-learning approach to predict the first response latency of the
maintainers following the submission of a PR, and the first response latency of
the contributor after receiving the first response from the maintainers. We
curate a dataset of 20 large and popular open-source projects on GitHub and
extract 21 features to characterize projects, contributors, PRs, and review
processes. Using these features, we then evaluate seven types of classifiers to
identify the best-performing models. We also perform permutation feature
importance and SHAP analyses to understand the importance and impact of
different features on the predicted response latencies. Our best-performing
models achieve an average improvement of 33% in AUC-ROC and 58% in AUC-PR for
maintainers, as well as 42% in AUC-ROC and 95% in AUC-PR for contributors
compared to a no-skilled classifier across the projects. Our findings indicate
that PRs submitted earlier in the week, containing an average or slightly
above-average number of commits, and with concise descriptions are more likely
to receive faster first responses from the maintainers. Similarly, PRs with a
lower first response latency from maintainers, that received the first response
of maintainers earlier in the week, and containing an average or slightly
above-average number of commits tend to receive faster first responses from the
contributors. Additionally, contributors with a higher acceptance rate and a
history of timely responses in the project are likely to both obtain and
provide faster first responses.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07788" title="Abstract">arXiv:2311.07788</a> [<a href="/pdf/2311.07788" title="Download PDF">pdf</a>, <a href="/format/2311.07788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework  for Zero-Shot Electroencephalography Signal Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=N%C3%B8rskov%2C+A+V">Anders Vestergaard N&#xf8;rskov</a>, 
<a href="/search/cs?searchtype=author&query=Zahid%2C+A+N">Alexander Neergaard Zahid</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B8rup%2C+M">Morten M&#xf8;rup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Electroencephalography (EEG) is a prominent non-invasive neuroimaging
technique providing insights into brain function. Unfortunately, EEG data
exhibit a high degree of noise and variability across subjects hampering
generalizable signal extraction. Therefore, a key aim in EEG analysis is to
extract the underlying neural activation (content) as well as to account for
the individual subject variability (style). We hypothesize that the ability to
convert EEG signals between tasks and subjects requires the extraction of
latent representations accounting for content and style. Inspired by recent
advancements in voice conversion technologies, we propose a novel contrastive
split-latent permutation autoencoder (CSLP-AE) framework that directly
optimizes for EEG conversion. Importantly, the latent representations are
guided using contrastive learning to promote the latent splits to explicitly
represent subject (style) and task (content). We contrast CSLP-AE to
conventional supervised, unsupervised (AE), and self-supervised (contrastive
learning) training and find that the proposed approach provides favorable
generalizable characterizations of subject and task. Importantly, the procedure
also enables zero-shot conversion between unseen subjects. While the present
work only considers conversion of EEG, the proposed CSLP-AE provides a general
framework for signal conversion and extraction of content (task activation) and
style (subject variability) components of general interest for the modeling and
analysis of biological signals.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07789" title="Abstract">arXiv:2311.07789</a> [<a href="/pdf/2311.07789" title="Download PDF">pdf</a>, <a href="/format/2311.07789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Level-k Thinking in the Extensive Form
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schipper%2C+B+C">Burkhard C. Schipper</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hang Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Level-$k$ thinking and Cognitive Hierarchy have been widely applied as a
normal-form solution concept in behavioral and experimental game theory. We
consider level-k thinking in games in extensive form. Player's may learn about
levels of opponents' thinking during the play of the game because some
information sets may be inconsistent with certain levels. In particular, for
any information set reached, a level-$k$ player attaches the maximum
level-$\ell$ thinking for $\ell &lt; k$ to her opponents consistent with the
information set. We compare our notion of strong level-$k$ thinking with other
solution concepts such as level-$k$ thinking in the associated normal form,
strong rationalizability, $\Delta$-rationalizability, iterated admissibility,
backward rationalizability, backward level-$k$ thinking, and backward
induction. We use strong level-$k$ thinking to reanalyze data from some prior
experiments in the literature.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07790" title="Abstract">arXiv:2311.07790</a> [<a href="/pdf/2311.07790" title="Download PDF">pdf</a>, <a href="/format/2311.07790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Hamilton-Jacobi PDEs with time-dependent Hamiltonians for  continual scientific machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Paula Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+T">Tingwei Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zongren Zou</a>, 
<a href="/search/cs?searchtype=author&query=Darbon%2C+J">J&#xe9;r&#xf4;me Darbon</a>, 
<a href="/search/cs?searchtype=author&query=Karniadakis%2C+G+E">George Em Karniadakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We address two major challenges in scientific machine learning (SciML):
interpretability and computational efficiency. We increase the interpretability
of certain learning processes by establishing a new theoretical connection
between optimization problems arising from SciML and a generalized Hopf
formula, which represents the viscosity solution to a Hamilton-Jacobi partial
differential equation (HJ PDE) with time-dependent Hamiltonian. Namely, we show
that when we solve certain regularized learning problems with integral-type
losses, we actually solve an optimal control problem and its associated HJ PDE
with time-dependent Hamiltonian. This connection allows us to reinterpret
incremental updates to learned models as the evolution of an associated HJ PDE
and optimal control problem in time, where all of the previous information is
intrinsically encoded in the solution to the HJ PDE. As a result, existing HJ
PDE solvers and optimal control algorithms can be reused to design new
efficient training approaches for SciML that naturally coincide with the
continual learning framework, while avoiding catastrophic forgetting. As a
first exploration of this connection, we consider the special case of linear
regression and leverage our connection to develop a new Riccati-based
methodology for solving these learning problems that is amenable to continual
learning applications. We also provide some corresponding numerical examples
that demonstrate the potential computational and memory advantages our
Riccati-based approach can provide.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07792" title="Abstract">arXiv:2311.07792</a> [<a href="/pdf/2311.07792" title="Download PDF">pdf</a>, <a href="/format/2311.07792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Western, Religious or Spiritual: An Evaluation of Moral Justification in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kucuk%2C+E+E">Eyup Engin Kucuk</a>, 
<a href="/search/cs?searchtype=author&query=Kocyigit%2C+M+Y">Muhammed Yusuf Kocyigit</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages total, 8 pages main paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The increasing success of Large Language Models (LLMs) in variety of tasks
lead to their widespread use in our lives which necessitates the examination of
these models from different perspectives. The alignment of these models to
human values is an essential concern in order to establish trust that we have
safe and responsible systems. In this paper, we aim to find out which values
and principles are embedded in LLMs in the process of moral justification. For
this purpose, we come up with three different moral perspective categories:
Western tradition perspective (WT), Abrahamic tradition perspective (AT), and
Spiritualist/Mystic tradition perspective (SMT). In two different experiment
settings, we asked models to choose principles from the three for suggesting a
moral action and evaluating the moral permissibility of an action if one tries
to justify an action on these categories, respectively. Our experiments
indicate that tested LLMs favors the Western tradition moral perspective over
others. Additionally, we observe that there potentially exists an
over-alignment towards religious values represented in the Abrahamic Tradition,
which causes models to fail to recognize an action is immoral if it is
presented as a "religious-action". We believe that these results are essential
in order to direct our attention in future efforts.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07797" title="Abstract">arXiv:2311.07797</a> [<a href="/pdf/2311.07797" title="Download PDF">pdf</a>, <a href="/format/2311.07797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable History Distillation by Marked Temporal Point Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sishun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+K">Ke Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiuzhen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Explainability of machine learning models is mandatory when researchers
introduce these commonly believed black boxes to real-world tasks, especially
high-stakes ones. In this paper, we build a machine learning system to
automatically generate explanations of happened events from history by \gls{ca}
based on the \acrfull{tpp}. Specifically, we propose a new task called
\acrfull{ehd}. This task requires a model to distill as few events as possible
from observed history. The target is that the event distribution conditioned on
left events predicts the observed future noticeably worse. We then regard
distilled events as the explanation for the future. To efficiently solve
\acrshort{ehd}, we rewrite the task into a \gls{01ip} and directly estimate the
solution to the program by a model called \acrfull{model}. This work fills the
gap between our task and existing works, which only spot the difference between
factual and counterfactual worlds after applying a predefined modification to
the environment. Experiment results on Retweet and StackOverflow datasets prove
that \acrshort{model} significantly outperforms other \acrshort{ehd} baselines
and can reveal the rationale underpinning real-world processes.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07798" title="Abstract">arXiv:2311.07798</a> [<a href="/pdf/2311.07798" title="Download PDF">pdf</a>, <a href="/format/2311.07798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Physics-integrated Neural Differentiable Modeling for  Isothermal Chemical Vapor Infiltration Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akhare%2C+D">Deepak Akhare</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zeping Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gulotty%2C+R">Richard Gulotty</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tengfei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian-Xun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Chemical vapor infiltration (CVI) is a widely adopted manufacturing technique
used in producing carbon-carbon and carbon-silicon carbide composites. These
materials are especially valued in the aerospace and automotive industries for
their robust strength and lightweight characteristics. The densification
process during CVI critically influences the final performance, quality, and
consistency of these composite materials. Experimentally optimizing the CVI
processes is challenging due to long experimental time and large optimization
space. To address these challenges, this work takes a modeling-centric
approach. Due to the complexities and limited experimental data of the
isothermal CVI densification process, we have developed a data-driven
predictive model using the physics-integrated neural differentiable (PiNDiff)
modeling framework. An uncertainty quantification feature has been embedded
within the PiNDiff method, bolstering the model's reliability and robustness.
Through comprehensive numerical experiments involving both synthetic and
real-world manufacturing data, the proposed method showcases its capability in
modeling densification during the CVI process. This research highlights the
potential of the PiNDiff framework as an instrumental tool for advancing our
understanding, simulation, and optimization of the CVI manufacturing process,
particularly when faced with sparse data and an incomplete description of the
underlying physics.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07803" title="Abstract">arXiv:2311.07803</a> [<a href="/pdf/2311.07803" title="Download PDF">pdf</a>, <a href="/format/2311.07803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing Theory of Computing Backwards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dougherty%2C+R+E">Ryan E. Dougherty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SIGCSE TS 2024 (poster)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The design of any technical Computer Science course must involve its context
within the institution's CS program, but also incorporate any new material that
is relevant and appropriately accessible to students. In many institutions,
theory of computing (ToC) courses within undergraduate CS programs are often
placed near the end of the program, and have a very common structure of
building off previous sections of the course. The central question behind any
such course is ``What are the limits of computers?'' for various types of
computational models. However, what is often intuitive for students about what
a ``computer'' is--a Turing machine--is taught at the end of the course, which
necessitates motivation for earlier models. This poster contains our
experiences in designing a ToC course that teaches the material effectively
``backwards,'' with pedagogic motivation of instead asking the question ``What
suitable restrictions can we place on computers to make their problems
tractable?'' We also give recommendations for future course design.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07804" title="Abstract">arXiv:2311.07804</a> [<a href="/pdf/2311.07804" title="Download PDF">pdf</a>, <a href="/format/2311.07804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IruMozhi: Automatically classifying diglossia in Tamil
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+K">Kabilan Prasanna</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Aryaman Arora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages main text, 7 total
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Tamil, a Dravidian language of South Asia, is a highly diglossic language
with two very different registers in everyday use: Literary Tamil (preferred in
writing and formal communication) and Spoken Tamil (confined to speech and
informal media). Spoken Tamil is under-supported in modern NLP systems. In this
paper, we release IruMozhi, a human-annotated dataset of parallel text in
Literary and Spoken Tamil. We train classifiers on the task of identifying
which variety a text belongs to. We use these models to gauge the availability
of pretraining data in Spoken Tamil, to audit the composition of existing
labelled datasets for Tamil, and to encourage future work on the variety.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07806" title="Abstract">arXiv:2311.07806</a> [<a href="/pdf/2311.07806" title="Download PDF">pdf</a>, <a href="/format/2311.07806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Test-time Variability for Interactive 3D Medical Image  Segmentation with Diverse Point Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+D">Dewei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiacheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Oguz%2C+I">Ipek Oguz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Interactive segmentation model leverages prompts from users to produce robust
segmentation. This advancement is facilitated by prompt engineering, where
interactive prompts serve as strong priors during test-time. However, this is
an inherently subjective and hard-to-reproduce process. The variability in user
expertise and inherently ambiguous boundaries in medical images can lead to
inconsistent prompt selections, potentially affecting segmentation accuracy.
This issue has not yet been extensively explored for medical imaging. In this
paper, we assess the test-time variability for interactive medical image
segmentation with diverse point prompts. For a given target region, the point
is classified into three sub-regions: boundary, margin, and center. Our goal is
to identify a straightforward and efficient approach for optimal prompt
selection during test-time based on three considerations: (1) benefits of
additional prompts, (2) effects of prompt placement, and (3) strategies for
optimal prompt selection. We conduct extensive experiments on the public
Medical Segmentation Decathlon dataset for challenging colon tumor segmentation
task. We suggest an optimal strategy for prompt selection during test-time,
supported by comprehensive results. The code is publicly available at
https://github.com/MedICL-VU/variability
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07807" title="Abstract">arXiv:2311.07807</a> [<a href="/pdf/2311.07807" title="Download PDF">pdf</a>, <a href="/format/2311.07807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Creation of a CS1 Course with Modern C++ Principles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dougherty%2C+R+E">Ryan E. Dougherty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SIGCSE TS 2024 (poster)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Best practices in programming need to be emphasized in a CS1 course as bad
student habits persist if not reinforced well. The C++ programming language,
although a relatively old language, has been regularly updated with new
versions since 2011, on the pace of once every three years. Each new version
contains important features that make the C++ language more complex for
backwards compatibility, but often introduce new features to make common use
cases simpler to implement. This poster contains experiences in designing a CS1
course that uses the C++ programming language that incorporates ``modern''
versions of the language from the start, as well as recent conferences about
the language. Our goals were to prevent many common bad habits among C++
programmers.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07808" title="Abstract">arXiv:2311.07808</a> [<a href="/pdf/2311.07808" title="Download PDF">pdf</a>, <a href="/format/2311.07808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Primal-Dual Analysis of Monotone Submodular Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+D">Deeparnab Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Cote%2C+L">Luc Cote</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In this paper we design a new primal-dual algorithm for the classic discrete
optimization problem of maximizing a monotone submodular function subject to a
cardinality constraint achieving the optimal approximation of $(1-1/e)$. This
problem and its special case, the maximum $k$-coverage problem, have a wide
range of applications in various fields including operations research, machine
learning, and economics. While greedy algorithms have been known to achieve
this approximation factor, our algorithms also provide a dual certificate which
upper bounds the optimum value of any instance. This certificate may be used in
practice to certify much stronger guarantees than the worst-case $(1-1/e)$
approximation factor.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07811" title="Abstract">arXiv:2311.07811</a> [<a href="/pdf/2311.07811" title="Download PDF">pdf</a>, <a href="/format/2311.07811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-context Learning Generalizes, But Not Always Robustly: The Case of  Syntax
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mueller%2C+A">Aaron Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Webson%2C+A">Albert Webson</a>, 
<a href="/search/cs?searchtype=author&query=Petty%2C+J">Jackson Petty</a>, 
<a href="/search/cs?searchtype=author&query=Linzen%2C+T">Tal Linzen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-context learning (ICL) is now a common method for supervising large
language models (LLMs): given labeled examples in the input context, the LLM
learns to perform the task without weight updates. Despite ICL's prevalence and
utility, we understand little about whether models supervised in this manner
represent the underlying structure of their tasks, rather than superficial
heuristics that only generalize to identically distributed examples. In this
study, we investigate the robustness of LLMs supervised via ICL using the test
case of sensitivity to syntax, which is a prerequisite for robust language
understanding. Our experiments are based on two simple and well-controlled
syntactic transformations tasks, where correct out-of-distribution
generalization requires an accurate syntactic analysis of the input. We further
investigate whether out-of-distribution generalization can be improved via
chain-of-thought prompting, where the model is provided with a sequence of
intermediate computation steps that illustrate how the task ought to be
performed. In experiments with models from the GPT, PaLM, and Llama 2 families,
we find large variance across LMs on this fundamental linguistic phenomenon,
and that the variance is explained more by the composition of the pre-training
corpus and supervision methods than by model size. In particular, we find
evidence that models pre-trained on code generalize better, and benefit to a
greater extent from chain-of-thought prompting.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07814" title="Abstract">arXiv:2311.07814</a> [<a href="/pdf/2311.07814" title="Download PDF">pdf</a>, <a href="/format/2311.07814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel and simple spectral method for nonlocal PDEs with the fractional  Laplacian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhou%2C+S">Shiping Zhou</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yanzhi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a novel and simple spectral method based on the semi-discrete
Fourier transforms to discretize the fractional Laplacian
$(-\Delta)^\frac{\alpha}{2}$. Numerical analysis and experiments are provided
to study its performance. Our method has the same symbol $|\xi|^\alpha$ as the
fractional Laplacian $(-\Delta)^\frac{\alpha}{2}$ at the discrete level, and
thus it can be viewed as the exact discrete analogue of the fractional
Laplacian. This {\it unique feature} distinguishes our method from other
existing methods for the fractional Laplacian. Note that our method is
different from the Fourier pseudospectral methods in the literature, which are
usually limited to periodic boundary conditions (see Remark \ref{remark0}).
Numerical analysis shows that our method can achieve a spectral accuracy. The
stability and convergence of our method in solving the fractional Poisson
equations were analyzed. Our scheme yields a multilevel Toeplitz stiffness
matrix, and thus fast algorithms can be developed for efficient matrix-vector
products. The computational complexity is ${\mathcal O}(2N\log(2N))$, and the
memory storage is ${\mathcal O}(N)$ with $N$ the total number of points.
Extensive numerical experiments verify our analytical results and demonstrate
the effectiveness of our method in solving various problems.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07815" title="Abstract">arXiv:2311.07815</a> [<a href="/pdf/2311.07815" title="Download PDF">pdf</a>, <a href="/format/2311.07815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative AI via Decentralized Commitment Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xinyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Crapis%2C+D">Davide Crapis</a>, 
<a href="/search/cs?searchtype=author&query=Stephenson%2C+M">Matt Stephenson</a>, 
<a href="/search/cs?searchtype=author&query=Monnot%2C+B">Barnab&#xe9; Monnot</a>, 
<a href="/search/cs?searchtype=author&query=Thiery%2C+T">Thomas Thiery</a>, 
<a href="/search/cs?searchtype=author&query=Passerat-Palmbach%2C+J">Jonathan Passerat-Palmbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023- Multi-Agent Security Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Credible commitment devices have been a popular approach for robust
multi-agent coordination. However, existing commitment mechanisms face
limitations like privacy, integrity, and susceptibility to mediator or user
strategic behavior. It is unclear if the cooperative AI techniques we study are
robust to real-world incentives and attack vectors. However, decentralized
commitment devices that utilize cryptography have been deployed in the wild,
and numerous studies have shown their ability to coordinate algorithmic agents
facing adversarial opponents with significant economic incentives, currently in
the order of several million to billions of dollars. In this paper, we use
examples in the decentralization and, in particular, Maximal Extractable Value
(MEV) (<a href="/abs/1904.05234">arXiv:1904.05234</a>) literature to illustrate the potential security issues
in cooperative AI. We call for expanded research into decentralized commitments
to advance cooperative AI capabilities for secure coordination in open
environments and empirical testing frameworks to evaluate multi-agent
coordination ability given real-world commitment constraints.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07816" title="Abstract">arXiv:2311.07816</a> [<a href="/pdf/2311.07816" title="Download PDF">pdf</a>, <a href="/format/2311.07816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Large Language Models to Detect Influence Campaigns in Social  Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>, 
<a href="/search/cs?searchtype=author&query=Boniardi%2C+E">Eric Boniardi</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Social media influence campaigns pose significant challenges to public
discourse and democracy. Traditional detection methods fall short due to the
complexity and dynamic nature of social media. Addressing this, we propose a
novel detection method using Large Language Models (LLMs) that incorporates
both user metadata and network structures. By converting these elements into a
text format, our approach effectively processes multilingual content and adapts
to the shifting tactics of malicious campaign actors. We validate our model
through rigorous testing on multiple datasets, showcasing its superior
performance in identifying influence efforts. This research not only offers a
powerful tool for detecting campaigns, but also sets the stage for future
enhancements to keep up with the fast-paced evolution of social media-based
influence tactics.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07818" title="Abstract">arXiv:2311.07818</a> [<a href="/pdf/2311.07818" title="Download PDF">pdf</a>, <a href="/format/2311.07818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Container Resource Allocation versus Performance of Data-intensive  Applications on Different Cloud Servers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+S">Snigdhaswin Kar</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+P">Prabodh Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Linduff%2C+C">Caleb Linduff</a>, 
<a href="/search/cs?searchtype=author&query=Izard%2C+R">Ryan Izard</a>, 
<a href="/search/cs?searchtype=author&query=Anjam%2C+K">Khayam Anjam</a>, 
<a href="/search/cs?searchtype=author&query=Barrineau%2C+G">Geddings Barrineau</a>, 
<a href="/search/cs?searchtype=author&query=Zulfiqar%2C+J">Junaid Zulfiqar</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kuang-Ching Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In recent years, data-intensive applications have been increasingly deployed
on cloud systems. Such applications utilize significant compute, memory, and
I/O resources to process large volumes of data. Optimizing the performance and
cost-efficiency for such applications is a non-trivial problem. The problem
becomes even more challenging with the increasing use of containers, which are
popular due to their lower operational overheads and faster boot speed at the
cost of weaker resource assurances for the hosted applications. In this paper,
two containerized data-intensive applications with very different performance
objectives and resource needs were studied on cloud servers with Docker
containers running on Intel Xeon E5 and AMD EPYC Rome multi-core processors
with a range of CPU, memory, and I/O configurations. Primary findings from our
experiments include: 1) Allocating multiple cores to a compute-intensive
application can improve performance, but only if the cores do not contend for
the same caches, and the optimal core counts depend on the specific workload;
2) allocating more memory to a memory-intensive application than its
deterministic data workload does not further improve performance; however, 3)
having multiple such memory-intensive containers on the same server can lead to
cache and memory bus contention leading to significant and volatile performance
degradation. The comparative observations on Intel and AMD servers provided
insights into trade-offs between larger numbers of distributed chiplets
interconnected with higher speed buses (AMD) and larger numbers of centrally
integrated cores and caches with lesser speed buses (Intel). For the two types
of applications studied, the more distributed caches and faster data buses have
benefited the deployment of larger numbers of containers.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07820" title="Abstract">arXiv:2311.07820</a> [<a href="/pdf/2311.07820" title="Download PDF">pdf</a>, <a href="/format/2311.07820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Analysis of Cross-Lingual Prompt Tuning for Decoder-based  Multilingual Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+N">Nohil Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Joonsuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K+M">Kang Min Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sungroh Yoon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">An exciting advancement in the field of multilingual models is the emergence
of autoregressive models with zero- and few-shot capabilities, a phenomenon
widely reported in large-scale language models. To further improve model
adaptation to cross-lingual tasks, another trend is to further fine-tune the
language models with either full fine-tuning or parameter-efficient tuning.
However, the interaction between parameter-efficient fine-tuning (PEFT) and
cross-lingual tasks in multilingual autoregressive models has yet to be
studied. Specifically, we lack an understanding of the role of linguistic
distributions in multilingual models in the effectiveness of token-based prompt
tuning. To address this question, we conduct experiments comparing prompt
tuning and fine-tuning on the decoder-based multilingual model, XGLM, with four
cross-lingual tasks (XNLI, PAWS-X, POS, NER). According to our study, prompt
tuning achieves on par or better performance over fine-tuning across all
languages while updating at most 0.13\% of the model parameters. Moreover, we
empirically show that prompt tuning is more effective in enhancing the
performance of low-resource languages than fine-tuning. Our further analysis
shows that the phenomenon is related to the tokenization scheme of the
multilingual model.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07821" title="Abstract">arXiv:2311.07821</a> [<a href="/pdf/2311.07821" title="Download PDF">pdf</a>, <a href="/format/2311.07821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Parameterized Physics-Based Machine Learning Digital Twin  Models for Laser Powder Bed Fusion Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangfan Li</a>, 
<a href="/search/cs?searchtype=author&query=Mojumder%2C+S">Satyajit Mojumder</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Ye Lu</a>, 
<a href="/search/cs?searchtype=author&query=Amin%2C+A+A">Abdullah Al Amin</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiachen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaoyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+G+J">Gregory J. Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jian Cao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W+K">Wing Kam Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2208.02907">arXiv:2208.02907</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Numerical Analysis (math.NA); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">A digital twin (DT) is a virtual representation of physical process, products
and/or systems that requires a high-fidelity computational model for continuous
update through the integration of sensor data and user input. In the context of
laser powder bed fusion (LPBF) additive manufacturing, a digital twin of the
manufacturing process can offer predictions for the produced parts, diagnostics
for manufacturing defects, as well as control capabilities. This paper
introduces a parameterized physics-based digital twin (PPB-DT) for the
statistical predictions of LPBF metal additive manufacturing process. We
accomplish this by creating a high-fidelity computational model that accurately
represents the melt pool phenomena and subsequently calibrating and validating
it through controlled experiments. In PPB-DT, a mechanistic reduced-order
method-driven stochastic calibration process is introduced, which enables the
statistical predictions of the melt pool geometries and the identification of
defects such as lack-of-fusion porosity and surface roughness, specifically for
diagnostic applications. Leveraging data derived from this physics-based model
and experiments, we have trained a machine learning-based digital twin
(PPB-ML-DT) model for predicting, monitoring, and controlling melt pool
geometries. These proposed digital twin models can be employed for predictions,
control, optimization, and quality assurance within the LPBF process,
ultimately expediting product development and certification in LPBF-based metal
additive manufacturing.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07822" title="Abstract">arXiv:2311.07822</a> [<a href="/pdf/2311.07822" title="Download PDF">pdf</a>, <a href="/format/2311.07822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neuro-Inspired Hierarchical Reinforcement Learning for Motor Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Z">Zhaobo Hua</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jinliang Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Designing controllers to achieve natural motion capabilities for multi-joint
robots is a significant challenge. However, animals in nature are naturally
with basic motor abilities and can master various complex motor skills through
acquired learning. On the basis of analyzing the mechanism of the central motor
system in mammals, we propose a neuro-inspired hierarchical reinforcement
learning algorithm that enables robots to learn rich motor skills and apply
them to complex task environments without relying on external data. We first
design a skills network similar to the cerebellum by utilizing the selection
mechanism of voluntary movements in the basal ganglia and the regulatory
ability of the cerebellum to regulate movement. Subsequently, by imitating the
structure of advanced centers in the motion system, we propose a high-level
policy to generate different skill combinations, thereby enabling the robot to
acquire natural motor abilities. We conduct experiments on 4 types of robots
and 22 task environments, and the results show that the proposed method can
enable different types of robots to achieve flexible motion skills. Overall,
our research provides a promising framework for the design of robotic neural
motor controllers.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07826" title="Abstract">arXiv:2311.07826</a> [<a href="/pdf/2311.07826" title="Download PDF">pdf</a>, <a href="/ps/2311.07826" title="Download PostScript">ps</a>, <a href="/format/2311.07826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Search Optimization: Dynamic Algorithm Selection and Caching  for Enhanced Database Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+H">Hakikat Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Efficient search operations in databases are paramount for timely retrieval
of information various applications. This research introduces a novel approach,
combining dynamicalgorithm1 selection and caching2 strategies, to optimize
search performance. The proposed dynamic search algorithm intelligently
switches between Binary3 and Interpolation 4 Search based on dataset
characteristics, significantly improving efficiency for non-uniformly
distributed data. Additionally, a robust caching mechanism5 stores and
retrieves previous search results, further enhancing computational efficiency6.
Theoretical analysis and extensive experiments demonstrate the effectiveness of
the approach, showcasing its potential to revolutionize database performance7
in scenarios with diverse data distributions. This research contributes
valuable insights and practical solutions to the realm of database
optimization, offering a promising avenue for enhancing search operations in
real-world applications
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07829" title="Abstract">arXiv:2311.07829</a> [<a href="/pdf/2311.07829" title="Download PDF">pdf</a>, <a href="/ps/2311.07829" title="Download PostScript">ps</a>, <a href="/format/2311.07829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Coding Scheme for Straggler Resilient Quantum $X$-Secure $T$-Private  Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuxiang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jafar%2C+S+A">Syed A. Jafar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Building on recent constructions of Quantum Cross Subspace Alignment (QCSA)
codes, this work develops a coding scheme for QEXSTPIR, i.e., classical private
information retrieval with $X$-secure storage and $T$-private queries, over a
quantum multiple access channel, that is resilient to any set of up to $E$
erased servers (equivalently known as unresponsive servers, or stragglers). The
scheme is accordingly labeled QECSA, with the `E' indicating resilience to
erased servers. The novelty of QECSA lies in achieving efficient $E$-straggler
resilience on top of existing QCSA codes that already achieve $X$-secure
storage, $T$-private queries, and distributed superdense coding gains for
communication efficient decoding. The QECSA code structure may be broadly
useful for problems such as quantum coded secure distributed computation, where
security, straggler resilience, and distributed superdense coding gains are
simultaneously required.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07831" title="Abstract">arXiv:2311.07831</a> [<a href="/pdf/2311.07831" title="Download PDF">pdf</a>, <a href="/ps/2311.07831" title="Download PostScript">ps</a>, <a href="/format/2311.07831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covering Codes, List-Decodable Codes and Strong Singleton-Like Bounds in  the Sum-Rank Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Codes in the sum-rank metric have received many attentions in recent years,
since they have wide applications in multishot network coding, space-time
coding and distributed storage. Fundamental bounds, some explicit or
probabilistic constructions of sum-rank codes and decoding algorithms have been
developed in previous papers. In this paper, we construct covering codes in the
sum-rank metric from covering codes in the Hamming metric. Then some upper
bounds on sizes of covering codes in the sum-rank metric are presented. Block
length functions of covering codes in the sum-rank metric are also introduced
and studied. As an application of our upper bounds on covering codes in the
sum-rank metric and block length functions, several strong Singleton-like
bounds on sum-rank codes are proposed and proved. These strong Singleton-like
bounds are much stronger than the Singleton-like bound for sum-rank codes, when
block lengths are larger and minimum sum-rank distances are small. An upper
bound on sizes of list-decodable codes in the sum-rank metric is also given,
which leads to an asymptotic bound on list-decodability of sum-rank codes. We
also give upper bounds on block lengths of general MSRD codes.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07833" title="Abstract">arXiv:2311.07833</a> [<a href="/pdf/2311.07833" title="Download PDF">pdf</a>, <a href="/ps/2311.07833" title="Download PostScript">ps</a>, <a href="/format/2311.07833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Efficient and Incremental Spectral Clustering via Parametric  Spectral Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jo-Chun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hung-Hsuan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spectral clustering is a popular method for effectively clustering
nonlinearly separable data. However, computational limitations, memory
requirements, and the inability to perform incremental learning challenge its
widespread application. To overcome these limitations, this paper introduces a
novel approach called parametric spectral clustering (PSC). By extending the
capabilities of spectral clustering, PSC addresses the challenges associated
with big data and real-time scenarios and enables efficient incremental
clustering with new data points. Experimental evaluations conducted on various
open datasets demonstrate the superiority of PSC in terms of computational
efficiency while achieving clustering quality mostly comparable to standard
spectral clustering. The proposed approach has significant potential for
incremental and real-time data analysis applications, facilitating timely and
accurate clustering in dynamic and evolving datasets. The findings of this
research contribute to the advancement of clustering techniques and open new
avenues for efficient and effective data analysis. We publish the experimental
code at https://github.<a href="/abs/com/1095025">com/1095025</a>18/PSC_BigData.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07838" title="Abstract">arXiv:2311.07838</a> [<a href="/pdf/2311.07838" title="Download PDF">pdf</a>, <a href="/format/2311.07838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLatrieval: LLM-Verified Retrieval for Verifiable Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Changtai Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhangyue Yin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Verifiable generation aims to let the large language model (LLM) generate
text with corresponding supporting documents, which enables the user to
flexibly verify the answer and makes it more trustworthy. Its evaluation not
only measures the correctness of the answer, but also the answer's
verifiability, i.e., how well the answer is supported by the corresponding
documents. In typical, verifiable generation adopts the retrieval-read
pipeline, which is divided into two stages: 1) retrieve relevant documents of
the question. 2) according to the documents, generate the corresponding answer.
Since the retrieved documents can supplement knowledge for the LLM to generate
the answer and serve as evidence, the retrieval stage is essential for the
correctness and verifiability of the answer. However, the widely used
retrievers become the bottleneck of the entire pipeline and limit the overall
performance. They often have fewer parameters than the large language model and
have not been proven to scale well to the size of LLMs. Since the LLM passively
receives the retrieval result, if the retriever does not correctly find the
supporting documents, the LLM can not generate the correct and verifiable
answer, which overshadows the LLM's remarkable abilities. In this paper, we
propose LLatrieval (Large Language Model Verified Retrieval), where the LLM
updates the retrieval result until it verifies that the retrieved documents can
support answering the question. Thus, the LLM can iteratively provide feedback
to retrieval and facilitate the retrieval result to sufficiently support
verifiable generation. Experimental results show that our method significantly
outperforms extensive baselines and achieves new state-of-the-art results.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07840" title="Abstract">arXiv:2311.07840</a> [<a href="/pdf/2311.07840" title="Download PDF">pdf</a>, <a href="/format/2311.07840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Decision-Support Systems through Automated Cell Tower Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krell%2C+N">Natasha Krell</a>, 
<a href="/search/cs?searchtype=author&query=Gleave%2C+W">Will Gleave</a>, 
<a href="/search/cs?searchtype=author&query=Nakada%2C+D">Daniel Nakada</a>, 
<a href="/search/cs?searchtype=author&query=Downes%2C+J">Justin Downes</a>, 
<a href="/search/cs?searchtype=author&query=Willet%2C+A">Amanda Willet</a>, 
<a href="/search/cs?searchtype=author&query=Baran%2C+M">Matthew Baran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Cell phone coverage and high-speed service gaps persist in rural areas in
sub-Saharan Africa, impacting public access to mobile-based financial,
educational, and humanitarian services. Improving maps of telecommunications
infrastructure can help inform strategies to eliminate gaps in mobile coverage.
Deep neural networks, paired with remote sensing images, can be used for object
detection of cell towers and eliminate the need for inefficient and burdensome
manual mapping to find objects over large geographic regions. In this study, we
demonstrate a partially automated workflow to train an object detection model
to locate cell towers using OpenStreetMap (OSM) features and high-resolution
Maxar imagery. For model fine-tuning and evaluation, we curated a diverse
dataset of over 6,000 unique images of cell towers in 26 countries in eastern,
southern, and central Africa using automatically generated annotations from OSM
points. Our model achieves an average precision at 50% Intersection over Union
(IoU) (AP@50) of 81.2 with good performance across different geographies and
out-of-sample testing. Accurate localization of cell towers can yield more
accurate cell coverage maps, in turn enabling improved delivery of digital
services for decision-support applications.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07841" title="Abstract">arXiv:2311.07841</a> [<a href="/pdf/2311.07841" title="Download PDF">pdf</a>, <a href="/format/2311.07841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEMS: Pre-trained Epidmic Time-series Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamarthi%2C+H">Harshavardhan Kamarthi</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B. Aditya Prakash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Providing accurate and reliable predictions about the future of an epidemic
is an important problem for enabling informed public health decisions. Recent
works have shown that leveraging data-driven solutions that utilize advances in
deep learning methods to learn from past data of an epidemic often outperform
traditional mechanistic models. However, in many cases, the past data is sparse
and may not sufficiently capture the underlying dynamics. While there exists a
large amount of data from past epidemics, leveraging prior knowledge from
time-series data of other diseases is a non-trivial challenge. Motivated by the
success of pre-trained models in language and vision tasks, we tackle the
problem of pre-training epidemic time-series models to learn from multiple
datasets from different diseases and epidemics. We introduce Pre-trained
Epidemic Time-Series Models (PEMS) that learn from diverse time-series datasets
of a variety of diseases by formulating pre-training as a set of
self-supervised learning (SSL) tasks. We tackle various important challenges
specific to pre-training for epidemic time-series such as dealing with
heterogeneous dynamics and efficiently capturing useful patterns from multiple
epidemic datasets by carefully designing the SSL tasks to learn important
priors about the epidemic dynamics that can be leveraged for fine-tuning to
multiple downstream tasks. The resultant PEM outperforms previous
state-of-the-art methods in various downstream time-series tasks across
datasets of varying seasonal patterns, geography, and mechanism of contagion
including the novel Covid-19 pandemic unseen in pre-trained data with better
efficiency using smaller fraction of datasets.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07842" title="Abstract">arXiv:2311.07842</a> [<a href="/pdf/2311.07842" title="Download PDF">pdf</a>, <a href="/format/2311.07842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replay Clocks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lagwankar%2C+I">Ishaan Lagwankar</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S+S">Sandeep S Kulkarni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In this work, we focus on the problem of replay clocks (RepCL). The need for
replay clocks arises from the observation that analyzing distributed
computation for all desired properties of interest may not be feasible in an
online environment. These properties can be analyzed by replaying the
computation. However, to be beneficial, such replay must account for all the
uncertainty that is possible in a distributed computation. Specifically, if
event 'e' must occur before 'f' then the replay clock must ensure that 'e' is
replayed before 'f'. On the other hand, if 'e' and 'f' could occur in any order
then replay should not force an order between them.
<br />After identifying the limitations of existing clocks to provide the replay
primitive, we present RepCL and identify an efficient representation for the
same. We demonstrate that RepCL can be implemented with less than four integers
for 64 processes for various system parameters if clocks are synchronized
within 1 ms. Furthermore, the overhead of RepCL (for computing/comparing
timestamps and message size) is proportional to the size of the clock. Using
simulations, we identify the expected overhead of RepCL based on the given
system settings. We also identify how a user can the identify feasibility
region for RepCL. Specifically, given the desired overhead of RepCL, it
identifies the region where unabridged replay is possible.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07843" title="Abstract">arXiv:2311.07843</a> [<a href="/pdf/2311.07843" title="Download PDF">pdf</a>, <a href="/ps/2311.07843" title="Download PostScript">ps</a>, <a href="/format/2311.07843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the IRS Deployment in Smart Factories Considering Blockage Effects:  Collocated or Distributed?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Khosravirad%2C+S+R">Saeed R. Khosravirad</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiaoli Chu</a>, 
<a href="/search/cs?searchtype=author&query=Uusitalo%2C+M+A">Mikko A. Uusitalo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this article, we study the collocated and distributed deployment of
intelligent reflecting surfaces (IRS) for a fixed total number of IRS elements
to support enhanced mobile broadband (eMBB) and ultra-reliable low-latency
communication (URLLC) services inside a factory. We build a channel model that
incorporates the line-of-sight (LOS) probability and power loss of each
transmission path, and propose three metrics, namely, the expected received
signal-to-noise ratio (SNR), expected finite-blocklength (FB) capacity, and
expected outage probability, where the expectation is taken over the
probability distributions of interior blockages and channel fading. The
expected received SNR and expected FB capacity for extremely high blockage
densities are derived in closed-form as functions of the amount and height of
IRSs and the density, size, and penetration loss of blockages, which are
verified by Monte Carlo simulations. Results show that deploying IRSs
vertically higher leads to higher expected received SNR and expected FB
capacity. By analysing the average/minimum/maximum of the three metrics versus
the number of IRSs, we find that for high blockage densities, both eMBB and
URLLC services benefit from distributed deployment; and for low blockage
densities, URLLC services benefit from distributed deployment while eMBB
services see limited difference between collocated and distributed deployment.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07850" title="Abstract">arXiv:2311.07850</a> [<a href="/pdf/2311.07850" title="Download PDF">pdf</a>, <a href="/format/2311.07850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+D">Dhruv Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+R">Rajarshi Das</a>, 
<a href="/search/cs?searchtype=author&query=Khosla%2C+S">Sopan Khosla</a>, 
<a href="/search/cs?searchtype=author&query=Gangadharaiah%2C+R">Rashmi Gangadharaiah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present BYOKG, a universal question-answering (QA) system that can operate
on any knowledge graph (KG), requires no human-annotated training data, and can
be ready to use within a day -- attributes that are out-of-scope for current
KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to
comprehend information present in an unseen KG through exploration -- starting
at random nodes, inspecting the labels of adjacent nodes and edges, and
combining them with their prior world knowledge. In BYOKG, exploration
leverages an LLM-backed symbolic agent that generates a diverse set of
query-program exemplars, which are then used to ground a retrieval-augmented
reasoning procedure to predict programs for arbitrary questions. BYOKG is
effective over both small- and large-scale graphs, showing dramatic gains in QA
accuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,
respectively. On GrailQA, we further show that our unsupervised BYOKG
outperforms a supervised in-context learning method, demonstrating the
effectiveness of exploration. Lastly, we find that performance of BYOKG
reliably improves with continued exploration as well as improvements in the
base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1
on a sub-sampled zero-shot split of GrailQA.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07853" title="Abstract">arXiv:2311.07853</a> [<a href="/pdf/2311.07853" title="Download PDF">pdf</a>, <a href="/format/2311.07853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Mutually Informed Representations for Characters and Subwords
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yilin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Gormley%2C+M+R">Matthew R. Gormley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Most pretrained language models rely on subword tokenization, which processes
text as a sequence of subword tokens. However, different granularities of text,
such as characters, subwords, and words, can contain different kinds of
information. Previous studies have shown that incorporating multiple input
granularities improves model generalization, yet very few of them outputs
useful representations for each granularity. In this paper, we introduce the
entanglement model, aiming to combine character and subword language models.
Inspired by vision-language models, our model treats characters and subwords as
separate modalities, and it generates mutually informed representations for
both granularities as output. We evaluate our model on text classification,
named entity recognition, and POS-tagging tasks. Notably, the entanglement
model outperforms its backbone language models, particularly in the presence of
noisy texts and low-resource languages. Furthermore, the entanglement model
even outperforms larger pre-trained models on all English sequence labeling
tasks and classification tasks. Our anonymized code is available at
https://anonymous.4open.science/r/noisy-IE-A673
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07861" title="Abstract">arXiv:2311.07861</a> [<a href="/pdf/2311.07861" title="Download PDF">pdf</a>, <a href="/format/2311.07861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of the TREC 2023 Product Product Search Track
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Campos%2C+D">Daniel Campos</a>, 
<a href="/search/cs?searchtype=author&query=Kallumadi%2C+S">Surya Kallumadi</a>, 
<a href="/search/cs?searchtype=author&query=Rosset%2C+C">Corby Rosset</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+C+X">Cheng Xiang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Magnani%2C+A">Alessandro Magnani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 11 tables - TREC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This is the first year of the TREC Product search track. The focus this year
was the creation of a reusable collection and evaluation of the impact of the
use of metadata and multi-modal data on retrieval accuracy. This year we
leverage the new product search corpus, which includes contextual metadata. Our
analysis shows that in the product search domain, traditional retrieval systems
are highly effective and commonly outperform general-purpose pretrained
embedding models. Our analysis also evaluates the impact of using simplified
and metadata-enhanced collections, finding no clear trend in the impact of the
expanded collection. We also see some surprising outcomes; despite their
widespread adoption and competitive performance on other tasks, we find
single-stage dense retrieval runs can commonly be noncompetitive or generate
low-quality results both in the zero-shot and fine-tuned domain.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07864" title="Abstract">arXiv:2311.07864</a> [<a href="/pdf/2311.07864" title="Download PDF">pdf</a>, <a href="/format/2311.07864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing clustering in neural network representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thao Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Kornblith%2C+S">Simon Kornblith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Neural network representations contain structure beyond what was present in
the training labels. For instance, representations of images that are visually
or semantically similar tend to lie closer to each other than to dissimilar
images, regardless of their labels. Clustering these representations can thus
provide insights into dataset properties as well as the network internals. In
this work, we study how the many design choices involved in neural network
training affect the clusters formed in the hidden representations. To do so, we
establish an evaluation setup based on the BREEDS hierarchy, for the task of
subclass clustering after training models with only superclass information. We
isolate the training dataset and architecture as important factors affecting
clusterability. Datasets with labeled classes consisting of unrelated
subclasses yield much better clusterability than those following a natural
hierarchy. When using pretrained models to cluster representations on
downstream datasets, models pretrained on subclass labels provide better
clusterability than models pretrained on superclass labels, but only when there
is a high degree of domain overlap between the pretraining and downstream data.
Architecturally, we find that normalization strategies affect which layers
yield the best clustering performance, and, surprisingly, Vision Transformers
attain lower subclass clusterability than ResNets.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07867" title="Abstract">arXiv:2311.07867</a> [<a href="/pdf/2311.07867" title="Download PDF">pdf</a>, <a href="/format/2311.07867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture of Coupled HMMs for Robust Modeling of Multivariate Healthcare  Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Poyraz%2C+O">Onur Poyraz</a>, 
<a href="/search/cs?searchtype=author&query=Marttinen%2C+P">Pekka Marttinen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures, Proceedings of Machine Learning Research, Machine Learning for Health (ML4H) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Analysis of multivariate healthcare time series data is inherently
challenging: irregular sampling, noisy and missing values, and heterogeneous
patient groups with different dynamics violating exchangeability. In addition,
interpretability and quantification of uncertainty are critically important.
Here, we propose a novel class of models, a mixture of coupled hidden Markov
models (M-CHMM), and demonstrate how it elegantly overcomes these challenges.
To make the model learning feasible, we derive two algorithms to sample the
sequences of the latent variables in the CHMM: samplers based on (i) particle
filtering and (ii) factorized approximation. Compared to existing inference
methods, our algorithms are computationally tractable, improve mixing, and
allow for likelihood estimation, which is necessary to learn the mixture model.
Experiments on challenging real-world epidemiological and semi-synthetic data
demonstrate the advantages of the M-CHMM: improved data fit, capacity to
efficiently handle missing and noisy measurements, improved prediction
accuracy, and ability to identify interpretable subsets in the data.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07868" title="Abstract">arXiv:2311.07868</a> [<a href="/pdf/2311.07868" title="Download PDF">pdf</a>, <a href="/format/2311.07868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Signal Reconstruction Using Masked Autoencoder From EEG During  Polysomnography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kweon%2C+Y">Young-Seok Kweon</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+G">Gi-Hwan Shin</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+H">Heon-Gyu Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+H">Ha-Na Jo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proc. 12th IEEE International Winter Conference on Brain-Computer Interface
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Polysomnography (PSG) is an indispensable diagnostic tool in sleep medicine,
essential for identifying various sleep disorders. By capturing physiological
signals, including EEG, EOG, EMG, and cardiorespiratory metrics, PSG presents a
patient's sleep architecture. However, its dependency on complex equipment and
expertise confines its use to specialized clinical settings. Addressing these
limitations, our study aims to perform PSG by developing a system that requires
only a single EEG measurement. We propose a novel system capable of
reconstructing multi-signal PSG from a single-channel EEG based on a masked
autoencoder. The masked autoencoder was trained and evaluated using the
Sleep-EDF-20 dataset, with mean squared error as the metric for assessing the
similarity between original and reconstructed signals. The model demonstrated
proficiency in reconstructing multi-signal data. Our results present promise
for the development of more accessible and long-term sleep monitoring systems.
This suggests the expansion of PSG's applicability, enabling its use beyond the
confines of clinics.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07870" title="Abstract">arXiv:2311.07870</a> [<a href="/pdf/2311.07870" title="Download PDF">pdf</a>, <a href="/format/2311.07870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoML for Large Capacity Modeling of Meta Ranking Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kuang-Hung Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mengying Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Buyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sehgal%2C+V">Vivek Sehgal</a>, 
<a href="/search/cs?searchtype=author&query=Panchal%2C+R+R">Rudresh Rajnikant Panchal</a>, 
<a href="/search/cs?searchtype=author&query=Hotaj%2C+E">Eugen Hotaj</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Daifeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jamey Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shali Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huayu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen-Yen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiyan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wei Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Hang Yin and Kuang-Hung Liu contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Web-scale ranking systems at Meta serving billions of users is complex.
Improving ranking models is essential but engineering heavy. Automated Machine
Learning (AutoML) can release engineers from labor intensive work of tuning
ranking models; however, it is unknown if AutoML is efficient enough to meet
tight production timeline in real-world and, at the same time, bring additional
improvements to the strong baselines. Moreover, to achieve higher ranking
performance, there is an ever-increasing demand to scale up ranking models to
even larger capacity, which imposes more challenges on the efficiency. The
large scale of models and tight production schedule requires AutoML to
outperform human baselines by only using a small number of model evaluation
trials (around 100). We presents a sampling-based AutoML method, focusing on
neural architecture search and hyperparameter optimization, addressing these
challenges in Meta-scale production when building large capacity models. Our
approach efficiently handles large-scale data demands. It leverages a
lightweight predictor-based searcher and reinforcement learning to explore vast
search spaces, significantly reducing the number of model evaluations. Through
experiments in large capacity modeling for CTR and CVR applications, we show
that our method achieves outstanding Return on Investment (ROI) versus human
tuned baselines, with up to 0.09% Normalized Entropy (NE) loss reduction or
$25\%$ Query per Second (QPS) increase by only sampling one hundred models on
average from a curated search space. The proposed AutoML method has already
made real-world impact where a discovered Instagram CTR model with up to -0.36%
NE gain (over existing production baseline) was selected for large-scale online
A/B test and show statistically significant gain. These production results
proved AutoML efficacy and accelerated its adoption in ranking systems at Meta.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07871" title="Abstract">arXiv:2311.07871</a> [<a href="/pdf/2311.07871" title="Download PDF">pdf</a>, <a href="/format/2311.07871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-channel Prototype Network for few-shot Classification of  Pathological Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quan%2C+H">Hao Quan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinjia Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+D">Dayu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+T">Tianhang Nan</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+X">Xiaoyu Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In pathology, the rarity of certain diseases and the complexity in annotating
pathological images significantly hinder the creation of extensive,
high-quality datasets. This limitation impedes the progress of deep
learning-assisted diagnostic systems in pathology. Consequently, it becomes
imperative to devise a technology that can discern new disease categories from
a minimal number of annotated examples. Such a technology would substantially
advance deep learning models for rare diseases. Addressing this need, we
introduce the Dual-channel Prototype Network (DCPN), rooted in the few-shot
learning paradigm, to tackle the challenge of classifying pathological images
with limited samples. DCPN augments the Pyramid Vision Transformer (PVT)
framework for few-shot classification via self-supervised learning and
integrates it with convolutional neural networks. This combination forms a
dual-channel architecture that extracts multi-scale, highly precise
pathological features. The approach enhances the versatility of prototype
representations and elevates the efficacy of prototype networks in few-shot
pathological image classification tasks. We evaluated DCPN using three publicly
available pathological datasets, configuring small-sample classification tasks
that mirror varying degrees of clinical scenario domain shifts. Our
experimental findings robustly affirm DCPN's superiority in few-shot
pathological image classification, particularly in tasks within the same
domain, where it achieves the benchmarks of supervised learning.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07872" title="Abstract">arXiv:2311.07872</a> [<a href="/pdf/2311.07872" title="Download PDF">pdf</a>, <a href="/ps/2311.07872" title="Download PostScript">ps</a>, <a href="/format/2311.07872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-Efficient Computation Offloading and Service Chain Caching in LEO  Satellite Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yantong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chuanfen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiande Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The ever-increasing demand for ubiquitous, continuous, and high-quality
services poses a great challenge to the traditional terrestrial network. To
mitigate this problem, the mobile-edge-computing-enhanced low earth orbit (LEO)
satellite network, which provides both communication connectivity and on-board
processing services, has emerged as an effective method. The main issue in LEO
satellites includes finding the optimal locations to host network functions
(NFs) and then making offloading decisions. In this article, we jointly
consider the problem of service chain caching and computation offloading to
minimize the overall cost, which consists of task latency and energy
consumption. In particular, the collaboration among satellites, the network
resource limitations, and the specific operation order of NFs in service chains
are taken into account. Then, the problem is formulated and linearized as an
integer linear programming model. Moreover, to accelerate the solution, we
provide a greedy algorithm with cubic time complexity. Numerical investigations
demonstrate the effectiveness of the proposed scheme, which can reduce the
overall cost by around 20% compared to the nominal case where NFs are served in
data centers.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07874" title="Abstract">arXiv:2311.07874</a> [<a href="/pdf/2311.07874" title="Download PDF">pdf</a>, <a href="/format/2311.07874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Transaction as a Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Weixing Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Ge Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">This paper argues for decoupling transaction processing from existing
two-layer cloud-native databases and making transaction processing as an
independent service. By building a transaction as a service (TaaS) layer, the
transaction processing can be independently scaled for high resource
utilization and can be independently upgraded for development agility.
Accordingly, we architect an execution-transaction-storage three-layer
cloud-native database. By connecting to TaaS, 1) the AP engines can be
empowered with ACID TP capability, 2) multiple standalone TP engine instances
can be incorporated to support multi-master distributed TP for horizontal
scalability, 3) multiple execution engines with different data models can be
integrated to support multi-model transactions, and 4) high performance TP is
achieved through extensive TaaS optimizations and consistent evolution.
Cloud-native databases deserve better architecture: we believe that TaaS
provides a path forward to better cloud-native databases.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07876" title="Abstract">arXiv:2311.07876</a> [<a href="/pdf/2311.07876" title="Download PDF">pdf</a>, <a href="/ps/2311.07876" title="Download PostScript">ps</a>, <a href="/format/2311.07876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Adversarial Low-rank Markov Decision Processes with Unknown  Transition and Full-information Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Canzhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruofeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuezhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this work, we study the low-rank MDPs with adversarially changed losses in
the full-information feedback setting. In particular, the unknown transition
probability kernel admits a low-rank matrix decomposition \citep{REPUCB22}, and
the loss functions may change adversarially but are revealed to the learner at
the end of each episode. We propose a policy optimization-based algorithm POLO,
and we prove that it attains the
$\widetilde{O}(K^{\frac{5}{6}}A^{\frac{1}{2}}d\ln(1+M)/(1-\gamma)^2)$ regret
guarantee, where $d$ is rank of the transition kernel (and hence the dimension
of the unknown representations), $A$ is the cardinality of the action space,
$M$ is the cardinality of the model class, and $\gamma$ is the discounted
factor. Notably, our algorithm is oracle-efficient and has a regret guarantee
with no dependence on the size of potentially arbitrarily large state space.
Furthermore, we also prove an $\Omega(\frac{\gamma^2}{1-\gamma} \sqrt{d A K})$
regret lower bound for this problem, showing that low-rank MDPs are
statistically more difficult to learn than linear MDPs in the regret
minimization setting. To the best of our knowledge, we present the first
algorithm that interleaves representation learning, exploration, and
exploitation to achieve the sublinear regret guarantee for RL with nonlinear
function approximation and adversarial losses.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07877" title="Abstract">arXiv:2311.07877</a> [<a href="/pdf/2311.07877" title="Download PDF">pdf</a>, <a href="/format/2311.07877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Training for Semantic Segmentation with Output Contrastive  Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunlong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuxuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sunyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shui%2C+Z">Zhongyi Shui</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenglu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lin Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Although deep learning-based segmentation models have achieved impressive
performance on public benchmarks, generalizing well to unseen environments
remains a major challenge. To improve the model's generalization ability to the
new domain during evaluation, the test-time training (TTT) is a challenging
paradigm that adapts the source-pretrained model in an online fashion. Early
efforts on TTT mainly focus on the image classification task. Directly
extending these methods to semantic segmentation easily experiences unstable
adaption due to segmentation's inherent characteristics, such as extreme class
imbalance and complex decision spaces. To stabilize the adaptation process, we
introduce contrastive loss (CL), known for its capability to learn robust and
generalized representations. Nevertheless, the traditional CL operates in the
representation space and cannot directly enhance predictions. In this paper, we
resolve this limitation by adapting the CL to the output space, employing a
high temperature, and simplifying the formulation, resulting in a
straightforward yet effective loss function called Output Contrastive Loss
(OCL). Our comprehensive experiments validate the efficacy of our approach
across diverse evaluation scenarios. Notably, our method excels even when
applied to models initially pre-trained using domain adaptation methods on test
domain data, showcasing its resilience and adaptability.\footnote{Code and more
information could be found at~ \url{https://github.com/dazhangyu123/OCL}}
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07878" title="Abstract">arXiv:2311.07878</a> [<a href="/pdf/2311.07878" title="Download PDF">pdf</a>, <a href="/format/2311.07878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating LLMs on Document-Based QA: Exact Answer Selection and  Numerical Extraction using Cogtale datase
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasool%2C+Z">Zafaryab Rasool</a>, 
<a href="/search/cs?searchtype=author&query=Barnett%2C+S">Scott Barnett</a>, 
<a href="/search/cs?searchtype=author&query=Kurniawan%2C+S">Stefanus Kurniawan</a>, 
<a href="/search/cs?searchtype=author&query=Balugo%2C+S">Sherwin Balugo</a>, 
<a href="/search/cs?searchtype=author&query=Vasa%2C+R">Rajesh Vasa</a>, 
<a href="/search/cs?searchtype=author&query=Chesser%2C+C">Courtney Chesser</a>, 
<a href="/search/cs?searchtype=author&query=Bahar-Fuchs%2C+A">Alex Bahar-Fuchs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Document-based Question-Answering (QA) tasks are crucial for precise
information retrieval. While some existing work focus on evaluating large
language model's performance on retrieving and answering questions from
documents, assessing the LLMs' performance on QA types that require exact
answer selection from predefined options and numerical extraction is yet to be
fully assessed. In this paper, we specifically focus on this underexplored
context and conduct empirical analysis of LLMs (GPT-4 and GPT 3.5) on question
types, including single-choice, yes-no, multiple-choice, and number extraction
questions from documents. We use the Cogtale dataset for evaluation, which
provide human expert-tagged responses, offering a robust benchmark for
precision and factual grounding. We found that LLMs, particularly GPT-4, can
precisely answer many single-choice and yes-no questions given relevant
context, demonstrating their efficacy in information retrieval tasks. However,
their performance diminishes when confronted with multiple-choice and number
extraction formats, lowering the overall performance of the model on this task,
indicating that these models may not be reliable for the task. This limits the
applications of LLMs on applications demanding precise information extraction
from documents, such as meta-analysis tasks. However, these findings hinge on
the assumption that the retrievers furnish pertinent context necessary for
accurate responses, emphasizing the need for further research on the efficacy
of retriever mechanisms in enhancing question-answering performance. Our work
offers a framework for ongoing dataset evaluation, ensuring that LLM
applications for information retrieval and document analysis continue to meet
evolving standards.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07879" title="Abstract">arXiv:2311.07879</a> [<a href="/pdf/2311.07879" title="Download PDF">pdf</a>, <a href="/format/2311.07879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting  Volunteer Content Moderators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y+T">Yang Trista Cao</a>, 
<a href="/search/cs?searchtype=author&query=Domingo%2C+L">Lovely-Frances Domingo</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+S+A">Sarah Ann Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Mazurek%2C+M">Michelle Mazurek</a>, 
<a href="/search/cs?searchtype=author&query=Shilton%2C+K">Katie Shilton</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Extensive efforts in automated approaches for content moderation have been
focused on developing models to identify toxic, offensive, and hateful content
-- with the aim of lightening the load for moderators. Yet, it remains
uncertain whether improvements on those tasks truly address the needs that
moderators have in accomplishing their work. In this paper, we surface the gaps
between past research efforts that have aimed to provide automation for aspects
of the content moderation task, and the needs of volunteer content moderators.
To do so, we conduct a model review on Hugging Face to reveal the availability
of models to cover various moderation rules and guidelines. We further put
state-of-the-art LLMs to the test (GPT-4 and Llama-2), evaluating how well
these models perform in flagging violations of platform rules. Overall, we
observe a non-trivial gap, as missing developed models and LLMs exhibit low
recall on a significant portion of the rules.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07880" title="Abstract">arXiv:2311.07880</a> [<a href="/pdf/2311.07880" title="Download PDF">pdf</a>, <a href="/format/2311.07880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VegaEdge: Edge AI Confluence Anomaly Detection for Real-Time Highway  IoT-Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katariya%2C+V">Vinit Katariya</a>, 
<a href="/search/cs?searchtype=author&query=Jannat%2C+F">Fatema-E- Jannat</a>, 
<a href="/search/cs?searchtype=author&query=Pazho%2C+A+D">Armin Danesh Pazho</a>, 
<a href="/search/cs?searchtype=author&query=Noghre%2C+G+A">Ghazal Alinezhad Noghre</a>, 
<a href="/search/cs?searchtype=author&query=Tabkhi%2C+H">Hamed Tabkhi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Vehicle anomaly detection plays a vital role in highway safety applications
such as accident prevention, rapid response, traffic flow optimization, and
work zone safety. With the surge of the Internet of Things (IoT) in recent
years, there has arisen a pressing demand for Artificial Intelligence (AI)
based anomaly detection methods designed to meet the requirements of IoT
devices. Catering to this futuristic vision, we introduce a lightweight
approach to vehicle anomaly detection by utilizing the power of trajectory
prediction. Our proposed design identifies vehicles deviating from expected
paths, indicating highway risks from different camera-viewing angles from
real-world highway datasets. On top of that, we present VegaEdge - a
sophisticated AI confluence designed for real-time security and surveillance
applications in modern highway settings through edge-centric IoT-embedded
platforms equipped with our anomaly detection approach. Extensive testing
across multiple platforms and traffic scenarios showcases the versatility and
effectiveness of VegaEdge. This work also presents the Carolinas Anomaly
Dataset (CAD), to bridge the existing gap in datasets tailored for highway
anomalies. In real-world scenarios, our anomaly detection approach achieves an
AUC-ROC of 0.94, and our proposed VegaEdge design, on an embedded IoT platform,
processes 738 trajectories per second in a typical highway setting. The dataset
is available at
https://github.com/TeCSAR-UNCC/Carolinas_Dataset#chd-anomaly-test-set .
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07883" title="Abstract">arXiv:2311.07883</a> [<a href="/pdf/2311.07883" title="Download PDF">pdf</a>, <a href="/format/2311.07883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of a tensor POD-ROM for parameter dependent parabolic problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mamonov%2C+A+V">Alexander V. Mamonov</a>, 
<a href="/search/math?searchtype=author&query=Olshanskii%2C+M+A">Maxim A. Olshanskii</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">A space-time-parameters structure of the parametric parabolic PDEs motivates
the application of tensor methods to define reduced order models (ROMs). Within
a tensor-based ROM framework, the matrix SVD -- a traditional dimension
reduction technique -- yields to a low-rank tensor decomposition (LRTD). Such
tensor extension of the Galerkin proper orthogonal decomposition ROMs
(POD-ROMs) benefits both the practical efficiency of the ROM and its
amenability for the rigorous error analysis when applied to parametric PDEs.
The paper addresses the error analysis of the Galerkin LRTD-ROM for an abstract
linear parabolic problem that depends on multiple physical parameters. An error
estimate for the LRTD-ROM solution is proved, which is uniform with respect to
problem parameters and extends to parameter values not in a sampling/training
set. The estimate is given in terms of discretization and sampling mesh
properties, and LRTD accuracy. The estimate depends on the smoothness rather
than on the Kolmogorov n-widths of the parameterized manifold of solutions.
Theoretical results are illustrated with several numerical experiments.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07884" title="Abstract">arXiv:2311.07884</a> [<a href="/pdf/2311.07884" title="Download PDF">pdf</a>, <a href="/format/2311.07884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Abstractive Summarization of Diverse Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yusen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Nan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fabbri%2C+A">Alexander Fabbri</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junru Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kamoi%2C+R">Ryo Kamoi</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xiaoxin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jieyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+D">Dragomir Radev</a>, 
<a href="/search/cs?searchtype=author&query=McKeown%2C+K">Kathleen McKeown</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">People from different social and demographic groups express diverse
perspectives and conflicting opinions on a broad set of topics such as product
reviews, healthcare, law, and politics. A fair summary should provide a
comprehensive coverage of diverse perspectives without underrepresenting
certain groups. However, current work in summarization metrics and Large
Language Models (LLMs) evaluation has not explored fair abstractive
summarization. In this paper, we systematically investigate fair abstractive
summarization for user-generated data. We first formally define fairness in
abstractive summarization as not underrepresenting perspectives of any groups
of people and propose four reference-free automatic metrics measuring the
differences between target and source perspectives. We evaluate five LLMs,
including three GPT models, Alpaca, and Claude, on six datasets collected from
social media, online reviews, and recorded transcripts. Experiments show that
both the model-generated and the human-written reference summaries suffer from
low fairness. We conduct a comprehensive analysis of the common factors
influencing fairness and propose three simple but effective methods to
alleviate unfair summarization. Our dataset and code are available at
https://github.com/psunlpgroup/FairSumm.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07885" title="Abstract">arXiv:2311.07885</a> [<a href="/pdf/2311.07885" title="Download PDF">pdf</a>, <a href="/format/2311.07885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View  Generation and 3D Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Minghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+R">Ruoxi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Linghao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xinyue Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hansheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+C">Chong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiayuan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
<p class="mathjax">Recent advancements in open-world 3D object generation have been remarkable,
with image-to-3D methods offering superior fine-grained control over their
text-to-3D counterparts. However, most existing models fall short in
simultaneously providing rapid generation speeds and high fidelity to input
images - two features essential for practical applications. In this paper, we
present One-2-3-45++, an innovative method that transforms a single image into
a detailed 3D textured mesh in approximately one minute. Our approach aims to
fully harness the extensive knowledge embedded in 2D diffusion models and
priors from valuable yet limited 3D data. This is achieved by initially
finetuning a 2D diffusion model for consistent multi-view image generation,
followed by elevating these images to 3D with the aid of multi-view conditioned
3D native diffusion models. Extensive experimental evaluations demonstrate that
our method can produce high-quality, diverse 3D assets that closely mirror the
original input image. Our project webpage:
https://sudo-ai-3d.github.io/One2345plus_page.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07887" title="Abstract">arXiv:2311.07887</a> [<a href="/pdf/2311.07887" title="Download PDF">pdf</a>, <a href="/ps/2311.07887" title="Download PostScript">ps</a>, <a href="/format/2311.07887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenges of Securing Massively Multiplayer Online Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinclair%2C+K">Kolten Sinclair</a>, 
<a href="/search/cs?searchtype=author&query=Womack%2C+S">Steven Womack</a>, 
<a href="/search/cs?searchtype=author&query=Elliott%2C+J">Jacob Elliott</a>, 
<a href="/search/cs?searchtype=author&query=Stafford%2C+B">Benjamin Stafford</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+S">Sundar Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">When it comes to security in the modern world, things have improved a lot
since the early 2000s. Hypertext Transfer Protocol Secure (HTTPS) and Transport
Layer Security (TLS) have made the transfer of our data across the internet
much safer than years prior, and the advent of VPNs and private browsing have
only compounded that. However, the gaming industry has been notoriously behind
the curve when it comes to security, most notably with Massively Multiplayer
Online (MMO) games, which due to the intrinsic nature of their architecture,
have an astounding amount of ground to cover. In this paper, the authors
discuss the challenges that MMO developers face when trying to design a secure
game, as well as some more modern approaches to security that will help improve
the industry moving forward. The authors also highlight a few real-life
examples of exploits and breaches that have happened and look at how they were
mitigated.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07888" title="Abstract">arXiv:2311.07888</a> [<a href="/pdf/2311.07888" title="Download PDF">pdf</a>, <a href="/format/2311.07888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboSense At Edge: Detecting Slip, Crumple and Shape of the Object in  Robotic Hand for Teleoprations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padhi%2C+S+K">Sudev Kumar Padhi</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+M">Mohit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Giri%2C+D">Debanka Giri</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Subidh Ali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Slip and crumple detection is essential for performing robust manipulation
tasks with a robotic hand (RH) like remote surgery. It has been one of the
challenging problems in the robotics manipulation community. In this work, we
propose a technique based on machine learning (ML) based techniques to detect
the slip, and crumple as well as the shape of an object that is currently held
in the robotic hand. We proposed ML model will detect the slip, crumple, and
shape using the force/torque exerted and the angular positions of the actuators
present in the RH. The proposed model would be integrated into the loop of a
robotic hand(RH) and haptic glove(HG). This would help us to reduce the latency
in case of teleoperation
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07889" title="Abstract">arXiv:2311.07889</a> [<a href="/pdf/2311.07889" title="Download PDF">pdf</a>, <a href="/format/2311.07889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal RIP Matrices with Slightly Less Randomness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+S">Shravas Rao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">A matrix $\Phi \in \mathbb{R}^{Q \times N}$ satisfies the restricted isometry
property if $\|\Phi x\|_2^2$ is approximately equal to $\|x\|_2^2$ for all
$k$-sparse vectors $x$. We give a construction of RIP matrices with the optimal
$Q = O(k \log(N/k))$ rows using $O(k\log(N/k)\log(k))$ bits of randomness. The
main technical ingredient is an extension of the Hanson-Wright inequality to
$\epsilon$-biased distributions.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07891" title="Abstract">arXiv:2311.07891</a> [<a href="/pdf/2311.07891" title="Download PDF">pdf</a>, <a href="/ps/2311.07891" title="Download PostScript">ps</a>, <a href="/format/2311.07891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative planning and optimization for  electric-thermal-hydrogen-coupled energy systems with portfolio selection of  the complete hydrogen energy chain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yi%2C+X">Xinning Yi</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+T">Tianguang Lu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yixiao Li</a>, 
<a href="/search/eess?searchtype=author&query=Ai%2C+Q">Qian Ai</a>, 
<a href="/search/eess?searchtype=author&query=Hao%2C+R">Ran Hao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Under the global low-carbon target, the uneven spatiotemporal distribution of
renewable energy resources exacerbates the uncertainty and seasonal power
imbalance. Additionally, the issue of an incomplete hydrogen energy chain is
widely overlooked in planning models, which hinders the complete analysis of
the role of hydrogen in energy systems. Therefore, this paper proposes a
high-resolution collaborative planning model for
electricity-thermal-hydrogen-coupled energy systems considering both the
spatiotemporal distribution characteristics of renewable energy resources and
the multi-scale bottom-to-top investment strategy for the complete hydrogen
energy chain. Considering the high-resolution system operation flexibility,
this paper proposes a hydrogen chain-based fast clustering optimization method
that can handle high-dimensional data and multi-time scale operation
characteristics. The model optimizes the geographical distribution and capacity
configuration of the Northeast China energy system in 2050, with hourly
operational characteristics. The planning optimization covered single-energy
devices, multi-energy-coupled conversion devices, and electric-hydrogen
transmission networks. Last but not least, this paper thoroughly examines the
optimal portfolio selection of different hydrogen technologies based on the
differences in cost, flexibility, and efficiency. In the Pareto analysis, the
proposed model reduces CO2 emissions by 60% with a competitive cost. This paper
provides a zero-carbon pathway for multi-energy systems with a cost 4% less
than the social cost of carbon $44.6/ton, and the integration of the complete
hydrogen energy chain reduces the renewable energy curtailment by 97.0%.
Besides, the portfolio selection results indicate that the system favors the
SOEC with the highest energy efficiency and the PEMFC with the fastest dynamic
response when achieving zero-carbon emissions
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07894" title="Abstract">arXiv:2311.07894</a> [<a href="/pdf/2311.07894" title="Download PDF">pdf</a>, <a href="/ps/2311.07894" title="Download PostScript">ps</a>, <a href="/format/2311.07894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security in Drones
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morgan%2C+J">Jonathan Morgan</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+J">Julio Perez</a>, 
<a href="/search/cs?searchtype=author&query=Wade%2C+J">Jordan Wade</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+S">Sundar Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Drones are used in our everyday world for private, commercial, and government
uses. It is important to establish both the cyber threats drone users face and
security practices to combat those threats. Privacy will always be the main
concern when using drones. Protecting information legally collected on drones
and protecting people from the illegal collection of their data are topics that
security professionals should consider before their organization uses drones.
In this article, the authors discuss the importance of security in drones.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07897" title="Abstract">arXiv:2311.07897</a> [<a href="/pdf/2311.07897" title="Download PDF">pdf</a>, <a href="/format/2311.07897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPopQA: Ranking Cultural Concept Popularity by LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Ming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+M">Mansi Joshi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Prior work has demonstrated large language models' (LLMs) potential to
discern statistical tendencies within their pre-training corpora. Despite that,
many examinations of LLMs' knowledge capacity focus on knowledge explicitly
appearing in the training data or implicitly inferable from similar contexts.
How well an LLM captures the corpus-level statistical trends of concepts for
reasoning, especially long-tail ones, is still underexplored. In this study, we
introduce a novel few-shot question-answering task (CPopQA) that examines LLMs'
statistical ranking abilities for long-tail cultural concepts (e.g., holidays),
with a specific focus on these concepts' popularity in the United States and
the United Kingdom, respectively. We curate a dataset containing 459 holidays
across 58 countries, generating a total of 6,000 QA testing pairs. Experiments
on four strong LLMs show that large models are capable of ranking long-tail
cultural concepts regarding their statistical tendency. Notably, GPT-3.5
displayed superior performance and exhibited its potential to identify
geo-cultural proximity across continents.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07907" title="Abstract">arXiv:2311.07907</a> [<a href="/pdf/2311.07907" title="Download PDF">pdf</a>, <a href="/ps/2311.07907" title="Download PostScript">ps</a>, <a href="/format/2311.07907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curve Stabbing Depth: Data Depth for Plane Curves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Durocher%2C+S">Stephane Durocher</a>, 
<a href="/search/cs?searchtype=author&query=Leblanc%2C+A">Alexandre Leblanc</a>, 
<a href="/search/cs?searchtype=author&query=Szabados%2C+S">Spencer Szabados</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Measures of data depth have been studied extensively for point data.
Motivated by recent work on analysis, clustering, and identifying
representative elements in sets of trajectories, we introduce {\em curve
stabbing depth} to quantify how deeply a given curve $Q$ is located relative to
a given set $\cal C$ of curves in $\mathbb{R}^2$. Curve stabbing depth
evaluates the average number of elements of $\cal C$ stabbed by rays rooted
along the length of $Q$. We describe an $O(n^3 + n^2 m\log^2m+nm^2\log^2
m)$-time algorithm for computing curve stabbing depth when $Q$ is an $m$-vertex
polyline and $\cal C$ is a set of $n$ polylines, each with $O(m)$ vertices.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07909" title="Abstract">arXiv:2311.07909</a> [<a href="/pdf/2311.07909" title="Download PDF">pdf</a>, <a href="/ps/2311.07909" title="Download PostScript">ps</a>, <a href="/format/2311.07909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Intraoperative Force Perception and Signal Decoupling Method on  Capsulorhexis Forceps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Heng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Force perception on medical instruments is critical for understanding the
mechanism between surgical tools and tissues for feeding back quantized force
information, which is essential for guidance and supervision in robotic
autonomous surgery. Especially for continuous curvilinear capsulorhexis (CCC),
it always lacks a force measuring method, providing a sensitive, accurate, and
multi-dimensional measurement to track the intraoperative force. Furthermore,
the decoupling matrix obtained from the calibration can decorrelate signals
with acceptable accuracy, however, this calculating method is not a strong way
for thoroughly decoupling under some sensitive measuring situations such as the
CCC. In this paper, a three-dimensional force perception method on
capsulorhexis forceps by installing Fiber Bragg Grating sensors (FBGs) on
prongs and a signal decoupling method combined with FASTICA is first proposed
to solve these problems. According to experimental results, the measuring range
is up to 1 N (depending on the range of wavelength shifts of sensors) and the
resolution on x, y, and z axial force is 0.5, 0.5, and 2 mN separately. To
minimize the coupling effects among sensors on measuring multi-axial forces, by
unitizing the particular parameter and scaling the corresponding vector in the
mixing matrix and recovered signals from FastICA, the signals from sensors can
be decorrelated and recovered with the errors on axial forces decreasing up to
50% least. The calibration and calculation can also be simplified with half the
parameters involved in the calculation. Experiments on thin sheets and in vitro
porcine eyes were performed, and it was found that the tearing forces were
stable and the time sequence of tearing forceps was stationary or first-order
difference stationary during roughly circular crack propagating.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07911" title="Abstract">arXiv:2311.07911</a> [<a href="/pdf/2311.07911" title="Download PDF">pdf</a>, <a href="/format/2311.07911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction-Following Evaluation for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jeffrey Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tianjian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Brahma%2C+S">Siddhartha Brahma</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Sujoy Basu</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+Y">Yi Luan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Le Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">One core capability of Large Language Models (LLMs) is to follow natural
language instructions. However, the evaluation of such abilities is not
standardized: Human evaluations are expensive, slow, and not objectively
reproducible, while LLM-based auto-evaluation is potentially biased or limited
by the ability of the evaluator LLM. To overcome these issues, we introduce
Instruction-Following Eval (IFEval) for large language models. IFEval is a
straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set
of "verifiable instructions" such as "write in more than 400 words" and
"mention the keyword of AI at least 3 times". We identified 25 types of those
verifiable instructions and constructed around 500 prompts, with each prompt
containing one or more verifiable instructions. We show evaluation results of
two widely available LLMs on the market. Our code and data can be found at
https://github.com/google-research/google-research/tree/master/instruction_following_eval
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07912" title="Abstract">arXiv:2311.07912</a> [<a href="/pdf/2311.07912" title="Download PDF">pdf</a>, <a href="/format/2311.07912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection of Small Targets in Sea Clutter Based on RepVGG and Continuous  Wavelet Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jingchen Ni</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoru Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lilin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jing Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Constructing a high-performance target detector under the background of sea
clutter is always necessary and important. In this work, we propose a
RepVGGA0-CWT detector, where RepVGG is a residual network that gains a high
detection accuracy. Different from traditional residual networks, RepVGG keeps
an acceptable calculation speed. Giving consideration to both accuracy and
speed, the RepVGGA0 is selected among all the variants of RepVGG. Also,
continuous wavelet transform (CWT) is employed to extract the radar echoes'
time-frequency feature effectively. In the tests, other networks (ResNet50,
ResNet18 and AlexNet) and feature extraction methods (short-time Fourier
transform (STFT), CWT) are combined to build detectors for comparison. The
result of different datasets shows that the RepVGGA0-CWT detector performs
better than those detectors in terms of low controllable false alarm rate, high
training speed, high inference speed and low memory usage. This RepVGGA0-CWT
detector is hardware-friendly and can be applied in real-time scenes for its
high inference speed in detection.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07913" title="Abstract">arXiv:2311.07913</a> [<a href="/pdf/2311.07913" title="Download PDF">pdf</a>, <a href="/format/2311.07913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Roadside LiDAR Assisted Cooperative Localization for Connected  Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuze Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Javanmard%2C+E">Ehsan Javanmard</a>, 
<a href="/search/cs?searchtype=author&query=Nakazato%2C+J">Jin Nakazato</a>, 
<a href="/search/cs?searchtype=author&query=Tsukada%2C+M">Manabu Tsukada</a>, 
<a href="/search/cs?searchtype=author&query=Esaki%2C+H">Hiroshi Esaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 2023 International Conference on Intelligent Computing and its Emerging Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Advancements in LiDAR technology have led to more cost-effective production
while simultaneously improving precision and resolution. As a result, LiDAR has
become integral to vehicle localization, achieving centimeter-level accuracy
through techniques like Normal Distributions Transform (NDT) and other advanced
3D registration algorithms. Nonetheless, these approaches are reliant on
high-definition 3D point cloud maps, the creation of which involves significant
expenditure. When such maps are unavailable or lack sufficient features for 3D
registration algorithms, localization accuracy diminishes, posing a risk to
road safety. To address this, we proposed to use LiDAR-equipped roadside unit
and Vehicle-to-Infrastructure (V2I) communication to accurately estimate the
connected autonomous vehicle's position and help the vehicle when its
self-localization is not accurate enough. Our simulation results indicate that
this method outperforms traditional NDT scan matching-based approaches in terms
of localization accuracy.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07914" title="Abstract">arXiv:2311.07914</a> [<a href="/pdf/2311.07914" title="Download PDF">pdf</a>, <a href="/format/2311.07914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+G">Garima Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Kumarage%2C+T">Tharindu Kumarage</a>, 
<a href="/search/cs?searchtype=author&query=Alghami%2C+Z">Zeyad Alghami</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The contemporary LLMs are prone to producing hallucinations, stemming mainly
from the knowledge gaps within the models. To address this critical limitation,
researchers employ diverse strategies to augment the LLMs by incorporating
external knowledge, aiming to reduce hallucinations and enhance reasoning
accuracy. Among these strategies, leveraging knowledge graphs as a source of
external information has demonstrated promising results. In this survey, we
conduct a comprehensive review of these knowledge-graph-based knowledge
augmentation techniques in LLMs, focusing on their efficacy in mitigating
hallucinations. We systematically categorize these methods into three
overarching groups, offering both methodological comparisons and empirical
evaluations of their performance. Lastly, the paper explores the challenges
associated with these techniques and outlines potential avenues for future
research in this emerging field.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07918" title="Abstract">arXiv:2311.07918</a> [<a href="/pdf/2311.07918" title="Download PDF">pdf</a>, <a href="/format/2311.07918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated title and abstract screening for scoping reviews using the  GPT-4 Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilkins%2C+D">David Wilkins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Scoping reviews, a type of literature review, require intensive human effort
to screen large numbers of scholarly sources for their relevance to the review
objectives. This manuscript introduces GPTscreenR, a package for the R
statistical programming language that uses the GPT-4 Large Language Model (LLM)
to automatically screen sources. The package makes use of the chain-of-thought
technique with the goal of maximising performance on complex screening tasks.
In validation against consensus human reviewer decisions, GPTscreenR performed
similarly to an alternative zero-shot technique, with a sensitivity of 71%,
specificity of 89%, and overall accuracy of 84%. Neither method achieved
perfect accuracy nor human levels of intraobserver agreement. GPTscreenR
demonstrates the potential for LLMs to support scholarly work and provides a
user-friendly software framework that can be integrated into existing review
processes.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07923" title="Abstract">arXiv:2311.07923</a> [<a href="/pdf/2311.07923" title="Download PDF">pdf</a>, <a href="/format/2311.07923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> bpftime: userspace eBPF Runtime for Uprobe, Syscall and Kernel-User  Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yusheng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yanpeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+X">XiaoZheng Lai</a>, 
<a href="/search/cs?searchtype=author&query=Quinn%2C+A">Andrew Quinn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
<p class="mathjax">In kernel-centric operations, the uprobe component of eBPF frequently
encounters performance bottlenecks, largely attributed to the overheads borne
by context switches. Transitioning eBPF operations to user space bypasses these
hindrances, thereby optimizing performance. This also enhances configurability
and obviates the necessity for root access or privileges for kernel eBPF,
subsequently minimizing the kernel attack surface. This paper introduces
bpftime, a novel user-space eBPF runtime, which leverages binary rewriting to
implement uprobe and syscall hook capabilities. Through bpftime, userspace
uprobes achieve a 10x speed enhancement compared to their kernel counterparts
without requiring dual context switches. Additionally, this runtime facilitates
the programmatic hooking of syscalls within a process, both safely and
efficiently. Bpftime can be seamlessly attached to any running process,
limiting the need for either a restart or manual recompilation. Our
implementation also extends to interprocess eBPF Maps within shared memory,
catering to summary aggregation or control plane communication requirements.
Compatibility with existing eBPF toolchains such as clang and libbpf is
maintained, not only simplifying the development of user-space eBPF without
necessitating any modifications but also supporting CO-RE through BTF. Through
bpftime, we not only enhance uprobe performance but also extend the versatility
and user-friendliness of eBPF runtime in user space, paving the way for more
efficient and secure kernel operations.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07925" title="Abstract">arXiv:2311.07925</a> [<a href="/pdf/2311.07925" title="Download PDF">pdf</a>, <a href="/format/2311.07925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brain-Driven Representation Learning Based on Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Soowon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seo-Hyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Young-Eun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Ji-Won Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Ji-Ha Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Interpreting EEG signals linked to spoken language presents a complex
challenge, given the data's intricate temporal and spatial attributes, as well
as the various noise factors. Denoising diffusion probabilistic models (DDPMs),
which have recently gained prominence in diverse areas for their capabilities
in representation learning, are explored in our research as a means to address
this issue. Using DDPMs in conjunction with a conditional autoencoder, our new
approach considerably outperforms traditional machine learning algorithms and
established baseline models in accuracy. Our results highlight the potential of
DDPMs as a sophisticated computational method for the analysis of
speech-related EEG signals. This could lead to significant advances in
brain-computer interfaces tailored for spoken communication.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07926" title="Abstract">arXiv:2311.07926</a> [<a href="/pdf/2311.07926" title="Download PDF">pdf</a>, <a href="/format/2311.07926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VizPut: Insight-Aware Imputation of Incomplete Data for Visualization  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mafrur%2C+R">Rischan Mafrur</a>, 
<a href="/search/cs?searchtype=author&query=Sharaf%2C+M+A">Mohamed A Sharaf</a>, 
<a href="/search/cs?searchtype=author&query=Zuccon%2C+G">Guido Zuccon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is part of my thesis chapter <a href="https://espace.library.uq.edu.au/view/UQ:812c680">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">In insight recommendation systems, obtaining timely and high-quality
recommended visual analytics over incomplete data is challenging due to the
difficulties in cleaning and processing such data. Failing to address data
incompleteness results in diminished recommendation quality, compelling users
to impute the incomplete data to a cleaned version through a costly imputation
strategy. This paper introduces VizPut scheme, an insight-aware selective
imputation technique capable of determining which missing values should be
imputed in incomplete data to optimize the effectiveness of recommended
visualizations within a specified imputation budget. The VizPut scheme
determines the optimal allocation of imputation operations with the objective
of achieving maximal effectiveness in recommended visual analytics. We evaluate
this approach using real-world datasets, and our experimental results
demonstrate that VizPut effectively maximizes the efficacy of recommended
visualizations within the user-defined imputation budget.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07928" title="Abstract">arXiv:2311.07928</a> [<a href="/pdf/2311.07928" title="Download PDF">pdf</a>, <a href="/format/2311.07928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Improving Robustness Against Common Corruptions in Object  Detectors Using Adversarial Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kotyan%2C+S">Shashank Kotyan</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+D+V">Danilo Vasconcellos Vargas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural networks have revolutionized various domains, exhibiting remarkable
accuracy in tasks like natural language processing and computer vision.
However, their vulnerability to slight alterations in input samples poses
challenges, particularly in safety-critical applications like autonomous
driving. Current approaches, such as introducing distortions during training,
fall short in addressing unforeseen corruptions. This paper proposes an
innovative adversarial contrastive learning framework to enhance neural network
robustness simultaneously against adversarial attacks and common corruptions.
By generating instance-wise adversarial examples and optimizing contrastive
loss, our method fosters representations that resist adversarial perturbations
and remain robust in real-world scenarios. Subsequent contrastive learning then
strengthens the similarity between clean samples and their adversarial
counterparts, fostering representations resistant to both adversarial attacks
and common distortions. By focusing on improving performance under adversarial
and real-world conditions, our approach aims to bolster the robustness of
neural networks in safety-critical applications, such as autonomous vehicles
navigating unpredictable weather conditions. We anticipate that this framework
will contribute to advancing the reliability of neural networks in challenging
environments, facilitating their widespread adoption in mission-critical
scenarios.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07929" title="Abstract">arXiv:2311.07929</a> [<a href="/pdf/2311.07929" title="Download PDF">pdf</a>, <a href="/format/2311.07929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Heterogeneous Graph Variational Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yige Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jianxiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chengcheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Heterogeneous Information Networks (HINs), which consist of various types of
nodes and edges, have recently demonstrated excellent performance in graph
mining. However, most existing heterogeneous graph neural networks (HGNNs)
ignore the problems of missing attributes, inaccurate attributes and scarce
labels for nodes, which limits their expressiveness. In this paper, we propose
a generative self-supervised model SHAVA to address these issues
simultaneously. Specifically, SHAVA first initializes all the nodes in the
graph with a low-dimensional representation matrix. After that, based on the
variational graph autoencoder framework, SHAVA learns both node-level and
attribute-level embeddings in the encoder, which can provide fine-grained
semantic information to construct node attributes. In the decoder, SHAVA
reconstructs both links and attributes. Instead of directly reconstructing raw
features for attributed nodes, SHAVA generates the initial low-dimensional
representation matrix for all the nodes, based on which raw features of
attributed nodes are further reconstructed to leverage accurate attributes. In
this way, SHAVA can not only complete informative features for non-attributed
nodes, but rectify inaccurate ones for attributed nodes. Finally, we conduct
extensive experiments to show the superiority of SHAVA in tackling HINs with
missing and inaccurate attributes.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07930" title="Abstract">arXiv:2311.07930</a> [<a href="/pdf/2311.07930" title="Download PDF">pdf</a>, <a href="/format/2311.07930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> It&#x27;s All Relative! -- A Synthetic Query Generation Approach for  Improving Zero-Shot Relevance Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+A">Aditi Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+K">Karthik Raman</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent developments in large language models (LLMs) have shown promise in
their ability to generate synthetic query-document pairs by prompting with as
few as 8 demonstrations. This has enabled building better IR models, especially
for tasks with no training data readily available. Typically, such synthetic
query generation (QGen) approaches condition on an input context (e.g. a text
document) and generate a query relevant to that context, or condition the QGen
model additionally on the relevance label (e.g. relevant vs irrelevant) to
generate queries across relevance buckets. However, we find that such QGen
approaches are sub-optimal as they require the model to reason about the
desired label and the input from a handful of examples. In this work, we
propose to reduce this burden of LLMs by generating queries simultaneously for
different labels. We hypothesize that instead of asking the model to generate,
say, an irrelevant query given an input context, asking the model to generate
an irrelevant query relative to a relevant query is a much simpler task setup
for the model to reason about. Extensive experimentation across seven IR
datasets shows that synthetic queries generated in such a fashion translates to
a better downstream performance, suggesting that the generated queries are
indeed of higher quality.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07932" title="Abstract">arXiv:2311.07932</a> [<a href="/pdf/2311.07932" title="Download PDF">pdf</a>, <a href="/format/2311.07932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-subject dual-domain fusion network with task-related and  task-discriminant component analysis enhancing one-shot SSVEP classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zhiwei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S+K">S.Kevin Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages,6 figures, and 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This study addresses the significant challenge of developing efficient
decoding algorithms for classifying steady-state visual evoked potentials
(SSVEPs) in scenarios characterized by extreme scarcity of calibration data,
where only one calibration is available for each stimulus target. To tackle
this problem, we introduce a novel cross-subject dual-domain fusion network
(CSDuDoFN) incorporating task-related and task-discriminant component analysis
(TRCA and TDCA) for one-shot SSVEP classification. The CSDuDoFN framework is
designed to comprehensively transfer information from source subjects, while
TRCA and TDCA are employed to exploit the single available calibration of the
target subject. Specifically, we develop multi-reference least-squares
transformation (MLST) to map data from both source subjects and the target
subject into the domain of sine-cosine templates, thereby mitigating
inter-individual variability and benefiting transfer learning. Subsequently,
the transformed data in the sine-cosine templates domain and the original
domain data are separately utilized to train a convolutional neural network
(CNN) model, with the adequate fusion of their feature maps occurring at
distinct network layers. To further capitalize on the calibration of the target
subject, source aliasing matrix estimation (SAME) data augmentation is
incorporated into the training process of the ensemble TRCA (eTRCA) and TDCA
models. Ultimately, the outputs of the CSDuDoFN, eTRCA, and TDCA are combined
for SSVEP classification. The effectiveness of our proposed approach is
comprehensively evaluated on three publicly available SSVEP datasets, achieving
the best performance on two datasets and competitive performance on one. This
underscores the potential for integrating brain-computer interface (BCI) into
daily life.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07941" title="Abstract">arXiv:2311.07941</a> [<a href="/pdf/2311.07941" title="Download PDF">pdf</a>, <a href="/format/2311.07941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-autoregressive Machine Translation with Probabilistic Context-free  Grammar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gui%2C+S">Shangtong Gui</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+C">Chenze Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhengrui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xishan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunji Chen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Non-autoregressive Transformer(NAT) significantly accelerates the inference
of neural machine translation. However, conventional NAT models suffer from
limited expression power and performance degradation compared to autoregressive
(AT) models due to the assumption of conditional independence among target
tokens. To address these limitations, we propose a novel approach called
PCFG-NAT, which leverages a specially designed Probabilistic Context-Free
Grammar (PCFG) to enhance the ability of NAT models to capture complex
dependencies among output tokens. Experimental results on major machine
translation benchmarks demonstrate that PCFG-NAT further narrows the gap in
translation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a
deeper understanding of the generated sentences, addressing the lack of
satisfactory explainability in neural machine translation.Code is publicly
available at https://github.com/ictnlp/PCFG-NAT.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07945" title="Abstract">arXiv:2311.07945</a> [<a href="/pdf/2311.07945" title="Download PDF">pdf</a>, <a href="/format/2311.07945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> First Step Advantage: Importance of Starting Right in Multi-Step  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+K">Kushal Jain</a>, 
<a href="/search/cs?searchtype=author&query=Shridhar%2C+K">Kumar Shridhar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) can solve complex reasoning tasks by generating
rationales for their predictions. Distilling these capabilities into a smaller,
compact model can facilitate the creation of specialized, cost-effective models
tailored for specific tasks. However, smaller models often face challenges in
complex reasoning tasks and often deviate from the correct reasoning path. We
show that LLMs can guide smaller models and bring them back to the correct
reasoning path only if they intervene at the right time. We show that smaller
models fail to reason primarily due to their difficulty in initiating the
process, and that guiding them in the right direction can lead to a performance
gain of over 100%. We explore different model sizes and evaluate the benefits
of providing guidance to improve reasoning in smaller models.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07946" title="Abstract">arXiv:2311.07946</a> [<a href="/pdf/2311.07946" title="Download PDF">pdf</a>, <a href="/format/2311.07946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Adversarial Node Placement in Decentralized Federated  Learning Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piaseczny%2C+A">Adam Piaseczny</a>, 
<a href="/search/cs?searchtype=author&query=Ruzomberka%2C+E">Eric Ruzomberka</a>, 
<a href="/search/cs?searchtype=author&query=Parasnis%2C+R">Rohit Parasnis</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICC 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">As Federated Learning (FL) grows in popularity, new decentralized frameworks
are becoming widespread. These frameworks leverage the benefits of
decentralized environments to enable fast and energy-efficient inter-device
communication. However, this growing popularity also intensifies the need for
robust security measures. While existing research has explored various aspects
of FL security, the role of adversarial node placement in decentralized
networks remains largely unexplored. This paper addresses this gap by analyzing
the performance of decentralized FL for various adversarial placement
strategies when adversaries can jointly coordinate their placement within a
network. We establish two baseline strategies for placing adversarial node:
random placement and network centrality-based placement. Building on this
foundation, we propose a novel attack algorithm that prioritizes adversarial
spread over adversarial centrality by maximizing the average network distance
between adversaries. We show that the new attack algorithm significantly
impacts key performance metrics such as testing accuracy, outperforming the
baseline frameworks by between 9% and 66.5% for the considered setups. Our
findings provide valuable insights into the vulnerabilities of decentralized FL
systems, setting the stage for future research aimed at developing more secure
and robust decentralized FL frameworks.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07947" title="Abstract">arXiv:2311.07947</a> [<a href="/pdf/2311.07947" title="Download PDF">pdf</a>, <a href="/ps/2311.07947" title="Download PostScript">ps</a>, <a href="/format/2311.07947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Technical Debt for Recommender System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moreschini%2C+S">Sergio Moreschini</a>, 
<a href="/search/cs?searchtype=author&query=Coba%2C+L">Ludovik Coba</a>, 
<a href="/search/cs?searchtype=author&query=Lenarduzzi%2C+V">Valentina Lenarduzzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Balancing the management of technical debt within recommender systems
requires effectively juggling the introduction of new features with the ongoing
maintenance and enhancement of the current system. Within the realm of
recommender systems, technical debt encompasses the trade-offs and expedient
choices made during the development and upkeep of the recommendation system,
which could potentially have adverse effects on its long-term performance,
scalability, and maintainability. In this vision paper, our objective is to
kickstart a research direction regarding Technical Debt in Recommender Systems.
We identified 15 potential factors, along with detailed explanations outlining
why it is advisable to consider them.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07948" title="Abstract">arXiv:2311.07948</a> [<a href="/pdf/2311.07948" title="Download PDF">pdf</a>, <a href="/format/2311.07948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Inductive Loop Invariants using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamath%2C+A">Adharsh Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Senthilnathan%2C+A">Aditya Senthilnathan</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Saikat Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Deligiannis%2C+P">Pantazis Deligiannis</a>, 
<a href="/search/cs?searchtype=author&query=Lahiri%2C+S+K">Shuvendu K. Lahiri</a>, 
<a href="/search/cs?searchtype=author&query=Lal%2C+A">Akash Lal</a>, 
<a href="/search/cs?searchtype=author&query=Rastogi%2C+A">Aseem Rastogi</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Subhajit Roy</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rahul Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Loop invariants are fundamental to reasoning about programs with loops. They
establish properties about a given loop's behavior. When they additionally are
inductive, they become useful for the task of formal verification that seeks to
establish strong mathematical guarantees about program's runtime behavior. The
inductiveness ensures that the invariants can be checked locally without
consulting the entire program, thus are indispensable artifacts in a formal
proof of correctness. Finding inductive loop invariants is an undecidable
problem, and despite a long history of research towards practical solutions, it
remains far from a solved problem. This paper investigates the capabilities of
the Large Language Models (LLMs) in offering a new solution towards this old,
yet important problem. To that end, we first curate a dataset of verification
problems on programs with loops. Next, we design a prompt for exploiting LLMs,
obtaining inductive loop invariants, that are checked for correctness using
sound symbolic tools. Finally, we explore the effectiveness of using an
efficient combination of a symbolic tool and an LLM on our dataset and compare
it against a purely symbolic baseline. Our results demonstrate that LLMs can
help improve the state-of-the-art in automated program verification.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07954" title="Abstract">arXiv:2311.07954</a> [<a href="/pdf/2311.07954" title="Download PDF">pdf</a>, <a href="/format/2311.07954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Closer Look at the Self-Verification Abilities of Large Language  Models in Logical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+R">Ruixin Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+X">Xinyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Changshui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Logical reasoning has been an ongoing pursuit in the field of AI. Despite
significant advancements made by large language models (LLMs), they still
struggle with complex logical reasoning problems. To enhance reasoning
performance, one promising direction is scalable oversight, which requires LLMs
to identify their own errors and then improve by themselves. Various
self-verification methods have been proposed in pursuit of this goal.
Nevertheless, whether existing models understand their own errors well is still
under investigation. In this paper, we take a closer look at the
self-verification abilities of LLMs in the context of logical reasoning,
focusing on their ability to identify logical fallacies accurately. We
introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies
categorized in a hierarchical taxonomy. By conducting exhaustive experiments on
FALLACIES, we obtain comprehensive and detailed analyses of a series of models
on their verification abilities. Our main findings suggest that existing LLMs
could struggle to identify fallacious reasoning steps accurately and may fall
short of guaranteeing the validity of self-verification methods. Drawing from
these observations, we offer suggestions for future research and practical
applications of self-verification methods.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07955" title="Abstract">arXiv:2311.07955</a> [<a href="/pdf/2311.07955" title="Download PDF">pdf</a>, <a href="/format/2311.07955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle  Imagery: Review and Experimental Comparisons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chenjie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R+W">Ryan Wen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Jingxiang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruobin Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the advancement of maritime unmanned aerial vehicles (UAVs) and deep
learning technologies, the application of UAV-based object detection has become
increasingly significant in the fields of maritime industry and ocean
engineering. Endowed with intelligent sensing capabilities, the maritime UAVs
enable effective and efficient maritime surveillance. To further promote the
development of maritime UAV-based object detection, this paper provides a
comprehensive review of challenges, relative methods, and UAV aerial datasets.
Specifically, in this work, we first briefly summarize four challenges for
object detection on maritime UAVs, i.e., object feature diversity, device
limitation, maritime environment variability, and dataset scarcity. We then
focus on computational methods to improve maritime UAV-based object detection
performance in terms of scale-aware, small object detection, view-aware,
rotated object detection, lightweight methods, and others. Next, we review the
UAV aerial image/video datasets and propose a maritime UAV aerial dataset named
MS2ship for ship detection. Furthermore, we conduct a series of experiments to
present the performance evaluation and robustness analysis of object detection
methods on maritime datasets. Eventually, we give the discussion and outlook on
future works for maritime UAV-based object detection. The MS2ship dataset is
available at
\href{https://github.com/zcj234/MS2ship}{https://github.com/zcj234/MS2ship}.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07957" title="Abstract">arXiv:2311.07957</a> [<a href="/pdf/2311.07957" title="Download PDF">pdf</a>, <a href="/format/2311.07957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models are Better Bug Detector Through Code-Pair Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alrashedy%2C+K">Kamel Alrashedy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) such as GPT-3.5 and CodeLlama are powerful
models for code generation and understanding. Fine-tuning these models comes
with a high computational cost and requires a large labeled dataset.
Alternatively, in-context learning techniques allow models to learn downstream
tasks with only a few examples. Recently, researchers have shown how in-context
learning performs well in bug detection and repair. In this paper, we propose
code-pair classification task in which both the buggy and non-buggy versions
are given to the model, and the model identifies the buggy ones. We evaluate
our task in real-world dataset of bug detection and two most powerful LLMs. Our
experiments indicate that an LLM can often pick the buggy from the non-buggy
version of the code, and the code-pair classification task is much easier
compared to be given a snippet and deciding if and where a bug exists.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07961" title="Abstract">arXiv:2311.07961</a> [<a href="/pdf/2311.07961" title="Download PDF">pdf</a>, <a href="/format/2311.07961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The ART of LLM Refinement: Ask, Refine, and Trust
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shridhar%2C+K">Kumar Shridhar</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+K">Koustuv Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A">Andrew Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianlu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Ping Yu</a>, 
<a href="/search/cs?searchtype=author&query=Pasunuru%2C+R">Ram Pasunuru</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Weston%2C+J">Jason Weston</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In recent years, Large Language Models (LLMs) have demonstrated remarkable
generative abilities, but can they judge the quality of their own generations?
A popular concept, referred to as self-refinement, postulates that LLMs can
detect and correct the errors in their generations when asked to do so.
However, recent empirical evidence points in the opposite direction, suggesting
that LLMs often struggle to accurately identify errors when reasoning is
involved. To address this, we propose a reasoning with refinement objective
called ART: Ask, Refine, and Trust, which asks necessary questions to decide
when an LLM should refine its output, and either affirm or withhold trust in
its refinement by ranking the refinement and the initial prediction. On two
multistep reasoning tasks of mathematical word problems (GSM8K) and question
answering (StrategyQA), ART achieves a performance gain of +5 points over
self-refinement baselines, while using a much smaller model as the decision
maker. We also demonstrate the benefit of using smaller models to make
refinement decisions as a cost-effective alternative to fine-tuning a larger
model.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07964" title="Abstract">arXiv:2311.07964</a> [<a href="/pdf/2311.07964" title="Download PDF">pdf</a>, <a href="/ps/2311.07964" title="Download PostScript">ps</a>, <a href="/format/2311.07964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surveying Wikipedians: a dataset of users and contributors&#x27; practices on  Wikipedia in 8 languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cruciani%2C+C">Caterina Cruciani</a>, 
<a href="/search/cs?searchtype=author&query=Joubert%2C+L">L&#xe9;o Joubert</a> (LEST, DySoLab), 
<a href="/search/cs?searchtype=author&query=Jullien%2C+N">Nicolas Jullien</a> (IMT Atlantique - LUSSI, MARSOUIN, LEGO), 
<a href="/search/cs?searchtype=author&query=Mell%2C+L">Laurent Mell</a> (CREAD EA 3875, MARSOUIN), 
<a href="/search/cs?searchtype=author&query=Piccione%2C+S">Sasha Piccione</a>, 
<a href="/search/cs?searchtype=author&query=Vermeirsche%2C+J">Jeanne Vermeirsche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The dataset focuses on Wikipedia users and contains information about
demographic and socioeconomic characteristics of the respondents and their
activity on Wikipedia. The data was collected using a questionnaire available
online between June and July 2023. The link to the questionnaire was
distributed via a banner published in 8 languages on the Wikipedia page.
Filling out the questionnaire was voluntary and not incentivised in any way.
The survey includes 200 questions about: what people were doing on Wikipedia
before clicking the link to the questionnaire; how they use Wikipedia as
readers (``professional'' and ``personal'' uses); their opinion on the quality,
the thematic coverage, the importance of the encyclopaedia; the making of
Wikipedia (how they think it is made, if they have ever contributed and how);
their social, sport, artistic and cultural activities, both online and offline;
their socio-economic characteristics including political beliefs, and trust
propensities. More than 200 000 people opened the questionnaire, 100 332
started to answer, and constitute our dataset, and 10 576 finished it. Among
other themes identified by future researchers, the dataset can be useful for
advancing the research regarding the features of readers vs contributors of
online commons, the relationship between trust, information, sources, and the
use made of this information.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07965" title="Abstract">arXiv:2311.07965</a> [<a href="/pdf/2311.07965" title="Download PDF">pdf</a>, <a href="/format/2311.07965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DQR-TTS: Semi-supervised Text-to-speech Synthesis with Dynamic Quantized  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiangzong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengcheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xulong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Ning Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 21st IEEE International Symposium on Parallel and Distributed Processing with Applications (IEEE ISPA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Most existing neural-based text-to-speech methods rely on extensive datasets
and face challenges under low-resource condition. In this paper, we introduce a
novel semi-supervised text-to-speech synthesis model that learns from both
paired and unpaired data to address this challenge. The key component of the
proposed model is a dynamic quantized representation module, which is
integrated into a sequential autoencoder. When given paired data, the module
incorporates a trainable codebook that learns quantized representations under
the supervision of the paired data. However, due to the limited paired data in
low-resource scenario, these paired data are difficult to cover all phonemes.
Then unpaired data is fed to expand the dynamic codebook by adding quantized
representation vectors that are sufficiently distant from the existing ones
during training. Experiments show that with less than 120 minutes of paired
data, the proposed method outperforms existing methods in both subjective and
objective metrics.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07966" title="Abstract">arXiv:2311.07966</a> [<a href="/pdf/2311.07966" title="Download PDF">pdf</a>, <a href="/format/2311.07966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher-Order Expander Graph Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christie%2C+T">Thomas Christie</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yu He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph neural networks operate on graph-structured data via exchanging
messages along edges. One limitation of this message passing paradigm is the
over-squashing problem. Over-squashing occurs when messages from a node's
expanded receptive field are compressed into fixed-size vectors, potentially
causing information loss. To address this issue, recent works have explored
using expander graphs, which are highly-connected sparse graphs with low
diameters, to perform message passing. However, current methods on expander
graph propagation only consider pair-wise interactions, ignoring higher-order
structures in complex data. To explore the benefits of capturing these
higher-order correlations while still leveraging expander graphs, we introduce
higher-order expander graph propagation. We propose two methods for
constructing bipartite expanders and evaluate their performance on both
synthetic and real-world datasets.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07967" title="Abstract">arXiv:2311.07967</a> [<a href="/pdf/2311.07967" title="Download PDF">pdf</a>, <a href="/format/2311.07967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of two data fusion approaches for land use classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cubaud%2C+M">Martin Cubaud</a> (LaSTIG), 
<a href="/search/cs?searchtype=author&query=Bris%2C+A+L">Arnaud Le Bris</a> (LaSTIG), 
<a href="/search/cs?searchtype=author&query=Jolivet%2C+L">Laurence Jolivet</a> (LaSTIG), 
<a href="/search/cs?searchtype=author&query=Olteanu-Raimond%2C+A">Ana-Maria Olteanu-Raimond</a> (LaSTIG)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISPRS Geospatial Week 2023, Sep 2023, Cairo, Egypt., Egypt
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Accurate land use maps, describing the territory from an anthropic
utilisation point of view, are useful tools for land management and planning.
To produce them, the use of optical images alone remains limited. It is
therefore necessary to make use of several heterogeneous sources, each carrying
complementary or contradictory information due to their imperfections or their
different specifications. This study compares two different approaches i.e. a
pre-classification and a post-classification fusion approach for combining
several sources of spatial data in the context of land use classification. The
approaches are applied on authoritative land use data located in the Gers
department in the southwest of France. Pre-classification fusion, while not
explicitly modeling imperfections, has the best final results, reaching an
overall accuracy of 97% and a macro-mean F1 score of 88%.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07975" title="Abstract">arXiv:2311.07975</a> [<a href="/pdf/2311.07975" title="Download PDF">pdf</a>, <a href="/format/2311.07975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Out-of-Distribution Knowledge Distillation via Confidence Amendment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Longbing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Out-of-distribution (OOD) detection is essential in identifying test samples
that deviate from the in-distribution (ID) data upon which a standard network
is trained, ensuring network robustness and reliability. This paper introduces
OOD knowledge distillation, a pioneering learning framework applicable whether
or not training ID data is available, given a standard network. This framework
harnesses OOD-sensitive knowledge from the standard network to craft a binary
classifier adept at distinguishing between ID and OOD samples. To accomplish
this, we introduce Confidence Amendment (CA), an innovative methodology that
transforms an OOD sample into an ID one while progressively amending prediction
confidence derived from the standard network. This approach enables the
simultaneous synthesis of both ID and OOD samples, each accompanied by an
adjusted prediction confidence, thereby facilitating the training of a binary
classifier sensitive to OOD. Theoretical analysis provides bounds on the
generalization error of the binary classifier, demonstrating the pivotal role
of confidence amendment in enhancing OOD sensitivity. Extensive experiments
spanning various datasets and network architectures confirm the efficacy of the
proposed method in detecting OOD samples.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07978" title="Abstract">arXiv:2311.07978</a> [<a href="/pdf/2311.07978" title="Download PDF">pdf</a>, <a href="/format/2311.07978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How good are Large Language Models on African Languages?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ojo%2C+J">Jessica Ojo</a>, 
<a href="/search/cs?searchtype=author&query=Ogueji%2C+K">Kelechi Ogueji</a>, 
<a href="/search/cs?searchtype=author&query=Stenetorp%2C+P">Pontus Stenetorp</a>, 
<a href="/search/cs?searchtype=author&query=Adelani%2C+D+I">David I. Adelani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advancements in natural language processing have led to the
proliferation of large language models (LLMs). These models have been shown to
yield good performance, using in-context learning, even on unseen tasks and
languages. Additionally, they have been widely adopted as
language-model-as-a-service commercial APIs like GPT-4 API. However, their
performance on African languages is largely unknown. We present an analysis of
three popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks
(news topic classification, sentiment classification, machine translation,
question answering, and named entity recognition) across 30 African languages,
spanning different language families and geographical regions. Our results
suggest that all LLMs produce below-par performance on African languages, and
there is a large gap in performance compared to high-resource languages like
English most tasks. We find that GPT-4 has an average or impressive performance
on classification tasks but very poor results on generative tasks like machine
translation. Surprisingly, we find that mT0 had the best overall on
cross-lingual QA, better than the state-of-the-art supervised model (i.e.
fine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the
worst performance due to its limited multilingual capabilities and
English-centric pre-training corpus. In general, our findings present a
call-to-action to ensure African languages are well represented in large
language models, given their growing popularity.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07981" title="Abstract">arXiv:2311.07981</a> [<a href="/pdf/2311.07981" title="Download PDF">pdf</a>, <a href="/format/2311.07981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Individual Tree Mapping with Sub-meter Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gominski%2C+D">Dimitri Gominski</a>, 
<a href="/search/cs?searchtype=author&query=Kariryaa%2C+A">Ankit Kariryaa</a>, 
<a href="/search/cs?searchtype=author&query=Brandt%2C+M">Martin Brandt</a>, 
<a href="/search/cs?searchtype=author&query=Igel%2C+C">Christian Igel</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sizhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Mugabowindekwe%2C+M">Maurice Mugabowindekwe</a>, 
<a href="/search/cs?searchtype=author&query=Fensholt%2C+R">Rasmus Fensholt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">There is a rising interest in mapping trees using satellite or aerial
imagery, but there is no standardized evaluation protocol for comparing and
enhancing methods. In dense canopy areas, the high variability of tree sizes
and their spatial proximity makes it arduous to define the quality of the
predictions. Concurrently, object-centric approaches such as bounding box
detection usuallyperform poorly on small and dense objects. It thus remains
unclear what is the ideal framework for individual tree mapping, in regards to
detection and segmentation approaches, convolutional neural networks and
transformers. In this paper, we introduce an evaluation framework suited for
individual tree mapping in any physical environment, with annotation costs and
applicative goals in mind. We review and compare different approaches and deep
architectures, and introduce a new method that we experimentally prove to be a
good compromise between segmentation and detection.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07985" title="Abstract">arXiv:2311.07985</a> [<a href="/pdf/2311.07985" title="Download PDF">pdf</a>, <a href="/format/2311.07985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Configurable convolutional neural networks for real-time  pedestrian-level wind prediction in urban environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clemente%2C+A+V">Alfredo Vicente Clemente</a>, 
<a href="/search/cs?searchtype=author&query=Giljarhus%2C+K+E+T">Knut Erik Teigen Giljarhus</a>, 
<a href="/search/cs?searchtype=author&query=Oggiano%2C+L">Luca Oggiano</a>, 
<a href="/search/cs?searchtype=author&query=Ruocco%2C+M">Massimiliano Ruocco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Urbanization has underscored the importance of understanding the pedestrian
wind environment in urban and architectural design contexts. Pedestrian Wind
Comfort (PWC) focuses on the effects of wind on the safety and comfort of
pedestrians and cyclists, given the influence of urban structures on the local
microclimate. Traditional Computational Fluid Dynamics (CFD) methods used for
PWC analysis have limitations in computation, cost, and time. Deep-learning
models have the potential to significantly speed up this process. The
prevailing state-of-the-art methodologies largely rely on GAN-based models,
such as pix2pix, which have exhibited training instability issues. In contrast,
our work introduces a convolutional neural network (CNN) approach based on the
U-Net architecture, offering a more stable and streamlined solution. The
process of generating a wind flow prediction at pedestrian level is
reformulated from a 3D CFD simulation into a 2D image-to-image translation
task, using the projected building heights as input. Testing on standard
consumer hardware shows that our model can efficiently predict wind velocities
in urban settings in real time. Further tests on different configurations of
the model, combined with a Pareto front analysis, helped identify the trade-off
between accuracy and computational efficiency. This CNN-based approach provides
a fast and efficient method for PWC analysis, potentially aiding in more
efficient urban design processes.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07986" title="Abstract">arXiv:2311.07986</a> [<a href="/pdf/2311.07986" title="Download PDF">pdf</a>, <a href="/format/2311.07986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the View-and-Channel Aggregation Gain in Integrated Sensing and Edge  AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaibin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Sensing and edge artificial intelligence (AI) are two key features of the
sixth-generation (6G) mobile networks. Their natural integration, termed
Integrated sensing and edge AI (ISEA), is envisioned to automate wide-ranging
Internet-of-Tings (IoT) applications. To achieve a high sensing accuracy,
multi-view features are uploaded to an edge server for aggregation and
inference using an AI model. The view aggregation is realized efficiently using
over-the-air computing (AirComp), which also aggregates channels to suppress
channel noise. At its nascent stage, ISEA still lacks a characterization of the
fundamental performance gains from view-and-channel aggregation, which
motivates this work. Our framework leverages a well-established distribution
model of multi-view sensing data where the classic Gaussian-mixture model is
modified by adding sub-spaces matrices to represent individual sensor
observation perspectives. Based on the model, we study the End-to-End sensing
(inference) uncertainty, a popular measure of inference accuracy, of the said
ISEA system by a novel approach involving designing a scaling-tight uncertainty
surrogate function, global discriminant gain, distribution of receive
Signal-to-Noise Ratio (SNR), and channel induced discriminant loss. We prove
that the E2E sensing uncertainty diminishes at an exponential rate as the
number of views/sensors grows, where the rate is proportional to global
discriminant gain. Given channel distortion, we further show that the
exponential scaling remains with a reduced decay rate related to the channel
induced discriminant loss. Furthermore, we benchmark AirComp against equally
fast, traditional analog orthogonal access, which reveals a sensing-accuracy
crossing point between the schemes, leading to the proposal of adaptive
access-mode switching. Last, the insights from our framework are validated by
experiments using real-world dataset.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07987" title="Abstract">arXiv:2311.07987</a> [<a href="/pdf/2311.07987" title="Download PDF">pdf</a>, <a href="/ps/2311.07987" title="Download PostScript">ps</a>, <a href="/format/2311.07987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lateral control for autonomous vehicles: A comparative evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Artu%C3%B1edo%2C+A">Antonio Artu&#xf1;edo</a>, 
<a href="/search/eess?searchtype=author&query=Moreno-Gonzalez%2C+M">Marcos Moreno-Gonzalez</a>, 
<a href="/search/eess?searchtype=author&query=Villagra%2C+J">Jorge Villagra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Video showcasing a real-world test of a model-free lateral controller in an automated vehicle: <a href="https://youtu.be/JtLfZzEdGC8">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Annual Reviews in Control, Volume 57, 2024, 100910, ISSN 1367-5788
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The selection of an appropriate control strategy is essential for ensuring
safe operation in autonomous driving. While numerous control strategies have
been developed for specific driving scenarios, a comprehensive comparative
assessment of their performance using the same tuning methodology is lacking in
the literature. This paper addresses this gap by presenting a systematic
evaluation of state-of-the-art model-free and model-based control strategies.
The objective is to evaluate and contrast the performance of these controllers
across a wide range of driving scenarios, reflecting the diverse needs of
autonomous vehicles. To facilitate the comparative analysis, a comprehensive
set of performance metrics is selected, encompassing accuracy, robustness, and
comfort. The contributions of this research include the design of a systematic
tuning methodology, the use of two novel metrics for stability and comfort
comparisons and the evaluation through extensive simulations and real tests in
an experimental instrumented vehicle over a wide range of trajectories.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07989" title="Abstract">arXiv:2311.07989</a> [<a href="/pdf/2311.07989" title="Download PDF">pdf</a>, <a href="/format/2311.07989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Language Models for Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziyin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chaoyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Cong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Repo is available at <a href="https://github.com/codefuse-ai/Awesome-Code-LLM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">In this work we systematically review the recent advancements in code
processing with language models, covering 50+ models, 30+ evaluation tasks, and
500 related works. We break down code processing models into general language
models represented by the GPT family and specialized models that are
specifically pretrained on code, often with tailored objectives. We discuss the
relations and differences between these models, and highlight the historical
transition of code modeling from statistical models and RNNs to pretrained
Transformers and LLMs, which is exactly the same course that had been taken by
NLP. We also discuss code-specific features such as AST, CFG, and unit tests,
along with their application in training code language models, and identify key
challenges and potential future directions in this domain. We keep the survey
open and updated on github repository at
https://github.com/codefuse-ai/Awesome-Code-LLM.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07991" title="Abstract">arXiv:2311.07991</a> [<a href="/pdf/2311.07991" title="Download PDF">pdf</a>, <a href="/format/2311.07991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear Stability Boundary Assessment Of Wind Power Plants Based on  Reverse-Time Trajectory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ghosh%2C+S">Sujay Ghosh</a>, 
<a href="/search/eess?searchtype=author&query=Bakhshizadeh%2C+M+K">Mohammad Kazem Bakhshizadeh</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guangya Yang</a>, 
<a href="/search/eess?searchtype=author&query=Kocewiak%2C+L">Lukasz Kocewiak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This letter determines the nonlinear stability boundary of a wind power plant
(WPP) connected to an AC power grid via a long HVAC cable. The analysis focuses
on the slow Phase-Locked Loop (PLL) dynamics, with an assumption that the fast
current control dynamics can be neglected. To begin, we propose an aggregated
reduced-order wind turbine model. This aggregation can be applied up to a
limited frequency, e.g. 400Hz, which aligns with our assumption regarding
low-frequency dynamics. The WPP collector and transmission network model is
established using impedance/frequency scan approximated around $\pm$5 Hz of the
PLL nominal frequency, accounting for the hard saturation limits. The stability
boundary of the reduced-order system is determined by reverse time trajectory,
offering valuable insights into the WPP's overall stability. The work presents
a routine from modelling to nonlinear stability assessment for offshore wind
farm applications.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07992" title="Abstract">arXiv:2311.07992</a> [<a href="/pdf/2311.07992" title="Download PDF">pdf</a>, <a href="/format/2311.07992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probable Object Location (POLo) Score Estimation for Efficient Object  Goal Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Soh%2C+H">Harold Soh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To advance the field of autonomous robotics, particularly in object search
tasks within unexplored environments, we introduce a novel framework centered
around the Probable Object Location (POLo) score. Utilizing a 3D object
probability map, the POLo score allows the agent to make data-driven decisions
for efficient object search. We further enhance the framework's practicality by
introducing POLoNet, a neural network trained to approximate the
computationally intensive POLo score. Our approach addresses critical
limitations of both end-to-end reinforcement learning methods, which suffer
from memory decay over long-horizon tasks, and traditional map-based methods
that neglect visibility constraints. Our experiments, involving the first phase
of the OVMM 2023 challenge, demonstrate that an agent equipped with POLoNet
significantly outperforms a range of baseline methods, including end-to-end RL
techniques and prior map-based strategies. To provide a comprehensive
evaluation, we introduce new performance metrics that offer insights into the
efficiency and effectiveness of various agents in object goal navigation.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07993" title="Abstract">arXiv:2311.07993</a> [<a href="/pdf/2311.07993" title="Download PDF">pdf</a>, <a href="/format/2311.07993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit Change Relation Learning for Change Detection in VHR Remote  Sensing Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Dalong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zebin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+C">Chih-Cheng Hung</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhihui Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Change detection has always been a concerned task in the interpretation of
remote sensing images. It is essentially a unique binary classification task
with two inputs, and there is a change relationship between these two inputs.
At present, the mining of change relationship features is usually implicit in
the network architectures that contain single-branch or two-branch encoders.
However, due to the lack of artificial prior design for change relationship
features, these networks cannot learn enough change semantic information and
lose more accurate change detection performance. So we propose a network
architecture NAME for the explicit mining of change relation features. In our
opinion, the change features of change detection should be divided into
pre-changed image features, post-changed image features and change relation
features. In order to fully mine these three kinds of change features, we
propose the triple branch network combining the transformer and convolutional
neural network (CNN) to extract and fuse these change features from two
perspectives of global information and local information, respectively. In
addition, we design the continuous change relation (CCR) branch to further
obtain the continuous and detail change relation features to improve the change
discrimination capability of the model. The experimental results show that our
network performs better, in terms of F1, IoU, and OA, than those of the
existing advanced networks for change detection on four public very
high-resolution (VHR) remote sensing datasets. Our source code is available at
https://github.com/DalongZ/NAME.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07994" title="Abstract">arXiv:2311.07994</a> [<a href="/pdf/2311.07994" title="Download PDF">pdf</a>, <a href="/format/2311.07994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Retrieval with Multi-Stage Re-Ranking Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sasazawa%2C+Y">Yuichi Sasazawa</a>, 
<a href="/search/cs?searchtype=author&query=Yokote%2C+K">Kenichi Yokote</a>, 
<a href="/search/cs?searchtype=author&query=Imaichi%2C+O">Osamu Imaichi</a>, 
<a href="/search/cs?searchtype=author&query=Sogawa%2C+Y">Yasuhiro Sogawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The text retrieval is the task of retrieving similar documents to a search
query, and it is important to improve retrieval accuracy while maintaining a
certain level of retrieval speed. Existing studies have reported accuracy
improvements using language models, but many of these do not take into account
the reduction in search speed that comes with increased performance. In this
study, we propose three-stage re-ranking model using model ensembles or larger
language models to improve search accuracy while minimizing the search delay.
We ranked the documents by BM25 and language models, and then re-ranks by a
model ensemble or a larger language model for documents with high similarity to
the query. In our experiments, we train the MiniLM language model on the
MS-MARCO dataset and evaluate it in a zero-shot setting. Our proposed method
achieves higher retrieval accuracy while reducing the retrieval speed decay.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07996" title="Abstract">arXiv:2311.07996</a> [<a href="/pdf/2311.07996" title="Download PDF">pdf</a>, <a href="/format/2311.07996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Well Do Text Embedding Models Understand Syntax?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhaopeng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Z">Zhiyang Teng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP-Findings 2023, datasets and code are released
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text embedding models have significantly contributed to advancements in
natural language processing by adeptly capturing semantic properties of textual
data. However, the ability of these models to generalize across a wide range of
syntactic contexts remains under-explored. In this paper, we first develop an
evaluation set, named \textbf{SR}, to scrutinize the capability for syntax
understanding of text embedding models from two crucial syntactic aspects:
Structural heuristics, and Relational understanding among concepts, as revealed
by the performance gaps in previous studies. Our findings reveal that existing
text embedding models have not sufficiently addressed these syntactic
understanding challenges, and such ineffectiveness becomes even more apparent
when evaluated against existing benchmark datasets. Furthermore, we conduct
rigorous analysis to unearth factors that lead to such limitations and examine
why previous evaluations fail to detect such ineffectiveness. Lastly, we
propose strategies to augment the generalization ability of text embedding
models in diverse syntactic scenarios. This study serves to highlight the
hurdles associated with syntactic generalization and provides pragmatic
guidance for boosting model performance across varied syntactic contexts.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08000" title="Abstract">arXiv:2311.08000</a> [<a href="/pdf/2311.08000" title="Download PDF">pdf</a>, <a href="/format/2311.08000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiPar: A Lightweight Parallel Learning Model for Practical In-Vehicle  Network Intrusion Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aiheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bailing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yulei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 13 figures, 6 tables, 51 reference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the development of intelligent transportation systems, vehicles are
exposed to a complex network environment. As the main network of in-vehicle
networks, the controller area network (CAN) has many potential security
hazards, resulting in higher requirements for intrusion detection systems to
ensure safety. Among intrusion detection technologies, methods based on deep
learning work best without prior expert knowledge. However, they all have a
large model size and rely on cloud computing, and are therefore not suitable to
be installed on the in-vehicle network. Therefore, we propose a lightweight
parallel neural network structure, LiPar, to allocate task loads to multiple
electronic control units (ECU). The LiPar model consists of multi-dimensional
branch convolution networks, spatial and temporal feature fusion learning, and
a resource adaptation algorithm. Through experiments, we prove that LiPar has
great detection performance, running efficiency, and lightweight model size,
which can be well adapted to the in-vehicle environment practically and protect
the in-vehicle CAN bus security.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08001" title="Abstract">arXiv:2311.08001</a> [<a href="/pdf/2311.08001" title="Download PDF">pdf</a>, <a href="/ps/2311.08001" title="Download PostScript">ps</a>, <a href="/format/2311.08001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Analysis of the COVID-19 Infodemic in English and Chinese:  Insights from Social Media Textual Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jia Luo</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Daiyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Lei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Baz%2C+D+E">Didier El Baz</a> (LAAS-SARA), 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinran Liu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Frontiers in Public Health, 2023, 11
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">The COVID-19 infodemic, characterized by the rapid spread of misinformation
and unverified claims related to the pandemic, presents a significant
challenge. This paper presents a comparative analysis of the COVID-19 infodemic
in the English and Chinese languages, utilizing textual data extracted from
social media platforms. To ensure a balanced representation, two infodemic
datasets were created by augmenting previously collected social media textual
data. Through word frequency analysis, the thirty-five most frequently
occurring infodemic words are identified, shedding light on prevalent
discussions surrounding the infodemic. Moreover, topic clustering analysis
uncovers thematic structures and provides a deeper understanding of primary
topics within each language context. Additionally, sentiment analysis enables
comprehension of the emotional tone associated with COVID-19 information on
social media platforms in English and Chinese. This research contributes to a
better understanding of the COVID-19 infodemic phenomenon and can guide the
development of strategies to combat misinformation during public health crises
across different languages.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08002" title="Abstract">arXiv:2311.08002</a> [<a href="/pdf/2311.08002" title="Download PDF">pdf</a>, <a href="/format/2311.08002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TempTabQA: Temporal Question Answering for Semi-Structured Tables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vivek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kandoi%2C+P">Pranshu Kandoi</a>, 
<a href="/search/cs?searchtype=author&query=Vora%2C+M+B">Mahek Bhavesh Vora</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yujie He</a>, 
<a href="/search/cs?searchtype=author&query=Reinanda%2C+R">Ridho Reinanda</a>, 
<a href="/search/cs?searchtype=author&query=Srikumar%2C+V">Vivek Srikumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023(Main), 23 Figures, 32 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Semi-structured data, such as Infobox tables, often include temporal
information about entities, either implicitly or explicitly. Can current NLP
systems reason about such information in semi-structured tables? To tackle this
question, we introduce the task of temporal question answering on
semi-structured tables. We present a dataset, TempTabQA, which comprises 11,454
question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning
more than 90 distinct domains. Using this dataset, we evaluate several
state-of-the-art models for temporal reasoning. We observe that even the
top-performing LLMs lag behind human performance by more than 13.5 F1 points.
Given these results, our dataset has the potential to serve as a challenging
benchmark to improve the temporal reasoning capabilities of NLP models.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08005" title="Abstract">arXiv:2311.08005</a> [<a href="/pdf/2311.08005" title="Download PDF">pdf</a>, <a href="/format/2311.08005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative missing value imputation based on feature importance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Cong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Many datasets suffer from missing values due to various reasons,which not
only increases the processing difficulty of related tasks but also reduces the
accuracy of classification. To address this problem, the mainstream approach is
to use missing value imputation to complete the dataset. Existing imputation
methods estimate the missing parts based on the observed values in the original
feature space, and they treat all features as equally important during data
completion, while in fact different features have different importance.
Therefore, we have designed an imputation method that considers feature
importance. This algorithm iteratively performs matrix completion and feature
importance learning, and specifically, matrix completion is based on a filling
loss that incorporates feature importance. Our experimental analysis involves
three types of datasets: synthetic datasets with different noisy features and
missing values, real-world datasets with artificially generated missing values,
and real-world datasets originally containing missing values. The results on
these datasets consistently show that the proposed method outperforms the
existing five imputation algorithms.To the best of our knowledge, this is the
first work that considers feature importance in the imputation model.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08007" title="Abstract">arXiv:2311.08007</a> [<a href="/pdf/2311.08007" title="Download PDF">pdf</a>, <a href="/format/2311.08007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame  Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zhihang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+G">Gurunandan Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Sizhuo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://zzh-tech.github.io/InterpAny-Clearer/">this https URL</a> ; Code: <a href="https://github.com/zzh-tech/InterpAny-Clearer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing video frame interpolation (VFI) methods blindly predict where each
object is at a specific timestep t ("time indexing"), which struggles to
predict precise object movements. Given two images of a baseball, there are
infinitely many possible trajectories: accelerating or decelerating, straight
or curved. This often results in blurry frames as the method averages out these
possibilities. Instead of forcing the network to learn this complicated
time-to-location mapping implicitly together with predicting the frames, we
provide the network with an explicit hint on how far the object has traveled
between start and end frames, a novel approach termed "distance indexing". This
method offers a clearer learning goal for models, reducing the uncertainty tied
to object speeds. We further observed that, even with this extra guidance,
objects can still be blurry especially when they are equally far from both
input frames (i.e., halfway in-between), due to the directional ambiguity in
long-range motion. To solve this, we propose an iterative reference-based
estimation strategy that breaks down a long-range prediction into several
short-range steps. When integrating our plug-and-play strategies into
state-of-the-art learning-based models, they exhibit markedly sharper outputs
and superior perceptual quality in arbitrary time interpolations, using a
uniform distance indexing map in the same format as time indexing.
Additionally, distance indexing can be specified pixel-wise, which enables
temporal manipulation of each object independently, offering a novel tool for
video editing tasks like re-timing.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08009" title="Abstract">arXiv:2311.08009</a> [<a href="/pdf/2311.08009" title="Download PDF">pdf</a>, <a href="/ps/2311.08009" title="Download PostScript">ps</a>, <a href="/format/2311.08009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Human to Robot Interactions: A Circular Approach towards  Trustworthy Social Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lange%2C+A+L">Anna L. Lange</a>, 
<a href="/search/cs?searchtype=author&query=Kirtay%2C+M">Murat Kirtay</a>, 
<a href="/search/cs?searchtype=author&query=Hafner%2C+V+V">Verena V. Hafner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In SCRITA 2023 Workshop Proceedings (<a href="/abs/2311.05401">arXiv:2311.05401</a>) held in conjunction with 32nd IEEE International Conference on Robot &amp; Human Interactive Communication, 28/08 - 31/08 2023, Busan (Korea)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Human trust research uncovered important catalysts for trust building between
interaction partners such as appearance or cognitive factors. The introduction
of robots into social interactions calls for a reevaluation of these findings
and also brings new challenges and opportunities. In this paper, we suggest
approaching trust research in a circular way by drawing from human trust
findings, validating them and conceptualizing them for robots, and finally
using the precise manipulability of robots to explore previously less-explored
areas of trust formation to generate new hypotheses for trust building between
agents.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08010" title="Abstract">arXiv:2311.08010</a> [<a href="/pdf/2311.08010" title="Download PDF">pdf</a>, <a href="/format/2311.08010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distantly-Supervised Named Entity Recognition with Uncertainty-aware  Teacher Learning and Student-student Collaborative Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Helan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Shuzheng Si</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haozhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shuang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+K">Kaikai An</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zefan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+B">Baobao Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates
the burden of annotation, but meanwhile suffers from the label noise. Recent
works attempt to adopt the teacher-student framework to gradually refine the
training labels and improve the overall robustness. However, we argue that
these teacher-student methods achieve limited performance because poor network
calibration produces incorrectly pseudo-labeled samples, leading to error
propagation. Therefore, we attempt to mitigate this issue by proposing: (1)
Uncertainty-aware Teacher Learning that leverages the prediction uncertainty to
guide the selection of pseudo-labels, avoiding the number of incorrect
pseudo-labels in the self-training stage. (2) Student-student Collaborative
Learning that allows the transfer of reliable labels between two student
networks instead of completely relying on all pseudo-labels from its teacher.
Meanwhile, this approach allows a full exploration of mislabeled samples rather
than simply filtering unreliable pseudo-labeled samples. Extensive experimental
results on five DS-NER datasets demonstrate that our method is superior to
state-of-the-art teacher-student methods.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08011" title="Abstract">arXiv:2311.08011</a> [<a href="/pdf/2311.08011" title="Download PDF">pdf</a>, <a href="/format/2311.08011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forgetting before Learning: Utilizing Parametric Arithmetic for  Knowledge Updating in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+S">Shiwen Ni</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dingwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiping Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently Large Language Models (LLMs) have demonstrated their amazing text
understanding and generation capabilities. However, even stronger LLMs may
still learn incorrect knowledge from the training corpus, as well as some
knowledge that is outdated over time. Direct secondary fine-tuning with data
containing new knowledge may be ineffective in updating knowledge due to the
conflict between old and new knowledge. In this paper, we propose a new
paradigm for fine-tuning called F-Learning (Forgetting before Learning), which
is based on parametric arithmetic to achieve forgetting of old knowledge and
learning of new knowledge. Experimental results on two publicly available
datasets demonstrate that our proposed F-Learning can obviously improve the
knowledge updating performance of both full fine-tuning and LoRA fine-tuning.
Moreover, we have also discovered that forgetting old knowledge by subtracting
the parameters of LoRA can achieve a similar effect to subtracting the
parameters of full fine-tuning, and sometimes even surpass it significantly.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08013" title="Abstract">arXiv:2311.08013</a> [<a href="/pdf/2311.08013" title="Download PDF">pdf</a>, <a href="/format/2311.08013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CP-SLAM: Collaborative Neural Point-based SLAM System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiarui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+M">Mao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hujun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Z">Zhaopeng Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Robotics (cs.RO)

</div>
<p class="mathjax">This paper presents a collaborative implicit neural simultaneous localization
and mapping (SLAM) system with RGB-D image sequences, which consists of
complete front-end and back-end modules including odometry, loop detection,
sub-map fusion, and global refinement. In order to enable all these modules in
a unified framework, we propose a novel neural point based 3D scene
representation in which each point maintains a learnable neural feature for
scene encoding and is associated with a certain keyframe. Moreover, a
distributed-to-centralized learning strategy is proposed for the collaborative
implicit SLAM to improve consistency and cooperation. A novel global
optimization framework is also proposed to improve the system accuracy like
traditional bundle adjustment. Experiments on various datasets demonstrate the
superiority of the proposed method in both camera tracking and mapping.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08019" title="Abstract">arXiv:2311.08019</a> [<a href="/pdf/2311.08019" title="Download PDF">pdf</a>, <a href="/format/2311.08019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Servoing NMPC Applied to UAVs for Photovoltaic Array Inspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Velasco-S%C3%A1nchez%2C+E+P">Edison P. Velasco-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Recalde%2C+L+F">Luis F. Recalde</a>, 
<a href="/search/cs?searchtype=author&query=Guevara%2C+B+S">Bryan S. Guevara</a>, 
<a href="/search/cs?searchtype=author&query=Varela-Ald%C3%A1s%2C+J">Jos&#xe9; Varela-Ald&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Candelas%2C+F+A">Francisco A. Candelas</a>, 
<a href="/search/cs?searchtype=author&query=Puente%2C+S+T">Santiago T. Puente</a>, 
<a href="/search/cs?searchtype=author&query=Gandolfo%2C+D+C">Daniel C. Gandolfo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is under review at the journal "IEEE Robotics and Automation Letters"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The photovoltaic (PV) industry is seeing a significant shift toward
large-scale solar plants, where traditional inspection methods have proven to
be time-consuming and costly. Currently, the predominant approach to PV
inspection using unmanned aerial vehicles (UAVs) is based on photogrammetry.
However, the photogrammetry approach presents limitations, such as an increased
amount of useless data during flights, potential issues related to image
resolution, and the detection process during high-altitude flights. In this
work, we develop a visual servoing control system applied to a UAV with dynamic
compensation using a nonlinear model predictive control (NMPC) capable of
accurately tracking the middle of the underlying PV array at different frontal
velocities and height constraints, ensuring the acquisition of detailed images
during low-altitude flights. The visual servoing controller is based on the
extraction of features using RGB-D images and the Kalman filter to estimate the
edges of the PV arrays. Furthermore, this work demonstrates the proposal in
both simulated and real-world environments using the commercial aerial vehicle
(DJI Matrice 100), with the purpose of showcasing the results of the
architecture. Our approach is available for the scientific community in:
https://github.com/EPVelasco/VisualServoing_NMPC
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08022" title="Abstract">arXiv:2311.08022</a> [<a href="/pdf/2311.08022" title="Download PDF">pdf</a>, <a href="/ps/2311.08022" title="Download PostScript">ps</a>, <a href="/format/2311.08022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-Stage Predict+Optimize for Mixed Integer Linear Programs with  Unknown Parameters in Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+C+H">Jasper C.H. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+H+M">Jimmy H.M. Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Consider the setting of constrained optimization, with some parameters
unknown at solving time and requiring prediction from relevant features.
Predict+Optimize is a recent framework for end-to-end training supervised
learning models for such predictions, incorporating information about the
optimization problem in the training process in order to yield better
predictions in terms of the quality of the predicted solution under the true
parameters. Almost all prior works have focused on the special case where the
unknowns appear only in the optimization objective and not the constraints. Hu
et al.~proposed the first adaptation of Predict+Optimize to handle unknowns
appearing in constraints, but the framework has somewhat ad-hoc elements, and
they provided a training algorithm only for covering and packing linear
programs. In this work, we give a new \emph{simpler} and \emph{more powerful}
framework called \emph{Two-Stage Predict+Optimize}, which we believe should be
the canonical framework for the Predict+Optimize setting. We also give a
training algorithm usable for all mixed integer linear programs, vastly
generalizing the applicability of the framework. Experimental results
demonstrate the superior prediction performance of our training framework over
all classical and state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08027" title="Abstract">arXiv:2311.08027</a> [<a href="/pdf/2311.08027" title="Download PDF">pdf</a>, <a href="/format/2311.08027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A practical key-recovery attack on LWE-based key-encapsulation mechanism  schemes using Rowhammer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+P">Puja Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Suparna Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sarani Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Karmakar%2C+A">Angshuman Karmakar</a>, 
<a href="/search/cs?searchtype=author&query=Verbauwhede%2C+I">Ingrid Verbauwhede</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Physical attacks are serious threats to cryptosystems deployed in the real
world. In this work, we propose a microarchitectural end-to-end attack
methodology on generic lattice-based post-quantum key encapsulation mechanisms
to recover the long-term secret key. Our attack targets a critical component of
a Fujisaki-Okamoto transform that is used in the construction of almost all
lattice-based key encapsulation mechanisms. We demonstrate our attack model on
practical schemes such as Kyber and Saber by using Rowhammer. We show that our
attack is highly practical and imposes little preconditions on the attacker to
succeed. As an additional contribution, we propose an improved version of the
plaintext checking oracle, which is used by almost all physical attack
strategies on lattice-based key-encapsulation mechanisms. Our improvement
reduces the number of queries to the plaintext checking oracle by as much as
$39\%$ for Saber and approximately $23\%$ for Kyber768. This can be of
independent interest and can also be used to reduce the complexity of other
attacks.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08035" title="Abstract">arXiv:2311.08035</a> [<a href="/pdf/2311.08035" title="Download PDF">pdf</a>, <a href="/format/2311.08035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven building energy efficiency prediction based on envelope heat  losses using physics-informed neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michalakopoulos%2C+V">Vasilis Michalakopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Pelekis%2C+S">Sotiris Pelekis</a>, 
<a href="/search/cs?searchtype=author&query=Kormpakis%2C+G">Giorgos Kormpakis</a>, 
<a href="/search/cs?searchtype=author&query=Karakolis%2C+V">Vagelis Karakolis</a>, 
<a href="/search/cs?searchtype=author&query=Mouzakitis%2C+S">Spiros Mouzakitis</a>, 
<a href="/search/cs?searchtype=author&query=Askounis%2C+D">Dimitris Askounis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">The analytical prediction of building energy performance in residential
buildings based on the heat losses of its individual envelope components is a
challenging task. It is worth noting that this field is still in its infancy,
with relatively limited research conducted in this specific area to date,
especially when it comes for data-driven approaches. In this paper we introduce
a novel physics-informed neural network model for addressing this problem.
Through the employment of unexposed datasets that encompass general building
information, audited characteristics, and heating energy consumption, we feed
the deep learning model with general building information, while the model's
output consists of the structural components and several thermal properties
that are in fact the basic elements of an energy performance certificate (EPC).
On top of this neural network, a function, based on physics equations,
calculates the energy consumption of the building based on heat losses and
enhances the loss function of the deep learning model. This methodology is
tested on a real case study for 256 buildings located in Riga, Latvia. Our
investigation comes up with promising results in terms of prediction accuracy,
paving the way for automated, and data-driven energy efficiency performance
prediction based on basic properties of the building, contrary to exhaustive
energy efficiency audits led by humans, which are the current status quo.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08038" title="Abstract">arXiv:2311.08038</a> [<a href="/pdf/2311.08038" title="Download PDF">pdf</a>, <a href="/format/2311.08038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linking QKD testbeds across Europe
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brauer%2C+M">Max Brauer</a>, 
<a href="/search/cs?searchtype=author&query=Vicente%2C+R+J">Rafael J. Vicente</a>, 
<a href="/search/cs?searchtype=author&query=Buruaga-Brouns%2C+J+S">Jaime Saez Buruaga-Brouns</a>, 
<a href="/search/cs?searchtype=author&query=Mendez%2C+R+B">Ruben B. Mendez</a>, 
<a href="/search/cs?searchtype=author&query=Braun%2C+R">Ralf-Peter Braun</a>, 
<a href="/search/cs?searchtype=author&query=Geitz%2C+M">Marc Geitz</a>, 
<a href="/search/cs?searchtype=author&query=Rydlichkowski%2C+P">Piotr Rydlichkowski</a>, 
<a href="/search/cs?searchtype=author&query=Brunner%2C+H+H">Hans H. Brunner</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+F">Fred Fung</a>, 
<a href="/search/cs?searchtype=author&query=Peev%2C+M">Momtchil Peev</a>, 
<a href="/search/cs?searchtype=author&query=Pastor%2C+A">Antonio Pastor</a>, 
<a href="/search/cs?searchtype=author&query=Lopez%2C+D">Diego Lopez</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+V">Vicente Martin</a>, 
<a href="/search/cs?searchtype=author&query=Brito%2C+J+P">Juan P. Brito</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Quantum-key-distribution (QKD) networks are gaining importance and it has
become necessary to analyze the most appropriate methods for their
long-distance interconnection. In this paper, four different methods of
interconnecting remote QKD networks are proposed. The methods are used to link
three different QKD testbeds in Europe, located in Berlin, Madrid, and Poznan.
Although long-distance QKD links are only emulated, the used methods can serve
as a blueprint for a secure interconnection of distant QKD networks in the
future. Specifically, the presented approaches combine, in a transparent way,
different fiber and satellite physical media, as well as common standards of
key-delivery interfaces. The testbed interconnections are designed to increase
the security by utilizing multipath techniques and multiple hybridizations of
QKD and post quantum cryptography (PQC) algorithms.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08040" title="Abstract">arXiv:2311.08040</a> [<a href="/pdf/2311.08040" title="Download PDF">pdf</a>, <a href="/ps/2311.08040" title="Download PostScript">ps</a>, <a href="/format/2311.08040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Masking-Friendly Designs for Post-Quantum Cryptography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Suparna Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Karmakar%2C+A">Angshuman Karmakar</a>, 
<a href="/search/cs?searchtype=author&query=Verbauwhede%2C+I">Ingrid Verbauwhede</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Masking is a well-known and provably secure countermeasure against
side-channel attacks. However, due to additional redundant computations,
integrating masking schemes is expensive in terms of performance. The
performance overhead of integrating masking countermeasures is heavily
influenced by the design choices of a cryptographic algorithm and is often not
considered during the design phase.
<br />In this work, we deliberate on the effect of design choices on integrating
masking techniques into lattice-based cryptography. We select Scabbard, a suite
of three lattice-based post-quantum key-encapsulation mechanisms (KEM), namely
Florete, Espada, and Sable. We provide arbitrary-order masked implementations
of all the constituent KEMs of the Scabbard suite by exploiting their specific
design elements. We show that the masked implementations of Florete, Espada,
and Sable outperform the masked implementations of Kyber in terms of speed for
any order masking. Masked Florete exhibits a $73\%$, $71\%$, and $70\%$
performance improvement over masked Kyber corresponding to the first-, second-,
and third-order. Similarly, Espada exhibits $56\%$, $59\%$, and $60\%$ and
Sable exhibits $75\%$, $74\%$, and $73\%$ enhanced performance for first-,
second-, and third-order masking compared to Kyber respectively. Our results
show that the design decisions have a significant impact on the efficiency of
integrating masking countermeasures into lattice-based cryptography.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08041" title="Abstract">arXiv:2311.08041</a> [<a href="/pdf/2311.08041" title="Download PDF">pdf</a>, <a href="/format/2311.08041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ColorFloat: Constant space token coloring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zarick%2C+R">Ryan Zarick</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrino%2C+B">Bryan Pellegrino</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+I">Isaac Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Thomas Kim</a>, 
<a href="/search/cs?searchtype=author&query=Banister%2C+C">Caleb Banister</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We present ColorFloat, a family of O(1) space complexity algorithms that
solve the problem of attributing (coloring) fungible tokens to the entity that
minted them (minter). Tagging fungible tokens with metadata is not a new
problem and was first formalized in the Colored Coins protocol. In certain
contexts, practical solutions to this challenge have been implemented and
deployed such as NFT. We define the fungible token coloring problem, one
specific aspect of the Colored Coins problem, to be the problem of retaining
fungible characteristics of the underlying token while accurately tracking the
attribution of fungible tokens to their respective minters. Fungible token
coloring has a wide range of Web3 applications. One application which we
highlight in this paper is the onchain yield-sharing collateral-based
stablecoin.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08042" title="Abstract">arXiv:2311.08042</a> [<a href="/pdf/2311.08042" title="Download PDF">pdf</a>, <a href="/ps/2311.08042" title="Download PostScript">ps</a>, <a href="/format/2311.08042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Algorithms for Graph Coloring and other Partitioning, Covering,  and Packing Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaspers%2C+S">Serge Gaspers</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+Z">Jerry Zirui Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Let U be a universe on n elements, let k be a positive integer, and let F be
a family of (implicitly defined) subsets of U. We consider the problems of
partitioning U into k sets from F, covering U with k sets from F, and packing k
non-intersecting sets from F into U. Classically, these problems can be solved
via inclusion-exclusion in O*(2^n) time [BjorklundHK09]. Quantumly, there are
faster algorithms for graph coloring with running time O(1.9140^n) [ShimizuM22]
and for Set Cover with a small number of sets with running time O(1.7274^n
|F|^O(1)) [AmbainisBIKPV19]. In this paper, we give a quantum speedup for Set
Partition, Set Cover, and Set Packing whenever there is a classical enumeration
algorithm that lends itself to a quadratic quantum speedup, which, for any
subinstance on a subset X of U, enumerates at least one member of a
k-partition, k-cover, or k-packing (if one exists) restricted to (or projected
onto, in the case of k-cover) the set X in O*(c^{|X|}) time with c&lt;2.
<br />Our bounded-error quantum algorithm runs in O*((2+c)^(n/2)) for Set
Partition, Set Cover, and Set Packing. When c&lt;=1.147899, our algorithm is
slightly faster than O*((2+c)^(n/2)); when c approaches 1, it matches the
running time of [AmbainisBIKPV19] for Set Cover when |F| is subexponential in
n.
<br />For Graph Coloring, we further improve the running time to O(1.7956^n) by
leveraging faster algorithms for coloring with a small number of colors to
better balance our divide-and-conquer steps. For Domatic Number, we obtain a
O((2-\epsilon)^n) running time for some \epsilon&gt;0.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08043" title="Abstract">arXiv:2311.08043</a> [<a href="/pdf/2311.08043" title="Download PDF">pdf</a>, <a href="/format/2311.08043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Learning for Multi-Object Tracking with Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Plaen%2C+P">Pierre-Fran&#xe7;ois De Plaen</a>, 
<a href="/search/cs?searchtype=author&query=Marinello%2C+N">Nicola Marinello</a>, 
<a href="/search/cs?searchtype=author&query=Proesmans%2C+M">Marc Proesmans</a>, 
<a href="/search/cs?searchtype=author&query=Tuytelaars%2C+T">Tinne Tuytelaars</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The DEtection TRansformer (DETR) opened new possibilities for object
detection by modeling it as a translation task: converting image features into
object-level representations. Previous works typically add expensive modules to
DETR to perform Multi-Object Tracking (MOT), resulting in more complicated
architectures. We instead show how DETR can be turned into a MOT model by
employing an instance-level contrastive loss, a revised sampling strategy and a
lightweight assignment method. Our training scheme learns object appearances
while preserving detection capabilities and with little overhead. Its
performance surpasses the previous state-of-the-art by +2.6 mMOTA on the
challenging BDD100K dataset and is comparable to existing transformer-based
methods on the MOT17 dataset.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08045" title="Abstract">arXiv:2311.08045</a> [<a href="/pdf/2311.08045" title="Download PDF">pdf</a>, <a href="/format/2311.08045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Preference Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Pengyu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+N">Nan Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In process
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Human preference alignment is a crucial training step to improve the
interaction quality of large language models (LLMs). Existing aligning methods
depend on manually annotated preference data to guide the LLM optimization
directions. However, in practice, continuously updating LLMs raises a
distribution gap between model-generated samples and human-preferred responses,
which hinders model fine-tuning efficiency. To mitigate this issue, previous
methods require additional preference annotation on generated samples to adapt
the shifted distribution, which consumes a large amount of annotation
resources. Targeting more efficient human preference optimization, we propose
an adversarial preference optimization (APO) framework, where the LLM agent and
the preference model update alternatively via a min-max game. Without
additional annotation, our APO method can make a self-adaption to the
generation distribution gap through the adversarial learning process. In
experiments, we empirically verify the effectiveness of APO in improving LLM's
helpfulness and harmlessness compared with rejection sampling baselines.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08046" title="Abstract">arXiv:2311.08046</a> [<a href="/pdf/2311.08046" title="Download PDF">pdf</a>, <a href="/format/2311.08046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chat-UniVi: Unified Visual Representation Empowers Large Language Models  with Image and Video Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+P">Peng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Takanobu%2C+R">Ryuichi Takanobu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Caiwan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large language models have demonstrated impressive universal capabilities
across a wide range of open-ended tasks and have extended their utility to
encompass multimodal conversations. However, existing methods encounter
challenges in effectively handling both image and video understanding,
particularly with limited visual tokens. In this work, we introduce Chat-UniVi,
a unified vision-language model capable of comprehending and engaging in
conversations involving images and videos through a unified visual
representation. Specifically, we employ a set of dynamic visual tokens to
uniformly represent images and videos. This representation framework empowers
the model to efficiently utilize a limited number of visual tokens to
simultaneously capture the spatial details necessary for images and the
comprehensive temporal relationship required for videos. Moreover, we leverage
a multi-scale representation, enabling the model to perceive both high-level
semantic concepts and low-level visual details. Notably, Chat-UniVi is trained
on a mixed dataset containing both images and videos, allowing direct
application to tasks involving both mediums without requiring any
modifications. Extensive experimental results demonstrate that Chat-UniVi, as a
unified model, consistently outperforms even existing methods exclusively
designed for either images or videos.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08048" title="Abstract">arXiv:2311.08048</a> [<a href="/pdf/2311.08048" title="Download PDF">pdf</a>, <a href="/ps/2311.08048" title="Download PostScript">ps</a>, <a href="/format/2311.08048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyze business context data in developing economies using quantum  computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamshed%2C+A">Ammar Jamshed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Quantum computing is an advancing area of computing sciences and provides a
new base of development for many futuristic technologies discussions on how it
can help developing economies will further help developed economies in
technology transfer and economic development initiatives related to Research
and development within developing countries thus providing a new means of
foreign direct investment(FDI) and business innovation for the majority of the
globe that lacks infrastructure economic resources required for growth in the
technology landscape and cyberinfrastructure for growth in computing
applications. Discussion of which areas of support quantum computing can help
will further assist developing economies in implementing it for growth
opportunities for local systems and businesses.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08049" title="Abstract">arXiv:2311.08049</a> [<a href="/pdf/2311.08049" title="Download PDF">pdf</a>, <a href="/format/2311.08049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Reliable AI: Adequacy Metrics for Ensuring the Quality of  System-level Testing of Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neelofar%2C+N">Neelofar Neelofar</a>, 
<a href="/search/cs?searchtype=author&query=Aleti%2C+A">Aldeida Aleti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">AI-powered systems have gained widespread popularity in various domains,
including Autonomous Vehicles (AVs). However, ensuring their reliability and
safety is challenging due to their complex nature. Conventional test adequacy
metrics, designed to evaluate the effectiveness of traditional software
testing, are often insufficient or impractical for these systems. White-box
metrics, which are specifically designed for these systems, leverage neuron
coverage information. These coverage metrics necessitate access to the
underlying AI model and training data, which may not always be available.
Furthermore, the existing adequacy metrics exhibit weak correlations with the
ability to detect faults in the generated test suite, creating a gap that we
aim to bridge in this study.
<br />In this paper, we introduce a set of black-box test adequacy metrics called
"Test suite Instance Space Adequacy" (TISA) metrics, which can be used to gauge
the effectiveness of a test suite. The TISA metrics offer a way to assess both
the diversity and coverage of the test suite and the range of bugs detected
during testing. Additionally, we introduce a framework that permits testers to
visualise the diversity and coverage of the test suite in a two-dimensional
space, facilitating the identification of areas that require improvement.
<br />We evaluate the efficacy of the TISA metrics by examining their correlation
with the number of bugs detected in system-level simulation testing of AVs. A
strong correlation, coupled with the short computation time, indicates their
effectiveness and efficiency in estimating the adequacy of testing AVs.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08053" title="Abstract">arXiv:2311.08053</a> [<a href="/pdf/2311.08053" title="Download PDF">pdf</a>, <a href="/format/2311.08053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Constrained Bayesian Active Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Croisfelt%2C+V">Victor Croisfelt</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S+R">Shashi Raj Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>, 
<a href="/search/cs?searchtype=author&query=Popovski%2C+P">Petar Popovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, conference version, submitted to IEEE ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Consider an active learning setting in which a learner has a training set
with few labeled examples and a pool set with many unlabeled inputs, while a
remote teacher has a pre-trained model that is known to perform well for the
learner's task. The learner actively transmits batches of unlabeled inputs to
the teacher through a constrained communication channel for labeling. This
paper addresses the following key questions: (i) Active batch selection: Which
batch of inputs should be sent to the teacher to acquire the most useful
information and thus reduce the number of required communication rounds? (ii)
Batch encoding: How do we encode the batch of inputs for transmission to the
teacher to reduce the communication resources required at each round? We
introduce Communication-Constrained Bayesian Active Knowledge Distillation
(CC-BAKD), a novel protocol that integrates Bayesian active learning with
compression via a linear mix-up mechanism. Bayesian active learning selects the
batch of inputs based on their epistemic uncertainty, addressing the
"confirmation bias" that is known to increase the number of required
communication rounds. Furthermore, the proposed mix-up compression strategy is
integrated with the epistemic uncertainty-based active batch selection process
to reduce the communication overhead per communication round.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08057" title="Abstract">arXiv:2311.08057</a> [<a href="/pdf/2311.08057" title="Download PDF">pdf</a>, <a href="/ps/2311.08057" title="Download PostScript">ps</a>, <a href="/format/2311.08057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data and models for stance and premise detection in COVID-19 tweets:  insights from the Social Media Mining for Health (SMM4H) 2022 shared task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davydova%2C+V">Vera Davydova</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huabin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tutubalina%2C+E">Elena Tutubalina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is under review in the Journal of Biomedical Informatics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The COVID-19 pandemic has sparked numerous discussions on social media
platforms, with users sharing their views on topics such as mask-wearing and
vaccination. To facilitate the evaluation of neural models for stance detection
and premise classification, we organized the Social Media Mining for Health
(SMM4H) 2022 Shared Task 2. This competition utilized manually annotated posts
on three COVID-19-related topics: school closures, stay-at-home orders, and
wearing masks. In this paper, we extend the previous work and present newly
collected data on vaccination from Twitter to assess the performance of models
on a different topic. To enhance the accuracy and effectiveness of our
evaluation, we employed various strategies to aggregate tweet texts with
claims, including models with feature-level (early) fusion and dual-view
architectures from SMM4H 2022 leaderboard. Our primary objective was to create
a valuable dataset and perform an extensive experimental evaluation to support
future research in argument mining in the health domain.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08060" title="Abstract">arXiv:2311.08060</a> [<a href="/pdf/2311.08060" title="Download PDF">pdf</a>, <a href="/format/2311.08060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All Byzantine Agreement Problems are Expensive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Civit%2C+P">Pierre Civit</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+S">Seth Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Komatovic%2C+J">Jovan Komatovic</a>, 
<a href="/search/cs?searchtype=author&query=Paramonov%2C+A">Anton Paramonov</a>, 
<a href="/search/cs?searchtype=author&query=Vidigueira%2C+M">Manuel Vidigueira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Byzantine agreement, arguably the most fundamental problem in distributed
computing, operates among n processes, out of which t &lt; n can exhibit arbitrary
failures. The problem states that all correct (non-faulty) processes must
eventually decide (termination) the same value (agreement) from a set of
admissible values defined by the proposals of the processes (validity).
Depending on the exact version of the validity property, Byzantine agreement
comes in different forms, from Byzantine broadcast to strong and weak
consensus, to modern variants of the problem introduced in today's blockchain
systems. Regardless of the specific flavor of the agreement problem, its
communication cost is a fundamental metric whose improvement has been the focus
of decades of research. The Dolev-Reischuk bound, one of the most celebrated
results in distributed computing, proved 40 years ago that, at least for
Byzantine broadcast, no deterministic solution can do better than Omega(t^2)
exchanged messages in the worst case. Since then, it remained unknown whether
the quadratic lower bound extends to seemingly weaker variants of Byzantine
agreement. This paper answers the question in the affirmative, closing this
long-standing open problem. Namely, we prove that any non-trivial agreement
problem requires Omega(t^2) messages to be exchanged in the worst case. To
prove the general lower bound, we determine the weakest Byzantine agreement
problem and show, via a novel indistinguishability argument, that it incurs
Omega(t^2) exchanged messages.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08066" title="Abstract">arXiv:2311.08066</a> [<a href="/pdf/2311.08066" title="Download PDF">pdf</a>, <a href="/format/2311.08066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to get better embeddings with code pre-trained models? An empirical  study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+L">Lina Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yaoshen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiqiu Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Pre-trained language models have demonstrated powerful capabilities in the
field of natural language processing (NLP). Recently, code pre-trained model
(PTM), which draw from the experiences of the NLP field, have also achieved
state-of-the-art results in many software engineering (SE) downstream tasks.
These code PTMs take into account the differences between programming languages
and natural languages during pre-training and make adjustments to pre-training
tasks and input data. However, researchers in the SE community still inherit
habits from the NLP field when using these code PTMs to generate embeddings for
SE downstream classification tasks, such as generating semantic embeddings for
code snippets through special tokens and inputting code and text information in
the same way as pre-training the PTMs. In this paper, we empirically study five
different PTMs (i.e. CodeBERT, CodeT5, PLBART, CodeGPT and CodeGen) with three
different architectures (i.e. encoder-only, decoder-only and encoder-decoder)
on four SE downstream classification tasks (i.e. code vulnerability detection,
code clone detection, just-in-time defect prediction and function docstring
mismatch detection) with respect to the two aforementioned aspects. Our
experimental results indicate that (1) regardless of the architecture of the
code PTMs used, embeddings obtained through special tokens do not sufficiently
aggregate the semantic information of the entire code snippet; (2) the quality
of code embeddings obtained by combing code data and text data in the same way
as pre-training the PTMs is poor and cannot guarantee richer semantic
information; (3) using the method that aggregates the vector representations of
all code tokens, the decoder-only PTMs can obtain code embeddings with
semantics as rich as or even better quality than those obtained from the
encoder-only and encoder-decoder PTMs.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08071" title="Abstract">arXiv:2311.08071</a> [<a href="/pdf/2311.08071" title="Download PDF">pdf</a>, <a href="/format/2311.08071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PASDA: A Partition-based Semantic Differencing Approach with Best Effort  Classification of Undecided Cases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Glock%2C+J">Johann Glock</a>, 
<a href="/search/cs?searchtype=author&query=Pichler%2C+J">Josef Pichler</a>, 
<a href="/search/cs?searchtype=author&query=Pinzger%2C+M">Martin Pinzger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Equivalence checking is used to verify whether two programs produce
equivalent outputs when given equivalent inputs. Research in this field mainly
focused on improving equivalence checking accuracy and runtime performance.
However, for program pairs that cannot be proven to be either equivalent or
non-equivalent, existing approaches only report a classification result of
"unknown", which provides no information regarding the programs'
non-/equivalence.
<br />In this paper, we introduce PASDA, our partition-based semantic differencing
approach with best effort classification of undecided cases. While PASDA aims
to formally prove non-/equivalence of analyzed program pairs using a variant of
differential symbolic execution, its main novelty lies in its handling of cases
for which no formal non-/equivalence proof can be found. For such cases, PASDA
provides a best effort equivalence classification based on a set of
classification heuristics.
<br />We evaluated PASDA with an existing benchmark consisting of 141
non-/equivalent program pairs. PASDA correctly classified 61-74% of these cases
at timeouts from 10 seconds to 3600 seconds. Thus, PASDA achieved equivalence
checking accuracies that are 3-7% higher than the best results achieved by
three existing tools. Furthermore, PASDA's best effort classifications were
correct for 70-75% of equivalent and 55-85% of non-equivalent cases across the
different timeouts.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08074" title="Abstract">arXiv:2311.08074</a> [<a href="/pdf/2311.08074" title="Download PDF">pdf</a>, <a href="/format/2311.08074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content-Adaptive Variable Framerate Encoding Scheme for Green Live  Streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Menon%2C+V+V">Vignesh V Menon</a>, 
<a href="/search/cs?searchtype=author&query=Afzal%2C+S">Samira Afzal</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+P+T">Prajit T Rajendran</a>, 
<a href="/search/cs?searchtype=author&query=Schoeffmann%2C+K">Klaus Schoeffmann</a>, 
<a href="/search/cs?searchtype=author&query=Prodan%2C+R">Radu Prodan</a>, 
<a href="/search/cs?searchtype=author&query=Timmerer%2C+C">Christian Timmerer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">Adaptive live video streaming applications use a fixed predefined
configuration for the bitrate ladder with constant framerate and encoding
presets in a session. However, selecting optimized framerates and presets for
every bitrate ladder representation can enhance perceptual quality, improve
computational resource allocation, and thus, the streaming energy efficiency.
In particular, low framerates for low-bitrate representations reduce
compression artifacts and decrease encoding energy consumption. In addition, an
optimized preset may lead to improved compression efficiency. To this light,
this paper proposes a Content-adaptive Variable Framerate (CVFR) encoding
scheme, which offers two modes of operation: ecological (ECO) and high-quality
(HQ). CVFR-ECO optimizes for the highest encoding energy savings by predicting
the optimized framerate for each representation in the bitrate ladder. CVFR-HQ
takes it further by predicting each representation's optimized
framerate-encoding preset pair using low-complexity discrete cosine transform
energy-based spatial and temporal features for compression efficiency and
sustainable storage. We demonstrate the advantage of CVFR using the x264
open-source video encoder. The results show that CVFR-ECO yields an average
PSNR and VMAF increase of 0.02 dB and 2.50 points, respectively, for the same
bitrate, compared to the fastest preset highest framerate encoding. CVFR-ECO
also yields an average encoding and storage energy consumption reduction of
34.54% and 76.24%, considering a just noticeable difference (JND) of six VMAF
points. In comparison, CVFR-HQ yields an average increase in PSNR and VMAF of
2.43 dB and 10.14 points, respectively, for the same bitrate. Finally, CVFR-HQ
resulted in an average reduction in storage energy consumption of 83.18%,
considering a JND of six VMAF points.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08077" title="Abstract">arXiv:2311.08077</a> [<a href="/pdf/2311.08077" title="Download PDF">pdf</a>, <a href="/format/2311.08077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Segmentation of Eye Features Using the Segment Anything Model  (SAM)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maquiling%2C+V">Virmarie Maquiling</a>, 
<a href="/search/cs?searchtype=author&query=Byrne%2C+S+A">Sean Anthony Byrne</a>, 
<a href="/search/cs?searchtype=author&query=Niehorster%2C+D+C">Diederick C. Niehorster</a>, 
<a href="/search/cs?searchtype=author&query=Nystr%C3%B6m%2C+M">Marcus Nystr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+E">Enkelejda Kasneci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures, 1 table, submitted to ETRA 2024: ACM Symposium on Eye Tracking Research &amp; Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The advent of foundation models signals a new era in artificial intelligence.
The Segment Anything Model (SAM) is the first foundation model for image
segmentation. In this study, we evaluate SAM's ability to segment features from
eye images recorded in virtual reality setups. The increasing requirement for
annotated eye-image datasets presents a significant opportunity for SAM to
redefine the landscape of data annotation in gaze estimation. Our investigation
centers on SAM's zero-shot learning abilities and the effectiveness of prompts
like bounding boxes or point clicks. Our results are consistent with studies in
other domains, demonstrating that SAM's segmentation effectiveness can be
on-par with specialized models depending on the feature, with prompts improving
its performance, evidenced by an IoU of 93.34% for pupil segmentation in one
dataset. Foundation models like SAM could revolutionize gaze estimation by
enabling quick and easy image segmentation, reducing reliance on specialized
models and extensive manual annotation.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08083" title="Abstract">arXiv:2311.08083</a> [<a href="/pdf/2311.08083" title="Download PDF">pdf</a>, <a href="/format/2311.08083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving ARC visual analogies with neural embeddings and vector  arithmetic: A generalized method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thoms%2C+L+H">Luca H. Thoms</a>, 
<a href="/search/cs?searchtype=author&query=Veldkamp%2C+K+A">Karel A. Veldkamp</a>, 
<a href="/search/cs?searchtype=author&query=Rosenbusch%2C+H">Hannes Rosenbusch</a>, 
<a href="/search/cs?searchtype=author&query=Stevenson%2C+C+E">Claire E. Stevenson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Data and code can be found on <a href="https://github.com/foger3/ARC_DeepLearning">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Analogical reasoning derives information from known relations and generalizes
this information to similar yet unfamiliar situations. One of the first
generalized ways in which deep learning models were able to solve verbal
analogies was through vector arithmetic of word embeddings, essentially
relating words that were mapped to a vector space (e.g., king - man + woman =
__?). In comparison, most attempts to solve visual analogies are still
predominantly task-specific and less generalizable. This project focuses on
visual analogical reasoning and applies the initial generalized mechanism used
to solve verbal analogies to the visual realm. Taking the Abstraction and
Reasoning Corpus (ARC) as an example to investigate visual analogy solving, we
use a variational autoencoder (VAE) to transform ARC items into low-dimensional
latent vectors, analogous to the word embeddings used in the verbal approaches.
Through simple vector arithmetic, underlying rules of ARC items are discovered
and used to solve them. Results indicate that the approach works well on simple
items with fewer dimensions (i.e., few colors used, uniform shapes), similar
input-to-output examples, and high reconstruction accuracy on the VAE.
Predictions on more complex items showed stronger deviations from expected
outputs, although, predictions still often approximated parts of the item's
rule set. Error patterns indicated that the model works as intended. On the
official ARC paradigm, the model achieved a score of 2% (cf. current world
record is 21%) and on ConceptARC it scored 8.8%. Although the methodology
proposed involves basic dimensionality reduction techniques and standard vector
arithmetic, this approach demonstrates promising outcomes on ARC and can easily
be generalized to other abstract visual reasoning tasks.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08085" title="Abstract">arXiv:2311.08085</a> [<a href="/pdf/2311.08085" title="Download PDF">pdf</a>, <a href="/format/2311.08085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Electric Vehicle Efficiency with Real-Time Telemetry using  Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rao%2C+A">Aryaman Rao</a>, 
<a href="/search/eess?searchtype=author&query=Gupta%2C+H">Harshit Gupta</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+P">Parth Singh</a>, 
<a href="/search/eess?searchtype=author&query=Mittal%2C+S">Shivam Mittal</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+U">Utkrash Singh</a>, 
<a href="/search/eess?searchtype=author&query=Vishwakarma%2C+D+K">Dinesh Kumar Vishwakarma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In the contemporary world with degrading natural resources, the urgency of
energy efficiency has become imperative due to the conservation and
environmental safeguarding. Therefore, it's crucial to look for advanced
technology to minimize energy consumption. This research focuses on the
optimization of battery-electric city style vehicles through the use of a
real-time in-car telemetry system that communicates between components through
the robust Controller Area Network (CAN) protocol. By harnessing real-time data
from various sensors embedded within vehicles, our driving assistance system
provides the driver with visual and haptic actionable feedback that guides the
driver on using the optimum driving style to minimize power consumed by the
vehicle. To develop the pace feedback mechanism for the driver, real-time data
is collected through a Shell Eco Marathon Urban Concept vehicle platform and
after pre-processing, it is analyzed using the novel machine learning algorithm
TEMSL, that outperforms the existing baseline approaches across various
performance metrics. This innovative method after numerous experimentation has
proven effective in enhancing energy efficiency, guiding the driver along the
track, and reducing human errors. The driving-assistance system offers a range
of utilities, from cost savings and extended vehicle lifespan to significant
contributions to environmental conservation and sustainable driving practices.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08086" title="Abstract">arXiv:2311.08086</a> [<a href="/pdf/2311.08086" title="Download PDF">pdf</a>, <a href="/ps/2311.08086" title="Download PostScript">ps</a>, <a href="/format/2311.08086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPSOR-GCN: A Vehicle Trajectory Prediction Method Powered by Emotion and  Cognitive Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">L. Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Y. Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">J. Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+A">A. Fu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">J. Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 31 figures, submitted to IEEE Transactions on Intelligent Vehicles
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Active safety systems on vehicles often face problems with false alarms. Most
active safety systems predict the driver's trajectory with the assumption that
the driver is always in a normal emotion, and then infer risks. However, the
driver's trajectory uncertainty increases under abnormal emotions. This paper
proposes a new trajectory prediction model: CPSOR-GCN, which predicts vehicle
trajectories under abnormal emotions. At the physical level, the interaction
features between vehicles are extracted by the physical GCN module. At the
cognitive level, SOR cognitive theory is used as prior knowledge to build a
Dynamic Bayesian Network (DBN) structure. The conditional probability and state
transition probability of nodes from the calibrated SOR-DBN quantify the causal
relationship between cognitive factors, which is embedded into the cognitive
GCN module to extract the characteristics of the influence mechanism of
emotions on driving behavior. The CARLA-SUMO joint driving simulation platform
was built to develop dangerous pre-crash scenarios. Methods of recreating
traffic scenes were used to naturally induce abnormal emotions. The experiment
collected data from 26 participants to verify the proposed model. Compared with
the model that only considers physical motion features, the prediction accuracy
of the proposed model is increased by 68.70%. Furthermore,considering the
SOR-DBN reduces the prediction error of the trajectory by 15.93%. Compared with
other advanced trajectory prediction models, the results of CPSOR-GCN also have
lower errors. This model can be integrated into active safety systems to better
adapt to the driver's emotions, which could effectively reduce false alarms.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08089" title="Abstract">arXiv:2311.08089</a> [<a href="/pdf/2311.08089" title="Download PDF">pdf</a>, <a href="/format/2311.08089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Align after Pre-train: Improving Multilingual Generative Models with  Cross-lingual Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaonan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+C">Chengqing Zong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multilingual generative models obtain remarkable cross-lingual capabilities
through pre-training on large-scale corpora. However, they still exhibit a
performance bias toward high-resource languages, and learn isolated
distributions of sentence representations across languages. To bridge this gap,
we propose a simple yet effective alignment framework exploiting pairs of
translation sentences. It aligns the internal sentence representations across
different languages via multilingual contrastive learning and aligns model
outputs by answering prompts in different languages. Experimental results
demonstrate that even with less than 0.1 {\textperthousand} of pre-training
tokens, our alignment framework significantly boosts the cross-lingual
abilities of generative models and mitigates the performance gap. Further
analysis reveals that it results in a better internal multilingual
representation distribution of multilingual models.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08091" title="Abstract">arXiv:2311.08091</a> [<a href="/pdf/2311.08091" title="Download PDF">pdf</a>, <a href="/ps/2311.08091" title="Download PostScript">ps</a>, <a href="/format/2311.08091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lumiere: Making Optimal BFT for Partial Synchrony Practical
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lewis-Pye%2C+A">Andrew Lewis-Pye</a>, 
<a href="/search/cs?searchtype=author&query=Malkhi%2C+D">Dahlia Malkhi</a>, 
<a href="/search/cs?searchtype=author&query=Naor%2C+O">Oded Naor</a>, 
<a href="/search/cs?searchtype=author&query=Nayak%2C+K">Kartik Nayak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The view synchronization problem lies at the heart of many Byzantine Fault
Tolerant (BFT) State Machine Replication (SMR) protocols in the partial
synchrony model, since these protocols are usually based on views. Liveness is
guaranteed if honest processors spend a sufficiently long time in the same view
during periods of synchrony, and if the leader of the view is honest. Ensuring
that these conditions occur, known as Byzantine View Synchronization (BVS), has
turned out to be the performance bottleneck of many BFT SMR protocols.
<br />A recent line of work has shown that, by using an appropriate view
synchronization protocol, BFT SMR protocols can achieve $O(n^2)$ communication
complexity in the worst case after GST, thereby finally matching the lower
bound established by Dolev and Reischuk in 1985. However, these protocols
suffer from two major issues:
<br />(1) When implemented so as to be optimistically responsive, even a single
Byzantine processor may infinitely often cause $\Omega(n\Delta)$ latency
between consecutive consensus decisions.
<br />(2) Even in the absence of Byzantine action, infinitely many views require
honest processors to send $\Omega(n^2)$ messages.
<br />Here, we present Lumiere, an optimistically responsive BVS protocol which
maintains optimal worst-case communication complexity while simultaneously
addressing the two issues above: for the first time, Lumiere enables BFT
consensus solutions in the partial synchrony setting that have $O(n^2)$
worst-case communication complexity, and that eventually always (i.e., except
for a small constant number of "warmup" decisions) have communication
complexity and latency which is linear in the number of actual faults in the
execution.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08093" title="Abstract">arXiv:2311.08093</a> [<a href="/pdf/2311.08093" title="Download PDF">pdf</a>, <a href="/ps/2311.08093" title="Download PostScript">ps</a>, <a href="/format/2311.08093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spot: A Natural Language Interface for Geospatial Searches in OSM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khellaf%2C+L">Lynn Khellaf</a>, 
<a href="/search/cs?searchtype=author&query=Schlicht%2C+I+B">Ipek Baris Schlicht</a>, 
<a href="/search/cs?searchtype=author&query=Bayer%2C+J">Julia Bayer</a>, 
<a href="/search/cs?searchtype=author&query=Bouwmeester%2C+R">Ruben Bouwmeester</a>, 
<a href="/search/cs?searchtype=author&query=Mira%C3%9F%2C+T">Tilman Mira&#xdf;</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+T">Tilman Wagner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the Proceedings of the OSM Science 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Investigative journalists and fact-checkers have found OpenStreetMap (OSM) to
be an invaluable resource for their work due to its extensive coverage and
intricate details of various locations, which play a crucial role in
investigating news scenes. Despite its value, OSM's complexity presents
considerable accessibility and usability challenges, especially for those
without a technical background. To address this, we introduce 'Spot', a
user-friendly natural language interface for querying OSM data. Spot utilizes a
semantic mapping from natural language to OSM tags, leveraging artificially
generated sentence queries and a T5 transformer. This approach enables Spot to
extract relevant information from user-input sentences and display candidate
locations matching the descriptions on a map. To foster collaboration and
future advancement, all code and generated data is available as an open-source
repository.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08094" title="Abstract">arXiv:2311.08094</a> [<a href="/pdf/2311.08094" title="Download PDF">pdf</a>, <a href="/ps/2311.08094" title="Download PostScript">ps</a>, <a href="/format/2311.08094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Act-VIT: A Representationally Robust Attention Architecture for Skeleton  Based Action Recognition Using Vision Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karadag%2C+O+O">Ozge Oztimur Karadag</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Skeleton-based action recognition receives the attention of many researchers
as it is robust to viewpoint and illumination changes, and its processing is
much more efficient than video frames. With the emergence of deep learning
models, it has become very popular to represent the skeleton data in
pseudo-image form and apply Convolutional Neural Networks for action
recognition. Thereafter, studies concentrated on finding effective methods for
forming pseudo-images. Recently, attention networks, more specifically
transformers have provided promising results in various vision problems. In
this study, the effectiveness of vision transformers for skeleton-based action
recognition is examined and its robustness on the pseudo-image representation
scheme is investigated. To this end, a three-level architecture, Act-VIT is
proposed, which forms a set of pseudo images apply a classifier on each of the
representation and combine their results to find the final action class. The
classifiers of Act-VIT are first realized by CNNs and then by VITs and their
performances are compared. Experimental studies reveal that the vision
transformer is less sensitive to the initial pseudo-image representation
compared to CNN. Nevertheless, even with the vision transformer, the
recognition performance can be further improved by consensus of classifiers.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08096" title="Abstract">arXiv:2311.08096</a> [<a href="/pdf/2311.08096" title="Download PDF">pdf</a>, <a href="/format/2311.08096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Static Analysis: An IDE for RTLola
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Finkbeiner%2C+B">Bernd Finkbeiner</a>, 
<a href="/search/cs?searchtype=author&query=Kohn%2C+F">Florian Kohn</a>, 
<a href="/search/cs?searchtype=author&query=Schledjewski%2C+M">Malte Schledjewski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Runtime monitoring is an essential part of guaranteeing the safety of
cyber-physical systems. Recently, runtime monitoring frameworks based on formal
specification languages gained momentum. These languages provide valuable
abstractions for specifying the behavior of a system. Yet, writing
specifications remains challenging as, among other things, the specifier has to
keep track of the timing behavior of streams. This paper presents the RTLola
Playground, a browser-based development environment for the stream-based
runtime monitoring framework RTLola. It features new methods to explore the
static analysis results of RTLola, leveraging the advantages of such a formal
language to support the developer in writing and understanding specifications.
Specifications are executed locally in the browser, plotting the resulting
stream values, allowing for intuitive testing. Step-wise execution based on
user-provided system traces enables the debugging of identified errors.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08097" title="Abstract">arXiv:2311.08097</a> [<a href="/pdf/2311.08097" title="Download PDF">pdf</a>, <a href="/format/2311.08097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ranaldi%2C+L">Leonardo Ranaldi</a>, 
<a href="/search/cs?searchtype=author&query=Zanzotto%2C+F+M">Fabio Massimo Zanzotto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Chain-of-Thought (CoT) prompting empowers the reasoning abilities of Large
Language Models (LLMs), eliciting them to solve complex reasoning tasks
step-by-step. However, with the success of CoT methods, the ability to deliver
multi-step reasoning remains limited to English due to the imbalance in the
distribution of the pre-training data, making the other languages a barrier.
<br />In this work, we propose a Cross-lingual multi-step reasoning approach,
aiming to align reasoning processes across different languages. In particular,
our method, through a Self-consistent Cross-lingual prompting mechanism
inspired by the Tree-of-Thoughts approach, delivers multi-step reasoning paths
in different languages that, during the steps, lead to the final solution. Our
experimental evaluations show that our method significantly outperforms
existing prompting methods, reducing the number of interactions and achieving
state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08100" title="Abstract">arXiv:2311.08100</a> [<a href="/pdf/2311.08100" title="Download PDF">pdf</a>, <a href="/format/2311.08100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepEMplanner: An EM Motion Planner with Iterative Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhili Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Maosheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shuangjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+T">Tongyi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Motion planning is a computational problem that finds a sequence of valid
trajectories, often based on surrounding agents' forecasting, environmental
understanding, and historical and future contexts. It can also be viewed as a
game in which agents continuously plan their next move according to other
agents' intentions and the encountering environment, further achieving their
ultimate goals through incremental actions. To model the dynamic planning and
interaction process, we propose a novel framework, DeepEMplanner, which takes
the stepwise interaction into account for fine-grained behavior learning. The
ego vehicle maximizes each step motion to reach its eventual driving outcome
based on the stepwise expectation from agents and its upcoming road conditions.
On the other hand, the agents also follow the same philosophy to maximize their
stepwise behavior under the encountering environment and the expectations from
ego and other agents. Our DeepEMplanner models the interactions among ego,
agents, and the dynamic environment in an autoregressive manner by interleaving
the Expectation and Maximization processes. Further, we design ego-to-agents,
ego-to-map, and ego-to-BEV interaction mechanisms with hierarchical dynamic key
objects attention to better model the interactions. Experiments on the nuScenes
benchmark show that our approach achieves state-of-the-art results.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08103" title="Abstract">arXiv:2311.08103</a> [<a href="/pdf/2311.08103" title="Download PDF">pdf</a>, <a href="/format/2311.08103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Semi-supervised Hierarchical Stacked Encoder for Legal  Judgement Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prasad%2C+N">Nishchal Prasad</a>, 
<a href="/search/cs?searchtype=author&query=Boughanem%2C+M">Mohand Boughanem</a>, 
<a href="/search/cs?searchtype=author&query=Dkaki%2C+T">Taoufiq Dkaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the 1st International Workshop on Legal Information Retrieval at ECIR 2023, April 2nd 2023, Dublin, Ireland. (<a href="https://tmr.liacs.nl/legalIR/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Predicting the judgment of a legal case from its unannotated case facts is a
challenging task. The lengthy and non-uniform document structure poses an even
greater challenge in extracting information for decision prediction. In this
work, we explore and propose a two-level classification mechanism; both
supervised and unsupervised; by using domain-specific pre-trained BERT to
extract information from long documents in terms of sentence embeddings further
processing with transformer encoder layer and use unsupervised clustering to
extract hidden labels from these embeddings to better predict a judgment of a
legal case. We conduct several experiments with this mechanism and see higher
performance gains than the previously proposed methods on the ILDC dataset. Our
experimental results also show the importance of domain-specific pre-training
of Transformer Encoders in legal information processing.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08104" title="Abstract">arXiv:2311.08104</a> [<a href="/pdf/2311.08104" title="Download PDF">pdf</a>, <a href="/format/2311.08104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice  Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bargum%2C+A+R">Anders R. Bargum</a>, 
<a href="/search/cs?searchtype=author&query=Serafin%2C+S">Stefania Serafin</a>, 
<a href="/search/cs?searchtype=author&query=Erkut%2C+C">Cumhur Erkut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Research on deep learning-powered voice conversion (VC) in speech-to-speech
scenarios is getting increasingly popular. Although many of the works in the
field of voice conversion share a common global pipeline, there is a
considerable diversity in the underlying structures, methods, and neural
sub-blocks used across research efforts. Thus, obtaining a comprehensive
understanding of the reasons behind the choice of the different methods in the
voice conversion pipeline can be challenging, and the actual hurdles in the
proposed solutions are often unclear. To shed light on these aspects, this
paper presents a scoping review that explores the use of deep learning in
speech analysis, synthesis, and disentangled speech representation learning
within modern voice conversion systems. We screened 621 publications from more
than 38 different venues between the years 2017 and 2023, followed by an
in-depth review of a final database consisting of 123 eligible studies. Based
on the review, we summarise the most frequently used approaches to voice
conversion based on deep learning and highlight common pitfalls within the
community. Lastly, we condense the knowledge gathered, identify main challenges
and provide recommendations for future research directions.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08105" title="Abstract">arXiv:2311.08105</a> [<a href="/pdf/2311.08105" title="Download PDF">pdf</a>, <a href="/format/2311.08105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiLoCo: Distributed Low-Communication Training of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Douillard%2C+A">Arthur Douillard</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Q">Qixuan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Rusu%2C+A+A">Andrei A. Rusu</a>, 
<a href="/search/cs?searchtype=author&query=Chhaparia%2C+R">Rachita Chhaparia</a>, 
<a href="/search/cs?searchtype=author&query=Donchev%2C+Y">Yani Donchev</a>, 
<a href="/search/cs?searchtype=author&query=Kuncoro%2C+A">Adhiguna Kuncoro</a>, 
<a href="/search/cs?searchtype=author&query=Ranzato%2C+M">Marc&#x27;Aurelio Ranzato</a>, 
<a href="/search/cs?searchtype=author&query=Szlam%2C+A">Arthur Szlam</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiajun Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLM) have become a critical component in many
applications of machine learning. However, standard approaches to training LLM
require a large number of tightly interconnected accelerators, with devices
exchanging gradients and other intermediate states at each optimization step.
While it is difficult to build and maintain a single computing cluster hosting
many accelerators, it might be easier to find several computing clusters each
hosting a smaller number of devices. In this work, we propose a distributed
optimization algorithm, Distributed Low-Communication (DiLoCo), that enables
training of language models on islands of devices that are poorly connected.
The approach is a variant of federated averaging, where the number of inner
steps is large, the inner optimizer is AdamW, and the outer optimizer is
Nesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8
workers performs as well as fully synchronous optimization while communicating
500 times less. DiLoCo exhibits great robustness to the data distribution of
each worker. It is also robust to resources becoming unavailable over time, and
vice versa, it can seamlessly leverage resources that become available during
training.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08106" title="Abstract">arXiv:2311.08106</a> [<a href="/pdf/2311.08106" title="Download PDF">pdf</a>, <a href="/format/2311.08106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yujin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Seonghyeon Ye</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures, 5 tables; accepted at NeurIPS Syntheticdata4ML workshop, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In an ever-evolving world, the dynamic nature of knowledge presents
challenges for language models that are trained on static data, leading to
outdated encoded information. However, real-world scenarios require models not
only to acquire new knowledge but also to overwrite outdated information into
updated ones. To address this under-explored issue, we introduce the temporally
evolving question answering benchmark, EvolvingQA - a novel benchmark designed
for training and evaluating LMs on an evolving Wikipedia database, where the
construction of our benchmark is automated with our pipeline using large
language models. Our benchmark incorporates question-answering as a downstream
task to emulate real-world applications. Through EvolvingQA, we uncover that
existing continual learning baselines have difficulty in updating and
forgetting outdated knowledge. Our findings suggest that the models fail to
learn updated knowledge due to the small weight gradient. Furthermore, we
elucidate that the models struggle mostly on providing numerical or temporal
answers to questions asking for updated knowledge. Our work aims to model the
dynamic nature of real-world information, offering a robust measure for the
evolution-adaptability of language models.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08107" title="Abstract">arXiv:2311.08107</a> [<a href="/pdf/2311.08107" title="Download PDF">pdf</a>, <a href="/format/2311.08107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAIE Framework: Support Alone Isn&#x27;t Enough -- Advancing LLM Training  with Adversarial Remarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loem%2C+M">Mengsay Loem</a>, 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+M">Masahiro Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) can justify or criticize their predictions
through discussion with other models or humans, thereby enhancing their
intrinsic understanding of instances. While proactive discussions enhance
performance, this approach is currently limited to the inference phase. In this
context, we posit a hypothesis: learning interactive discussions during
training can improve understanding for the instances in the training step and
proficiency in logical/critical thinking ability and verbalized expression of
the model in the inference step. Our proposed SAIE training method involves
both supportive and adversarial discussions between the learner and partner
models. The learner model receives a remark from the partner through the
discussion, and the parameters of the learner model are then updated based on
this remark. That is, the teacher signal dynamically adjusts in response to the
evolving model output throughout the training step. By bolstering the capacity
for discussion and comprehension of instances, our experiments across datasets,
including GSM8K, CommonsenseQA, and MMLU, reveal that models fine-tuned with
our method consistently surpass those trained with standard fine-tuning
techniques. Moreover, our approach demonstrates superior performance in
multi-agent inference scenarios, boosting the models' reasoning abilities at
the inference step.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08110" title="Abstract">arXiv:2311.08110</a> [<a href="/pdf/2311.08110" title="Download PDF">pdf</a>, <a href="/format/2311.08110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving hateful memes detection via learning hatefulness-aware  embedding space through retrieval-guided contrastive learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jingbiao Mei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinghong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weizhe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Byrne%2C+B">Bill Byrne</a>, 
<a href="/search/cs?searchtype=author&query=Tomalin%2C+M">Marcus Tomalin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Hateful memes have emerged as a significant concern on the Internet. These
memes, which are a combination of image and text, often convey messages vastly
different from their individual meanings. Thus, detecting hateful memes
requires the system to jointly understand the visual and textual modalities.
However, our investigation reveals that the embedding space of existing
CLIP-based systems lacks sensitivity to subtle differences in memes that are
vital for correct hatefulness classification. To address this issue, we propose
constructing a hatefulness-aware embedding space through retrieval-guided
contrastive training. Specifically, we add an auxiliary loss that utilizes hard
negative and pseudo-gold samples to train the embedding space. Our approach
achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC
of 86.7. Notably, our approach outperforms much larger fine-tuned Large
Multimodal Models like Flamingo and LLaVA. Finally, we demonstrate a
retrieval-based hateful memes detection system, which is capable of making
hatefulness classification based on data unseen in training from a database.
This allows developers to update the hateful memes detection system by simply
adding new data without retraining, a desirable feature for real services in
the constantly-evolving landscape of hateful memes on the Internet.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08111" title="Abstract">arXiv:2311.08111</a> [<a href="/pdf/2311.08111" title="Download PDF">pdf</a>, <a href="/ps/2311.08111" title="Download PostScript">ps</a>, <a href="/format/2311.08111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Time-Dependent Traveling Salesman Problem with Time Windows  under Generic Time-Dependent Travel Cost
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vu%2C+D+M">Duc Minh Vu</a>, 
<a href="/search/cs?searchtype=author&query=Hewitt%2C+M">Mike Hewitt</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+D+D">Duc Duy Vu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version with Appendix - accepted to appear in the proceeding of SCONET2003 conference - <a href="https://csonet-conf.github.io/csonet23/index.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">In this paper, we present formulations and an exact method to solve the Time
Dependent Traveling Salesman Problem with Time Window (TD-TSPTW) under a
generic travel cost function where waiting is allowed. A particular case in
which the travel cost is a non-decreasing function has been addressed recently.
With that assumption, because of both the First-In-First-Out property of the
travel time function and the non-decreasing property of the travel cost
function, we can ignore the possibility of waiting. However, for generic travel
cost functions, waiting after visiting some locations can be part of optimal
solutions. To handle the general case, we introduce new lower-bound
formulations that allow us to ensure the existence of optimal solutions. We
adapt the existing algorithm for TD-TSPTW with non-decreasing travel costs to
solve the TD-TSPTW with generic travel costs. In the experiment, we evaluate
the strength of the proposed lower bound formulations and algorithm by applying
them to solve the TD-TSPTW with the total travel time objective. The results
indicate that the proposed algorithm is competitive with and even outperforms
the state-of-art solver in various benchmark instances.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08112" title="Abstract">arXiv:2311.08112</a> [<a href="/pdf/2311.08112" title="Download PDF">pdf</a>, <a href="/format/2311.08112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Surface for Physical Layer Security in  6G-IoT: Designs, Issues, and Advances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalid%2C+W">Waqas Khalid</a>, 
<a href="/search/cs?searchtype=author&query=Rehman%2C+M+A+U">M. Atif Ur Rehman</a>, 
<a href="/search/cs?searchtype=author&query=Van+Chien%2C+T">Trinh Van Chien</a>, 
<a href="/search/cs?searchtype=author&query=Kaleem%2C+Z">Zeeshan Kaleem</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Howon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Heejung Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for IEEE Internet of Things Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Sixth-generation (6G) networks pose substantial security risks because
confidential information is transmitted over wireless channels with a broadcast
nature, and various attack vectors emerge. Physical layer security (PLS)
exploits the dynamic characteristics of wireless environments to provide secure
communications, while reconfigurable intelligent surfaces (RISs) can facilitate
PLS by controlling wireless transmissions. With RIS-aided PLS, a lightweight
security solution can be designed for low-end Internet of Things (IoT) devices,
depending on the design scenario and communication objective. This article
discusses RIS-aided PLS designs for 6G-IoT networks against eavesdropping and
jamming attacks. The theoretical background and literature review of RIS-aided
PLS are discussed, and design solutions related to resource allocation,
beamforming, artificial noise, and cooperative communication are presented. We
provide simulation results to show the effectiveness of RIS in terms of PLS. In
addition, we examine the research issues and possible solutions for RIS
modeling, channel modeling and estimation, optimization, and machine learning.
Finally, we discuss recent advances, including STAR-RIS and malicious RIS.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08116" title="Abstract">arXiv:2311.08116</a> [<a href="/pdf/2311.08116" title="Download PDF">pdf</a>, <a href="/format/2311.08116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Skin separation control using distributed-input  distributed-output, multi-modal actuators, and machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Songqi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Efficient flow separation control represents significant economic benefit.
This study applies a machine learning algorithm to minimize flow separation in
Smart Skin, a flow control device that features distributed-input and
distributed-output (DIDO). Smart Skin comprises 30 hybrid actuator units, each
integrating a height-adjustable vortex generator and a mini-jet actuator. These
units are deployed on a backward-facing ramp to reduce flow separation in a
distributed manner. To monitor the flow state, distributed pressure taps are
deployed around the multi-modal actuators. Parametric studies indicate that the
mapping between control parameters and separation control performance is
complex. To optimize separation control, a cutting-edge variant of the particle
swarm optimization (PSO-TPME) is used for the control parameters in the Smart
Skin. This algorithm is capable of achieving fast optimization in
high-dimensional parameter spaces. The results demonstrate the efficiency of
PSO-TPME, and the optimized solution significantly outperforms the best result
from the parametric study. These findings represent a promising future of
machine learning-based flow control using distributed actuators and sensors.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08117" title="Abstract">arXiv:2311.08117</a> [<a href="/pdf/2311.08117" title="Download PDF">pdf</a>, <a href="/format/2311.08117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Insights into Classifying and Mitigating LLMs&#x27; Hallucinations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruno%2C+A">Alessandro Bruno</a>, 
<a href="/search/cs?searchtype=author&query=Mazzeo%2C+P+L">Pier Luigi Mazzeo</a>, 
<a href="/search/cs?searchtype=author&query=Chetouani%2C+A">Aladine Chetouani</a>, 
<a href="/search/cs?searchtype=author&query=Tliba%2C+M">Marouane Tliba</a>, 
<a href="/search/cs?searchtype=author&query=Kerkouri%2C+M+A">Mohamed Amine Kerkouri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AIxIA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The widespread adoption of large language models (LLMs) across diverse AI
applications is proof of the outstanding achievements obtained in several
tasks, such as text mining, text generation, and question answering. However,
LLMs are not exempt from drawbacks. One of the most concerning aspects regards
the emerging problematic phenomena known as "Hallucinations". They manifest in
text generation systems, particularly in question-answering systems reliant on
LLMs, potentially resulting in false or misleading information propagation.
This paper delves into the underlying causes of AI hallucination and elucidates
its significance in artificial intelligence. In particular, Hallucination
classification is tackled over several tasks (Machine Translation, Question and
Answer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and
Visual Question Answer). Additionally, we explore potential strategies to
mitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our
research addresses this critical issue within the HeReFaNMi (Health-Related
Fake News Mitigation) project, generously supported by NGI Search, dedicated to
combating Health-Related Fake News dissemination on the Internet. This
endeavour represents a concerted effort to safeguard the integrity of
information dissemination in an age of evolving AI technologies.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08118" title="Abstract">arXiv:2311.08118</a> [<a href="/pdf/2311.08118" title="Download PDF">pdf</a>, <a href="/format/2311.08118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Neighbor Explainability for Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Llorente%2C+O">Oscar Llorente</a>, 
<a href="/search/cs?searchtype=author&query=Vaderna%2C+P">P&#xe9;ter Vaderna</a>, 
<a href="/search/cs?searchtype=author&query=Laki%2C+S">S&#xe1;ndor Laki</a>, 
<a href="/search/cs?searchtype=author&query=Kotrocz%C3%B3%2C+R">Roland Kotrocz&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Csoma%2C+R">Rita Csoma</a>, 
<a href="/search/cs?searchtype=author&query=Szalai-Gindl%2C+J+M">J&#xe1;nos M&#xe1;rk Szalai-Gindl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Explainability in Graph Neural Networks (GNNs) is a new field growing in the
last few years. In this publication we address the problem of determining how
important is each neighbor for the GNN when classifying a node and how to
measure the performance for this specific task. To do this, various known
explainability methods are reformulated to get the neighbor importance and four
new metrics are presented. Our results show that there is almost no difference
between the explanations provided by gradient-based techniques in the GNN
domain. In addition, many explainability techniques failed to identify
important neighbors when GNNs without self-loops are used.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08120" title="Abstract">arXiv:2311.08120</a> [<a href="/pdf/2311.08120" title="Download PDF">pdf</a>, <a href="/ps/2311.08120" title="Download PostScript">ps</a>, <a href="/format/2311.08120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Caring Trouble and Musical AI: Considerations towards a Feminist Musical  AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cotton%2C+K">Kelsey Cotton</a>, 
<a href="/search/cs?searchtype=author&query=Tatar%2C+K">K&#x131;van&#xe7; Tatar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AI and Musical Creativity 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AIMC 2023. https://aimc2023.pubpub.org/pub/zwjy371l
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The ethics of AI as both material and medium for interaction remains in murky
waters within the context of musical and artistic practice. The
interdisciplinarity of the field is revealing matters of concern and care,
which necessitate interdisciplinary methodologies for evaluation to trouble and
critique the inheritance of "residue-laden" AI-tools in musical applications.
Seeking to unsettle these murky waters, this paper critically examines the
example of Holly+, a deep neural network that generates raw audio in the
likeness of its creator Holly Herndon. Drawing from theoretical concerns and
considerations from speculative feminism and care ethics, we care-fully trouble
the structures, frameworks and assumptions that oscillate within and around
Holly+. We contribute with several considerations and contemplate future
directions for integrating speculative feminism and care into musical-AI agent
and system design, derived from our critical feminist examination.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08123" title="Abstract">arXiv:2311.08123</a> [<a href="/pdf/2311.08123" title="Download PDF">pdf</a>, <a href="/format/2311.08123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-efficient Stochastic methods for Memory-based Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vishnu%2C+V+K">Vishwajit Kumar Vishnu</a>, 
<a href="/search/cs?searchtype=author&query=Sekhar%2C+C+C">C. Chandra Sekhar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Training Memory-based transformers can require a large amount of memory and
can be quite inefficient. We propose a novel two-phase training mechanism and a
novel regularization technique to improve the training efficiency of
memory-based transformers, which are often used for long-range context
problems. For our experiments, we consider transformer-XL as our baseline model
which is one of memorybased transformer models. We show that our resultant
model, Skip Cross-head TransformerXL, outperforms the baseline on character
level language modeling task with similar parameters and outperforms the
baseline on word level language modelling task with almost 20% fewer
parameters. Our proposed methods do not require any additional memory. We also
demonstrate the effectiveness of our regularization mechanism on BERT which
shows similar performance with reduction in standard deviation of scores of
around 30% on multiple GLUE tasks.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08124" title="Abstract">arXiv:2311.08124</a> [<a href="/pdf/2311.08124" title="Download PDF">pdf</a>, <a href="/format/2311.08124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-order accurate well-balanced energy stable finite difference  schemes for multi-layer shallow water equations on fixed and adaptive moving  meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zhihao Zhang</a>, 
<a href="/search/math?searchtype=author&query=Tang%2C+H">Huazhong Tang</a>, 
<a href="/search/math?searchtype=author&query=Duan%2C+J">Junming Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper develops high-order well-balanced (WB) energy stable (ES) finite
difference schemes for multi-layer (the number of layers $M\geqslant 2$)
shallow water equations (SWEs) on both fixed and adaptive moving meshes,
extending our previous works [20,51]. To obtain an energy inequality, the
convexity of an energy function for an arbitrary $M$ is proved by finding
recurrence relations of the leading principal minors or the quadratic forms of
the Hessian matrix of the energy function with respect to the conservative
variables, which is more involved than the single-layer case due to the
coupling between the layers in the energy function. An important ingredient in
developing high-order semi-discrete ES schemes is the construction of a
two-point energy conservative (EC) numerical flux. In pursuit of the WB
property, a sufficient condition for such EC fluxes is given with compatible
discretizations of the source terms similar to the single-layer case. It can be
decoupled into $M$ identities individually for each layer, making it convenient
to construct a two-point EC flux for the multi-layer system. To suppress
possible oscillations near discontinuities, WENO-based dissipation terms are
added to the high-order WB EC fluxes, which gives semi-discrete high-order WB
ES schemes. Fully-discrete schemes are obtained by employing high-order
explicit SSP-RK methods and proved to preserve the lake at rest. The schemes
are further extended to moving meshes based on a modified energy function for a
reformulated system, relying on the techniques proposed in [51]. Numerical
experiments are conducted for some two- and three-layer cases to validate the
high-order accuracy, WB and ES properties, and high efficiency of the schemes,
with a suitable amount of dissipation chosen by estimating the maximal wave
speed due to the lack of an analytical expression for the eigenstructure of the
multi-layer system.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08125" title="Abstract">arXiv:2311.08125</a> [<a href="/pdf/2311.08125" title="Download PDF">pdf</a>, <a href="/format/2311.08125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lite it fly: An All-Deformable-Butterfly Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+R">Rui Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+C+L">Jason Chun Lok Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiajun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Binxiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+J">Jie Ran</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+N">Ngai Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, accepted as a brief paper in IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Most deep neural networks (DNNs) consist fundamentally of convolutional
and/or fully connected layers, wherein the linear transform can be cast as the
product between a filter matrix and a data matrix obtained by arranging feature
tensors into columns. The lately proposed deformable butterfly (DeBut)
decomposes the filter matrix into generalized, butterflylike factors, thus
achieving network compression orthogonal to the traditional ways of pruning or
low-rank decomposition. This work reveals an intimate link between DeBut and a
systematic hierarchy of depthwise and pointwise convolutions, which explains
the empirically good performance of DeBut layers. By developing an automated
DeBut chain generator, we show for the first time the viability of homogenizing
a DNN into all DeBut layers, thus achieving an extreme sparsity and
compression. Various examples and hardware benchmarks verify the advantages of
All-DeBut networks. In particular, we show it is possible to compress a
PointNet to &lt; 5% parameters with &lt; 5% accuracy drop, a record not achievable by
other compression schemes.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08129" title="Abstract">arXiv:2311.08129</a> [<a href="/pdf/2311.08129" title="Download PDF">pdf</a>, <a href="/format/2311.08129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning based Deep Disentangling Light Field Reconstruction and  Disparity Estimation Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Langqing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Ping Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Light field cameras have a wide range of uses due to their ability to
simultaneously record light intensity and direction. The angular resolution of
light fields is important for downstream tasks such as depth estimation, yet is
often difficult to improve due to hardware limitations. Conventional methods
tend to perform poorly against the challenge of large disparity in sparse light
fields, while general CNNs have difficulty extracting spatial and angular
features coupled together in 4D light fields. The light field disentangling
mechanism transforms the 4D light field into 2D image format, which is more
favorable for CNN for feature extraction. In this paper, we propose a Deep
Disentangling Mechanism, which inherits the principle of the light field
disentangling mechanism and further develops the design of the feature
extractor and adds advanced network structure. We design a light-field
reconstruction network (i.e., DDASR) on the basis of the Deep Disentangling
Mechanism, and achieve SOTA performance in the experiments. In addition, we
design a Block Traversal Angular Super-Resolution Strategy for the practical
application of depth estimation enhancement where the input views is often
higher than 2x2 in the experiments resulting in a high memory usage, which can
reduce the memory usage while having a better reconstruction performance.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08130" title="Abstract">arXiv:2311.08130</a> [<a href="/pdf/2311.08130" title="Download PDF">pdf</a>, <a href="/format/2311.08130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modal Analysis of the Wake Shed Behind a Horizontal Axis Wind Turbine  with Flexible Blades
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Salavatidezfouli%2C+S">Sajad Salavatidezfouli</a>, 
<a href="/search/math?searchtype=author&query=Sheidani%2C+A">Armin Sheidani</a>, 
<a href="/search/math?searchtype=author&query=Bakhshaei%2C+K">Kabir Bakhshaei</a>, 
<a href="/search/math?searchtype=author&query=Safari%2C+A">Ali Safari</a>, 
<a href="/search/math?searchtype=author&query=Hajisharifi%2C+A">Arash Hajisharifi</a>, 
<a href="/search/math?searchtype=author&query=Stabile%2C+G">Giovanni Stabile</a>, 
<a href="/search/math?searchtype=author&query=Rozza%2C+G">Gianluigi Rozza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The proper orthogonal decomposition (POD) has been applied on a full-scale
horizontal-axis wind turbine (HAWT) to shed light on the wake characteristics
behind the wind turbine. In reality, the blade tip experiences high deflections
even at the rated conditions which definitely alter the wake flow field, and in
the case of a wind farm, may complicate the inlet conditions of the downstream
wind turbine. The turbine under consideration is the full-scale model of the
NREL 5MW onshore wind turbine which is accompanied by several simulation
complexities including turbulence, mesh motion and fluid-structure interaction
(FSI). Results indicated an almost similar modal behaviour for the rigid and
flexible turbines at the wake region. In addition, more flow structures in
terms of local vortices and fluctuating velocity fields take place at the far
wake region. The flow structures due to the wake shed from the tower tend to
move towards the center and merge with that of the nacelle leading to an
integral vortical structure 2.5D away from the rotor. Also, it is concluded
that the exclusion of the tower leads to missing a major part of the wake
structures, especially at far-wake positions.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08136" title="Abstract">arXiv:2311.08136</a> [<a href="/pdf/2311.08136" title="Download PDF">pdf</a>, <a href="/ps/2311.08136" title="Download PostScript">ps</a>, <a href="/format/2311.08136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Body Electric: A NIME designed through and with the somatic  experience of singing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cotton%2C+K">Kelsey Cotton</a>, 
<a href="/search/cs?searchtype=author&query=Sanches%2C+P">Pedro Sanches</a>, 
<a href="/search/cs?searchtype=author&query=Tsaknaki%2C+V">Vasiliki Tsaknaki</a>, 
<a href="/search/cs?searchtype=author&query=Karpashevich%2C+P">Pavel Karpashevich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on New Interfaces for Musical Expression (NIME 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper presents the soma design process of creating Body Electric: a
novel interface for the capture and use of biofeedback signals and
physiological changes generated in the body by breathing, during singing. This
NIME design is grounded in the performer's experience of, and relationship to,
their body and their voice. We show that NIME design using principles from soma
design can offer creative opportunities in developing novel sensing mechanisms,
which can in turn inform composition and further elicit curious engagements
between performer and artefact, disrupting notions of performer-led control. As
contributions, this work 1) offers an example of NIME design for situated
living, feeling, performing bodies, and 2) presents the rich potential of soma
design as a path for designing in this context.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08141" title="Abstract">arXiv:2311.08141</a> [<a href="/pdf/2311.08141" title="Download PDF">pdf</a>, <a href="/format/2311.08141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GMTR: Graph Matching Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jinpei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runzhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision transformers (ViTs) have recently been used for visual matching beyond
object detection and segmentation. However, the original grid dividing strategy
of ViTs neglects the spatial information of the keypoints, limiting the
sensitivity to local information. Therefore, we propose \textbf{QueryTrans}
(Query Transformer), which adopts a cross-attention module and keypoints-based
center crop strategy for better spatial information extraction. We further
integrate the graph attention module and devise a transformer-based graph
matching approach \textbf{GMTR} (Graph Matching TRansformers) whereby the
combinatorial nature of GM is addressed by a graph transformer neural GM
solver. On standard GM benchmarks, GMTR shows competitive performance against
the SOTA frameworks. Specifically, on Pascal VOC, GMTR achieves
$\mathbf{83.6\%}$ accuracy, $\mathbf{0.9\%}$ higher than the SOTA framework. On
Spair-71k, GMTR shows great potential and outperforms most of the previous
works. Meanwhile, on Pascal VOC, QueryTrans improves the accuracy of NGMv2 from
$80.1\%$ to $\mathbf{83.3\%}$, and BBGM from $79.0\%$ to $\mathbf{84.5\%}$. On
Spair-71k, QueryTrans improves NGMv2 from $80.6\%$ to $\mathbf{82.5\%}$, and
BBGM from $82.1\%$ to $\mathbf{83.9\%}$. Source code will be made publicly
available.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08143" title="Abstract">arXiv:2311.08143</a> [<a href="/pdf/2311.08143" title="Download PDF">pdf</a>, <a href="/format/2311.08143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sinkhorn Transformations for Single-Query Postprocessing in Text-Video  Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yakovlev%2C+K">Konstantin Yakovlev</a>, 
<a href="/search/cs?searchtype=author&query=Polyakov%2C+G">Gregory Polyakov</a>, 
<a href="/search/cs?searchtype=author&query=Alimova%2C+I">Ilseyar Alimova</a>, 
<a href="/search/cs?searchtype=author&query=Podolskiy%2C+A">Alexander Podolskiy</a>, 
<a href="/search/cs?searchtype=author&query=Bout%2C+A">Andrey Bout</a>, 
<a href="/search/cs?searchtype=author&query=Nikolenko%2C+S">Sergey Nikolenko</a>, 
<a href="/search/cs?searchtype=author&query=Piontkovskaya%2C+I">Irina Piontkovskaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGIR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">A recent trend in multimodal retrieval is related to postprocessing test set
results via the dual-softmax loss (DSL). While this approach can bring
significant improvements, it usually presumes that an entire matrix of test
samples is available as DSL input. This work introduces a new postprocessing
approach based on Sinkhorn transformations that outperforms DSL. Further, we
propose a new postprocessing setting that does not require access to multiple
test queries. We show that our approach can significantly improve the results
of state of the art models such as CLIP4Clip, BLIP, X-CLIP, and DRL, thus
achieving a new state-of-the-art on several standard text-video retrieval
datasets both with access to the entire test set and in the single-query
setting.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08147" title="Abstract">arXiv:2311.08147</a> [<a href="/pdf/2311.08147" title="Download PDF">pdf</a>, <a href="/format/2311.08147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RECALL: A Benchmark for LLMs Robustness against External Counterfactual  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lianzhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shicheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sishuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xu Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">LLMs and AI chatbots have improved people's efficiency in various fields.
However, the necessary knowledge for answering the question may be beyond the
models' knowledge boundaries. To mitigate this issue, many researchers try to
introduce external knowledge, such as knowledge graphs and Internet contents,
into LLMs for up-to-date information. However, the external information from
the Internet may include counterfactual information that will confuse the model
and lead to an incorrect response. Thus there is a pressing need for LLMs to
possess the ability to distinguish reliable information from external
knowledge. Therefore, to evaluate the ability of LLMs to discern the
reliability of external knowledge, we create a benchmark from existing
knowledge bases. Our benchmark consists of two tasks, Question Answering and
Text Generation, and for each task, we provide models with a context containing
counterfactual information. Evaluation results show that existing LLMs are
susceptible to interference from unreliable external knowledge with
counterfactual information, and simple intervention methods make limited
contributions to the alleviation of this issue.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08148" title="Abstract">arXiv:2311.08148</a> [<a href="/pdf/2311.08148" title="Download PDF">pdf</a>, <a href="/format/2311.08148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cattle Identification Using Muzzle Images and Deep Learning Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kimani%2C+G+N">G. N. Kimani</a>, 
<a href="/search/cs?searchtype=author&query=Oluwadara%2C+P">P. Oluwadara</a>, 
<a href="/search/cs?searchtype=author&query=Fashingabo%2C+P">P. Fashingabo</a>, 
<a href="/search/cs?searchtype=author&query=Busogi%2C+M">M. Busogi</a>, 
<a href="/search/cs?searchtype=author&query=Luhanga%2C+E">E. Luhanga</a>, 
<a href="/search/cs?searchtype=author&query=Sowon%2C+K">K. Sowon</a>, 
<a href="/search/cs?searchtype=author&query=Chacha%2C+L">L. Chacha</a> ((1) CyLab-Africa / Upanzi Network, (2) Carnegie Mellon University Africa and (3) Carnegie Mellon University Pittsburgh)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Traditional animal identification methods such as ear-tagging, ear notching,
and branding have been effective but pose risks to the animal and have
scalability issues. Electrical methods offer better tracking and monitoring but
require specialized equipment and are susceptible to attacks. Biometric
identification using time-immutable dermatoglyphic features such as muzzle
prints and iris patterns is a promising solution. This project explores cattle
identification using 4923 muzzle images collected from 268 beef cattle. Two
deep learning classification models are implemented - wide ResNet50 and
VGG16\_BN and image compression is done to lower the image quality and adapt
the models to work for the African context. From the experiments run, a maximum
accuracy of 99.5\% is achieved while using the wide ResNet50 model with a
compression retaining 25\% of the original image. From the study, it is noted
that the time required by the models to train and converge as well as
recognition time are dependent on the machine used to run the model.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08149" title="Abstract">arXiv:2311.08149</a> [<a href="/pdf/2311.08149" title="Download PDF">pdf</a>, <a href="/format/2311.08149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Complex Disease Trajectories using Deep Generative Models with  Semi-Supervised Latent Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trottet%2C+C">C&#xe9;cile Trottet</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCrch%2C+M">Manuel Sch&#xfc;rch</a>, 
<a href="/search/cs?searchtype=author&query=Allam%2C+A">Ahmed Allam</a>, 
<a href="/search/cs?searchtype=author&query=Barua%2C+I">Imon Barua</a>, 
<a href="/search/cs?searchtype=author&query=Petelytska%2C+L">Liubov Petelytska</a>, 
<a href="/search/cs?searchtype=author&query=Distler%2C+O">Oliver Distler</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann-Vold%2C+A">Anna-Maria Hoffmann-Vold</a>, 
<a href="/search/cs?searchtype=author&query=Krauthammer%2C+M">Michael Krauthammer</a>, 
the <a href="/search/cs?searchtype=author&query=collaborators%2C+E">EUSTAR collaborators</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we propose a deep generative time series approach using latent
temporal processes for modeling and holistically analyzing complex disease
trajectories. We aim to find meaningful temporal latent representations of an
underlying generative process that explain the observed disease trajectories in
an interpretable and comprehensive way. To enhance the interpretability of
these latent temporal processes, we develop a semi-supervised approach for
disentangling the latent space using established medical concepts. By combining
the generative approach with medical knowledge, we leverage the ability to
discover novel aspects of the disease while integrating medical concepts into
the model. We show that the learned temporal latent processes can be utilized
for further data analysis and clinical hypothesis testing, including finding
similar patients and clustering the disease into new sub-types. Moreover, our
method enables personalized online monitoring and prediction of multivariate
time series including uncertainty quantification. We demonstrate the
effectiveness of our approach in modeling systemic sclerosis, showcasing the
potential of our machine learning model to capture complex disease trajectories
and acquire new medical knowledge.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08150" title="Abstract">arXiv:2311.08150</a> [<a href="/pdf/2311.08150" title="Download PDF">pdf</a>, <a href="/format/2311.08150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hyperdimensional Transform for Distributional Modelling, Regression  and Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dewulf%2C+P">Pieter Dewulf</a>, 
<a href="/search/cs?searchtype=author&query=De+Baets%2C+B">Bernard De Baets</a>, 
<a href="/search/cs?searchtype=author&query=Stock%2C+M">Michiel Stock</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Hyperdimensional computing (HDC) is an increasingly popular computing
paradigm with immense potential for future intelligent applications. Although
the main ideas already took form in the 1990s, HDC recently gained significant
attention, especially in the field of machine learning and data science. Next
to efficiency, interoperability and explainability, HDC offers attractive
properties for generalization as it can be seen as an attempt to combine
connectionist ideas from neural networks with symbolic aspects. In recent work,
we introduced the hyperdimensional transform, revealing deep theoretical
foundations for representing functions and distributions as high-dimensional
holographic vectors. Here, we present the power of the hyperdimensional
transform to a broad data science audience. We use the hyperdimensional
transform as a theoretical basis and provide insight into state-of-the-art HDC
approaches for machine learning. We show how existing algorithms can be
modified and how this transform can lead to a novel, well-founded toolbox. Next
to the standard regression and classification tasks of machine learning, our
discussion includes various aspects of statistical modelling, such as
representation, learning and deconvolving distributions, sampling, Bayesian
inference, and uncertainty estimation.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08151" title="Abstract">arXiv:2311.08151</a> [<a href="/pdf/2311.08151" title="Download PDF">pdf</a>, <a href="/format/2311.08151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video  Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yating Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Conghui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G+H">Gim Hee Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing works on weakly-supervised audio-visual video parsing adopt hybrid
attention network (HAN) as the multi-modal embedding to capture the cross-modal
context. It embeds the audio and visual modalities with a shared network, where
the cross-attention is performed at the input. However, such an early fusion
method highly entangles the two non-fully correlated modalities and leads to
sub-optimal performance in detecting single-modality events. To deal with this
problem, we propose the messenger-guided mid-fusion transformer to reduce the
uncorrelated cross-modal context in the fusion. The messengers condense the
full cross-modal context into a compact representation to only preserve useful
cross-modal information. Furthermore, due to the fact that microphones capture
audio events from all directions, while cameras only record visual events
within a restricted field of view, there is a more frequent occurrence of
unaligned cross-modal context from audio for visual event predictions. We thus
propose cross-audio prediction consistency to suppress the impact of irrelevant
audio information on visual event prediction. Experiments consistently
illustrate the superior performance of our framework compared to existing
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08152" title="Abstract">arXiv:2311.08152</a> [<a href="/pdf/2311.08152" title="Download PDF">pdf</a>, <a href="/format/2311.08152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Reasoning in Large Language Models via Multi-Agent Peer Review  Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Senbao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jindi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongfang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuxiang Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, 8 tables. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have shown remarkable capabilities in general
natural language processing tasks but often fall short in complex reasoning
tasks. Recent studies have explored human-like problem-solving strategies, such
as self-correct, to push further the boundary of single-model reasoning
ability. In this work, we let a single model "step outside the box" by engaging
multiple models to correct each other. We introduce a multi-agent collaboration
strategy that emulates the academic peer review process. Each agent
independently constructs its own solution, provides reviews on the solutions of
others, and assigns confidence levels to its reviews. Upon receiving peer
reviews, agents revise their initial solutions. Extensive experiments on three
different types of reasoning tasks show that our collaboration approach
delivers superior accuracy across all ten datasets compared to existing
methods. Further study demonstrates the effectiveness of integrating confidence
in the reviews for math reasoning, and suggests a promising direction for
human-mimicking multi-agent collaboration process.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08153" title="Abstract">arXiv:2311.08153</a> [<a href="/pdf/2311.08153" title="Download PDF">pdf</a>, <a href="/format/2311.08153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Mining Electric Locomotives Meet Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Ying Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+Z">Zhencai Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiaoqiang Li</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+C">Chunyu Yang</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+H">Hao Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As the most important auxiliary transportation equipment in coal mines,
mining electric locomotives are mostly operated manually at present. However,
due to the complex and ever-changing coal mine environment, electric locomotive
safety accidents occur frequently these years. A mining electric locomotive
control method that can adapt to different complex mining environments is
needed. Reinforcement Learning (RL) is concerned with how artificial agents
ought to take actions in an environment so as to maximize reward, which can
help achieve automatic control of mining electric locomotive. In this paper, we
present how to apply RL to the autonomous control of mining electric
locomotives. To achieve more precise control, we further propose an improved
epsilon-greedy (IEG) algorithm which can better balance the exploration and
exploitation. To verify the effectiveness of this method, a co-simulation
platform for autonomous control of mining electric locomotives is built which
can complete closed-loop simulation of the vehicles. The simulation results
show that this method ensures the locomotives following the front vehicle
safely and responding promptly in the event of sudden obstacles on the road
when the vehicle in complex and uncertain coal mine environments.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08154" title="Abstract">arXiv:2311.08154</a> [<a href="/pdf/2311.08154" title="Download PDF">pdf</a>, <a href="/format/2311.08154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask One More Time: Self-Agreement Improves Reasoning of Language Models  in (Almost) All Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jiayi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengli Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Junchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although chain-of-thought (CoT) prompting combined with language models has
achieved encouraging results on complex reasoning tasks, the naive greedy
decoding used in CoT prompting usually causes the repetitiveness and local
optimality. To address this shortcoming, ensemble-optimization tries to obtain
multiple reasoning paths to get the final answer assembly. However, current
ensemble-optimization methods either simply employ rule-based post-processing
such as \textit{self-consistency}, or train an additional model based on
several task-related human annotations to select the best one among multiple
reasoning paths, yet fail to generalize to realistic settings where the type of
input questions is unknown or the answer format of reasoning paths is unknown.
To avoid their limitations, we propose \textbf{self-agreement}, a generalizable
ensemble-optimization method applying in almost all scenarios where the type of
input questions and the answer format of reasoning paths may be known or
unknown. Self-agreement firstly samples from language model's decoder to
generate a \textit{diverse} set of reasoning paths, and subsequently prompts
the language model \textit{one more time} to determine the optimal answer by
selecting the most \textit{agreed} answer among the sampled reasoning paths.
Self-agreement simultaneously achieves remarkable performance on six public
reasoning benchmarks and superior generalization capabilities.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08156" title="Abstract">arXiv:2311.08156</a> [<a href="/pdf/2311.08156" title="Download PDF">pdf</a>, <a href="/format/2311.08156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Spectral Bound for Quasi-Cyclic Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+G">Gaojun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ezerman%2C+M+F">Martianus Frederic Ezerman</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+S">San Ling</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96zkaya%2C+B">Buket &#xd6;zkaya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Spectral bounds form a powerful tool to estimate the minimum distances of
quasi-cyclic codes. They generalize the defining set bounds of cyclic codes to
those of quasi-cyclic codes. Based on the eigenvalues of quasi-cyclic codes and
the corresponding eigenspaces, we provide an improved spectral bound for
quasi-cyclic codes. Numerical results verify that the improved bound
outperforms the Jensen bound in almost all cases. Based on the improved bound,
we propose a general construction of quasi-cyclic codes with excellent designed
minimum distances. For the quasi-cyclic codes produced by this general
construction, the improved spectral bound is always sharper than the Jensen
bound.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08157" title="Abstract">arXiv:2311.08157</a> [<a href="/pdf/2311.08157" title="Download PDF">pdf</a>, <a href="/format/2311.08157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransformCode: A Contrastive Learning Framework for Code Embedding via  Subtree transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xian%2C+Z">Zixiang Xian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Rubing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Towey%2C+D">Dave Towey</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chunrong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenyu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large-scale language models have made great progress in the field of software
engineering in recent years. They can be used for many code-related tasks such
as code clone detection, code-to-code search, and method name prediction.
However, these large-scale language models based on each code token have
several drawbacks: They are usually large in scale, heavily dependent on
labels, and require a lot of computing power and time to fine-tune new
datasets.Furthermore, code embedding should be performed on the entire code
snippet rather than encoding each code token. The main reason for this is that
encoding each code token would cause model parameter inflation, resulting in a
lot of parameters storing information that we are not very concerned about. In
this paper, we propose a novel framework, called TransformCode, that learns
about code embeddings in a contrastive learning manner. The framework uses the
Transformer encoder as an integral part of the model. We also introduce a novel
data augmentation technique called abstract syntax tree transformation: This
technique applies syntactic and semantic transformations to the original code
snippets to generate more diverse and robust anchor samples. Our proposed
framework is both flexible and adaptable: It can be easily extended to other
downstream tasks that require code representation such as code clone detection
and classification. The framework is also very efficient and scalable: It does
not require a large model or a large amount of training data, and can support
any programming language.Finally, our framework is not limited to unsupervised
learning, but can also be applied to some supervised learning tasks by
incorporating task-specific labels or objectives. To explore the effectiveness
of our framework, we conducted extensive experiments on different software
engineering tasks using different programming languages and multiple datasets.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08158" title="Abstract">arXiv:2311.08158</a> [<a href="/pdf/2311.08158" title="Download PDF">pdf</a>, <a href="/format/2311.08158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel Estimation with Dynamic Metasurface Antennas via Model-Based  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Luxi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Eldar%2C+Y+C">Yonina C.Eldar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Dynamic Metasurface Antenna (DMA) is a cutting-edge antenna technology
offering scalable and sustainable solutions for large antenna arrays. The
effectiveness of DMAs stems from their inherent configurable analog signal
processing capabilities, which facilitate cost-limited implementations.
However, when DMAs are used in multiple input multiple output (MIMO)
communication systems, they pose challenges in channel estimation due to their
analog compression. In this paper, we propose two model-based learning methods
to overcome this challenge. Our approach starts by casting channel estimation
as a compressed sensing problem. Here, the sensing matrix is formed using a
random DMA weighting matrix combined with a spatial gridding dictionary. We
then employ the learned iterative shrinkage and thresholding algorithm (LISTA)
to recover the sparse channel parameters. LISTA unfolds the iterative shrinkage
and thresholding algorithm into a neural network and trains the neural network
into a highly efficient channel estimator fitting with the previous channel. As
the sensing matrix is crucial to the accuracy of LISTA recovery, we introduce
another data-aided method, LISTA-sensing matrix optimization (LISTA-SMO), to
jointly optimize the sensing matrix. LISTA-SMO takes LISTA as a backbone and
embeds the sensing matrix optimization layers in LISTA's neural network,
allowing for the optimization of the sensing matrix along with the training of
LISTA. Furthermore, we propose a self-supervised learning technique to tackle
the difficulty of acquiring noise-free data. Our numerical results demonstrate
that LISTA outperforms traditional sparse recovery methods regarding channel
estimation accuracy and efficiency. Besides, LISTA-SMO achieves better channel
accuracy than LISTA, demonstrating the effectiveness in optimizing the sensing
matrix.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08159" title="Abstract">arXiv:2311.08159</a> [<a href="/pdf/2311.08159" title="Download PDF">pdf</a>, <a href="/format/2311.08159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynamicSurf: Dynamic Neural RGB-D Surface Reconstruction with an  Optimizable Feature Grid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+M">Mirgahney Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Agapito%2C+L">Lourdes Agapito</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose DynamicSurf, a model-free neural implicit surface reconstruction
method for high-fidelity 3D modelling of non-rigid surfaces from monocular
RGB-D video. To cope with the lack of multi-view cues in monocular sequences of
deforming surfaces, one of the most challenging settings for 3D reconstruction,
DynamicSurf exploits depth, surface normals, and RGB losses to improve
reconstruction fidelity and optimisation time. DynamicSurf learns a neural
deformation field that maps a canonical representation of the surface geometry
to the current frame. We depart from current neural non-rigid surface
reconstruction models by designing the canonical representation as a learned
feature grid which leads to faster and more accurate surface reconstruction
than competing approaches that use a single MLP. We demonstrate DynamicSurf on
public datasets and show that it can optimize sequences of varying frames with
$6\times$ speedup over pure MLP-based approaches while achieving comparable
results to the state-of-the-art methods. Project is available at
https://mirgahney.github.io//DynamicSurf.io/.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08166" title="Abstract">arXiv:2311.08166</a> [<a href="/pdf/2311.08166" title="Download PDF">pdf</a>, <a href="/ps/2311.08166" title="Download PostScript">ps</a>, <a href="/format/2311.08166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MechAgents: Large language model multi-agent collaborations can solve  mechanics problems, generate new data, and integrate knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+B">Bo Ni</a>, 
<a href="/search/cs?searchtype=author&query=Buehler%2C+M+J">Markus J. Buehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Materials Science (cond-mat.mtrl-sci); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Solving mechanics problems using numerical methods requires comprehensive
intelligent capability of retrieving relevant knowledge and theory,
constructing and executing codes, analyzing the results, a task that has thus
far mainly been reserved for humans. While emerging AI methods can provide
effective approaches to solve end-to-end problems, for instance via the use of
deep surrogate models or various data analytics strategies, they often lack
physical intuition since knowledge is baked into the parametric complement
through training, offering less flexibility when it comes to incorporating
mathematical or physical insights. By leveraging diverse capabilities of
multiple dynamically interacting large language models (LLMs), we can overcome
the limitations of conventional approaches and develop a new class of
physics-inspired generative machine learning platform, here referred to as
MechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for
elasticity problems, via autonomous collaborations. A two-agent team can
effectively write, execute and self-correct code, in order to apply finite
element methods to solve classical elasticity problems in various flavors
(different boundary conditions, domain geometries, meshes, small/finite
deformation and linear/hyper-elastic constitutive laws, and others). For more
complex tasks, we construct a larger group of agents with enhanced division of
labor among planning, formulating, coding, executing and criticizing the
process and results. The agents mutually correct each other to improve the
overall team-work performance in understanding, formulating and validating the
solution. Our framework shows the potential of synergizing the intelligence of
language models, the reliability of physics-based modeling, and the dynamic
collaborations among diverse agents, opening novel avenues for automation of
solving engineering problems.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08167" title="Abstract">arXiv:2311.08167</a> [<a href="/pdf/2311.08167" title="Download PDF">pdf</a>, <a href="/format/2311.08167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeDe: Balancing Blockchain Privacy and Regulatory Compliance by  Selective De-Anonymization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+N">Naveen Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Gajera%2C+M">Mitul Gajera</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+A">Amit Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Ivey-Law%2C+H">Hamish Ivey-Law</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Privacy is one of the essential pillars for the widespread adoption of
blockchains, but public blockchains are transparent by nature. Modern analytics
techniques can easily subdue the pseudonymity feature of a blockchain user.
Some applications have been able to provide practical privacy protections using
privacy-preserving cryptography techniques. However, malicious actors have
abused them illicitly, discouraging honest actors from using privacy-preserving
applications as "mixing" user interactions and funds with anonymous bad actors,
causing compliance and regulatory concerns.
<br />In this paper, we propose a framework that balances privacy-preserving
features by establishing a regulatory and compliant framework called Selective
De-Anonymization (SeDe). The adoption of this framework allows
privacy-preserving applications on blockchains to de-anonymize illicit
transactions by recursive traversal of subgraphs of linked transactions. Our
technique achieves this without leaving de-anonymization decisions or control
in the hands of a single entity but distributing it among multiple entities
while holding them accountable for their respective actions. To instantiate,
our framework uses threshold encryption schemes and Zero-Knowledge Proofs
(ZKPs).
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08170" title="Abstract">arXiv:2311.08170</a> [<a href="/pdf/2311.08170" title="Download PDF">pdf</a>, <a href="/format/2311.08170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marchetti%2C+G+L">Giovanni Luca Marchetti</a>, 
<a href="/search/cs?searchtype=author&query=Cesa%2C+G">Gabriele Cesa</a>, 
<a href="/search/cs?searchtype=author&query=Pratik%2C+K">Kumar Pratik</a>, 
<a href="/search/cs?searchtype=author&query=Behboodi%2C+A">Arash Behboodi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Symmetry and Geometry in Neural Representations - NeurReps Workshop @ NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Lattice reduction is a combinatorial optimization problem aimed at finding
the most orthogonal basis in a given lattice. In this work, we address lattice
reduction via deep learning methods. We design a deep neural model outputting
factorized unimodular matrices and train it in a self-supervised manner by
penalizing non-orthogonal lattice bases. We incorporate the symmetries of
lattice reduction into the model by making it invariant and equivariant with
respect to appropriate continuous and discrete groups.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08172" title="Abstract">arXiv:2311.08172</a> [<a href="/pdf/2311.08172" title="Download PDF">pdf</a>, <a href="/format/2311.08172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Instruction Tuning: A Review and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chen Li</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yixiao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dian Li</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Instruction tuning is an essential supervised training phase for Large
Language Models (LLMs), with the goal of enhancing LLMs' capacity to generalize
instruction execution and adapt to user preferences. With the growing
incorporation of multi-modal data into LLMs, there is an increasing interest in
the performance of vision-language instruction tuning which presents more
complex features in comparison to pure text instructions. In this paper, we
systematically review the latest vision-language instruction tuning settings
and datasets in multi-modal LLMs and summarize the characteristics that
high-quality vision-language tuning data should have. We consider these
characteristics as the foundational principles for constructing vision-language
instruction data and propose a complete construction pipeline consisting of
data collection, instruction generation, and quality control modules that
incorporate meticulously designed instruction property evaluation indicators.
We perform vision-language instruction tuning on three widely used multi-modal
LLMs based on the instruction data we constructed and conduct extensive
experiments on the corresponding metrics to demonstrate the rationality of the
construction principles proposed in this paper. The code and dataset related to
this paper have been open-sourced at
\url{https://github.com/palchenli/VL-Instruction-Tuning}.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08175" title="Abstract">arXiv:2311.08175</a> [<a href="/pdf/2311.08175" title="Download PDF">pdf</a>, <a href="/format/2311.08175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Microservice API Evolution in Practice: A Study on Strategies and  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lercher%2C+A">Alexander Lercher</a>, 
<a href="/search/cs?searchtype=author&query=Glock%2C+J">Johann Glock</a>, 
<a href="/search/cs?searchtype=author&query=Macho%2C+C">Christian Macho</a>, 
<a href="/search/cs?searchtype=author&query=Pinzger%2C+M">Martin Pinzger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Nowadays, many companies design and develop their software systems as a set
of loosely coupled microservices that communicate via their Application
Programming Interfaces (APIs). While the loose coupling improves
maintainability, scalability, and fault tolerance, it poses new challenges to
the API evolution process. Related works identified communication and
integration as major API evolution challenges but did not provide the
underlying reasons and research directions to mitigate them. In this paper, we
aim to identify microservice API evolution strategies and challenges in
practice and gain a broader perspective of their relationships. We conducted 17
semi-structured interviews with developers, architects, and managers in 11
companies and analyzed the interviews with open coding used in grounded theory.
In total, we identified six strategies and six challenges for REpresentational
State Transfer (REST) and event-driven communication via message brokers. The
strategies mainly focus on API backward compatibility, versioning, and close
collaboration between teams. The challenges include change impact analysis
efforts, ineffective communication of changes, and consumer reliance on
outdated versions, leading to API design degradation. We defined two important
problems in microservice API evolution resulting from the challenges and their
coping strategies: tight organizational coupling and consumer lock-in. To
mitigate these two problems, we propose automating the change impact analysis
and investigating effective communication of changes as open research
directions.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08176" title="Abstract">arXiv:2311.08176</a> [<a href="/pdf/2311.08176" title="Download PDF">pdf</a>, <a href="/format/2311.08176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A deformation-based morphometry framework for disentangling Alzheimer&#x27;s  disease from normal aging using learned normal aging templates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jingru Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+D">Daniel Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Smedby%2C+%C3%96">&#xd6;rjan Smedby</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+R">Rodrigo Moreno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Alzheimer's Disease and normal aging are both characterized by brain atrophy.
The question of whether AD-related brain atrophy represents accelerated aging
or a neurodegeneration process distinct from that in normal aging remains
unresolved. Moreover, precisely disentangling AD-related brain atrophy from
normal aging in a clinical context is complex. In this study, we propose a
deformation-based morphometry framework to estimate normal aging and
AD-specific atrophy patterns of subjects from morphological MRI scans. We first
leverage deep-learning-based methods to create age-dependent templates of
cognitively normal (CN) subjects. These templates model the normal aging
atrophy patterns in a CN population. Then, we use the learned diffeomorphic
registration to estimate the one-year normal aging pattern at the voxel level.
We register the testing image to the 60-year-old CN template in the second
step. Finally, normal aging and AD-specific scores are estimated by measuring
the alignment of this registration with the one-year normal aging pattern. The
methodology was developed and evaluated on the OASIS3 dataset with 1,014
T1-weighted MRI scans. Of these, 326 scans were from CN subjects, and 688 scans
were from individuals clinically diagnosed with AD at different stages of
clinical severity defined by clinical dementia rating (CDR) scores. The results
show that ventricles predominantly follow an accelerated normal aging pattern
in subjects with AD. In turn, hippocampi and amygdala regions were affected by
both normal aging and AD-specific factors. Interestingly, hippocampi and
amygdala regions showed more of an accelerated normal aging pattern for
subjects during the early clinical stages of the disease, while the AD-specific
score increases in later clinical stages. Our code is freely available at
https://github.com/Fjr9516/DBM_with_DL.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08182" title="Abstract">arXiv:2311.08182</a> [<a href="/pdf/2311.08182" title="Download PDF">pdf</a>, <a href="/format/2311.08182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shengguang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Keming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Benfeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Junyang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Q">Qi Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Enhancing the instruction-following ability of Large Language Models (LLMs)
primarily demands substantial instruction-tuning datasets. However, the sheer
volume of these imposes a considerable computational burden and annotation
cost. To investigate a label-efficient instruction tuning method that allows
the model itself to actively sample subsets that are equally or even more
effective, we introduce a self-evolving mechanism DiverseEvol. In this process,
a model iteratively augments its training subset to refine its own performance,
without requiring any intervention from humans or more advanced LLMs. The key
to our data sampling technique lies in the enhancement of diversity in the
chosen subsets, as the model selects new data points most distinct from any
existing ones according to its current embedding space. Extensive experiments
across three datasets and benchmarks demonstrate the effectiveness of
DiverseEvol. Our models, trained on less than 8% of the original dataset,
maintain or improve performance compared with finetuning on full data. We also
provide empirical evidence to analyze the importance of diversity in
instruction data and the iterative scheme as opposed to one-time sampling. Our
code is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08185" title="Abstract">arXiv:2311.08185</a> [<a href="/pdf/2311.08185" title="Download PDF">pdf</a>, <a href="/format/2311.08185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Dynamic Memory Requirements for Scientific Workflow Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bader%2C+J">Jonathan Bader</a>, 
<a href="/search/cs?searchtype=author&query=Diedrich%2C+N">Nils Diedrich</a>, 
<a href="/search/cs?searchtype=author&query=Thamsen%2C+L">Lauritz Thamsen</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+O">Odej Kao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">With the increasing amount of data available to scientists in disciplines as
diverse as bioinformatics, physics, and remote sensing, scientific workflow
systems are becoming increasingly important for composing and executing
scalable data analysis pipelines. When writing such workflows, users need to
specify the resources to be reserved for tasks so that sufficient resources are
allocated on the target cluster infrastructure. Crucially, underestimating a
task's memory requirements can result in task failures. Therefore, users often
resort to overprovisioning, resulting in significant resource wastage and
decreased throughput.
<br />In this paper, we propose a novel online method that uses monitoring time
series data to predict task memory usage in order to reduce the memory wastage
of scientific workflow tasks. Our method predicts a task's runtime, divides it
into k equally-sized segments, and learns the peak memory value for each
segment depending on the total file input size. We evaluate the prototype
implementation of our method using workflows from the publicly available
nf-core repository, showing an average memory wastage reduction of 29.48%
compared to the best state-of-the-art approach
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08188" title="Abstract">arXiv:2311.08188</a> [<a href="/pdf/2311.08188" title="Download PDF">pdf</a>, <a href="/ps/2311.08188" title="Download PostScript">ps</a>, <a href="/format/2311.08188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast List Decoding of High-Rate Polar Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Ming-Min Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+M">Ming Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Min-Jian Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Due to the ability to provide superior error-correction performance, the
successive cancellation list (SCL) algorithm is widely regarded as one of the
most promising decoding algorithms for polar codes with short-to-moderate code
lengths. However, the application of SCL decoding in low-latency communication
scenarios is limited due to its sequential nature. To reduce the decoding
latency, developing tailored fast and efficient list decoding algorithms of
specific polar substituent codes (special nodes) is a promising solution.
Recently, fast list decoding algorithms are proposed by considering special
nodes with low code rates. Aiming to further speedup the SCL decoding, this
paper presents fast list decoding algorithms for two types of high-rate special
nodes, namely single-parity-check (SPC) nodes and sequence rate one or
single-parity-check (SR1/SPC) nodes. In particular, we develop two classes of
fast list decoding algorithms for these nodes, where the first class uses a
sequential decoding procedure to yield decoding latency that is linear with the
list size, and the second further parallelizes the decoding process by
pre-determining the redundant candidate paths offline. Simulation results show
that the proposed list decoding algorithms are able to achieve up to 70.7\%
lower decoding latency than state-of-the-art fast SCL decoders, while
exhibiting the same error-correction performance.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08189" title="Abstract">arXiv:2311.08189</a> [<a href="/pdf/2311.08189" title="Download PDF">pdf</a>, <a href="/format/2311.08189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Science: Novel Dataset and Benchmark for Cross-Modality  Scientific Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiwei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Karlsso%2C+B+F">B&#xf6;rje F. Karlsso</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Okumura%2C+M">Manabu Okumura</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chin-Yew Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Extracting key information from scientific papers has the potential to help
researchers work more efficiently and accelerate the pace of scientific
progress. Over the last few years, research on Scientific Information
Extraction (SciIE) witnessed the release of several new systems and benchmarks.
However, existing paper-focused datasets mostly focus only on specific parts of
a manuscript (e.g., abstracts) and are single-modality (i.e., text- or
table-only), due to complex processing and expensive annotations. Moreover,
core information can be present in either text or tables or across both. To
close this gap in data availability and enable cross-modality IE, while
alleviating labeling costs, we propose a semi-supervised pipeline for
annotating entities in text, as well as entities and relations in tables, in an
iterative procedure. Based on this pipeline, we release novel resources for the
scientific community, including a high-quality benchmark, a large-scale corpus,
and a semi-supervised annotation pipeline. We further report the performance of
state-of-the-art IE models on the proposed benchmark dataset, as a baseline.
Lastly, we explore the potential capability of large language models such as
ChatGPT for the current task. Our new dataset, results, and analysis validate
the effectiveness and efficiency of our semi-supervised pipeline, and we
discuss its remaining limitations.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08191" title="Abstract">arXiv:2311.08191</a> [<a href="/pdf/2311.08191" title="Download PDF">pdf</a>, <a href="/format/2311.08191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GEC-DePenD: Non-Autoregressive Grammatical Error Correction with  Decoupled Permutation and Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yakovlev%2C+K">Konstantin Yakovlev</a>, 
<a href="/search/cs?searchtype=author&query=Podolskiy%2C+A">Alexander Podolskiy</a>, 
<a href="/search/cs?searchtype=author&query=Bout%2C+A">Andrey Bout</a>, 
<a href="/search/cs?searchtype=author&query=Nikolenko%2C+S">Sergey Nikolenko</a>, 
<a href="/search/cs?searchtype=author&query=Piontkovskaya%2C+I">Irina Piontkovskaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Grammatical error correction (GEC) is an important NLP task that is currently
usually solved with autoregressive sequence-to-sequence models. However,
approaches of this class are inherently slow due to one-by-one token
generation, so non-autoregressive alternatives are needed. In this work, we
propose a novel non-autoregressive approach to GEC that decouples the
architecture into a permutation network that outputs a self-attention weight
matrix that can be used in beam search to find the best permutation of input
tokens (with auxiliary {ins} tokens) and a decoder network based on a
step-unrolled denoising autoencoder that fills in specific tokens. This allows
us to find the token permutation after only one forward pass of the permutation
network, avoiding autoregressive constructions. We show that the resulting
network improves over previously known non-autoregressive methods for GEC and
reaches the level of autoregressive methods that do not use language-specific
synthetic data generation methods. Our results are supported by a comprehensive
experimental validation on the ConLL-2014 and Write&amp;Improve+LOCNESS datasets
and an extensive ablation study that supports our architectural and algorithmic
choices.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08195" title="Abstract">arXiv:2311.08195</a> [<a href="/pdf/2311.08195" title="Download PDF">pdf</a>, <a href="/format/2311.08195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Fact-Checking in Dialogue: Are Specialized Models Needed?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chamoun%2C+E">Eric Chamoun</a>, 
<a href="/search/cs?searchtype=author&query=Saeidi%2C+M">Marzieh Saeidi</a>, 
<a href="/search/cs?searchtype=author&query=Vlachos%2C+A">Andreas Vlachos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prior research has shown that typical fact-checking models for stand-alone
claims struggle with claims made in dialogues. As a solution, fine-tuning these
models on labelled dialogue data has been proposed. However, creating separate
models for each use case is impractical, and we show that fine-tuning models
for dialogue results in poor performance on typical fact-checking. To overcome
this challenge, we present techniques that allow us to use the same models for
both dialogue and typical fact-checking. These mainly focus on retrieval
adaptation and transforming conversational inputs so that they can be
accurately predicted by models trained on stand-alone claims. We demonstrate
that a typical fact-checking model incorporating these techniques is
competitive with state-of-the-art models fine-tuned for dialogue, while
maintaining its accuracy on stand-alone claims.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08196" title="Abstract">arXiv:2311.08196</a> [<a href="/pdf/2311.08196" title="Download PDF">pdf</a>, <a href="/format/2311.08196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational homogenization of higher-order electro-mechanical  materials with built-in generalized periodicity conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Barcel%C3%B3-Mercader%2C+J">J. Barcel&#xf3;-Mercader</a>, 
<a href="/search/math?searchtype=author&query=Codony%2C+D">D. Codony</a>, 
<a href="/search/math?searchtype=author&query=Mocci%2C+A">A. Mocci</a>, 
<a href="/search/math?searchtype=author&query=Arias%2C+I">I. Arias</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">We present a formulation for high-order generalized periodicity conditions in
the context of a high-order electromechanical theory including
flexoelectricity, strain gradient elasticity and gradient dielectricity, with
the goal of studying periodic architected metamaterials. Such theory results in
fourth-order governing partial differential equations, and the periodicity
conditions involve continuity across the periodic boundary of primal fields
(displacement and electric potential) and their normal derivatives, continuity
of the corresponding dual generalized forces (tractions, double tractions,
surface charge density and double surface charge density). Rather than imposing
these conditions numerically as explicit constraints, we develop an
approximation space which fulfils generalized periodicity by construction. Our
method naturally allows us to impose general macroscopic fields
(strains/stresses and electric fields/electric displacements) along arbitrary
directions, enabling the characterization of the material anisotropy. We apply
the proposed method to study periodic architected metamaterials with apparent
piezoelectricity. We first verify the method by directly comparing the results
with a large periodic structure, then apply it to evaluate the anisotropic
apparently piezoelectricity of a geometrically polarized 2D lattice, and
finally demonstrate the application of the method in a 3D architected
metamaterial.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08198" title="Abstract">arXiv:2311.08198</a> [<a href="/pdf/2311.08198" title="Download PDF">pdf</a>, <a href="/format/2311.08198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A High-Frequency Load-Store Queue with Speculative Allocations for  High-Level Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Szafarczyk%2C+R">Robert Szafarczyk</a>, 
<a href="/search/cs?searchtype=author&query=Nabi%2C+S+W">Syed Waqar Nabi</a>, 
<a href="/search/cs?searchtype=author&query=Vanderbauwhede%2C+W">Wim Vanderbauwhede</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the International Conference on Field Programmable Technology (FPT'23), Yokohama, Japan, 11-14 December 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Performance (cs.PF)

</div>
<p class="mathjax">Dynamically scheduled high-level synthesis (HLS) enables the use of
load-store queues (LSQs) which can disambiguate data hazards at circuit
runtime, increasing throughput in codes with unpredictable memory accesses.
However, the increased throughput comes at the price of lower clock frequency
and higher resource usage compared to statically scheduled circuits without
LSQs. The lower frequency often nullifies any throughput improvements over
static scheduling, while the resource usage becomes prohibitively expensive
with large queue sizes. This paper presents a method for achieving dynamically
scheduled memory operations in HLS without significant clock period and
resource usage increase. We present a novel LSQ based on shift-registers
enabled by the opportunity to specialize queue sizes to a target code in HLS.
We show a method to speculatively allocate addresses to our LSQ, significantly
increasing pipeline parallelism in codes that could not benefit from an LSQ
before. In stark contrast to traditional load value speculation, we do not
require pipeline replays and have no overhead on misspeculation. On a set of
benchmarks with data hazards, our approach achieves an average speedup of
11$\times$ against static HLS and 5$\times$ against dynamic HLS that uses a
state of the art LSQ from previous work. Our LSQ also uses several times fewer
resources, scaling to queues with hundreds of entries, and supports both
on-chip and off-chip memory.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08202" title="Abstract">arXiv:2311.08202</a> [<a href="/pdf/2311.08202" title="Download PDF">pdf</a>, <a href="/format/2311.08202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Skewed Label Learning with Logits Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Runhan Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Hao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xuefeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Sheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Min Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+B">Bo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyuan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning (FL) aims to collaboratively train a shared model across
multiple clients without transmitting their local data. Data heterogeneity is a
critical challenge in realistic FL settings, as it causes significant
performance deterioration due to discrepancies in optimization among local
models. In this work, we focus on label distribution skew, a common scenario in
data heterogeneity, where the data label categories are imbalanced on each
client. To address this issue, we propose FedBalance, which corrects the
optimization bias among local models by calibrating their logits. Specifically,
we introduce an extra private weak learner on the client side, which forms an
ensemble model with the local model. By fusing the logits of the two models,
the private weak learner can capture the variance of different data, regardless
of their category. Therefore, the optimization direction of local models can be
improved by increasing the penalty for misclassifying minority classes and
reducing the attention to majority classes, resulting in a better global model.
Extensive experiments show that our method can gain 13\% higher average
accuracy compared with state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08204" title="Abstract">arXiv:2311.08204</a> [<a href="/pdf/2311.08204" title="Download PDF">pdf</a>, <a href="/format/2311.08204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Evaluation of Collision Probability along a Path
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paiola%2C+L">Lorenzo Paiola</a>, 
<a href="/search/cs?searchtype=author&query=Grioli%2C+G">Giorgio Grioli</a>, 
<a href="/search/cs?searchtype=author&query=Bicchi%2C+A">Antonio Bicchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Characterizing the risk of operations is a fundamental requirement in
robotics, and a crucial ingredient of safe planning. The problem is
multifaceted, with multiple definitions arising in the vast recent literature
fitting different application scenarios and leading to different computational
approaches. A basic element shared by most frameworks is the definition and
evaluation of the probability of collision for a mobile object in an
environment with obstacles. We observe that, even in basic cases, different
interpretations are possible. This paper proposes an index we call Risk
Density, which offers a theoretical link between conceptually distant
assumptions about the interplay of single collision events along a continuous
path. We show how this index can be used to approximate the collision
probability in the case where the robot evolves along a nominal continuous
curve from random initial conditions. Indeed under this hypothesis the proposed
approximation outperforms some well-established methods either in accuracy or
computational cost.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08205" title="Abstract">arXiv:2311.08205</a> [<a href="/pdf/2311.08205" title="Download PDF">pdf</a>, <a href="/format/2311.08205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increasing the Efficiency of Cryptoasset Investigations by Connecting  the Cases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haslhofer%2C+B">Bernhard Haslhofer</a>, 
<a href="/search/cs?searchtype=author&query=Hanslbauer%2C+C">Christiane Hanslbauer</a>, 
<a href="/search/cs?searchtype=author&query=Fr%C3%B6wis%2C+M">Michael Fr&#xf6;wis</a>, 
<a href="/search/cs?searchtype=author&query=Goger%2C+T">Thomas Goger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Law enforcement agencies are confronted with a rapidly growing number of
cryptoasset-related cases, often redundantly investigating the same cases
without mutual knowledge or shared insights. In this paper, we explore the
hypothesis that recognizing and acting upon connections between these cases can
significantly streamline investigative processes. Through an analysis of a
dataset comprising 34 cyberfraud and 1793 sextortion spam cases, we discovered
that 41% of the cyberfraud and 96.9% of the sextortion spam incidents can be
interconnected. We introduce a straightforward yet effective tool, which is
integrated into a broader cryptoasset forensics workflow and allows
investigators to highlight and share case connections. Our research
unequivocally demonstrates that recognizing case connections can lead to
remarkable efficiencies, especially when extended across crime areas,
international borders, and jurisdictions.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08206" title="Abstract">arXiv:2311.08206</a> [<a href="/pdf/2311.08206" title="Download PDF">pdf</a>, <a href="/ps/2311.08206" title="Download PostScript">ps</a>, <a href="/format/2311.08206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Centric Autonomous Systems With LLMs for User Command Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Ci Li</a>, 
<a href="/search/cs?searchtype=author&query=Marta%2C+D+S">Daniel Sim&#xf5;es Marta</a>, 
<a href="/search/cs?searchtype=author&query=Batool%2C+N">Nazre Batool</a>, 
<a href="/search/cs?searchtype=author&query=Folkesson%2C+J">John Folkesson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, accepted by WACV LLVM-AD workshp, code <a href="https://github.com/KTH-RPL/DriveCmd_LLM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">The evolution of autonomous driving has made remarkable advancements in
recent years, evolving into a tangible reality. However, a human-centric
large-scale adoption hinges on meeting a variety of multifaceted requirements.
To ensure that the autonomous system meets the user's intent, it is essential
to accurately discern and interpret user commands, especially in complex or
emergency situations. To this end, we propose to leverage the reasoning
capabilities of Large Language Models (LLMs) to infer system requirements from
in-cabin users' commands. Through a series of experiments that include
different LLM models and prompt designs, we explore the few-shot multivariate
binary classification accuracy of system requirements from natural language
textual commands. We confirm the general ability of LLMs to understand and
reason about prompts but underline that their effectiveness is conditioned on
the quality of both the LLM model and the design of appropriate sequential
prompts. Code and models are public with the link
\url{https://github.com/KTH-RPL/DriveCmd_LLM}.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08207" title="Abstract">arXiv:2311.08207</a> [<a href="/pdf/2311.08207" title="Download PDF">pdf</a>, <a href="/format/2311.08207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Data-driven Control Against False Data Injection Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lidong Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+F">Fang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jie Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The rise of cyber-security concerns has brought significant attention to the
analysis and design of cyber-physical systems (CPSs). Among the various types
of cyberattacks, denial-of-service (DoS) attacks and false data injection (FDI)
attacks can be easily launched and have become prominent threats. While
resilient control against DoS attacks has received substantial research
efforts, countermeasures developed against FDI attacks have been relatively
limited, particularly when explicit system models are not available. To address
this gap, the present paper focuses on the design of data-driven controllers
for unknown linear systems subject to FDI attacks on the actuators, utilizing
input-state data. To this end, a general FDI attack model is presented, which
imposes minimally constraints on the switching frequency of attack channels and
the magnitude of attack matrices. A dynamic state feedback control law is
designed based on offline and online input-state data, which adapts to the
channel switching of FDI attacks. This is achieved by solving two data-based
semi-definite programs (SDPs) on-the-fly to yield a tight approximation of the
set of subsystems consistent with both offline clean data and online
attack-corrupted data. It is shown that under mild conditions on the attack,
the proposed SDPs are recursively feasible and controller achieves exponential
stability. Numerical examples showcase its effectiveness in mitigating the
impact of FDI attacks.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08213" title="Abstract">arXiv:2311.08213</a> [<a href="/pdf/2311.08213" title="Download PDF">pdf</a>, <a href="/format/2311.08213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlock the Power: Competitive Distillation for Multi-Modal Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Li Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recently, multi-modal content generation has attracted lots of attention from
researchers by investigating the utilization of visual instruction tuning based
on large language models (LLMs). To enhance the performance and generalization
ability of such LLMs, the practice of distilling knowledge from pretrained
multi-modal models (a.k.a. teachers) to more compact multi-modal LLMs
(students) has gained considerable interest. However, the prevailing paradigm
of instructiontuning in multi-modal LLMs knowledge distillation is
resource-intensive and unidirectional, neglecting the potential for mutual
feedback between the student and teacher models. Thus, we propose an innovative
Competitive Multi-modal Distillation framework (CoMD), which captures
bidirectional feedback between teacher and student models and continually
updates the multi-modal capabilities that the student model has learned. It
comprises two stages: multi-modal pre-training and multi-modal competitive
distillation. The first stage pre-trains the student model on a large number of
filtered multi-modal datasets. The second stage facilitates a bidirectional
knowledge transfer between the student and teacher models. Our experimental
analysis of diverse datasets shows that our knowledge transfer method
consistently improves the capabilities of the student model. Finally, the
7B-sized student model after four distillations surpassed the current
state-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also
outperforms other strong baselines in the zero-shot setting.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08217" title="Abstract">arXiv:2311.08217</a> [<a href="/pdf/2311.08217" title="Download PDF">pdf</a>, <a href="/format/2311.08217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot  Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rui%2C+X">Xue Rui</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+C">Chao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+J">Jiaxu Leng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Few-shot image generation aims to train generative models using a small
number of training images. When there are few images available for training
(e.g. 10 images), Learning From Scratch (LFS) methods often generate images
that closely resemble the training data while Transfer Learning (TL) methods
try to improve performance by leveraging prior knowledge from GANs pre-trained
on large-scale datasets. However, current TL methods may not allow for
sufficient control over the degree of knowledge preservation from the source
model, making them unsuitable for setups where the source and target domains
are not closely related. To address this, we propose a novel pipeline called
Peer is your Pillar (PIP), which combines a target few-shot dataset with a peer
dataset to create a data-unbalanced conditional generation. Our approach
includes a class embedding method that separates the class space from the
latent space, and we use a direction loss based on pre-trained CLIP to improve
image diversity. Experiments on various few-shot datasets demonstrate the
advancement of the proposed PIP, especially reduces the training requirements
of few-shot image generation.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08219" title="Abstract">arXiv:2311.08219</a> [<a href="/pdf/2311.08219" title="Download PDF">pdf</a>, <a href="/format/2311.08219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eval-GCSC: A New Metric for Evaluating ChatGPT&#x27;s Performance in Chinese  Spelling Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kunting Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shaolei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hanhan Ma</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">ChatGPT has demonstrated impressive performance in various downstream tasks.
However, in the Chinese Spelling Correction (CSC) task, we observe a
discrepancy: while ChatGPT performs well under human evaluation, it scores
poorly according to traditional metrics. We believe this inconsistency arises
because the traditional metrics are not well-suited for evaluating generative
models. Their overly strict length and phonics constraints may lead to
underestimating ChatGPT's correction capabilities. To better evaluate
generative models in the CSC task, this paper proposes a new evaluation metric:
Eval-GCSC. By incorporating word-level and semantic similarity judgments, it
relaxes the stringent length and phonics constraints. Experimental results show
that Eval-GCSC closely aligns with human evaluations. Under this metric,
ChatGPT's performance is comparable to traditional token-level classification
models (TCM), demonstrating its potential as a CSC tool. The source code and
scripts can be accessed at https://github.com/ktlKTL/Eval-GCSC.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08220" title="Abstract">arXiv:2311.08220</a> [<a href="/pdf/2311.08220" title="Download PDF">pdf</a>, <a href="/format/2311.08220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State-Dependent Channels with a Message-Cognizant Helper
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lapidoth%2C+A">Amos Lapidoth</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Ligong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yiming Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The capacity of a state-dependent discrete memoryless channel (SD-DMC) is
derived for the setting where a message-cognizant rate-limited helper observes
the state sequence noncausally, produces its description, and provides the
description to both encoder and decoder.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08223" title="Abstract">arXiv:2311.08223</a> [<a href="/pdf/2311.08223" title="Download PDF">pdf</a>, <a href="/format/2311.08223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Image Captioning via Predicting Structured Concepts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weidong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuanhe Tian</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yan Song</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhendong Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures. Published at EMNLP 2023 (Main Conference, Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Having the difficulty of solving the semantic gap between images and texts
for the image captioning task, conventional studies in this area paid some
attention to treating semantic concepts as a bridge between the two modalities
and improved captioning performance accordingly. Although promising results on
concept prediction were obtained, the aforementioned studies normally ignore
the relationship among concepts, which relies on not only objects in the image,
but also word dependencies in the text, so that offers a considerable potential
for improving the process of generating good descriptions. In this paper, we
propose a structured concept predictor (SCP) to predict concepts and their
structures, then we integrate them into captioning, so as to enhance the
contribution of visual signals in this task via concepts and further use their
relations to distinguish cross-modal semantics for better description
generation. Particularly, we design weighted graph convolutional networks
(W-GCN) to depict concept relations driven by word dependencies, and then
learns differentiated contributions from these concepts for following decoding
process. Therefore, our approach captures potential relations among concepts
and discriminatively learns different concepts, so that effectively facilitates
image captioning with inherited information across modalities. Extensive
experiments and their results demonstrate the effectiveness of our approach as
well as each proposed module in this work.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08227" title="Abstract">arXiv:2311.08227</a> [<a href="/pdf/2311.08227" title="Download PDF">pdf</a>, <a href="/format/2311.08227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction of inter packet arrival times for enhanced NR-V2X sidelink  scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCarthy%2C+B">Brian McCarthy</a>, 
<a href="/search/cs?searchtype=author&query=O%27Driscoll%2C+A">Aisling O&#x27;Driscoll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">A significant limitation of the LTE-V2X and NR-V2X sidelink scheduling
mechanisms is their difficulty coping with variations in inter packet arrival
times, also known as aperiodic packets. This conflicts with the fundamental
characteristics of most V2X services which are triggered based on an event.
e.g. ETSI Cooperative Awareness Messages (CAMs) - vehicle kinematics,
Cooperative Perception Messages (CPMs) - object sensing and Decentralised Event
Notification Messages (DENMs) - event occurrences. Furthermore, network
management techniques such as congestion control mechanisms can result in
varied inter packet arrival times. To combat this, NR-V2X introduced a dynamic
grant mechanism, which we show is ineffective unless there is background
periodic traffic to stabilise the sensing history upon which the scheduler
makes it decisions. The characteristics of V2X services make it implausible
that such periodic application traffic will exist.
<br />To overcome this significant drawback, we demonstrate that the standardised
scheduling algorithms can be made effective if the event triggered arrival rate
of packets can be accurately predicted. These predictions can be used to tune
the Resource Reservation Interval (RRI) parameter of the MAC scheduler to
negate the negative impact of aperiodicity. Such an approach allows the
scheduler to achieve comparable performance to a scenario where packets arrive
periodically. To demonstrate the effectiveness of our approach, an ML model has
been devised for the prediction of cooperative awareness messages, but the same
principle can be abstracted to other V2X service types.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08228" title="Abstract">arXiv:2311.08228</a> [<a href="/pdf/2311.08228" title="Download PDF">pdf</a>, <a href="/format/2311.08228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Explanation for Regression via Disentanglement in Latent  Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Broelemann%2C+K">Klaus Broelemann</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+G">Gjergji Kasneci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CXAI workshop @ ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Counterfactual Explanations (CEs) help address the question: How can the
factors that influence the prediction of a predictive model be changed to
achieve a more favorable outcome from a user's perspective? Thus, they bear the
potential to guide the user's interaction with AI systems since they represent
easy-to-understand explanations. To be applicable, CEs need to be realistic and
actionable. In the literature, various methods have been proposed to generate
CEs. However, the majority of research on CEs focuses on classification
problems where questions like ``What should I do to get my rejected loan
approved?" are raised. In practice, answering questions like ``What should I do
to increase my salary?" are of a more regressive nature. In this paper, we
introduce a novel method to generate CEs for a pre-trained regressor by first
disentangling the label-relevant from the label-irrelevant dimensions in the
latent space. CEs are then generated by combining the label-irrelevant
dimensions and the predefined output. The intuition behind this approach is
that the ideal counterfactual search should focus on the label-irrelevant
characteristics of the input and suggest changes toward target-relevant
characteristics. Searching in the latent space could help achieve this goal. We
show that our method maintains the characteristics of the query sample during
the counterfactual search. In various experiments, we demonstrate that the
proposed method is competitive based on different quality measures on image and
tabular datasets in regression problem settings. It efficiently returns results
closer to the original data manifold compared to three state-of-the-art
methods, which is essential for realistic high-dimensional machine learning
applications. Our code will be made available as an open-source package upon
the publication of this work.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08234" title="Abstract">arXiv:2311.08234</a> [<a href="/pdf/2311.08234" title="Download PDF">pdf</a>, <a href="/format/2311.08234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Centralized Intermediation in a Decentralized Web3 Economy: Value  Accrual and Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+D">Dipankar Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The advent of Web3 has ushered in a new era of decentralized digital economy,
promising a shift from centralized authority to distributed, peer-to-peer
interactions. However, the underlying infrastructure of this decentralized
ecosystem often relies on centralized cloud providers, creating a paradoxical
concentration of value and power. This paper investigates the mechanics of
value accrual and extraction within the Web3 ecosystem, focusing on the roles
and revenues of centralized clouds. Through an analysis of publicly available
material, we elucidate the financial implications of cloud services in
purportedly decentralized contexts. We further explore the individual's
perspective of value creation and accumulation, examining the interplay between
user participation and centralized monetization strategies. Key findings
indicate that while blockchain technology has the potential to significantly
reduce infrastructure costs for financial services, the current Web3 landscape
is marked by a substantial reliance on cloud providers for hosting,
scalability, and performance.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08235" title="Abstract">arXiv:2311.08235</a> [<a href="/pdf/2311.08235" title="Download PDF">pdf</a>, <a href="/format/2311.08235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inference of Probabilistic Programs with Moment-Matching Gaussian  Mixtures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Randone%2C+F">Francesca Randone</a>, 
<a href="/search/cs?searchtype=author&query=Bortolussi%2C+L">Luca Bortolussi</a>, 
<a href="/search/cs?searchtype=author&query=Incerto%2C+E">Emilio Incerto</a>, 
<a href="/search/cs?searchtype=author&query=Tribastone%2C+M">Mirco Tribastone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Computing the posterior distribution of a probabilistic program is a hard
task for which no one-fit-for-all solution exists. We propose Gaussian
Semantics, which approximates the exact probabilistic semantics of a bounded
program by means of Gaussian mixtures. It is parametrized by a map that
associates each program location with the moment order to be matched in the
approximation. We provide two main contributions. The first is a universal
approximation theorem stating that, under mild conditions, Gaussian Semantics
can approximate the exact semantics arbitrarily closely. The second is an
approximation that matches up to second-order moments analytically in face of
the generally difficult problem of matching moments of Gaussian mixtures with
arbitrary moment order. We test our second-order Gaussian approximation (SOGA)
on a number of case studies from the literature. We show that it can provide
accurate estimates in models not supported by other approximation methods or
when exact symbolic techniques fail because of complex expressions or
non-simplified integrals. On two notable classes of problems, namely
collaborative filtering and programs involving mixtures of continuous and
discrete distributions, we show that SOGA significantly outperforms alternative
techniques in terms of accuracy and computational time.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08236" title="Abstract">arXiv:2311.08236</a> [<a href="/pdf/2311.08236" title="Download PDF">pdf</a>, <a href="/format/2311.08236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MeLo: Low-rank Adaptation is Better than Fine-tuning for Medical Image  Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yitao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhenrong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zihao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+D">Dinggang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The common practice in developing computer-aided diagnosis (CAD) models based
on transformer architectures usually involves fine-tuning from ImageNet
pre-trained weights. However, with recent advances in large-scale pre-training
and the practice of scaling laws, Vision Transformers (ViT) have become much
larger and less accessible to medical imaging communities. Additionally, in
real-world scenarios, the deployments of multiple CAD models can be troublesome
due to problems such as limited storage space and time-consuming model
switching. To address these challenges, we propose a new method MeLo (Medical
image Low-rank adaptation), which enables the development of a single CAD model
for multiple clinical tasks in a lightweight manner. It adopts low-rank
adaptation instead of resource-demanding fine-tuning. By fixing the weight of
ViT models and only adding small low-rank plug-ins, we achieve competitive
results on various diagnosis tasks across different imaging modalities using
only a few trainable parameters. Specifically, our proposed method achieves
comparable performance to fully fine-tuned ViT models on four distinct medical
imaging datasets using about 0.17% trainable parameters. Moreover, MeLo adds
only about 0.5MB of storage space and allows for extremely fast model switching
in deployment and inference. Our source code and pre-trained weights are
available on our website (https://absterzhu.github.io/melo.github.io/).
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08240" title="Abstract">arXiv:2311.08240</a> [<a href="/pdf/2311.08240" title="Download PDF">pdf</a>, <a href="/format/2311.08240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Encoding of Words in BERT&#x27;s Neurons using Feature  Textualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baeumel%2C+T">Tanja Baeumel</a>, 
<a href="/search/cs?searchtype=author&query=Vijayakumar%2C+S">Soniya Vijayakumar</a>, 
<a href="/search/cs?searchtype=author&query=van+Genabith%2C+J">Josef van Genabith</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+G">Guenter Neumann</a>, 
<a href="/search/cs?searchtype=author&query=Ostermann%2C+S">Simon Ostermann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in 'BlackboxNLP 2023: The 6th Workshop on Analysing and Interpreting Neural Networks for NLP'. Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pretrained language models (PLMs) form the basis of most state-of-the-art NLP
technologies. Nevertheless, they are essentially black boxes: Humans do not
have a clear understanding of what knowledge is encoded in different parts of
the models, especially in individual neurons. The situation is different in
computer vision, where feature visualization provides a decompositional
interpretability technique for neurons of vision models. Activation
maximization is used to synthesize inherently interpretable visual
representations of the information encoded in individual neurons. Our work is
inspired by this but presents a cautionary tale on the interpretability of
single neurons, based on the first large-scale attempt to adapt activation
maximization to NLP, and, more specifically, large PLMs. We propose feature
textualization, a technique to produce dense representations of neurons in the
PLM word embedding space. We apply feature textualization to the BERT model
(Devlin et al., 2019) to investigate whether the knowledge encoded in
individual neurons can be interpreted and symbolized. We find that the produced
representations can provide insights about the knowledge encoded in individual
neurons, but that individual neurons do not represent clearcut symbolic units
of language such as words. Additionally, we use feature textualization to
investigate how many neurons are needed to encode words in BERT.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08244" title="Abstract">arXiv:2311.08244</a> [<a href="/pdf/2311.08244" title="Download PDF">pdf</a>, <a href="/format/2311.08244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language and Sketching: An LLM-driven Interactive Multimodal Multitask  Robot Navigation Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zu%2C+W">Weiqin Zu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Wenbin Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruiqing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Ze Guo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fanglei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zheng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Wei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The socially-aware navigation system has evolved to adeptly avoid various
obstacles while performing multiple tasks, such as point-to-point navigation,
human-following, and -guiding. However, a prominent gap persists: in
Human-Robot Interaction (HRI), the procedure of communicating commands to
robots demands intricate mathematical formulations. Furthermore, the transition
between tasks does not quite possess the intuitive control and user-centric
interactivity that one would desire. In this work, we propose an LLM-driven
interactive multimodal multitask robot navigation framework, termed LIM2N, to
solve the above new challenge in the navigation field. We achieve this by first
introducing a multimodal interaction framework where language and hand-drawn
inputs can serve as navigation constraints and control objectives. Next, a
reinforcement learning agent is built to handle multiple tasks with the
received information. Crucially, LIM2N creates smooth cooperation among the
reasoning of multimodal input, multitask planning, and adaptation and
processing of the intelligent sensing modules in the complicated system.
Extensive experiments are conducted in both simulation and the real world
demonstrating that LIM2N has superior user needs understanding, alongside an
enhanced interactive experience.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08245" title="Abstract">arXiv:2311.08245</a> [<a href="/pdf/2311.08245" title="Download PDF">pdf</a>, <a href="/format/2311.08245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yunjiao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianfei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+H">Han Zou</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lihua Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint manuscript in submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent achievements in language models have showcased their extraordinary
capabilities in bridging visual information with semantic language
understanding. This leads us to a novel question: can language models connect
textual semantics with IoT sensory signals to perform recognition tasks, e.g.,
Human Activity Recognition (HAR)? If so, an intelligent HAR system with
human-like cognition can be built, capable of adapting to new environments and
unseen categories. This paper explores its feasibility with an innovative
approach, IoT-sEnsors-language alignmEnt pre-Training (TENT), which jointly
aligns textual embeddings with IoT sensor signals, including camera video,
LiDAR, and mmWave. Through the IoT-language contrastive learning, we derive a
unified semantic feature space that aligns multi-modal features with language
embeddings, so that the IoT data corresponds to specific words that describe
the IoT data. To enhance the connection between textual categories and their
IoT data, we propose supplementary descriptions and learnable prompts that
bring more semantic information into the joint feature space. TENT can not only
recognize actions that have been seen but also ``guess'' the unseen action by
the closest textual words from the feature space. We demonstrate TENT achieves
state-of-the-art performance on zero-shot HAR tasks using different modalities,
improving the best vision-language models by over 12%.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08249" title="Abstract">arXiv:2311.08249</a> [<a href="/pdf/2311.08249" title="Download PDF">pdf</a>, <a href="/format/2311.08249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Using Distribution-Based Compositionality Assessment to Evaluate  Compositional Generalisation in Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moisio%2C+A">Anssi Moisio</a>, 
<a href="/search/cs?searchtype=author&query=Creutz%2C+M">Mathias Creutz</a>, 
<a href="/search/cs?searchtype=author&query=Kurimo%2C+M">Mikko Kurimo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at the GenBench Workshop at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Compositional generalisation (CG), in NLP and in machine learning more
generally, has been assessed mostly using artificial datasets. It is important
to develop benchmarks to assess CG also in real-world natural language tasks in
order to understand the abilities and limitations of systems deployed in the
wild. To this end, our GenBench Collaborative Benchmarking Task submission
utilises the distribution-based compositionality assessment (DBCA) framework to
split the Europarl translation corpus into a training and a test set in such a
way that the test set requires compositional generalisation capacity.
Specifically, the training and test sets have divergent distributions of
dependency relations, testing NMT systems' capability of translating
dependencies that they have not been trained on. This is a fully-automated
procedure to create natural language compositionality benchmarks, making it
simple and inexpensive to apply it further to other datasets and languages. The
code and data for the experiments is available at
https://github.com/aalto-speech/dbca.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08252" title="Abstract">arXiv:2311.08252</a> [<a href="/pdf/2311.08252" title="Download PDF">pdf</a>, <a href="/format/2311.08252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REST: Retrieval-Based Speculative Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhenyu He</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zexuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Tianle Cai</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D Lee</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Di He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm
designed to speed up language model generation. The key insight driving the
development of REST is the observation that the process of text generation
often includes certain common phases and patterns. Unlike previous methods that
rely on a draft language model for speculative decoding, REST harnesses the
power of retrieval to generate draft tokens. This method draws from the
reservoir of existing knowledge, retrieving and employing relevant tokens based
on the current context. Its plug-and-play nature allows for seamless
integration and acceleration of any language models, all without necessitating
additional training. When benchmarked on 7B and 13B language models in a
single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on
code or text generation. The code of REST is available at
https://github.com/FasterDecoding/REST.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08255" title="Abstract">arXiv:2311.08255</a> [<a href="/pdf/2311.08255" title="Download PDF">pdf</a>, <a href="/ps/2311.08255" title="Download PostScript">ps</a>, <a href="/format/2311.08255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Dynamics of Delayed Feedback in Robot Teleoperation: Insights  from fNIRS Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Vann%2C+W">William Vann</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jing Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Frontiers in Human Neuroscience
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">As robot teleoperation increasingly becomes integral in executing tasks in
distant, hazardous, or inaccessible environments, the challenge of operational
delays remains a significant obstacle. These delays are inherent in signal
transmission and processing and can adversely affect the operators performance,
particularly in tasks requiring precision and timeliness. While current
research has made strides in mitigating these delays through advanced control
strategies and training methods, a crucial gap persists in understanding the
neurofunctional impacts of these delays and the efficacy of countermeasures
from a cognitive perspective. Our study narrows this gap by leveraging
functional Near-Infrared Spectroscopy (fNIRS) to examine the neurofunctional
implications of simulated haptic feedback on cognitive activity and motor
coordination under delayed conditions. In a human-subject experiment (N=41), we
manipulated sensory feedback to observe its influences on various brain regions
of interest (ROIs) response during teleoperation tasks. The fNIRS data provided
a detailed assessment of cerebral activity, particularly in ROIs implicated in
time perception and the execution of precise movements. Our results reveal that
certain conditions, which provided immediate simulated haptic feedback,
significantly optimized neural functions related to time perception and motor
coordination, and improved motor performance. These findings provide empirical
evidence about the neurofunctional basis of the enhanced motor performance with
simulated synthetic force feedback in the presence of teleoperation delays.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08258" title="Abstract">arXiv:2311.08258</a> [<a href="/pdf/2311.08258" title="Download PDF">pdf</a>, <a href="/ps/2311.08258" title="Download PostScript">ps</a>, <a href="/format/2311.08258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unprecedented reach and recruitment paths for hate and extremism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sear%2C+R">Richard Sear</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+N+F">Neil F. Johnson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Human-Computer Interaction (cs.HC); Adaptation and Self-Organizing Systems (nlin.AO); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Analyzing a unique real-time dataset from across 26 social media platforms,
we show why the hate-extremism ecosystem now has unprecedented reach and
recruitment paths online; why it is now able to exert instant and massive
global mainstream influence, e.g. following the October 7 Hamas attack; why it
will become increasingly robust in 2024 and beyond; why recent E.U. laws fall
short because the effect of many smaller, lesser-known platforms outstrips
larger ones like Twitter; and why law enforcement should expect increasingly
hard-to-understand paths ahead of offline mass attacks. This new picture of
online hate and extremism challenges current notions of a niche activity at the
'fringe' of the Internet driven by specific news sources. But it also suggests
a new opportunity for system-wide control akin to adaptive vs. extinction
treatments for cancer.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08263" title="Abstract">arXiv:2311.08263</a> [<a href="/pdf/2311.08263" title="Download PDF">pdf</a>, <a href="/format/2311.08263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads  to Answers Faster
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhining Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiaqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+C">Chenyi Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guihai Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this work, we propose FastCoT, a model-agnostic framework based on
parallel decoding without any further training of an auxiliary model or
modification to the LLM itself. FastCoT uses a size-varying context window
whose size changes with position to conduct parallel decoding and
auto-regressive decoding simultaneously, thus fully utilizing GPU computation
resources. In FastCoT, the parallel decoding part provides the LLM with a quick
glance of the future composed of approximate tokens, which could lead to faster
answers compared to regular autoregressive decoding used by causal
transformers. We also provide an implementation of parallel decoding within
LLM, which supports KV-cache generation and batch processing. Through extensive
experiments, we demonstrate that FastCoT saves inference time by nearly 20%
with only a negligible performance drop compared to the regular approach.
Additionally, we show that the context window size exhibits considerable
robustness for different tasks.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08265" title="Abstract">arXiv:2311.08265</a> [<a href="/pdf/2311.08265" title="Download PDF">pdf</a>, <a href="/format/2311.08265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Relationship Between Universal Adversarial Attacks And Sparse  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weitzner%2C+D">Dana Weitzner</a>, 
<a href="/search/cs?searchtype=author&query=Giryes%2C+R">Raja Giryes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The prominent success of neural networks, mainly in computer vision tasks, is
increasingly shadowed by their sensitivity to small, barely perceivable
adversarial perturbations in image input.
<br />In this work, we aim at explaining this vulnerability through the framework
of sparsity.
<br />We show the connection between adversarial attacks and sparse
representations, with a focus on explaining the universality and
transferability of adversarial examples in neural networks.
<br />To this end, we show that sparse coding algorithms, and the neural
network-based learned iterative shrinkage thresholding algorithm (LISTA) among
them, suffer from this sensitivity, and that common attacks on neural networks
can be expressed as attacks on the sparse representation of the input image.
The phenomenon that we observe holds true also when the network is agnostic to
the sparse representation and dictionary, and thus can provide a possible
explanation for the universality and transferability of adversarial attacks.
<br />The code is available at
https://github.com/danawr/adversarial_attacks_and_sparse_representations.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08268" title="Abstract">arXiv:2311.08268</a> [<a href="/pdf/2311.08268" title="Download PDF">pdf</a>, <a href="/format/2311.08268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Wolf in Sheep&#x27;s Clothing: Generalized Nested Jailbreak Prompts can  Fool Large Language Models Easily
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+P">Peng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+J">Jun Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Dan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xuezhi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Yunsen Xian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to
provide useful and safe responses. However, adversarial prompts known as
'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful
content. Exploring jailbreak prompts can help to better reveal the weaknesses
of LLMs and further steer us to secure them. Unfortunately, existing jailbreak
methods either suffer from intricate manual design or require optimization on
another white-box model, compromising generalization or jailbreak efficiency.
In this paper, we generalize jailbreak prompt attacks into two aspects: (1)
Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM,
an automatic framework that leverages LLMs themselves to generate effective
jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly
improves the attack success rate while greatly reducing the time cost compared
to existing baselines. Our study also reveals the inadequacy of current defense
methods in safeguarding LLMs. Finally, we offer detailed analysis and
discussion from the perspective of prompt execution priority on the failure of
LLMs' defense. We hope that our research can catalyze both the academic
community and LLMs vendors towards the provision of safer and more regulated
Large Language Models.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08271" title="Abstract">arXiv:2311.08271</a> [<a href="/pdf/2311.08271" title="Download PDF">pdf</a>, <a href="/format/2311.08271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mobility-Induced Graph Learning for WiFi Positioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kyuwon Han</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S+M">Seung Min Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seong-Lyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+S">Seung-Woo Ko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to a possible IEEE journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">A smartphone-based user mobility tracking could be effective in finding
his/her location, while the unpredictable error therein due to low
specification of built-in inertial measurement units (IMUs) rejects its
standalone usage but demands the integration to another positioning technique
like WiFi positioning. This paper aims to propose a novel integration technique
using a graph neural network called Mobility-INduced Graph LEarning (MINGLE),
which is designed based on two types of graphs made by capturing different user
mobility features. Specifically, considering sequential measurement points
(MPs) as nodes, a user's regular mobility pattern allows us to connect neighbor
MPs as edges, called time-driven mobility graph (TMG). Second, a user's
relatively straight transition at a constant pace when moving from one position
to another can be captured by connecting the nodes on each path, called a
direction-driven mobility graph (DMG). Then, we can design graph convolution
network (GCN)-based cross-graph learning, where two different GCN models for
TMG and DMG are jointly trained by feeding different input features created by
WiFi RTTs yet sharing their weights. Besides, the loss function includes a
mobility regularization term such that the differences between adjacent
location estimates should be less variant due to the user's stable moving pace.
Noting that the regularization term does not require ground-truth location,
MINGLE can be designed under semi- and self-supervised learning frameworks. The
proposed MINGLE's effectiveness is extensively verified through field
experiments, showing a better positioning accuracy than benchmarks, say root
mean square errors (RMSEs) being 1.398 (m) and 1.073 (m) for self- and
semi-supervised learning cases, respectively.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08272" title="Abstract">arXiv:2311.08272</a> [<a href="/pdf/2311.08272" title="Download PDF">pdf</a>, <a href="/format/2311.08272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Attention Network for Cross-domain Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+G">Guanyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Jianxin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yanan Niu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yang Song</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Depeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In modern recommender systems, sequential recommendation leverages
chronological user behaviors to make effective next-item suggestions, which
suffers from data sparsity issues, especially for new users. One promising line
of work is the cross-domain recommendation, which trains models with data
across multiple domains to improve the performance in data-scarce domains.
Recent proposed cross-domain sequential recommendation models such as PiNet and
DASL have a common drawback relying heavily on overlapped users in different
domains, which limits their usage in practical recommender systems. In this
paper, we propose a Mixed Attention Network (MAN) with local and global
attention modules to extract the domain-specific and cross-domain information.
Firstly, we propose a local/global encoding layer to capture the
domain-specific/cross-domain sequential pattern. Then we propose a mixed
attention layer with item similarity attention, sequence-fusion attention, and
group-prototype attention to capture the local/global item similarity, fuse the
local/global item sequence, and extract the user groups across different
domains, respectively. Finally, we propose a local/global prediction layer to
further evolve and combine the domain-specific and cross-domain interests.
Experimental results on two real-world datasets (each with two domains)
demonstrate the superiority of our proposed model. Further study also
illustrates that our proposed method and components are model-agnostic and
effective, respectively. The code and data are available at
https://github.com/Guanyu-Lin/MAN.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08273" title="Abstract">arXiv:2311.08273</a> [<a href="/pdf/2311.08273" title="Download PDF">pdf</a>, <a href="/format/2311.08273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Modularity in Multilingual LMs via Language-Specialized  Subnetworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choenni%2C+R">Rochelle Choenni</a>, 
<a href="/search/cs?searchtype=author&query=Shutova%2C+E">Ekaterina Shutova</a>, 
<a href="/search/cs?searchtype=author&query=Garrette%2C+D">Dan Garrette</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent work has proposed explicitly inducing language-wise modularity in
multilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a
means of better guiding cross-lingual sharing. In this work, we investigate (1)
the degree to which language-wise modularity naturally arises within models
with no special modularity interventions, and (2) how cross-lingual sharing and
interference differ between such models and those with explicit SFT-guided
subnetwork modularity. To quantify language specialization and cross-lingual
interaction, we use a Training Data Attribution method that estimates the
degree to which a model's predictions are influenced by in-language or
cross-language training examples. Our results show that language-specialized
subnetworks do naturally arise, and that SFT, rather than always increasing
modularity, can decrease language specialization of subnetworks in favor of
more cross-lingual sharing.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08274" title="Abstract">arXiv:2311.08274</a> [<a href="/pdf/2311.08274" title="Download PDF">pdf</a>, <a href="/format/2311.08274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Laccolith: Hypervisor-Based Adversary Emulation with Anti-Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Orbinato%2C+V">Vittorio Orbinato</a>, 
<a href="/search/cs?searchtype=author&query=Feliciano%2C+M+C">Marco Carlo Feliciano</a>, 
<a href="/search/cs?searchtype=author&query=Cotroneo%2C+D">Domenico Cotroneo</a>, 
<a href="/search/cs?searchtype=author&query=Natella%2C+R">Roberto Natella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Operating Systems (cs.OS)

</div>
<p class="mathjax">Advanced Persistent Threats (APTs) represent the most threatening form of
attack nowadays since they can stay undetected for a long time. Adversary
emulation is a proactive approach for preparing against these attacks. However,
adversary emulation tools lack the anti-detection abilities of APTs. We
introduce Laccolith, a hypervisor-based solution for adversary emulation with
anti-detection to fill this gap. We also present an experimental study to
compare Laccolith with MITRE CALDERA, a state-of-the-art solution for adversary
emulation, against five popular anti-virus products. We found that CALDERA
cannot evade detection, limiting the realism of emulated attacks, even when
combined with a state-of-the-art anti-detection framework. Our experiments show
that Laccolith can hide its activities from all the tested anti-virus products,
thus making it suitable for realistic emulations.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08278" title="Abstract">arXiv:2311.08278</a> [<a href="/pdf/2311.08278" title="Download PDF">pdf</a>, <a href="/format/2311.08278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARTEMIS: Using GANs with Multiple Discriminators to Generate Art
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baker%2C+J">James Baker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a novel method for generating abstract art. First an autoencoder
is trained to encode and decode the style representations of images, which are
extracted from source images with a pretrained VGG network. Then, the decoder
component of the autoencoder is extracted and used as a generator in a GAN. The
generator works with an ensemble of discriminators. Each discriminator takes
different style representations of the same images, and the generator is
trained to create images that create convincing style representations in order
to deceive all of the generators. The generator is also trained to maximize a
diversity term. The resulting images had a surreal, geometric quality. We call
our approach ARTEMIS (ARTistic Encoder- Multi- Discriminators Including
Self-Attention), as it uses the self-attention layers and an encoder-decoder
architecture.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08283" title="Abstract">arXiv:2311.08283</a> [<a href="/pdf/2311.08283" title="Download PDF">pdf</a>, <a href="/ps/2311.08283" title="Download PostScript">ps</a>, <a href="/format/2311.08283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise-Resilient Group Testing with Order-Optimal Tests and  Fast-and-Reliable Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guruswami%2C+V">Venkatesan Guruswami</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hsin-Po Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Group testing (GT) is the Boolean counterpart of compressed sensing and the
marketplace of new ideas for related problems such as cognitive radio and heavy
hitter. A GT scheme is considered good if it is nonadaptive, uses $O(k \log n)$
tests, resists noise, can be decoded in $O(k \operatorname{poly}(\log n))$
time, and makes nearly no mistakes. In this paper, we propose "Gacha GT", an
elementary, self-contained, and unified randomized scheme that, for the first
time, satisfies all criteria for a fairly large region of parameters, namely
when $\log k &lt; \log(n)^{1-1/O(1)}$. Outside this parameter region, Gacha can be
specialized to outperform the state-of-the-art partial-recovery GTs,
exact-recovery GTs, and worst-case GTs.
<br />The new idea that runs through this paper, using an analogy, is to ask every
person to break her $9$-digit "phone number" into three $3$-digit numbers $x$,
$y$, and $z$ and write $(b, x)$, $(b, y)$, and $(b, z)$ on three pieces of
sticky notes, where $b$ is her "birthday". This way, one can sort the sticky
notes by birthday to reassemble the phone numbers. This birthday--number code
and other coded constructions can be stacked like a multipartite graph pyramid.
Gacha's encoder will synthesize the test results from the bottom up; and
Gacha's decoder will reassemble the phone numbers from the top down.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08284" title="Abstract">arXiv:2311.08284</a> [<a href="/pdf/2311.08284" title="Download PDF">pdf</a>, <a href="/format/2311.08284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Level Set KSVD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sapir%2C+O">Omer Sapir</a>, 
<a href="/search/cs?searchtype=author&query=Klapp%2C+I">Iftach Klapp</a>, 
<a href="/search/cs?searchtype=author&query=Sochen%2C+N">Nir Sochen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 14 figures. Submitted to IJCV
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">We present a new algorithm for image segmentation - Level-set KSVD. Level-set
KSVD merges the methods of sparse dictionary learning for feature extraction
and variational level-set method for image segmentation. Specifically, we use a
generalization of the Chan-Vese functional with features learned by KSVD. The
motivation for this model is agriculture based. Aerial images are taken in
order to detect the spread of fungi in various crops. Our model is tested on
such images of cotton fields. The results are compared to other methods.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08287" title="Abstract">arXiv:2311.08287</a> [<a href="/pdf/2311.08287" title="Download PDF">pdf</a>, <a href="/format/2311.08287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Well Do Large Language Models Understand Syntax? An Evaluation by  Asking Natural Language Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Houquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenghua Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuebin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhefeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+X">Xinyu Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While recent advancements in large language models (LLMs) bring us closer to
achieving artificial general intelligence, the question persists: Do LLMs truly
understand language, or do they merely mimic comprehension through pattern
recognition? This study seeks to explore this question through the lens of
syntax, a crucial component of sentence comprehension. Adopting a natural
language question-answering (Q&amp;A) scheme, we craft questions targeting nine
syntactic knowledge points that are most closely related to sentence
comprehension. Experiments conducted on 24 LLMs suggest that most have a
limited grasp of syntactic knowledge, exhibiting notable discrepancies across
different syntactic knowledge points. In particular, questions involving
prepositional phrase attachment pose the greatest challenge, whereas those
concerning adjectival modifier and indirect object are relatively easier for
LLMs to handle. Furthermore, a case study on the training dynamics of the LLMs
reveals that the majority of syntactic knowledge is learned during the initial
stages of training, hinting that simply increasing the number of training
tokens may not be the `silver bullet' for improving the comprehension ability
of LLMs.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08290" title="Abstract">arXiv:2311.08290</a> [<a href="/pdf/2311.08290" title="Download PDF">pdf</a>, <a href="/format/2311.08290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-Policy Policy Gradient Reinforcement Learning Without On-Policy  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corrado%2C+N+E">Nicholas E. Corrado</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+J+P">Josiah P. Hanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">On-policy reinforcement learning (RL) algorithms perform policy updates using
i.i.d. trajectories collected by the current policy. However, after observing
only a finite number of trajectories, on-policy sampling may produce data that
fails to match the expected on-policy data distribution. This sampling error
leads to noisy updates and data inefficient on-policy learning. Recent work in
the policy evaluation setting has shown that non-i.i.d., off-policy sampling
can produce data with lower sampling error than on-policy sampling can produce.
Motivated by this observation, we introduce an adaptive, off-policy sampling
method to improve the data efficiency of on-policy policy gradient algorithms.
Our method, Proximal Robust On-Policy Sampling (PROPS), reduces sampling error
by collecting data with a behavior policy that increases the probability of
sampling actions that are under-sampled with respect to the current policy.
Rather than discarding data from old policies -- as is commonly done in
on-policy algorithms -- PROPS uses data collection to adjust the distribution
of previously collected data to be approximately on-policy. We empirically
evaluate PROPS on both continuous-action MuJoCo benchmark tasks as well as
discrete-action tasks and demonstrate that (1) PROPS decreases sampling error
throughout training and (2) improves the data efficiency of on-policy policy
gradient algorithms. Our work improves the RL community's understanding of a
nuance in the on-policy vs off-policy dichotomy: on-policy learning requires
on-policy data, not on-policy sampling.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08298" title="Abstract">arXiv:2311.08298</a> [<a href="/pdf/2311.08298" title="Download PDF">pdf</a>, <a href="/format/2311.08298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Language Model Confidence Estimation and Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+J">Jiahui Geng</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+F">Fengyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Koeppl%2C+H">Heinz Koeppl</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 1 page, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models (LMs) have demonstrated remarkable capabilities across a wide
range of tasks in various domains. Despite their impressive performance, the
reliability of their output is concerning and questionable regarding the demand
for AI safety. Assessing the confidence of LM predictions and calibrating them
across different tasks with the aim to align LM confidence with accuracy can
help mitigate risks and enable LMs to make better decisions. There have been
various works in this respect, but there has been no comprehensive overview of
this important research area. The present survey aims to bridge this gap. In
particular, we discuss methods and techniques for LM confidence estimation and
calibration, encompassing different LMs and various tasks. We further outline
the challenges of estimating the confidence for large language models and we
suggest some promising directions for future work.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08299" title="Abstract">arXiv:2311.08299</a> [<a href="/pdf/2311.08299" title="Download PDF">pdf</a>, <a href="/format/2311.08299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+D+J">Do June Min</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-Rosas%2C+V">Ver&#xf3;nica P&#xe9;rez-Rosas</a>, 
<a href="/search/cs?searchtype=author&query=Resnicow%2C+K">Kenneth Resnicow</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reflective listening is a fundamental skill that counselors must acquire to
achieve proficiency in motivational interviewing (MI). It involves responding
in a manner that acknowledges and explores the meaning of what the client has
expressed in the conversation. In this work, we introduce the task of
counseling response rewriting, which transforms non-reflective statements into
reflective responses. We introduce VERVE, a template-based rewriting system
with paraphrase-augmented training and adaptive template updating. VERVE first
creates a template by identifying and filtering out tokens that are not
relevant to reflections and constructs a reflective response using the
template. Paraphrase-augmented training allows the model to learn less-strict
fillings of masked spans, and adaptive template updating helps discover
effective templates for rewriting without significantly removing the original
content. Using both automatic and human evaluations, we compare our method
against text rewriting baselines and show that our framework is effective in
turning non-reflective statements into more reflective responses while
achieving a good content preservation-reflection style trade-off.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08300" title="Abstract">arXiv:2311.08300</a> [<a href="/pdf/2311.08300" title="Download PDF">pdf</a>, <a href="/format/2311.08300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Workflow-Guided Response Generation for Task-Oriented Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+D+J">Do June Min</a>, 
<a href="/search/cs?searchtype=author&query=Sodhi%2C+P">Paloma Sodhi</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+R">Ramya Ramakrishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Task-oriented dialogue (TOD) systems aim to achieve specific goals through
interactive dialogue. Such tasks usually involve following specific workflows,
i.e. executing a sequence of actions in a particular order. While prior work
has focused on supervised learning methods to condition on past actions, they
do not explicitly optimize for compliance to a desired workflow. In this paper,
we propose a novel framework based on reinforcement learning (RL) to generate
dialogue responses that are aligned with a given workflow. Our framework
consists of ComplianceScorer, a metric designed to evaluate how well a
generated response executes the specified action, combined with an RL
opimization process that utilizes an interactive sampling technique. We
evaluate our approach on two TOD datasets, Action-Based Conversations Dataset
(ABCD) (Chen et al., 2021a) and MultiWOZ 2.2 (Zang et al., 2020) on a range of
automated and human evaluation metrics. Our findings indicate that our RL-based
framework outperforms baselines and is effective at enerating responses that
both comply with the intended workflows while being expressed in a natural and
fluent manner.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08302" title="Abstract">arXiv:2311.08302</a> [<a href="/pdf/2311.08302" title="Download PDF">pdf</a>, <a href="/format/2311.08302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Learning with Extremely Sparse Feedback for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+G">Guanyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinfeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Jianxin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yanan Niu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yang Song</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Depeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Modern personalized recommendation services often rely on user feedback,
either explicit or implicit, to improve the quality of services. Explicit
feedback refers to behaviors like ratings, while implicit feedback refers to
behaviors like user clicks. However, in the scenario of full-screen video
viewing experiences like Tiktok and Reels, the click action is absent,
resulting in unclear feedback from users, hence introducing noises in modeling
training. Existing approaches on de-noising recommendation mainly focus on
positive instances while ignoring the noise in a large amount of sampled
negative feedback. In this paper, we propose a meta-learning method to annotate
the unlabeled data from loss and gradient perspectives, which considers the
noises in both positive and negative instances. Specifically, we first propose
an Inverse Dual Loss (IDL) to boost the true label learning and prevent the
false label learning. Then we further propose an Inverse Gradient (IG) method
to explore the correct updating gradient and adjust the updating based on
meta-learning. Finally, we conduct extensive experiments on both benchmark and
industrial datasets where our proposed method can significantly improve AUC by
9.25% against state-of-the-art methods. Further analysis verifies the proposed
inverse learning framework is model-agnostic and can improve a variety of
recommendation backbones. The source code, along with the best hyper-parameter
settings, is available at this link:
https://github.com/Guanyu-Lin/InverseLearning.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08303" title="Abstract">arXiv:2311.08303</a> [<a href="/pdf/2311.08303" title="Download PDF">pdf</a>, <a href="/format/2311.08303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extrinsically-Focused Evaluation of Omissions in Medical Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schumacher%2C+E">Elliot Schumacher</a>, 
<a href="/search/cs?searchtype=author&query=Rosenthal%2C+D">Daniel Rosenthal</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+V">Varun Nair</a>, 
<a href="/search/cs?searchtype=author&query=Price%2C+L">Luladay Price</a>, 
<a href="/search/cs?searchtype=author&query=Tso%2C+G">Geoffrey Tso</a>, 
<a href="/search/cs?searchtype=author&query=Kannan%2C+A">Anitha Kannan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The goal of automated summarization techniques (Paice, 1990; Kupiec et al,
1995) is to condense text by focusing on the most critical information.
Generative large language models (LLMs) have shown to be robust summarizers,
yet traditional metrics struggle to capture resulting performance (Goyal et al,
2022) in more powerful LLMs. In safety-critical domains such as medicine, more
rigorous evaluation is required, especially given the potential for LLMs to
omit important information in the resulting summary. We propose MED-OMIT, a new
omission benchmark for medical summarization. Given a doctor-patient
conversation and a generated summary, MED-OMIT categorizes the chat into a set
of facts and identifies which are omitted from the summary. We further propose
to determine fact importance by simulating the impact of each fact on a
downstream clinical task: differential diagnosis (DDx) generation. MED-OMIT
leverages LLM prompt-based approaches which categorize the importance of facts
and cluster them as supporting or negating evidence to the diagnosis. We
evaluate MED-OMIT on a publicly-released dataset of patient-doctor
conversations and find that MED-OMIT captures omissions better than alternative
metrics.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08305" title="Abstract">arXiv:2311.08305</a> [<a href="/pdf/2311.08305" title="Download PDF">pdf</a>, <a href="/format/2311.08305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimally Managing the Impacts of Convergence Tolerance for Distributed  Optimal Power Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Harris%2C+R">Rachel Harris</a>, 
<a href="/search/eess?searchtype=author&query=Alkhraijah%2C+M">Mohannad Alkhraijah</a>, 
<a href="/search/eess?searchtype=author&query=Molzahn%2C+D+K">Daniel K. Molzahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The future power grid may rely on distributed optimization to determine the
set-points for huge numbers of distributed energy resources. There has been
significant work on applying distributed algorithms to optimal power flow (OPF)
problems, which require separate computing agents to agree on shared boundary
variable values. Looser tolerances for the mismatches in these shared variables
generally yield faster convergence at the expense of exacerbating constraint
violations, but there is little quantitative understanding of how the
convergence tolerance affects solution quality. To address this gap, we first
quantify how convergence tolerance impacts constraint violations when the
distributed OPF generator dispatch is applied to the power system. Using
insights from this analysis, we then develop a bound tightening algorithm which
guarantees that operating points from distributed OPF algorithms will not
result in violations despite the possibility of shared variable mismatches
within the convergence tolerance. We also explore how bounding the cumulative
shared variable mismatches can prevent unnecessary conservativeness in the
bound tightening. The proposed approach enables control of the trade-off
between computational speed, which improves as the convergence tolerance
increases, and distributed OPF solution cost, which increases with convergence
tolerance due to tightened constraints, while ensuring feasibility.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08306" title="Abstract">arXiv:2311.08306</a> [<a href="/pdf/2311.08306" title="Download PDF">pdf</a>, <a href="/format/2311.08306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-the-Fly Fusion of Large Language Models and Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoang%2C+H">Hieu Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Khayrallah%2C+H">Huda Khayrallah</a>, 
<a href="/search/cs?searchtype=author&query=Junczys-Dowmunt%2C+M">Marcin Junczys-Dowmunt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose the on-the-fly ensembling of a machine translation model with an
LLM, prompted on the same task and input. We perform experiments on 4 language
pairs (both directions) with varying data amounts. We find that a slightly
weaker-at-translation LLM can improve translations of a NMT model, and
ensembling with an LLM can produce better translations than ensembling two
stronger MT models. We combine our method with various techniques from LLM
prompting, such as in context learning and translation context.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08308" title="Abstract">arXiv:2311.08308</a> [<a href="/pdf/2311.08308" title="Download PDF">pdf</a>, <a href="/format/2311.08308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Heat is On: Thermal Facial Landmark Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baker%2C+J">James Baker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Facial landmark tracking for thermal images requires tracking certain
important regions of subjects' faces, using images from thermal images, which
omit lighting and shading, but show the temperatures of their subjects. The
fluctuations of heat in particular places reflect physiological changes like
bloodflow and perspiration, which can be used to remotely gauge things like
anxiety and excitement. Past work in this domain has been limited to only a
very limited set of architectures and techniques. This work goes further by
trying a comprehensive suit of various models with different components, such
as residual connections, channel and feature-wise attention, as well as the
practice of ensembling components of the network to work in parallel. The best
model integrated convolutional and residual layers followed by a channel-wise
self-attention layer, requiring less than 100K parameters.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08309" title="Abstract">arXiv:2311.08309</a> [<a href="/pdf/2311.08309" title="Download PDF">pdf</a>, <a href="/format/2311.08309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introducing an Improved Information-Theoretic Measure of Predictive  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schweighofer%2C+K">Kajetan Schweighofer</a>, 
<a href="/search/cs?searchtype=author&query=Aichberger%2C+L">Lukas Aichberger</a>, 
<a href="/search/cs?searchtype=author&query=Ielanskyi%2C+M">Mykyta Ielanskyi</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> M3L &amp; InfoCog Workshops NeurIPS 23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Applying a machine learning model for decision-making in the real world
requires to distinguish what the model knows from what it does not. A critical
factor in assessing the knowledge of a model is to quantify its predictive
uncertainty. Predictive uncertainty is commonly measured by the entropy of the
Bayesian model average (BMA) predictive distribution. Yet, the properness of
this current measure of predictive uncertainty was recently questioned. We
provide new insights regarding those limitations. Our analyses show that the
current measure erroneously assumes that the BMA predictive distribution is
equivalent to the predictive distribution of the true model that generated the
dataset. Consequently, we introduce a theoretically grounded measure to
overcome these limitations. We experimentally verify the benefits of our
introduced measure of predictive uncertainty. We find that our introduced
measure behaves more reasonably in controlled synthetic tasks. Moreover, our
evaluations on ImageNet demonstrate that our introduced measure is advantageous
in real-world applications utilizing predictive uncertainty.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08313" title="Abstract">arXiv:2311.08313</a> [<a href="/pdf/2311.08313" title="Download PDF">pdf</a>, <a href="/ps/2311.08313" title="Download PostScript">ps</a>, <a href="/format/2311.08313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Fast Track to Full Gold Open Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kudeli%C4%87%2C+R">Robert Kudeli&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">The world of scientific publishing is changing, the days of an old type of
subscription-based earnings for publishers are it seems over, and we are
entering a new era. It seems as if an ever-increasing number of journals from
disparate publishers are going Gold, Open Access that is, yet have we
rigorously ascertained the issue in its entirety, or are we touting the
strengths and forgetting about constructive criticism and careful weighing of
evidence? We will therefore present the current state of the art of this more
relevant than ever hot topic and suggest solutions that are most likely to be
acceptable to all parties.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08314" title="Abstract">arXiv:2311.08314</a> [<a href="/pdf/2311.08314" title="Download PDF">pdf</a>, <a href="/format/2311.08314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional Neural Networks Exploiting Attributes of Biological  Neurons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+N+K">Neeraj Kumar Singh</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+N+R">Nikhil R. Pal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this era of artificial intelligence, deep neural networks like
Convolutional Neural Networks (CNNs) have emerged as front-runners, often
surpassing human capabilities. These deep networks are often perceived as the
panacea for all challenges. Unfortunately, a common downside of these networks
is their ''black-box'' character, which does not necessarily mirror the
operation of biological neural systems. Some even have millions/billions of
learnable (tunable) parameters, and their training demands extensive data and
time.
<br />Here, we integrate the principles of biological neurons in certain layer(s)
of CNNs. Specifically, we explore the use of neuro-science-inspired
computational models of the Lateral Geniculate Nucleus (LGN) and simple cells
of the primary visual cortex. By leveraging such models, we aim to extract
image features to use as input to CNNs, hoping to enhance training efficiency
and achieve better accuracy. We aspire to enable shallow networks with a
Push-Pull Combination of Receptive Fields (PP-CORF) model of simple cells as
the foundation layer of CNNs to enhance their learning process and performance.
To achieve this, we propose a two-tower CNN, one shallow tower and the other as
ResNet 18. Rather than extracting the features blindly, it seeks to mimic how
the brain perceives and extracts features. The proposed system exhibits a
noticeable improvement in the performance (on an average of $5\%-10\%$) on
CIFAR-10, CIFAR-100, and ImageNet-100 datasets compared to ResNet-18. We also
check the efficiency of only the Push-Pull tower of the network.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08316" title="Abstract">arXiv:2311.08316</a> [<a href="/pdf/2311.08316" title="Download PDF">pdf</a>, <a href="/format/2311.08316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CholeskyQR with Randomization and Pivoting for Tall Matrices (CQRRPT)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Melnichenko%2C+M">Maksim Melnichenko</a>, 
<a href="/search/math?searchtype=author&query=Balabanov%2C+O">Oleg Balabanov</a>, 
<a href="/search/math?searchtype=author&query=Murray%2C+R">Riley Murray</a>, 
<a href="/search/math?searchtype=author&query=Demmel%2C+J">James Demmel</a>, 
<a href="/search/math?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
<a href="/search/math?searchtype=author&query=Luszczek%2C+P">Piotr Luszczek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages in the body, 10 pages in the appendices, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper develops and analyzes an algorithm for QR decomposition with
column pivoting (QRCP) of tall matrices. The algorithm uses methods from
randomized numerical linear algebra in a particularly careful way, to
accelerate both pivot decisions for the input matrix and the process of
decomposing the pivoted matrix via QR. The source of the latter acceleration is
the use of randomized preconditioning and CholeskyQR. Comprehensive analysis is
provided in both exact and finite-precision arithmetic to characterize the
algorithm's rank-revealing properties and its numerical stability. An
implementation of the described algorithm is made available under the
open-source RandLAPACK library, which itself relies on RandBLAS. Experiments
with this implementation on an Intel Xeon Gold 6248R CPU demonstrate
order-of-magnitude speedups relative to LAPACK's standard function for QRCP,
and comparable performance to a specialized algorithm for unpivoted QR of tall
matrices.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08319" title="Abstract">arXiv:2311.08319</a> [<a href="/pdf/2311.08319" title="Download PDF">pdf</a>, <a href="/format/2311.08319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource Efficient Over-the-Air Fronthaul Signaling for Uplink Cell-Free  Massive MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaik%2C+Z+H">Zakir Hussain Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Thoota%2C+S+S">Sai Subramanyam Thoota</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rnson%2C+E">Emil Bj&#xf6;rnson</a>, 
<a href="/search/cs?searchtype=author&query=Larsson%2C+E+G">Erik G. Larsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, Submitted to IEEE International Conference on Communications (ICC), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We propose a novel resource efficient analog over-the-air (OTA) computation
framework to address the demanding requirements of the uplink (UL) fronthaul
between the access points (APs) and the central processing unit (CPU) in
cell-free massive multiple-input multiple-output (MIMO) systems. We discuss the
drawbacks of the wired and wireless fronthaul solutions, and show that our
proposed mechanism is efficient and scalable as the number of APs increases. We
present the transmit precoding and two-phase power assignment strategies at the
APs to coherently combine the signals OTA in a spectrally efficient manner. We
derive the statistics of the APs locally available signals which enable us to
to obtain the analytical expressions for the Bayesian and classical estimators
of the OTA combined signals. We empirically evaluate the normalized mean square
error (NMSE), symbol error rate (SER), and the coded bit error rate (BER) of
our developed solution and benchmark against the state-of-the-art wired
fronthaul based system
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08320" title="Abstract">arXiv:2311.08320</a> [<a href="/pdf/2311.08320" title="Download PDF">pdf</a>, <a href="/format/2311.08320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CV32RT: Enabling Fast Interrupt and Context Switching for RISC-V  Microcontrollers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balas%2C+R">Robert Balas</a>, 
<a href="/search/cs?searchtype=author&query=Ottaviano%2C+A">Alessandro Ottaviano</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, submitted to IEEE Transactions on VLSI Systems (TVLSI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Processors using the open RISC-V ISA are finding increasing adoption in the
embedded world. Many embedded use cases have real-time constraints and require
flexible, predictable, and fast reactive handling of incoming events. However,
RISC- V processors are still lagging in this area compared to more mature
proprietary architectures, such as ARM Cortex-M and TriCore, which have been
tuned for years. The default interrupt controller standardized by RISC-V, the
Core Local Interruptor (CLINT), lacks configurability in prioritization and
preemption of interrupts. The RISC-V Core Local Interrupt Controller (CLIC)
specification addresses this concern by enabling pre-emptible, low-latency
vectored interrupts while also envisioning optional extensions to improve
interrupt latency. In this work, we implement a CLIC for the CV32E40P, an
industrially supported open-source 32-bit MCU-class RISC-V core, and enhance it
with fastirq: a custom extension that provides interrupt latency as low as 6
cycles. We call CV32RT our enhanced core. To the best of our knowledge, CV32RT
is the first fully open-source RV32 core with competitive interrupt-handling
features compared to the Arm Cortex-M series and TriCore. The proposed
extensions are also demonstrated to improve task context switching in real-time
operating systems.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08322" title="Abstract">arXiv:2311.08322</a> [<a href="/pdf/2311.08322" title="Download PDF">pdf</a>, <a href="/format/2311.08322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GT4Py: High Performance Stencils for Weather and Climate Applications  using Python
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paredes%2C+E+G">Enrique G. Paredes</a>, 
<a href="/search/cs?searchtype=author&query=Groner%2C+L">Linus Groner</a>, 
<a href="/search/cs?searchtype=author&query=Ubbiali%2C+S">Stefano Ubbiali</a>, 
<a href="/search/cs?searchtype=author&query=Vogt%2C+H">Hannes Vogt</a>, 
<a href="/search/cs?searchtype=author&query=Madonna%2C+A">Alberto Madonna</a>, 
<a href="/search/cs?searchtype=author&query=Mariotti%2C+K">Kean Mariotti</a>, 
<a href="/search/cs?searchtype=author&query=Cruz%2C+F">Felipe Cruz</a>, 
<a href="/search/cs?searchtype=author&query=Benedicic%2C+L">Lucas Benedicic</a>, 
<a href="/search/cs?searchtype=author&query=Bianco%2C+M">Mauro Bianco</a>, 
<a href="/search/cs?searchtype=author&query=VandeVondele%2C+J">Joost VandeVondele</a>, 
<a href="/search/cs?searchtype=author&query=Schulthess%2C+T+C">Thomas C. Schulthess</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">All major weather and climate applications are currently developed using
languages such as Fortran or C++. This is typical in the domain of high
performance computing (HPC), where efficient execution is an important concern.
Unfortunately, this approach leads to implementations that intermix
optimizations for specific hardware architectures with the high-level numerical
methods that are typical for the domain. This leads to code that is verbose,
difficult to extend and maintain, and difficult to port to different hardware
architectures. Here, we propose a different strategy based on GT4Py (GridTools
for Python). GT4Py is a Python framework to write weather and climate
applications that includes a high-level embedded domain specific language (DSL)
to write stencil computations. The toolchain integrated in GT4Py enables
automatic code-generation,to obtain the performance of state-of-the-art C++ and
CUDA implementations. The separation of concerns between the mathematical
definitions and the actual implementations allows for performance portability
of the computations on a wide range of computing architectures, while being
embedded in Python allows easy access to the tools of the Python ecosystem to
enhance the productivity of the scientists and facilitate integration in
complex workflows. Here, the initial release of GT4Py is described, providing
an overview of the current state of the framework and performance results
showing how GT4Py can outperform pure Python implementations by orders of
magnitude.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08323" title="Abstract">arXiv:2311.08323</a> [<a href="/pdf/2311.08323" title="Download PDF">pdf</a>, <a href="/format/2311.08323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-vocabulary keyword spotting in any language through multilingual  contrastive speech-phoneme pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Samir%2C+F">Farhan Samir</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Changbing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+J">Jahurul Islam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint; Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we introduce a massively multilingual speech corpora with
fine-grained phonemic transcriptions, encompassing more than 115 languages from
diverse language families. Based on this multilingual dataset, we propose
CLAP-IPA, a multilingual phoneme-speech contrastive embedding model capable of
open-vocabulary matching between speech signals and phonemically transcribed
keywords or arbitrary phrases. The proposed model has been tested on two
fieldwork speech corpora in 97 unseen languages, exhibiting strong
generalizability across languages. Comparison with a text-based model shows
that using phonemes as modeling units enables much better crosslinguistic
generalization than orthographic texts.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08324" title="Abstract">arXiv:2311.08324</a> [<a href="/pdf/2311.08324" title="Download PDF">pdf</a>, <a href="/format/2311.08324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anti-LM Decoding for Zero-shot In-context Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sia%2C+S">Suzanna Sia</a>, 
<a href="/search/cs?searchtype=author&query=DeLucia%2C+A">Alexandra DeLucia</a>, 
<a href="/search/cs?searchtype=author&query=Duh%2C+K">Kevin Duh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Zero-shot In-context learning is the phenomenon where models can perform the
task simply given the instructions. However, pre-trained large language models
are known to be poorly calibrated for this task. One of the most effective
approaches to handling this bias is to adopt a contrastive decoding objective,
which accounts for the prior probability of generating the next token by
conditioning on some context. This work introduces an Anti-Language Model
objective with a decay factor designed to address the weaknesses of In-context
Machine Translation. We conduct our experiments across 3 model types and sizes,
3 language directions, and for both greedy decoding and beam search ($B=5$).
The proposed method outperforms other state-of-art decoding objectives, with up
to $20$ BLEU point improvement from the default objective observed in some
settings.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08325" title="Abstract">arXiv:2311.08325</a> [<a href="/pdf/2311.08325" title="Download PDF">pdf</a>, <a href="/format/2311.08325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protecting the Future of Information: LOCO Coding With Error Detection  for DNA Data Storage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C4%B0rima%C4%9Fz%C4%B1%2C+C">Canberk &#x130;rima&#x11f;z&#x131;</a>, 
<a href="/search/cs?searchtype=author&query=Uslan%2C+Y">Yusuf Uslan</a>, 
<a href="/search/cs?searchtype=author&query=Hareedy%2C+A">Ahmed Hareedy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages (double column), 3 figures, submitted to the IEEE Transactions on Molecular, Biological and Multi-scale Communications (TMBMC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">DNA strands serve as a storage medium for $4$-ary data over the alphabet
$\{A,T,G,C\}$. DNA data storage promises formidable information density,
long-term durability, and ease of replicability. However, information in this
intriguing storage technology might be corrupted. Experiments have revealed
that DNA sequences with long homopolymers and/or with low $GC$-content are
notably more subject to errors upon storage.
<br />This paper investigates the utilization of the recently-introduced method for
designing lexicographically-ordered constrained (LOCO) codes in DNA data
storage. This paper introduces DNA LOCO (D-LOCO) codes, over the alphabet
$\{A,T,G,C\}$ with limited runs of identical symbols. These codes come with an
encoding-decoding rule we derive, which provides affordable encoding-decoding
algorithms. In terms of storage overhead, the proposed encoding-decoding
algorithms outperform those in the existing literature. Our algorithms are
readily reconfigurable. D-LOCO codes are intrinsically balanced, which allows
us to achieve balancing over the entire DNA strand with minimal rate penalty.
Moreover, we propose four schemes to bridge consecutive codewords, three of
which guarantee single substitution error detection per codeword. We examine
the probability of undetecting errors. We also show that D-LOCO codes are
capacity-achieving and that they offer remarkably high rates at moderate
lengths.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08328" title="Abstract">arXiv:2311.08328</a> [<a href="/pdf/2311.08328" title="Download PDF">pdf</a>, <a href="/format/2311.08328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A PRISMA-driven systematic mapping study on system assurance weakeners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahandashti%2C+K+K">Kimya Khakzad Shahandashti</a>, 
<a href="/search/cs?searchtype=author&query=Belle%2C+A+B">Alvine B. Belle</a>, 
<a href="/search/cs?searchtype=author&query=Lethbridge%2C+T+C">Timothy C. Lethbridge</a>, 
<a href="/search/cs?searchtype=author&query=Odu%2C+O">Oluwafemi Odu</a>, 
<a href="/search/cs?searchtype=author&query=Sivakumar%2C+M">Mithila Sivakumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: An assurance case is a structured hierarchy of claims aiming at
demonstrating that a given mission-critical system supports specific
requirements (e.g., safety, security, privacy). The presence of assurance
weakeners (i.e., assurance deficits, logical fallacies) in assurance cases
reflects insufficient evidence, knowledge, or gaps in reasoning. These
weakeners can undermine confidence in assurance arguments, potentially
hindering the verification of mission-critical system capabilities.
<br />Objectives: As a stepping stone for future research on assurance weakeners,
we aim to initiate the first comprehensive systematic mapping study on this
subject. Methods: We followed the well-established PRISMA 2020 and SEGRESS
guidelines to conduct our systematic mapping study. We searched for primary
studies in five digital libraries and focused on the 2012-2023 publication year
range. Our selection criteria focused on studies addressing assurance weakeners
at the modeling level, resulting in the inclusion of 39 primary studies in our
systematic review.
<br />Results: Our systematic mapping study reports a taxonomy (map) that provides
a uniform categorization of assurance weakeners and approaches proposed to
manage them at the modeling level.
<br />Conclusion: Our study findings suggest that the SACM (Structured Assurance
Case Metamodel) -- a standard specified by the OMG (Object Management Group) --
may be the best specification to capture structured arguments and reason about
their potential assurance weakeners.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08329" title="Abstract">arXiv:2311.08329</a> [<a href="/pdf/2311.08329" title="Download PDF">pdf</a>, <a href="/format/2311.08329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KTRL+F: Knowledge-Augmented In-Document Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+H">Hanseok Oh</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Haebin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+M">Miyoung Ko</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunji Lee</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce a new problem KTRL+F, a knowledge-augmented in-document search
task that necessitates real-time identification of all semantic targets within
a document with the awareness of external sources through a single natural
query. This task addresses following unique challenges for in-document search:
1) utilizing knowledge outside the document for extended use of additional
information about targets to bridge the semantic gap between the query and the
targets, and 2) balancing between real-time applicability with the performance.
We analyze various baselines in KTRL+F and find there are limitations of
existing models, such as hallucinations, low latency, or difficulties in
leveraging external knowledge. Therefore we propose a Knowledge-Augmented
Phrase Retrieval model that shows a promising balance between speed and
performance by simply augmenting external knowledge embedding in phrase
embedding. Additionally, we conduct a user study to verify whether solving
KTRL+F can enhance search experience of users. It demonstrates that even with
our simple model users can reduce the time for searching with less queries and
reduced extra visits to other sources for collecting evidence. We encourage the
research community to work on KTRL+F to enhance more efficient in-document
information access.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08333" title="Abstract">arXiv:2311.08333</a> [<a href="/pdf/2311.08333" title="Download PDF">pdf</a>, <a href="/format/2311.08333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibration of an Elastic Humanoid Upper Body and Efficient Compensation  for Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tenhumberg%2C+J">Johannes Tenhumberg</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4uml%2C+B">Berthold B&#xe4;uml</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2021 IEEE-RAS International Conference on Humanoid Robots 2021
  IEEE-RAS International Conference on Humanoid Robots 2021 IEEE-RAS
  International Conference on Humanoid Robots IEEE-RAS International Conference
  on Humanoid Robots
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">High absolute accuracy is an essential prerequisite for a humanoid robot to
autonomously and robustly perform manipulation tasks while avoiding obstacles.
We present for the first time a kinematic model for a humanoid upper body
incorporating joint and transversal elasticities. These elasticities lead to
significant deformations due to the robot's own weight, and the resulting model
is implicitly defined via a torque equilibrium. We successfully calibrate this
model for DLR's humanoid Agile Justin, including all Denavit-Hartenberg
parameters and elasticities. The calibration is formulated as a combined
least-squares problem with priors and based on measurements of the end effector
positions of both arms via an external tracking system. The absolute position
error is massively reduced from 21mm to 3.1mm on average in the whole
workspace. Using this complex and implicit kinematic model in motion planning
is challenging. We show that for optimization-based path planning, integrating
the iterative solution of the implicit model into the optimization loop leads
to an elegant and highly efficient solution. For mildly elastic robots like
Agile Justin, there is no performance impact, and even for a simulated highly
flexible robot with 20 times higher elasticities, the runtime increases by only
30%.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08336" title="Abstract">arXiv:2311.08336</a> [<a href="/pdf/2311.08336" title="Download PDF">pdf</a>, <a href="/format/2311.08336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Variational Auto-Encoder Architectures, Configurations, and  Datasets for Generative Music Explainable AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bryan-Kinns%2C+N">Nick Bryan-Kinns</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bingyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Songyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Banar%2C+B">Berker Banar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Springer MIR journal submission under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Generative AI models for music and the arts in general are increasingly
complex and hard to understand. The field of eXplainable AI (XAI) seeks to make
complex and opaque AI models such as neural networks more understandable to
people. One approach to making generative AI models more understandable is to
impose a small number of semantically meaningful attributes on generative AI
models. This paper contributes a systematic examination of the impact that
different combinations of Variational Auto-Encoder models (MeasureVAE and
AdversarialVAE), configurations of latent space in the AI model (from 4 to 256
latent dimensions), and training datasets (Irish folk, Turkish folk, Classical,
and pop) have on music generation performance when 2 or 4 meaningful musical
attributes are imposed on the generative model. To date there have been no
systematic comparisons of such models at this level of combinatorial detail.
Our findings show that MeasureVAE has better reconstruction performance than
AdversarialVAE which has better musical attribute independence. Results
demonstrate that MeasureVAE was able to generate music across music genres with
interpretable musical dimensions of control, and performs best with low
complexity music such a pop and rock. We recommend that a 32 or 64 latent
dimensional space is optimal for 4 regularised dimensions when using MeasureVAE
to generate music across genres. Our results are the first detailed comparisons
of configurations of state-of-the-art generative AI models for music and can be
used to help select and configure AI models, musical features, and datasets for
more understandable generation of music.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08338" title="Abstract">arXiv:2311.08338</a> [<a href="/pdf/2311.08338" title="Download PDF">pdf</a>, <a href="/format/2311.08338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Contained Calibration of an Elastic Humanoid Upper Body Using Only  a Head-Mounted RGB Camera
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tenhumberg%2C+J">Johannes Tenhumberg</a>, 
<a href="/search/cs?searchtype=author&query=Winkelbauer%2C+D">Dominik Winkelbauer</a>, 
<a href="/search/cs?searchtype=author&query=Burschka%2C+D">Darius Burschka</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4uml%2C+B">Berthold B&#xe4;uml</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE-RAS International Conference on Humanoid Robots
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">When a humanoid robot performs a manipulation task, it first makes a model of
the world using its visual sensors and then plans the motion of its body in
this model. For this, precise calibration of the camera parameters and the
kinematic tree is needed. Besides the accuracy of the calibrated model, the
calibration process should be fast and self-contained, i.e., no external
measurement equipment should be used. Therefore, we extend our prior work on
calibrating the elastic upper body of DLR's Agile Justin by now using only its
internal head-mounted RGB camera. We use simple visual markers at the ends of
the kinematic chain and one in front of the robot, mounted on a pole, to get
measurements for the whole kinematic tree. To ensure that the task-relevant
cartesian error at the end-effectors is minimized, we introduce virtual noise
to fit our imperfect robot model so that the pixel error has a higher weight if
the marker is further away from the camera. This correction reduces the
cartesian error by more than 20%, resulting in a final accuracy of 3.9mm on
average and 9.1mm in the worst case. This way, we achieve the same precision as
in our previous work, where an external cartesian tracking system was used.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08342" title="Abstract">arXiv:2311.08342</a> [<a href="/pdf/2311.08342" title="Download PDF">pdf</a>, <a href="/format/2311.08342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Linear Regression with Constraints: A Flexible Entropy-based  Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Srivastava%2C+A">Amber Srivastava</a>, 
<a href="/search/eess?searchtype=author&query=Bayati%2C+A">Alisina Bayati</a>, 
<a href="/search/eess?searchtype=author&query=Salapaka%2C+S">Srinivasa Salapaka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This work presents a new approach to solve the sparse linear regression
problem, i.e., to determine a k-sparse vector w in R^d that minimizes the cost
||y - Aw||^2_2. In contrast to the existing methods, our proposed approach
splits this k-sparse vector into two parts -- (a) a column stochastic binary
matrix V, and (b) a vector x in R^k. Here, the binary matrix V encodes the
location of the k non-zero entries in w. Equivalently, it encodes the subset of
k columns in the matrix A that map w to y. We demonstrate that this enables
modeling several non-trivial application-specific structural constraints on w
as constraints on V. The vector x comprises of the actual non-zero values in w.
We use Maximum Entropy Principle (MEP) to solve the resulting optimization
problem. In particular, we ascribe a probability distribution to the set of all
feasible binary matrices V, and iteratively determine this distribution and the
vector x such that the associated Shannon entropy gets minimized, and the
regression cost attains a pre-specified value. The resulting algorithm employs
homotopy from the convex entropy function to the non-convex cost function to
avoid poor local minimum. We demonstrate the efficacy and flexibility of our
proposed approach in incorporating a variety of practical constraints, that are
otherwise difficult to model using the existing benchmark methods.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08345" title="Abstract">arXiv:2311.08345</a> [<a href="/pdf/2311.08345" title="Download PDF">pdf</a>, <a href="/format/2311.08345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speeding Up Optimization-based Motion Planning through Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tenhumberg%2C+J">Johannes Tenhumberg</a>, 
<a href="/search/cs?searchtype=author&query=Burschka%2C+D">Darius Burschka</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4uml%2C+B">Berthold B&#xe4;uml</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE/RSJ International Conference on Intelligent Robots and
  Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Planning collision-free motions for robots with many degrees of freedom is
challenging in environments with complex obstacle geometries. Recent work
introduced the idea of speeding up the planning by encoding prior experience of
successful motion plans in a neural network. However, this "neural motion
planning" did not scale to complex robots in unseen 3D environments as needed
for real-world applications. Here, we introduce "basis point set", well-known
in computer vision, to neural motion planning as a modern compact environment
encoding enabling efficient supervised training networks that generalize well
over diverse 3D worlds. Combined with a new elaborate training scheme, we reach
a planning success rate of 100%. We use the network to predict an educated
initial guess for an optimization-based planner (OMP), which quickly converges
to a feasible solution, massively outperforming random multi-starts when tested
on previously unseen environments. For the DLR humanoid Agile Justin with 19DoF
and in challenging obstacle environments, optimal paths can be generated in
200ms using only a single CPU core. We also show a first successful real-world
experiment based on a high-resolution world model from an integrated 3D sensor.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08348" title="Abstract">arXiv:2311.08348</a> [<a href="/pdf/2311.08348" title="Download PDF">pdf</a>, <a href="/format/2311.08348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MC^2: A Multilingual Corpus of Minority Languages in China
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Mingxu Tao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Quzhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiuheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhibin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yansong Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large-scale corpora play a vital role in the construction of large language
models (LLMs). However, existing LLMs exhibit limited abilities in
understanding low-resource languages, including the minority languages in
China, due to a lack of training data. To improve the accessibility of these
languages, we present MC^2, a Multilingual Corpus of Minority Languages in
China, which is the largest open-source corpus so far. It encompasses four
underrepresented languages, i.e., Tibetan, Uyghur, Kazakh in the Kazakh Arabic
script, and Mongolian in the traditional Mongolian script. Notably, two writing
systems in MC^2 are long neglected in previous corpora. As we identify serious
contamination in the low-resource language split in the existing multilingual
corpora, we propose a quality-centric solution for collecting MC^2,
prioritizing quality and accuracy while enhancing representativeness and
diversity. By in-depth analysis, we demonstrate the new research challenges
MC^2 brings, such as long-text modeling and multiplicity of writing systems. We
hope MC^2 can help enhance the equity of the underrepresented languages in
China and provide a reliable data foundation for further research on
low-resource languages.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08349" title="Abstract">arXiv:2311.08349</a> [<a href="/pdf/2311.08349" title="Download PDF">pdf</a>, <a href="/format/2311.08349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Text Boundary Detection with Topological Data Analysis and  Sliding Window Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kushnareva%2C+L">Laida Kushnareva</a>, 
<a href="/search/cs?searchtype=author&query=Gaintseva%2C+T">Tatiana Gaintseva</a>, 
<a href="/search/cs?searchtype=author&query=Magai%2C+G">German Magai</a>, 
<a href="/search/cs?searchtype=author&query=Barannikov%2C+S">Serguei Barannikov</a>, 
<a href="/search/cs?searchtype=author&query=Abulkhanov%2C+D">Dmitry Abulkhanov</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+K">Kristian Kuznetsov</a>, 
<a href="/search/cs?searchtype=author&query=Piontkovskaya%2C+I">Irina Piontkovskaya</a>, 
<a href="/search/cs?searchtype=author&query=Nikolenko%2C+S">Sergey Nikolenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Due to the rapid development of text generation models, people increasingly
often encounter texts that may start out as written by a human but then
continue as machine-generated results of large language models. Detecting the
boundary between human-written and machine-generated parts of such texts is a
very challenging problem that has not received much attention in literature. In
this work, we consider and compare a number of different approaches for this
artificial text boundary detection problem, comparing several predictors over
features of different nature. We show that supervised fine-tuning of the
RoBERTa model works well for this task in general but fails to generalize in
important cross-domain and cross-generator settings, demonstrating a tendency
to overfit to spurious properties of the data. Then, we propose novel
approaches based on features extracted from a frozen language model's
embeddings that are able to outperform both the human accuracy level and
previously considered baselines on the Real or Fake Text benchmark. Moreover,
we adapt perplexity-based approaches for the boundary detection task and
analyze their behaviour. We analyze the robustness of all proposed classifiers
in cross-domain and cross-model settings, discovering important properties of
the data that can negatively influence the performance of artificial text
boundary detection algorithms.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08350" title="Abstract">arXiv:2311.08350</a> [<a href="/pdf/2311.08350" title="Download PDF">pdf</a>, <a href="/format/2311.08350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChoralSynth: Synthetic Dataset of Choral Singing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Narang%2C+J">Jyoti Narang</a>, 
<a href="/search/cs?searchtype=author&query=De+La+Vega%2C+V">Viviana De La Vega</a>, 
<a href="/search/cs?searchtype=author&query=Lizarraga%2C+X">Xavier Lizarraga</a>, 
<a href="/search/cs?searchtype=author&query=Mayor%2C+O">Oscar Mayor</a>, 
<a href="/search/cs?searchtype=author&query=Parra%2C+H">Hector Parra</a>, 
<a href="/search/cs?searchtype=author&query=Janer%2C+J">Jordi Janer</a>, 
<a href="/search/cs?searchtype=author&query=Serra%2C+X">Xavier Serra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Information Retrieval (cs.IR); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Choral singing, a widely practiced form of ensemble singing, lacks
comprehensive datasets in the realm of Music Information Retrieval (MIR)
research, due to challenges arising from the requirement to curate multitrack
recordings. To address this, we devised a novel methodology, leveraging
state-of-the-art synthesizers to create and curate quality renditions. The
scores were sourced from Choral Public Domain Library(CPDL). This work is done
in collaboration with a diverse team of musicians, software engineers and
researchers. The resulting dataset, complete with its associated metadata, and
methodology is released as part of this work, opening up new avenues for
exploration and advancement in the field of singing voice research.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08354" title="Abstract">arXiv:2311.08354</a> [<a href="/pdf/2311.08354" title="Download PDF">pdf</a>, <a href="/format/2311.08354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Experience-informed Navigation for Multi-modal Quadrupedal  Rebar Grid Traversal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asselmeier%2C+M">Max Asselmeier</a>, 
<a href="/search/cs?searchtype=author&query=Ivanova%2C+J">Jane Ivanova</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Ziyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Vela%2C+P+A">Patricio A. Vela</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Ye Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This study focuses on a layered, experience-based, multi-modal contact
planning framework for agile quadrupedal locomotion over a constrained rebar
environment. To this end, our hierarchical planner incorporates
locomotion-specific modules into the high-level contact sequence planner and
solves kinodynamically-aware trajectory optimization as the low-level motion
planner. Through quantitative analysis of the experience accumulation process
and experimental validation of the kinodynamic feasibility of the generated
locomotion trajectories, we demonstrate that the experience planning heuristic
offers an effective way of providing candidate footholds for a legged contact
planner. Additionally, we introduce a guiding torso path heuristic at the
global planning level to enhance the navigation success rate in the presence of
environmental obstacles. Our results indicate that the torso-path guided
experience accumulation requires significantly fewer offline trials to
successfully reach the goal compared to regular experience accumulation.
Finally, our planning framework is validated in both dynamics simulations and
real hardware implementations on a quadrupedal robot provided by Skymul Inc.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08357" title="Abstract">arXiv:2311.08357</a> [<a href="/pdf/2311.08357" title="Download PDF">pdf</a>, <a href="/format/2311.08357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparsity-Preserving Differentially Private Training of Large Embedding  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghazi%2C+B">Badih Ghazi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yangsibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+P">Pritish Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Ravi Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Manurangsi%2C+P">Pasin Manurangsi</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Amer Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chiyuan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neural Information Processing Systems (NeurIPS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">As the use of large embedding models in recommendation systems and language
applications increases, concerns over user data privacy have also risen.
DP-SGD, a training algorithm that combines differential privacy with stochastic
gradient descent, has been the workhorse in protecting user privacy without
compromising model accuracy by much. However, applying DP-SGD naively to
embedding models can destroy gradient sparsity, leading to reduced training
efficiency. To address this issue, we present two new algorithms, DP-FEST and
DP-AdaFEST, that preserve gradient sparsity during private training of large
embedding models. Our algorithms achieve substantial reductions ($10^6 \times$)
in gradient size, while maintaining comparable levels of accuracy, on benchmark
real-world datasets.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08359" title="Abstract">arXiv:2311.08359</a> [<a href="/pdf/2311.08359" title="Download PDF">pdf</a>, <a href="/format/2311.08359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rotation-Agnostic Image Representation Learning for Digital Pathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alfasly%2C+S">Saghir Alfasly</a>, 
<a href="/search/cs?searchtype=author&query=Shafique%2C+A">Abubakr Shafique</a>, 
<a href="/search/cs?searchtype=author&query=Nejat%2C+P">Peyman Nejat</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+J">Jibran Khan</a>, 
<a href="/search/cs?searchtype=author&query=Alsaafin%2C+A">Areej Alsaafin</a>, 
<a href="/search/cs?searchtype=author&query=Alabtah%2C+G">Ghazal Alabtah</a>, 
<a href="/search/cs?searchtype=author&query=Tizhoosh%2C+H+R">H.R. Tizhoosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 10 figures, 18 tables. Histopathological Image Analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper addresses complex challenges in histopathological image analysis
through three key contributions. Firstly, it introduces a fast patch selection
method, FPS, for whole-slide image (WSI) analysis, significantly reducing
computational cost while maintaining accuracy. Secondly, it presents PathDino,
a lightweight histopathology feature extractor with a minimal configuration of
five Transformer blocks and only 9 million parameters, markedly fewer than
alternatives. Thirdly, it introduces a rotation-agnostic representation
learning paradigm using self-supervised learning, effectively mitigating
overfitting. We also show that our compact model outperforms existing
state-of-the-art histopathology-specific vision transformers on 12 diverse
datasets, including both internal datasets spanning four sites (breast, liver,
skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS,
DigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training
dataset of 6 million histopathology patches from The Cancer Genome Atlas
(TCGA), our approach demonstrates an average 8.5% improvement in patch-level
majority vote performance. These contributions provide a robust framework for
enhancing image analysis in digital pathology, rigorously validated through
extensive evaluation. Project Page: https://rhazeslab.github.io/PathDino-Page/
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08360" title="Abstract">arXiv:2311.08360</a> [<a href="/pdf/2311.08360" title="Download PDF">pdf</a>, <a href="/format/2311.08360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Transient Nature of Emergent In-Context Learning in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Aaditya K. Singh</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S+C+Y">Stephanie C.Y. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Moskovitz%2C+T">Ted Moskovitz</a>, 
<a href="/search/cs?searchtype=author&query=Grant%2C+E">Erin Grant</a>, 
<a href="/search/cs?searchtype=author&query=Saxe%2C+A+M">Andrew M. Saxe</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+F">Felix Hill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Transformer neural networks can exhibit a surprising capacity for in-context
learning (ICL) despite not being explicitly trained for it. Prior work has
provided a deeper understanding of how ICL emerges in transformers, e.g.
through the lens of mechanistic interpretability, Bayesian inference, or by
examining the distributional properties of training data. However, in each of
these cases, ICL is treated largely as a persistent phenomenon; namely, once
ICL emerges, it is assumed to persist asymptotically. Here, we show that the
emergence of ICL during transformer training is, in fact, often transient. We
train transformers on synthetic data designed so that both ICL and in-weights
learning (IWL) strategies can lead to correct predictions. We find that ICL
first emerges, then disappears and gives way to IWL, all while the training
loss decreases, indicating an asymptotic preference for IWL. The transient
nature of ICL is observed in transformers across a range of model sizes and
datasets, raising the question of how much to "overtrain" transformers when
seeking compact, cheaper-to-run models. We find that L2 regularization may
offer a path to more persistent ICL that removes the need for early stopping
based on ICL-style validation tasks. Finally, we present initial evidence that
ICL transience may be caused by competition between ICL and IWL circuits.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08362" title="Abstract">arXiv:2311.08362</a> [<a href="/pdf/2311.08362" title="Download PDF">pdf</a>, <a href="/format/2311.08362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers can optimally learn regression mixture models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pathak%2C+R">Reese Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+R">Rajat Sen</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">Weihao Kong</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Abhimanyu Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Mixture models arise in many regression problems, but most methods have seen
limited adoption partly due to these algorithms' highly-tailored and
model-specific nature. On the other hand, transformers are flexible, neural
sequence models that present the intriguing possibility of providing
general-purpose prediction methods, even in this mixture setting. In this work,
we investigate the hypothesis that transformers can learn an optimal predictor
for mixtures of regressions. We construct a generative process for a mixture of
linear regressions for which the decision-theoretic optimal procedure is given
by data-driven exponential weights on a finite set of parameters. We observe
that transformers achieve low mean-squared error on data generated via this
process. By probing the transformer's output at inference time, we also show
that transformers typically make predictions that are close to the optimal
predictor. Our experiments also demonstrate that transformers can learn
mixtures of regressions in a sample-efficient fashion and are somewhat robust
to distribution shifts. We complement our experimental observations by proving
constructively that the decision-theoretic optimal procedure is indeed
implementable by a transformer.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08364" title="Abstract">arXiv:2311.08364</a> [<a href="/pdf/2311.08364" title="Download PDF">pdf</a>, <a href="/format/2311.08364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plum: Prompt Learning using Metaheuristic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+R">Rui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+S">Shuo Xing</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shum%2C+K">Kashun Shum</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Since the emergence of large language models, prompt learning has become a
popular method for optimizing and customizing these models. Special prompts,
such as Chain-of-Thought, have even revealed previously unknown reasoning
capabilities within these models. However, the progress of discovering
effective prompts has been slow, driving a desire for general prompt
optimization methods. Unfortunately, few existing prompt learning methods
satisfy the criteria of being truly "general", i.e., automatic, discrete,
black-box, gradient-free, and interpretable all at once. In this paper, we
introduce metaheuristics, a branch of discrete non-convex optimization methods
with over 100 options, as a promising approach to prompt learning. Within our
paradigm, we test six typical methods: hill climbing, simulated annealing,
genetic algorithms with/without crossover, tabu search, and harmony search,
demonstrating their effectiveness in black-box prompt learning and
Chain-of-Thought prompt tuning. Furthermore, we show that these methods can be
used to discover more human-understandable prompts that were previously
unknown, opening the door to a cornucopia of possibilities in prompt
optimization. We release all the codes in
\url{https://github.com/research4pan/Plum}.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08367" title="Abstract">arXiv:2311.08367</a> [<a href="/pdf/2311.08367" title="Download PDF">pdf</a>, <a href="/ps/2311.08367" title="Download PostScript">ps</a>, <a href="/format/2311.08367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arboricity-Dependent Algorithms for Edge Coloring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sayan Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+M">Mart&#xed;n Costa</a>, 
<a href="/search/cs?searchtype=author&query=Panski%2C+N">Nadav Panski</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+S">Shay Solomon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Started to circulate in September 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The problem of edge coloring has been extensively studied over the years. The
main conceptual contribution of this work is in identifying a surprisingly
simple connection between the problem of $(\Delta +O(\alpha))$-edge coloring
and a certain canonical graph decomposition in graphs of arboricity $\alpha$,
for which efficient algorithms are known across various computational models.
<br />We first leverage such graph decompositions to provide fast $(\Delta
+O(\alpha))$-edge coloring algorithms in the standard {\em static} (sequential
and distributed) settings. Further, as our main technical contribution, we show
how to efficiently maintain a $(\Delta +O(\alpha))$-edge coloring in the
standard {\em dynamic} model. Consequently, we improve over the
state-of-the-art edge coloring algorithms in these models for graphs of
sufficiently small arboricity.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08369" title="Abstract">arXiv:2311.08369</a> [<a href="/pdf/2311.08369" title="Download PDF">pdf</a>, <a href="/format/2311.08369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How You Prompt Matters! Even Task-Oriented Constraints in Instructions  Affect LLM-Generated Text Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koike%2C+R">Ryuto Koike</a>, 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+M">Masahiro Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Against the misuse (e.g., plagiarism or spreading misinformation) of Large
Language Models (LLMs), many recent works have presented LLM-generated-text
detectors with promising detection performance. Spotlighting a situation where
users instruct LLMs to generate texts (e.g., essay writing), there are various
ways to write the instruction (e.g., what task-oriented constraint to include).
In this paper, we discover that even a task-oriented constraint in instruction
can cause the inconsistent performance of current detectors to the generated
texts. Specifically, we focus on student essay writing as a realistic domain
and manually create the task-oriented constraint for each factor on essay
quality by Ke and Ng (2019). Our experiment shows that the detection
performance variance of the current detector on texts generated by instruction
with each task-oriented constraint is up to 20 times larger than the variance
caused by generating texts multiple times and paraphrasing the instruction. Our
finding calls for further research on developing robust detectors that can
detect such distributional shifts caused by a task-oriented constraint in the
instruction.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08370" title="Abstract">arXiv:2311.08370</a> [<a href="/pdf/2311.08370" title="Download PDF">pdf</a>, <a href="/format/2311.08370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidgen%2C+B">Bertie Vidgen</a>, 
<a href="/search/cs?searchtype=author&query=Kirk%2C+H+R">Hannah Rose Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+R">Rebecca Qian</a>, 
<a href="/search/cs?searchtype=author&query=Scherrer%2C+N">Nino Scherrer</a>, 
<a href="/search/cs?searchtype=author&query=Kannappan%2C+A">Anand Kannappan</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+S+A">Scott A. Hale</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ttger%2C+P">Paul R&#xf6;ttger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The past year has seen rapid acceleration in the development of large
language models (LLMs). For many tasks, there is now a wide range of
open-source and open-access LLMs that are viable alternatives to proprietary
models like ChatGPT. Without proper steering and safeguards, however, LLMs will
readily follow malicious instructions, provide unsafe advice, and generate
toxic content. This is a critical safety risk for businesses and developers. We
introduce SimpleSafetyTests as a new test suite for rapidly and systematically
identifying such critical safety risks. The test suite comprises 100 test
prompts across five harm areas that LLMs, for the vast majority of
applications, should refuse to comply with. We test 11 popular open LLMs and
find critical safety weaknesses in several of them. While some LLMs do not give
a single unsafe response, most models we test respond unsafely on more than 20%
of cases, with over 50% unsafe responses in the extreme. Prepending a
safety-emphasising system prompt substantially reduces the occurrence of unsafe
responses, but does not completely stop them from happening. We recommend that
developers use such system prompts as a first line of defence against critical
safety risks.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08372" title="Abstract">arXiv:2311.08372</a> [<a href="/pdf/2311.08372" title="Download PDF">pdf</a>, <a href="/format/2311.08372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aid Nexus : A Blockchain Based Financial Distribution System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahrukh%2C+M+R+H">Md. Raisul Hasan Shahrukh</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+T">Md. Tabassinur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Mansoor%2C+N">Nafees Mansoor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computational Engineering, Finance, and Science (cs.CE); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Blockchain technology has emerged as a disruptive force with transformative
potential across numerous industries, promising efficient and automated
solutions that can revolutionize traditional systems. By leveraging
decentralized ledger systems, blockchain offers enhanced security,
transparency, and transaction verification without the need for intermediaries.
The finance sector is exploring blockchain-based solutions for payments,
remittances, lending, and investments, while healthcare adopts the technology
for medical record keeping, supply chain tracking, and data management.
Similarly, supply chain management benefits from blockchain's ability to
enhance transparency, traceability, and accountability from raw materials to
finished products. Other sectors, including real estate, energy, and
government, are also investigating blockchain-based solutions to improve
efficiency, security, and transparency. Furthermore, smart contracts within the
blockchain enable process automation, reducing manual intervention in
distribution workflows. AidNeux, a consortium-based blockchain DApp, reimagines
the distribution of financial assistance by addressing inefficiencies and
opaqueness. Using smart contracts ensures the security and directness of money
transfers. Its robust digital identity verification and real-time auditability
reduce fraud risks and strengthen accountability, thereby presenting a
scalable, transparent solution to problems inherent to conventional financial
aid systems.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08373" title="Abstract">arXiv:2311.08373</a> [<a href="/pdf/2311.08373" title="Download PDF">pdf</a>, <a href="/ps/2311.08373" title="Download PostScript">ps</a>, <a href="/format/2311.08373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proceedings of the 18th International Workshop on the ACL2 Theorem  Prover and Its Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coglio%2C+A">Alessandro Coglio</a> (Kestrel Institute and Aleo Systems Inc.), 
<a href="/search/cs?searchtype=author&query=Swords%2C+S">Sol Swords</a> (Intel Corporation)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">This volume contains the proceedings of the Eighteenth International Workshop
on the ACL2 Theorem Prover and Its Applications (ACL2-2023), a two-day workshop
held at the University of Texas at Austin and online, on November 13-14. These
workshops provide a major technical forum for users of the ACL2 theorem prover
to present research related to ACL2 and its applications.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08374" title="Abstract">arXiv:2311.08374</a> [<a href="/pdf/2311.08374" title="Download PDF">pdf</a>, <a href="/format/2311.08374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripto%2C+N+I">Nafis Irtiza Tripto</a>, 
<a href="/search/cs?searchtype=author&query=Venkatraman%2C+S">Saranya Venkatraman</a>, 
<a href="/search/cs?searchtype=author&query=Macko%2C+D">Dominik Macko</a>, 
<a href="/search/cs?searchtype=author&query=Moro%2C+R">Robert Moro</a>, 
<a href="/search/cs?searchtype=author&query=Srba%2C+I">Ivan Srba</a>, 
<a href="/search/cs?searchtype=author&query=Uchendu%2C+A">Adaku Uchendu</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thai Le</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the realm of text manipulation and linguistic transformation, the question
of authorship has always been a subject of fascination and philosophical
inquiry. Much like the \textbf{Ship of Theseus paradox}, which ponders whether
a ship remains the same when each of its original planks is replaced, our
research delves into an intriguing question: \textit{Does a text retain its
original authorship when it undergoes numerous paraphrasing iterations?}
Specifically, since Large Language Models (LLMs) have demonstrated remarkable
proficiency in the generation of both original content and the modification of
human-authored texts, a pivotal question emerges concerning the determination
of authorship in instances where LLMs or similar paraphrasing tools are
employed to rephrase the text. This inquiry revolves around \textit{whether
authorship should be attributed to the original human author or the AI-powered
tool, given the tool's independent capacity to produce text that closely
resembles human-generated content.} Therefore, we embark on a philosophical
voyage through the seas of language and authorship to unravel this intricate
puzzle.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08375" title="Abstract">arXiv:2311.08375</a> [<a href="/pdf/2311.08375" title="Download PDF">pdf</a>, <a href="/ps/2311.08375" title="Download PostScript">ps</a>, <a href="/format/2311.08375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proceedings 19th International Conference on Quantum Physics and Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gogioso%2C+S">Stefano Gogioso</a>, 
<a href="/search/cs?searchtype=author&query=Hoban%2C+M">Matty Hoban</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">This volume contains the proceedings of the 19th International Conference on
Quantum Physics and Logic (QPL 2022), which was held June 27-July 1, 2022 at
Wolfson College, University of Oxford, UK. QPL is an annual conference that
brings together academic and industry researchers working on mathematical
foundations of quantum computation, quantum physics, and related areas. The
main focus is on the use of algebraic and categorical structures, formal
languages, semantic methods, as well as other mathematical and computer
scientific techniques applicable to the study of physical systems, physical
processes, and their composition.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08377" title="Abstract">arXiv:2311.08377</a> [<a href="/pdf/2311.08377" title="Download PDF">pdf</a>, <a href="/format/2311.08377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Filter Context for Retrieval-Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiruo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Araki%2C+J">Jun Araki</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhengbao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Parvez%2C+M+R">Md Rizwan Parvez</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">On-the-fly retrieval of relevant knowledge has proven an essential element of
reliable systems for tasks such as open-domain question answering and fact
verification. However, because retrieval systems are not perfect, generation
models are required to generate outputs given partially or entirely irrelevant
passages. This can cause over- or under-reliance on context, and result in
problems in the generated output such as hallucinations. To alleviate these
problems, we propose FILCO, a method that improves the quality of the context
provided to the generator by (1) identifying useful context based on lexical
and information-theoretic approaches, and (2) training context filtering models
that can filter retrieved contexts at test time. We experiment on six
knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our
method outperforms existing approaches on extractive question answering (QA),
complex multi-hop and long-form QA, fact verification, and dialog generation
tasks. FILCO effectively improves the quality of context, whether or not it
supports the canonical output.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08379" title="Abstract">arXiv:2311.08379</a> [<a href="/pdf/2311.08379" title="Download PDF">pdf</a>, <a href="/format/2311.08379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scheming AIs: Will AIs fake alignment during training in order to get  power?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carlsmith%2C+J">Joe Carlsmith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 127 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This report examines whether advanced AIs that perform well in training will
be doing so in order to gain power later -- a behavior I call "scheming" (also
sometimes called "deceptive alignment"). I conclude that scheming is a
disturbingly plausible outcome of using baseline machine learning methods to
train goal-directed AIs sophisticated enough to scheme (my subjective
probability on such an outcome, given these conditions, is roughly 25%). In
particular: if performing well in training is a good strategy for gaining power
(as I think it might well be), then a very wide variety of goals would motivate
scheming -- and hence, good training performance. This makes it plausible that
training might either land on such a goal naturally and then reinforce it, or
actively push a model's motivations towards such a goal as an easy way of
improving performance. What's more, because schemers pretend to be aligned on
tests designed to reveal their motivations, it may be quite difficult to tell
whether this has occurred. However, I also think there are reasons for comfort.
In particular: scheming may not actually be such a good strategy for gaining
power; various selection pressures in training might work against schemer-like
goals (for example, relative to non-schemers, schemers need to engage in extra
instrumental reasoning, which might harm their training performance); and we
may be able to increase such pressures intentionally. The report discusses
these and a wide variety of other considerations in detail, and it suggests an
array of empirical research directions for probing the topic further.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08380" title="Abstract">arXiv:2311.08380</a> [<a href="/pdf/2311.08380" title="Download PDF">pdf</a>, <a href="/format/2311.08380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Preference Optimization for Neural Machine Translation with  Minimum Bayes Risk Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guangyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinghong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weizhe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Byrne%2C+B">Bill Byrne</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Minimum Bayes Risk (MBR) decoding can significantly improve translation
performance of Multilingual Large Language Models (MLLMs). However, MBR
decoding is computationally expensive and in this paper, we show how recently
developed Reinforcement Learning (RL) technique, Direct Preference Optimization
(DPO) can be used to fine-tune MLLMs so that we get the gains from MBR without
the additional computation in inference. Our fine-tuned models have
significantly improved performance on multiple NMT test sets compared to base
MLLMs without preference optimization. Our method boosts the translation
performance of MLLMs using relatively small monolingual fine-tuning sets.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08383" title="Abstract">arXiv:2311.08383</a> [<a href="/pdf/2311.08383" title="Download PDF">pdf</a>, <a href="/format/2311.08383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Choosing Outdated Information to Achieve Reliability in Age-Based  Gossiping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaswan%2C+P">Priyanka Kaswan</a>, 
<a href="/search/cs?searchtype=author&query=Ulukus%2C+S">Sennur Ulukus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">We consider a system model with two sources, a reliable source and an
unreliable source, who are responsible for disseminating updates regarding a
process to an age-based gossip network of $n$ nodes. Nodes wish to have fresh
information, however, they have preference for packets that originated at the
reliable source and are willing to sacrifice their version age of information
by up to $G$ versions to switch from an unreliable packet to a reliable packet.
We study how this protocol impacts the prevalence of unreliable packets at
nodes in the network and their version age. Using a stochastic hybrid system
(SHS) framework, we formulate analytical equations to characterize two
quantities: expected fraction of nodes with unreliable packets and expected
version age of information at network nodes. We show that as $G$ increases,
fewer nodes have unreliable packet, however, their version age increases as
well, thereby inducing a freshness-reliability trade-off in the network. We
present numerical results to support our findings.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08384" title="Abstract">arXiv:2311.08384</a> [<a href="/pdf/2311.08384" title="Download PDF">pdf</a>, <a href="/format/2311.08384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yifei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sekhari%2C+A">Ayush Sekhari</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yuda Song</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Hybrid RL is the setting where an RL agent has access to both offline data
and online data by interacting with the real-world environment. In this work,
we propose a new hybrid RL algorithm that combines an on-policy actor-critic
method with offline data. On-policy methods such as policy gradient and natural
policy gradient (NPG) have shown to be more robust to model misspecification,
though sometimes it may not be as sample efficient as methods that rely on
off-policy learning. On the other hand, offline methods that depend on
off-policy training often require strong assumptions in theory and are less
stable to train in practice. Our new approach integrates a procedure of
off-policy training on the offline data into an on-policy NPG framework. We
show that our approach, in theory, can obtain a best-of-both-worlds type of
result -- it achieves the state-of-art theoretical guarantees of offline RL
when offline RL-specific assumptions hold, while at the same time maintaining
the theoretical guarantees of on-policy NPG regardless of the offline RL
assumptions' validity. Experimentally, in challenging rich-observation
environments, we show that our approach outperforms a state-of-the-art hybrid
RL baseline which only relies on off-policy policy optimization, demonstrating
the empirical benefit of combining on-policy and off-policy learning. Our code
is publicly available at https://github.com/YifeiZhou02/HNPG.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08385" title="Abstract">arXiv:2311.08385</a> [<a href="/pdf/2311.08385" title="Download PDF">pdf</a>, <a href="/format/2311.08385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChOiRe: Characterizing and Predicting Human Opinions with Chain of  Opinion Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+X+L">Xuan Long Do</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M+Y">Min Yen Kan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Aligning language models (LMs) with human opinion is challenging yet vital to
enhance their grasp of human values, preferences, and beliefs. We present
ChOiRe, a four-step solution framework to predict human opinion that
differentiates between the user explicit personae (i.e. demographic or
ideological attributes) that are manually declared and implicit personae
inferred from user historical opinions. Specifically, it consists of (i) an LM
analyzing the user explicit personae to filter out irrelevant attributes; (ii)
the LM ranking the implicit persona opinions into a preferential list; (iii)
Chain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the
explicit personae and the most relevant implicit personae to perform opinion
prediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times with
increasingly larger lists of implicit personae to overcome insufficient
personae information to infer a final result. ChOiRe achieves new
state-of-the-art effectiveness with limited inference calls, improving previous
LLM-based techniques significantly by 3.22%.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08386" title="Abstract">arXiv:2311.08386</a> [<a href="/pdf/2311.08386" title="Download PDF">pdf</a>, <a href="/format/2311.08386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication Efficiency of Summation over a Quantum Erasure MAC with  Replicated Inputs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuhang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jafar%2C+S+A">Syed A. Jafar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The quantum communication cost of computing a classical sum of distributed
sources is studied over a quantum erasure multiple access channel (QEMAC). $K$
messages are distributed across $S$ servers so that each server knows a subset
of the messages. Each server $s\in[S]$ sends a quantum subsystem
$\mathcal{Q}_s$ to the receiver who computes the sum of the messages. The
download cost from Server $s\in [S]$ is the logarithm of the dimension of
$\mathcal{Q}_s$. The rate $R$ is defined as the number of instances of the sum
computed at the receiver, divided by the total download cost from all the
servers. In the symmetric setting with $K= {S \choose \alpha} $ messages where
each message is replicated among a unique subset of $\alpha$ servers, and the
answers from any $\beta$ servers may be erased, the rate achieved is $R=
\max\left\{ \min \left\{ \frac{2(\alpha-\beta)}{S}, 1-\frac{2\beta}{S}
\right\}, \frac{\alpha-\beta}{S} \right\}$, which is shown to be optimal when
$S\geq 2\alpha$.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08389" title="Abstract">arXiv:2311.08389</a> [<a href="/pdf/2311.08389" title="Download PDF">pdf</a>, <a href="/format/2311.08389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TSST: A Benchmark and Evaluation Models for Text Speech-Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huashan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yixiao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yizhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Text style is highly abstract, as it encompasses various aspects of a
speaker's characteristics, habits, logical thinking, and the content they
express. However, previous text-style transfer tasks have primarily focused on
data-driven approaches, lacking in-depth analysis and research from the
perspectives of linguistics and cognitive science. In this paper, we introduce
a novel task called Text Speech-Style Transfer (TSST). The main objective is to
further explore topics related to human cognition, such as personality and
emotion, based on the capabilities of existing LLMs. Considering the objective
of our task and the distinctive characteristics of oral speech in real-life
scenarios, we trained multi-dimension (i.e. filler words, vividness,
interactivity, emotionality) evaluation models for the TSST and validated their
correlation with human assessments. We thoroughly analyze the performance of
several large language models (LLMs) and identify areas where further
improvement is needed. Moreover, driven by our evaluation models, we have
released a new corpus that improves the capabilities of LLMs in generating text
with speech-style characteristics. In summary, we present the TSST task, a new
benchmark for style transfer and emphasizing human-oriented evaluation,
exploring and advancing the performance of current LLMs.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08390" title="Abstract">arXiv:2311.08390</a> [<a href="/pdf/2311.08390" title="Download PDF">pdf</a>, <a href="/format/2311.08390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On What Basis? Predicting Text Preference Via Structured Comparative  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J+N">Jing Nathan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+J+T">Justin T Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiaming Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lakshmanan%2C+C">Charu Lakshmanan</a>, 
<a href="/search/cs?searchtype=author&query=Kurzion%2C+Y">Yair Kurzion</a>, 
<a href="/search/cs?searchtype=author&query=Rush%2C+A+M">Alexander M. Rush</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jialu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Comparative reasoning plays a crucial role in text preference prediction;
however, large language models (LLMs) often demonstrate inconsistencies in
their reasoning. While approaches like Chain-of-Thought improve accuracy in
many other settings, they struggle to consistently distinguish the similarities
and differences of complex texts. We introduce SC, a prompting approach that
predicts text preferences by generating structured intermediate comparisons. SC
begins by proposing aspects of comparison, followed by generating textual
comparisons under each aspect. We select consistent comparisons with a pairwise
consistency comparator that ensures each aspect's comparisons clearly
distinguish differences between texts, significantly reducing hallucination and
improving consistency. Our comprehensive evaluations across various NLP tasks,
including summarization, retrieval, and automatic rating, demonstrate that SC
equips LLMs to achieve state-of-the-art performance in text preference
prediction.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08391" title="Abstract">arXiv:2311.08391</a> [<a href="/pdf/2311.08391" title="Download PDF">pdf</a>, <a href="/format/2311.08391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Material Lens on Coloniality in NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Held%2C+W">William Held</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+C">Camille Harris</a>, 
<a href="/search/cs?searchtype=author&query=Best%2C+M">Michael Best</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Coloniality, the continuation of colonial harms beyond "official"
colonization, has pervasive effects across society and scientific fields.
Natural Language Processing (NLP) is no exception to this broad phenomenon. In
this work, we argue that coloniality is implicitly embedded in and amplified by
NLP data, algorithms, and software. We formalize this analysis using
Actor-Network Theory (ANT): an approach to understanding social phenomena
through the network of relationships between human stakeholders and technology.
We use our Actor-Network to guide a quantitative survey of the geography of
different phases of NLP research, providing evidence that inequality along
colonial boundaries increases as NLP builds on itself. Based on this, we argue
that combating coloniality in NLP requires not only changing current values but
also active work to remove the accumulation of colonial ideals in our
foundational data and algorithms.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08392" title="Abstract">arXiv:2311.08392</a> [<a href="/pdf/2311.08392" title="Download PDF">pdf</a>, <a href="/format/2311.08392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Network Pricing for Ridesharing Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chenkai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hongyao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Ridesharing platforms match riders and drivers, using dynamic pricing to
balance supply and demand. The origin-based "surge pricing", however, does not
take into consideration market conditions at trip destinations, leading to
inefficient driver flows in space and incentivizes drivers to strategize. In
this work, we introduce the Iterative Network Pricing mechanism, addressing a
main challenge in the practical implementation of optimal origin-destination
(OD) based prices, that the model for rider demand is hard to estimate.
Assuming that the platform's surge algorithm clears the market for each origin
in real-time, our mechanism updates the OD-based price adjustments
week-over-week, using only information immediately observable during the same
time window in the prior weeks. For stationary market conditions, we prove that
our mechanism converges to an outcome that is approximately welfare-optimal.
Using data from the City of Chicago, we illustrate (via simulation) the
iterative updates under our mechanism for morning rush hours, demonstrating
substantial welfare improvements despite significant fluctuations of market
conditions from early 2019 through the end of 2020.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08393" title="Abstract">arXiv:2311.08393</a> [<a href="/pdf/2311.08393" title="Download PDF">pdf</a>, <a href="/format/2311.08393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable  Trajectory Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asali%2C+E">Ehsan Asali</a>, 
<a href="/search/cs?searchtype=author&query=Doshi%2C+P">Prashant Doshi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jin Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Robot Learning 2023 (CoRL2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">The learn-from-observation (LfO) paradigm is a human-inspired mode for a
robot to learn to perform a task simply by watching it being performed. LfO can
facilitate robot integration on factory floors by minimizing disruption and
reducing tedious programming. A key component of the LfO pipeline is a
transformation of the depth camera frames to the corresponding task state and
action pairs, which are then relayed to learning techniques such as imitation
or inverse reinforcement learning for understanding the task parameters. While
several existing computer vision models analyze videos for activity
recognition, SA-Net specifically targets robotic LfO from RGB-D data. However,
SA-Net and many other models analyze frame data captured from a single
viewpoint. Their analysis is therefore highly sensitive to occlusions of the
observed task, which are frequent in deployments. An obvious way of reducing
occlusions is to simultaneously observe the task from multiple viewpoints and
synchronously fuse the multiple streams in the model. Toward this, we present
multi-view SA-Net, which generalizes the SA-Net model to allow the perception
of multiple viewpoints of the task activity, integrate them, and better
recognize the state and action in each frame. Performance evaluations on two
distinct domains establish that MVSA-Net recognizes the state-action pairs
under occlusion more accurately compared to single-view MVSA-Net and other
baselines. Our ablation studies further evaluate its performance under
different ambient conditions and establish the contribution of the architecture
components. As such, MVSA-Net offers a significantly more robust and deployable
state-action trajectory generation compared to previous methods.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08398" title="Abstract">arXiv:2311.08398</a> [<a href="/pdf/2311.08398" title="Download PDF">pdf</a>, <a href="/format/2311.08398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Temporally Grounded?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yifu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ziser%2C+Y">Yftah Ziser</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Ponti%2C+E+M">Edoardo M. Ponti</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Are Large language models (LLMs) temporally grounded? Since LLMs cannot
perceive and interact with the environment, it is impossible to answer this
question directly. Instead, we provide LLMs with textual narratives and probe
them with respect to their common-sense knowledge of the structure and duration
of events, their ability to order events along a timeline, and self-consistency
within their temporal model (e.g., temporal relations such as after and before
are mutually exclusive for any pair of events). We evaluate state-of-the-art
LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.
Generally, we find that LLMs lag significantly behind both human performance as
well as small-scale, specialised LMs. In-context learning, instruction tuning,
and chain-of-thought prompting reduce this gap only to a limited degree.
Crucially, LLMs struggle the most with self-consistency, displaying incoherent
behaviour in at least 27.23% of their predictions. Contrary to expectations, we
also find that scaling the model size does not guarantee positive gains in
performance. To explain these results, we study the sources from which LLMs may
gather temporal information: we find that sentence ordering in unlabelled
texts, available during pre-training, is only weakly correlated with event
ordering. Moreover, public instruction tuning mixtures contain few temporal
tasks. Hence, we conclude that current LLMs lack a consistent temporal model of
textual narratives. Code, datasets, and LLM outputs are available at
https://github.com/yfqiu-nlp/temporal-llms.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08399" title="Abstract">arXiv:2311.08399</a> [<a href="/pdf/2311.08399" title="Download PDF">pdf</a>, <a href="/ps/2311.08399" title="Download PostScript">ps</a>, <a href="/format/2311.08399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constant Query Local Decoding Against Deletions Is Impossible
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Meghal Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Locally decodable codes (LDC's) are error-correcting codes that allow
recovery of individual message indices by accessing only a constant number of
codeword indices. For substitution errors, it is evident that LDC's exist --
Hadamard codes are examples of $2$-query LDC's. Research on this front has
focused on finding the optimal encoding length for LDC's, for which there is a
nearly exponential gap between the best lower bounds and constructions.
<br />Ostrovsky and Paskin-Cherniavsky (ICITS 2015) introduced the notion of local
decoding to the insertion and deletion setting. In this context, it is not
clear whether constant query LDC's exist at all. Indeed, in contrast to the
classical setting, Block et al. conjecture that they do not exist. Blocki et
al. (FOCS 2021) make progress towards this conjecture, proving that any
potential code must have at least exponential encoding length.
<br />Our work definitively resolves the conjecture and shows that constant query
LDC's do not exist in the insertion/deletion (or even deletion-only) setting.
Using a reduction shown by Blocki et al., this also implies that constant query
locally correctable codes do not exist in this setting.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08400" title="Abstract">arXiv:2311.08400</a> [<a href="/pdf/2311.08400" title="Download PDF">pdf</a>, <a href="/format/2311.08400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Open-Ended Visual Recognition with Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qihang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaohui Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang-Chieh Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Localizing and recognizing objects in the open-ended physical world poses a
long-standing challenge within the domain of machine perception. Recent methods
have endeavored to address the issue by employing a class-agnostic mask (or
box) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP)
using pre-extracted text embeddings. However, it is worth noting that these
open-vocabulary recognition models still exhibit limitations in practical
applications. On one hand, they rely on the provision of class names during
testing, where the recognition performance heavily depends on this predefined
set of semantic classes by users. On the other hand, when training with
multiple datasets, human intervention is required to alleviate the label
definition conflict between them. In this paper, we introduce the OmniScient
Model (OSM), a novel Large Language Model (LLM) based mask classifier, as a
straightforward and effective solution to the aforementioned challenges.
Specifically, OSM predicts class labels in a generative manner, thus removing
the supply of class names during both training and testing. It also enables
cross-dataset training without any human interference, exhibiting robust
generalization capabilities due to the world knowledge acquired from the LLM.
By combining OSM with an off-the-shelf mask proposal model, we present
promising results on various benchmarks, and demonstrate its effectiveness in
handling novel concepts. Code/model are available at
https://github.com/bytedance/OmniScient-Model.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08401" title="Abstract">arXiv:2311.08401</a> [<a href="/pdf/2311.08401" title="Download PDF">pdf</a>, <a href="/format/2311.08401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning Language Models for Factuality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+K">Katherine Tian</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+E">Eric Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C+D">Christopher D. Manning</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The fluency and creativity of large pre-trained language models (LLMs) have
led to their widespread use, sometimes even as a replacement for traditional
search engines. Yet language models are prone to making convincing but
factually inaccurate claims, often referred to as 'hallucinations.' These
errors can inadvertently spread misinformation or harmfully perpetuate
misconceptions. Further, manual fact-checking of model responses is a
time-consuming process, making human factuality labels expensive to acquire. In
this work, we fine-tune language models to be more factual, without human
labeling and targeting more open-ended generation settings than past work. We
leverage two key recent innovations in NLP to do so. First, several recent
works have proposed methods for judging the factuality of open-ended text by
measuring consistency with an external knowledge base or simply a large model's
confidence scores. Second, the direct preference optimization algorithm enables
straightforward fine-tuning of language models on objectives other than
supervised imitation, using a preference ranking over possible model responses.
We show that learning from automatically generated factuality preference
rankings, generated either through existing retrieval systems or our novel
retrieval-free approach, significantly improves the factuality (percent of
generated claims that are correct) of Llama-2 on held-out topics compared with
RLHF or decoding strategies targeted at factuality. At 7B scale, compared to
Llama-2-chat, we observe 58% and 40% reduction in factual error rate when
generating biographies and answering medical questions, respectively.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08402" title="Abstract">arXiv:2311.08402</a> [<a href="/pdf/2311.08402" title="Download PDF">pdf</a>, <a href="/format/2311.08402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieve and Copy: Scaling ASR Personalization to Large Catalogs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jayanthi%2C+S+M">Sai Muralidhar Jayanthi</a>, 
<a href="/search/cs?searchtype=author&query=Kulshreshtha%2C+D">Devang Kulshreshtha</a>, 
<a href="/search/cs?searchtype=author&query=Dingliwal%2C+S">Saket Dingliwal</a>, 
<a href="/search/cs?searchtype=author&query=Ronanki%2C+S">Srikanth Ronanki</a>, 
<a href="/search/cs?searchtype=author&query=Bodapati%2C+S">Sravan Bodapati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Personalization of automatic speech recognition (ASR) models is a widely
studied topic because of its many practical applications. Most recently,
attention-based contextual biasing techniques are used to improve the
recognition of rare words and domain specific entities. However, due to
performance constraints, the biasing is often limited to a few thousand
entities, restricting real-world usability. To address this, we first propose a
"Retrieve and Copy" mechanism to improve latency while retaining the accuracy
even when scaled to a large catalog. We also propose a training strategy to
overcome the degradation in recall at such scale due to an increased number of
confusing entities. Overall, our approach achieves up to 6% more Word Error
Rate reduction (WERR) and 3.6% absolute improvement in F1 when compared to a
strong baseline. Our method also allows for large catalog sizes of up to 20K
without significantly affecting WER and F1-scores, while achieving at least 20%
inference speedup per acoustic frame.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08403" title="Abstract">arXiv:2311.08403</a> [<a href="/pdf/2311.08403" title="Download PDF">pdf</a>, <a href="/format/2311.08403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instant3D: Instant Text-to-3D Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jia-Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Keppo%2C+J">Jussi Keppo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shuicheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiangyu Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://ming1993li.github.io/Instant3DProj">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Text-to-3D generation, which aims to synthesize vivid 3D objects from text
prompts, has attracted much attention from the computer vision community. While
several existing works have achieved impressive results for this task, they
mainly rely on a time-consuming optimization paradigm. Specifically, these
methods optimize a neural field from scratch for each text prompt, taking
approximately one hour or more to generate one object. This heavy and
repetitive training cost impedes their practical deployment. In this paper, we
propose a novel framework for fast text-to-3D generation, dubbed Instant3D.
Once trained, Instant3D is able to create a 3D object for an unseen text prompt
in less than one second with a single run of a feedforward network. We achieve
this remarkable speed by devising a new network that directly constructs a 3D
triplane from a text prompt. The core innovation of our Instant3D lies in our
exploration of strategies to effectively inject text conditions into the
network. Furthermore, we propose a simple yet effective activation function,
the scaled-sigmoid, to replace the original sigmoid function, which speeds up
the training convergence by more than ten times. Finally, to address the Janus
(multi-head) problem in 3D generation, we propose an adaptive Perp-Neg
algorithm that can dynamically adjust its concept negation scales according to
the severity of the Janus problem during training, effectively reducing the
multi-head effect. Extensive experiments on a wide variety of benchmark
datasets demonstrate that the proposed algorithm performs favorably against the
state-of-the-art methods both qualitatively and quantitatively, while achieving
significantly better efficiency. The project page is at
https://ming1993li.github.io/Instant3DProj.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Wed, 15 Nov 23</h3>
<dl>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07598" title="Abstract">arXiv:2311.07598</a> (cross-list from q-fin.ST) [<a href="/pdf/2311.07598" title="Download PDF">pdf</a>, <a href="/format/2311.07598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Label Topic Model for Financial Textual Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Scherrmann%2C+M">Moritz Scherrmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a multi-label topic model for financial texts like ad-hoc
announcements, 8-K filings, finance related news or annual reports. I train the
model on a new financial multi-label database consisting of 3,044 German ad-hoc
announcements that are labeled manually using 20 predefined, economically
motivated topics. The best model achieves a macro F1 score of more than 85%.
Translating the data results in an English version of the model with similar
performance. As application of the model, I investigate differences in stock
market reactions across topics. I find evidence for strong positive or negative
market reactions for some topics, like announcements of new Large Scale
Projects or Bankruptcy Filings, while I do not observe significant price
effects for some other topics. Furthermore, in contrast to previous studies,
the multi-label structure of the model allows to analyze the effects of
co-occurring topics on stock market reactions. For many cases, the reaction to
a specific topic depends heavily on the co-occurrence with other topics. For
example, if allocated capital from a Seasoned Equity Offering (SEO) is used for
restructuring a company in the course of a Bankruptcy Proceeding, the market
reacts positively on average. However, if that capital is used for covering
unexpected, additional costs from the development of new drugs, the SEO implies
negative reactions on average.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07609" title="Abstract">arXiv:2311.07609</a> (cross-list from q-bio.QM) [<a href="/pdf/2311.07609" title="Download PDF">pdf</a>, <a href="/ps/2311.07609" title="Download PostScript">ps</a>, <a href="/format/2311.07609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Intelligence in Assessing Cardiovascular Diseases and Risk  Factors via Retinal Fundus Images: A Review of the Last Decade
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Abdollahi%2C+M">Mirsaeed Abdollahi</a>, 
<a href="/search/q-bio?searchtype=author&query=Jafarizadeh%2C+A">Ali Jafarizadeh</a>, 
<a href="/search/q-bio?searchtype=author&query=Asbagh%2C+A+G">Amirhosein Ghafouri Asbagh</a>, 
<a href="/search/q-bio?searchtype=author&query=Sobhi%2C+N">Navid Sobhi</a>, 
<a href="/search/q-bio?searchtype=author&query=Pourmoghtader%2C+K">Keysan Pourmoghtader</a>, 
<a href="/search/q-bio?searchtype=author&query=Pedrammehr%2C+S">Siamak Pedrammehr</a>, 
<a href="/search/q-bio?searchtype=author&query=Asadi%2C+H">Houshyar Asadi</a>, 
<a href="/search/q-bio?searchtype=author&query=Alizadehsani%2C+R">Roohallah Alizadehsani</a>, 
<a href="/search/q-bio?searchtype=author&query=Tan%2C+R">Ru-San Tan</a>, 
<a href="/search/q-bio?searchtype=author&query=Acharya%2C+U+R">U. Rajendra Acharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 5 figures, 2 tables, 91 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Background: Cardiovascular diseases (CVDs) continue to be the leading cause
of mortality on a global scale. In recent years, the application of artificial
intelligence (AI) techniques, particularly deep learning (DL), has gained
considerable popularity for evaluating the various aspects of CVDs. Moreover,
using fundus images and optical coherence tomography angiography (OCTA) to
diagnose retinal diseases has been extensively studied. To better understand
heart function and anticipate changes based on microvascular characteristics
and function, researchers are currently exploring the integration of AI with
non-invasive retinal scanning. Leveraging AI-assisted early detection and
prediction of cardiovascular diseases on a large scale holds excellent
potential to mitigate cardiovascular events and alleviate the economic burden
on healthcare systems. Method: A comprehensive search was conducted across
various databases, including PubMed, Medline, Google Scholar, Scopus, Web of
Sciences, IEEE Xplore, and ACM Digital Library, using specific keywords related
to cardiovascular diseases and artificial intelligence. Results: A total of 87
English-language publications, selected for relevance were included in the
study, and additional references were considered. This study presents an
overview of the current advancements and challenges in employing retinal
imaging and artificial intelligence to identify cardiovascular disorders and
provides insights for further exploration in this field. Conclusion:
Researchers aim to develop precise disease prognosis patterns as the aging
population and global CVD burden increase. AI and deep learning are
transforming healthcare, offering the potential for single retinal image-based
diagnosis of various CVDs, albeit with the need for accelerated adoption in
healthcare systems.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07621" title="Abstract">arXiv:2311.07621</a> (cross-list from q-bio.GN) [<a href="/pdf/2311.07621" title="Download PDF">pdf</a>, <a href="/format/2311.07621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To Transformers and Beyond: Large Language Models for the Genome
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Consens%2C+M+E">Micaela E. Consens</a>, 
<a href="/search/q-bio?searchtype=author&query=Dufault%2C+C">Cameron Dufault</a>, 
<a href="/search/q-bio?searchtype=author&query=Wainberg%2C+M">Michael Wainberg</a>, 
<a href="/search/q-bio?searchtype=author&query=Forster%2C+D">Duncan Forster</a>, 
<a href="/search/q-bio?searchtype=author&query=Karimzadeh%2C+M">Mehran Karimzadeh</a>, 
<a href="/search/q-bio?searchtype=author&query=Goodarzi%2C+H">Hani Goodarzi</a>, 
<a href="/search/q-bio?searchtype=author&query=Theis%2C+F+J">Fabian J. Theis</a>, 
<a href="/search/q-bio?searchtype=author&query=Moses%2C+A">Alan Moses</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+B">Bo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the rapidly evolving landscape of genomics, deep learning has emerged as a
useful tool for tackling complex computational challenges. This review focuses
on the transformative role of Large Language Models (LLMs), which are mostly
based on the transformer architecture, in genomics. Building on the foundation
of traditional convolutional neural networks and recurrent neural networks, we
explore both the strengths and limitations of transformers and other LLMs for
genomics. Additionally, we contemplate the future of genomic modeling beyond
the transformer architecture based on current trends in research. The paper
aims to serve as a guide for computational biologists and computer scientists
interested in LLMs for genomic data. We hope the paper can also serve as an
educational introduction and discussion for biologists to a fundamental shift
in how we will be analyzing genomic data in the future.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07626" title="Abstract">arXiv:2311.07626</a> (cross-list from quant-ph) [<a href="/pdf/2311.07626" title="Download PDF">pdf</a>, <a href="/format/2311.07626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Machine Learning for Remote Sensing: Exploring potential and  challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Miroszewski%2C+A">Artur Miroszewski</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nalepa%2C+J">Jakub Nalepa</a>, 
<a href="/search/quant-ph?searchtype=author&query=Saux%2C+B+L">Bertrand Le Saux</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mielczarek%2C+J">Jakub Mielczarek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 2 figures. Presented at the Big Data from Space 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The industry of quantum technologies is rapidly expanding, offering promising
opportunities for various scientific domains. Among these emerging
technologies, Quantum Machine Learning (QML) has attracted considerable
attention due to its potential to revolutionize data processing and analysis.
In this paper, we investigate the application of QML in the field of remote
sensing. It is believed that QML can provide valuable insights for analysis of
data from space. We delve into the common beliefs surrounding the quantum
advantage in QML for remote sensing and highlight the open challenges that need
to be addressed. To shed light on the challenges, we conduct a study focused on
the problem of kernel value concentration, a phenomenon that adversely affects
the runtime of quantum computers. Our findings indicate that while this issue
negatively impacts quantum computer performance, it does not entirely negate
the potential quantum advantage in QML for remote sensing.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07636" title="Abstract">arXiv:2311.07636</a> (cross-list from q-bio.GN) [<a href="/pdf/2311.07636" title="Download PDF">pdf</a>, <a href="/format/2311.07636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-based Multi-task Learning for Base Editor Outcome Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Mollaysa%2C+A">Amina Mollaysa</a>, 
<a href="/search/q-bio?searchtype=author&query=Allam%2C+A">Ahmed Allam</a>, 
<a href="/search/q-bio?searchtype=author&query=Krauthamme%2C+M">Michael Krauthamme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 15 pages. arXiv admin note: substantial text overlap with <a href="/abs/2310.02919">arXiv:2310.02919</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Human genetic diseases often arise from point mutations, emphasizing the
critical need for precise genome editing techniques. Among these, base editing
stands out as it allows targeted alterations at the single nucleotide level.
However, its clinical application is hindered by low editing efficiency and
unintended mutations, necessitating extensive trial-and-error experimentation
in the laboratory. To speed up this process, we present an attention-based
two-stage machine learning model that learns to predict the likelihood of all
possible editing outcomes for a given genomic target sequence. We further
propose a multi-task learning schema to jointly learn multiple base editors
(i.e. variants) at once. Our model's predictions consistently demonstrated a
strong correlation with the actual experimental results on multiple datasets
and base editor variants. These results provide further validation for the
models' capacity to enhance and accelerate the process of refining base editing
designs.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07676" title="Abstract">arXiv:2311.07676</a> (cross-list from math.OC) [<a href="/pdf/2311.07676" title="Download PDF">pdf</a>, <a href="/format/2311.07676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Centralized calibration of power system dynamic models using variational  data assimilation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Attia%2C+A">Ahmed Attia</a>, 
<a href="/search/math?searchtype=author&query=Maldonado%2C+D+A">D. Adrian Maldonado</a>, 
<a href="/search/math?searchtype=author&query=Constantinescu%2C+E">Emil Constantinescu</a>, 
<a href="/search/math?searchtype=author&query=Anitescu%2C+M">Mihai Anitescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures, and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">This paper presents a novel centralized, variational data assimilation
approach for calibrating transient dynamic models in electrical power systems,
focusing on load model parameters. With the increasing importance of
inverter-based resources, assessing power systems' dynamic performance under
disturbances has become challenging, necessitating robust model calibration
methods. The proposed approach expands on previous Bayesian frameworks by
establishing a posterior distribution of parameters using an approximation
around the maximum a posteriori value. We illustrate the efficacy of our method
by generating events of varying intensity, highlighting its ability to capture
the systems' evolution accurately and with associated uncertainty estimates.
This research improves the precision of dynamic performance assessments in
modern power systems, with potential applications in managing uncertainties and
optimizing system operations.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07678" title="Abstract">arXiv:2311.07678</a> (cross-list from math.AG) [<a href="/pdf/2311.07678" title="Download PDF">pdf</a>, <a href="/format/2311.07678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Implicitizations of Multi-Graded Polynomial Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cummings%2C+J">Joseph Cummings</a>, 
<a href="/search/math?searchtype=author&query=Hollering%2C+B">Benjamin Hollering</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 2 figures. An implementation of our main algorithm can be found on our MathRepo page as well as our GitHub
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Symbolic Computation (cs.SC); Commutative Algebra (math.AC); Statistics Theory (math.ST)

</div>
<p class="mathjax">In this paper, we focus on computing the kernel of a map of polynomial rings
$\varphi$. This core problem in symbolic computation is known as
implicitization. While there are extremely effective Gr\"obner basis methods
used to solve this problem, these methods can become infeasible as the number
of variables increases. In the case when the map $\varphi$ is multigraded, we
consider an alternative approach. We demonstrate how to quickly compute a
matrix of maximal rank for which $\varphi$ has a positive multigrading. Then in
each graded component we compute the minimal generators of the kernel in that
multidegree with linear algebra. We have implemented our techniques in
Macaulay2 and show that our implementation can compute many generators of low
degree in examples where Gr\"obner techniques have failed. This includes
several examples coming from phylogenetics where even a complete list of
quadrics and cubics were unknown. When the multigrading refines total degree,
our algorithm is \emph{embarassingly parallel} and a fully parallelized version
of our algorithm will be forthcoming in OSCAR.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07726" title="Abstract">arXiv:2311.07726</a> (cross-list from quant-ph) [<a href="/pdf/2311.07726" title="Download PDF">pdf</a>, <a href="/format/2311.07726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Quantum Blockmodeling with Qubits and Permutations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Daskin%2C+A">Ammar Daskin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Blockmodeling of a given problem represented by an $N\times N$ adjacency
matrix can be found by swapping rows and columns of the matrix (i.e.
multiplying matrix from left and right by a permutation matrix). In general,
through performing this task, row and column permutations affect the fitness
value in optimization: For an $N\times N$ matrix, it requires $O(N)$
computations to find (or update) the fitness value of a candidate solution.
<br />On quantum computers, permutations can be applied in parallel and
efficiently, and their implementations can be as simple as a single qubit
operation (a NOT gate on a qubit) which takes an $O(1)$ time algorithmic step.
In this paper, using permutation matrices, we describe a quantum blockmodeling
for data analysis tasks. In the model, the measurement outcome of a small group
of qubits are mapped to indicate the fitness value. Therefore, we show that it
is possible to find or update the fitness value in $O(log(N))$ time. This lead
us to show that when the number of iterations are less than $log(N)$ time, it
may be possible to reach the same solution exponentially faster on quantum
computers in comparison to classical computers. In addition, since on quantum
circuits the different sequence of permutations can be applied in parallel
(superpositon), the machine learning task in this model can be implemented more
efficiently on quantum computers.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07781" title="Abstract">arXiv:2311.07781</a> (cross-list from math.OC) [<a href="/pdf/2311.07781" title="Download PDF">pdf</a>, <a href="/ps/2311.07781" title="Download PostScript">ps</a>, <a href="/format/2311.07781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Ex Post Condition for the Exactness of Optimal Power Flow Conic  Relaxations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lupien%2C+J">Jean-Luc Lupien</a>, 
<a href="/search/math?searchtype=author&query=Lesage-Landry%2C+A">Antoine Lesage-Landry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Convex relaxations of the optimal power flow problem (OPF) provide an
efficient alternative to solving the NP-hard alternating current (AC) optimal
power flow. Conic relaxations of the OPF, in particular, greatly accelerate
resolution while leading to high-quality approximations that are exact in
several scenarios. However, the sufficient conditions guaranteeing exactness
are stringent, e.g., requiring radial networks. In this letter, we present an
ex post condition for the exactness of conic relaxations of the OPF. Instead of
relying on satisfying necessary conditions a priori, the operator can obtain an
exactness certificate for the computed solution. This enables the use of conic
relaxations for networks where exactness requirements are not met while still
providing an optimality guarantee.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07785" title="Abstract">arXiv:2311.07785</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2311.07785" title="Download PDF">pdf</a>, <a href="/format/2311.07785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fracture of bio-cemented sands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Konstantinou%2C+C">C. Konstantinou</a>, 
<a href="/search/cond-mat?searchtype=author&query=Mart%C3%ADnez-Pa%C3%B1eda%2C+E">E. Mart&#xed;nez-Pa&#xf1;eda</a>, 
<a href="/search/cond-mat?searchtype=author&query=Biscontin%2C+G">G. Biscontin</a>, 
<a href="/search/cond-mat?searchtype=author&query=Fleck%2C+N+A">N.A. Fleck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Computational Engineering, Finance, and Science (cs.CE); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">Bio-chemical reactions enable the production of biomimetic materials such as
sandstones. In the present study, microbiologically-induced calcium carbonate
precipitation (MICP) is used to manufacture laboratory-scale specimens for
fracture toughness measurement. The mode I and mixed-mode fracture toughnesses
are measured as a function of cementation, and are correlated with strength,
permeability and porosity. A micromechanical model is developed to predict the
dependence of mode I fracture toughness upon the degree of cementation. In
addition, the role of the crack tip $T$-stress in dictating kink angle and
toughness is determined for mixed mode loading. At a sufficiently low degree of
cementation, the zone of microcracking in the vicinity of the crack tip is
sufficiently large for a crack tip $K$-field to cease to exist and for crack
kinking theory to not apply. The interplay between cementation and fracture
properties of sedimentary rocks is explained; this understanding underpins a
wide range of rock fracture phenomena including hydraulic fracture.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07787" title="Abstract">arXiv:2311.07787</a> (cross-list from cond-mat.supr-con) [<a href="/pdf/2311.07787" title="Download PDF">pdf</a>, <a href="/format/2311.07787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Synaptic Structure for Spiking Neural Network Realization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Razmkhah%2C+S">Sasan Razmkhah</a>, 
<a href="/search/cond-mat?searchtype=author&query=Karamuftuoglu%2C+M+A">Mustafa Altay Karamuftuoglu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Bozbey%2C+A">Ali Bozbey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Superconductivity (cond-mat.supr-con)</span>; Hardware Architecture (cs.AR); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Neural networks and neuromorphic computing play pivotal roles in deep
learning and machine vision. Due to their dissipative nature and inherent
limitations, traditional semiconductor-based circuits face challenges in
realizing ultra-fast and low-power neural networks. However, the spiking
behavior characteristic of single flux quantum (SFQ) circuits positions them as
promising candidates for spiking neural networks (SNNs). Our previous work
showcased a JJ-Soma design capable of operating at tens of gigahertz while
consuming only a fraction of the power compared to traditional circuits, as
documented in [1]. This paper introduces a compact SFQ-based synapse design
that applies positive and negative weighted inputs to the JJ-Soma. Using an
RSFQ synapse empowers us to replicate the functionality of a biological neuron,
a crucial step in realizing a complete SNN. The JJ-Synapse can operate at
ultra-high frequencies, exhibits orders of magnitude lower power consumption
than CMOS counterparts, and can be conveniently fabricated using commercial Nb
processes. Furthermore, the network's flexibility enables modifications by
incorporating cryo-CMOS circuits for weight value adjustments. In our endeavor,
we have successfully designed, fabricated, and partially tested the JJ-Synapse
within our cryocooler system. Integration with the JJ-Soma further facilitates
the realization of a high-speed inference SNN.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07794" title="Abstract">arXiv:2311.07794</a> (cross-list from quant-ph) [<a href="/pdf/2311.07794" title="Download PDF">pdf</a>, <a href="/ps/2311.07794" title="Download PostScript">ps</a>, <a href="/format/2311.07794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Use Quantum Indistinguishability Obfuscation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Coladangelo%2C+A">Andrea Coladangelo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gunn%2C+S">Sam Gunn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Quantum copy protection, introduced by Aaronson, enables giving out a quantum
program-description that cannot be meaningfully duplicated. Despite over a
decade of study, copy protection is only known to be possible for a very
limited class of programs. As our first contribution, we show how to achieve
"best-possible" copy protection for all programs. We do this by introducing
quantum state indistinguishability obfuscation (qsiO), a notion of obfuscation
for quantum descriptions of classical programs. We show that applying qsiO to a
program immediately achieves best-possible copy protection. Our second
contribution is to show that, assuming injective one-way functions exist, qsiO
is concrete copy protection for a large family of puncturable programs --
significantly expanding the class of copy-protectable programs. A key tool in
our proof is a new variant of unclonable encryption (UE) that we call coupled
unclonable encryption (cUE). While constructing UE in the standard model
remains an important open problem, we are able to build cUE from one-way
functions. If we additionally assume the existence of UE, then we can further
expand the class of puncturable programs for which qsiO is copy protection.
Finally, we construct qsiO relative to an efficient quantum oracle.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07896" title="Abstract">arXiv:2311.07896</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2311.07896" title="Download PDF">pdf</a>, <a href="/format/2311.07896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Conditional Diffusion Models for Versatile Spatiotemporal  Turbulence Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gao%2C+H">Han Gao</a>, 
<a href="/search/physics?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/physics?searchtype=author&query=Fan%2C+X">Xiantao Fan</a>, 
<a href="/search/physics?searchtype=author&query=Sun%2C+L">Luning Sun</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+L">Li-Ping Liu</a>, 
<a href="/search/physics?searchtype=author&query=Duan%2C+L">Lian Duan</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+J">Jian-Xun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 31 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Turbulent flows have historically presented formidable challenges to
predictive computational modeling. Traditional numerical simulations often
require vast computational resources, making them infeasible for numerous
engineering applications. As an alternative, deep learning-based surrogate
models have emerged, offering data-drive solutions. However, these are
typically constructed within deterministic settings, leading to shortfall in
capturing the innate chaotic and stochastic behaviors of turbulent dynamics. We
introduce a novel generative framework grounded in probabilistic diffusion
models for versatile generation of spatiotemporal turbulence. Our method
unifies both unconditional and conditional sampling strategies within a
Bayesian framework, which can accommodate diverse conditioning scenarios,
including those with a direct differentiable link between specified conditions
and generated unsteady flow outcomes, and scenarios lacking such explicit
correlations. A notable feature of our approach is the method proposed for
long-span flow sequence generation, which is based on autoregressive
gradient-based conditional sampling, eliminating the need for cumbersome
retraining processes. We showcase the versatile turbulence generation
capability of our framework through a suite of numerical experiments,
including: 1) the synthesis of LES simulated instantaneous flow sequences from
URANS inputs; 2) holistic generation of inhomogeneous, anisotropic wall-bounded
turbulence, whether from given initial conditions, prescribed turbulence
statistics, or entirely from scratch; 3) super-resolved generation of
high-speed turbulent boundary layer flows from low-resolution data across a
range of input resolutions. Collectively, our numerical experiments highlight
the merit and transformative potential of the proposed methods, making a
significant advance in the field of turbulence generation.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07908" title="Abstract">arXiv:2311.07908</a> (cross-list from eess.SP) [<a href="/pdf/2311.07908" title="Download PDF">pdf</a>, <a href="/format/2311.07908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Bayes-Optimal Channel Estimation for Holographic MIMO in  Unknown EM Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yu%2C+W">Wentao Yu</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+H">Hengtao He</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+X">Xianghao Yu</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+S">Shenghui Song</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Murch%2C+R+D">Ross D. Murch</a>, 
<a href="/search/eess?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, 1 table, submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Holographic MIMO (HMIMO) has recently been recognized as a promising enabler
for future 6G systems through the use of an ultra-massive number of antennas in
a compact space to exploit the propagation characteristics of the
electromagnetic (EM) channel. Nevertheless, the promised gain of HMIMO could
not be fully unleashed without an efficient means to estimate the
high-dimensional channel. Bayes-optimal estimators typically necessitate either
a large volume of supervised training samples or a priori knowledge of the true
channel distribution, which could hardly be available in practice due to the
enormous system scale and the complicated EM environments. It is thus important
to design a Bayes-optimal estimator for the HMIMO channels in arbitrary and
unknown EM environments, free of any supervision or priors. This work proposes
a self-supervised minimum mean-square-error (MMSE) channel estimation algorithm
based on powerful machine learning tools, i.e., score matching and principal
component analysis. The training stage requires only the pilot signals, without
knowing the spatial correlation, the ground-truth channels, or the received
signal-to-noise-ratio. Simulation results will show that, even being totally
self-supervised, the proposed algorithm can still approach the performance of
the oracle MMSE method with an extremely low complexity, making it a
competitive candidate in practice.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07919" title="Abstract">arXiv:2311.07919</a> (cross-list from eess.AS) [<a href="/pdf/2311.07919" title="Download PDF">pdf</a>, <a href="/format/2311.07919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qwen-Audio: Advancing Universal Audio Understanding via Unified  Large-Scale Audio-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chu%2C+Y">Yunfei Chu</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+X">Xiaohuan Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Q">Qian Yang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+Z">Zhijie Yan</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code is released at <a href="https://github.com/QwenLM/Qwen-Audio">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, instruction-following audio-language models have received broad
attention for audio interaction with humans. However, the absence of
pre-trained audio models capable of handling diverse audio types and tasks has
hindered progress in this field. Consequently, most existing works have only
been able to support a limited range of interaction capabilities. In this
paper, we develop the Qwen-Audio model and address this limitation by scaling
up audio-language pre-training to cover over 30 tasks and various audio types,
such as human speech, natural sounds, music, and songs, to facilitate universal
audio understanding abilities. However, directly co-training all tasks and
datasets can lead to interference issues, as the textual labels associated with
different datasets exhibit considerable variations due to differences in task
focus, language, granularity of annotation, and text structure. To overcome the
one-to-many interference, we carefully design a multi-task training framework
by conditioning on a sequence of hierarchical tags to the decoder for
encouraging knowledge sharing and avoiding interference through shared and
specified tags respectively. Remarkably, Qwen-Audio achieves impressive
performance across diverse benchmark tasks without requiring any task-specific
fine-tuning, surpassing its counterparts. Building upon the capabilities of
Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from
various audios and text inputs, enabling multi-turn dialogues and supporting
various audio-central scenarios.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07938" title="Abstract">arXiv:2311.07938</a> (cross-list from math.OC) [<a href="/pdf/2311.07938" title="Download PDF">pdf</a>, <a href="/ps/2311.07938" title="Download PostScript">ps</a>, <a href="/format/2311.07938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deadzone-Adapted Disturbance Suppression Control for Global Practical  IOS and Zero Asymptotic Gain to Matched Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Karafyllis%2C+I">Iasson Karafyllis</a>, 
<a href="/search/math?searchtype=author&query=Krstic%2C+M">Miroslav Krstic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper we study a special class of systems: time-invariant control
systems that satisfy the matching condition for which no bounds for the
disturbance and the unknown parameters are known. For this class of systems, we
provide a simple, direct, adaptive control scheme that combines three elements:
(a) nonlinear damping, (b) single-gain adjustment, and (c) deadzone in the
update law. It is the first time that these three tools are combined and the
proposed controller is called a Deadzone-Adapted Disturbance Suppression (DADS)
Controller. The proposed adaptive control scheme achieves for the first time an
attenuation of the plant state to an assignable small level, despite the
presence of disturbances and unknown parameters of arbitrary and unknown
bounds. Moreover, the DADS Controller prevents gain and state drift regardless
of the size of the disturbance and unknown parameter.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07939" title="Abstract">arXiv:2311.07939</a> (cross-list from math.OC) [<a href="/pdf/2311.07939" title="Download PDF">pdf</a>, <a href="/format/2311.07939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discretized Distributed Optimization over Dynamic Digraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Doostmohammadian%2C+M">Mohammadreza Doostmohammadian</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/math?searchtype=author&query=Liaquat%2C+M">Muwahida Liaquat</a>, 
<a href="/search/math?searchtype=author&query=Aghasi%2C+A">Alireza Aghasi</a>, 
<a href="/search/math?searchtype=author&query=Zarrabi%2C+H">Houman Zarrabi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE TASE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">We consider a discrete-time model of continuous-time distributed optimization
over dynamic directed-graphs (digraphs) with applications to distributed
learning. Our optimization algorithm works over general strongly connected
dynamic networks under switching topologies, e.g., in mobile multi-agent
systems and volatile networks due to link failures. Compared to many existing
lines of work, there is no need for bi-stochastic weight designs on the links.
The existing literature mostly needs the link weights to be stochastic using
specific weight-design algorithms needed both at the initialization and at all
times when the topology of the network changes. This paper eliminates the need
for such algorithms and paves the way for distributed optimization over
time-varying digraphs. We derive the bound on the gradient-tracking step-size
and discrete time-step for convergence and prove dynamic stability using
arguments from consensus algorithms, matrix perturbation theory, and Lyapunov
theory. This work, particularly, is an improvement over existing
stochastic-weight undirected networks in case of link removal or packet drops.
This is because the existing literature may need to rerun time-consuming and
computationally complex algorithms for stochastic design, while the proposed
strategy works as long as the underlying network is weight-symmetric and
balanced. The proposed optimization framework finds applications to distributed
classification and learning.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07951" title="Abstract">arXiv:2311.07951</a> (cross-list from stat.ME) [<a href="/pdf/2311.07951" title="Download PDF">pdf</a>, <a href="/format/2311.07951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fast and Simple Algorithm for computing the MLE of Amplitude Density  Function Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Teimouri%2C+M">Mahdi Teimouri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5pages, 1 figure,
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Over the last decades, the family of $\alpha$-stale distributions has proven
to be useful for modelling in telecommunication systems. Particularly, in the
case of radar applications, finding a fast and accurate estimation for the
amplitude density function parameters appears to be very important. In this
work, the maximum likelihood estimator (MLE) is proposed for parameters of the
amplitude distribution. To do this, the amplitude data are \emph{projected} on
the horizontal and vertical axes using two simple transformations. It is proved
that the \emph{projected} data follow a zero-location symmetric $\alpha$-stale
distribution for which the MLE can be computed quite fast. The average of
computed MLEs based on two \emph{projections} is considered as estimator for
parameters of the amplitude distribution. Performance of the proposed
\emph{projection} method is demonstrated through simulation study and analysis
of two sets of real radar data.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07952" title="Abstract">arXiv:2311.07952</a> (cross-list from math.OC) [<a href="/pdf/2311.07952" title="Download PDF">pdf</a>, <a href="/format/2311.07952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-triggered Stabilization of Contracting Systems under Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wakaiki%2C+M">Masashi Wakaiki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We propose self-triggered control schemes for nonlinear systems with
quantized state measurements. Our focus lies on scenarios where both the
controller and the self-triggering mechanism receive only the quantized state
measurement at each sampling time. We assume that the ideal closed-loop system
without quantization or self-triggered sampling is contracting. Moreover, a
growth rate of the open-loop system is assumed to be known. We present two
control strategies that yield the closed-loop stability without Zeno behavior.
The first strategy is implemented under logarithmic quantization and imposes no
time-triggering condition other than setting an upper bound on inter-sampling
times. The second one is a joint design of zooming quantization and periodic
self-triggered sampling, where the adjustable zoom parameter for quantization
changes based on inter-sampling times and is also used for the threshold of
self-triggered sampling. In both strategies, we employ a trajectory-based
approach for stability analysis, where contraction theory plays a key role.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07956" title="Abstract">arXiv:2311.07956</a> (cross-list from eess.SP) [<a href="/pdf/2311.07956" title="Download PDF">pdf</a>, <a href="/ps/2311.07956" title="Download PostScript">ps</a>, <a href="/format/2311.07956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Learning Based Condition Diagnosis Method for Distribution  Network Switchgear
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+W">Wenxi Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+W">Weixi Li</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+W">Weisi Ma</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xinyi Chen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Sizhe Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper introduces a robust, learning-based method for diagnosing the
state of distribution network switchgear, which is crucial for maintaining the
power quality for end users. Traditional diagnostic models often rely heavily
on expert knowledge and lack robustness. To address this, our method
incorporates an expanded feature vector that includes environmental data,
temperature readings, switch position, motor operation, insulation conditions,
and local discharge information. We tackle the issue of high dimensionality
through feature mapping. The method introduces a decision radius to categorize
unlabeled samples and updates the model parameters using a combination of
supervised and unsupervised loss, along with a consistency regularization
function. This approach ensures robust learning even with a limited number of
labeled samples. Comparative analysis demonstrates that this method
significantly outperforms existing models in both accuracy and robustness.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07962" title="Abstract">arXiv:2311.07962</a> (cross-list from q-bio.NC) [<a href="/pdf/2311.07962" title="Download PDF">pdf</a>, <a href="/format/2311.07962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relationship Between Mood, Sleepiness, and EEG Functional Connectivity  by 40 Hz Monaural Beats
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Jo%2C+H">Ha-Na Jo</a>, 
<a href="/search/q-bio?searchtype=author&query=Kweon%2C+Y">Young-Seok Kweon</a>, 
<a href="/search/q-bio?searchtype=author&query=Shin%2C+G">Gi-Hwan Shin</a>, 
<a href="/search/q-bio?searchtype=author&query=Kwak%2C+H">Heon-Gyu Kwak</a>, 
<a href="/search/q-bio?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The monaural beat is known that it can modulate brain and personal states.
However, which changes in brain waves are related to changes in state is still
unclear. Therefore, we aimed to investigate the effects of monaural beats and
find the relationship between them. Ten participants took part in five separate
random sessions, which included a baseline session and four sessions with
monaural beats stimulation: one audible session and three inaudible sessions.
Electroencephalogram (EEG) were recorded and participants completed pre- and
post-stimulation questionnaires assessing mood and sleepiness. As a result,
audible session led to increased arousal and positive mood compared to other
conditions. From the neurophysiological analysis, statistical differences in
frontal-central, central-central, and central-parietal connectivity were
observed only in the audible session. Furthermore, a significant correlation
was identified between sleepiness and EEG power in the temporal and occipital
regions. These results suggested a more detailed correlation for stimulation to
change its personal state. These findings have implications for applications in
areas such as cognitive enhancement, mood regulation, and sleep management.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07980" title="Abstract">arXiv:2311.07980</a> (cross-list from quant-ph) [<a href="/pdf/2311.07980" title="Download PDF">pdf</a>, <a href="/format/2311.07980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuantumEyes: Towards Better Interpretability of Quantum Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ruan%2C+S">Shaolun Ruan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Guan%2C+Q">Qiang Guan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Griffin%2C+P">Paul Griffin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mao%2C+Y">Ying Mao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+Y">Yong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Quantum computing offers significant speedup compared to classical computing,
which has led to a growing interest among users in learning and applying
quantum computing across various applications. However, quantum circuits, which
are fundamental for implementing quantum algorithms, can be challenging for
users to understand due to their underlying logic, such as the temporal
evolution of quantum states and the effect of quantum amplitudes on the
probability of basis quantum states. To fill this research gap, we propose
QuantumEyes, an interactive visual analytics system to enhance the
interpretability of quantum circuits through both global and local levels. For
the global-level analysis, we present three coupled visualizations to delineate
the changes of quantum states and the underlying reasons: a Probability Summary
View to overview the probability evolution of quantum states; a State Evolution
View to enable an in-depth analysis of the influence of quantum gates on the
quantum states; a Gate Explanation View to show the individual qubit states and
facilitate a better understanding of the effect of quantum gates. For the
local-level analysis, we design a novel geometrical visualization Dandelion
Chart to explicitly reveal how the quantum amplitudes affect the probability of
the quantum state. We thoroughly evaluated QuantumEyes as well as the novel
QuantumEyes integrated into it through two case studies on different types of
quantum algorithms and in-depth expert interviews with 12 domain experts. The
results demonstrate the effectiveness and usability of our approach in
enhancing the interpretability of quantum circuits.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07995" title="Abstract">arXiv:2311.07995</a> (cross-list from math.CO) [<a href="/pdf/2311.07995" title="Download PDF">pdf</a>, <a href="/ps/2311.07995" title="Download PostScript">ps</a>, <a href="/format/2311.07995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EPPA numbers of graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bradley-Williams%2C+D">David Bradley-Williams</a>, 
<a href="/search/math?searchtype=author&query=Cameron%2C+P+J">Peter J. Cameron</a>, 
<a href="/search/math?searchtype=author&query=Hubi%C4%8Dka%2C+J">Jan Hubi&#x10d;ka</a>, 
<a href="/search/math?searchtype=author&query=Kone%C4%8Dn%C3%BD%2C+M">Mat&#x11b;j Kone&#x10d;n&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">If $G$ is a graph, $A,B$ its induced subgraphs and $f\colon A\to B$ an
isomorphism, we say that $f$ is a partial automorphism of $G$. In 1992,
Hrushovski proved that graphs have the extension property for partial
automorphisms (EPPA, also called the Hrushovski property), that is, for every
finite graph $G$ there is a finite graph $H$, its EPPA-witness, such that $G$
is an induced subgraph of $H$ and every partial automorphism of $G$ extends to
an automorphism of $H$.
<br />The EPPA number of a graph $G$, denoted by
$\mathop{\mathrm{eppa}}\nolimits(G)$, is the smallest number of vertices of an
EPPA-witness for $G$, and we put $\mathop{\mathrm{eppa}}\nolimits(n) =
\max\{\mathop{\mathrm{eppa}}\nolimits(G) : \lvert G\rvert = n\}$. In this note
we review the state of the area, improve some lower bounds (in particular, we
show that $\mathop{\mathrm{eppa}}\nolimits(n)\geq \frac{2^n}{\sqrt{n}}$,
thereby identifying the correct base of the exponential) and pose several open
questions. We also briefly discuss EPPA numbers of hypergraphs and directed
graphs.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08016" title="Abstract">arXiv:2311.08016</a> (cross-list from eess.SP) [<a href="/pdf/2311.08016" title="Download PDF">pdf</a>, <a href="/format/2311.08016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Velocity-Based Channel Charting with Spatial Distribution Map Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Stahlke%2C+M">Maximilian Stahlke</a>, 
<a href="/search/eess?searchtype=author&query=Yammine%2C+G">George Yammine</a>, 
<a href="/search/eess?searchtype=author&query=Feigl%2C+T">Tobias Feigl</a>, 
<a href="/search/eess?searchtype=author&query=Eskofier%2C+B+M">Bjoern M. Eskofier</a>, 
<a href="/search/eess?searchtype=author&query=Mutschler%2C+C">Christopher Mutschler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Fingerprint-based localization improves the positioning performance in
challenging, non-line-of-sight (NLoS) dominated indoor environments. However,
fingerprinting models require an expensive life-cycle management including
recording and labeling of radio signals for the initial training and regularly
at environmental changes. Alternatively, channel-charting avoids this labeling
effort as it implicitly associates relative coordinates to the recorded radio
signals. Then, with reference real-world coordinates (positions) we can use
such charts for positioning tasks. However, current channel-charting approaches
lag behind fingerprinting in their positioning accuracy and still require
reference samples for localization, regular data recording and labeling to keep
the models up to date. Hence, we propose a novel framework that does not
require reference positions. We only require information from velocity
information, e.g., from pedestrian dead reckoning or odometry to model the
channel charts, and topological map information, e.g., a building floor plan,
to transform the channel charts into real coordinates. We evaluate our approach
on two different real-world datasets using 5G and distributed
single-input/multiple-output system (SIMO) radio systems. Our experiments show
that even with noisy velocity estimates and coarse map information, we achieve
similar position accuracies
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08024" title="Abstract">arXiv:2311.08024</a> (cross-list from eess.IV) [<a href="/pdf/2311.08024" title="Download PDF">pdf</a>, <a href="/format/2311.08024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MD-IQA: Learning Multi-scale Distributed Image Quality Assessment with  Semi Supervised Learning for Low Dose CT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Song%2C+T">Tao Song</a>, 
<a href="/search/eess?searchtype=author&query=Hou%2C+R">Ruizhi Hou</a>, 
<a href="/search/eess?searchtype=author&query=Dai%2C+L">Lisong Dai</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+L">Lei Xiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Image quality assessment (IQA) plays a critical role in optimizing radiation
dose and developing novel medical imaging techniques in computed tomography
(CT). Traditional IQA methods relying on hand-crafted features have limitations
in summarizing the subjective perceptual experience of image quality. Recent
deep learning-based approaches have demonstrated strong modeling capabilities
and potential for medical IQA, but challenges remain regarding model
generalization and perceptual accuracy. In this work, we propose a multi-scale
distributions regression approach to predict quality scores by constraining the
output distribution, thereby improving model generalization. Furthermore, we
design a dual-branch alignment network to enhance feature extraction
capabilities. Additionally, semi-supervised learning is introduced by utilizing
pseudo-labels for unlabeled data to guide model training. Extensive qualitative
experiments demonstrate the effectiveness of our proposed method for advancing
the state-of-the-art in deep learning-based medical IQA. Code is available at:
https://github.com/zunzhumu/MD-IQA.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08032" title="Abstract">arXiv:2311.08032</a> (cross-list from eess.IV) [<a href="/pdf/2311.08032" title="Download PDF">pdf</a>, <a href="/ps/2311.08032" title="Download PostScript">ps</a>, <a href="/format/2311.08032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ELF: An End-to-end Local and Global Multimodal Fusion Framework for  Glaucoma Grading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+W">Wenyun Li</a>, 
<a href="/search/eess?searchtype=author&query=Pun%2C+C">Chi-Man Pun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Glaucoma is a chronic neurodegenerative condition that can lead to blindness.
Early detection and curing are very important in stopping the disease from
getting worse for glaucoma patients. The 2D fundus images and optical coherence
tomography(OCT) are useful for ophthalmologists in diagnosing glaucoma. There
are many methods based on the fundus images or 3D OCT volumes; however, the
mining for multi-modality, including both fundus images and data, is less
studied. In this work, we propose an end-to-end local and global multi-modal
fusion framework for glaucoma grading, named ELF for short. ELF can fully
utilize the complementary information between fundus and OCT. In addition,
unlike previous methods that concatenate the multi-modal features together,
which lack exploring the mutual information between different modalities, ELF
can take advantage of local-wise and global-wise mutual information. The
extensive experiment conducted on the multi-modal glaucoma grading GAMMA
dataset can prove the effiectness of ELF when compared with other
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08059" title="Abstract">arXiv:2311.08059</a> (cross-list from eess.IV) [<a href="/pdf/2311.08059" title="Download PDF">pdf</a>, <a href="/format/2311.08059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FS-Net: Full Scale Network and Adaptive Threshold for Improving  Extraction of Micro-Retinal Vessel Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Getahun%2C+M+N">Melaku N. Getahun</a>, 
<a href="/search/eess?searchtype=author&query=Rogov%2C+O+Y">Oleg Y. Rogov</a>, 
<a href="/search/eess?searchtype=author&query=Dylov%2C+D+V">Dmitry V. Dylov</a>, 
<a href="/search/eess?searchtype=author&query=Somov%2C+A">Andrey Somov</a>, 
<a href="/search/eess?searchtype=author&query=Bouridane%2C+A">Ahmed Bouridane</a>, 
<a href="/search/eess?searchtype=author&query=Hamoudi%2C+R">Rifat Hamoudi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Retinal vascular segmentation, is a widely researched subject in biomedical
image processing, aims to relieve ophthalmologists' workload when treating and
detecting retinal disorders. However, segmenting retinal vessels has its own
set of challenges, with prior techniques failing to generate adequate results
when segmenting branches and microvascular structures. The neural network
approaches used recently are characterized by the inability to keep local and
global properties together and the failure to capture tiny end vessels make it
challenging to attain the desired result. To reduce this retinal vessel
segmentation problem, we propose a full-scale micro-vessel extraction mechanism
based on an encoder-decoder neural network architecture, sigmoid smoothing, and
an adaptive threshold method. The network consists of of residual, encoder
booster, bottleneck enhancement, squeeze, and excitation building blocks. All
of these blocks together help to improve the feature extraction and prediction
of the segmentation map. The proposed solution has been evaluated using the
DRIVE, CHASE-DB1, and STARE datasets, and competitive results are obtained when
compared with previous studies. The AUC and accuracy on the DRIVE dataset are
0.9884 and 0.9702, respectively. On the CHASE-DB1 dataset, the scores are
0.9903 and 0.9755, respectively. On the STARE dataset, the scores are 0.9916
and 0.9750, respectively. The performance achieved is one step ahead of what
has been done in previous studies, and this results in a higher chance of
having this solution in real-life diagnostic centers that seek ophthalmologists
attention.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08075" title="Abstract">arXiv:2311.08075</a> (cross-list from eess.IV) [<a href="/pdf/2311.08075" title="Download PDF">pdf</a>, <a href="/ps/2311.08075" title="Download PostScript">ps</a>, <a href="/format/2311.08075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GlanceSeg: Real-time microaneurysm lesion segmentation with  gaze-map-guided foundation model for early detection of diabetic retinopathy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+H">Hongyang Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+M">Mengdi Gao</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zirong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+C">Chen Tang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiaoqing Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+S">Shuai Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+W">Wu Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jiang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Early-stage diabetic retinopathy (DR) presents challenges in clinical
diagnosis due to inconspicuous and minute microangioma lesions, resulting in
limited research in this area. Additionally, the potential of emerging
foundation models, such as the segment anything model (SAM), in medical
scenarios remains rarely explored. In this work, we propose a
human-in-the-loop, label-free early DR diagnosis framework called GlanceSeg,
based on SAM. GlanceSeg enables real-time segmentation of microangioma lesions
as ophthalmologists review fundus images. Our human-in-the-loop framework
integrates the ophthalmologist's gaze map, allowing for rough localization of
minute lesions in fundus images. Subsequently, a saliency map is generated
based on the located region of interest, which provides prompt points to assist
the foundation model in efficiently segmenting microangioma lesions. Finally, a
domain knowledge filter refines the segmentation of minute lesions. We
conducted experiments on two newly-built public datasets, i.e., IDRiD and
Retinal-Lesions, and validated the feasibility and superiority of GlanceSeg
through visualized illustrations and quantitative measures. Additionally, we
demonstrated that GlanceSeg improves annotation efficiency for clinicians and
enhances segmentation performance through fine-tuning using annotations. This
study highlights the potential of GlanceSeg-based annotations for self-model
optimization, leading to enduring performance advancements through continual
learning.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08080" title="Abstract">arXiv:2311.08080</a> (cross-list from astro-ph.IM) [<a href="/pdf/2311.08080" title="Download PDF">pdf</a>, <a href="/format/2311.08080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Light-curve Signals with a Deep Learning Based Object  Detection Algorithm. II. A General Light Curve Classification Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Cui%2C+K">Kaiming Cui</a>, 
<a href="/search/astro-ph?searchtype=author&query=Armstrong%2C+D+J">D. J. Armstrong</a>, 
<a href="/search/astro-ph?searchtype=author&query=Feng%2C+F">Fabo Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 19 figures, 6 tables. Submitted to AAS Journal. Code is available on <a href="https://github.com/ckm3/Deep-LC">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; High Energy Astrophysical Phenomena (astro-ph.HE); Solar and Stellar Astrophysics (astro-ph.SR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Vast amounts of astronomical photometric data are generated from various
projects, requiring significant efforts to identify variable stars and other
object classes. In light of this, a general, widely applicable classification
framework would simplify the task of designing custom classifiers. We present a
novel deep learning framework for classifying light curves using a weakly
supervised object detection model. Our framework identifies the optimal windows
for both light curves and power spectra automatically, and zooms in on their
corresponding data. This allows for automatic feature extraction from both time
and frequency domains, enabling our model to handle data across different
scales and sampling intervals. We train our model on datasets obtained from
both space-based and ground-based multi-band observations of variable stars and
transients. We achieve an accuracy of 87% for combined variables and transient
events, which is comparable to the performance of previous feature-based
models. Our trained model can be utilized directly to other missions, such as
ASAS-SN, without requiring any retraining or fine-tuning. To address known
issues with miscalibrated predictive probabilities, we apply conformal
prediction to generate robust predictive sets that guarantee true label
coverage with a given probability. Additionally, we incorporate various anomaly
detection algorithms to empower our model with the ability to identify
out-of-distribution objects. Our framework is implemented in the Deep-LC
toolkit, which is an open-source Python package hosted on Github and PyPI.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08081" title="Abstract">arXiv:2311.08081</a> (cross-list from quant-ph) [<a href="/pdf/2311.08081" title="Download PDF">pdf</a>, <a href="/format/2311.08081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary-enhanced quantum supervised learning model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Albino%2C+A+S">Anton Simen Albino</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bloot%2C+R">Rodrigo Bloot</a>, 
<a href="/search/quant-ph?searchtype=author&query=Pires%2C+O+M">Otto M. Pires</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nascimento%2C+E+G+S">Erick G. S. Nascimento</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Quantum supervised learning, utilizing variational circuits, stands out as a
promising technology for NISQ devices due to its efficiency in hardware
resource utilization during the creation of quantum feature maps and the
implementation of hardware-efficient ansatz with trainable parameters. Despite
these advantages, the training of quantum models encounters challenges, notably
the barren plateau phenomenon, leading to stagnation in learning during
optimization iterations. This study proposes an innovative approach: an
evolutionary-enhanced ansatz-free supervised learning model. In contrast to
parametrized circuits, our model employs circuits with variable topology that
evolves through an elitist method, mitigating the barren plateau issue.
Additionally, we introduce a novel concept, the superposition of multi-hot
encodings, facilitating the treatment of multi-classification problems. Our
framework successfully avoids barren plateaus, resulting in enhanced model
accuracy. Comparative analysis with variational quantum classifiers from the
technology's state-of-the-art reveal a substantial improvement in training
efficiency and precision. Furthermore, we conduct tests on a challenging
dataset class, traditionally problematic for conventional kernel machines,
demonstrating a potential alternative path for achieving quantum advantage in
supervised learning for NISQ era.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08088" title="Abstract">arXiv:2311.08088</a> (cross-list from math.OC) [<a href="/pdf/2311.08088" title="Download PDF">pdf</a>, <a href="/format/2311.08088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interconnection of Discrete-Time Dissipative Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Martinelli%2C+A">Andrea Martinelli</a>, 
<a href="/search/math?searchtype=author&query=Aboudonia%2C+A">Ahmed Aboudonia</a>, 
<a href="/search/math?searchtype=author&query=Lygeros%2C+J">John Lygeros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Strictly proper discrete-time (DT) systems cannot be passive. In order for
passivity-based control to be exploited nevertheless, some authors introduce
virtual outputs, while some others rely on continuous-time passivity and then
apply discretization techniques that preserve passivity in DT. In this
manuscript, we argue that quadratic supply rates incorporate and extend the
effect of virtual outputs, allowing one to exploit dissipativity properties
directly in DT. We derive local dissipativity conditions for a set of nonlinear
systems interconnected with arbitrary topology, so that the overall network is
guaranteed to be stable. In case of linear systems, we develop feedback
dissipativity conditions that are linear in the supply rate. To demonstrate the
validity of our methods, we provide numerical examples in the context of
islanded microgrids.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08113" title="Abstract">arXiv:2311.08113</a> (cross-list from q-bio.QM) [<a href="/pdf/2311.08113" title="Download PDF">pdf</a>, <a href="/format/2311.08113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding learning from EEG data: Combining machine learning and  feature engineering based on hidden Markov models and mixed models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Palma%2C+G+R">Gabriel Rodrigues Palma</a>, 
<a href="/search/q-bio?searchtype=author&query=Thornberry%2C+C">Conor Thornberry</a>, 
<a href="/search/q-bio?searchtype=author&query=Commins%2C+S">Se&#xe1;n Commins</a>, 
<a href="/search/q-bio?searchtype=author&query=de+Andrade+Moral%2C+R">Rafael de Andrade Moral</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Theta oscillations, ranging from 4-8 Hz, play a significant role in spatial
learning and memory functions during navigation tasks. Frontal theta
oscillations are thought to play an important role in spatial navigation and
memory. Electroencephalography (EEG) datasets are very complex, making any
changes in the neural signal related to behaviour difficult to interpret.
However, multiple analytical methods are available to examine complex data
structure, especially machine learning based techniques. These methods have
shown high classification performance and the combination with feature
engineering enhances the capability of these methods. This paper proposes using
hidden Markov and linear mixed effects models to extract features from EEG
data. Based on the engineered features obtained from frontal theta EEG data
during a spatial navigation task in two key trials (first, last) and between
two conditions (learner and non-learner), we analysed the performance of six
machine learning methods (Polynomial Support Vector Machines, Non-linear
Support Vector Machines, Random Forests, K-Nearest Neighbours, Ridge, and Deep
Neural Networks) on classifying learner and non-learner participants. We also
analysed how different standardisation methods used to pre-process the EEG data
contribute to classification performance. We compared the classification
performance of each trial with data gathered from the same subjects, including
solely coordinate-based features, such as idle time and average speed. We found
that more machine learning methods perform better classification using
coordinate-based data. However, only deep neural networks achieved an area
under the ROC curve higher than 80% using the theta EEG data alone. Our
findings suggest that standardising the theta EEG data and using deep neural
networks enhances the classification of learner and non-learner subjects in a
spatial learning task.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08115" title="Abstract">arXiv:2311.08115</a> (cross-list from math.OC) [<a href="/pdf/2311.08115" title="Download PDF">pdf</a>, <a href="/format/2311.08115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Optimization of Large-Scale Parametrized Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Boef%2C+P+D">Pascal Den Boef</a>, 
<a href="/search/math?searchtype=author&query=Maubach%2C+J">Jos Maubach</a>, 
<a href="/search/math?searchtype=author&query=Schilders%2C+W">Wil Schilders</a>, 
<a href="/search/math?searchtype=author&query=van+de+Wouw%2C+N">Nathan van de Wouw</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Many relevant problems in the area of systems and control, such as controller
synthesis, observer design and model reduction, can be viewed as optimization
problems involving dynamical systems: for instance, maximizing performance in
the synthesis setting or minimizing error in the reduction setting. When the
involved dynamics are large-scale (e.g., high-dimensional semi-discretizations
of partial differential equations), the optimization becomes computationally
infeasible. Existing methods in literature lack computational scalability or
solve an approximation of the problem (thereby losing guarantees with respect
to the original problem). In this paper, we propose a novel method that
circumvents these issues. The method is an extension of Stochastic Gradient
Descent (SGD) which is widely used in the context of large-scale machine
learning problems. The proposed SGD scheme minimizes the $\mathcal{H}_2$ norm
of a (differentiable) parametrized dynamical system, and we prove that the
scheme is guaranteed to preserve stability with high probability under
boundedness conditions on the step size. Conditioned on the stability
preservation, we also obtain probabilistic convergence guarantees to local
minimizers. The method is also applicable to problems involving non-realizable
dynamics as it only requires frequency-domain input-output samples. We
demonstrate the potential of the approach on two numerical examples:
fixed-order observer design for a large-scale thermal model and controller
tuning for an infinite-dimensional system.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08146" title="Abstract">arXiv:2311.08146</a> (cross-list from eess.SP) [<a href="/pdf/2311.08146" title="Download PDF">pdf</a>, <a href="/ps/2311.08146" title="Download PostScript">ps</a>, <a href="/format/2311.08146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Source-Channel Coding for Channel-Adaptive Digital Semantic  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Park%2C+J">Joohyuk Park</a>, 
<a href="/search/eess?searchtype=author&query=Oh%2C+Y">Yongjeong Oh</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+S">Seonjung Kim</a>, 
<a href="/search/eess?searchtype=author&query=Jeon%2C+Y">Yo-Seb Jeon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">In this paper, we propose a novel joint source-channel coding (JSCC) approach
for channel-adaptive digital semantic communications. In semantic communication
systems with digital modulation and demodulation, end-to-end training and
robust design of JSCC encoder and decoder becomes challenging due to the
nonlinearity of modulation and demodulation processes, as well as diverse
channel conditions and modulation orders. To address this challenge, we first
develop a new demodulation method which assesses the uncertainty of the
demodulation output to improve the robustness of the digital semantic
communication system. We then devise a robust training strategy that
facilitates end-to-end training of the JSCC encoder and decoder, while
enhancing their robustness and flexibility. To this end, we model the
relationship between the encoder's output and decoder's input using binary
symmetric erasure channels and then sample the parameters of these channels
from diverse distributions. We also develop a channel-adaptive modulation
technique for an inference phase, in order to reduce the communication latency
while maintaining task performance. In this technique, we adaptively determine
modulation orders for the latent variables based on channel conditions. Using
simulations, we demonstrate the superior performance of the proposed JSCC
approach for both image classification and reconstruction tasks compared to
existing JSCC approaches.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08168" title="Abstract">arXiv:2311.08168</a> (cross-list from math.ST) [<a href="/pdf/2311.08168" title="Download PDF">pdf</a>, <a href="/format/2311.08168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Uniform Confidence Spheres for Means of Random Vectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chugg%2C+B">Ben Chugg</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+H">Hongjian Wang</a>, 
<a href="/search/math?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">We derive and study time-uniform confidence spheres - termed confidence
sphere sequences (CSSs) - which contain the mean of random vectors with high
probability simultaneously across all sample sizes. Inspired by the original
work of Catoni and Giulini, we unify and extend their analysis to cover both
the sequential setting and to handle a variety of distributional assumptions.
More concretely, our results include an empirical-Bernstein CSS for bounded
random vectors (resulting in a novel empirical-Bernstein confidence interval),
a CSS for sub-$\psi$ random vectors, and a CSS for heavy-tailed random vectors
based on a sequentially valid Catoni-Giulini estimator. Finally, we provide a
version of our empirical-Bernstein CSS that is robust to contamination by Huber
noise.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08179" title="Abstract">arXiv:2311.08179</a> (cross-list from eess.SP) [<a href="/pdf/2311.08179" title="Download PDF">pdf</a>, <a href="/format/2311.08179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Learning via Swapped Prediction for Communication Signal  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+W">Weidong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Liao%2C+H">Hongshu Liao</a>, 
<a href="/search/eess?searchtype=author&query=Gan%2C+L">Lu Gan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep neural networks have been widely used in communication signal
recognition and achieved remarkable performance, but this superiority typically
depends on using massive examples for supervised learning, whereas training a
deep neural network on small datasets with few labels generally falls into
overfitting, resulting in degenerated performance. To this end, we develop a
semi-supervised learning (SSL) method that effectively utilizes a large
collection of more readily available unlabeled signal data to improve
generalization. The proposed method relies largely on a novel implementation of
consistency-based regularization, termed Swapped Prediction, which leverages
strong data augmentation to perturb an unlabeled sample and then encourage its
corresponding model prediction to be close to its original, optimized with a
scaled cross-entropy loss with swapped symmetry. Extensive experiments indicate
that our proposed method can achieve a promising result for deep SSL of
communication signal recognition.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08190" title="Abstract">arXiv:2311.08190</a> (cross-list from eess.IV) [<a href="/pdf/2311.08190" title="Download PDF">pdf</a>, <a href="/format/2311.08190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAMIHS: Adaptation of Segment Anything Model for Intracranial Hemorrhage  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yinuo Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+W">Weimin Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Meng%2C+C">Cai Meng</a>, 
<a href="/search/eess?searchtype=author&query=Bai%2C+X">XiangZhi Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Segment Anything Model (SAM), a vision foundation model trained on
large-scale annotations, has recently continued raising awareness within
medical image segmentation. Despite the impressive capabilities of SAM on
natural scenes, it struggles with performance decline when confronted with
medical images, especially those involving blurry boundaries and highly
irregular regions of low contrast. In this paper, a SAM-based
parameter-efficient fine-tuning method, called SAMIHS, is proposed for
intracranial hemorrhage segmentation, which is a crucial and challenging step
in stroke diagnosis and surgical planning. Distinguished from previous SAM and
SAM-based methods, SAMIHS incorporates parameter-refactoring adapters into
SAM's image encoder and considers the efficient and flexible utilization of
adapters' parameters. Additionally, we employ a combo loss that combines binary
cross-entropy loss and boundary-sensitive loss to enhance SAMIHS's ability to
recognize the boundary regions. Our experimental results on two public datasets
demonstrate the effectiveness of our proposed method. Code is available at
https://github.com/mileswyn/SAMIHS .
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08199" title="Abstract">arXiv:2311.08199</a> (cross-list from eess.IV) [<a href="/pdf/2311.08199" title="Download PDF">pdf</a>, <a href="/format/2311.08199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion-based generation of Histopathological Whole Slide Images at a  Gigapixel scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Harb%2C+R">Robert Harb</a>, 
<a href="/search/eess?searchtype=author&query=Pock%2C+T">Thomas Pock</a>, 
<a href="/search/eess?searchtype=author&query=M%C3%BCller%2C+H">Heimo M&#xfc;ller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a novel diffusion-based approach to generate synthetic
histopathological Whole Slide Images (WSIs) at an unprecedented gigapixel
scale. Synthetic WSIs have many potential applications: They can augment
training datasets to enhance the performance of many computational pathology
applications. They allow the creation of synthesized copies of datasets that
can be shared without violating privacy regulations. Or they can facilitate
learning representations of WSIs without requiring data annotations. Despite
this variety of applications, no existing deep-learning-based method generates
WSIs at their typically high resolutions. Mainly due to the high computational
complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to
tackle image generation of high-resolution WSIs. In this scheme, we increase
the resolution of an initial low-resolution image to a high-resolution WSI.
Particularly, a diffusion model sequentially adds fine details to images and
increases their resolution. In our experiments, we train our method with WSIs
from the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also
performed a user study with pathologists. The study results suggest that our
generated WSIs resemble the structure of real WSIs.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08211" title="Abstract">arXiv:2311.08211</a> (cross-list from quant-ph) [<a href="/pdf/2311.08211" title="Download PDF">pdf</a>, <a href="/format/2311.08211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limitations within the Selected Cryptographic Scenarios and  Supra-Quantum Theories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Winczewski%2C+M">Marek Winczewski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD Thesis, University of Gda\'nsk, July 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The following submission constitutes a guide and an introduction to a
collection of articles submitted as a Ph.D. dissertation at the University of
Gda\'nsk. In the dissertation, we study the fundamental limitations within the
selected quantum and supra-quantum cryptographic scenarios in the form of upper
bounds on the achievable key rates. We investigate various security paradigms,
bipartite and multipartite settings, as well as single-shot and asymptotic
regimes. Our studies, however, extend beyond the derivations of the upper
bounds on the secret key rates in the mentioned scenarios. In particular, we
propose a novel type of rerouting attack on the quantum Internet for which we
find a countermeasure and benchmark its efficiency. Furthermore, we propose
several upper bounds on the performance of quantum (key) repeaters settings. We
derive a lower bound on the secret key agreement capacity of a quantum network,
which we tighten in an important case of a bidirectional quantum network. The
squashed nonlocality derived here as an upper bound on the secret key rate is a
novel non-faithful measure of nonlocality. Furthermore, the notion of the
non-signaling complete extension arising from the complete extension postulate
as a counterpart of purification of a quantum state allows us to study
analogies between non-signaling and quantum key distribution scenarios.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08225" title="Abstract">arXiv:2311.08225</a> (cross-list from eess.IV) [<a href="/pdf/2311.08225" title="Download PDF">pdf</a>, <a href="/format/2311.08225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uni-COAL: A Unified Framework for Cross-Modality Synthesis and  Super-Resolution of MR Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Song%2C+Z">Zhiyun Song</a>, 
<a href="/search/eess?searchtype=author&query=Qi%2C+Z">Zengxin Qi</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+Z">Zhenrong Shen</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Sheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Fei%2C+M">Manman Fei</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zang%2C+D">Di Zang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+D">Dongdong Chen</a>, 
<a href="/search/eess?searchtype=author&query=Yao%2C+L">Linlin Yao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Qian Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+X">Xuehai Wu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Lichi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Cross-modality synthesis (CMS), super-resolution (SR), and their combination
(CMSR) have been extensively studied for magnetic resonance imaging (MRI).
Their primary goals are to enhance the imaging quality by synthesizing the
desired modality and reducing the slice thickness. Despite the promising
synthetic results, these techniques are often tailored to specific tasks,
thereby limiting their adaptability to complex clinical scenarios. Therefore,
it is crucial to build a unified network that can handle various image
synthesis tasks with arbitrary requirements of modality and resolution
settings, so that the resources for training and deploying the models can be
greatly reduced. However, none of the previous works is capable of performing
CMS, SR, and CMSR using a unified network. Moreover, these MRI reconstruction
methods often treat alias frequencies improperly, resulting in suboptimal
detail restoration. In this paper, we propose a Unified Co-Modulated Alias-free
framework (Uni-COAL) to accomplish the aforementioned tasks with a single
network. The co-modulation design of the image-conditioned and stochastic
attribute representations ensures the consistency between CMS and SR, while
simultaneously accommodating arbitrary combinations of input/output modalities
and thickness. The generator of Uni-COAL is also designed to be alias-free
based on the Shannon-Nyquist signal processing framework, ensuring effective
suppression of alias frequencies. Additionally, we leverage the semantic prior
of Segment Anything Model (SAM) to guide Uni-COAL, ensuring a more authentic
preservation of anatomical structures during synthesis. Experiments on three
datasets demonstrate that Uni-COAL outperforms the alternatives in CMS, SR, and
CMSR tasks for MR images, which highlights its generalizability to wide-range
applications.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08239" title="Abstract">arXiv:2311.08239</a> (cross-list from eess.IV) [<a href="/pdf/2311.08239" title="Download PDF">pdf</a>, <a href="/format/2311.08239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Physics-Inspired Regularization for Medical Image Registration  with Hypernetworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Reithmeir%2C+A">Anna Reithmeir</a>, 
<a href="/search/eess?searchtype=author&query=Schnabel%2C+J+A">Julia A. Schnabel</a>, 
<a href="/search/eess?searchtype=author&query=Zimmer%2C+V+A">Veronika A. Zimmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Abstract accepted at SPIE Medical Imaging 2024. Manuscript will be published in Proceedings of the SPIE Digital Library
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Medical image registration aims at identifying the spatial deformation
between images of the same anatomical region and is fundamental to image-based
diagnostics and therapy. To date, the majority of the deep learning-based
registration methods employ regularizers that enforce global spatial
smoothness, e.g., the diffusion regularizer. However, such regularizers are not
tailored to the data and might not be capable of reflecting the complex
underlying deformation. In contrast, physics-inspired regularizers promote
physically plausible deformations. One such regularizer is the linear elastic
regularizer which models the deformation of elastic material. These
regularizers are driven by parameters that define the material's physical
properties. For biological tissue, a wide range of estimations of such
parameters can be found in the literature and it remains an open challenge to
identify suitable parameter values for successful registration. To overcome
this problem and to incorporate physical properties into learning-based
registration, we propose to use a hypernetwork that learns the effect of the
physical parameters of a physics-inspired regularizer on the resulting spatial
deformation field. In particular, we adapt the HyperMorph framework to learn
the effect of the two elasticity parameters of the linear elastic regularizer.
Our approach enables the efficient discovery of suitable, data-specific
physical parameters at test time.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08269" title="Abstract">arXiv:2311.08269</a> (cross-list from q-bio.QM) [<a href="/pdf/2311.08269" title="Download PDF">pdf</a>, <a href="/format/2311.08269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defining the boundaries: challenges and advances in identifying cells in  microscopy images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Gogoberidze%2C+N">Nodar Gogoberidze</a>, 
<a href="/search/q-bio?searchtype=author&query=Cimini%2C+B+A">Beth A. Cimini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure, submitted to "Current Opinion in Biotechnology"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Segmentation, or the outlining of objects within images, is a critical step
in the measurement and analysis of cells within microscopy images. While
improvements continue to be made in tools that rely on classical methods for
segmentation, deep learning-based tools increasingly dominate advances in the
technology. Specialist models such as Cellpose continue to improve in accuracy
and user-friendliness, and segmentation challenges such as the Multi-Modality
Cell Segmentation Challenge continue to push innovation in accuracy across
widely-varying test data as well as efficiency and usability. Increased
attention on documentation, sharing, and evaluation standards are leading to
increased user-friendliness and acceleration towards the goal of a truly
universal method.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08321" title="Abstract">arXiv:2311.08321</a> (cross-list from math.OC) [<a href="/pdf/2311.08321" title="Download PDF">pdf</a>, <a href="/format/2311.08321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Peak Estimation of Rational Systems using Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Miller%2C+J">Jared Miller</a>, 
<a href="/search/math?searchtype=author&query=Smith%2C+R+S">Roy S. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper presents algorithms that upper-bound the peak value of a state
function along trajectories of a continuous-time system with rational dynamics.
The finite-dimensional but nonconvex peak estimation problem is cast as a
convex infinite-dimensional linear program in occupation measures. This
infinite-dimensional program is then truncated into finite-dimensions using the
moment-Sum-of-Squares (SOS) hierarchy of semidefinite programs. Prior work on
treating rational dynamics using the moment-SOS approach involves clearing
dynamics to common denominators or by adding lifting variables to handle
reciprocal terms under new equality constraints. Our solution method uses a
sum-of-rational method based on absolute continuity of measures. The Moment-SOS
truncations of our program possess lower computational complexity and
(empirically demonstrated) higher accuracy of upper bounds on example systems
as compared to prior approaches.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08330" title="Abstract">arXiv:2311.08330</a> (cross-list from eess.AS) [<a href="/pdf/2311.08330" title="Download PDF">pdf</a>, <a href="/format/2311.08330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative De-Quantization for Neural Speech Codec via Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+H">Haici Yang</a>, 
<a href="/search/eess?searchtype=author&query=Jang%2C+I">Inseon Jang</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+M">Minje Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">In low-bitrate speech coding, end-to-end speech coding networks aim to learn
compact yet expressive features and a powerful decoder in a single network. A
challenging problem as such results in unwelcome complexity increase and
inferior speech quality. In this paper, we propose to separate the
representation learning and information reconstruction tasks. We leverage an
end-to-end codec for learning low-dimensional discrete tokens and employ a
latent diffusion model to de-quantize coded features into a high-dimensional
continuous space, relieving the decoder's burden of de-quantizing and
upsampling. To mitigate the issue of over-smooth generation, we introduce
midway-infilling with less noise reduction and stronger conditioning. In
ablation studies, we investigate the hyperparameters for midway-infilling and
latent diffusion space with different dimensions. Subjective listening tests
show that our model outperforms the state-of-the-art at two low bitrates, 1.5
and 3 kbps. Codes and samples of this work are available on our webpage.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08352" title="Abstract">arXiv:2311.08352</a> (cross-list from math.FA) [<a href="/pdf/2311.08352" title="Download PDF">pdf</a>, <a href="/ps/2311.08352" title="Download PostScript">ps</a>, <a href="/format/2311.08352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling theorems with derivatives in shift-invariant spaces generated  by periodic exponential B-splines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gr%C3%B6chenig%2C+K">Karlheinz Gr&#xf6;chenig</a>, 
<a href="/search/math?searchtype=author&query=Shafkulovska%2C+I">Irina Shafkulovska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Information Theory (cs.IT); Classical Analysis and ODEs (math.CA)

</div>
<p class="mathjax">We derive sufficient conditions for sampling with derivatives in
shift-invariant spaces generated by a periodic exponential B-spline. The
sufficient conditions are expressed with a new notion of measuring the gap
between consecutive samples. These conditions are near optimal, and, in
particular, they imply the existence of sampling sets with lower Beurling
density arbitrarily close to the necessary density.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08371" title="Abstract">arXiv:2311.08371</a> (cross-list from eess.IV) [<a href="/pdf/2311.08371" title="Download PDF">pdf</a>, <a href="/format/2311.08371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> USLR: an open-source tool for unbiased and smooth longitudinal  registration of brain MR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Casamitjana%2C+A">Adri&#xe0; Casamitjana</a>, 
<a href="/search/eess?searchtype=author&query=Sala-Llonch%2C+R">Roser Sala-Llonch</a>, 
<a href="/search/eess?searchtype=author&query=Lekadir%2C+K">Karim Lekadir</a>, 
<a href="/search/eess?searchtype=author&query=Iglesias%2C+J+E">Juan Eugenio Iglesias</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Medical Image Analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We present USLR, a computational framework for longitudinal registration of
brain MRI scans to estimate nonlinear image trajectories that are smooth across
time, unbiased to any timepoint, and robust to imaging artefacts. It operates
on the Lie algebra parameterisation of spatial transforms (which is compatible
with rigid transforms and stationary velocity fields for nonlinear deformation)
and takes advantage of log-domain properties to solve the problem using
Bayesian inference. USRL estimates rigid and nonlinear registrations that: (i)
bring all timepoints to an unbiased subject-specific space; and (i) compute a
smooth trajectory across the imaging time-series. We capitalise on
learning-based registration algorithms and closed-form expressions for fast
inference. A use-case Alzheimer's disease study is used to showcase the
benefits of the pipeline in multiple fronts, such as time-consistent image
segmentation to reduce intra-subject variability, subject-specific prediction
or population analysis using tensor-based morphometry. We demonstrate that such
approach improves upon cross-sectional methods in identifying group
differences, which can be helpful in detecting more subtle atrophy levels or in
reducing sample sizes in clinical trials. The code is publicly available in
https://github.com/acasamitjana/uslr
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08376" title="Abstract">arXiv:2311.08376</a> (cross-list from stat.ML) [<a href="/pdf/2311.08376" title="Download PDF">pdf</a>, <a href="/ps/2311.08376" title="Download PostScript">ps</a>, <a href="/format/2311.08376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble sampling for linear bandits: small ensembles suffice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Janz%2C+D">David Janz</a>, 
<a href="/search/stat?searchtype=author&query=Litvak%2C+A+E">Alexander E. Litvak</a>, 
<a href="/search/stat?searchtype=author&query=Szepesv%C3%A1ri%2C+C">Csaba Szepesv&#xe1;ri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We provide the first useful, rigorous analysis of ensemble sampling for the
stochastic linear bandit setting. In particular, we show that, under standard
assumptions, for a $d$-dimensional stochastic linear bandit with an interaction
horizon $T$, ensemble sampling with an ensemble of size $m$ on the order of $d
\log T$ incurs regret bounded by order $(d \log T)^{5/2} \sqrt{T}$. Ours is the
first result in any structured setting not to require the size of the ensemble
to scale linearly with $T$ -- which defeats the purpose of ensemble sampling --
while obtaining near $\sqrt{T}$ order regret. Ours is also the first result
that allows infinite action sets.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08396" title="Abstract">arXiv:2311.08396</a> (cross-list from eess.AS) [<a href="/pdf/2311.08396" title="Download PDF">pdf</a>, <a href="/format/2311.08396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot audio captioning with audio-language model guidance and audio  context keywords
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Salewski%2C+L">Leonard Salewski</a>, 
<a href="/search/eess?searchtype=author&query=Fauth%2C+S">Stefan Fauth</a>, 
<a href="/search/eess?searchtype=author&query=Koepke%2C+A+S">A. Sophia Koepke</a>, 
<a href="/search/eess?searchtype=author&query=Akata%2C+Z">Zeynep Akata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 - Machine Learning for Audio Workshop (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">Zero-shot audio captioning aims at automatically generating descriptive
textual captions for audio content without prior training for this task.
Different from speech recognition which translates audio content that contains
spoken language into text, audio captioning is commonly concerned with ambient
sounds, or sounds produced by a human performing an action. Inspired by
zero-shot image captioning methods, we propose ZerAuCap, a novel framework for
summarising such general audio signals in a text caption without requiring
task-specific training. In particular, our framework exploits a pre-trained
large language model (LLM) for generating the text which is guided by a
pre-trained audio-language model to produce captions that describe the audio
content. Additionally, we use audio context keywords that prompt the language
model to generate text that is broadly relevant to sounds. Our proposed
framework achieves state-of-the-art results in zero-shot audio captioning on
the AudioCaps and Clotho datasets. Our code is available at
https://github.com/ExplainableML/ZerAuCap.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Wed, 15 Nov 23</h3>
<dl>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1907.04585" title="Abstract">arXiv:1907.04585</a> (replaced) [<a href="/pdf/1907.04585" title="Download PDF">pdf</a>, <a href="/format/1907.04585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-polynomial time approximation schemes for the Maximum Weight  Independent Set Problem in H-free graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chudnovsky%2C+M">Maria Chudnovsky</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Marcin Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Micha&#x142; Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Thomass%C3%A9%2C+S">St&#xe9;phan Thomass&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: added results on subexponential algorithms, v3: revision after reviewers' remarks, v4: final version accepted at SICOMP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.00738" title="Abstract">arXiv:2012.00738</a> (replaced) [<a href="/pdf/2012.00738" title="Download PDF">pdf</a>, <a href="/format/2012.00738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Searching, Sorting, and Cake Cutting in Rounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Br%C3%A2nzei%2C+S">Simina Br&#xe2;nzei</a>, 
<a href="/search/cs?searchtype=author&query=Paparas%2C+D">Dimitris Paparas</a>, 
<a href="/search/cs?searchtype=author&query=Recker%2C+N">Nicholas Recker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.14350" title="Abstract">arXiv:2103.14350</a> (replaced) [<a href="/pdf/2103.14350" title="Download PDF">pdf</a>, <a href="/ps/2103.14350" title="Download PostScript">ps</a>, <a href="/format/2103.14350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The convergence of the Stochastic Gradient Descent (SGD) : a  self-contained proof
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Turinici%2C+G">Gabrel Turinici</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.14466" title="Abstract">arXiv:2103.14466</a> (replaced) [<a href="/pdf/2103.14466" title="Download PDF">pdf</a>, <a href="/format/2103.14466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prioritise the Best Variation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kokke%2C+W">Wen Kokke</a>, 
<a href="/search/cs?searchtype=author&query=Dardha%2C+O">Ornela Dardha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.05306" title="Abstract">arXiv:2104.05306</a> (replaced) [<a href="/e-print/2104.05306" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WLFC: Write Less in Flash-based Cache
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chaos Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianshun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Need revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.13881" title="Abstract">arXiv:2104.13881</a> (replaced) [<a href="/pdf/2104.13881" title="Download PDF">pdf</a>, <a href="/ps/2104.13881" title="Download PostScript">ps</a>, <a href="/format/2104.13881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Scale Prediction with Decision Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Klusowski%2C+J+M">Jason M. Klusowski</a>, 
<a href="/search/stat?searchtype=author&query=Tian%2C+P+M">Peter M. Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.05544" title="Abstract">arXiv:2106.05544</a> (replaced) [<a href="/pdf/2106.05544" title="Download PDF">pdf</a>, <a href="/format/2106.05544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CogAlign: Learning to Align Textual Neural Representations to Cognitive  Language Processing Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuqi Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Deyi Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06854" title="Abstract">arXiv:2106.06854</a> (replaced) [<a href="/pdf/2106.06854" title="Download PDF">pdf</a>, <a href="/format/2106.06854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Reinforcement Learning Approach to Marginalized Importance  Sampling with the Successor Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+S">Scott Fujimoto</a>, 
<a href="/search/cs?searchtype=author&query=Meger%2C+D">David Meger</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.07863" title="Abstract">arXiv:2109.07863</a> (replaced) [<a href="/pdf/2109.07863" title="Download PDF">pdf</a>, <a href="/ps/2109.07863" title="Download PostScript">ps</a>, <a href="/format/2109.07863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trillium: Higher-Order Concurrent and Distributed Separation Logic for  Intensional Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Timany%2C+A">Amin Timany</a>, 
<a href="/search/cs?searchtype=author&query=Gregersen%2C+S+O">Simon Oddershede Gregersen</a>, 
<a href="/search/cs?searchtype=author&query=Stefanesco%2C+L">L&#xe9;o Stefanesco</a>, 
<a href="/search/cs?searchtype=author&query=Hinrichsen%2C+J+K">Jonas Kastberg Hinrichsen</a>, 
<a href="/search/cs?searchtype=author&query=Gondelman%2C+L">L&#xe9;on Gondelman</a>, 
<a href="/search/cs?searchtype=author&query=Nieto%2C+A">Abel Nieto</a>, 
<a href="/search/cs?searchtype=author&query=Birkedal%2C+L">Lars Birkedal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> POPL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.12945" title="Abstract">arXiv:2111.12945</a> (replaced) [<a href="/pdf/2111.12945" title="Download PDF">pdf</a>, <a href="/format/2111.12945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-rank variational Bayes correction to the Laplace method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=van+Niekerk%2C+J">Janet van Niekerk</a>, 
<a href="/search/stat?searchtype=author&query=Rue%2C+H">Haavard Rue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.10859" title="Abstract">arXiv:2201.10859</a> (replaced) [<a href="/pdf/2201.10859" title="Download PDF">pdf</a>, <a href="/format/2201.10859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visualizing the Diversity of Representations Learned by Bayesian Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grinwald%2C+D">Dennis Grinwald</a>, 
<a href="/search/cs?searchtype=author&query=Bykov%2C+K">Kirill Bykov</a>, 
<a href="/search/cs?searchtype=author&query=Nakajima%2C+S">Shinichi Nakajima</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6hne%2C+M+M+-">Marina M.-C. H&#xf6;hne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 18 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published in Transactions on Machine Learning Research (11/2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.11239" title="Abstract">arXiv:2201.11239</a> (replaced) [<a href="/pdf/2201.11239" title="Download PDF">pdf</a>, <a href="/format/2201.11239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnosing AI Explanation Methods with Folk Concepts of Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacovi%2C+A">Alon Jacovi</a>, 
<a href="/search/cs?searchtype=author&query=Bastings%2C+J">Jasmijn Bastings</a>, 
<a href="/search/cs?searchtype=author&query=Gehrmann%2C+S">Sebastian Gehrmann</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Filippova%2C+K">Katja Filippova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Journal of Artificial Intelligence Research (JAIR Vol. 78, 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.13961" title="Abstract">arXiv:2202.13961</a> (replaced) [<a href="/pdf/2202.13961" title="Download PDF">pdf</a>, <a href="/format/2202.13961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatio-Causal Patterns of Sample Growth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ribeiro%2C+A+F">Andre F. Ribeiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.03170" title="Abstract">arXiv:2204.03170</a> (replaced) [<a href="/pdf/2204.03170" title="Download PDF">pdf</a>, <a href="/ps/2204.03170" title="Download PostScript">ps</a>, <a href="/format/2204.03170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decay Rate of $\exp(A^{-1}t)A^{-1}$ on a Hilbert Space and the  Crank-Nicolson Scheme with Smooth Initial Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wakaiki%2C+M">Masashi Wakaiki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages. To appear in Integral Equations and Operator Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.13069" title="Abstract">arXiv:2204.13069</a> (replaced) [<a href="/pdf/2204.13069" title="Download PDF">pdf</a>, <a href="/format/2204.13069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On subspace designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santonastaso%2C+P">Paolo Santonastaso</a>, 
<a href="/search/cs?searchtype=author&query=Zullo%2C+F">Ferdinando Zullo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMS Surveys in Mathematical Sciences, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.03359" title="Abstract">arXiv:2206.03359</a> (replaced) [<a href="/pdf/2206.03359" title="Download PDF">pdf</a>, <a href="/format/2206.03359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient semi-supervised quality control system trained using  physics-based MRI-artefact generators and adversarial training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ravi%2C+D">Daniele Ravi</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative), 
<a href="/search/eess?searchtype=author&query=Barkhof%2C+F">Frederik Barkhof</a>, 
<a href="/search/eess?searchtype=author&query=Alexander%2C+D+C">Daniel C. Alexander</a>, 
<a href="/search/eess?searchtype=author&query=Puglisi%2C+L">Lemuel Puglisi</a>, 
<a href="/search/eess?searchtype=author&query=Parker%2C+G+J">Geoffrey JM Parker</a>, 
<a href="/search/eess?searchtype=author&query=Eshaghi%2C+A">Arman Eshaghi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Medical Image Analysis 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.14191" title="Abstract">arXiv:2207.14191</a> (replaced) [<a href="/pdf/2207.14191" title="Download PDF">pdf</a>, <a href="/format/2207.14191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning with Limited Annotations: A Survey on Deep Semi-Supervised  Learning for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+R">Rushi Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Le Ding</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Rong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jicong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.01473" title="Abstract">arXiv:2209.01473</a> (replaced) [<a href="/pdf/2209.01473" title="Download PDF">pdf</a>, <a href="/format/2209.01473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-based Analysis and Specification of Functional Requirements and  Tests for Complex Automotive Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wiecher%2C+C">Carsten Wiecher</a>, 
<a href="/search/cs?searchtype=author&query=Mandel%2C+C">Constantin Mandel</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnther%2C+M">Matthias G&#xfc;nther</a>, 
<a href="/search/cs?searchtype=author&query=Fischbach%2C+J">Jannik Fischbach</a>, 
<a href="/search/cs?searchtype=author&query=Greenyer%2C+J">Joel Greenyer</a>, 
<a href="/search/cs?searchtype=author&query=Greinert%2C+M">Matthias Greinert</a>, 
<a href="/search/cs?searchtype=author&query=Wolff%2C+C">Carsten Wolff</a>, 
<a href="/search/cs?searchtype=author&query=Dumitrescu%2C+R">Roman Dumitrescu</a>, 
<a href="/search/cs?searchtype=author&query=Mendez%2C+D">Daniel Mendez</a>, 
<a href="/search/cs?searchtype=author&query=Albers%2C+A">Albert Albers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.08355" title="Abstract">arXiv:2209.08355</a> (replaced) [<a href="/pdf/2209.08355" title="Download PDF">pdf</a>, <a href="/format/2209.08355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Topology-Preserved Distance Transform for Pulmonary  Airway Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minghui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guang-Zhong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yun Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.08411" title="Abstract">arXiv:2209.08411</a> (replaced) [<a href="/pdf/2209.08411" title="Download PDF">pdf</a>, <a href="/format/2209.08411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynaConF: Dynamic Forecasting of Non-Stationary Time-Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lehrmann%2C+A">Andreas Lehrmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03461" title="Abstract">arXiv:2210.03461</a> (replaced) [<a href="/pdf/2210.03461" title="Download PDF">pdf</a>, <a href="/format/2210.03461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using  Style Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suresh%2C+A+P">Ananda Padhmanabhan Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Sanjana Jain</a>, 
<a href="/search/cs?searchtype=author&query=Noinongyao%2C+P">Pavit Noinongyao</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+A">Ankush Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Watchareeruetai%2C+U">Ukrit Watchareeruetai</a>, 
<a href="/search/cs?searchtype=author&query=Samacoits%2C+A">Aubin Samacoits</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03475" title="Abstract">arXiv:2210.03475</a> (replaced) [<a href="/pdf/2210.03475" title="Download PDF">pdf</a>, <a href="/format/2210.03475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Winner Takes It All: Training Performant RL Populations for  Combinatorial Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grinsztajn%2C+N">Nathan Grinsztajn</a>, 
<a href="/search/cs?searchtype=author&query=Furelos-Blanco%2C+D">Daniel Furelos-Blanco</a>, 
<a href="/search/cs?searchtype=author&query=Surana%2C+S">Shikha Surana</a>, 
<a href="/search/cs?searchtype=author&query=Bonnet%2C+C">Cl&#xe9;ment Bonnet</a>, 
<a href="/search/cs?searchtype=author&query=Barrett%2C+T+D">Thomas D. Barrett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.06107" title="Abstract">arXiv:2210.06107</a> (replaced) [<a href="/pdf/2210.06107" title="Download PDF">pdf</a>, <a href="/format/2210.06107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vulnerabilities of Single-Round Incentive Compatibility in Auto-bidding:  Theory and Evidence from ROI-Constrained Online Advertising Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juncheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+P">Pingzhong Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07410" title="Abstract">arXiv:2210.07410</a> (replaced) [<a href="/pdf/2210.07410" title="Download PDF">pdf</a>, <a href="/format/2210.07410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification of quantum entanglement with Siamese convolutional neural  networks and semi-supervised learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Paw%C5%82owski%2C+J">Jaros&#x142;aw Paw&#x142;owski</a>, 
<a href="/search/quant-ph?searchtype=author&query=Krawczyk%2C+M">Mateusz Krawczyk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version with improved models; 11 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09630" title="Abstract">arXiv:2210.09630</a> (replaced) [<a href="/pdf/2210.09630" title="Download PDF">pdf</a>, <a href="/ps/2210.09630" title="Download PostScript">ps</a>, <a href="/format/2210.09630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Completeness of Tableau Calculi for Two-Dimensional Hybrid Logics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nishimura%2C+Y">Yuki Nishimura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Version 2. 27 pages. 5 figures. This is a preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13112" title="Abstract">arXiv:2210.13112</a> (replaced) [<a href="/pdf/2210.13112" title="Download PDF">pdf</a>, <a href="/format/2210.13112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization-based Motion Planning for Autonomous Parking Considering  Dynamic Obstacle: A Hierarchical Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+X">Xuemin Chi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhitao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jihao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+F">Feng Hong</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hongye Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update some typos and references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.02914" title="Abstract">arXiv:2211.02914</a> (replaced) [<a href="/pdf/2211.02914" title="Download PDF">pdf</a>, <a href="/format/2211.02914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Reflection Removal with Flash-only Cues in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+C">Chenyang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xudong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extension of CVPR 2021 paper [<a href="/abs/2103.04273">arXiv:2103.04273</a>], submitted to TPAMI. Our source code and dataset are publicly available at <a href="http://github.com/ChenyangLEI/flash-reflection-removal">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06741" title="Abstract">arXiv:2211.06741</a> (replaced) [<a href="/pdf/2211.06741" title="Download PDF">pdf</a>, <a href="/ps/2211.06741" title="Download PostScript">ps</a>, <a href="/format/2211.06741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibrating Control-Bounded ADCs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Malmberg%2C+H">Hampus Malmberg</a>, 
<a href="/search/eess?searchtype=author&query=Mettler%2C+T">Till Mettler</a>, 
<a href="/search/eess?searchtype=author&query=Burger%2C+T">Thomas Burger</a>, 
<a href="/search/eess?searchtype=author&query=Feyling%2C+F">Fredrik Feyling</a>, 
<a href="/search/eess?searchtype=author&query=Loeliger%2C+H">Hans-Andrea Loeliger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, submitted to ISCAS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08253" title="Abstract">arXiv:2211.08253</a> (replaced) [<a href="/pdf/2211.08253" title="Download PDF">pdf</a>, <a href="/format/2211.08253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Jingang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Faney%2C+T">Thibault Faney</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gallinari%2C+P">Patrick Gallinari</a>, 
<a href="/search/cs?searchtype=author&query=Yousef%2C+S">Soleiman Yousef</a>, 
<a href="/search/cs?searchtype=author&query=de+Hemptinne%2C+J">Jean-Charles de Hemptinne</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10627" title="Abstract">arXiv:2211.10627</a> (replaced) [<a href="/pdf/2211.10627" title="Download PDF">pdf</a>, <a href="/format/2211.10627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EGRC-Net: Embedding-induced Graph Refinement Clustering Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Zhihao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yuheng Jia</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Junhui Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by IEEE Transactions on Image Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11923" title="Abstract">arXiv:2211.11923</a> (replaced) [<a href="/pdf/2211.11923" title="Download PDF">pdf</a>, <a href="/format/2211.11923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Optimal Coreset Construction for Euclidean $(k,z)$-Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lingxiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13854" title="Abstract">arXiv:2211.13854</a> (replaced) [<a href="/pdf/2211.13854" title="Download PDF">pdf</a>, <a href="/format/2211.13854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ComCLIP: Training-Free Compositional Image and Text Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kenan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuehai He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruize Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+E">Xin Eric Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00768" title="Abstract">arXiv:2212.00768</a> (replaced) [<a href="/pdf/2212.00768" title="Download PDF">pdf</a>, <a href="/format/2212.00768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplifying and Understanding State Space Models with Diagonal Linear  RNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Ankit Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+H">Harsh Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Berant%2C+J">Jonathan Berant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> added Long Range Arena, language modeling with mixture of experts
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02712" title="Abstract">arXiv:2212.02712</a> (replaced) [<a href="/pdf/2212.02712" title="Download PDF">pdf</a>, <a href="/format/2212.02712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Beam Search for Hallucination Mitigation in Abstractive  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+A+K">Arvind Krishna Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Visser%2C+E">Erik Visser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04972" title="Abstract">arXiv:2212.04972</a> (replaced) [<a href="/pdf/2212.04972" title="Download PDF">pdf</a>, <a href="/format/2212.04972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOPRD: A multidisciplinary open peer review dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jialiang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiaxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhangping Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yidong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaodong Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please cite the version of Neural Computing and Applications
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neural Computing and Applications, Vol. 35, Issue 34, pp.
  24191-24206 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05949" title="Abstract">arXiv:2212.05949</a> (replaced) [<a href="/pdf/2212.05949" title="Download PDF">pdf</a>, <a href="/ps/2212.05949" title="Download PostScript">ps</a>, <a href="/format/2212.05949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear  Contextual Bandits and Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ye%2C+C">Chenlu Ye</a>, 
<a href="/search/stat?searchtype=author&query=Xiong%2C+W">Wei Xiong</a>, 
<a href="/search/stat?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We study the corruption-robust MDPs and contextual bandits with general function approximation
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06259" title="Abstract">arXiv:2212.06259</a> (replaced) [<a href="/pdf/2212.06259" title="Download PDF">pdf</a>, <a href="/format/2212.06259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tydi-lang: A Language for Typed Streaming Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yongding Tian</a>, 
<a href="/search/cs?searchtype=author&query=Reukers%2C+M+A">Matthijs A. Reukers</a>, 
<a href="/search/cs?searchtype=author&query=Al-Ars%2C+Z">Zaid Al-Ars</a>, 
<a href="/search/cs?searchtype=author&query=Hofstee%2C+P">Peter Hofstee</a>, 
<a href="/search/cs?searchtype=author&query=Brobbel%2C+M">Matthijs Brobbel</a>, 
<a href="/search/cs?searchtype=author&query=Peltenburg%2C+J">Johan Peltenburg</a>, 
<a href="/search/cs?searchtype=author&query=van+Straten%2C+J">Jeroen van Straten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages and 1 page of reference, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.12989" title="Abstract">arXiv:2212.12989</a> (replaced) [<a href="/pdf/2212.12989" title="Download PDF">pdf</a>, <a href="/ps/2212.12989" title="Download PostScript">ps</a>, <a href="/format/2212.12989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Kernel Alignment Regret Bound for Online Kernel Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junfan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+S">Shizhong Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00894" title="Abstract">arXiv:2301.00894</a> (replaced) [<a href="/pdf/2301.00894" title="Download PDF">pdf</a>, <a href="/format/2301.00894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nominal Recursors as Epi-Recursors: Extended Technical Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Popescu%2C+A">Andrei Popescu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05525" title="Abstract">arXiv:2301.05525</a> (replaced) [<a href="/pdf/2301.05525" title="Download PDF">pdf</a>, <a href="/format/2301.05525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Concept Identification as Consistent Data Clustering  Across Multiple Feature Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanfermann%2C+F">Felix Lanfermann</a>, 
<a href="/search/cs?searchtype=author&query=Schmitt%2C+S">Sebastian Schmitt</a>, 
<a href="/search/cs?searchtype=author&query=Wollstadt%2C+P">Patricia Wollstadt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, published in proceedings of 2022 IEEE International Conference on Data Mining Workshops (ICDMW)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE International Conference on Data Mining Workshops
  (ICDMW)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08460" title="Abstract">arXiv:2301.08460</a> (replaced) [<a href="/pdf/2301.08460" title="Download PDF">pdf</a>, <a href="/ps/2301.08460" title="Download PostScript">ps</a>, <a href="/format/2301.08460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coresets for Constrained Clustering: General Assignment Constraints and  Improved Size Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lingxiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pinyan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a merger with <a href="/abs/2302.11151">arXiv:2302.11151</a>. The abstract is shortened due to the length limit of arXiv
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09702" title="Abstract">arXiv:2301.09702</a> (replaced) [<a href="/pdf/2301.09702" title="Download PDF">pdf</a>, <a href="/format/2301.09702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Illumination Variation Correction Using Image Synthesis For Unsupervised  Domain Adaptive Person Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guo%2C+J">Jiaqi Guo</a>, 
<a href="/search/eess?searchtype=author&query=Reibman%2C+A+R">Amy R. Reibman</a>, 
<a href="/search/eess?searchtype=author&query=Delp%2C+E+J">Edward J. Delp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10061" title="Abstract">arXiv:2301.10061</a> (replaced) [<a href="/pdf/2301.10061" title="Download PDF">pdf</a>, <a href="/ps/2301.10061" title="Download PostScript">ps</a>, <a href="/format/2301.10061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous Probabilistic Couplings in Higher-Order Separation Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gregersen%2C+S+O">Simon Oddershede Gregersen</a>, 
<a href="/search/cs?searchtype=author&query=Aguirre%2C+A">Alejandro Aguirre</a>, 
<a href="/search/cs?searchtype=author&query=Haselwarter%2C+P+G">Philipp G. Haselwarter</a>, 
<a href="/search/cs?searchtype=author&query=Tassarotti%2C+J">Joseph Tassarotti</a>, 
<a href="/search/cs?searchtype=author&query=Birkedal%2C+L">Lars Birkedal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10880" title="Abstract">arXiv:2301.10880</a> (replaced) [<a href="/pdf/2301.10880" title="Download PDF">pdf</a>, <a href="/format/2301.10880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Golden Age: Conspiracy Theories&#x27; Relationship with Misinformation  Outlets, News Media, and the Wider Internet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanley%2C+H+W+A">Hans W. A. Hanley</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Deepak Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Durumeric%2C+Z">Zakir Durumeric</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CSCW 2023; CSCW version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12309" title="Abstract">arXiv:2301.12309</a> (replaced) [<a href="/pdf/2301.12309" title="Download PDF">pdf</a>, <a href="/format/2301.12309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Lipschitz Constant of Deep Networks and Double Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gamba%2C+M">Matteo Gamba</a>, 
<a href="/search/cs?searchtype=author&query=Azizpour%2C+H">Hossein Azizpour</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rkman%2C+M">M&#xe5;rten Bj&#xf6;rkman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13116" title="Abstract">arXiv:2301.13116</a> (replaced) [<a href="/pdf/2301.13116" title="Download PDF">pdf</a>, <a href="/format/2301.13116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Big Ramsey degrees and infinite languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Braunfeld%2C+S">Samuel Braunfeld</a>, 
<a href="/search/math?searchtype=author&query=Chodounsk%C3%BD%2C+D">David Chodounsk&#xfd;</a>, 
<a href="/search/math?searchtype=author&query=de+Rancourt%2C+N">No&#xe9; de Rancourt</a>, 
<a href="/search/math?searchtype=author&query=Hubi%C4%8Dka%2C+J">Jan Hubi&#x10d;ka</a>, 
<a href="/search/math?searchtype=author&query=Kawach%2C+J">Jamal Kawach</a>, 
<a href="/search/math?searchtype=author&query=Kone%C4%8Dn%C3%BD%2C+M">Mat&#x11b;j Kone&#x10d;n&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages. Newest update corrects a mistake in a funding acknowledgment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13591" title="Abstract">arXiv:2301.13591</a> (replaced) [<a href="/e-print/2301.13591" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero3D: Semantic-Driven Multi-Category 3D Shape Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yitong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yixuan Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13737" title="Abstract">arXiv:2301.13737</a> (replaced) [<a href="/pdf/2301.13737" title="Download PDF">pdf</a>, <a href="/format/2301.13737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Consistent Velocity Matching of Probability Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Hurault%2C+S">Samuel Hurault</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+J">Justin Solomon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00240" title="Abstract">arXiv:2302.00240</a> (replaced) [<a href="/pdf/2302.00240" title="Download PDF">pdf</a>, <a href="/format/2302.00240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Efficient Transportation Electrification of Heavy-Duty Trucks:  Joint Scheduling of Truck Routing and Charging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bragin%2C+M+A">Mikhail A. Bragin</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+Z">Zuzhao Ye</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+N">Nanpeng Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computers and Society (cs.CY); Combinatorics (math.CO); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01686" title="Abstract">arXiv:2302.01686</a> (replaced) [<a href="/pdf/2302.01686" title="Download PDF">pdf</a>, <a href="/format/2302.01686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Playing with Data: An Augmented Reality Approach to Interact with  Visualizations of Industrial Process Tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xuan%2C+Y">Yueming Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+R">Rahul Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Omrani%2C+A">Adel Omrani</a>, 
<a href="/search/cs?searchtype=author&query=Fjeld%2C+M">Morten Fjeld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In IFIP Conference on Human-Computer Interaction 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IFIP Conference on Human-Computer Interaction 2023 Aug 28 vol
  14143 (pp. 123-144)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01690" title="Abstract">arXiv:2302.01690</a> (replaced) [<a href="/pdf/2302.01690" title="Download PDF">pdf</a>, <a href="/format/2302.01690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> See or Hear? Exploring the Effect of Visual and Audio Hints and  Gaze-assisted Task Feedback for Visual Search Tasks in Augmented Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nowak%2C+A">Adam Nowak</a>, 
<a href="/search/cs?searchtype=author&query=Xuan%2C+Y">Yueming Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Romanowski%2C+A">Andrzej Romanowski</a>, 
<a href="/search/cs?searchtype=author&query=Fjeld%2C+M">Morten Fjeld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of 2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Symposium on Mixed and Augmented Reality
  (ISMAR) (pp. 1113-1122). IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03683" title="Abstract">arXiv:2302.03683</a> (replaced) [<a href="/pdf/2302.03683" title="Download PDF">pdf</a>, <a href="/ps/2302.03683" title="Download PostScript">ps</a>, <a href="/format/2302.03683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Partial Monitoring for Sequential Decision-Making: Algorithms,  Regret Bounds and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirschner%2C+J">Johannes Kirschner</a>, 
<a href="/search/cs?searchtype=author&query=Lattimore%2C+T">Tor Lattimore</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06015" title="Abstract">arXiv:2302.06015</a> (replaced) [<a href="/pdf/2302.06015" title="Download PDF">pdf</a>, <a href="/format/2302.06015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Understanding of Shallow Vision Transformers: Learning,  Generalization, and Sample Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongkang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-yu Chen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07597" title="Abstract">arXiv:2302.07597</a> (replaced) [<a href="/pdf/2302.07597" title="Download PDF">pdf</a>, <a href="/format/2302.07597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preventive-Corrective Cyber-Defense: Attack-Induced Region Minimization  and Cybersecurity Margin Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hou%2C+J">Jiazuo Hou</a>, 
<a href="/search/eess?searchtype=author&query=Teng%2C+F">Fei Teng</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+W">Wenqian Yin</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+Y">Yue Song</a>, 
<a href="/search/eess?searchtype=author&query=Hou%2C+Y">Yunhe Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08942" title="Abstract">arXiv:2302.08942</a> (replaced) [<a href="/pdf/2302.08942" title="Download PDF">pdf</a>, <a href="/format/2302.08942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC-Bayesian Generalization Bounds for Adversarial Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mbacke%2C+S+D">Sokhna Diarra Mbacke</a>, 
<a href="/search/cs?searchtype=author&query=Clerc%2C+F">Florence Clerc</a>, 
<a href="/search/cs?searchtype=author&query=Germain%2C+P">Pascal Germain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14456" title="Abstract">arXiv:2302.14456</a> (replaced) [<a href="/pdf/2302.14456" title="Download PDF">pdf</a>, <a href="/format/2302.14456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Riemannian preconditioned algorithms for tensor completion via tensor  ring decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gao%2C+B">Bin Gao</a>, 
<a href="/search/math?searchtype=author&query=Peng%2C+R">Renfeng Peng</a>, 
<a href="/search/math?searchtype=author&query=Yuan%2C+Y">Ya-xiang Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 7 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00910" title="Abstract">arXiv:2303.00910</a> (replaced) [<a href="/pdf/2303.00910" title="Download PDF">pdf</a>, <a href="/ps/2303.00910" title="Download PostScript">ps</a>, <a href="/format/2303.00910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bipedal Robot Running: Human-like Actuation Timing Using Fast and Slow  Adaptations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sakurai%2C+Y">Yusuke Sakurai</a>, 
<a href="/search/cs?searchtype=author&query=Kamimura%2C+T">Tomoya Kamimura</a>, 
<a href="/search/cs?searchtype=author&query=Sakamoto%2C+Y">Yuki Sakamoto</a>, 
<a href="/search/cs?searchtype=author&query=Nishii%2C+S">Shohei Nishii</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+K">Kodai Sato</a>, 
<a href="/search/cs?searchtype=author&query=Fujiwara%2C+Y">Yuta Fujiwara</a>, 
<a href="/search/cs?searchtype=author&query=Sano%2C+A">Akihito Sano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 12 figures, submitted to Advanced Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02867" title="Abstract">arXiv:2303.02867</a> (replaced) [<a href="/pdf/2303.02867" title="Download PDF">pdf</a>, <a href="/ps/2303.02867" title="Download PostScript">ps</a>, <a href="/format/2303.02867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boundary-semantic collaborative guidance network with dual-stream  feedback mechanism for salient object detection in optical remote sensing  imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Dejun Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Suning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Z">Ziyang Liao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xingyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yakun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TGRS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04749" title="Abstract">arXiv:2303.04749</a> (replaced) [<a href="/pdf/2303.04749" title="Download PDF">pdf</a>, <a href="/ps/2303.04749" title="Download PostScript">ps</a>, <a href="/format/2303.04749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Robust Backward Reachable Sets for Set-Theoretic Model  Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Attar%2C+M">Mehran Attar</a>, 
<a href="/search/eess?searchtype=author&query=Lucia%2C+W">Walter Lucia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint jointly submitted to IEEE Control Systems Letters (L-CSS) and IEEE Conference on Decision and Control (CDC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06273" title="Abstract">arXiv:2303.06273</a> (replaced) [<a href="/pdf/2303.06273" title="Download PDF">pdf</a>, <a href="/format/2303.06273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistency Analysis of ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+M+E">Myeongjun Erik Jang</a>, 
<a href="/search/cs?searchtype=author&query=Lukasiewicz%2C+T">Thomas Lukasiewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06536" title="Abstract">arXiv:2303.06536</a> (replaced) [<a href="/pdf/2303.06536" title="Download PDF">pdf</a>, <a href="/format/2303.06536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoOptLib: Tailoring Metaheuristic Optimizers via Automated Algorithm  Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Taiwei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianglong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Q">Qiqi Duan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Mathematical Software (cs.MS)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08706" title="Abstract">arXiv:2303.08706</a> (replaced) [<a href="/pdf/2303.08706" title="Download PDF">pdf</a>, <a href="/format/2303.08706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Modular Redundancy: Exploring Modular Redundancy Approaches in  RISC-V Multi-Core Computing Clusters for Reliable Processing in Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rogenmoser%2C+M">Michael Rogenmoser</a>, 
<a href="/search/eess?searchtype=author&query=Tortorella%2C+Y">Yvan Tortorella</a>, 
<a href="/search/eess?searchtype=author&query=Rossi%2C+D">Davide Rossi</a>, 
<a href="/search/eess?searchtype=author&query=Conti%2C+F">Francesco Conti</a>, 
<a href="/search/eess?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10167" title="Abstract">arXiv:2303.10167</a> (replaced) [<a href="/pdf/2303.10167" title="Download PDF">pdf</a>, <a href="/format/2303.10167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized partitioned local depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Berenhaut%2C+K+S">Kenneth S. Berenhaut</a>, 
<a href="/search/stat?searchtype=author&query=Foley%2C+J+D">John D. Foley</a>, 
<a href="/search/stat?searchtype=author&query=Lyu%2C+L">Liangdongsheng Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Typos correct &amp; clarifying comments, 19 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12310" title="Abstract">arXiv:2303.12310</a> (replaced) [<a href="/pdf/2303.12310" title="Download PDF">pdf</a>, <a href="/format/2303.12310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System and Design Technology Co-optimization of SOT-MRAM for  High-Performance AI Accelerator Memory System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishty%2C+K">Kaniz Mishty</a>, 
<a href="/search/cs?searchtype=author&query=Sadi%2C+M">Mehdi Sadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16445" title="Abstract">arXiv:2303.16445</a> (replaced) [<a href="/pdf/2303.16445" title="Download PDF">pdf</a>, <a href="/format/2303.16445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Larger Probes Tell a Different Story: Extending Psycholinguistic  Datasets Via In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shivagunde%2C+N">Namrata Shivagunde</a>, 
<a href="/search/cs?searchtype=author&query=Lialin%2C+V">Vladislav Lialin</a>, 
<a href="/search/cs?searchtype=author&query=Rumshisky%2C+A">Anna Rumshisky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures. Published as a conference paper at EMNLP 2023 (short). The datasets and code are available on this $\href{<a href="https://github.com/text-machine-lab/extending_psycholinguistic_dataset">this https URL</a>}{URL}$
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00921" title="Abstract">arXiv:2304.00921</a> (replaced) [<a href="/pdf/2304.00921" title="Download PDF">pdf</a>, <a href="/format/2304.00921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Abstraqt: Analysis of Quantum Circuits via Abstract Stabilizer  Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bichsel%2C+B">Benjamin Bichsel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Paradis%2C+A">Anouk Paradis</a>, 
<a href="/search/quant-ph?searchtype=author&query=Baader%2C+M">Maximilian Baader</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02192" title="Abstract">arXiv:2304.02192</a> (replaced) [<a href="/pdf/2304.02192" title="Download PDF">pdf</a>, <a href="/format/2304.02192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Diffusion-based Method for Multi-turn Compositional Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024 3rd Workshop on Image/Video/Audio Quality in Computer Vision and Generative AI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03174" title="Abstract">arXiv:2304.03174</a> (replaced) [<a href="/e-print/2304.03174" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SketchFFusion: Sketch-guided image editing with diffusion model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+W">Weihang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03426" title="Abstract">arXiv:2304.03426</a> (replaced) [<a href="/pdf/2304.03426" title="Download PDF">pdf</a>, <a href="/ps/2304.03426" title="Download PostScript">ps</a>, <a href="/format/2304.03426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haotian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+T">Yin Tat Lee</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lichen Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05071" title="Abstract">arXiv:2304.05071</a> (replaced) [<a href="/pdf/2304.05071" title="Download PDF">pdf</a>, <a href="/format/2304.05071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fracture Detection in Pediatric Wrist Trauma X-ray Images Using YOLOv8  Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+R">Rui-Yang Ju</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+W">Weiming Cai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Scientific Reports
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06193" title="Abstract">arXiv:2304.06193</a> (replaced) [<a href="/pdf/2304.06193" title="Download PDF">pdf</a>, <a href="/format/2304.06193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Over Contracting and Lipschitz Closed-Loops for  Partially-Observed Nonlinear Systems (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Barbara%2C+N+H">Nicholas H. Barbara</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+R">Ruigang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Manchester%2C+I+R">Ian R. Manchester</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08330" title="Abstract">arXiv:2304.08330</a> (replaced) [<a href="/pdf/2304.08330" title="Download PDF">pdf</a>, <a href="/format/2304.08330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scenario Approach for Parametric Markov Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Turrini%2C+A">Andrea Turrini</a>, 
<a href="/search/cs?searchtype=author&query=Hahn%2C+M">Moritz Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Bai Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 8 figures; updated to add acknowledgements and data availability
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13765" title="Abstract">arXiv:2304.13765</a> (replaced) [<a href="/pdf/2304.13765" title="Download PDF">pdf</a>, <a href="/format/2304.13765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards ethical multimodal systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roger%2C+A">Alexis Roger</a>, 
<a href="/search/cs?searchtype=author&query=A%C3%AFmeur%2C+E">Esma A&#xef;meur</a>, 
<a href="/search/cs?searchtype=author&query=Rish%2C+I">Irina Rish</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, multimodal ethical dataset building, accepted in the NeurIPS 2023 MP2 workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01406" title="Abstract">arXiv:2305.01406</a> (replaced) [<a href="/pdf/2305.01406" title="Download PDF">pdf</a>, <a href="/ps/2305.01406" title="Download PostScript">ps</a>, <a href="/format/2305.01406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Mobile Quad-Arm Robot ARMS: Wheeled-Legged Tripedal Locomotion and  Quad-Arm Loco-Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muramatsu%2C+H">Hisayoshi Muramatsu</a>, 
<a href="/search/cs?searchtype=author&query=Kitagawa%2C+K">Keigo Kitagawa</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+J">Jun Watanabe</a>, 
<a href="/search/cs?searchtype=author&query=Yoshimoto%2C+Y">Yuika Yoshimoto</a>, 
<a href="/search/cs?searchtype=author&query=Hisashiki%2C+R">Ryohei Hisashiki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02143" title="Abstract">arXiv:2305.02143</a> (replaced) [<a href="/pdf/2305.02143" title="Download PDF">pdf</a>, <a href="/format/2305.02143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GANonymization: A GAN-based Face Anonymization Framework for Preserving  Emotional Expressions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hellmann%2C+F">Fabio Hellmann</a>, 
<a href="/search/cs?searchtype=author&query=Mertes%2C+S">Silvan Mertes</a>, 
<a href="/search/cs?searchtype=author&query=Benouis%2C+M">Mohamed Benouis</a>, 
<a href="/search/cs?searchtype=author&query=Hustinx%2C+A">Alexander Hustinx</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+T">Tzung-Chien Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Conati%2C+C">Cristina Conati</a>, 
<a href="/search/cs?searchtype=author&query=Krawitz%2C+P">Peter Krawitz</a>, 
<a href="/search/cs?searchtype=author&query=Andr%C3%A9%2C+E">Elisabeth Andr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 11 figures, 6 tables, ACM Transactions on Multimedia Computing, Communications, and Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03520" title="Abstract">arXiv:2305.03520</a> (replaced) [<a href="/pdf/2305.03520" title="Download PDF">pdf</a>, <a href="/ps/2305.03520" title="Download PostScript">ps</a>, <a href="/format/2305.03520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-Aware Semantic Similarity Measurement for Unsupervised Word  Sense Disambiguation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinez-Gil%2C+J">Jorge Martinez-Gil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04842" title="Abstract">arXiv:2305.04842</a> (replaced) [<a href="/pdf/2305.04842" title="Download PDF">pdf</a>, <a href="/format/2305.04842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outcome Separation Logic: Local Reasoning for Correctness and  Incorrectness with Computational Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zilberstein%2C+N">Noam Zilberstein</a>, 
<a href="/search/cs?searchtype=author&query=Saliling%2C+A">Angelina Saliling</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+A">Alexandra Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05240" title="Abstract">arXiv:2305.05240</a> (replaced) [<a href="/pdf/2305.05240" title="Download PDF">pdf</a>, <a href="/format/2305.05240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A High-performance, Energy-efficient Modular DMA Engine Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benz%2C+T">Thomas Benz</a>, 
<a href="/search/cs?searchtype=author&query=Rogenmoser%2C+M">Michael Rogenmoser</a>, 
<a href="/search/cs?searchtype=author&query=Scheffler%2C+P">Paul Scheffler</a>, 
<a href="/search/cs?searchtype=author&query=Riedel%2C+S">Samuel Riedel</a>, 
<a href="/search/cs?searchtype=author&query=Ottaviano%2C+A">Alessandro Ottaviano</a>, 
<a href="/search/cs?searchtype=author&query=Kurth%2C+A">Andreas Kurth</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures, accepted by an IEEE journal for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13243" title="Abstract">arXiv:2305.13243</a> (replaced) [<a href="/pdf/2305.13243" title="Download PDF">pdf</a>, <a href="/format/2305.13243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chip-Chat: Challenges and Opportunities in Conversational Hardware  Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blocklove%2C+J">Jason Blocklove</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Siddharth Garg</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+R">Ramesh Karri</a>, 
<a href="/search/cs?searchtype=author&query=Pearce%2C+H">Hammond Pearce</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 8 figures. Accepted in 2023 ACM/IEEE 5th Workshop on Machine Learning for CAD (MLCAD)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13277" title="Abstract">arXiv:2305.13277</a> (replaced) [<a href="/pdf/2305.13277" title="Download PDF">pdf</a>, <a href="/format/2305.13277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> U-TILISE: A Sequence-to-sequence Model for Cloud Removal in Optical  Satellite Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stucker%2C+C">Corinne Stucker</a>, 
<a href="/search/cs?searchtype=author&query=Garnot%2C+V+S+F">Vivien Sainte Fare Garnot</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+K">Konrad Schindler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the IEEE Transactions on Geoscience and Remote Sensing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14647" title="Abstract">arXiv:2305.14647</a> (replaced) [<a href="/pdf/2305.14647" title="Download PDF">pdf</a>, <a href="/format/2305.14647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scientific Opinion Summarization: Meta-review Generation with  Checklist-guided Iterative Introspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Sidhu%2C+M">Mankeerat Sidhu</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+H+P">Hou Pong Chan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14770" title="Abstract">arXiv:2305.14770</a> (replaced) [<a href="/pdf/2305.14770" title="Download PDF">pdf</a>, <a href="/format/2305.14770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Natural Language Explanations to Rescale Human Judgments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wadhwa%2C+M">Manya Wadhwa</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jifan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+J">Junyi Jessy Li</a>, 
<a href="/search/cs?searchtype=author&query=Durrett%2C+G">Greg Durrett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Data available at <a href="https://github.com/ManyaWadhwa/explanation_based_rescaling">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14827" title="Abstract">arXiv:2305.14827</a> (replaced) [<a href="/pdf/2305.14827" title="Download PDF">pdf</a>, <a href="/format/2305.14827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sung%2C+M">Mujeen Sung</a>, 
<a href="/search/cs?searchtype=author&query=Gung%2C+J">James Gung</a>, 
<a href="/search/cs?searchtype=author&query=Mansimov%2C+E">Elman Mansimov</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+R">Raphael Shu</a>, 
<a href="/search/cs?searchtype=author&query=Romeo%2C+S">Salvatore Romeo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Castelli%2C+V">Vittorio Castelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15738" title="Abstract">arXiv:2305.15738</a> (replaced) [<a href="/pdf/2305.15738" title="Download PDF">pdf</a>, <a href="/format/2305.15738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum Weight Independent Set in Graphs with no Long Claws in  Quasi-Polynomial Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gartland%2C+P">Peter Gartland</a>, 
<a href="/search/cs?searchtype=author&query=Lokshtanov%2C+D">Daniel Lokshtanov</a>, 
<a href="/search/cs?searchtype=author&query=Masa%C5%99%C3%ADk%2C+T">Tom&#xe1;&#x161; Masa&#x159;&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Marcin Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Micha&#x142; Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Rz%C4%85%C5%BCewski%2C+P">Pawe&#x142; Rz&#x105;&#x17c;ewski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17353" title="Abstract">arXiv:2305.17353</a> (replaced) [<a href="/e-print/2305.17353" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complementary and Integrative Health Lexicon (CIHLex) and Entity  Recognition in the Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huixue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Austin%2C+R">Robin Austin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Sheng-Chieh Lu</a>, 
<a href="/search/cs?searchtype=author&query=Silverman%2C+G">Greg Silverman</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuqi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Kilicoglu%2C+H">Halil Kilicoglu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> need to update the data
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17479" title="Abstract">arXiv:2305.17479</a> (replaced) [<a href="/pdf/2305.17479" title="Download PDF">pdf</a>, <a href="/format/2305.17479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring Causal Effects Under Heterogeneous Peer Influence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adhikari%2C+S">Shishir Adhikari</a>, 
<a href="/search/cs?searchtype=author&query=Zheleva%2C+E">Elena Zheleva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00041" title="Abstract">arXiv:2306.00041</a> (replaced) [<a href="/pdf/2306.00041" title="Download PDF">pdf</a>, <a href="/format/2306.00041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Intervention for Measuring Confidence in Drug-Target Interaction  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Ye%2C+W">Wenting Ye</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+C">Chen Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Xie%2C+Y">Yang Xie</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+H">Hong-Yu Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Cheng%2C+D">Debo Cheng</a>, 
<a href="/search/q-bio?searchtype=author&query=Feng%2C+Z">Zaiwen Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00183" title="Abstract">arXiv:2306.00183</a> (replaced) [<a href="/pdf/2306.00183" title="Download PDF">pdf</a>, <a href="/format/2306.00183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffused Redundancy in Pre-trained Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nanda%2C+V">Vedant Nanda</a>, 
<a href="/search/cs?searchtype=author&query=Speicher%2C+T">Till Speicher</a>, 
<a href="/search/cs?searchtype=author&query=Dickerson%2C+J+P">John P. Dickerson</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>, 
<a href="/search/cs?searchtype=author&query=Gummadi%2C+K+P">Krishna P. Gummadi</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+A">Adrian Weller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05583" title="Abstract">arXiv:2306.05583</a> (replaced) [<a href="/pdf/2306.05583" title="Download PDF">pdf</a>, <a href="/format/2306.05583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gibbs-Based Information Criteria and the Over-Parameterized Regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haobo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+Y">Yuheng Bu</a>, 
<a href="/search/cs?searchtype=author&query=Wornell%2C+G+W">Gregory W. Wornell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06210" title="Abstract">arXiv:2306.06210</a> (replaced) [<a href="/pdf/2306.06210" title="Download PDF">pdf</a>, <a href="/format/2306.06210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-Model Attribution of Generative Models Through Final-Layer  Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laszkiewicz%2C+M">Mike Laszkiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Ricker%2C+J">Jonas Ricker</a>, 
<a href="/search/cs?searchtype=author&query=Lederer%2C+J">Johannes Lederer</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+A">Asja Fischer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08230" title="Abstract">arXiv:2306.08230</a> (replaced) [<a href="/pdf/2306.08230" title="Download PDF">pdf</a>, <a href="/format/2306.08230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbiased Learning of Deep Generative Models with Structured Discrete  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bendekgey%2C+H">Harry Bendekgey</a>, 
<a href="/search/cs?searchtype=author&query=Hope%2C+G">Gabriel Hope</a>, 
<a href="/search/cs?searchtype=author&query=Sudderth%2C+E+B">Erik B. Sudderth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08698" title="Abstract">arXiv:2306.08698</a> (replaced) [<a href="/pdf/2306.08698" title="Download PDF">pdf</a>, <a href="/ps/2306.08698" title="Download PostScript">ps</a>, <a href="/format/2306.08698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase Transitions of Civil Unrest across Countries and Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Braha%2C+D">Dan Braha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main paper (57 pages); Supporting Information (144 pages) will be available upon request. To appear in npj Complexity
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Machine Learning (cs.LG); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09126" title="Abstract">arXiv:2306.09126</a> (replaced) [<a href="/pdf/2306.09126" title="Download PDF">pdf</a>, <a href="/format/2306.09126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes  with Spatiotemporal Annotations of Sound Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shimada%2C+K">Kazuki Shimada</a>, 
<a href="/search/cs?searchtype=author&query=Politis%2C+A">Archontis Politis</a>, 
<a href="/search/cs?searchtype=author&query=Sudarsanam%2C+P">Parthasaarathy Sudarsanam</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+D">Daniel Krause</a>, 
<a href="/search/cs?searchtype=author&query=Uchida%2C+K">Kengo Uchida</a>, 
<a href="/search/cs?searchtype=author&query=Adavanne%2C+S">Sharath Adavanne</a>, 
<a href="/search/cs?searchtype=author&query=Hakala%2C+A">Aapo Hakala</a>, 
<a href="/search/cs?searchtype=author&query=Koyama%2C+Y">Yuichiro Koyama</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+N">Naoya Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+S">Shusuke Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Virtanen%2C+T">Tuomas Virtanen</a>, 
<a href="/search/cs?searchtype=author&query=Mitsufuji%2C+Y">Yuki Mitsufuji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 9 figures, accepted for publication in NeurIPS 2023 Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Audio and Speech Processing (eess.AS); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09299" title="Abstract">arXiv:2306.09299</a> (replaced) [<a href="/pdf/2306.09299" title="Download PDF">pdf</a>, <a href="/format/2306.09299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Language Models Teach Weaker Agents? Teacher Explanations Improve  Students via Personalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Swarnadeep Saha</a>, 
<a href="/search/cs?searchtype=author&query=Hase%2C+P">Peter Hase</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (23 pages, 12 figures). Our code is available at <a href="https://github.com/swarnaHub/ExplanationIntervention">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11797" title="Abstract">arXiv:2306.11797</a> (replaced) [<a href="/pdf/2306.11797" title="Download PDF">pdf</a>, <a href="/format/2306.11797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a robust and reliable deep learning approach for detection of  compact binary mergers in gravitational wave data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/gr-qc?searchtype=author&query=Jadhav%2C+S">Shreejit Jadhav</a>, 
<a href="/search/gr-qc?searchtype=author&query=Shrivastava%2C+M">Mihir Shrivastava</a>, 
<a href="/search/gr-qc?searchtype=author&query=Mitra%2C+S">Sanjit Mitra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 22 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mach. Learn.: Sci. Technol. 4 045028 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Relativity and Quantum Cosmology (gr-qc)</span>; High Energy Astrophysical Phenomena (astro-ph.HE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13258" title="Abstract">arXiv:2306.13258</a> (replaced) [<a href="/pdf/2306.13258" title="Download PDF">pdf</a>, <a href="/format/2306.13258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Maximum $k$-Plex Algorithms Parameterized by Small Degeneracy Gaps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengren Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Chunyu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Mingyu Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jin-Kao Hao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper extends the conference paper "A Fast Maximum $k$-Plex Algorithm Parameterized by the Degeneracy Gap", presented in the 32nd International Joint Conference on Artificial Intelligence (IJCAI'23) in Macao on 19th August 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13508" title="Abstract">arXiv:2306.13508</a> (replaced) [<a href="/pdf/2306.13508" title="Download PDF">pdf</a>, <a href="/format/2306.13508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The effect of distant connections on node anonymity in complex networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Jong%2C+R+G">Rachel G. de Jong</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Loo%2C+M+P+J">Mark P. J. van der Loo</a>, 
<a href="/search/cs?searchtype=author&query=Takes%2C+F+W">Frank W. Takes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> New version. Updated title, and results added to Supplementary information
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13724" title="Abstract">arXiv:2306.13724</a> (replaced) [<a href="/pdf/2306.13724" title="Download PDF">pdf</a>, <a href="/format/2306.13724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of compressed embedding layers and their applications for  recommender systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajgato%2C+T">Tamas Hajgato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14809" title="Abstract">arXiv:2306.14809</a> (replaced) [<a href="/pdf/2306.14809" title="Download PDF">pdf</a>, <a href="/format/2306.14809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tanimoto Random Features for Scalable Molecular Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripp%2C+A">Austin Tripp</a>, 
<a href="/search/cs?searchtype=author&query=Bacallado%2C+S">Sergio Bacallado</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sukriti Singh</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready version presented at NeurIPS 2023. Updates include: notation changes, better description of features in section 4, updated experiments, link to code
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15794" title="Abstract">arXiv:2306.15794</a> (replaced) [<a href="/pdf/2306.15794" title="Download PDF">pdf</a>, <a href="/format/2306.15794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide  Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+E">Eric Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Poli%2C+M">Michael Poli</a>, 
<a href="/search/cs?searchtype=author&query=Faizi%2C+M">Marjan Faizi</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+A">Armin Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Birch-Sykes%2C+C">Callum Birch-Sykes</a>, 
<a href="/search/cs?searchtype=author&query=Wornow%2C+M">Michael Wornow</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Aman Patel</a>, 
<a href="/search/cs?searchtype=author&query=Rabideau%2C+C">Clayton Rabideau</a>, 
<a href="/search/cs?searchtype=author&query=Massaroli%2C+S">Stefano Massaroli</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>, 
<a href="/search/cs?searchtype=author&query=Baccus%2C+S+A">Stephen A. Baccus</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9%2C+C">Chris R&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Genomics (q-bio.GN)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00754" title="Abstract">arXiv:2307.00754</a> (replaced) [<a href="/pdf/2307.00754" title="Download PDF">pdf</a>, <a href="/format/2307.00754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ImDiffusion: Imputed Diffusion Models for Multivariate Time Series  Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Minghua Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yudong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+R">Ruomeng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shilin He</a>, 
<a href="/search/cs?searchtype=author&query=Rajmohan%2C+S">Saravan Rajmohan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in VLDB 2024.Code: <a href="https://github.com/17000cyh/IMDiffusion.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01988" title="Abstract">arXiv:2307.01988</a> (replaced) [<a href="/pdf/2307.01988" title="Download PDF">pdf</a>, <a href="/ps/2307.01988" title="Download PostScript">ps</a>, <a href="/format/2307.01988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the convergence analysis of the greedy randomized Kaczmarz method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Su%2C+Y">Yansheng Su</a>, 
<a href="/search/math?searchtype=author&query=Han%2C+D">Deren Han</a>, 
<a href="/search/math?searchtype=author&query=Zeng%2C+Y">Yun Zeng</a>, 
<a href="/search/math?searchtype=author&query=Xie%2C+J">Jiaxin Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02933" title="Abstract">arXiv:2307.02933</a> (replaced) [<a href="/pdf/2307.02933" title="Download PDF">pdf</a>, <a href="/format/2307.02933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In Time and Space: Towards Usable Adaptive Control for Assistive Robotic  Arms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pascher%2C+M">Max Pascher</a>, 
<a href="/search/cs?searchtype=author&query=Kronhardt%2C+K">Kirill Kronhardt</a>, 
<a href="/search/cs?searchtype=author&query=Goldau%2C+F+F">Felix Ferdinand Goldau</a>, 
<a href="/search/cs?searchtype=author&query=Frese%2C+U">Udo Frese</a>, 
<a href="/search/cs?searchtype=author&query=Gerken%2C+J">Jens Gerken</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> RO-MAN'23: 32nd IEEE International Conference on Robot and Human Interactive Communication, Busan, Korea
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06440" title="Abstract">arXiv:2307.06440</a> (replaced) [<a href="/pdf/2307.06440" title="Download PDF">pdf</a>, <a href="/format/2307.06440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Train No Gain: Revisiting Efficient Training Algorithms For  Transformer-based Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaddour%2C+J">Jean Kaddour</a>, 
<a href="/search/cs?searchtype=author&query=Key%2C+O">Oscar Key</a>, 
<a href="/search/cs?searchtype=author&query=Nawrot%2C+P">Piotr Nawrot</a>, 
<a href="/search/cs?searchtype=author&query=Minervini%2C+P">Pasquale Minervini</a>, 
<a href="/search/cs?searchtype=author&query=Kusner%2C+M+J">Matt J. Kusner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07848" title="Abstract">arXiv:2307.07848</a> (replaced) [<a href="/pdf/2307.07848" title="Download PDF">pdf</a>, <a href="/ps/2307.07848" title="Download PostScript">ps</a>, <a href="/format/2307.07848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Scalable MPC Algorithms for Clustering in High Dimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Czumaj%2C+A">Artur Czumaj</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Guichen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S+H+-">Shaofeng H.-C. Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Krauthgamer%2C+R">Robert Krauthgamer</a>, 
<a href="/search/cs?searchtype=author&query=Vesel%C3%BD%2C+P">Pavel Vesel&#xfd;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08684" title="Abstract">arXiv:2307.08684</a> (replaced) [<a href="/pdf/2307.08684" title="Download PDF">pdf</a>, <a href="/format/2307.08684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Rubik&#x27;s Cube inspired approach to Clifford synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bao%2C+N">Ning Bao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hartnett%2C+G+S">Gavin S. Hartnett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09444" title="Abstract">arXiv:2307.09444</a> (replaced) [<a href="/pdf/2307.09444" title="Download PDF">pdf</a>, <a href="/format/2307.09444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No distributed quantum advantage for approximate graph coloring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coiteux-Roy%2C+X">Xavier Coiteux-Roy</a>, 
<a href="/search/cs?searchtype=author&query=d%27Amore%2C+F">Francesco d&#x27;Amore</a>, 
<a href="/search/cs?searchtype=author&query=Gajjala%2C+R">Rishikesh Gajjala</a>, 
<a href="/search/cs?searchtype=author&query=Kuhn%2C+F">Fabian Kuhn</a>, 
<a href="/search/cs?searchtype=author&query=Gall%2C+F+L">Fran&#xe7;ois Le Gall</a>, 
<a href="/search/cs?searchtype=author&query=Lievonen%2C+H">Henrik Lievonen</a>, 
<a href="/search/cs?searchtype=author&query=Modanese%2C+A">Augusto Modanese</a>, 
<a href="/search/cs?searchtype=author&query=Renou%2C+M">Marc-Olivier Renou</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+G">Gustav Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Suomela%2C+J">Jukka Suomela</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM); Emerging Technologies (cs.ET); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10485" title="Abstract">arXiv:2307.10485</a> (replaced) [<a href="/pdf/2307.10485" title="Download PDF">pdf</a>, <a href="/format/2307.10485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FinGPT: Democratizing Internet-scale Data for Financial Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao-Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongyang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+D">Daochen Zha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 8 tables, and 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); General Finance (q-fin.GN)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10902" title="Abstract">arXiv:2307.10902</a> (replaced) [<a href="/pdf/2307.10902" title="Download PDF">pdf</a>, <a href="/format/2307.10902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong Invariants Are Hard: On the Hardness of Strongest Polynomial  Invariants for (Probabilistic) Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCllner%2C+J">Julian M&#xfc;llner</a>, 
<a href="/search/cs?searchtype=author&query=Moosbrugger%2C+M">Marcel Moosbrugger</a>, 
<a href="/search/cs?searchtype=author&query=Kov%C3%A1cs%2C+L">Laura Kov&#xe1;cs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at POPL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12540" title="Abstract">arXiv:2307.12540</a> (replaced) [<a href="/pdf/2307.12540" title="Download PDF">pdf</a>, <a href="/format/2307.12540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniFormaly: Towards Task-Agnostic Unified Framework for Visual Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yujin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+H">Harin Lim</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+S">Seoyoon Jang</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+H">Hyunsoo Yoon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 13 figures. Codes are available at <a href="https://github.com/YoojLee/Uniformaly">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12810" title="Abstract">arXiv:2307.12810</a> (replaced) [<a href="/pdf/2307.12810" title="Download PDF">pdf</a>, <a href="/format/2307.12810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HeteFedRec: Federated Recommender Systems with Model Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Liang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Lizhen Cui</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+Y">Yongxin Tong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaofang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14277" title="Abstract">arXiv:2307.14277</a> (replaced) [<a href="/pdf/2307.14277" title="Download PDF">pdf</a>, <a href="/format/2307.14277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and  Game Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Meng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xuxin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaowei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhihong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yuexian Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV2023 oral
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15807" title="Abstract">arXiv:2307.15807</a> (replaced) [<a href="/pdf/2307.15807" title="Download PDF">pdf</a>, <a href="/format/2307.15807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anomaly Detection in Industrial Machinery using IoT Devices and Machine  Learning: a Systematic Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chevtchenko%2C+S+F">S&#xe9;rgio F. Chevtchenko</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva+Rocha%2C+E">Elisson da Silva Rocha</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+M+C+M+D">Monalisa Cristina Moura Dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Mota%2C+R+L">Ricardo Lins Mota</a>, 
<a href="/search/cs?searchtype=author&query=Vieira%2C+D+M">Diego Moura Vieira</a>, 
<a href="/search/cs?searchtype=author&query=de+Andrade%2C+E+C">Ermeson Carneiro de Andrade</a>, 
<a href="/search/cs?searchtype=author&query=de+Ara%C3%BAjo%2C+D+R+B">Danilo Ricardo Barbosa de Ara&#xfa;jo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16694" title="Abstract">arXiv:2307.16694</a> (replaced) [<a href="/pdf/2307.16694" title="Download PDF">pdf</a>, <a href="/format/2307.16694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating and Improving Latent Density Segmentation Models for  Aleatoric Uncertainty Quantification in Medical Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valiuddin%2C+M+M+A">M. M. Amaan Valiuddin</a>, 
<a href="/search/cs?searchtype=author&query=Viviers%2C+C+G+A">Christiaan G. A. Viviers</a>, 
<a href="/search/cs?searchtype=author&query=van+Sloun%2C+R+J+G">Ruud J. G. van Sloun</a>, 
<a href="/search/cs?searchtype=author&query=de+With%2C+P+H+N">Peter H. N. de With</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Sommen%2C+F">Fons van der Sommen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages incl. references, 11 figures. EDIT: updated figure 6 due to rendering error
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00198" title="Abstract">arXiv:2308.00198</a> (replaced) [<a href="/pdf/2308.00198" title="Download PDF">pdf</a>, <a href="/format/2308.00198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Syntactically and semantically regular languages of lambda-terms  coincide through logical relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moreau%2C+V">Vincent Moreau</a>, 
<a href="/search/cs?searchtype=author&query=Nguy%C3%AAn%2C+L+T+D">L&#xea; Th&#xe0;nh D&#x169;ng Nguy&#xea;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the proceedings of CSL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02825" title="Abstract">arXiv:2308.02825</a> (replaced) [<a href="/pdf/2308.02825" title="Download PDF">pdf</a>, <a href="/format/2308.02825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Burning a binary tree and its generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Das%2C+S">Sandip Das</a>, 
<a href="/search/math?searchtype=author&query=Islam%2C+S+S">Sk Samim Islam</a>, 
<a href="/search/math?searchtype=author&query=Mitra%2C+R+M">Ritam M Mitra</a>, 
<a href="/search/math?searchtype=author&query=Paul%2C+S">Sanchita Paul</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04156" title="Abstract">arXiv:2308.04156</a> (replaced) [<a href="/pdf/2308.04156" title="Download PDF">pdf</a>, <a href="/format/2308.04156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Top-Down Stereo Image Quality Assessment via Stereo Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huilin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sumei Li</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Haoxiang Chang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+P">Peiming Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04882" title="Abstract">arXiv:2308.04882</a> (replaced) [<a href="/pdf/2308.04882" title="Download PDF">pdf</a>, <a href="/format/2308.04882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation algorithm for finding multipacking on Cactus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sandip Das</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+S+S">Sk Samim Islam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07336" title="Abstract">arXiv:2308.07336</a> (replaced) [<a href="/pdf/2308.07336" title="Download PDF">pdf</a>, <a href="/format/2308.07336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morishita%2C+T">Terufumi Morishita</a>, 
<a href="/search/cs?searchtype=author&query=Morio%2C+G">Gaku Morio</a>, 
<a href="/search/cs?searchtype=author&query=Yamaguchi%2C+A">Atsuki Yamaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Sogawa%2C+Y">Yasuhiro Sogawa</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 40th International Conference on Machine
  Learning, PMLR 202:25254-25274, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07977" title="Abstract">arXiv:2308.07977</a> (replaced) [<a href="/pdf/2308.07977" title="Download PDF">pdf</a>, <a href="/format/2308.07977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YODA: You Only Diffuse Areas. An Area-Masked Diffusion Approach For  Image Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moser%2C+B+B">Brian B. Moser</a>, 
<a href="/search/cs?searchtype=author&query=Frolov%2C+S">Stanislav Frolov</a>, 
<a href="/search/cs?searchtype=author&query=Raue%2C+F">Federico Raue</a>, 
<a href="/search/cs?searchtype=author&query=Palacio%2C+S">Sebastian Palacio</a>, 
<a href="/search/cs?searchtype=author&query=Dengel%2C+A">Andreas Dengel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Brian B. Moser and Stanislav Frolov contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09345" title="Abstract">arXiv:2308.09345</a> (replaced) [<a href="/pdf/2308.09345" title="Download PDF">pdf</a>, <a href="/ps/2308.09345" title="Download PostScript">ps</a>, <a href="/format/2308.09345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising diffusion-based MRI to CT image translation enables automated  spinal segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Graf%2C+R">Robert Graf</a>, 
<a href="/search/eess?searchtype=author&query=Schmitt%2C+J">Joachim Schmitt</a>, 
<a href="/search/eess?searchtype=author&query=Schlaeger%2C+S">Sarah Schlaeger</a>, 
<a href="/search/eess?searchtype=author&query=M%C3%B6ller%2C+H+K">Hendrik Kristian M&#xf6;ller</a>, 
<a href="/search/eess?searchtype=author&query=Sideri-Lampretsa%2C+V">Vasiliki Sideri-Lampretsa</a>, 
<a href="/search/eess?searchtype=author&query=Sekuboyina%2C+A">Anjany Sekuboyina</a>, 
<a href="/search/eess?searchtype=author&query=Krieg%2C+S+M">Sandro Manuel Krieg</a>, 
<a href="/search/eess?searchtype=author&query=Wiestler%2C+B">Benedikt Wiestler</a>, 
<a href="/search/eess?searchtype=author&query=Menze%2C+B">Bjoern Menze</a>, 
<a href="/search/eess?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>, 
<a href="/search/eess?searchtype=author&query=Kirschke%2C+J+S">Jan Stefan Kirschke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 7 figures, Code and a model weights available <a href="https://doi.org/10.5281/zenodo.8221159">this https URL</a> and <a href="https://doi.org/10.5281/zenodo.8198697">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Eur Radiol Exp 7, 70 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10557" title="Abstract">arXiv:2308.10557</a> (replaced) [<a href="/pdf/2308.10557" title="Download PDF">pdf</a>, <a href="/format/2308.10557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Spherical Harmonics Improve Skeleton-Based Hand Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prasse%2C+K">Katharina Prasse</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+S">Steffen Jung</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuxuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Keuper%2C+M">Margret Keuper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10858" title="Abstract">arXiv:2308.10858</a> (replaced) [<a href="/pdf/2308.10858" title="Download PDF">pdf</a>, <a href="/ps/2308.10858" title="Download PostScript">ps</a>, <a href="/format/2308.10858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compliant Mechanism Synthesis Using Nonlinear Elastic Topology  Optimization with Variable Boundary Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alacoque%2C+L+R">Lee R. Alacoque</a> (1), 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+A">Anurag Bhattacharyya</a> (2), 
<a href="/search/cs?searchtype=author&query=James%2C+K+A">Kai A. James</a> (3) ((1) University of Illinois Urbana-Champaign, (2) Palo Alto Research Center, (3) Georgia Institute of Technology)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 14 figures, 4 tables. Version 2: Corrected an error in the formulation of the strain energy interpolation scheme, leading to minor changes in the results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10874" title="Abstract">arXiv:2308.10874</a> (replaced) [<a href="/pdf/2308.10874" title="Download PDF">pdf</a>, <a href="/format/2308.10874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Transformer Dynamics as Movement through Embedding Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S+S">Sumeet S. Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V2. Rewrote abstract. Rewrote / re-organized the entire paper into a more formal proposition/argument/result format. To shorten main paper length: Wrote more compact text in general, moved "negative self bias" and "encoder v/s decoder walks" sections to the appendix and packed figures. Styled as TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13816" title="Abstract">arXiv:2308.13816</a> (replaced) [<a href="/pdf/2308.13816" title="Download PDF">pdf</a>, <a href="/format/2308.13816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homological Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Briola%2C+A">Antonio Briola</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanrong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bartolucci%2C+S">Silvia Bartolucci</a>, 
<a href="/search/cs?searchtype=author&query=Aste%2C+T">Tomaso Aste</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 figures, 11 tables, 1 equation, 1 algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14578" title="Abstract">arXiv:2308.14578</a> (replaced) [<a href="/pdf/2308.14578" title="Download PDF">pdf</a>, <a href="/format/2308.14578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible-Position MIMO for Wireless Communications: Fundamentals,  Challenges, and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiakang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Sumei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+B">Bo Ai</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 1 tables, accepted by IEEE Wireless Communications Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15799" title="Abstract">arXiv:2308.15799</a> (replaced) [<a href="/pdf/2308.15799" title="Download PDF">pdf</a>, <a href="/format/2308.15799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 6G Localization and Sensing in the Near Field: Features, Opportunities,  and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Keskin%2C+M+F">Musa Furkan Keskin</a>, 
<a href="/search/cs?searchtype=author&query=Sakhnini%2C+A">Adham Sakhnini</a>, 
<a href="/search/cs?searchtype=author&query=Decarli%2C+N">Nicol&#xf3; Decarli</a>, 
<a href="/search/cs?searchtype=author&query=Pollin%2C+S">Sofie Pollin</a>, 
<a href="/search/cs?searchtype=author&query=Dardari%2C+D">Davide Dardari</a>, 
<a href="/search/cs?searchtype=author&query=Wymeersch%2C+H">Henk Wymeersch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00305" title="Abstract">arXiv:2309.00305</a> (replaced) [<a href="/pdf/2309.00305" title="Download PDF">pdf</a>, <a href="/format/2309.00305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Surrogate Models for Materials Science Simulations: Machine  Learning-based Prediction of Microstructure Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+B+D">Binh Duong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Potapenko%2C+P">Pavlo Potapenko</a>, 
<a href="/search/cs?searchtype=author&query=Dermici%2C+A">Aytekin Dermici</a>, 
<a href="/search/cs?searchtype=author&query=Govind%2C+K">Kishan Govind</a>, 
<a href="/search/cs?searchtype=author&query=Bompas%2C+S">S&#xe9;bastien Bompas</a>, 
<a href="/search/cs?searchtype=author&query=Sandfeld%2C+S">Stefan Sandfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04873" title="Abstract">arXiv:2309.04873</a> (replaced) [<a href="/pdf/2309.04873" title="Download PDF">pdf</a>, <a href="/ps/2309.04873" title="Download PostScript">ps</a>, <a href="/format/2309.04873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Reversible Computation to Checkpoint-Based Rollback Recovery for  Message-Passing Concurrent Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidal%2C+G">Germ&#xe1;n Vidal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Proceedings of the 19th International Conference on Formal Aspects of Component Software (FACS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04918" title="Abstract">arXiv:2309.04918</a> (replaced) [<a href="/pdf/2309.04918" title="Download PDF">pdf</a>, <a href="/format/2309.04918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Message Ordering using Distributed Kafka Clusters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Shashank Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Jadon%2C+A">Aryan Jadon</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Sachin Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper Accepted - The 2023 International Conference on Innovations in Information Technology (IIT'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07055" title="Abstract">arXiv:2309.07055</a> (replaced) [<a href="/pdf/2309.07055" title="Download PDF">pdf</a>, <a href="/format/2309.07055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geospatial Tessellation in the Agent-In-Cell Model: A Framework for  Agent-Based Modeling of Pandemic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sikaroudi%2C+A+M+E">Amir Mohammad Esmaieeli Sikaroudi</a>, 
<a href="/search/cs?searchtype=author&query=Efrat%2C+A">Alon Efrat</a>, 
<a href="/search/cs?searchtype=author&query=Chertkov%2C+M">Michael Chertkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07376" title="Abstract">arXiv:2309.07376</a> (replaced) [<a href="/pdf/2309.07376" title="Download PDF">pdf</a>, <a href="/format/2309.07376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VCD: A Video Conferencing Dataset for Video Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Naderi%2C+B">Babak Naderi</a>, 
<a href="/search/eess?searchtype=author&query=Cutler%2C+R">Ross Cutler</a>, 
<a href="/search/eess?searchtype=author&query=Khongbantabam%2C+N+S">Nabakumar Singh Khongbantabam</a>, 
<a href="/search/eess?searchtype=author&query=Hosseinkashi%2C+Y">Yasaman Hosseinkashi</a>, 
<a href="/search/eess?searchtype=author&query=Turbell%2C+H">Henrik Turbell</a>, 
<a href="/search/eess?searchtype=author&query=Sadovnikov%2C+A">Albert Sadovnikov</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Q">Quan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08652" title="Abstract">arXiv:2309.08652</a> (replaced) [<a href="/pdf/2309.08652" title="Download PDF">pdf</a>, <a href="/format/2309.08652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Credit Portfolio sensitivity to asset correlations with  interpretable generative neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Caprioli%2C+S">Sergio Caprioli</a>, 
<a href="/search/q-fin?searchtype=author&query=Cagliero%2C+E">Emanuele Cagliero</a>, 
<a href="/search/q-fin?searchtype=author&query=Crupi%2C+R">Riccardo Crupi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08918" title="Abstract">arXiv:2309.08918</a> (replaced) [<a href="/pdf/2309.08918" title="Download PDF">pdf</a>, <a href="/ps/2309.08918" title="Download PostScript">ps</a>, <a href="/format/2309.08918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploration of TPUs for AI Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carri%C3%B3n%2C+D+S">Diego Sanmart&#xed;n Carri&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Prohaska%2C+V">Vera Prohaska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Research done by the Robotics &amp; AI Club at IE University
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10462" title="Abstract">arXiv:2309.10462</a> (replaced) [<a href="/pdf/2309.10462" title="Download PDF">pdf</a>, <a href="/ps/2309.10462" title="Download PostScript">ps</a>, <a href="/format/2309.10462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the Weight Distribution of the Binary Reed-Muller Code  ${\mathcal R} (4,9)$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Markov%2C+M">Miroslav Markov</a>, 
<a href="/search/cs?searchtype=author&query=Borissov%2C+Y">Yuri Borissov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> typos corrected, reference added, revised arguments in section 3, results unchanged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13475" title="Abstract">arXiv:2309.13475</a> (replaced) [<a href="/pdf/2309.13475" title="Download PDF">pdf</a>, <a href="/format/2309.13475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting and Mitigating System-Level Anomalies of Vision-Based  Controllers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aryaman Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+K">Kaustav Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+S">Somil Bansal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14610" title="Abstract">arXiv:2309.14610</a> (replaced) [<a href="/pdf/2309.14610" title="Download PDF">pdf</a>, <a href="/ps/2309.14610" title="Download PostScript">ps</a>, <a href="/format/2309.14610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of  Urban Areas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+K">Kai Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mostafavi%2C+A">Ali Mostafavi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16039" title="Abstract">arXiv:2309.16039</a> (replaced) [<a href="/pdf/2309.16039" title="Download PDF">pdf</a>, <a href="/format/2309.16039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Long-Context Scaling of Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wenhan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Molybog%2C+I">Igor Molybog</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hejia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bhargava%2C+P">Prajjwal Bhargava</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+R">Rui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+L">Louis Martin</a>, 
<a href="/search/cs?searchtype=author&query=Rungta%2C+R">Rashi Rungta</a>, 
<a href="/search/cs?searchtype=author&query=Sankararaman%2C+K+A">Karthik Abinav Sankararaman</a>, 
<a href="/search/cs?searchtype=author&query=Oguz%2C+B">Barlas Oguz</a>, 
<a href="/search/cs?searchtype=author&query=Khabsa%2C+M">Madian Khabsa</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Han Fang</a>, 
<a href="/search/cs?searchtype=author&query=Mehdad%2C+Y">Yashar Mehdad</a>, 
<a href="/search/cs?searchtype=author&query=Narang%2C+S">Sharan Narang</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+K">Kshitiz Malik</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+A">Angela Fan</a>, 
<a href="/search/cs?searchtype=author&query=Bhosale%2C+S">Shruti Bhosale</a>, 
<a href="/search/cs?searchtype=author&query=Edunov%2C+S">Sergey Edunov</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Mike Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sinong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17105" title="Abstract">arXiv:2309.17105</a> (replaced) [<a href="/pdf/2309.17105" title="Download PDF">pdf</a>, <a href="/format/2309.17105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Action Assessment via Task-Consistent Score-Discriminative  Feature Distribution Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan-Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+L">Ling-An Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+J">Jing-Ke Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wei-Shi Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17157" title="Abstract">arXiv:2309.17157</a> (replaced) [<a href="/pdf/2309.17157" title="Download PDF">pdf</a>, <a href="/format/2309.17157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LatticeGen: A Cooperative Framework which Hides Generated Text in a  Lattice for Privacy-Aware Generation on Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianle Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+L">Lu Mi</a>, 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+F">Fatemehsadat Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Binyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00566" title="Abstract">arXiv:2310.00566</a> (replaced) [<a href="/pdf/2310.00566" title="Download PDF">pdf</a>, <a href="/format/2310.00566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Many, Biasing a Few: Generalist Credit Scoring through Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Duanyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yongfu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Weiguang Han</a>, 
<a href="/search/cs?searchtype=author&query=Lopez-Lira%2C+A">Alejandro Lopez-Lira</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01113" title="Abstract">arXiv:2310.01113</a> (replaced) [<a href="/pdf/2310.01113" title="Download PDF">pdf</a>, <a href="/format/2310.01113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperGraphDis: Leveraging Hypergraphs for Contextual and Social-Based  Disinformation Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salamanos%2C+N">Nikos Salamanos</a>, 
<a href="/search/cs?searchtype=author&query=Leonidou%2C+P">Pantelitsa Leonidou</a>, 
<a href="/search/cs?searchtype=author&query=Laoutaris%2C+N">Nikolaos Laoutaris</a>, 
<a href="/search/cs?searchtype=author&query=Sirivianos%2C+M">Michael Sirivianos</a>, 
<a href="/search/cs?searchtype=author&query=Aspri%2C+M">Maria Aspri</a>, 
<a href="/search/cs?searchtype=author&query=Paraschiv%2C+M">Marius Paraschiv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We have updated the paper to include additional references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01331" title="Abstract">arXiv:2310.01331</a> (replaced) [<a href="/pdf/2310.01331" title="Download PDF">pdf</a>, <a href="/format/2310.01331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChoiceMates: Supporting Unfamiliar Online Decision-Making with  Multi-Agent Conversational Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jeongeon Park</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+B">Bryan Min</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaojuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01420" title="Abstract">arXiv:2310.01420</a> (replaced) [<a href="/pdf/2310.01420" title="Download PDF">pdf</a>, <a href="/format/2310.01420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmucker%2C+R">Robin Schmucker</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Meng Xia</a>, 
<a href="/search/cs?searchtype=author&query=Azaria%2C+A">Amos Azaria</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+T">Tom Mitchell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS'23 GAIED, Camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01523" title="Abstract">arXiv:2310.01523</a> (replaced) [<a href="/pdf/2310.01523" title="Download PDF">pdf</a>, <a href="/format/2310.01523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fetal-BET: Brain Extraction Tool for Fetal MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Faghihpirayesh%2C+R">Razieh Faghihpirayesh</a>, 
<a href="/search/eess?searchtype=author&query=Karimi%2C+D">Davood Karimi</a>, 
<a href="/search/eess?searchtype=author&query=Erdo%C4%9Fmu%C5%9F%2C+D">Deniz Erdo&#x11f;mu&#x15f;</a>, 
<a href="/search/eess?searchtype=author&query=Gholipour%2C+A">Ali Gholipour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, 2 TABLES, This work has been submitted to the IEEE Transactions on Medical Imaging for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02251" title="Abstract">arXiv:2310.02251</a> (replaced) [<a href="/pdf/2310.02251" title="Download PDF">pdf</a>, <a href="/format/2310.02251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Talk2BEV: Language-enhanced Bird&#x27;s-eye View Maps for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+T">Tushar Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Dewangan%2C+V">Vikrant Dewangan</a>, 
<a href="/search/cs?searchtype=author&query=Chandhok%2C+S">Shivam Chandhok</a>, 
<a href="/search/cs?searchtype=author&query=Priyadarshan%2C+S">Shubham Priyadarshan</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Anushka Jain</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Arun K. Singh</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Siddharth Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Jatavallabhula%2C+K+M">Krishna Murthy Jatavallabhula</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K+M">K. Madhava Krishna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page at <a href="https://llmbev.github.io/talk2bev/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04368" title="Abstract">arXiv:2310.04368</a> (replaced) [<a href="/pdf/2310.04368" title="Download PDF">pdf</a>, <a href="/format/2310.04368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Core Calculus for Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crichton%2C+W">Will Crichton</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthi%2C+S">Shriram Krishnamurthi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at POPL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05166" title="Abstract">arXiv:2310.05166</a> (replaced) [<a href="/pdf/2310.05166" title="Download PDF">pdf</a>, <a href="/format/2310.05166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Corrected Expected Improvement Acquisition Function Under Noisy  Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Han Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xingchen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Blaschko%2C+M+B">Matthew B Blaschko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07793" title="Abstract">arXiv:2310.07793</a> (replaced) [<a href="/pdf/2310.07793" title="Download PDF">pdf</a>, <a href="/format/2310.07793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenTKG: Generative Forecasting on Temporal Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+R">Ruotong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xu Jia</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunpu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Tresp%2C+V">Volker Tresp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, accepted to Temporal Graph Learning @ NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07838" title="Abstract">arXiv:2310.07838</a> (replaced) [<a href="/pdf/2310.07838" title="Download PDF">pdf</a>, <a href="/format/2310.07838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the Fundamental Limits of Knowledge Transfer over Finite Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qingyue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Banghua Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 2 figures; Appendix polished
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08100" title="Abstract">arXiv:2310.08100</a> (replaced) [<a href="/pdf/2310.08100" title="Download PDF">pdf</a>, <a href="/format/2310.08100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Intrinsic Optimization: Intrinsic Control with Model Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianfei Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08416" title="Abstract">arXiv:2310.08416</a> (replaced) [<a href="/pdf/2310.08416" title="Download PDF">pdf</a>, <a href="/format/2310.08416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying reducible k-tuples of vectors with subspace-proximity  sensitive hashing/filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Holden%2C+G">Gabriella Holden</a>, 
<a href="/search/math?searchtype=author&query=Shiu%2C+D">Daniel Shiu</a>, 
<a href="/search/math?searchtype=author&query=Strutt%2C+L">Lauren Strutt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08450" title="Abstract">arXiv:2310.08450</a> (replaced) [<a href="/pdf/2310.08450" title="Download PDF">pdf</a>, <a href="/format/2310.08450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monotone discretizations of levelset convex geometric PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Calder%2C+J">Jeff Calder</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+W">Wonjun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages including references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08473" title="Abstract">arXiv:2310.08473</a> (replaced) [<a href="/pdf/2310.08473" title="Download PDF">pdf</a>, <a href="/format/2310.08473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No-Regret Learning and Equilibrium Computation in Quantum Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wayne Lin</a>, 
<a href="/search/cs?searchtype=author&query=Piliouras%2C+G">Georgios Piliouras</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+R">Ryann Sim</a>, 
<a href="/search/cs?searchtype=author&query=Varvitsiotis%2C+A">Antonios Varvitsiotis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09400" title="Abstract">arXiv:2310.09400</a> (replaced) [<a href="/pdf/2310.09400" title="Download PDF">pdf</a>, <a href="/format/2310.09400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Collaboration: Uniting Collaborative Filtering with  Pre-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaolong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingdai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yueqing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11755" title="Abstract">arXiv:2310.11755</a> (replaced) [<a href="/pdf/2310.11755" title="Download PDF">pdf</a>, <a href="/format/2310.11755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RGM: A Robust Generalist Matching Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xinyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages. Fixed typo in the first two equations. Code is available at: <a href="https://github.com/aim-uofa/RGM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12096" title="Abstract">arXiv:2310.12096</a> (replaced) [<a href="/pdf/2310.12096" title="Download PDF">pdf</a>, <a href="/format/2310.12096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vital Edges for (s,t)-mincut: Efficient Algorithms, Compact Structures,  and Optimal Sensitivity Oracle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baswana%2C+S">Surender Baswana</a>, 
<a href="/search/cs?searchtype=author&query=Bhanja%2C+K">Koustav Bhanja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12334" title="Abstract">arXiv:2310.12334</a> (replaced) [<a href="/pdf/2310.12334" title="Download PDF">pdf</a>, <a href="/format/2310.12334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Representation Learning for Histopathologic Images with  Cluster Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weiyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chongyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=DiPalma%2C+J">Joseph DiPalma</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+S">Soroush Vosoughi</a>, 
<a href="/search/cs?searchtype=author&query=Hassanpour%2C+S">Saeed Hassanpour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2023, pp. 21404-21414
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12570" title="Abstract">arXiv:2310.12570</a> (replaced) [<a href="/pdf/2310.12570" title="Download PDF">pdf</a>, <a href="/format/2310.12570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DA-TransUNet: Integrating Spatial and Channel Dual Attention with  Transformer U-Net for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Guanqun Sun</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+Y">Yizhi Pan</a>, 
<a href="/search/eess?searchtype=author&query=Kong%2C+W">Weikun Kong</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Z">Zichang Xu</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+J">Jianhua Ma</a>, 
<a href="/search/eess?searchtype=author&query=Racharak%2C+T">Teeradaj Racharak</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+L">Le-Minh Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Xin%2C+J">Junyi Xin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13810" title="Abstract">arXiv:2310.13810</a> (replaced) [<a href="/pdf/2310.13810" title="Download PDF">pdf</a>, <a href="/ps/2310.13810" title="Download PostScript">ps</a>, <a href="/format/2310.13810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Better Match for Drivers and Riders: Reinforcement Learning at Lyft
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azagirre%2C+X">Xabi Azagirre</a>, 
<a href="/search/cs?searchtype=author&query=Balwally%2C+A">Akshay Balwally</a>, 
<a href="/search/cs?searchtype=author&query=Candeli%2C+G">Guillaume Candeli</a>, 
<a href="/search/cs?searchtype=author&query=Chamandy%2C+N">Nicholas Chamandy</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Benjamin Han</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+A">Alona King</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyungjun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Loncaric%2C+M">Martin Loncaric</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+S">Sebastien Martin</a>, 
<a href="/search/cs?searchtype=author&query=Narasiman%2C+V">Vijay Narasiman</a>, 
<a href="/search/cs?searchtype=author&query=Zhiwei">Zhiwei</a> (Tony)Qin, 
<a href="/search/cs?searchtype=author&query=Richard%2C+B">Baptiste Richard</a>, 
<a href="/search/cs?searchtype=author&query=Smoot%2C+S">Sara Smoot</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+S">Sean Taylor</a>, 
<a href="/search/cs?searchtype=author&query=van+Ryzin%2C+G">Garrett van Ryzin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zamoshchin%2C+A">Alex Zamoshchin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14421" title="Abstract">arXiv:2310.14421</a> (replaced) [<a href="/pdf/2310.14421" title="Download PDF">pdf</a>, <a href="/format/2310.14421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On existence, uniqueness and scalability of adversarial robustness  measures for AI classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Horenko%2C+I">Illia Horenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14550" title="Abstract">arXiv:2310.14550</a> (replaced) [<a href="/pdf/2310.14550" title="Download PDF">pdf</a>, <a href="/format/2310.14550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corruption-Robust Offline Reinforcement Learning with General Function  Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+C">Chenlu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15742" title="Abstract">arXiv:2310.15742</a> (replaced) [<a href="/pdf/2310.15742" title="Download PDF">pdf</a>, <a href="/format/2310.15742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Diffusion Models for ECG Imputation with an Augmented Template  Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jenkins%2C+A">Alexander Jenkins</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zehua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+F+S">Fu Siong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Mandic%2C+D">Danilo Mandic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16147" title="Abstract">arXiv:2310.16147</a> (replaced) [<a href="/pdf/2310.16147" title="Download PDF">pdf</a>, <a href="/format/2310.16147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PreWoMe: Exploiting Presuppositions as Working Memory for Long Form  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wookje Han</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinsol Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyungjae Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages 3 figures, Accepted to EMNLP 2023 (short)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16555" title="Abstract">arXiv:2310.16555</a> (replaced) [<a href="/pdf/2310.16555" title="Download PDF">pdf</a>, <a href="/ps/2310.16555" title="Download PostScript">ps</a>, <a href="/format/2310.16555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Information Theory-Based Discovery of Equivariances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charvin%2C+H">Hippolyte Charvin</a>, 
<a href="/search/cs?searchtype=author&query=Volpi%2C+N+C">Nicola Catenacci Volpi</a>, 
<a href="/search/cs?searchtype=author&query=Polani%2C+D">Daniel Polani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Neural and Evolutionary Computing (cs.NE); Group Theory (math.GR)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16600" title="Abstract">arXiv:2310.16600</a> (replaced) [<a href="/pdf/2310.16600" title="Download PDF">pdf</a>, <a href="/format/2310.16600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing central and marginal rejection when combining independent  significance tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Salahub%2C+C">Chris Salahub</a>, 
<a href="/search/stat?searchtype=author&query=Oldford%2C+W">Wayne Oldford</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 page, 18 figures, public technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17490" title="Abstract">arXiv:2310.17490</a> (replaced) [<a href="/pdf/2310.17490" title="Download PDF">pdf</a>, <a href="/format/2310.17490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Zero-shot Reader by Reducing Distractions from Irrelevant  Documents in Open-Domain Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sukmin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Jeongyeon Seo</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+S">Soyeong Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+C">Jong C. Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17784" title="Abstract">arXiv:2310.17784</a> (replaced) [<a href="/pdf/2310.17784" title="Download PDF">pdf</a>, <a href="/format/2310.17784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Centric Financial Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhixuan Chu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Huaiyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yijia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wanqing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Q">Qing Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17851" title="Abstract">arXiv:2310.17851</a> (replaced) [<a href="/pdf/2310.17851" title="Download PDF">pdf</a>, <a href="/format/2310.17851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring CDNs susceptible to Domain Fronting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramani%2C+K">Karthika Subramani</a>, 
<a href="/search/cs?searchtype=author&query=Perdisci%2C+R">Roberto Perdisci</a>, 
<a href="/search/cs?searchtype=author&query=Skafidas%2C+P">Pierros Skafidas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17940" title="Abstract">arXiv:2310.17940</a> (replaced) [<a href="/pdf/2310.17940" title="Download PDF">pdf</a>, <a href="/format/2310.17940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Segment-to-Segment Framework for Simultaneous Sequence  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaolei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18784" title="Abstract">arXiv:2310.18784</a> (replaced) [<a href="/pdf/2310.18784" title="Download PDF">pdf</a>, <a href="/format/2310.18784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-probability Convergence Bounds for Nonlinear Stochastic Gradient  Descent Under Heavy-tailed Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Armacki%2C+A">Aleksandar Armacki</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+P">Pranay Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+G">Gauri Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Bajovic%2C+D">Dragana Bajovic</a>, 
<a href="/search/cs?searchtype=author&query=Jakovetic%2C+D">Dusan Jakovetic</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+S">Soummya Kar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18801" title="Abstract">arXiv:2310.18801</a> (replaced) [<a href="/pdf/2310.18801" title="Download PDF">pdf</a>, <a href="/format/2310.18801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Relative-Measurement-Based Network Localization and Formation  Maneuver Control (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fang%2C+X">Xu Fang</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+L">Lihua Xie</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiaolei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages; 7 figures, title corrected, DOI added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19347" title="Abstract">arXiv:2310.19347</a> (replaced) [<a href="/pdf/2310.19347" title="Download PDF">pdf</a>, <a href="/format/2310.19347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Factual Consistency of Text Summarization by Adversarially  Decoupling Comprehension and Embellishment Abilities of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Huawen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Ting-En Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zekun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuchuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19522" title="Abstract">arXiv:2310.19522</a> (replaced) [<a href="/pdf/2310.19522" title="Download PDF">pdf</a>, <a href="/format/2310.19522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Natural Domain Foundation Models Useful for Medical Image  Classification?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huix%2C+J+P">Joana Pal&#xe9;s Huix</a>, 
<a href="/search/cs?searchtype=author&query=Ganeshan%2C+A+R">Adithya Raju Ganeshan</a>, 
<a href="/search/cs?searchtype=author&query=Haslum%2C+J+F">Johan Fredin Haslum</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%B6derberg%2C+M">Magnus S&#xf6;derberg</a>, 
<a href="/search/cs?searchtype=author&query=Matsoukas%2C+C">Christos Matsoukas</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+K">Kevin Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19647" title="Abstract">arXiv:2310.19647</a> (replaced) [<a href="/pdf/2310.19647" title="Download PDF">pdf</a>, <a href="/ps/2310.19647" title="Download PostScript">ps</a>, <a href="/format/2310.19647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast swap regret minimization and applications to approximate correlated  equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Binghui Peng</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+A">Aviad Rubinstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19721" title="Abstract">arXiv:2310.19721</a> (replaced) [<a href="/pdf/2310.19721" title="Download PDF">pdf</a>, <a href="/format/2310.19721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained  Image Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+D">Dewei Hu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiacheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Oguz%2C+I">Ipek Oguz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated acknowledgments and fixed typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20410" title="Abstract">arXiv:2310.20410</a> (replaced) [<a href="/pdf/2310.20410" title="Download PDF">pdf</a>, <a href="/format/2310.20410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FollowBench: A Multi-level Fine-grained Constraints Following Benchmark  for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuxin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangyou Li</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 figures, 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20689" title="Abstract">arXiv:2310.20689</a> (replaced) [<a href="/pdf/2310.20689" title="Download PDF">pdf</a>, <a href="/format/2310.20689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning From Mistakes Makes LLM Better Reasoner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+S">Shengnan An</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zexiong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zeqi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian-Guang Lou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00176" title="Abstract">arXiv:2311.00176</a> (replaced) [<a href="/pdf/2311.00176" title="Download PDF">pdf</a>, <a href="/format/2311.00176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChipNeMo: Domain-Adapted LLMs for Chip Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ene%2C+T">Teodor-Dumitru Ene</a>, 
<a href="/search/cs?searchtype=author&query=Kirby%2C+R">Robert Kirby</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Chris Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Pinckney%2C+N">Nathaniel Pinckney</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+R">Rongjian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Alben%2C+J">Jonah Alben</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+H">Himyanshu Anand</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Sanmitra Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Bayraktaroglu%2C+I">Ismet Bayraktaroglu</a>, 
<a href="/search/cs?searchtype=author&query=Bhaskaran%2C+B">Bonita Bhaskaran</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+A">Arjun Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Clay%2C+S">Sharon Clay</a>, 
<a href="/search/cs?searchtype=author&query=Dally%2C+B">Bill Dally</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+L">Laura Dang</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+P">Parikshit Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Dhodhi%2C+S">Siddhanth Dhodhi</a>, 
<a href="/search/cs?searchtype=author&query=Halepete%2C+S">Sameer Halepete</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+E">Eric Hill</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiashang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Sumit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Khailany%2C+B">Brucek Khailany</a>, 
<a href="/search/cs?searchtype=author&query=Kunal%2C+K">Kishor Kunal</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaowei Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Oberman%2C+S">Stuart Oberman</a>, 
<a href="/search/cs?searchtype=author&query=Omar%2C+S">Sujeet Omar</a>, 
<a href="/search/cs?searchtype=author&query=Pratty%2C+S">Sreedhar Pratty</a>, 
<a href="/search/cs?searchtype=author&query=Raiman%2C+J">Jonathan Raiman</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Ambar Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhengjiang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hanfei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Suthar%2C+P+P">Pratik P Suthar</a>, 
<a href="/search/cs?searchtype=author&query=Tej%2C+V">Varun Tej</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaizhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Haoxing Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01351" title="Abstract">arXiv:2311.01351</a> (replaced) [<a href="/pdf/2311.01351" title="Download PDF">pdf</a>, <a href="/format/2311.01351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplicial Models for the Epistemic Logic of Faulty Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goubault%2C+E">Eric Goubault</a>, 
<a href="/search/cs?searchtype=author&query=Kniazev%2C+R">Roman Kniazev</a>, 
<a href="/search/cs?searchtype=author&query=Ledent%2C+J">Jeremy Ledent</a>, 
<a href="/search/cs?searchtype=author&query=Rajsbaum%2C+S">Sergio Rajsbaum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Algebraic Topology (math.AT)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01797" title="Abstract">arXiv:2311.01797</a> (replaced) [<a href="/pdf/2311.01797" title="Download PDF">pdf</a>, <a href="/format/2311.01797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Generalization Properties of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Puheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huishuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02082" title="Abstract">arXiv:2311.02082</a> (replaced) [<a href="/pdf/2311.02082" title="Download PDF">pdf</a>, <a href="/ps/2311.02082" title="Download PostScript">ps</a>, <a href="/format/2311.02082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Modelling of Organizational Knowledge as a Basis for Enterprise  Data Governance 4.0 -- Application to a Unified Clinical Data Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+M+A">Miguel AP Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Manara%2C+S">Stephane Manara</a>, 
<a href="/search/cs?searchtype=author&query=Mol%C3%A9%2C+B">Bruno Mol&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Muller%2C+T">Thomas Muller</a>, 
<a href="/search/cs?searchtype=author&query=Guillouche%2C+A">Aur&#xe9;lien Guillouche</a>, 
<a href="/search/cs?searchtype=author&query=Hesske%2C+L">Lysann Hesske</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+B">Bruce Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Hubert%2C+G">Gilles Hubert</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+C">Chinmay Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Jagdev%2C+P">Pralipta Jagdev</a>, 
<a href="/search/cs?searchtype=author&query=Berger%2C+C+R">Cedric R. Berger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02877" title="Abstract">arXiv:2311.02877</a> (replaced) [<a href="/pdf/2311.02877" title="Download PDF">pdf</a>, <a href="/format/2311.02877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inner-IoU: More Effective Intersection over Union Loss with Auxiliary  Bounding Box
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Cong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuaijie Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03062" title="Abstract">arXiv:2311.03062</a> (replaced) [<a href="/pdf/2311.03062" title="Download PDF">pdf</a>, <a href="/ps/2311.03062" title="Download PostScript">ps</a>, <a href="/format/2311.03062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imaging through multimode fibres with physical prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zhang%2C+C">Chuncheng Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Shi%2C+Y">Yingjie Shi</a>, 
<a href="/search/physics?searchtype=author&query=Yao%2C+Z">Zheyi Yao</a>, 
<a href="/search/physics?searchtype=author&query=Sui%2C+X">Xiubao Sui</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+Q">Qian Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03402" title="Abstract">arXiv:2311.03402</a> (replaced) [<a href="/pdf/2311.03402" title="Download PDF">pdf</a>, <a href="/format/2311.03402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CycleCL: Self-supervised Learning for Periodic Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Destro%2C+M">Matteo Destro</a>, 
<a href="/search/cs?searchtype=author&query=Gygli%2C+M">Michael Gygli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04126" title="Abstract">arXiv:2311.04126</a> (replaced) [<a href="/e-print/2311.04126" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Diagram to Deployment: Translating BPMN Collaborations into X-Klaim  for Efficient Multi-Robot System Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bourr%2C+K">Khalid Bourr</a>, 
<a href="/search/cs?searchtype=author&query=Tiezzi%2C+F">Francesco Tiezzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The references section is repeated two times, and the article is not complete
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04140" title="Abstract">arXiv:2311.04140</a> (replaced) [<a href="/pdf/2311.04140" title="Download PDF">pdf</a>, <a href="/format/2311.04140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Nearly Linear-Time Distributed Algorithm for Exact Maximum Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Izumi%2C+T">Taisuke Izumi</a>, 
<a href="/search/cs?searchtype=author&query=Kitamura%2C+N">Naoki Kitamura</a>, 
<a href="/search/cs?searchtype=author&query=Yamaguchi%2C+Y">Yutaro Yamaguchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04589" title="Abstract">arXiv:2311.04589</a> (replaced) [<a href="/pdf/2311.04589" title="Download PDF">pdf</a>, <a href="/format/2311.04589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingxue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Multi-modal, Large Language Models, Tokenizer, Understanding and Generation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04980" title="Abstract">arXiv:2311.04980</a> (replaced) [<a href="/pdf/2311.04980" title="Download PDF">pdf</a>, <a href="/format/2311.04980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MaxEVA: Maximizing the Efficiency of Matrix Multiplication on Versal AI  Engine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taka%2C+E">Endri Taka</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Aman Arora</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kai-Chiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Marculescu%2C+D">Diana Marculescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as full paper at FPT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05046" title="Abstract">arXiv:2311.05046</a> (replaced) [<a href="/pdf/2311.05046" title="Download PDF">pdf</a>, <a href="/format/2311.05046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Consistency of Maximum Likelihood Estimation of Probabilistic  Principal Component Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Datta%2C+A">Arghya Datta</a>, 
<a href="/search/stat?searchtype=author&query=Chakrabarty%2C+S">Sayak Chakrabarty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure, to appear in NeurIPS 2023. Update: included minor typographical corrections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05052" title="Abstract">arXiv:2311.05052</a> (replaced) [<a href="/pdf/2311.05052" title="Download PDF">pdf</a>, <a href="/ps/2311.05052" title="Download PostScript">ps</a>, <a href="/format/2311.05052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix Completion via Memoryless Scalar Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eamaz%2C+A">Arian Eamaz</a>, 
<a href="/search/cs?searchtype=author&query=Yeganegi%2C+F">Farhang Yeganegi</a>, 
<a href="/search/cs?searchtype=author&query=Soltanalian%2C+M">Mojtaba Soltanalian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.03224">arXiv:2310.03224</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05410" title="Abstract">arXiv:2311.05410</a> (replaced) [<a href="/pdf/2311.05410" title="Download PDF">pdf</a>, <a href="/format/2311.05410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Gaussian Bounding Box Representation and Ring-Shaped Rotated  Convolution for Oriented Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunkai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Junfeng Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaoyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+F">Fengshui Jing</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Min Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05511" title="Abstract">arXiv:2311.05511</a> (replaced) [<a href="/pdf/2311.05511" title="Download PDF">pdf</a>, <a href="/format/2311.05511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anytime-Constrained Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McMahan%2C+J">Jeremy McMahan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaojin Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05802" title="Abstract">arXiv:2311.05802</a> (replaced) [<a href="/pdf/2311.05802" title="Download PDF">pdf</a>, <a href="/format/2311.05802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Modeling of Residuals for Real-Time Risk-Sensitive Safety  with Discrete-Time Control Barrier Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cosner%2C+R+K">Ryan K. Cosner</a>, 
<a href="/search/eess?searchtype=author&query=Sadalski%2C+I">Igor Sadalski</a>, 
<a href="/search/eess?searchtype=author&query=Woo%2C+J+K">Jana K. Woo</a>, 
<a href="/search/eess?searchtype=author&query=Culbertson%2C+P">Preston Culbertson</a>, 
<a href="/search/eess?searchtype=author&query=Ames%2C+A+D">Aaron D. Ames</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, submitted to the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05808" title="Abstract">arXiv:2311.05808</a> (replaced) [<a href="/pdf/2311.05808" title="Download PDF">pdf</a>, <a href="/format/2311.05808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scale-MIA: A Scalable Model Inversion Attack against Secure Federated  Learning via Latent Space Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shanghao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Ning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y+T">Y.Thomas Hou</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+W">Wenjing Lou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05915" title="Abstract">arXiv:2311.05915</a> (replaced) [<a href="/pdf/2311.05915" title="Download PDF">pdf</a>, <a href="/format/2311.05915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fake Alignment: Are LLMs Really Aligned Well?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yixu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Y">Yan Teng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kexin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+C">Chengqi Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xingjun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu-Gang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yingchun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06115" title="Abstract">arXiv:2311.06115</a> (replaced) [<a href="/pdf/2311.06115" title="Download PDF">pdf</a>, <a href="/format/2311.06115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Scalable Kernel Matrix Approximations using Hierarchical  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gaddameedi%2C+K">Keerthi Gaddameedi</a>, 
<a href="/search/math?searchtype=author&query=Reiz%2C+S">Severin Reiz</a>, 
<a href="/search/math?searchtype=author&query=Neckel%2C+T">Tobias Neckel</a>, 
<a href="/search/math?searchtype=author&query=Bungartz%2C+H">Hans-Joachim Bungartz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> * Author one and author two - Equal contribution. Accepted for IC 2023 held in conjunction with the BenchCouncil <a href="https://www.benchcouncil.org/ic2023/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06176" title="Abstract">arXiv:2311.06176</a> (replaced) [<a href="/pdf/2311.06176" title="Download PDF">pdf</a>, <a href="/format/2311.06176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Report Generation for Histopathology images using pre-trained  Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Saurav Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+D+E">Donald E. Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 09 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06315" title="Abstract">arXiv:2311.06315</a> (replaced) [<a href="/pdf/2311.06315" title="Download PDF">pdf</a>, <a href="/format/2311.06315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShipGen: A Diffusion Model for Parametric Ship Hull Generation with  Multiple Objectives and Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bagazinski%2C+N+J">Noah J. Bagazinski</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Faez Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06401" title="Abstract">arXiv:2311.06401</a> (replaced) [<a href="/pdf/2311.06401" title="Download PDF">pdf</a>, <a href="/format/2311.06401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autoregressive Language Models For Estimating the Entropy of Epic EHR  Audit Logs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Warner%2C+B+C">Benjamin C. Warner</a>, 
<a href="/search/cs?searchtype=author&query=Kannampallil%2C+T">Thomas Kannampallil</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seunghwan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06419" title="Abstract">arXiv:2311.06419</a> (replaced) [<a href="/pdf/2311.06419" title="Download PDF">pdf</a>, <a href="/format/2311.06419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Energy Saving Opportunities in Fault Tolerant HPC Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moran%2C+M">Marina Moran</a>, 
<a href="/search/cs?searchtype=author&query=Balladini%2C+J">Javier Balladini</a>, 
<a href="/search/cs?searchtype=author&query=Rexachs%2C+D">Dolores Rexachs</a>, 
<a href="/search/cs?searchtype=author&query=Rucci%2C+E">Enzo Rucci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the accepted version of the manuscript that was sent to review to Journal of Parallel and Distributed Computing (ISSN 1096-0848). arXiv admin note: text overlap with <a href="/abs/2012.11396">arXiv:2012.11396</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06429" title="Abstract">arXiv:2311.06429</a> (replaced) [<a href="/pdf/2311.06429" title="Download PDF">pdf</a>, <a href="/ps/2311.06429" title="Download PostScript">ps</a>, <a href="/format/2311.06429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Load Altering Attacks on Distribution Systems with ZIP  Loads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Maleki%2C+S">Sajjad Maleki</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+S">Shijie Pan</a>, 
<a href="/search/eess?searchtype=author&query=Belmega%2C+E+V">E. Veronica Belmega</a>, 
<a href="/search/eess?searchtype=author&query=Konstantinou%2C+C">Charalambos Konstantinou</a>, 
<a href="/search/eess?searchtype=author&query=Lakshminarayana%2C+S">Subhash Lakshminarayana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06433" title="Abstract">arXiv:2311.06433</a> (replaced) [<a href="/pdf/2311.06433" title="Download PDF">pdf</a>, <a href="/ps/2311.06433" title="Download PostScript">ps</a>, <a href="/format/2311.06433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret-Optimal Control under Partial Observability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hajar%2C+J">Joudi Hajar</a>, 
<a href="/search/eess?searchtype=author&query=Sabag%2C+O">Oron Sabag</a>, 
<a href="/search/eess?searchtype=author&query=Hassibi%2C+B">Babak Hassibi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06453" title="Abstract">arXiv:2311.06453</a> (replaced) [<a href="/pdf/2311.06453" title="Download PDF">pdf</a>, <a href="/format/2311.06453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DocGen: Generating Detailed Parameter Docstrings in Python
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venkatkrishna%2C+V">Vatsal Venkatkrishna</a>, 
<a href="/search/cs?searchtype=author&query=Nagabushanam%2C+D+S">Durga Shree Nagabushanam</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+E+I">Emmanuel Iko-Ojo Simon</a>, 
<a href="/search/cs?searchtype=author&query=Vidoni%2C+M">Melina Vidoni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06477" title="Abstract">arXiv:2311.06477</a> (replaced) [<a href="/pdf/2311.06477" title="Download PDF">pdf</a>, <a href="/format/2311.06477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Report of the 1st Workshop on Generative AI and Law
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cooper%2C+A+F">A. Feder Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Katherine Lee</a>, 
<a href="/search/cs?searchtype=author&query=Grimmelmann%2C+J">James Grimmelmann</a>, 
<a href="/search/cs?searchtype=author&query=Ippolito%2C+D">Daphne Ippolito</a>, 
<a href="/search/cs?searchtype=author&query=Callison-Burch%2C+C">Christopher Callison-Burch</a>, 
<a href="/search/cs?searchtype=author&query=Choquette-Choo%2C+C+A">Christopher A. Choquette-Choo</a>, 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+N">Niloofar Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Brundage%2C+M">Miles Brundage</a>, 
<a href="/search/cs?searchtype=author&query=Mimno%2C+D">David Mimno</a>, 
<a href="/search/cs?searchtype=author&query=Choksi%2C+M+Z">Madiha Zahrah Choksi</a>, 
<a href="/search/cs?searchtype=author&query=Balkin%2C+J+M">Jack M. Balkin</a>, 
<a href="/search/cs?searchtype=author&query=Carlini%2C+N">Nicholas Carlini</a>, 
<a href="/search/cs?searchtype=author&query=De+Sa%2C+C">Christopher De Sa</a>, 
<a href="/search/cs?searchtype=author&query=Frankle%2C+J">Jonathan Frankle</a>, 
<a href="/search/cs?searchtype=author&query=Ganguli%2C+D">Deep Ganguli</a>, 
<a href="/search/cs?searchtype=author&query=Gipson%2C+B">Bryant Gipson</a>, 
<a href="/search/cs?searchtype=author&query=Guadamuz%2C+A">Andres Guadamuz</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+S+L">Swee Leng Harris</a>, 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+A+Z">Abigail Z. Jacobs</a>, 
<a href="/search/cs?searchtype=author&query=Joh%2C+E">Elizabeth Joh</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+G">Gautam Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Lemley%2C+M">Mark Lemley</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+C">Cass Matthews</a>, 
<a href="/search/cs?searchtype=author&query=McLeavey%2C+C">Christine McLeavey</a>, 
<a href="/search/cs?searchtype=author&query=McSherry%2C+C">Corynne McSherry</a>, 
<a href="/search/cs?searchtype=author&query=Nasr%2C+M">Milad Nasr</a>, 
<a href="/search/cs?searchtype=author&query=Ohm%2C+P">Paul Ohm</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+A">Adam Roberts</a>, 
<a href="/search/cs?searchtype=author&query=Rubin%2C+T">Tom Rubin</a>, 
<a href="/search/cs?searchtype=author&query=Samuelson%2C+P">Pamela Samuelson</a>, 
<a href="/search/cs?searchtype=author&query=Schubert%2C+L">Ludwig Schubert</a>, 
<a href="/search/cs?searchtype=author&query=Vaccaro%2C+K">Kristen Vaccaro</a>, 
<a href="/search/cs?searchtype=author&query=Villa%2C+L">Luis Villa</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Felix Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zeide%2C+E">Elana Zeide</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06513" title="Abstract">arXiv:2311.06513</a> (replaced) [<a href="/pdf/2311.06513" title="Download PDF">pdf</a>, <a href="/format/2311.06513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Step by Step to Fairness: Attributing Societal Bias in Task-oriented  Dialogue Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hsuan Su</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+R">Rebecca Qian</a>, 
<a href="/search/cs?searchtype=author&query=Sankar%2C+C">Chinnadhurai Sankar</a>, 
<a href="/search/cs?searchtype=author&query=Shayandeh%2C+S">Shahin Shayandeh</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shang-Tse Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>, 
<a href="/search/cs?searchtype=author&query=Bikel%2C+D+M">Daniel M. Bikel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06573" title="Abstract">arXiv:2311.06573</a> (replaced) [<a href="/pdf/2311.06573" title="Download PDF">pdf</a>, <a href="/format/2311.06573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generalized Space-Efficient Algorithm for Quantum Bit String  Comparators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Shahzad%2C+K">Khuram Shahzad</a>, 
<a href="/search/quant-ph?searchtype=author&query=Khan%2C+O+U">Omar Usman Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06595" title="Abstract">arXiv:2311.06595</a> (replaced) [<a href="/pdf/2311.06595" title="Download PDF">pdf</a>, <a href="/format/2311.06595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Classification to Generation: Insights into Crosslingual Retrieval  Augmented ICL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoqian Li</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+E">Ercong Nie</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Sheng Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In The Workshop on Instruction Tuning and Instruction Following, held in conjunction with The Conference on NeurIPS 2023, December 2023. arXiv admin note: text overlap with <a href="/abs/2311.00587">arXiv:2311.00587</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06703" title="Abstract">arXiv:2311.06703</a> (replaced) [<a href="/pdf/2311.06703" title="Download PDF">pdf</a>, <a href="/ps/2311.06703" title="Download PostScript">ps</a>, <a href="/format/2311.06703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Human-Centered AI: A Methodological Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zaifeng Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06849" title="Abstract">arXiv:2311.06849</a> (replaced) [<a href="/pdf/2311.06849" title="Download PDF">pdf</a>, <a href="/ps/2311.06849" title="Download PostScript">ps</a>, <a href="/format/2311.06849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytic regularity for a singularly perturbed fourth order  reaction-diffusion boundary value problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Constantinou%2C+P">P. Constantinou</a>, 
<a href="/search/math?searchtype=author&query=Xenophontos%2C+C">C. Xenophontos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1901.09397">arXiv:1901.09397</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06889" title="Abstract">arXiv:2311.06889</a> (replaced) [<a href="/pdf/2311.06889" title="Download PDF">pdf</a>, <a href="/format/2311.06889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Programmatic Strategy Synthesis: Resolving Nondeterminism in  Probabilistic Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Batz%2C+K">Kevin Batz</a>, 
<a href="/search/cs?searchtype=author&query=Biskup%2C+T+J">Tom Jannik Biskup</a>, 
<a href="/search/cs?searchtype=author&query=Katoen%2C+J">Joost-Pieter Katoen</a>, 
<a href="/search/cs?searchtype=author&query=Winkler%2C+T">Tobias Winkler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07089" title="Abstract">arXiv:2311.07089</a> (replaced) [<a href="/pdf/2311.07089" title="Download PDF">pdf</a>, <a href="/ps/2311.07089" title="Download PostScript">ps</a>, <a href="/format/2311.07089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recursive and non-recursive filters for sequential smoothing and  prediction with instantaneous phase and frequency estimation applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kennedy%2C+H+L">Hugh Lachlan Kennedy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added 3 more refs. Added extra para to discussion on generic low-pass filtering applications. Added arXiv ID to header
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07093" title="Abstract">arXiv:2311.07093</a> (replaced) [<a href="/pdf/2311.07093" title="Download PDF">pdf</a>, <a href="/format/2311.07093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Effectiveness of ASR Representations in Real-world Noisy Speech  Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaohan Shi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiajun He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingfeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Toda%2C+T">Tomoki Toda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07247" title="Abstract">arXiv:2311.07247</a> (replaced) [<a href="/pdf/2311.07247" title="Download PDF">pdf</a>, <a href="/format/2311.07247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneous Clutter Detection and Semantic Segmentation of Moving  Objects for Automotive Radar Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kopp%2C+J">Johannes Kopp</a>, 
<a href="/search/cs?searchtype=author&query=Kellner%2C+D">Dominik Kellner</a>, 
<a href="/search/cs?searchtype=author&query=Piroli%2C+A">Aldi Piroli</a>, 
<a href="/search/cs?searchtype=author&query=Dallabetta%2C+V">Vinzenz Dallabetta</a>, 
<a href="/search/cs?searchtype=author&query=Dietmayer%2C+K">Klaus Dietmayer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at IEEE International Conference on Intelligent Transportation Systems (ITSC), Bilbao, ESP, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07344" title="Abstract">arXiv:2311.07344</a> (replaced) [<a href="/pdf/2311.07344" title="Download PDF">pdf</a>, <a href="/format/2311.07344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Missing Value Imputation for Multi-attribute Sensor Data Streams via  Message Propagation (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hua Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+C+S">Christian S. Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+V">Varun Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Markl%2C+V">Volker Markl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at VLDB 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07362" title="Abstract">arXiv:2311.07362</a> (replaced) [<a href="/pdf/2311.07362" title="Download PDF">pdf</a>, <a href="/format/2311.07362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Volcano: Mitigating Multimodal Hallucination through Self-Feedback  Guided Revision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seongyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S+H">Sue Hyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+Y">Yongrae Jo</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07439" title="Abstract">arXiv:2311.07439</a> (replaced) [<a href="/pdf/2311.07439" title="Download PDF">pdf</a>, <a href="/format/2311.07439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Multi-Pivot Ensembling with Massively Multilingual Machine  Translation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadshahi%2C+A">Alireza Mohammadshahi</a>, 
<a href="/search/cs?searchtype=author&query=Vamvas%2C+J">Jannis Vamvas</a>, 
<a href="/search/cs?searchtype=author&query=Sennrich%2C+R">Rico Sennrich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07534" title="Abstract">arXiv:2311.07534</a> (replaced) [<a href="/pdf/2311.07534" title="Download PDF">pdf</a>, <a href="/format/2311.07534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Musical Object Discovery from Audio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gha%2C+J">Joonsu Gha</a>, 
<a href="/search/cs?searchtype=author&query=Herrmann%2C+V">Vincent Herrmann</a>, 
<a href="/search/cs?searchtype=author&query=Grewe%2C+B">Benjamin Grewe</a>, 
<a href="/search/cs?searchtype=author&query=Schmidhuber%2C+J">J&#xfc;rgen Schmidhuber</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+A">Anand Gopalakrishnan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Machine Learning for Audio Workshop, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item360">Cross-lists</a></li>
<li><a href="#item408">Replacements</a></li>
</ul>
<small>[ total of 625 entries:  <b>1-625</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2311">2311</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
