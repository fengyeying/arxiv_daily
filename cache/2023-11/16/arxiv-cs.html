<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Tue 14 Nov 23  to  Wed 15 Nov 23, announced Thu, 16 Nov 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item364">Cross-lists</a></li>
<li><a href="#item422">Replacements</a></li>
</ul>
<small>[ total of 616 entries:  <b>1-616</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Thu, 16 Nov 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08409" title="Abstract">arXiv:2311.08409</a> [<a href="/pdf/2311.08409" title="Download PDF">pdf</a>, <a href="/ps/2311.08409" title="Download PostScript">ps</a>, <a href="/format/2311.08409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Whole-Body Task Space Control for Humanoid Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paredes%2C+V">Victor Paredes</a>, 
<a href="/search/cs?searchtype=author&query=Hereid%2C+A">Ayonga Hereid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Complex robotic systems require whole-body controllers to deal with contact
interactions, handle closed kinematic chains, and track task-space control
objectives. However, for many applications, safety-critical controllers are
important to steer away from undesired robot configurations to prevent unsafe
behaviors. A prime example is legged robotics, where we can have tasks such as
balance control, regulation of torso orientation, and, most importantly,
walking. As the coordination of multi-body systems is non-trivial, following a
combination of those tasks might lead to configurations that are deemed
dangerous, such as stepping on its support foot during walking, leaning the
torso excessively, or producing excessive centroidal momentum, resulting in
non-human-like walking. To address these challenges, we propose a formulation
of an inverse dynamics control enhanced with exponential control barrier
functions for robotic systems with numerous degrees of freedom. Our approach
utilizes a quadratic program that respects closed kinematic chains, minimizes
the control objectives, and imposes desired constraints on the Zero Moment
Point, friction cone, and torque. More importantly, it also ensures the forward
invariance of a general user-defined high Relative-Degree safety set. We
demonstrate the effectiveness of our method by applying it to the 3D biped
robot Digit, both in simulation and with hardware experiments.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08410" title="Abstract">arXiv:2311.08410</a> [<a href="/pdf/2311.08410" title="Download PDF">pdf</a>, <a href="/format/2311.08410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploration of the Assessment for AVP Algorithm Training in Underground  Parking Garages Simulation Scenario
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">The autonomous valet parking (AVP) functionality in self-driving vehicles is
currently capable of handling most simple parking tasks. However, further
training is necessary to enable the AVP algorithm to adapt to complex scenarios
and complete parking tasks in any given situation. Training algorithms with
real-world data is time-consuming and labour-intensive, and the current state
of constructing simulation environments is predominantly manual. This paper
introduces an approach to automatically generate 3D underground garage
simulation scenarios of varying difficulty levels based on pre-input 2D
underground parking structure plans.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08412" title="Abstract">arXiv:2311.08412</a> [<a href="/pdf/2311.08412" title="Download PDF">pdf</a>, <a href="/format/2311.08412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Large Language Models as a Source of Common-Sense Knowledge  for Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ocker%2C+F">Felix Ocker</a>, 
<a href="/search/cs?searchtype=author&query=Deigm%C3%B6ller%2C+J">J&#xf6;rg Deigm&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Eggert%2C+J">Julian Eggert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ISWC 2023 Posters and Demos: 22nd International Semantic Web Conference, November 6-10, 2023, Athens, Greece
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Service robots need common-sense knowledge to help humans in everyday
situations as it enables them to understand the context of their actions.
However, approaches that use ontologies face a challenge because common-sense
knowledge is often implicit, i.e., it is obvious to humans but not explicitly
stated. This paper investigates if Large Language Models (LLMs) can fill this
gap. Our experiments reveal limited effectiveness in the selective extraction
of contextual action knowledge, suggesting that LLMs may not be sufficient on
their own. However, the large-scale extraction of general, actionable knowledge
shows potential, indicating that LLMs can be a suitable tool for efficiently
creating ontologies for robots. This paper shows that the technique used for
knowledge extraction can be applied to populate a minimalist ontology,
showcasing the potential of LLMs in synergy with formal knowledge
representation.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08413" title="Abstract">arXiv:2311.08413</a> [<a href="/pdf/2311.08413" title="Download PDF">pdf</a>, <a href="/format/2311.08413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Safety Shell: an Architecture to Handle Functional Insufficiencies  in Automated Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanselaar%2C+C+A+J">C. A. J. Hanselaar</a>, 
<a href="/search/cs?searchtype=author&query=Silvas%2C+E">E. Silvas</a>, 
<a href="/search/cs?searchtype=author&query=Terechko%2C+A">A. Terechko</a>, 
<a href="/search/cs?searchtype=author&query=Heemels%2C+W+P+M+H">W. P. M. H. Heemels</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 23 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">To enable highly automated vehicles where the driver is no longer a safety
backup, the vehicle must deal with various Functional Insufficiencies (FIs).
Thus-far, there is no widely accepted functional architecture that maximizes
the availability of autonomy and ensures safety in complex vehicle operational
design domains. In this paper, we present a survey of existing methods that
strive to prevent or handle FIs. We observe that current design-time methods of
preventing FIs lack completeness guarantees. Complementary solutions for
on-line handling cannot suitably increase safety without seriously impacting
availability of journey continuing autonomous functionality. To fill this gap,
we propose the Safety Shell, a scalable multi-channel architecture and
arbitration design, built upon preexisting functional safety redundant channel
architectures. We compare this novel approach to existing architectures using
numerical case studies. The results show that the Safety Shell architecture
allows the automated vehicle to be as safe or safer compared to alternatives,
while simultaneously improving availability of vehicle autonomy, thereby
increasing the possible coverage of on-line functional insufficiency handling.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08422" title="Abstract">arXiv:2311.08422</a> [<a href="/pdf/2311.08422" title="Download PDF">pdf</a>, <a href="/ps/2311.08422" title="Download PostScript">ps</a>, <a href="/format/2311.08422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> k-Parameter Approach for False In-Season Anomaly Suppression in Daily  Time Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zha%2C+V+Y">Vincent Yuansang Zha</a>, 
<a href="/search/cs?searchtype=author&query=Kommaraju%2C+V">Vaishnavi Kommaraju</a>, 
<a href="/search/cs?searchtype=author&query=Obi-Njoku%2C+O">Okenna Obi-Njoku</a>, 
<a href="/search/cs?searchtype=author&query=Dakshinamoorthy%2C+V">Vijay Dakshinamoorthy</a>, 
<a href="/search/cs?searchtype=author&query=Agnihotri%2C+A">Anirudh Agnihotri</a>, 
<a href="/search/cs?searchtype=author&query=Kirsten%2C+N">Nantes Kirsten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Detecting anomalies in a daily time series with a weekly pattern is a common
task with a wide range of applications. A typical way of performing the task is
by using decomposition method. However, the method often generates false
positive results where a data point falls within its weekly range but is just
off from its weekday position. We refer to this type of anomalies as "in-season
anomalies", and propose a k-parameter approach to address the issue. The
approach provides configurable extra tolerance for in-season anomalies to
suppress misleading alerts while preserving real positives. It yields favorable
result.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08424" title="Abstract">arXiv:2311.08424</a> [<a href="/pdf/2311.08424" title="Download PDF">pdf</a>, <a href="/format/2311.08424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Multi-Programming-Language Commits and Their Impacts on  Software Quality: An Empirical Study on Apache Projects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zengyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaoxiao Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qinyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Peng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+R">Ran Mo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chen Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint accepted for publication in Journal of Systems and Software, 2022. arXiv admin note: substantial text overlap with <a href="/abs/2103.11691">arXiv:2103.11691</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: Modern software systems (e.g., Apache Spark) are usually written in
multiple programming languages (PLs). There is little understanding on the
phenomenon of multi-programming-language commits (MPLCs), which involve
modified source files written in multiple PLs. Objective: This work aims to
explore MPLCs and their impacts on development difficulty and software quality.
Methods: We performed an empirical study on eighteen non-trivial Apache
projects with 197,566 commits. Results: (1) the most commonly used PL
combination consists of all the four PLs, i.e., C/C++, Java, JavaScript, and
Python; (2) 9% of the commits from all the projects are MPLCs, and the
proportion of MPLCs in 83% of the projects goes to a relatively stable level;
(3) more than 90% of the MPLCs from all the projects involve source files in
two PLs; (4) the change complexity of MPLCs is significantly higher than that
of non-MPLCs; (5) issues fixed in MPLCs take significantly longer to be
resolved than issues fixed in non-MPLCs in 89% of the projects; (6) MPLCs do
not show significant effects on issue reopen; (7) source files undergoing MPLCs
tend to be more bug-prone; and (8) MPLCs introduce more bugs than non-MPLCs.
Conclusions: MPLCs are related to increased development difficulty and
decreased software quality.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08425" title="Abstract">arXiv:2311.08425</a> [<a href="/pdf/2311.08425" title="Download PDF">pdf</a>, <a href="/ps/2311.08425" title="Download PostScript">ps</a>, <a href="/format/2311.08425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Research and experimental verification on low-frequency long-range  underwater sound propagation dispersion characteristics under dual-channel  sound speed profiles in the Chukchi Plateau
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+J">Jinbao Weng</a> (1), 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yubo Qi</a> (2), 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yanming Yang</a> (1), 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hongtao Wen</a> (1), 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongtao Zhou</a> (1), 
<a href="/search/cs?searchtype=author&query=Xue%2C+R">Ruichao Xue</a> (1) ((1) Laboratory of Ocean acoustics and Remote Sensing, Institute of Oceanography, Ministry of Natural Resources, Xiamen, Fujian, China, (2) State key laboratory of acoustics, Institute of Acoustics, Chinese Academy of Sciences, Beijing, China)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Numerical Analysis (math.NA); Atmospheric and Oceanic Physics (physics.ao-ph); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">The dual-channel sound speed profiles of the Chukchi Plateau and the Canadian
Basin have become current research hotspots due to their excellent
low-frequency sound signal propagation ability. Previous research has mainly
focused on using sound propagation theory to explain the changes in sound
signal energy. This article is mainly based on the theory of normal modes to
study the fine structure of low-frequency wide-band sound propagation
dispersion under dual-channel sound speed profiles. In this paper, the problem
of the intersection of normal mode dispersion curves caused by the dual-channel
sound speed profile (SSP) has been explained, the blocking effect of seabed
terrain changes on dispersion structures has been analyzed, and the normal
modes has been separated by using modified warping operator. The above research
results have been verified through a long-range seismic exploration experiment
at the Chukchi Plateau. At the same time, based on the acoustic signal
characteristics in this environment, two methods for estimating the distance of
sound sources have been proposed, and the experiment data at sea has also
verified these two methods.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08427" title="Abstract">arXiv:2311.08427</a> [<a href="/pdf/2311.08427" title="Download PDF">pdf</a>, <a href="/format/2311.08427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Transportable Causal Network Model Based on Observational  Healthcare Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernasconi%2C+A">Alice Bernasconi</a>, 
<a href="/search/cs?searchtype=author&query=Zanga%2C+A">Alessio Zanga</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+P+J+F">Peter J.F. Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Stella%2C+M+S+F">Marco Scutari Fabio Stella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
<p class="mathjax">Over the last decades, many prognostic models based on artificial
intelligence techniques have been used to provide detailed predictions in
healthcare. Unfortunately, the real-world observational data used to train and
validate these models are almost always affected by biases that can strongly
impact the outcomes validity: two examples are values missing not-at-random and
selection bias. Addressing them is a key element in achieving transportability
and in studying the causal relationships that are critical in clinical decision
making, going beyond simpler statistical approaches based on probabilistic
association.
<br />In this context, we propose a novel approach that combines selection
diagrams, missingness graphs, causal discovery and prior knowledge into a
single graphical model to estimate the cardiovascular risk of adolescent and
young females who survived breast cancer. We learn this model from data
comprising two different cohorts of patients. The resulting causal network
model is validated by expert clinicians in terms of risk assessment, accuracy
and explainability, and provides a prognostic model that outperforms competing
machine learning methods.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08429" title="Abstract">arXiv:2311.08429</a> [<a href="/pdf/2311.08429" title="Download PDF">pdf</a>, <a href="/format/2311.08429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Purpose in the Machine: Do Traffic Simulators Produce Distributionally  Equivalent Outcomes for Reinforcement Learning Applications?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rex Chen</a>, 
<a href="/search/cs?searchtype=author&query=Carley%2C+K+M">Kathleen M. Carley</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Sadeh%2C+N">Norman Sadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages; accepted version, published at the 2023 Winter Simulation Conference (WSC '23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Traffic simulators are used to generate data for learning in intelligent
transportation systems (ITSs). A key question is to what extent their modelling
assumptions affect the capabilities of ITSs to adapt to various scenarios when
deployed in the real world. This work focuses on two simulators commonly used
to train reinforcement learning (RL) agents for traffic applications, CityFlow
and SUMO. A controlled virtual experiment varying driver behavior and
simulation scale finds evidence against distributional equivalence in
RL-relevant measures from these simulators, with the root mean squared error
and KL divergence being significantly greater than 0 for all assessed measures.
While granular real-world validation generally remains infeasible, these
findings suggest that traffic simulators are not a deus ex machina for RL
training: understanding the impacts of inter-simulator differences is necessary
to train and deploy RL-based ITSs.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08430" title="Abstract">arXiv:2311.08430</a> [<a href="/pdf/2311.08430" title="Download PDF">pdf</a>, <a href="/format/2311.08430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rankitect: Ranking Architecture Search Battling World-class Engineers at  Meta Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wei Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kuang-Hung Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fedorov%2C+I">Igor Fedorov</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+W">Weiwei Chu</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+K">Kaveh Hassani</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mengying Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Buyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Dehua Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Guang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+F">Fangqiu Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiyan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yuchen Hao</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+L">Liang Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen-Yen Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Wei Wen and Kuang-Hung Liu contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Neural Architecture Search (NAS) has demonstrated its efficacy in computer
vision and potential for ranking systems. However, prior work focused on
academic problems, which are evaluated at small scale under well-controlled
fixed baselines. In industry system, such as ranking system in Meta, it is
unclear whether NAS algorithms from the literature can outperform production
baselines because of: (1) scale - Meta ranking systems serve billions of users,
(2) strong baselines - the baselines are production models optimized by
hundreds to thousands of world-class engineers for years since the rise of deep
learning, (3) dynamic baselines - engineers may have established new and
stronger baselines during NAS search, and (4) efficiency - the search pipeline
must yield results quickly in alignment with the productionization life cycle.
In this paper, we present Rankitect, a NAS software framework for ranking
systems at Meta. Rankitect seeks to build brand new architectures by composing
low level building blocks from scratch. Rankitect implements and improves
state-of-the-art (SOTA) NAS methods for comprehensive and fair comparison under
the same search space, including sampling-based NAS, one-shot NAS, and
Differentiable NAS (DNAS). We evaluate Rankitect by comparing to multiple
production ranking models at Meta. We find that Rankitect can discover new
models from scratch achieving competitive tradeoff between Normalized Entropy
loss and FLOPs. When utilizing search space designed by engineers, Rankitect
can generate better models than engineers, achieving positive offline
evaluation and online A/B test at Meta scale.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08431" title="Abstract">arXiv:2311.08431</a> [<a href="/pdf/2311.08431" title="Download PDF">pdf</a>, <a href="/ps/2311.08431" title="Download PostScript">ps</a>, <a href="/format/2311.08431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assuring the emotional and cultural intelligence of intelligent software  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belle%2C+A+B">Alvine B. Belle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Intelligent software systems (e.g., conversational agents, profiling systems,
hiring systems) are often designed in a manner which may perpetuates anti-Black
racism and other forms of socio-cultural discrimination. This may reinforce
social inequities by supporting the automation of consequential and sometimes
unfair decisions that may be made by such systems and which may have an adverse
impact on credit scores, insurance payouts, and even health evaluations, just
to name a few. My lightning talk will therefore emphasize the need to propose a
new type of non-functional requirements called ECI (emotional and cultural
intelligence) requirements that will aim at developing discrimination-aware
intelligent software systems. Such systems will notably be able to behave
empathetically toward everyone, including minoritized groups and will ensure
they are treated fairly. My talk will also emphasize the need to develop novel
system assurance solutions to assure these ECI requirements are sufficiently
supported by intelligent software systems.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08434" title="Abstract">arXiv:2311.08434</a> [<a href="/pdf/2311.08434" title="Download PDF">pdf</a>, <a href="/format/2311.08434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uplift Modeling based on Graph Neural Network Combined with Causal  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xinyan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yangze Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Longhan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jing Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Uplift modeling is a fundamental component of marketing effect modeling,
which is commonly employed to evaluate the effects of treatments on outcomes.
Through uplift modeling, we can identify the treatment with the greatest
benefit. On the other side, we can identify clients who are likely to make
favorable decisions in response to a certain treatment. In the past, uplift
modeling approaches relied heavily on the difference-in-difference (DID)
architecture, paired with a machine learning model as the estimation learner,
while neglecting the link and confidential information between features. We
proposed a framework based on graph neural networks that combine causal
knowledge with an estimate of uplift value. Firstly, we presented a causal
representation technique based on CATE (conditional average treatment effect)
estimation and adjacency matrix structure learning. Secondly, we suggested a
more scalable uplift modeling framework based on graph convolution networks for
combining causal knowledge. Our findings demonstrate that this method works
effectively for predicting uplift values, with small errors in typical
simulated data, and its effectiveness has been verified in actual industry
marketing data.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08438" title="Abstract">arXiv:2311.08438</a> [<a href="/pdf/2311.08438" title="Download PDF">pdf</a>, <a href="/format/2311.08438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LocaliseBot: Multi-view 3D object localisation with differentiable  rendering for robot grasping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vijayaraghavan%2C+S">Sujal Vijayaraghavan</a>, 
<a href="/search/cs?searchtype=author&query=Alqasemi%2C+R">Redwan Alqasemi</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+R">Rajiv Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Sudeep Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Robot grasp typically follows five stages: object detection, object
localisation, object pose estimation, grasp pose estimation, and grasp
planning. We focus on object pose estimation. Our approach relies on three
pieces of information: multiple views of the object, the camera's extrinsic
parameters at those viewpoints, and 3D CAD models of objects. The first step
involves a standard deep learning backbone (FCN ResNet) to estimate the object
label, semantic segmentation, and a coarse estimate of the object pose with
respect to the camera. Our novelty is using a refinement module that starts
from the coarse pose estimate and refines it by optimisation through
differentiable rendering. This is a purely vision-based approach that avoids
the need for other information such as point cloud or depth images. We evaluate
our object pose estimation approach on the ShapeNet dataset and show
improvements over the state of the art. We also show that the estimated object
pose results in 99.65% grasp accuracy with the ground truth grasp candidates on
the Object Clutter Indoor Dataset (OCID) Grasp dataset, as computed using
standard practice.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08441" title="Abstract">arXiv:2311.08441</a> [<a href="/pdf/2311.08441" title="Download PDF">pdf</a>, <a href="/format/2311.08441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MasterRTL: A Pre-Synthesis PPA Estimation Framework for Any RTL Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+W">Wenji Fang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Ceyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wills%2C+L+W">Lisa Wu Wills</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhiyao Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the Proceedings of 42nd IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">In modern VLSI design flow, the register-transfer level (RTL) stage is a
critical point, where designers define precise design behavior with hardware
description languages (HDLs) like Verilog. Since the RTL design is in the
format of HDL code, the standard way to evaluate its quality requires
time-consuming subsequent synthesis steps with EDA tools. This time-consuming
process significantly impedes design optimization at the early RTL stage.
Despite the emergence of some recent ML-based solutions, they fail to maintain
high accuracy for any given RTL design. In this work, we propose an innovative
pre-synthesis PPA estimation framework named MasterRTL. It first converts the
HDL code to a new bit-level design representation named the simple operator
graph (SOG). By only adopting single-bit simple operators, this SOG proves to
be a general representation that unifies different design types and styles. The
SOG is also more similar to the target gate-level netlist, reducing the gap
between RTL representation and netlist. In addition to the new SOG
representation, MasterRTL proposes new ML methods for the RTL-stage modeling of
timing, power, and area separately. Compared with state-of-the-art solutions,
the experiment on a comprehensive dataset with 90 different designs shows
accuracy improvement by 0.33, 0.22, and 0.15 in correlation for total negative
slack (TNS), worst negative slack (WNS), and power, respectively.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08469" title="Abstract">arXiv:2311.08469</a> [<a href="/pdf/2311.08469" title="Download PDF">pdf</a>, <a href="/format/2311.08469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenting Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+J+T">Justin T Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J+D">Jena D. Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Brahman%2C+F">Faeze Brahman</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sanjiban Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X+L">Xiang Lorraine Li</a>, 
<a href="/search/cs?searchtype=author&query=Suhr%2C+A">Alane Suhr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language technologies that accurately model the dynamics of events must
perform commonsense reasoning. Existing work evaluating commonsense reasoning
focuses on making inferences about common, everyday situations. To instead
investigate the ability to model unusual, unexpected, and unlikely situations,
we explore the task of uncommonsense abductive reasoning. Given a piece of
context with an unexpected outcome, this task requires reasoning abductively to
generate a natural language explanation that makes the unexpected outcome more
likely in the context. To this end, we curate and release a new English
language corpus called UNcommonsense. We characterize the differences between
the performance of human explainers and the best performing large language
models, finding that model-enhanced human-written explanations achieve the
highest quality by trading off between specificity and diversity. Finally, we
experiment with several online imitation learning algorithms to train open and
accessible language models on this task. When compared with the vanilla
supervised fine-tuning approach, these methods consistently reduce lose rates
on both common and uncommonsense abductive reasoning judged by human
evaluators.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08472" title="Abstract">arXiv:2311.08472</a> [<a href="/pdf/2311.08472" title="Download PDF">pdf</a>, <a href="/format/2311.08472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selecting Shots for Demographic Fairness in Few-Shot Learning with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aguirre%2C+C">Carlos Aguirre</a>, 
<a href="/search/cs?searchtype=author&query=Sasse%2C+K">Kuleen Sasse</a>, 
<a href="/search/cs?searchtype=author&query=Cachola%2C+I">Isabel Cachola</a>, 
<a href="/search/cs?searchtype=author&query=Dredze%2C+M">Mark Dredze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, work in NLP has shifted to few-shot (in-context) learning, with
large language models (LLMs) performing well across a range of tasks. However,
while fairness evaluations have become a standard for supervised methods,
little is known about the fairness of LLMs as prediction systems. Further,
common standard methods for fairness involve access to models weights or are
applied during finetuning, which are not applicable in few-shot learning. Do
LLMs exhibit prediction biases when used for standard NLP tasks? In this work,
we explore the effect of shots, which directly affect the performance of
models, on the fairness of LLMs as NLP classification systems. We consider how
different shot selection strategies, both existing and new demographically
sensitive methods, affect model fairness across three standard fairness
datasets. We discuss how future work can include LLM fairness evaluations.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08473" title="Abstract">arXiv:2311.08473</a> [<a href="/pdf/2311.08473" title="Download PDF">pdf</a>, <a href="/format/2311.08473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time topology optimization via learnable mappings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garayalde%2C+G">Gabriel Garayalde</a>, 
<a href="/search/cs?searchtype=author&query=Torzoni%2C+M">Matteo Torzoni</a>, 
<a href="/search/cs?searchtype=author&query=Bruggi%2C+M">Matteo Bruggi</a>, 
<a href="/search/cs?searchtype=author&query=Corigliano%2C+A">Alberto Corigliano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">In traditional topology optimization, the computing time required to
iteratively update the material distribution within a design domain strongly
depends on the complexity or size of the problem, limiting its application in
real engineering contexts. This work proposes a multi-stage machine learning
strategy that aims to predict an optimal topology and the related stress fields
of interest, either in 2D or 3D, without resorting to any iterative analysis
and design process. The overall topology optimization is treated as regression
task in a low-dimensional latent space, that encodes the variability of the
target designs. First, a fully-connected model is employed to surrogate the
functional link between the parametric input space characterizing the design
problem and the latent space representation of the corresponding optimal
topology. The decoder branch of an autoencoder is then exploited to reconstruct
the desired optimal topology from its latent representation. The deep learning
models are trained on a dataset generated through a standard method of topology
optimization implementing the solid isotropic material with penalization, for
varying boundary and loading conditions. The underlying hypothesis behind the
proposed strategy is that optimal topologies share enough common patterns to be
compressed into small latent space representations without significant
information loss. Results relevant to a 2D Messerschmitt-B\"olkow-Blohm beam
and a 3D bridge case demonstrate the capabilities of the proposed framework to
provide accurate optimal topology predictions in a fraction of a second.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08478" title="Abstract">arXiv:2311.08478</a> [<a href="/pdf/2311.08478" title="Download PDF">pdf</a>, <a href="/format/2311.08478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduction of large-scale RLCk models via low-rank balanced truncation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Giamouzis%2C+C">Christos Giamouzis</a>, 
<a href="/search/math?searchtype=author&query=Garyfallou%2C+D">Dimitrios Garyfallou</a>, 
<a href="/search/math?searchtype=author&query=Vagenas%2C+A">Anastasis Vagenas</a>, 
<a href="/search/math?searchtype=author&query=Evmorfopoulos%2C+N">Nestor Evmorfopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Hardware Architecture (cs.AR); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Model order reduction (MOR) is an important step in the design process of
integrated circuits. Specifically, the electromagnetic models extracted from
modern complex designs result in a large number of passive elements that
introduce limitations in the simulation process. MOR techniques based on
balanced truncation (BT) can overcome these limitations by producing compact
reduced-order models (ROMs) that approximate the behavior of the original
models at the input/output ports. In this paper, we present a low-rank BT
method that exploits the extended Krylov subspace and efficient implementation
techniques for the reduction of large-scale models. Experimental evaluation on
a diverse set of analog and mixed-signal circuits with millions of elements
indicates that up to x5.5 smaller ROMs can be produced with similar accuracy to
ANSYS RaptorX ROMs.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08479" title="Abstract">arXiv:2311.08479</a> [<a href="/pdf/2311.08479" title="Download PDF">pdf</a>, <a href="/format/2311.08479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Foundation Models to Improve Lightweight Clients in Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xidong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wan-Yi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Willmott%2C+D">Devin Willmott</a>, 
<a href="/search/cs?searchtype=author&query=Condessa%2C+F">Filipe Condessa</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenzhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+M+R">Madan Ravi Ganesh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 Pages + Appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) is a distributed training paradigm that enables
clients scattered across the world to cooperatively learn a global model
without divulging confidential data. However, FL faces a significant challenge
in the form of heterogeneous data distributions among clients, which leads to a
reduction in performance and robustness. A recent approach to mitigating the
impact of heterogeneous data distributions is through the use of foundation
models, which offer better performance at the cost of larger computational
overheads and slower inference speeds. We introduce foundation model
distillation to assist in the federated training of lightweight client models
and increase their performance under heterogeneous data settings while keeping
inference costs low. Our results show improvement in the global model
performance on a balanced testing set, which contains rarely observed samples,
even under extreme non-IID client data distributions. We conduct a thorough
evaluation of our framework with different foundation model backbones on
CIFAR10, with varying degrees of heterogeneous data distributions ranging from
class-specific data partitions across clients to dirichlet data sampling,
parameterized by values between 0.01 and 1.0.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08481" title="Abstract">arXiv:2311.08481</a> [<a href="/pdf/2311.08481" title="Download PDF">pdf</a>, <a href="/format/2311.08481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functionality learning through specification instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Araujo%2C+P+H+L">Pedro Henrique Luz de Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+B">Benjamin Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Test suites assess natural language processing models' performance on
specific functionalities: cases of interest involving model robustness,
fairness, or particular linguistic capabilities. They enable fine-grained
evaluations of model aspects that would otherwise go unnoticed in standard
evaluation datasets, but they do not address the problem of how to fix the
failure cases. Previous work has explored functionality learning by fine-tuning
models on suite data. While this improves performance on seen functionalities,
it often does not generalize to unseen ones and can harm general performance.
<br />This paper analyses a fine-tuning-free approach to functionality learning.
For each functionality in a suite, we generate a specification instruction that
encodes it. We combine the obtained specification instructions to create
specification-augmented prompts, which we feed to language models pre-trained
on natural instruction data to generate suite predictions. A core aspect of our
analysis is to measure the effect that including a set of specifications has on
a held-out set of unseen, qualitatively different specifications. Our
experiments across four tasks and models ranging from 80M to 175B parameters
show that smaller models struggle to follow specification instructions.
However, larger models (&gt; 3B params.) can benefit from specifications and even
generalize desirable behaviors across functionalities.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08483" title="Abstract">arXiv:2311.08483</a> [<a href="/pdf/2311.08483" title="Download PDF">pdf</a>, <a href="/format/2311.08483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploration of Hyperledger Besu in Designing Private Blockchain-based  Financial Distribution Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahrukh%2C+M+R+H">Md. Raisul Hasan Shahrukh</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+T">Md. Tabassinur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Mansoor%2C+N">Nafees Mansoor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computational Engineering, Finance, and Science (cs.CE); Software Engineering (cs.SE)

</div>
<p class="mathjax">Blockchain, a decentralized technology that provides unrivaled security,
transparency, and process validation, is redefining the operational landscape
across numerous industries. This article focuses on the development of an
innovative consortium blockchain based financial distribution application. This
paper illuminates the transformative role of blockchain technology in a variety
of sectors by drawing on a plethora of academic literature and current industry
practices. It demonstrates the diverse applications of blockchain, ranging from
remittances to lending and investments in finance to data administration in
healthcare and supply chain tracking. The paper reveals the design and
potential of a consortium blockchain based application for financial
distribution. Utilizing the capabilities of Hyperledger Besu, the application
is tailored to improve security, scalability, and interoperability, thereby
contributing to a more integrated financial ecosystem. The investigation sheds
light on the combination of consortium blockchain controlled access and
Hyprledger Besu comprehensive functionality, proposing a secure, transparent,
and efficient financial transaction environment. The investigation serves as a
resource for academics, industry professionals, and policymakers alike,
highlighting the vast potential of blockchain technology, enabled by platforms
such as Hyperledger Besu, in accelerating the evolution of traditional systems
toward a more decentralized, secure, and efficient future.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08485" title="Abstract">arXiv:2311.08485</a> [<a href="/pdf/2311.08485" title="Download PDF">pdf</a>, <a href="/format/2311.08485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Identification of Sexual Orientation and Gender Identity  Discriminatory Texts from Issue Comments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sultana%2C+S">Sayma Sultana</a>, 
<a href="/search/cs?searchtype=author&query=Sarker%2C+J">Jaydeb Sarker</a>, 
<a href="/search/cs?searchtype=author&query=Israt%2C+F">Farzana Israt</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+R">Rajshakhar Paul</a>, 
<a href="/search/cs?searchtype=author&query=Bosu%2C+A">Amiangshu Bosu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Under submission to TOSEM, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In an industry dominated by straight men, many developers representing other
gender identities and sexual orientations often encounter hateful or
discriminatory messages. Such communications pose barriers to participation for
women and LGBTQ+ persons. Due to sheer volume, manual inspection of all
communications for discriminatory communication is infeasible for a large-scale
Free Open-Source Software (FLOSS) community. To address this challenge, this
study aims to develop an automated mechanism to identify Sexual orientation and
Gender identity Discriminatory (SGID) texts from software developers'
communications. On this goal, we trained and evaluated SGID4SE ( Sexual
orientation and Gender Identity Discriminatory text identification for (4)
Software Engineering texts) as a supervised learning-based SGID detection tool.
SGID4SE incorporates six preprocessing steps and ten state-of-the-art
algorithms. SGID4SE implements six different strategies to improve the
performance of the minority class. We empirically evaluated each strategy and
identified an optimum configuration for each algorithm. In our ten-fold
cross-validation-based evaluations, a BERT-based model boosts the best
performance with 85.9% precision, 80.0% recall, and 82.9% F1-Score for the SGID
class. This model achieves 95.7% accuracy and 80.4% Matthews Correlation
Coefficient. Our dataset and tool establish a foundation for further research
in this direction.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08487" title="Abstract">arXiv:2311.08487</a> [<a href="/pdf/2311.08487" title="Download PDF">pdf</a>, <a href="/format/2311.08487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alignment is not sufficient to prevent large language models from  generating harmful information: A psychoanalytic perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wei Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jia Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) are central to a multitude of applications but
struggle with significant risks, notably in generating harmful content and
biases. Drawing an analogy to the human psyche's conflict between evolutionary
survival instincts and societal norm adherence elucidated in Freud's
psychoanalysis theory, we argue that LLMs suffer a similar fundamental
conflict, arising between their inherent desire for syntactic and semantic
continuity, established during the pre-training phase, and the post-training
alignment with human values. This conflict renders LLMs vulnerable to
adversarial attacks, wherein intensifying the models' desire for continuity can
circumvent alignment efforts, resulting in the generation of harmful
information. Through a series of experiments, we first validated the existence
of the desire for continuity in LLMs, and further devised a straightforward yet
powerful technique, such as incomplete sentences, negative priming, and
cognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle
to prevent the generation of harmful information. In summary, our study
uncovers the root of LLMs' vulnerabilities to adversarial attacks, hereby
questioning the efficacy of solely relying on sophisticated alignment methods,
and further advocates for a new training idea that integrates modal concepts
alongside traditional amodal concepts, aiming to endow LLMs with a more nuanced
understanding of real-world contexts and ethical considerations.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08488" title="Abstract">arXiv:2311.08488</a> [<a href="/pdf/2311.08488" title="Download PDF">pdf</a>, <a href="/format/2311.08488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUDD: A New Re-Identification Dataset with Efficient Annotation for  Off-Road Racers in Extreme Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tyo%2C+J">Jacob Tyo</a>, 
<a href="/search/cs?searchtype=author&query=Olarinre%2C+M">Motolani Olarinre</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+Y">Youngseog Chung</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Re-identifying individuals in unconstrained environments remains an open
challenge in computer vision. We introduce the Muddy Racer re-IDentification
Dataset (MUDD), the first large-scale benchmark for matching identities of
motorcycle racers during off-road competitions. MUDD exhibits heavy mud
occlusion, motion blurring, complex poses, and extreme lighting conditions
previously unseen in existing re-id datasets. We present an annotation
methodology incorporating auxiliary information that reduced labeling time by
over 65%. We establish benchmark performance using state-of-the-art re-id
models including OSNet and ResNet-50. Without fine-tuning, the best models
achieve only 33% Rank-1 accuracy. Fine-tuning on MUDD boosts results to 79%
Rank-1, but significant room for improvement remains. We analyze the impact of
real-world factors including mud, pose, lighting, and more. Our work exposes
open problems in re-identifying individuals under extreme conditions. We hope
MUDD serves as a diverse and challenging benchmark to spur progress in robust
re-id, especially for computer vision applications in emerging sports
analytics. All code and data can be found at https://github.com/JacobTyo/MUDD.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08491" title="Abstract">arXiv:2311.08491</a> [<a href="/pdf/2311.08491" title="Download PDF">pdf</a>, <a href="/format/2311.08491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Bidirectional Mixed-Traffic Overtaking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tariq%2C+F+M">Faizan M. Tariq</a>, 
<a href="/search/cs?searchtype=author&query=Suriyarachchi%2C+N">Nilesh Suriyarachchi</a>, 
<a href="/search/cs?searchtype=author&query=Mavridis%2C+C">Christos Mavridis</a>, 
<a href="/search/cs?searchtype=author&query=Baras%2C+J+S">John S. Baras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in: 2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE 25th International Conference on Intelligent
  Transportation Systems (ITSC), Macau, China, 2022, pp. 2494-2501
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Safe overtaking, especially in a bidirectional mixed-traffic setting, remains
a key challenge for Connected Autonomous Vehicles (CAVs). The presence of
human-driven vehicles (HDVs), behavior unpredictability, and blind spots
resulting from sensor occlusion make this a challenging control problem. To
overcome these difficulties, we propose a cooperative communication-based
approach that utilizes the information shared between CAVs to reduce the
effects of sensor occlusion while benefiting from the local velocity prediction
based on past tracking data. Our control framework aims to perform overtaking
maneuvers with the objective of maximizing velocity while prioritizing safety
and passenger comfort. Our method is also capable of reactively adjusting its
plan to dynamic changes in the environment. The performance of the proposed
approach is verified using realistic traffic simulations.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08494" title="Abstract">arXiv:2311.08494</a> [<a href="/pdf/2311.08494" title="Download PDF">pdf</a>, <a href="/format/2311.08494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Paradigm in Blockchain-based Financial Aid Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahrukh%2C+M+R+H">Md. Raisul Hasan Shahrukh</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+T">Md. Tabassinur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Mansoor%2C+N">Nafees Mansoor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computational Engineering, Finance, and Science (cs.CE); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Blockchain technology has emerged as a game-changer in a variety of
industries, providing robust solutions that can supplant conventional
procedures. The unique potential of this technology originates from its
decentralized ledger systems, which enable enhanced security, transparency, and
the validation of transactions without the need for intermediaries. Notably,
the financial sector is making substantial progress toward implementing
blockchain solutions for a variety of operations, including remittances,
lending, and investments. The healthcare industry is simultaneously
incorporating this technology into systems for managing medical records,
tracing supply chains, and data management. Similarly, the capacity of
blockchain to enhance transparency, traceability, and accountability is widely
acknowledged in supply chain management, from the procurement of basic
materials to the delivery of finished goods. Diverse industries, including real
estate, energy, and government, are actively investigating the potential of
blockchain to improve efficiency, security, and transparency. Notably,
Hyperledger Besu, an open-source blockchain platform, is used to implement
smart contracts that automate processes and reduce manual intervention along
distribution pathways. This exhaustive review examines the transformative
potential of blockchain technology across a variety of industries, discussing
the obstacles encountered and providing key insights into future research and
development directions. This paper seeks to serve as a pivotal resource for
academics, industry stakeholders, and policymakers by synthesizing existing
scholarly literature and shedding light on significant findings.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08496" title="Abstract">arXiv:2311.08496</a> [<a href="/pdf/2311.08496" title="Download PDF">pdf</a>, <a href="/format/2311.08496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Differentiable Predictive Control with Safety Guarantees: A  Predictive Safety Filter Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cortez%2C+W+S">Wenceslao Shaw Cortez</a>, 
<a href="/search/cs?searchtype=author&query=Drgona%2C+J">Jan Drgona</a>, 
<a href="/search/cs?searchtype=author&query=Vrabie%2C+D">Draguna Vrabie</a>, 
<a href="/search/cs?searchtype=author&query=Halappanavar%2C+M">Mahantesh Halappanavar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Automatica
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we propose a novel predictive safety filter that is robust to
bounded perturbations and is combined with a learning-based control called
differentiable predictive control (DPC). The proposed method provides rigorous
guarantees of safety in the presence of bounded perturbations and implements
DPC so long as the DPC control satisfies the system constraints. The approach
also incorporates two forms of event-triggering to reduce online computation.
The approach is comprised of a robust predictive safety filter that extends
upon existing work to reject disturbances for discrete-time, time-varying
nonlinear systems with time-varying constraints. The safety filter is based on
novel concepts of robust, discrete-time barrier functions and can be used to
filter any control law. Here we use the safety filter in conjunction with DPC
as a promising policy optimization method. The approach is demonstrated on a
single-integrator, two-tank system, and building example.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08500" title="Abstract">arXiv:2311.08500</a> [<a href="/pdf/2311.08500" title="Download PDF">pdf</a>, <a href="/format/2311.08500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Density Steering of Gaussian Mixture Models for Discrete-Time Linear  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Balci%2C+I">Isin Balci</a>, 
<a href="/search/eess?searchtype=author&query=Bakolas%2C+E">Efstathios Bakolas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we study the finite-horizon optimal density steering problem
for discrete-time stochastic linear dynamical systems. Specifically, we focus
on steering probability densities represented as Gaussian mixture models which
are known to give good approximations for general smooth probability density
functions. We then revisit the covariance steering problem for Gaussian
distributions and derive its optimal control policy. Subsequently, we propose a
randomized policy to enhance the numerical tractability of the problem and
demonstrate that under this policy the state distribution remains a Gaussian
mixture. By leveraging these results, we reduce the Gaussian mixture steering
problem to a linear program. We also discuss the problem of steering general
distributions using Gaussian mixture approximations. Finally, we present
results of non-trivial numerical experiments and demonstrate that our approach
can be applied to general distribution steering problems.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08503" title="Abstract">arXiv:2311.08503</a> [<a href="/pdf/2311.08503" title="Download PDF">pdf</a>, <a href="/format/2311.08503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MADG: Margin-based Adversarial Learning for Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dayal%2C+A">Aveen Dayal</a>, 
<a href="/search/cs?searchtype=author&query=B.%2C+V+K">Vimal K. B.</a>, 
<a href="/search/cs?searchtype=author&query=Cenkeramaddi%2C+L+R">Linga Reddy Cenkeramaddi</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+C+K">C. Krishna Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Abhinav Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+V+N">Vineeth N Balasubramanian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Domain Generalization (DG) techniques have emerged as a popular approach to
address the challenges of domain shift in Deep Learning (DL), with the goal of
generalizing well to the target domain unseen during the training. In recent
years, numerous methods have been proposed to address the DG setting, among
which one popular approach is the adversarial learning-based methodology. The
main idea behind adversarial DG methods is to learn domain-invariant features
by minimizing a discrepancy metric. However, most adversarial DG methods use
0-1 loss based $\mathcal{H}\Delta\mathcal{H}$ divergence metric. In contrast,
the margin loss-based discrepancy metric has the following advantages: more
informative, tighter, practical, and efficiently optimizable. To mitigate this
gap, this work proposes a novel adversarial learning DG algorithm, MADG,
motivated by a margin loss-based discrepancy metric. The proposed MADG model
learns domain-invariant features across all source domains and uses adversarial
training to generalize well to the unseen target domain. We also provide a
theoretical analysis of the proposed MADG model based on the unseen target
error bound. Specifically, we construct the link between the source and unseen
domains in the real-valued hypothesis space and derive the generalization bound
using margin loss and Rademacher complexity. We extensively experiment with the
MADG model on popular real-world DG datasets, VLCS, PACS, OfficeHome,
DomainNet, and TerraIncognita. We evaluate the proposed algorithm on
DomainBed's benchmark and observe consistent performance across all the
datasets.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08505" title="Abstract">arXiv:2311.08505</a> [<a href="/pdf/2311.08505" title="Download PDF">pdf</a>, <a href="/format/2311.08505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Structured Chain-of-Thought: Integrating Multiple Sources of  Knowledge for Improved Language Model Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xin Su</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Tiep Le</a>, 
<a href="/search/cs?searchtype=author&query=Bethard%2C+S">Steven Bethard</a>, 
<a href="/search/cs?searchtype=author&query=Howard%2C+P">Phillip Howard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">An important open question pertaining to the use of large language models for
knowledge-intensive tasks is how to effectively integrate knowledge from three
sources: the model's parametric memory, external structured knowledge, and
external unstructured knowledge. Most existing prompting methods either rely
solely on one or two of these sources, or require repeatedly invoking large
language models to generate similar or identical content. In this work, we
overcome these limitations by introducing a novel semi-structured prompting
approach that seamlessly integrates the model's parametric memory with
unstructured knowledge from text documents and structured knowledge from
knowledge graphs. Experimental results on open-domain multi-hop question
answering datasets demonstrate that our prompting method significantly
surpasses existing techniques, even exceeding those which require fine-tuning.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08511" title="Abstract">arXiv:2311.08511</a> [<a href="/pdf/2311.08511" title="Download PDF">pdf</a>, <a href="/format/2311.08511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoRE-CoG: Conversational Recommendation of Entities using Constrained  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+H">Harshvardhan Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Pruthi%2C+K">Kanav Pruthi</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarti%2C+S">Soumen Chakrabarti</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">End-to-end conversational recommendation systems (CRS) generate responses by
leveraging both dialog history and a knowledge base (KB). A CRS mainly faces
three key challenges: (1) at each turn, it must decide if recommending a KB
entity is appropriate; if so, it must identify the most relevant KB entity to
recommend; and finally, it must recommend the entity in a fluent utterance that
is consistent with the conversation history. Recent CRSs do not pay sufficient
attention to these desiderata, often generating unfluent responses or not
recommending (relevant) entities at the right turn. We introduce a new CRS we
call CoRE-CoG. CoRE-CoG addresses the limitations in prior systems by
implementing (1) a recommendation trigger that decides if the system utterance
should include an entity, (2) a type pruning module that improves the relevance
of recommended entities, and (3) a novel constrained response generator to make
recommendations while maintaining fluency. Together, these modules ensure
simultaneous accurate recommendation decisions and fluent system utterances.
Experiments with recent benchmarks show the superiority particularly on
conditional generation sub-tasks with close to 10 F1 and 4 Recall@1 percent
points gain over baselines.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08513" title="Abstract">arXiv:2311.08513</a> [<a href="/pdf/2311.08513" title="Download PDF">pdf</a>, <a href="/ps/2311.08513" title="Download PostScript">ps</a>, <a href="/format/2311.08513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query Efficient Weighted Stochastic Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Derakhshan%2C+M">Mahsa Derakhshan</a>, 
<a href="/search/cs?searchtype=author&query=Saneian%2C+M">Mohammad Saneian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In this paper, we study the weighted stochastic matching problem. Let $G=(V,
E)$ be a given edge-weighted graph and let its realization $\mathcal{G}$ be a
random subgraph of $G$ that includes each edge $e\in E$ independently with a
known probability $p_e$. The goal in this problem is to pick a sparse subgraph
$Q$ of $G$ without prior knowledge of $G$'s realization, such that the maximum
weight matching among the realized edges of $Q$ (i.e. the subgraph $Q\cap
\mathcal{G}$) in expectation approximates the maximum weight matching of the
entire realization $\mathcal{G}$.
<br />Attaining any constant approximation ratio for this problem requires
selecting a subgraph of max-degree $\Omega(1/p)$ where $p=\min_{e\in E} p_e$.
On the positive side, there exists a $(1-\epsilon)$-approximation algorithm by
Behnezhad and Derakhshan, albeit at the cost of max-degree having exponential
dependence on $1/p$. Within the $\text{poly}(1/p)$ regime, however, the
best-known algorithm achieves a $0.536$ approximation ratio due to Dughmi,
Kalayci, and Patel improving over the $0.501$ approximation algorithm by
Behnezhad, Farhadi, Hajiaghayi, and Reyhani.
<br />In this work, we present a 0.68 approximation algorithm with $O(1/p)$ queries
per vertex, which is asymptotically tight. This is even an improvement over the
best-known approximation ratio of $2/3$ for unweighted graphs within the
$\text{poly}(1/p)$ regime due to Assadi and Bernstein. The $2/3$ approximation
ratio is proven tight in the presence of a few correlated edges in
$\mathcal{G}$, indicating that surpassing the $2/3$ barrier should rely on the
independent realization of edges. Our analysis involves reducing the problem to
designing a randomized matching algorithm on a given stochastic graph with some
variance-bounding properties.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08514" title="Abstract">arXiv:2311.08514</a> [<a href="/pdf/2311.08514" title="Download PDF">pdf</a>, <a href="/format/2311.08514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An algorithm for Tambara-Yamagami quantum invariants of 3-manifolds,  parameterized by the first Betti number
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delaney%2C+C">Colleen Delaney</a> (UC Berkeley), 
<a href="/search/cs?searchtype=author&query=Maria%2C+C">Cl&#xe9;ment Maria</a> (INRIA &amp; FGV/EMAp), 
<a href="/search/cs?searchtype=author&query=Samperton%2C+E">Eric Samperton</a> (Purdue)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, including 3 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Geometric Topology (math.GT); Quantum Algebra (math.QA)

</div>
<p class="mathjax">Quantum topology provides various frameworks for defining and computing
invariants of manifolds. One such framework of substantial interest in both
mathematics and physics is the Turaev-Viro-Barrett-Westbury state sum
construction, which uses the data of a spherical fusion category to define
topological invariants of triangulated 3-manifolds via tensor network
contractions. In this work we consider a restricted class of state sum
invariants of 3-manifolds derived from Tambara-Yamagami categories. These
categories are particularly simple, being entirely specified by three pieces of
data: a finite abelian group, a bicharacter of that group, and a sign $\pm 1$.
Despite being one of the simplest sources of state sum invariants, the
computational complexities of Tambara-Yamagami invariants are yet to be fully
understood.
<br />We make substantial progress on this problem. Our main result is the
existence of a general fixed parameter tractable algorithm for all such
topological invariants, where the parameter is the first Betti number of the
3-manifold with $\mathbb{Z}/2\mathbb{Z}$ coefficients. We also explain that
these invariants are sometimes #P-hard to compute (and we expect that this is
almost always the case).
<br />Contrary to other domains of computational topology, such as graphs on
surfaces, very few hard problems in 3-manifold topology are known to admit FPT
algorithms with a topological parameter. However, such algorithms are of
particular interest as their complexity depends only polynomially on the
combinatorial representation of the input, regardless of size or combinatorial
width. Additionally, in the case of Betti numbers, the parameter itself is
easily computable in polynomial time.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08516" title="Abstract">arXiv:2311.08516</a> [<a href="/pdf/2311.08516" title="Download PDF">pdf</a>, <a href="/format/2311.08516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs cannot find reasoning errors, but can correct them!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tyen%2C+G">Gladys Tyen</a>, 
<a href="/search/cs?searchtype=author&query=Mansoor%2C+H">Hassan Mansoor</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peter Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mak%2C+T">Tony Mak</a>, 
<a href="/search/cs?searchtype=author&query=C%C4%83rbune%2C+V">Victor C&#x103;rbune</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">While self-correction has shown promise in improving LLM outputs in terms of
style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent
attempts to self-correct logical or reasoning errors often cause correct
answers to become incorrect, resulting in worse performances overall (Huang et
al., 2023). In this paper, we break down the self-correction process into two
core components: mistake finding and output correction. For mistake finding, we
release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought
reasoning traces. We provide benchmark numbers for several state-of-the-art
LLMs, and demonstrate that LLMs generally struggle with finding logical
mistakes. For output correction, we propose a backtracking method which
provides large improvements when given information on mistake location. We
construe backtracking as a lightweight alternative to reinforcement learning
methods, and show that it remains effective with a reward model at 60-70%
accuracy.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08525" title="Abstract">arXiv:2311.08525</a> [<a href="/pdf/2311.08525" title="Download PDF">pdf</a>, <a href="/format/2311.08525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Rotation Invariance in Deep Neural Networks through Artificial  Mental Rotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tuggener%2C+L">Lukas Tuggener</a>, 
<a href="/search/cs?searchtype=author&query=Stadelmann%2C+T">Thilo Stadelmann</a>, 
<a href="/search/cs?searchtype=author&query=Schmidhuber%2C+J">J&#xfc;rgen Schmidhuber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Humans and animals recognize objects irrespective of the beholder's point of
view, which may drastically change their appearances. Artificial pattern
recognizers also strive to achieve this, e.g., through translational invariance
in convolutional neural networks (CNNs). However, both CNNs and vision
transformers (ViTs) perform very poorly on rotated inputs. Here we present
artificial mental rotation (AMR), a novel deep learning paradigm for dealing
with in-plane rotations inspired by the neuro-psychological concept of mental
rotation. Our simple AMR implementation works with all common CNN and ViT
architectures. We test it on ImageNet, Stanford Cars, and Oxford Pet. With a
top-1 error (averaged across datasets and architectures) of $0.743$, AMR
outperforms the current state of the art (rotational data augmentation, average
top-1 error of $0.626$) by $19\%$. We also easily transfer a trained AMR module
to a downstream task to improve the performance of a pre-trained semantic
segmentation model on rotated CoCo from $32.7$ to $55.2$ IoU.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08526" title="Abstract">arXiv:2311.08526</a> [<a href="/pdf/2311.08526" title="Download PDF">pdf</a>, <a href="/format/2311.08526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLiNER: Generalist Model for Named Entity Recognition using  Bidirectional Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaratiana%2C+U">Urchade Zaratiana</a>, 
<a href="/search/cs?searchtype=author&query=Tomeh%2C+N">Nadi Tomeh</a>, 
<a href="/search/cs?searchtype=author&query=Holat%2C+P">Pierre Holat</a>, 
<a href="/search/cs?searchtype=author&query=Charnois%2C+T">Thierry Charnois</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Named Entity Recognition (NER) is essential in various Natural Language
Processing (NLP) applications. Traditional NER models are effective but limited
to a set of predefined entity types. In contrast, Large Language Models (LLMs)
can extract arbitrary entities through natural language instructions, offering
greater flexibility. However, their size and cost, particularly for those
accessed via APIs like ChatGPT, make them impractical in resource-limited
scenarios. In this paper, we introduce a compact NER model trained to identify
any type of entity. Leveraging a bidirectional transformer encoder, our model,
GLiNER, facilitates parallel entity extraction, an advantage over the slow
sequential token generation of LLMs. Through comprehensive testing, GLiNER
demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs
in zero-shot evaluations on various NER benchmarks.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08530" title="Abstract">arXiv:2311.08530</a> [<a href="/pdf/2311.08530" title="Download PDF">pdf</a>, <a href="/format/2311.08530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SceneScore: Learning a Cost Function for Object Arrangement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapelyukh%2C+I">Ivan Kapelyukh</a>, 
<a href="/search/cs?searchtype=author&query=Johns%2C+E">Edward Johns</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at CoRL 2023 LEAP Workshop. Webpage: <a href="https://sites.google.com/view/scenescore">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Arranging objects correctly is a key capability for robots which unlocks a
wide range of useful tasks. A prerequisite for creating successful arrangements
is the ability to evaluate the desirability of a given arrangement. Our method
"SceneScore" learns a cost function for arrangements, such that desirable,
human-like arrangements have a low cost. We learn the distribution of training
arrangements offline using an energy-based model, solely from example images
without requiring environment interaction or human supervision. Our model is
represented by a graph neural network which learns object-object relations,
using graphs constructed from images. Experiments demonstrate that the learned
cost function can be used to predict poses for missing objects, generalise to
novel objects using semantic features, and can be composed with other cost
functions to satisfy constraints at inference time.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08533" title="Abstract">arXiv:2311.08533</a> [<a href="/pdf/2311.08533" title="Download PDF">pdf</a>, <a href="/format/2311.08533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Processing for Financial Regulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Achitouv%2C+I">Ixandra Achitouv</a>, 
<a href="/search/cs?searchtype=author&query=Gorduza%2C+D">Dragos Gorduza</a>, 
<a href="/search/cs?searchtype=author&query=Jacquier%2C+A">Antoine Jacquier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP)

</div>
<p class="mathjax">This article provides an understanding of Natural Language Processing
techniques in the framework of financial regulation, more specifically in order
to perform semantic matching search between rules and policy when no dataset is
available for supervised learning. We outline how to outperform simple
pre-trained sentences-transformer models using freely available resources and
explain the mathematical concepts behind the key building blocks of Natural
Language Processing.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08535" title="Abstract">arXiv:2311.08535</a> [<a href="/pdf/2311.08535" title="Download PDF">pdf</a>, <a href="/ps/2311.08535" title="Download PostScript">ps</a>, <a href="/format/2311.08535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taxonomy, Semantic Data Schema, and Schema Alignment for Open Data in  Urban Building Energy Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianli Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jia Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Urban Building Energy Modeling (UBEM) is a critical tool to provide
quantitative analysis on building decarbonization, sustainability,
building-to-grid integration, and renewable energy applications on city,
regional, and national scales. Researchers usually use open data as inputs to
build and calibrate UBEM. However, open data are from thousands of sources
covering various perspectives of weather, building characteristics, etc.
Besides, a lack of semantic features of open data further increases the
engineering effort to process information to be directly used for UBEM as
inputs. In this paper, we first reviewed open data types used for UBEM and
developed a taxonomy to categorize open data. Based on that, we further
developed a semantic data schema for each open data category to maintain data
consistency and improve model automation for UBEM. In a case study, we use
three popular open data to show how they can be automatically processed based
on the proposed schematic data structure using large language models. The
accurate results generated by large language models indicate the
machine-readability and human-interpretability of the developed semantic data
schema.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08536" title="Abstract">arXiv:2311.08536</a> [<a href="/pdf/2311.08536" title="Download PDF">pdf</a>, <a href="/format/2311.08536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Frequency Load Identification using CNN-BiLSTM Attention Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Azzam%2C+A">Amanie Azzam</a>, 
<a href="/search/eess?searchtype=author&query=Sanami%2C+S">Saba Sanami</a>, 
<a href="/search/eess?searchtype=author&query=Aghdam%2C+A+G">Amir G. Aghdam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Non-intrusive Load Monitoring (NILM) is an established technique for
effective and cost-efficient electricity consumption management. The method is
used to estimate appliance-level power consumption from aggregated power
measurements. This paper presents a hybrid learning approach, consisting of a
convolutional neural network (CNN) and a bidirectional long short-term memory
(BILSTM), featuring an integrated attention mechanism, all within the context
of disaggregating low-frequency power data. While prior research has been
mainly focused on high-frequency data disaggregation, our study takes a
distinct direction by concentrating on low-frequency data. The proposed hybrid
CNN-BILSTM model is adept at extracting both temporal (time-related) and
spatial (location-related) features, allowing it to precisely identify energy
consumption patterns at the appliance level. This accuracy is further enhanced
by the attention mechanism, which aids the model in pinpointing crucial parts
of the data for more precise event detection and load disaggregation. We
conduct simulations using the existing low-frequency REDD dataset to assess our
model performance. The results demonstrate that our proposed approach
outperforms existing methods in terms of accuracy and computation time.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08538" title="Abstract">arXiv:2311.08538</a> [<a href="/pdf/2311.08538" title="Download PDF">pdf</a>, <a href="/format/2311.08538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending Multilingual Machine Translation through Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+W">Wen Lai</a>, 
<a href="/search/cs?searchtype=author&query=Hangya%2C+V">Viktor Hangya</a>, 
<a href="/search/cs?searchtype=author&query=Fraser%2C+A">Alexander Fraser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the growing variety of languages supported by existing multilingual
neural machine translation (MNMT) models, most of the world's languages are
still being left behind. We aim to extend large-scale MNMT models to a new
language, allowing for translation between the newly added and all of the
already supported languages in a challenging scenario: using only a parallel
corpus between the new language and English. Previous approaches, such as
continued training on parallel data including the new language, suffer from
catastrophic forgetting (i.e., performance on other languages is reduced). Our
novel approach Imit-MNMT treats the task as an imitation learning process,
which mimicks the behavior of an expert, a technique widely used in the
computer vision area, but not well explored in NLP. More specifically, we
construct a pseudo multi-parallel corpus of the new and the original languages
by pivoting through English, and imitate the output distribution of the
original MNMT model. Extensive experiments show that our approach significantly
improves the translation performance between the new and the original
languages, without severe catastrophic forgetting. We also demonstrate that our
approach is capable of solving copy and off-target problems, which are two
common issues existence in current large-scale MNMT models.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08539" title="Abstract">arXiv:2311.08539</a> [<a href="/pdf/2311.08539" title="Download PDF">pdf</a>, <a href="/format/2311.08539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Adversarial Examples for Multi-Camera Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R%C4%83du%C5%A3oiu%2C+A">Ana R&#x103;du&#x163;oiu</a>, 
<a href="/search/cs?searchtype=author&query=Schulze%2C+J">Jan-Philipp Schulze</a>, 
<a href="/search/cs?searchtype=author&query=Sperl%2C+P">Philip Sperl</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6ttinger%2C+K">Konstantin B&#xf6;ttinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural networks build the foundation of several intelligent systems, which,
however, are known to be easily fooled by adversarial examples. Recent advances
made these attacks possible even in air-gapped scenarios, where the autonomous
system observes its surroundings by, e.g., a camera. We extend these ideas in
our research and evaluate the robustness of multi-camera setups against such
physical adversarial examples. This scenario becomes ever more important with
the rise in popularity of autonomous vehicles, which fuse the information of
several cameras for their driving decision. While we find that multi-camera
setups provide some robustness towards past attack methods, we see that this
advantage reduces when optimizing on multiple perspectives at once. We propose
a novel attack method that we call Transcender-MC, where we incorporate online
3D renderings and perspective projections in the training process. Moreover, we
motivate that certain data augmentation techniques can facilitate the
generation of successful adversarial examples even further. Transcender-MC is
11% more effective in successfully attacking multi-camera setups than
state-of-the-art methods. Our findings offer valuable insights regarding the
resilience of object detection in a setup with multiple cameras and motivate
the need of developing adequate defense mechanisms against them.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08545" title="Abstract">arXiv:2311.08545</a> [<a href="/pdf/2311.08545" title="Download PDF">pdf</a>, <a href="/format/2311.08545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Continual Pre-training for Building Domain Specific Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+K">Karan Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+A">Aitzaz Ahmad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated remarkable open-domain
capabilities. Traditionally, LLMs tailored for a domain are trained from
scratch to excel at handling domain-specific tasks. In this work, we explore an
alternative strategy of continual pre-training as a means to develop
domain-specific LLMs. We introduce FinPythia-6.9B, developed through
domain-adaptive continual pre-training on the financial domain. Continual
pre-trained FinPythia showcases consistent improvements on financial tasks over
the original foundational model. We further explore simple but effective data
selection strategies for continual pre-training. Our data selection strategies
outperforms vanilla continual pre-training's performance with just 10% of
corpus size and cost, without any degradation on open-domain standard tasks.
Our work proposes an alternative solution to building domain-specific LLMs from
scratch in a cost-effective manner.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08547" title="Abstract">arXiv:2311.08547</a> [<a href="/pdf/2311.08547" title="Download PDF">pdf</a>, <a href="/format/2311.08547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepThought: An Architecture for Autonomous Self-motivated Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+A+L">Arlindo L. Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Domingos%2C+T">Tiago Domingos</a>, 
<a href="/search/cs?searchtype=author&query=Figueiredo%2C+M">M&#xe1;rio Figueiredo</a>, 
<a href="/search/cs?searchtype=author&query=Lima%2C+P+U">Pedro U. Lima</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The ability of large language models (LLMs) to engage in credible dialogues
with humans, taking into account the training data and the context of the
conversation, has raised discussions about their ability to exhibit intrinsic
motivations, agency, or even some degree of consciousness. We argue that the
internal architecture of LLMs and their finite and volatile state cannot
support any of these properties. By combining insights from complementary
learning systems, global neuronal workspace, and attention schema theories, we
propose to integrate LLMs and other deep learning systems into an architecture
for cognitive language agents able to exhibit properties akin to agency,
self-motivation, even some features of meta-cognition.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08550" title="Abstract">arXiv:2311.08550</a> [<a href="/pdf/2311.08550" title="Download PDF">pdf</a>, <a href="/ps/2311.08550" title="Download PostScript">ps</a>, <a href="/format/2311.08550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Approach using ARIMA, Kalman Filter and LSTM for Accurate Wind  Speed Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mohapatra%2C+M+R">Manas Ranjan Mohapatra</a>, 
<a href="/search/eess?searchtype=author&query=Radhakrishnan%2C+R">Rahul Radhakrishnan</a>, 
<a href="/search/eess?searchtype=author&query=Shukla%2C+R+M">Raj Mani Shukla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Present energy demand and modernization are leading to greater fossil fuel
consumption, which has increased environmental pollution and led to climate
change. Hence to decrease dependency on conventional energy sources, renewable
energy sources are considered. Wind energy is a long-term renewable energy
resource but its intermittent nature makes it difficult in harnessing it. Since
wind speed prediction is vital there are different methodologies for wind speed
estimation available in the literature. In this work, a new hybrid model is
proposed by combining auto-regressive integrated moving average (ARIMA), Kalman
filter and long short-term memory (LSTM) for estimating wind speed which works
more accurately than the existing methods proposed in the literature. From
simulations, it is observed that the proposed method works with better accuracy
when compared to the existing methods.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08552" title="Abstract">arXiv:2311.08552</a> [<a href="/pdf/2311.08552" title="Download PDF">pdf</a>, <a href="/format/2311.08552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UT5: Pretraining Non autoregressive T5 with unrolled denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salem%2C+M+G">Mahmoud G. Salem</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jiayu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chu-Cheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Frederick Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advances in Transformer-based Large Language Models have made great
strides in natural language generation. However, to decode K tokens, an
autoregressive model needs K sequential forward passes, which may be a
performance bottleneck for large language models. Many non-autoregressive (NAR)
research are aiming to address this sequentiality bottleneck, albeit many have
focused on a dedicated architecture in supervised benchmarks. In this work, we
studied unsupervised pretraining for non auto-regressive T5 models via unrolled
denoising and shown its SoTA results in downstream generation tasks such as
SQuAD question generation and XSum.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08554" title="Abstract">arXiv:2311.08554</a> [<a href="/pdf/2311.08554" title="Download PDF">pdf</a>, <a href="/ps/2311.08554" title="Download PostScript">ps</a>, <a href="/format/2311.08554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How international research teams respond to disruption in their mobility  patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Walther%2C+O+J">Olivier J. Walther</a>, 
<a href="/search/cs?searchtype=author&query=Prieto-Curiel%2C+R">Rafael Prieto-Curiel</a>, 
<a href="/search/cs?searchtype=author&query=Odera%2C+E">Erica Odera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Combining social network analysis with personal interviews, the paper
examines how the social structure and internal composition of three
Africa-focused international research networks contributes to their resilience.
It shows that research networks are structured around a small number of highly
influential coordinators. This structure facilitates information exchange and
trust between countries and across fields. The study also suggests that the
surveyed teams tend to exchange information or trust each other irrespective of
their social and professional attributes, indicating that diversity is key to
understanding their responses to major shocks such as the COVID-19 pandemic. In
a second part, the paper analyzes how the spatial constraints imposed by
distance and borders affect their ability to function internationally. It shows
that the probability of exchanging information, trusting each other, and
co-publishing decreases considerably with distance and that research
communities are more likely formed inside the same country than
internationally. Interviews reveal that teams responded to travel bans and
border closure by emphasizing what they already did best, suggesting that
resilience should be considered as an evolutionary attribute of a system.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08557" title="Abstract">arXiv:2311.08557</a> [<a href="/pdf/2311.08557" title="Download PDF">pdf</a>, <a href="/format/2311.08557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-light Pedestrian Detection in Visible and Infrared Image Feeds:  Issues and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vachhani%2C+H">Hrishikesh Vachhani</a>, 
<a href="/search/cs?searchtype=author&query=Akilan%2C+T">Thangarajah Akilan</a>, 
<a href="/search/cs?searchtype=author&query=Devmurari%2C+Y">Yash Devmurari</a>, 
<a href="/search/cs?searchtype=author&query=Shaik%2C+N">Nisharaff Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+D">Dhruvisha Patel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Pedestrian detection has become a cornerstone for several high-level tasks,
including autonomous driving, intelligent transportation, and traffic
surveillance. There are several works focussed on pedestrian detection using
visible images, mainly in the daytime. However, this task is very intriguing
when the environmental conditions change to poor lighting or nighttime.
Recently, new ideas have been spurred to use alternative sources, such as Far
InfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light
conditions. This study comprehensively reviews recent developments in low-light
pedestrian detection approaches. It systematically categorizes and analyses
various algorithms from region-based to non-region-based and graph-based
learning methodologies by highlighting their methodologies, implementation
issues, and challenges. It also outlines the key benchmark datasets that can be
used for research and development of advanced pedestrian detection algorithms,
particularly in low-light situations
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08562" title="Abstract">arXiv:2311.08562</a> [<a href="/pdf/2311.08562" title="Download PDF">pdf</a>, <a href="/format/2311.08562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAgIC: Benchmarking Large Language Model Powered Multi-Agent in  Cognition, Adaptability, Rationality and Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Daquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hongyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S+K">See Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiashi Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have marked a significant advancement in the
field of natural language processing, demonstrating exceptional capabilities in
reasoning, tool usage, and memory. As their applications extend into
multi-agent environments, a need has arisen for a comprehensive evaluation
framework that captures their abilities in reasoning, planning, collaboration,
and more. This work introduces a novel benchmarking framework specifically
tailored to assess LLMs within multi-agent settings, providing quantitative
metrics to evaluate their judgment, reasoning, deception, self-awareness,
collaboration, coordination, and rationality. We utilize games such as
Chameleon and Undercover, alongside game theory scenarios like Cost Sharing,
Multi-player Prisoner's Dilemma, and Public Good, to create diverse testing
environments. Our framework is fortified with the Probabilistic Graphical
Modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex
social and cognitive dimensions. The benchmark evaluates seven multi-agent
systems powered by different LLMs, quantitatively highlighting a significant
capability gap over threefold between the strongest, GPT-4, and the weakest,
Llama-2-70B. It also confirms that our PGM enhancement boosts the inherent
abilities of all selected models by 50% on average. Our codes are released here
https://github.com/cathyxl/MAgIC.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08566" title="Abstract">arXiv:2311.08566</a> [<a href="/pdf/2311.08566" title="Download PDF">pdf</a>, <a href="/ps/2311.08566" title="Download PostScript">ps</a>, <a href="/format/2311.08566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COMET: A Cross-Layer Optimized Optical Phase Change Main Memory  Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sunny%2C+F">Febin Sunny</a>, 
<a href="/search/cs?searchtype=author&query=Shafiee%2C+A">Amin Shafiee</a>, 
<a href="/search/cs?searchtype=author&query=Charbonnier%2C+B">Benoit Charbonnier</a>, 
<a href="/search/cs?searchtype=author&query=Nikdast%2C+M">Mahdi Nikdast</a>, 
<a href="/search/cs?searchtype=author&query=Pasricha%2C+S">Sudeep Pasricha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Traditional DRAM-based main memory systems face several challenges with
memory refresh overhead, high latency, and low throughput as the industry moves
towards smaller DRAM cells. These issues have been exacerbated by the emergence
of data-intensive applications in recent years. Memories based on phase change
materials (PCMs) offer promising solutions to these challenges. PCMs store data
in the material's phase, which can shift between amorphous and crystalline
states when external thermal energy is supplied. This is often achieved using
electrical pulses. Alternatively, using laser pulses and integration with
silicon photonics offers a unique opportunity to realize high-bandwidth and
low-latency photonic memories. Such a memory system may in turn open the
possibility of realizing fully photonic computing systems. But to realize
photonic memories, several challenges that are unique to the photonic domain
such as crosstalk, optical loss management, and laser power overhead have to be
addressed. In this work, we present COMET, the first cross-layer optimized
optical main memory architecture that uses PCMs. In architecting COMET, we
explore how to use silicon photonics and PCMs together to design a large-scale
main memory system while addressing associated challenges. We explore
challenges and propose solutions at the PCM cell, photonic memory circuit, and
memory architecture levels. Based on our evaluations, COMET offers 7.1x better
bandwidth, 15.1x lower EPB, and 3x lower latencies than the best-known prior
work on photonic main memory architecture design.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08568" title="Abstract">arXiv:2311.08568</a> [<a href="/pdf/2311.08568" title="Download PDF">pdf</a>, <a href="/ps/2311.08568" title="Download PostScript">ps</a>, <a href="/format/2311.08568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Imitation Learning On Aggregated Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Woillemont%2C+P+L+P">Pierre Le Pelletier de Woillemont</a>, 
<a href="/search/cs?searchtype=author&query=Labory%2C+R">R&#xe9;mi Labory</a>, 
<a href="/search/cs?searchtype=author&query=Corruble%2C+V">Vincent Corruble</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Inverse Reinforcement Learning (IRL) learns an optimal policy, given some
expert demonstrations, thus avoiding the need for the tedious process of
specifying a suitable reward function. However, current methods are constrained
by at least one of the following requirements. The first one is the need to
fully solve a forward Reinforcement Learning (RL) problem in the inner loop of
the algorithm, which might be prohibitively expensive in many complex
environments. The second one is the need for full trajectories from the
experts, which might not be easily available. The third one is the assumption
that the expert data is homogeneous rather than a collection from various
experts or possibly alternative solutions to the same task. Such constraints
make IRL approaches either not scalable or not usable on certain existing
systems. In this work we propose an approach which removes these requirements
through a dynamic, adaptive method called Adversarial Imitation Learning on
Aggregated Data (AILAD). It learns conjointly both a non linear reward function
and the associated optimal policy using an adversarial framework. The reward
learner only uses aggregated data. Moreover, it generates diverse behaviors
producing a distribution over the aggregated data matching that of the experts.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08569" title="Abstract">arXiv:2311.08569</a> [<a href="/pdf/2311.08569" title="Download PDF">pdf</a>, <a href="/ps/2311.08569" title="Download PostScript">ps</a>, <a href="/format/2311.08569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Quantification in Neural-Network Based Pain Intensity  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozek%2C+B">Burcu Ozek</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhenyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Radhakrishnan%2C+S">Srinivasan Radhakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Kamarthi%2C+S">Sagar Kamarthi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Improper pain management can lead to severe physical or mental consequences,
including suffering, and an increased risk of opioid dependency. Assessing the
presence and severity of pain is imperative to prevent such outcomes and
determine the appropriate intervention. However, the evaluation of pain
intensity is challenging because different individuals experience pain
differently. To overcome this, researchers have employed machine learning
models to evaluate pain intensity objectively. However, these efforts have
primarily focused on point estimation of pain, disregarding the inherent
uncertainty and variability present in the data and model. Consequently, the
point estimates provide only partial information for clinical decision-making.
This study presents a neural network-based method for objective pain interval
estimation, incorporating uncertainty quantification. This work explores three
algorithms: the bootstrap method, lower and upper bound estimation (LossL)
optimized by genetic algorithm, and modified lower and upper bound estimation
(LossS) optimized by gradient descent algorithm. Our empirical results reveal
that LossS outperforms the other two by providing a narrower prediction
interval. As LossS outperforms, we assessed its performance in three different
scenarios for pain assessment: (1) a generalized approach (single model for the
entire population), (2) a personalized approach (separate model for each
individual), and (3) a hybrid approach (separate model for each cluster of
individuals). Our findings demonstrate the hybrid approach's superior
performance, with notable practicality in clinical contexts. It has the
potential to be a valuable tool for clinicians, enabling objective pain
intensity assessment while taking uncertainty into account. This capability is
crucial in facilitating effective pain management and reducing the risks
associated with improper treatment.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08572" title="Abstract">arXiv:2311.08572</a> [<a href="/pdf/2311.08572" title="Download PDF">pdf</a>, <a href="/format/2311.08572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Multilingual Summarisation: An Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Whitehouse%2C+C">Chenxi Whitehouse</a>, 
<a href="/search/cs?searchtype=author&query=Huot%2C+F">Fantine Huot</a>, 
<a href="/search/cs?searchtype=author&query=Bastings%2C+J">Jasmijn Bastings</a>, 
<a href="/search/cs?searchtype=author&query=Dehghani%2C+M">Mostafa Dehghani</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chu-Cheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lapata%2C+M">Mirella Lapata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the increasing prevalence of Large Language Models, traditional full
fine-tuning approaches face growing challenges, especially in memory-intensive
tasks. This paper investigates the potential of Parameter-Efficient
Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), for complex and
under-explored multilingual summarisation tasks. We conduct an extensive study
across different data availability scenarios, including full-data, low-data,
and cross-lingual transfer, leveraging models of different sizes. Our findings
reveal that LoRA lags behind full fine-tuning when trained with full data,
however, it excels in low-data scenarios and cross-lingual transfer.
Interestingly, as models scale up, the performance gap between LoRA and full
fine-tuning diminishes. Additionally, we investigate effective strategies for
few-shot cross-lingual transfer, finding that continued LoRA tuning achieves
the best performance compared to both full fine-tuning and dynamic composition
of language-specific LoRA modules.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08575" title="Abstract">arXiv:2311.08575</a> [<a href="/pdf/2311.08575" title="Download PDF">pdf</a>, <a href="/ps/2311.08575" title="Download PostScript">ps</a>, <a href="/format/2311.08575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian Approximation of Convex Sets by Intersections of Halfspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De%2C+A">Anindya De</a>, 
<a href="/search/cs?searchtype=author&query=Nadimpalli%2C+S">Shivam Nadimpalli</a>, 
<a href="/search/cs?searchtype=author&query=Servedio%2C+R+A">Rocco A. Servedio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 64 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS); Metric Geometry (math.MG); Probability (math.PR)

</div>
<p class="mathjax">We study the approximability of general convex sets in $\mathbb{R}^n$ by
intersections of halfspaces, where the approximation quality is measured with
respect to the standard Gaussian distribution $N(0,I_n)$ and the complexity of
an approximation is the number of halfspaces used. While a large body of
research has considered the approximation of convex sets by intersections of
halfspaces under distance metrics such as the Lebesgue measure and Hausdorff
distance, prior to our work there has not been a systematic study of convex
approximation under the Gaussian distribution.
<br />We establish a range of upper and lower bounds, both for general convex sets
and for specific natural convex sets that are of particular interest. Our
results demonstrate that the landscape of approximation is intriguingly
different under the Gaussian distribution versus previously studied distance
measures. For example, we show that $2^{\Theta(\sqrt{n})}$ halfspaces are both
necessary and sufficient to approximate the origin-centered $\ell_2$ ball of
Gaussian volume 1/2 to any constant accuracy, and that for $1 \leq p &lt; 2$, the
origin-centered $\ell_p$ ball of Gaussian volume 1/2 can be approximated to any
constant accuracy as an intersection of $2^{\widetilde{O}(n^{3/4})}$ many
halfspaces. These bounds are quite different from known approximation results
under more commonly studied distance measures.
<br />Our results are proved using techniques from many different areas. These
include classical results on convex polyhedral approximation, Cram\'er-type
bounds on large deviations from probability theory, and -- perhaps surprisingly
-- a range of topics from computational complexity, including computational
learning theory, unconditional pseudorandomness, and the study of influences
and noise sensitivity in the analysis of Boolean functions.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08576" title="Abstract">arXiv:2311.08576</a> [<a href="/pdf/2311.08576" title="Download PDF">pdf</a>, <a href="/format/2311.08576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Evaluating AI Systems for Moral Status Using Self-Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+R">Robert Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">As AI systems become more advanced and widely deployed, there will likely be
increasing debate over whether AI systems could have conscious experiences,
desires, or other states of potential moral significance. It is important to
inform these discussions with empirical evidence to the extent possible. We
argue that under the right circumstances, self-reports, or an AI system's
statements about its own internal states, could provide an avenue for
investigating whether AI systems have states of moral significance.
Self-reports are the main way such states are assessed in humans ("Are you in
pain?"), but self-reports from current systems like large language models are
spurious for many reasons (e.g. often just reflecting what humans would say).
To make self-reports more appropriate for this purpose, we propose to train
models to answer many kinds of questions about themselves with known answers,
while avoiding or limiting training incentives that bias self-reports. The hope
of this approach is that models will develop introspection-like capabilities,
and that these capabilities will generalize to questions about states of moral
significance. We then propose methods for assessing the extent to which these
techniques have succeeded: evaluating self-report consistency across contexts
and between similar models, measuring the confidence and resilience of models'
self-reports, and using interpretability to corroborate self-reports. We also
discuss challenges for our approach, from philosophical difficulties in
interpreting self-reports to technical reasons why our proposal might fail. We
hope our discussion inspires philosophers and AI researchers to criticize and
improve our proposed methodology, as well as to run experiments to test whether
self-reports can be made reliable enough to provide information about states of
moral significance.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08577" title="Abstract">arXiv:2311.08577</a> [<a href="/pdf/2311.08577" title="Download PDF">pdf</a>, <a href="/format/2311.08577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding AI-Generated Faces in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Porcile%2C+G+J+A">Gonzalo J. Aniano Porcile</a>, 
<a href="/search/cs?searchtype=author&query=Gindi%2C+J">Jack Gindi</a>, 
<a href="/search/cs?searchtype=author&query=Mundra%2C+S">Shivansh Mundra</a>, 
<a href="/search/cs?searchtype=author&query=Verbus%2C+J+R">James R. Verbus</a>, 
<a href="/search/cs?searchtype=author&query=Farid%2C+H">Hany Farid</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">AI-based image generation has continued to rapidly improve, producing
increasingly more realistic images with fewer obvious visual flaws.
AI-generated images are being used to create fake online profiles which in turn
are being used for spam, fraud, and disinformation campaigns. As the general
problem of detecting any type of manipulated or synthesized content is
receiving increasing attention, here we focus on a more narrow task of
distinguishing a real face from an AI-generated face. This is particularly
applicable when tackling inauthentic online accounts with a fake user profile
photo. We show that by focusing on only faces, a more resilient and
general-purpose artifact can be detected that allows for the detection of
AI-generated faces from a variety of GAN- and diffusion-based synthesis
engines, and across image resolutions (as low as 128 x 128 pixels) and
qualities.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08578" title="Abstract">arXiv:2311.08578</a> [<a href="/pdf/2311.08578" title="Download PDF">pdf</a>, <a href="/format/2311.08578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A solver for linear scalar ordinary differential equations whose running  time is bounded independent of frequency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aubry%2C+M">Murdock Aubry</a>, 
<a href="/search/math?searchtype=author&query=Bremer%2C+J">James Bremer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2308.03288">arXiv:2308.03288</a>, <a href="/abs/2309.13848">arXiv:2309.13848</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">When the eigenvalues of the coefficient matrix for a linear scalar ordinary
differential equation are of large magnitude, its solutions exhibit complicated
behaviour, such as high-frequency oscillations, rapid growth or rapid decay.
The cost of representing such solutions using standard techniques grows with
the magnitudes of the eigenvalues. As a consequence, the running times of most
solvers for ordinary differential equations also grow with these eigenvalues.
However, a large class of scalar ordinary differential equations with
slowly-varying coefficients admit slowly-varying phase functions that can be
represented at a cost which is bounded independent of the magnitudes of the
eigenvalues of the corresponding coefficient matrix. Here, we introduce a
numerical algorithm for constructing slowly-varying phase functions which
represent the solutions of a linear scalar ordinary differential equation. Our
method's running time depends on the complexity of the equation's coefficients,
but is bounded independent of the magnitudes of the equation's eigenvalues.
Once the phase functions have been constructed, essentially any reasonable
initial or boundary value problem for the scalar equation can be easily solved.
We present the results of numerical experiments showing that, despite its
greater generality, our algorithm is competitive with state-of-the-art methods
for solving highly-oscillatory second order differential equations. We also
compare our method with Magnus-type exponential integrators and find that our
approach is orders of magnitude faster in the high-frequency regime.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08579" title="Abstract">arXiv:2311.08579</a> [<a href="/pdf/2311.08579" title="Download PDF">pdf</a>, <a href="/format/2311.08579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational  AutoEncoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Valentino%2C+M">Marco Valentino</a>, 
<a href="/search/cs?searchtype=author&query=Carvalho%2C+D+S">Danilo S. Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Pratt-Hartmann%2C+I">Ian Pratt-Hartmann</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+A">Andr&#xe9; Freitas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The injection of syntactic information in Variational AutoEncoders (VAEs) has
been shown to result in an overall improvement of performances and
generalisation. An effective strategy to achieve such a goal is to separate the
encoding of distributional semantic features and syntactic structures into
heterogeneous latent spaces via multi-task learning or dual encoder
architectures. However, existing works employing such techniques are limited to
LSTM-based VAEs. In this paper, we investigate latent space separation methods
for structural syntactic injection in Transformer-based VAE architectures
(i.e., Optimus). Specifically, we explore how syntactic structures can be
leveraged in the encoding stage through the integration of graph-based and
sequential models, and how multiple, specialised latent representations can be
injected into the decoder's attention mechanism via low-rank operators. Our
empirical evaluation, carried out on natural language sentences and
mathematical expressions, reveals that the proposed end-to-end VAE architecture
can result in a better overall organisation of the latent space, alleviating
the information loss occurring in standard VAE setups, resulting in enhanced
performances on language modelling and downstream generation tasks.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08581" title="Abstract">arXiv:2311.08581</a> [<a href="/pdf/2311.08581" title="Download PDF">pdf</a>, <a href="/format/2311.08581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Drivable 3D Gaussian Avatars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zielonka%2C+W">Wojciech Zielonka</a>, 
<a href="/search/cs?searchtype=author&query=Bagautdinov%2C+T">Timur Bagautdinov</a>, 
<a href="/search/cs?searchtype=author&query=Saito%2C+S">Shunsuke Saito</a>, 
<a href="/search/cs?searchtype=author&query=Zollh%C3%B6fer%2C+M">Michael Zollh&#xf6;fer</a>, 
<a href="/search/cs?searchtype=author&query=Thies%2C+J">Justus Thies</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+J">Javier Romero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://zielon.github.io/d3ga/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable
model for human bodies rendered with Gaussian splats. Current photorealistic
drivable avatars require either accurate 3D registrations during training,
dense input images during testing, or both. The ones based on neural radiance
fields also tend to be prohibitively slow for telepresence applications. This
work uses the recently presented 3D Gaussian Splatting (3DGS) technique to
render realistic humans at real-time framerates, using dense calibrated
multi-view videos as input. To deform those primitives, we depart from the
commonly used point deformation method of linear blend skinning (LBS) and use a
classic volumetric deformation method: cage deformations. Given their smaller
size, we drive these deformations with joint angles and keypoints, which are
more suitable for communication applications. Our experiments on nine subjects
with varied body shapes, clothes, and motions obtain higher-quality results
than state-of-the-art methods when using the same training and test data.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08582" title="Abstract">arXiv:2311.08582</a> [<a href="/pdf/2311.08582" title="Download PDF">pdf</a>, <a href="/format/2311.08582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DREAMPlaceFPGA-MP: An Open-Source GPU-Accelerated Macro Placer for  Modern FPGAs with Cascade Shapes and Region Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhili Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Rajarathnam%2C+R+S">Rachel Selina Rajarathnam</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhixing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+D+Z">David Z. Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">FPGA macro placement plays a pivotal role in routability and timing closer to
the modern FPGA physical design flow. In modern FPGAs, macros could be subject
to complex cascade shape constraints requiring instances to be placed in
consecutive sites. In addition, in real-world FPGA macro placement scenarios,
designs could have various region constraints that specify boundaries within
which certain design instances and macros should be placed. In this work, we
present DREAMPlaceFPGA-MP, an open-source GPU-accelerated FPGA macro-placer
that efficiently generates legal placements for macros while honoring cascade
shape requirements and region constraints. Treating multiple macros in a
cascade shape as a large single instance and restricting instances to their
respective regions, DREAMPlaceFPGA-MP obtains roughly legal placements. The
macros are legalized in multiple steps to efficiently handle cascade shapes and
region constraints. Our experimental results demonstrate that DREAMPlaceFPGA-MP
is among the top contestants of the MLCAD 2023 FPGA Macro-Placement Contest.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08583" title="Abstract">arXiv:2311.08583</a> [<a href="/pdf/2311.08583" title="Download PDF">pdf</a>, <a href="/ps/2311.08583" title="Download PostScript">ps</a>, <a href="/format/2311.08583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOSAIC: A Multi-Objective Optimization Framework for Sustainable  Datacenter Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+S">Sirui Qi</a>, 
<a href="/search/cs?searchtype=author&query=Milojicic%2C+D">Dejan Milojicic</a>, 
<a href="/search/cs?searchtype=author&query=Bash%2C+C">Cullen Bash</a>, 
<a href="/search/cs?searchtype=author&query=Pasricha%2C+S">Sudeep Pasricha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In recent years, cloud service providers have been building and hosting
datacenters across multiple geographical locations to provide robust services.
However, the geographical distribution of datacenters introduces growing
pressure to both local and global environments, particularly when it comes to
water usage and carbon emissions. Unfortunately, efforts to reduce the
environmental impact of such datacenters often lead to an increase in the cost
of datacenter operations. To co-optimize the energy cost, carbon emissions, and
water footprint of datacenter operation from a global perspective, we propose a
novel framework for multi-objective sustainable datacenter management (MOSAIC)
that integrates adaptive local search with a collaborative decomposition-based
evolutionary algorithm to intelligently manage geographical workload
distribution and datacenter operations. Our framework sustainably allocates
workloads to datacenters while taking into account multiple geography- and
time-based factors including renewable energy sources, variable energy costs,
power usage efficiency, carbon factors, and water intensity in energy. Our
experimental results show that, compared to the best-known prior work
frameworks, MOSAIC can achieve 27.45x speedup and 1.53x improvement in Pareto
Hypervolume while reducing the carbon footprint by up to 1.33x, water footprint
by up to 3.09x, and energy costs by up to 1.40x. In the simultaneous
three-objective co-optimization scenario, MOSAIC achieves a cumulative
improvement across all objectives (carbon, water, cost) of up to 4.61x compared
to the state-of-the-arts.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08584" title="Abstract">arXiv:2311.08584</a> [<a href="/pdf/2311.08584" title="Download PDF">pdf</a>, <a href="/format/2311.08584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asking More Informative Questions for Grounded Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keh%2C+S">Sedrick Keh</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+J+T">Justin T. Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+D">Daniel Fried</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">When a model is trying to gather information in an interactive setting, it
benefits from asking informative questions. However, in the case of a grounded
multi-turn image identification task, previous studies have been constrained to
polar yes/no questions, limiting how much information the model can gain in a
single turn. We present an approach that formulates more informative,
open-ended questions. In doing so, we discover that off-the-shelf visual
question answering (VQA) models often make presupposition errors, which
standard information gain question selection methods fail to account for. To
address this issue, we propose a method that can incorporate presupposition
handling into both question selection and belief updates. Specifically, we use
a two-stage process, where the model first filters out images which are
irrelevant to a given question, then updates its beliefs about which image the
user intends. Through self-play and human evaluations, we show that our method
is successful in asking informative open-ended questions, increasing accuracy
over the past state-of-the-art by 14%, while resulting in 48% more efficient
games in human evaluations.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08588" title="Abstract">arXiv:2311.08588</a> [<a href="/pdf/2311.08588" title="Download PDF">pdf</a>, <a href="/format/2311.08588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeScope: An Execution-based Multilingual Multitask Multidimensional  Benchmark for Evaluating LLMs on Code Understanding and Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Weixiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haitian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunkun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tingyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weishan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Li Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shuiguang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sundaram%2C+H">Hari Sundaram</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable performance on
coding related tasks, particularly on assisting humans in programming and
facilitating programming automation. However, existing benchmarks for
evaluating the code understanding and generation capacities of LLMs suffer from
severe limitations. First, most benchmarks are deficient as they focus on a
narrow range of popular programming languages and specific tasks, whereas the
real-world software development scenarios show dire need to implement systems
with multilingual programming environments to satisfy diverse requirements.
Practical programming practices also strongly expect multi-task settings for
testing coding capabilities of LLMs comprehensively and robustly. Second, most
benchmarks also fail to consider the actual executability and the consistency
of execution results of the generated code. To bridge these gaps between
existing benchmarks and expectations from practical applications, we introduce
CodeScope, an execution-based, multilingual, multi-task, multi-dimensional
evaluation benchmark for comprehensively gauging LLM capabilities on coding
tasks. CodeScope covers 43 programming languages and 8 coding tasks. It
evaluates the coding performance of LLMs from three dimensions (perspectives):
difficulty, efficiency, and length. To facilitate execution-based evaluations
of code generation, we develop MultiCodeEngine, an automated code execution
engine that supports 14 programming languages. Finally, we systematically
evaluate and analyze 8 mainstream LLMs on CodeScope tasks and demonstrate the
superior breadth and challenges of CodeScope for evaluating LLMs on code
understanding and generation tasks compared to other benchmarks. The CodeScope
benchmark and datasets are publicly available at
https://github.com/WeixiangYAN/CodeScope.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08589" title="Abstract">arXiv:2311.08589</a> [<a href="/pdf/2311.08589" title="Download PDF">pdf</a>, <a href="/format/2311.08589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Carbon Responder: Coordinating Demand Response for the Datacenter Fleet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+J">Jiali Xing</a>, 
<a href="/search/cs?searchtype=author&query=Acun%2C+B">Bilge Acun</a>, 
<a href="/search/cs?searchtype=author&query=Sundarrajan%2C+A">Aditya Sundarrajan</a>, 
<a href="/search/cs?searchtype=author&query=Brooks%2C+D">David Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Chakkaravarthy%2C+M">Manoj Chakkaravarthy</a>, 
<a href="/search/cs?searchtype=author&query=Avila%2C+N">Nikky Avila</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Carole-Jean Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B+C">Benjamin C. Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">The increasing integration of renewable energy sources results in
fluctuations in carbon intensity throughout the day. To mitigate their carbon
footprint, datacenters can implement demand response (DR) by adjusting their
load based on grid signals. However, this presents challenges for private
datacenters with diverse workloads and services. One of the key challenges is
efficiently and fairly allocating power curtailment across different workloads.
In response to these challenges, we propose the Carbon Responder framework.
<br />The Carbon Responder framework aims to reduce the carbon footprint of
heterogeneous workloads in datacenters by modulating their power usage. Unlike
previous studies, Carbon Responder considers both online and batch workloads
with different service level objectives and develops accurate performance
models to achieve performance-aware power allocation. The framework supports
three alternative policies: Efficient DR, Fair and Centralized DR, and Fair and
Decentralized DR. We evaluate Carbon Responder polices using production
workload traces from a private hyperscale datacenter. Our experimental results
demonstrate that the efficient Carbon Responder policy reduces the carbon
footprint by around 2x as much compared to baseline approaches adapted from
existing methods. The fair Carbon Responder policies distribute the performance
penalties and carbon reduction responsibility fairly among workloads.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08590" title="Abstract">arXiv:2311.08590</a> [<a href="/pdf/2311.08590" title="Download PDF">pdf</a>, <a href="/format/2311.08590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEMA: Plug-in External Memory Adaptation for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">HyunJin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y+J">Young Jin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bak%2C+J">JinYeong Bak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained language models (PLMs) have demonstrated impressive performance
across various downstream NLP tasks. Nevertheless, the resource requirements of
pre-training large language models in terms of memory and training compute pose
significant challenges. Furthermore, due to the substantial resources required,
many PLM weights are confidential. Consequently, users are compelled to share
their data with model owners for fine-tuning on specific tasks. To overcome the
limitations, we introduce Plug-in External Memory Adaptation (PEMA), a
Parameter-Efficient Fine-Tuning (PEFT) approach designed for fine-tuning PLMs
without the need for all weights. PEMA can be integrated into the context
representation of test data during inference to execute downstream tasks. It
leverages an external memory to store context representations generated by a
PLM, mapped with the desired target word. Our method entails training
LoRA-based weight matrices within the final layer of the PLM for enhanced
efficiency. The probability is then interpolated with the next-word
distribution from the PLM to perform downstream tasks. To improve the
generation quality, we propose a novel interpolation strategy named Gradual
Unrolling. To demonstrate the effectiveness of our proposed method, we conduct
experiments to demonstrate the efficacy of PEMA with a syntactic dataset and
assess its performance on machine translation and style transfer tasks using
real datasets. PEMA outperforms other PEFT methods in terms of memory and
latency efficiency for training and inference. Furthermore, it outperforms
other baselines in preserving the meaning of sentences while generating
appropriate language and styles.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08592" title="Abstract">arXiv:2311.08592</a> [<a href="/pdf/2311.08592" title="Download PDF">pdf</a>, <a href="/format/2311.08592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AART: AI-Assisted Red-Teaming with Diverse Data Generation for New  LLM-powered Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radharapu%2C+B">Bhaktipriya Radharapu</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+K">Kevin Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Aroyo%2C+L">Lora Aroyo</a>, 
<a href="/search/cs?searchtype=author&query=Lahoti%2C+P">Preethi Lahoti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Adversarial testing of large language models (LLMs) is crucial for their safe
and responsible deployment. We introduce a novel approach for automated
generation of adversarial evaluation datasets to test the safety of LLM
generations on new downstream applications. We call it AI-assisted Red-Teaming
(AART) - an automated alternative to current manual red-teaming efforts. AART
offers a data generation and augmentation pipeline of reusable and customizable
recipes that reduce human effort significantly and enable integration of
adversarial testing earlier in new product development. AART generates
evaluation datasets with high diversity of content characteristics critical for
effective adversarial testing (e.g. sensitive and harmful concepts, specific to
a wide range of cultural and geographic regions and application scenarios). The
data generation is steered by AI-assisted recipes to define, scope and
prioritize diversity within the application context. This feeds into a
structured LLM-generation process that scales up evaluation priorities.
Compared to some state-of-the-art tools, AART shows promising results in terms
of concept coverage and data quality.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08593" title="Abstract">arXiv:2311.08593</a> [<a href="/pdf/2311.08593" title="Download PDF">pdf</a>, <a href="/format/2311.08593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACID: Abstractive, Content-Based IDs for Document Retrieval with  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Keung%2C+P">Phillip Keung</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Daniel Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Kasai%2C+J">Jungo Kasai</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a new approach
for end-to-end document retrieval that directly generates document identifiers
given an input query. Techniques for designing effective, high-quality document
IDs remain largely unexplored. We introduce ACID, in which each document's ID
is composed of abstractive keyphrases generated by a large language model,
rather than an integer ID sequence as done in past work. We compare our method
with the current state-of-the-art technique for ID generation, which produces
IDs through hierarchical clustering of document embeddings. We also examine
simpler methods to generate natural-language document IDs, including the naive
approach of using the first k words of each document as its ID or words with
high BM25 scores in that document. We show that using ACID improves top-10 and
top-20 accuracy by 15.6% and 14.4% (relative) respectively versus the
state-of-the-art baseline on the MSMARCO 100k retrieval task, and 4.4% and 4.0%
respectively on the Natural Questions 100k retrieval task. Our results
demonstrate the effectiveness of human-readable, natural-language IDs in
generative retrieval with LMs. The code for reproducing our results and the
keyword-augmented datasets will be released on formal publication.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08594" title="Abstract">arXiv:2311.08594</a> [<a href="/pdf/2311.08594" title="Download PDF">pdf</a>, <a href="/format/2311.08594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Temporal IRT: Fast, Accurate, and Explainable Inference of  Dynamic Learner Proficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yunsung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sankaranarayanan%2C+S">Sreechan Sankaranarayanan</a>, 
<a href="/search/cs?searchtype=author&query=Piech%2C+C">Chris Piech</a>, 
<a href="/search/cs?searchtype=author&query=Thille%2C+C">Candace Thille</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 16th International Conference on Educational Data Mining (EDM'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Dynamic Item Response Models extend the standard Item Response Theory (IRT)
to capture temporal dynamics in learner ability. While these models have the
potential to allow instructional systems to actively monitor the evolution of
learner proficiency in real time, existing dynamic item response models rely on
expensive inference algorithms that scale poorly to massive datasets. In this
work, we propose Variational Temporal IRT (VTIRT) for fast and accurate
inference of dynamic learner proficiency. VTIRT offers orders of magnitude
speedup in inference runtime while still providing accurate inference.
Moreover, the proposed algorithm is intrinsically interpretable by virtue of
its modular design. When applied to 9 real student datasets, VTIRT consistently
yields improvements in predicting future learner performance over other learner
proficiency models.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08595" title="Abstract">arXiv:2311.08595</a> [<a href="/pdf/2311.08595" title="Download PDF">pdf</a>, <a href="/format/2311.08595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Parallel Tensor Times Same Vector for Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shivakumar%2C+S">Shruti Shivakumar</a>, 
<a href="/search/math?searchtype=author&query=Amburg%2C+I">Ilya Amburg</a>, 
<a href="/search/math?searchtype=author&query=Aksoy%2C+S+G">Sinan G. Aksoy</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jiajia Li</a>, 
<a href="/search/math?searchtype=author&query=Young%2C+S+J">Stephen J. Young</a>, 
<a href="/search/math?searchtype=author&query=Aluru%2C+S">Srinivas Aluru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Combinatorics (math.CO)

</div>
<p class="mathjax">Hypergraphs are a popular paradigm to represent complex real-world networks
exhibiting multi-way relationships of varying sizes. Mining centrality in
hypergraphs via symmetric adjacency tensors has only recently become
computationally feasible for large and complex datasets. To enable scalable
computation of these and related hypergraph analytics, here we focus on the
Sparse Symmetric Tensor Times Same Vector (S$^3$TTVc) operation. We introduce
the Compound Compressed Sparse Symmetric (CCSS) format, an extension of the
compact CSS format for hypergraphs of varying hyperedge sizes and present a
shared-memory parallel algorithm to compute S$^3$TTVc. We experimentally show
S$^3$TTVc computation using the CCSS format achieves better performance than
the naive baseline, and is subsequently more performant for hypergraph
$H$-eigenvector centrality.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08596" title="Abstract">arXiv:2311.08596</a> [<a href="/pdf/2311.08596" title="Download PDF">pdf</a>, <a href="/format/2311.08596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are You Sure? Challenging LLMs Leads to Performance Drops in The  FlipFlop Experiment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laban%2C+P">Philippe Laban</a>, 
<a href="/search/cs?searchtype=author&query=Murakhovs%27ka%2C+L">Lidiya Murakhovs&#x27;ka</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chien-Sheng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The interactive nature of Large Language Models (LLMs) theoretically allows
models to refine and improve their answers, yet systematic analysis of the
multi-turn behavior of LLMs remains limited. In this paper, we propose the
FlipFlop experiment: in the first round of the conversation, an LLM responds to
a prompt containing a classification task. In a second round, the LLM is
challenged with a follow-up phrase like "Are you sure?", offering an
opportunity for the model to reflect on its initial answer, and decide whether
to confirm or flip its answer. A systematic study of nine LLMs on seven
classification tasks reveals that models flip their answers on average 46% of
the time and that all models see a deterioration of accuracy between their
first and final prediction, with an average drop of 17%. The FlipFlop
experiment illustrates the universality of sycophantic behavior in LLMs and
provides a robust framework to analyze model behavior and evaluate potential
solutions.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08597" title="Abstract">arXiv:2311.08597</a> [<a href="/pdf/2311.08597" title="Download PDF">pdf</a>, <a href="/format/2311.08597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stopping Methods for Technology Assisted Reviews based on Point  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stevenson%2C+M">Mark Stevenson</a>, 
<a href="/search/cs?searchtype=author&query=Bin-Hezam%2C+R">Reem Bin-Hezam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Transactions on Information Systems (TOIS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Technology Assisted Review (TAR), which aims to reduce the effort required to
screen collections of documents for relevance, is used to develop systematic
reviews of medical evidence and identify documents that must be disclosed in
response to legal proceedings. Stopping methods are algorithms which determine
when to stop screening documents during the TAR process, helping to ensure that
workload is minimised while still achieving a high level of recall. This paper
proposes a novel stopping method based on point processes, which are
statistical models that can be used to represent the occurrence of random
events. The approach uses rate functions to model the occurrence of relevant
documents in the ranking and compares four candidates, including one that has
not previously been used for this purpose (hyperbolic). Evaluation is carried
out using standard datasets (CLEF e-Health, TREC Total Recall, TREC Legal), and
this work is the first to explore stopping method robustness by reporting
performance on a range of rankings of varying effectiveness. Results show that
the proposed method achieves the desired level of recall without requiring an
excessive number of documents to be examined in the majority of cases and also
compares well against multiple alternative approaches.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08598" title="Abstract">arXiv:2311.08598</a> [<a href="/pdf/2311.08598" title="Download PDF">pdf</a>, <a href="/format/2311.08598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DALA: A Distribution-Aware LoRA-Based Adversarial Attack against  Pre-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiangjue Dong</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained language models (PLMs) that achieve success in applications are
susceptible to adversarial attack methods that are capable of generating
adversarial examples with minor perturbations. Although recent attack methods
can achieve a relatively high attack success rate (ASR), our observation shows
that the generated adversarial examples have a different data distribution
compared with the original examples. Specifically, these adversarial examples
exhibit lower confidence levels and higher distance to the training data
distribution. As a result, they are easy to detect using very simple detection
methods, diminishing the actual effectiveness of these attack methods. To solve
this problem, we propose a Distribution-Aware LoRA-based Adversarial Attack
(DALA) method, which considers the distribution shift of adversarial examples
to improve attack effectiveness under detection methods. We further design a
new evaluation metric NASR combining ASR and detection for the attack task. We
conduct experiments on four widely-used datasets and validate the attack
effectiveness on ASR and NASR of the adversarial examples generated by DALA on
the BERT-base model and the black-box LLaMA2-7b model.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08600" title="Abstract">arXiv:2311.08600</a> [<a href="/pdf/2311.08600" title="Download PDF">pdf</a>, <a href="/format/2311.08600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Derivation of sixth-order exponential Runge--Kutta methods for stiff  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luan%2C+V+T">Vu Thai Luan</a>, 
<a href="/search/math?searchtype=author&query=Alhsmy%2C+T">Trky Alhsmy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work constructs the first-ever sixth-order exponential Runge--Kutta
(ExpRK) methods for the time integration of stiff parabolic PDEs. First, we
leverage the exponential B-series theory to restate the stiff order conditions
for ExpRK methods of arbitrary order based on an essential set of trees only.
Then, we explicitly provide the 36 order conditions required for sixth-order
methods and present convergence results. In addition, we are able to solve the
36 stiff order conditions in both their weak and strong forms, resulting in two
families of sixth-order parallel stages ExpRK schemes. Interestingly, while
these new schemes require a high number of stages, they can be implemented
efficiently similar to the cost of a 6-stage method. Numerical experiments are
given to confirm the accuracy and efficiency of the new schemes.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08605" title="Abstract">arXiv:2311.08605</a> [<a href="/pdf/2311.08605" title="Download PDF">pdf</a>, <a href="/format/2311.08605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating the Ocean of Biases: Political Bias Attribution in Language  Models via Causal Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jenny%2C+D+F">David F. Jenny</a>, 
<a href="/search/cs?searchtype=author&query=Billeter%2C+Y">Yann Billeter</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhijing Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The rapid advancement of Large Language Models (LLMs) has sparked intense
debate regarding their ability to perceive and interpret complex
socio-political landscapes. In this study, we undertake an exploration of
decision-making processes and inherent biases within LLMs, exemplified by
ChatGPT, specifically contextualizing our analysis within political debates. We
aim not to critique or validate LLMs' values, but rather to discern how they
interpret and adjudicate "good arguments." By applying Activity Dependency
Networks (ADNs), we extract the LLMs' implicit criteria for such assessments
and illustrate how normative values influence these perceptions. We discuss the
consequences of our findings for human-AI alignment and bias mitigation. Our
code and data at https://github.com/david-jenny/LLM-Political-Study.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08607" title="Abstract">arXiv:2311.08607</a> [<a href="/pdf/2311.08607" title="Download PDF">pdf</a>, <a href="/format/2311.08607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Generalizable SER: Soft Labeling and Data Augmentation for  Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osman%2C+M">Mohamed Osman</a>, 
<a href="/search/cs?searchtype=author&query=Nadeem%2C+T">Tamer Nadeem</a>, 
<a href="/search/cs?searchtype=author&query=Khoriba%2C+G">Ghada Khoriba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as talk at NeurIPS ML for Audio workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recognizing emotions in spoken communication is crucial for advanced
human-machine interaction. Current emotion detection methodologies often
display biases when applied cross-corpus. To address this, our study
amalgamates 16 diverse datasets, resulting in 375 hours of data across
languages like English, Chinese, and Japanese. We propose a soft labeling
system to capture gradational emotional intensities. Using the Whisper encoder
and data augmentation methods inspired by contrastive learning, our method
emphasizes the temporal dynamics of emotions. Our validation on four
multilingual datasets demonstrates notable zero-shot generalization. We publish
our open source model weights and initial promising results after fine-tuning
on Hume-Prosody.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08608" title="Abstract">arXiv:2311.08608</a> [<a href="/pdf/2311.08608" title="Download PDF">pdf</a>, <a href="/format/2311.08608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Radar Inertial Odometry for 3D State Estimation using mmWave  Imaging Radar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jui-Te Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruoyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hinduja%2C+A">Akshay Hinduja</a>, 
<a href="/search/cs?searchtype=author&query=Kaess%2C+M">Michael Kaess</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">State estimation is a crucial component for the successful implementation of
robotic systems, relying on sensors such as cameras, LiDAR, and IMUs. However,
in real-world scenarios, the performance of these sensors is degraded by
challenging environments, e.g. adverse weather conditions and low-light
scenarios. The emerging 4D imaging radar technology is capable of providing
robust perception in adverse conditions. Despite its potential, challenges
remain for indoor settings where noisy radar data does not present clear
geometric features. Moreover, disparities in radar data resolution and field of
view (FOV) can lead to inaccurate measurements. While prior research has
explored radar-inertial odometry based on Doppler velocity information,
challenges remain for the estimation of 3D motion because of the discrepancy in
the FOV and resolution of the radar sensor. In this paper, we address Doppler
velocity measurement uncertainties. We present a method to optimize body frame
velocity while managing Doppler velocity uncertainty. Based on our
observations, we propose a dual imaging radar configuration to mitigate the
challenge of discrepancy in radar data. To attain high-precision 3D state
estimation, we introduce a strategy that seamlessly integrates radar data with
a consumer-grade IMU sensor using fixed-lag smoothing optimization. Finally, we
evaluate our approach using real-world 3D motion data.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08610" title="Abstract">arXiv:2311.08610</a> [<a href="/pdf/2311.08610" title="Download PDF">pdf</a>, <a href="/format/2311.08610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Converting Transformers to Polynomial Form for Secure Inference Over  Homomorphic Encryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zimerman%2C+I">Itamar Zimerman</a>, 
<a href="/search/cs?searchtype=author&query=Baruch%2C+M">Moran Baruch</a>, 
<a href="/search/cs?searchtype=author&query=Drucker%2C+N">Nir Drucker</a>, 
<a href="/search/cs?searchtype=author&query=Ezov%2C+G">Gilad Ezov</a>, 
<a href="/search/cs?searchtype=author&query=Soceanu%2C+O">Omri Soceanu</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+L">Lior Wolf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Designing privacy-preserving deep learning models is a major challenge within
the deep learning community. Homomorphic Encryption (HE) has emerged as one of
the most promising approaches in this realm, enabling the decoupling of
knowledge between the model owner and the data owner. Despite extensive
research and application of this technology, primarily in convolutional neural
networks, incorporating HE into transformer models has been challenging because
of the difficulties in converting these models into a polynomial form. We break
new ground by introducing the first polynomial transformer, providing the first
demonstration of secure inference over HE with transformers. This includes a
transformer architecture tailored for HE, alongside a novel method for
converting operators to their polynomial equivalent. This innovation enables us
to perform secure inference on LMs with WikiText-103. It also allows us to
perform image classification with CIFAR-100 and Tiny-ImageNet. Our models yield
results comparable to traditional methods, bridging the performance gap with
transformers of similar scale and underscoring the viability of HE for
state-of-the-art applications. Finally, we assess the stability of our models
and conduct a series of ablations to quantify the contribution of each model
component.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08614" title="Abstract">arXiv:2311.08614</a> [<a href="/pdf/2311.08614" title="Download PDF">pdf</a>, <a href="/format/2311.08614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XplainLLM: A QA Explanation Dataset for Understanding LLM  Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zichen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianda Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gaidhani%2C+M">Mitali Gaidhani</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Ambuj Singh</a>, 
<a href="/search/cs?searchtype=author&query=Sra%2C+M">Misha Sra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures, 7 tables. Our dataset is available at: <a href="https://github.com/chen-zichen/XplainLLM_dataset.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have recently made impressive strides in natural
language understanding tasks. Despite their remarkable performance,
understanding their decision-making process remains a big challenge. In this
paper, we look into bringing some transparency to this process by introducing a
new explanation dataset for question answering (QA) tasks that integrates
knowledge graphs (KGs) in a novel way. Our dataset includes 12,102
question-answer-explanation (QAE) triples. Each explanation in the dataset
links the LLM's reasoning to entities and relations in the KGs. The explanation
component includes a why-choose explanation, a why-not-choose explanation, and
a set of reason-elements that underlie the LLM's decision. We leverage KGs and
graph attention networks (GAT) to find the reason-elements and transform them
into why-choose and why-not-choose explanations that are comprehensible to
humans. Through quantitative and qualitative evaluations, we demonstrate the
potential of our dataset to improve the in-context learning of LLMs, and
enhance their interpretability and explainability. Our work contributes to the
field of explainable AI by enabling a deeper understanding of the LLMs
decision-making process to make them more transparent and thereby, potentially
more reliable, to researchers and practitioners alike. Our dataset is available
at: https://github.com/chen-zichen/XplainLLM_dataset.git
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08618" title="Abstract">arXiv:2311.08618</a> [<a href="/pdf/2311.08618" title="Download PDF">pdf</a>, <a href="/format/2311.08618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the k-th Eigenvalue of Symmetric $H^2$-Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Apriansyah%2C+M+R">M. Ridwan Apriansyah</a>, 
<a href="/search/math?searchtype=author&query=Yokota%2C+R">Rio Yokota</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 52nd International Conference on Parallel
  Processing (2023) 11-20
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The numerical solution of eigenvalue problems is essential in various
application areas of scientific and engineering domains. In many problem
classes, the practical interest is only a small subset of eigenvalues so it is
unnecessary to compute all of the eigenvalues. Notable examples are the
electronic structure problems where the $k$-th smallest eigenvalue is closely
related to the electronic properties of materials. In this paper, we consider
the $k$-th eigenvalue problems of symmetric dense matrices with low-rank
off-diagonal blocks. We present a linear time generalized LDL decomposition of
$\mathcal{H}^2$ matrices and combine it with the bisection eigenvalue algorithm
to compute the $k$-th eigenvalue with controllable accuracy. In addition, if
more than one eigenvalue is required, some of the previous computations can be
reused to compute the other eigenvalues in parallel. Numerical experiments show
that our method is more efficient than the state-of-the-art dense eigenvalue
solver in LAPACK/ScaLAPACK and ELPA. Furthermore, tests on electronic state
calculations of carbon nanomaterials demonstrate that our method outperforms
the existing HSS-based bisection eigenvalue algorithm on 3D problems.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08620" title="Abstract">arXiv:2311.08620</a> [<a href="/pdf/2311.08620" title="Download PDF">pdf</a>, <a href="/format/2311.08620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toucan: Token-Aware Character Level Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fleshman%2C+W">William Fleshman</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Character-level language models obviate the need for separately trained
tokenizers, but efficiency suffers from longer sequence lengths. Learning to
combine character representations into tokens has made training these models
more efficient, but they still require decoding characters individually. We
propose Toucan, an augmentation to character-level models to make them
"token-aware". Comparing our method to prior work, we demonstrate significant
speed-ups in character generation without a loss in language modeling
performance. We then explore differences between our learned dynamic
tokenization of character sequences with popular fixed vocabulary solutions
such as Byte-Pair Encoding and WordPiece, finding our approach leads to a
greater amount of longer sequences tokenized as single items. Our project and
code are available at https://nlp.jhu.edu/nuggets/.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08621" title="Abstract">arXiv:2311.08621</a> [<a href="/pdf/2311.08621" title="Download PDF">pdf</a>, <a href="/format/2311.08621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross Device Federated Intrusion Detector for Early Stage Botnet  Propagation in IoT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Famera%2C+A+G">Angela Grace Famera</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+R+M">Raj Mani Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Bhunia%2C+S">Suman Bhunia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper submitted to conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">A botnet is an army of zombified computers infected with malware and
controlled by malicious actors to carry out tasks such as Distributed Denial of
Service (DDoS) attacks. Billions of Internet of Things (IoT) devices are
primarily targeted to be infected as bots since they are configured with weak
credentials or contain common vulnerabilities. Detecting botnet propagation by
monitoring the network traffic is difficult as they easily blend in with
regular network traffic. The traditional machine learning (ML) based Intrusion
Detection System (IDS) requires the raw data to be captured and sent to the ML
processor to detect intrusion. In this research, we examine the viability of a
cross-device federated intrusion detection mechanism where each device runs the
ML model on its data and updates the model weights to the central coordinator.
This mechanism ensures the client's data is not shared with any third party,
terminating privacy leakage. The model examines each data packet separately and
predicts anomalies. We evaluate our proposed mechanism on a real botnet
propagation dataset called MedBIoT. Overall, the proposed method produces an
average accuracy of 71%, precision 78%, recall 71%, and f1-score 68%. In
addition, we also examined whether any device taking part in federated learning
can employ a poisoning attack on the overall system.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08622" title="Abstract">arXiv:2311.08622</a> [<a href="/pdf/2311.08622" title="Download PDF">pdf</a>, <a href="/format/2311.08622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple-Question Multiple-Answer Text-VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+P">Peng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Appalaraju%2C+S">Srikar Appalaraju</a>, 
<a href="/search/cs?searchtype=author&query=Manmatha%2C+R">R. Manmatha</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yusheng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Mahadevan%2C+V">Vijay Mahadevan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do
text-VQA in encoder-decoder transformer models. The text-VQA task requires a
model to answer a question by understanding multi-modal content: text
(typically from OCR) and an associated image. To the best of our knowledge,
almost all previous approaches for text-VQA process a single question and its
associated content to predict a single answer. In order to answer multiple
questions from the same image, each question and content are fed into the model
multiple times. In contrast, our proposed MQMA approach takes multiple
questions and content as input at the encoder and predicts multiple answers at
the decoder in an auto-regressive manner at the same time. We make several
novel architectural modifications to standard encoder-decoder transformers to
support MQMA. We also propose a novel MQMA denoising pre-training task which is
designed to teach the model to align and delineate multiple questions and
content with associated answers. MQMA pre-trained model achieves
state-of-the-art results on multiple text-VQA datasets, each with strong
baselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%),
DocVQA (+1.1%) absolute improvements over the previous state-of-the-art
approaches.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08623" title="Abstract">arXiv:2311.08623</a> [<a href="/pdf/2311.08623" title="Download PDF">pdf</a>, <a href="/format/2311.08623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder  Transformer Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+P">Peng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+P">Pengkai Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tian Li</a>, 
<a href="/search/cs?searchtype=author&query=Appalaraju%2C+S">Srikar Appalaraju</a>, 
<a href="/search/cs?searchtype=author&query=Mahadevan%2C+V">Vijay Mahadevan</a>, 
<a href="/search/cs?searchtype=author&query=Manmatha%2C+R">R. Manmatha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Encoder-decoder transformer models have achieved great success on various
vision-language (VL) tasks, but they suffer from high inference latency.
Typically, the decoder takes up most of the latency because of the
auto-regressive decoding. To accelerate the inference, we propose an approach
of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit
encoder-decoder transformer model which is trained with deep supervision so
that each of its decoder layers is capable of generating plausible predictions.
In addition, we leverage simple yet practical techniques, including shared
generation head and adaptation modules, to keep accuracy when exiting at
shallow decoder layers. Based on the multi-exit model, we perform step-level
dynamic early exit during inference, where the model may decide to use fewer
decoder layers based on its confidence of the current layer at each individual
decoding step. Considering different number of decoder layers may be used at
different decoding steps, we compute deeper-layer decoder features of previous
decoding steps just-in-time, which ensures the features from different decoding
steps are semantically aligned. We evaluate our approach with two
state-of-the-art encoder-decoder transformer models on various VL tasks. We
show our approach can reduce overall inference latency by 30%-60% with
comparable or even higher accuracy compared to baselines.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08625" title="Abstract">arXiv:2311.08625</a> [<a href="/pdf/2311.08625" title="Download PDF">pdf</a>, <a href="/format/2311.08625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Statistical Verification Method of Random Permutations for Hiding  Countermeasure Against Side-Channel Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jong-Yeon Park</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+J">Jang-Won Ju</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+W">Wonil Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+B">Bo-Gyeong Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kachi%2C+Y">Yasuyuki Kachi</a>, 
<a href="/search/cs?searchtype=author&query=Sakurai%2C+K">Kouichi Sakurai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">As NIST is putting the final touches on the standardization of PQC (Post
Quantum Cryptography) public key algorithms, it is a racing certainty that
peskier cryptographic attacks undeterred by those new PQC algorithms will
surface. Such a trend in turn will prompt more follow-up studies of attacks and
countermeasures. As things stand, from the attackers' perspective, one viable
form of attack that can be implemented thereupon is the so-called "side-channel
attack". Two best-known countermeasures heralded to be durable against
side-channel attacks are: "masking" and "hiding". In that dichotomous picture,
of particular note are successful single-trace attacks on some of the NIST's
PQC then-candidates, which worked to the detriment of the former: "masking". In
this paper, we cast an eye over the latter: "hiding". Hiding proves to be
durable against both side-channel attacks and another equally robust type of
attacks called "fault injection attacks", and hence is deemed an auspicious
countermeasure to be implemented. Mathematically, the hiding method is
fundamentally based on random permutations. There has been a cornucopia of
studies on generating random permutations. However, those are not tied to
implementation of the hiding method. In this paper, we propose a reliable and
efficient verification of permutation implementation, through employing
Fisher-Yates' shuffling method. We introduce the concept of an n-th order
permutation and explain how it can be used to verify that our implementation is
more efficient than its previous-gen counterparts for hiding countermeasures.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08631" title="Abstract">arXiv:2311.08631</a> [<a href="/pdf/2311.08631" title="Download PDF">pdf</a>, <a href="/format/2311.08631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influence of Video Dynamics on EEG-based Single-Trial Video Target  Surveillance Syste
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwak%2C+H">Heon-Gyu Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sung-Jin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Hyeon-Taek Han</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Ji-Hoon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BCI winter 2024 conference submitting paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Target detection models are one of the widely used deep learning-based
applications for reducing human efforts on video surveillance and patrol.
However, the application of conventional computer vision-based target detection
models in military usage can result in limited performance, due to the lack of
sample data of hostile targets. In this paper, we present the possibility of
the electroencephalography-based video target detection model, which could be
applied as a supportive module of the military video surveillance system. The
proposed framework and detection model showed prospective performance achieving
a mean macro $F_{\beta}$ of 0.6522 with asynchronous real-time data from five
subjects, in a certain video stimulus, but not on some video stimuli. By
analyzing the results of experiments using each video stimulus, we present the
factors that would affect the performance of electroencephalography-based video
target detection models.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08635" title="Abstract">arXiv:2311.08635</a> [<a href="/pdf/2311.08635" title="Download PDF">pdf</a>, <a href="/format/2311.08635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+G">Guangyin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fuxian Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jincai Huang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the AAAI Conference on Artificial Intelligence,
  37(12), 14268-14276 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Traffic congestion event prediction is an important yet challenging task in
intelligent transportation systems. Many existing works about traffic
prediction integrate various temporal encoders and graph convolution networks
(GCNs), called spatio-temporal graph-based neural networks, which focus on
predicting dense variables such as flow, speed and demand in time snapshots,
but they can hardly forecast the traffic congestion events that are sparsely
distributed on the continuous time axis. In recent years, neural point process
(NPP) has emerged as an appropriate framework for event prediction in
continuous time scenarios. However, most conventional works about NPP cannot
model the complex spatio-temporal dependencies and congestion evolution
patterns. To address these limitations, we propose a spatio-temporal graph
neural point process framework, named STGNPP for traffic congestion event
prediction. Specifically, we first design the spatio-temporal graph learning
module to fully capture the long-range spatio-temporal dependencies from the
historical traffic state data along with the road network. The extracted
spatio-temporal hidden representation and congestion event information are then
fed into a continuous gated recurrent unit to model the congestion evolution
patterns. In particular, to fully exploit the periodic information, we also
improve the intensity function calculation of the point process with a periodic
gated mechanism. Finally, our model simultaneously predicts the occurrence time
and duration of the next congestion. Extensive experiments on two real-world
datasets demonstrate that our method achieves superior performance in
comparison to existing state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08637" title="Abstract">arXiv:2311.08637</a> [<a href="/pdf/2311.08637" title="Download PDF">pdf</a>, <a href="/format/2311.08637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Proofs as Structured Explanations: Proposing Several Tasks on  Explainable Natural Language Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abzianidze%2C+L">Lasha Abzianidze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this position paper, we propose a way of exploiting formal proofs to put
forward several explainable natural language inference (NLI) tasks. The formal
proofs will be produced by a reliable and high-performing logic-based NLI
system. Taking advantage of the in-depth information available in the generated
formal proofs, we show how it can be used to define NLI tasks with structured
explanations. The proposed tasks can be ordered according to difficulty defined
in terms of the granularity of explanations. We argue that the tasks will
suffer with substantially fewer shortcomings than the existing explainable NLI
tasks (or datasets).
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08639" title="Abstract">arXiv:2311.08639</a> [<a href="/pdf/2311.08639" title="Download PDF">pdf</a>, <a href="/format/2311.08639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ColorTrace: Fungible token coloring and attribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zarick%2C+R">Ryan Zarick</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrino%2C+B">Bryan Pellegrino</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+I">Isaac Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Thomas Kim</a>, 
<a href="/search/cs?searchtype=author&query=Banister%2C+C">Caleb Banister</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">We formally define the fungible token coloring problem of attributing
(coloring) fungible tokens to originating entities (minters), and present, to
our knowledge, the first practical onchain algorithm to solve it. Tracking
attribution of colored tokens losslessly using existing approaches such as the
Colored Coins protocol is computationally intractable due to the per-wallet
storage requirements growing in proportion to the number of minters. Our first
contribution is an elegant solution to the single-chain token coloring problem,
where colored tokens are atomically burned and minted to ensure each wallet
only contains tokens of a single color. Our second contribution is an extension
to this single-chain token coloring algorithm to allow safe and efficient
crosschain token transfers. We present ColorTrace, an onchain algorithm to
achieve globally consistent, economically feasible, fungible token coloring.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08640" title="Abstract">arXiv:2311.08640</a> [<a href="/pdf/2311.08640" title="Download PDF">pdf</a>, <a href="/format/2311.08640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multistage Collaborative Knowledge Distillation from Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiachen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenlong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Drozdov%2C+A">Andrew Drozdov</a>, 
<a href="/search/cs?searchtype=author&query=Rozonoyer%2C+B">Benjamin Rozonoyer</a>, 
<a href="/search/cs?searchtype=author&query=Sultan%2C+M+A">Md Arafat Sultan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jay-Yoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Iyyer%2C+M">Mohit Iyyer</a>, 
<a href="/search/cs?searchtype=author&query=McCallum%2C+A">Andrew McCallum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study semi-supervised sequence prediction tasks where labeled data are too
scarce to effectively finetune a model and at the same time few-shot prompting
of a large language model (LLM) has suboptimal performance. This happens when a
task, such as parsing, is expensive to annotate and also unfamiliar to a
pretrained LLM. In this paper, we present a discovery that student models
distilled from a prompted LLM can often generalize better than their teacher on
such tasks. Leveraging this finding, we propose a new distillation method,
multistage collaborative knowledge distillation from an LLM (MCKD), for such
tasks. MCKD first prompts an LLM using few-shot in-context learning to produce
pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of
students are trained on disjoint partitions of the pseudolabeled data. Each
student subsequently produces new and improved pseudolabels for the unseen
partition to supervise the next round of student(s) with. We show the benefit
of multistage cross-partition labeling on two constituency parsing tasks. On
CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the
performance of supervised finetuning with 500 examples and outperforms the
prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08644" title="Abstract">arXiv:2311.08644</a> [<a href="/pdf/2311.08644" title="Download PDF">pdf</a>, <a href="/format/2311.08644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable by Design: Wrapper Boxes Combine Neural Performance with  Faithful Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yiheng Su</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+J">Juni Jessy Li</a>, 
<a href="/search/cs?searchtype=author&query=Lease%2C+M">Matthew Lease</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Can we preserve the accuracy of neural models while also providing faithful
explanations? We present wrapper boxes, a general approach to generate
faithful, example-based explanations for model predictions while maintaining
predictive performance. After training a neural model as usual, its learned
feature representation is input to a classic, interpretable model to perform
the actual prediction. This simple strategy is surprisingly effective, with
results largely comparable to those of the original neural model, as shown
across three large pre-trained language models, two datasets of varying scale,
four classic models, and four evaluation metrics. Moreover, because these
classic models are interpretable by design, the subset of training examples
that determine classic model predictions can be shown directly to users.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08646" title="Abstract">arXiv:2311.08646</a> [<a href="/pdf/2311.08646" title="Download PDF">pdf</a>, <a href="/format/2311.08646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Painterly Image Harmonization via Adversarial Residual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xudong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Li Niu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Junyan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liqing Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image compositing plays a vital role in photo editing. After inserting a
foreground object into another background image, the composite image may look
unnatural and inharmonious. When the foreground is photorealistic and the
background is an artistic painting, painterly image harmonization aims to
transfer the style of background painting to the foreground object, which is a
challenging task due to the large domain gap between foreground and background.
In this work, we employ adversarial learning to bridge the domain gap between
foreground feature map and background feature map. Specifically, we design a
dual-encoder generator, in which the residual encoder produces the residual
features added to the foreground feature map from main encoder. Then, a
pixel-wise discriminator plays against the generator, encouraging the refined
foreground feature map to be indistinguishable from background feature map.
Extensive experiments demonstrate that our method could achieve more harmonious
and visually appealing results than previous methods.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08648" title="Abstract">arXiv:2311.08648</a> [<a href="/pdf/2311.08648" title="Download PDF">pdf</a>, <a href="/format/2311.08648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explore Spurious Correlations at the Concept Level in Language Models  for Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Paiheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+W">Wei Ai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models (LMs) have gained great achievement in various NLP tasks for
both fine-tuning and in-context learning (ICL) methods. Despite its outstanding
performance, evidence shows that spurious correlations caused by imbalanced
label distributions in training data (or exemplars in ICL) lead to robustness
issues. However, previous studies mostly focus on word- and phrase-level
features and fail to tackle it from the concept level, partly due to the lack
of concept labels and subtle and diverse expressions of concepts in text. In
this paper, we first use the LLM to label the concept for each text and then
measure the concept bias of models for fine-tuning or ICL on the test data.
Second, we propose a data rebalancing method to mitigate the spurious
correlations by adding the LLM-generated counterfactual data to make a balanced
label distribution for each concept. We verify the effectiveness of our
mitigation method and show its superiority over the token removal method.
Overall, our results show that there exist label distribution biases in
concepts across multiple text classification datasets, and LMs will utilize
these shortcuts to make predictions in both fine-tuning and ICL methods.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08649" title="Abstract">arXiv:2311.08649</a> [<a href="/pdf/2311.08649" title="Download PDF">pdf</a>, <a href="/format/2311.08649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI  Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Juyeon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Feldt%2C+R">Robert Feldt</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Shin Yoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">GUI testing checks if a software system behaves as expected when users
interact with its graphical interface, e.g., testing specific functionality or
validating relevant use case scenarios. Currently, deciding what to test at
this high level is a manual task since automated GUI testing tools target lower
level adequacy metrics such as structural code coverage or activity coverage.
We propose DroidAgent, an autonomous GUI testing agent for Android, for
semantic, intent-driven automation of GUI testing. It is based on Large
Language Models and support mechanisms such as long- and short-term memory.
Given an Android app, DroidAgent sets relevant task goals and subsequently
tries to achieve them by interacting with the app. Our empirical evaluation of
DroidAgent using 15 apps from the Themis benchmark shows that it can set up and
perform realistic tasks, with a higher level of autonomy. For example, when
testing a messaging app, DroidAgent created a second account and added a first
account as a friend, testing a realistic use case, without human intervention.
On average, DroidAgent achieved 61% activity coverage, compared to 51% for
current state-of-the-art GUI testing techniques. Further, manual analysis shows
that 317 out of the 374 autonomously created tasks are realistic and relevant
to app functionalities, and also that DroidAgent interacts deeply with the apps
and covers more features.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08652" title="Abstract">arXiv:2311.08652</a> [<a href="/pdf/2311.08652" title="Download PDF">pdf</a>, <a href="/format/2311.08652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refining Perception Contracts: Case Studies in Vision-based Safe  Auto-landing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangge Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B+C">Benjamin C Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yixuan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+D">Daniel Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+S">Sayan Mitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Perception contracts provide a method for evaluating safety of control
systems that use machine learning for perception. A perception contract is a
specification for testing the ML components, and it gives a method for proving
end-to-end system-level safety requirements. The feasibility of contract-based
testing and assurance was established earlier in the context of straight lane
keeping: a 3-dimensional system with relatively simple dynamics. This paper
presents the analysis of two 6 and 12-dimensional flight control systems that
use multi-stage, heterogeneous, ML-enabled perception. The paper advances
methodology by introducing an algorithm for constructing data and requirement
guided refinement of perception contracts (DaRePC). The resulting analysis
provides testable contracts which establish the state and environment
conditions under which an aircraft can safety touchdown on the runway and a
drone can safely pass through a sequence of gates. It can also discover
conditions (e.g., low-horizon sun) that can possibly violate the safety of the
vision-based control system.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08655" title="Abstract">arXiv:2311.08655</a> [<a href="/pdf/2311.08655" title="Download PDF">pdf</a>, <a href="/ps/2311.08655" title="Download PostScript">ps</a>, <a href="/format/2311.08655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of AlexNet for Medical Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wenhao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Junding Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuihua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yudong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, the rapid development of deep learning has led to a wide
range of applications in the field of medical image classification. The
variants of neural network models with ever-increasing performance share some
commonalities: to try to mitigate overfitting, improve generalization, avoid
gradient vanishing and exploding, etc. AlexNet first utilizes the dropout
technique to mitigate overfitting and the ReLU activation function to avoid
gradient vanishing. Therefore, we focus our discussion on AlexNet, which has
contributed greatly to the development of CNNs in 2012. After reviewing over 40
papers, including journal papers and conference papers, we give a narrative on
the technical details, advantages, and application areas of AlexNet.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08656" title="Abstract">arXiv:2311.08656</a> [<a href="/pdf/2311.08656" title="Download PDF">pdf</a>, <a href="/format/2311.08656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling of the Electric Vehicle Charging Infrastructure as Cyber  Physical Power Systems: A Review on Components, Standards, Vulnerabilities  and Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mitikiri%2C+S+B">Sagar Babu Mitikiri</a>, 
<a href="/search/eess?searchtype=author&query=Babu%2C+K+V+S+M">K. Victor Sam Moses Babu</a>, 
<a href="/search/eess?searchtype=author&query=Dwivedi%2C+D">Divyanshi Dwivedi</a>, 
<a href="/search/eess?searchtype=author&query=Srinivas%2C+V+L">Vedantham Lakshmi Srinivas</a>, 
<a href="/search/eess?searchtype=author&query=Chakraborty%2C+P">Pratyush Chakraborty</a>, 
<a href="/search/eess?searchtype=author&query=Yemula%2C+P+K">Pradeep Kumar Yemula</a>, 
<a href="/search/eess?searchtype=author&query=Pal%2C+M">Mayukha Pal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The increasing number of electric vehicles (EVs) has led to the growing need
to establish EV charging infrastructures (EVCIs) with fast charging
capabilities to reduce congestion at the EV charging stations (EVCS) and also
provide alternative solutions for EV owners without residential charging
facilities. The EV charging stations are broadly classified based on i) where
the charging equipment is located - on-board and off-board charging stations,
and ii) the type of current and power levels - AC and DC charging stations. The
DC charging stations are further classified into fast and extreme fast charging
stations. This article focuses mainly on several components that model the EVCI
as a cyberphysical system (CPS).
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08657" title="Abstract">arXiv:2311.08657</a> [<a href="/pdf/2311.08657" title="Download PDF">pdf</a>, <a href="/format/2311.08657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConeQuest: A Benchmark for Cone Segmentation on Mars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Purohit%2C+M">Mirali Purohit</a>, 
<a href="/search/cs?searchtype=author&query=Adler%2C+J">Jacob Adler</a>, 
<a href="/search/cs?searchtype=author&query=Kerner%2C+H">Hannah Kerner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Over the years, space scientists have collected terabytes of Mars data from
satellites and rovers. One important set of features identified in Mars orbital
images is pitted cones, which are interpreted to be mud volcanoes believed to
form in regions that were once saturated in water (i.e., a lake or ocean).
Identifying pitted cones globally on Mars would be of great importance, but
expert geologists are unable to sort through the massive orbital image archives
to identify all examples. However, this task is well suited for computer
vision. Although several computer vision datasets exist for various
Mars-related tasks, there is currently no open-source dataset available for
cone detection/segmentation. Furthermore, previous studies trained models using
data from a single region, which limits their applicability for global
detection and mapping. Motivated by this, we introduce ConeQuest, the first
expert-annotated public dataset to identify cones on Mars. ConeQuest consists
of &gt;13k samples from 3 different regions of Mars. We propose two benchmark
tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size
Generalization. We finetune and evaluate widely-used segmentation models on
both benchmark tasks. Results indicate that cone segmentation is a challenging
open problem not solved by existing segmentation models, which achieve an
average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and
(ii), respectively. We believe this new benchmark dataset will facilitate the
development of more accurate and robust models for cone segmentation. Data and
code are available at https://github.com/kerner-lab/ConeQuest.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08662" title="Abstract">arXiv:2311.08662</a> [<a href="/pdf/2311.08662" title="Download PDF">pdf</a>, <a href="/format/2311.08662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Set Inoculation: Assessing Model Robustness Across Multiple  Challenge Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vatsal Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Pandya%2C+P">Pranshu Pandya</a>, 
<a href="/search/cs?searchtype=author&query=Kataria%2C+T">Tushar Kataria</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vivek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Dan Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 Figure, 12 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Language models, given their black-box nature, often exhibit sensitivity to
input perturbations, leading to trust issues due to hallucinations. To bolster
trust, it's essential to understand these models' failure modes and devise
strategies to enhance their performance. In this study, we propose a framework
to study the effect of input perturbations on language models of different
scales, from pre-trained models to large language models (LLMs). We use
fine-tuning to train a robust model to perturbations, and we investigate
whether exposure to one perturbation improves or degrades the model's
performance on other perturbations. To address multi-perturbation robustness,
we suggest three distinct training strategies. We also extend the framework to
LLMs via a chain of thought(COT) prompting with exemplars. We instantiate our
framework for the Tabular-NLI task and show that the proposed strategies train
the model robust to different perturbations without losing accuracy on a given
dataset.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08666" title="Abstract">arXiv:2311.08666</a> [<a href="/pdf/2311.08666" title="Download PDF">pdf</a>, <a href="/ps/2311.08666" title="Download PostScript">ps</a>, <a href="/format/2311.08666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> It Takes Two to Negotiate: Modeling Social Exchange in Online  Multiplayer Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaidka%2C+K">Kokil Jaidka</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+H">Hansin Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+L">Lynnette Ng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 11 figures. Accepted to CSCW '24 and forthcoming the Proceedings of ACM HCI '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
<p class="mathjax">Online games are dynamic environments where players interact with each other,
which offers a rich setting for understanding how players negotiate their way
through the game to an ultimate victory. This work studies online player
interactions during the turn-based strategy game, Diplomacy. We annotated a
dataset of over 10,000 chat messages for different negotiation strategies and
empirically examined their importance in predicting long- and short-term game
outcomes. Although negotiation strategies can be predicted reasonably
accurately through the linguistic modeling of the chat messages, more is needed
for predicting short-term outcomes such as trustworthiness. On the other hand,
they are essential in graph-aware reinforcement learning approaches to predict
long-term outcomes, such as a player's success, based on their prior
negotiation history. We close with a discussion of the implications and impact
of our work. The dataset is available at
https://github.com/kj2013/claff-diplomacy.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08667" title="Abstract">arXiv:2311.08667</a> [<a href="/pdf/2311.08667" title="Download PDF">pdf</a>, <a href="/format/2311.08667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EDMSound: Spectrogram Based Diffusion Models for Efficient and  High-Quality Audio Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Ge Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yutong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Carbonneau%2C+M">Marc-Andr&#xe9; Carbonneau</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Z">Zhiyao Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS Workshop: Machine Learning for Audio
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio diffusion models can synthesize a wide variety of sounds. Existing
models often operate on the latent domain with cascaded phase recovery modules
to reconstruct waveform. This poses challenges when generating high-fidelity
audio. In this paper, we propose EDMSound, a diffusion-based generative model
in spectrogram domain under the framework of elucidated diffusion models (EDM).
Combining with efficient deterministic sampler, we achieved similar Fr\'echet
audio distance (FAD) score as top-ranked baseline with only 10 steps and
reached state-of-the-art performance with 50 steps on the DCASE2023 foley sound
generation benchmark. We also revealed a potential concern regarding diffusion
based audio generation models that they tend to generate samples with high
perceptual similarity to the data from training data. Project page:
https://agentcooper2002.github.io/EDMSound/
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08669" title="Abstract">arXiv:2311.08669</a> [<a href="/pdf/2311.08669" title="Download PDF">pdf</a>, <a href="/format/2311.08669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Calibration for Multilingual Question Answering Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yahan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dan%2C+S">Soham Dan</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Dan Roth</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Insup Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under Submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multilingual pre-trained language models are incredibly effective at Question
Answering (QA), a core task in Natural Language Understanding, achieving high
accuracies on several multilingual benchmarks. However, little is known about
how well they are calibrated. In this paper, we study the calibration
properties of several pre-trained multilingual large language models (LLMs) on
a variety of question-answering tasks. We perform extensive experiments,
spanning both extractive and generative QA model designs and diverse languages,
spanning both high-resource and low-resource ones. We study different
dimensions of calibration in in-distribution, out-of-distribution, and
cross-lingual transfer settings, and investigate strategies to improve it,
including post-hoc methods and regularized fine-tuning. We demonstrate
automatically translated data augmentation as a highly effective technique to
improve model calibration. We also conduct a number of ablation experiments to
study the effect of model size on calibration and how multilingual models
compare with their monolingual counterparts for diverse tasks and languages.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08670" title="Abstract">arXiv:2311.08670</a> [<a href="/pdf/2311.08670" title="Download PDF">pdf</a>, <a href="/format/2311.08670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control  and Contrastive Learning with Negative Samples Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yimin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xulong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianzong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Ning Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 21st IEEE International Symposium on Parallel and Distributed Processing with Applications (IEEE ISPA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Better disentanglement of speech representation is essential to improve the
quality of voice conversion. Recently contrastive learning is applied to voice
conversion successfully based on speaker labels. However, the performance of
model will reduce in conversion between similar speakers. Hence, we propose an
augmented negative sample selection to address the issue. Specifically, we
create hard negative samples based on the proposed speaker fusion module to
improve learning ability of speaker encoder. Furthermore, considering the
fine-grain modeling of speaker style, we employ a reference encoder to extract
fine-grained style and conduct the augmented contrastive learning on global
style. The experimental results show that the proposed method outperforms
previous work in voice conversion tasks.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08673" title="Abstract">arXiv:2311.08673</a> [<a href="/pdf/2311.08673" title="Download PDF">pdf</a>, <a href="/format/2311.08673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking  Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianzong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yimin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Ziqi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xulong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Ning Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 21st IEEE International Symposium on Parallel and Distributed Processing with Applications (IEEE ISPA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper proposes a talking face generation method named "CP-EB" that takes
an audio signal as input and a person image as reference, to synthesize a
photo-realistic people talking video with head poses controlled by a short
video clip and proper eye blinking embedding. It's noted that not only the head
pose but also eye blinking are both important aspects for deep fake detection.
The implicit control of poses by video has already achieved by the state-of-art
work. According to recent research, eye blinking has weak correlation with
input audio which means eye blinks extraction from audio and generation are
possible. Hence, we propose a GAN-based architecture to extract eye blink
feature from input audio and reference video respectively and employ
contrastive training between them, then embed it into the concatenated features
of identity and poses to generate talking face images. Experimental results
show that the proposed method can generate photo-realistic talking face with
synchronous lips motions, natural head poses and blinking eyes.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08674" title="Abstract">arXiv:2311.08674</a> [<a href="/pdf/2311.08674" title="Download PDF">pdf</a>, <a href="/format/2311.08674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Precision Fruit Localization Using Active Laser-Camera Scanning:  Robust Laser Line Extraction for 2D-3D Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+P">Pengyu Chu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaojian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaixiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lammers%2C+K">Kyle Lammers</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Renfu Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Recent advancements in deep learning-based approaches have led to remarkable
progress in fruit detection, enabling robust fruit identification in complex
environments. However, much less progress has been made on fruit 3D
localization, which is equally crucial for robotic harvesting. Complex fruit
shape/orientation, fruit clustering, varying lighting conditions, and
occlusions by leaves and branches have greatly restricted existing sensors from
achieving accurate fruit localization in the natural orchard environment. In
this paper, we report on the design of a novel localization technique, called
Active Laser-Camera Scanning (ALACS), to achieve accurate and robust fruit 3D
localization. The ALACS hardware setup comprises a red line laser, an RGB color
camera, a linear motion slide, and an external RGB-D camera. Leveraging the
principles of dynamic-targeting laser-triangulation, ALACS enables precise
transformation of the projected 2D laser line from the surface of apples to the
3D positions. To facilitate laser pattern acquisitions, a Laser Line Extraction
(LLE) method is proposed for robust and high-precision feature extraction on
apples. Comprehensive evaluations of LLE demonstrated its ability to extract
precise patterns under variable lighting and occlusion conditions. The ALACS
system achieved average apple localization accuracies of 6.9 11.2 mm at
distances ranging from 1.0 m to 1.6 m, compared to 21.5 mm by a commercial
RealSense RGB-D camera, in an indoor experiment. Orchard evaluations
demonstrated that ALACS has achieved a 95% fruit detachment rate versus a 71%
rate by the RealSense camera. By overcoming the challenges of apple 3D
localization, this research contributes to the advancement of robotic fruit
harvesting technology.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08675" title="Abstract">arXiv:2311.08675</a> [<a href="/pdf/2311.08675" title="Download PDF">pdf</a>, <a href="/format/2311.08675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coreset Selection with Prioritized Multiple Objectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xiaobo Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiale Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaokun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Coreset selection is powerful in reducing computational costs and
accelerating data processing for deep learning algorithms. It strives to
identify a small subset from large-scale data, so that training only on the
subset practically performs on par with full data. When coreset selection is
applied in realistic scenes, under the premise that the identified coreset has
achieved comparable model performance, practitioners regularly desire the
identified coreset can have a size as small as possible for lower costs and
greater acceleration. Motivated by this desideratum, for the first time, we
pose the problem of "coreset selection with prioritized multiple objectives",
in which the smallest coreset size under model performance constraints is
explored. Moreover, to address this problem, an innovative method is proposed,
which maintains optimization priority order over the model performance and
coreset size, and efficiently optimizes them in the coreset selection
procedure. Theoretically, we provide the convergence guarantee of the proposed
method. Empirically, extensive experiments confirm its superiority compared
with previous strategies, often yielding better model performance with smaller
coreset sizes.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08677" title="Abstract">arXiv:2311.08677</a> [<a href="/pdf/2311.08677" title="Download PDF">pdf</a>, <a href="/format/2311.08677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning for Sparse Principal Component Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ciou%2C+S+C">Sin Cheng Ciou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P+J">Pin Jui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+E+Y">Elvin Y. Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yuh-Jye Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures, 1 table. Accepted by IEEE BigData 2023, Sorrento, Italy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">In the rapidly evolving realm of machine learning, algorithm effectiveness
often faces limitations due to data quality and availability. Traditional
approaches grapple with data sharing due to legal and privacy concerns. The
federated learning framework addresses this challenge. Federated learning is a
decentralized approach where model training occurs on client sides, preserving
privacy by keeping data localized. Instead of sending raw data to a central
server, only model updates are exchanged, enhancing data security. We apply
this framework to Sparse Principal Component Analysis (SPCA) in this work. SPCA
aims to attain sparse component loadings while maximizing data variance for
improved interpretability. Beside the L1 norm regularization term in
conventional SPCA, we add a smoothing function to facilitate gradient-based
optimization methods. Moreover, in order to improve computational efficiency,
we introduce a least squares approximation to original SPCA. This enables
analytic solutions on the optimization processes, leading to substantial
computational improvements. Within the federated framework, we formulate SPCA
as a consensus optimization problem, which can be solved using the Alternating
Direction Method of Multipliers (ADMM). Our extensive experiments involve both
IID and non-IID random features across various data owners. Results on
synthetic and public datasets affirm the efficacy of our federated SPCA
approach.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08682" title="Abstract">arXiv:2311.08682</a> [<a href="/pdf/2311.08682" title="Download PDF">pdf</a>, <a href="/ps/2311.08682" title="Download PostScript">ps</a>, <a href="/format/2311.08682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Recommender System Performance by Histogram Equalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommender system has been researched for decades with millions of different
versions of algorithms created in the industry. In spite of the huge amount of
work spent on the field, there are many basic questions to be answered in the
field. The most fundamental question to be answered is the accuracy problem,
and in recent years, fairness becomes the new buzz word for researchers. In
this paper, we borrow an idea from image processing, namely, histogram
equalization. As a preprocessing step to recommender system algorithms,
histogram equalization could enhance both the accuracy and fairness metrics of
the recommender system algorithms. In the experiment section, we prove that our
new approach could improve vanilla algorithms by a large margin in accuracy
metric and stay competitive on fairness metrics.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08684" title="Abstract">arXiv:2311.08684</a> [<a href="/pdf/2311.08684" title="Download PDF">pdf</a>, <a href="/ps/2311.08684" title="Download PostScript">ps</a>, <a href="/format/2311.08684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moonrise: Novel and Cartoon Writing System Built Upon Blockchain Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Writing novels or drawing cartoons is a prolonged and interesting process
that needs imagination and a lot of rethinking and rewriting. Blockchain
systems has a very strong feature that tampering is not allowed for the system.
In order to keep the revision history of novel writing / cartoon drawing, we
apply blockchain systems such as HyperLedger to the problem and create a novel
writer / cartoon illustrator system that is capable of keeping record of what
has been written and greatly enhancing the writing performance of the author.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08685" title="Abstract">arXiv:2311.08685</a> [<a href="/pdf/2311.08685" title="Download PDF">pdf</a>, <a href="/format/2311.08685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safer-Instruct: Aligning Language Models with Automated Preference Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Taiwei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jieyu Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for
enhancing model safety in language models. However, annotating preference data
for RLHF is a resource-intensive and creativity-demanding process, while
automatic generation methods face limitations in data diversity and quality. In
response, we present Safer-Instruct, a novel pipeline for semi-automatically
constructing large-scale preference datasets. Our approach leverages reversed
instruction tuning, instruction induction, and expert model evaluation to
efficiently generate high-quality preference data without human annotators. We
evaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an
expert model, generating approximately 10K preference samples. Finetuning an
Alpaca model on this dataset demonstrates improved harmlessness while
maintaining competitive performance on conversation and downstream tasks.
Safer-Instruct addresses the challenges in preference data acquisition,
advancing the development of safer and more responsible AI systems. Our code
and data are available at https://github.com/uscnlp-lime/safer-instruct
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08686" title="Abstract">arXiv:2311.08686</a> [<a href="/pdf/2311.08686" title="Download PDF">pdf</a>, <a href="/format/2311.08686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System-Wide Emergency Policy for Transitioning from Main to Secondary  Fuel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pagnier%2C+L">Laurent Pagnier</a>, 
<a href="/search/eess?searchtype=author&query=Goldshtein%2C+I">Igal Goldshtein</a>, 
<a href="/search/eess?searchtype=author&query=Hyett%2C+C">Criston Hyett</a>, 
<a href="/search/eess?searchtype=author&query=Ferrando%2C+R">Robert Ferrando</a>, 
<a href="/search/eess?searchtype=author&query=Alisse%2C+J">Jean Alisse</a>, 
<a href="/search/eess?searchtype=author&query=Saban%2C+L">Lilah Saban</a>, 
<a href="/search/eess?searchtype=author&query=Chertkov%2C+M">Michael Chertkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Inspired by the challenges of running the Israel's power system -- with its
increasing integration of renewables, significant load uncertainty, and primary
reliance on natural gas -- we investigate an emergency scenario where there's a
need to transition temporarily to a pricier secondary fuel until the emergency
resolves. Our objective is to devise tools that can assist power system
operators in making decisions during such critical periods. We frame this
challenge as a Markov Decision Process (MDP) optimization, considering
uncertainties like potential failures of dual-fuel generators during the
transition, operator attentiveness under stress, available but finite amount of
primary fuel (linepack available in the natural gas part of the system), power
forecast (net demand after renewable production), and the cost implications of
unavoidable load shedding. By solving the MDP in a simplified context, we
identify viable policies through simulations of multiple parametrized Markov
Processes (MPs). We verify our methodology using a realistic open-source model
replicating Israel's power-gas infrastructure and outline next steps for
refining and adapting this approach.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08687" title="Abstract">arXiv:2311.08687</a> [<a href="/pdf/2311.08687" title="Download PDF">pdf</a>, <a href="/format/2311.08687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Eye on Clinical BERT: Investigating Language Model Generalization for  Diabetic Eye Disease Phenotyping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harrigian%2C+K">Keith Harrigian</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+T">Tina Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gonzales%2C+A">Anthony Gonzales</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+C+X">Cindy X. Cai</a>, 
<a href="/search/cs?searchtype=author&query=Dredze%2C+M">Mark Dredze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diabetic eye disease is a major cause of blindness worldwide. The ability to
monitor relevant clinical trajectories and detect lapses in care is critical to
managing the disease and preventing blindness. Alas, much of the information
necessary to support these goals is found only in the free text of the
electronic medical record. To fill this information gap, we introduce a system
for extracting evidence from clinical text of 19 clinical concepts related to
diabetic eye disease and inferring relevant attributes for each. In developing
this ophthalmology phenotyping system, we are also afforded a unique
opportunity to evaluate the effectiveness of clinical language models at
adapting to new clinical domains. Across multiple training paradigms, we find
that BERT language models pretrained on out-of-distribution clinical data offer
no significant improvement over BERT language models pretrained on non-clinical
data for our domain. Our study tempers recent claims that language models
pretrained on clinical data are necessary for clinical NLP tasks and highlights
the importance of not treating clinical language data as a single homogeneous
domain.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08690" title="Abstract">arXiv:2311.08690</a> [<a href="/pdf/2311.08690" title="Download PDF">pdf</a>, <a href="/format/2311.08690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling CMF Estimation in Data-Constrained Scenarios: A  Semantic-Encoding Knowledge Mining Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yanlin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Michael Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Methodology (stat.ME)

</div>
<p class="mathjax">Precise estimation of Crash Modification Factors (CMFs) is central to
evaluating the effectiveness of various road safety treatments and prioritizing
infrastructure investment accordingly. While customized study for each
countermeasure scenario is desired, the conventional CMF estimation approaches
rely heavily on the availability of crash data at given sites. This not only
makes the estimation costly, but the results are also less transferable, since
the intrinsic similarities between different safety countermeasure scenarios
are not fully explored. Aiming to fill this gap, this study introduces a novel
knowledge-mining framework for CMF prediction. This framework delves into the
connections of existing countermeasures and reduces the reliance of CMF
estimation on crash data availability and manual data collection. Specifically,
it draws inspiration from human comprehension processes and introduces advanced
Natural Language Processing (NLP) techniques to extract intricate variations
and patterns from existing CMF knowledge. It effectively encodes unstructured
countermeasure scenarios into machine-readable representations and models the
complex relationships between scenarios and CMF values. This new data-driven
framework provides a cost-effective and adaptable solution that complements the
case-specific approaches for CMF estimation, which is particularly beneficial
when availability of crash data or time imposes constraints. Experimental
validation using real-world CMF Clearinghouse data demonstrates the
effectiveness of this new approach, which shows significant accuracy
improvements compared to baseline methods. This approach provides insights into
new possibilities of harnessing accumulated transportation knowledge in various
applications.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08692" title="Abstract">arXiv:2311.08692</a> [<a href="/pdf/2311.08692" title="Download PDF">pdf</a>, <a href="/format/2311.08692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Routing to the Expert: Efficient Reward-guided Ensemble of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Keming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hongyi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+R">Runji Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Junyang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The complementary potential of Large Language Models (LLM) assumes
off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and
tasks so that an ensemble of LLMs can achieve consistently better performance.
Existing ensemble methods for LLMs mainly focus on reward model ranking of
outputs, leading to significant computation overhead. To combat this issue, we
revisit the complementary potential of LLMs and further elaborate it by mining
latent expertise with off-the-shelf reward models. We propose Zooter, a
reward-guided routing method distilling rewards on training queries to train a
routing function, which can precisely distribute each query to the LLM with
expertise about it. We also integrate a tag-based label enhancement to mitigate
noise from uncertainty when using rewards as silver supervision. Zooter shows
computation efficiency in inference as it introduces only a minor computation
overhead of a routing function compared with reward model ranking methods. We
evaluate Zooter on a comprehensive benchmark collection with 26 subsets on
different domains and tasks. Zooter outperforms the best single model on
average and ranks first on 44% of tasks, even surpassing multiple reward model
ranking methods.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08695" title="Abstract">arXiv:2311.08695</a> [<a href="/pdf/2311.08695" title="Download PDF">pdf</a>, <a href="/format/2311.08695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attribute Diversity Determines the Systematicity Gap in VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berlot-Attwell%2C+I">Ian Berlot-Attwell</a>, 
<a href="/search/cs?searchtype=author&query=Carrell%2C+A+M">A. Michael Carrell</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+K+K">Kumar Krishna Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+Y">Yash Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Saphra%2C+N">Naomi Saphra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The degree to which neural networks can generalize to new combinations of
familiar concepts, and the conditions under which they are able to do so, has
long been an open question. In this work, we study the systematicity gap in
visual question answering: the performance difference between reasoning on
previously seen and unseen combinations of object attributes. To test, we
introduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased
quantity of training data does not reduce the systematicity gap, increased
training data diversity of the attributes in the unseen combination does. In
all, our experiments suggest that the more distinct attribute type combinations
are seen during training, the more systematic we can expect the resulting model
to be.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08698" title="Abstract">arXiv:2311.08698</a> [<a href="/pdf/2311.08698" title="Download PDF">pdf</a>, <a href="/ps/2311.08698" title="Download PostScript">ps</a>, <a href="/format/2311.08698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial General Intelligence, Existential Risk, and Human Risk  Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandel%2C+D+R">David R. Mandel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Artificial general intelligence (AGI) does not yet exist, but given the pace
of technological development in artificial intelligence, it is projected to
reach human-level intelligence within roughly the next two decades. After that,
many experts expect it to far surpass human intelligence and to do so rapidly.
The prospect of superintelligent AGI poses an existential risk to humans
because there is no reliable method for ensuring that AGI goals stay aligned
with human goals. Drawing on publicly available forecaster and opinion data,
the author examines how experts and non-experts perceive risk from AGI. The
findings indicate that the perceived risk of a world catastrophe or extinction
from AGI is greater than for other existential risks. The increase in perceived
risk over the last year is also steeper for AGI than for other existential
threats (e.g., nuclear war or human-caused climate change). That AGI is a
pressing existential risk is something on which experts and non-experts agree,
but the basis for such agreement currently remains obscure.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08702" title="Abstract">arXiv:2311.08702</a> [<a href="/pdf/2311.08702" title="Download PDF">pdf</a>, <a href="/format/2311.08702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debate Helps Supervise Unreliable Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michael%2C+J">Julian Michael</a>, 
<a href="/search/cs?searchtype=author&query=Mahdi%2C+S">Salsabila Mahdi</a>, 
<a href="/search/cs?searchtype=author&query=Rein%2C+D">David Rein</a>, 
<a href="/search/cs?searchtype=author&query=Petty%2C+J">Jackson Petty</a>, 
<a href="/search/cs?searchtype=author&query=Dirani%2C+J">Julien Dirani</a>, 
<a href="/search/cs?searchtype=author&query=Padmakumar%2C+V">Vishakh Padmakumar</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 84 pages, 13 footnotes, 5 figures, 4 tables, 28 debate transcripts; data and code at <a href="https://github.com/julianmichael/debate/tree/2023-nyu-experiments">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">As AI systems are used to answer more difficult questions and potentially
help create new knowledge, judging the truthfulness of their outputs becomes
more difficult and more important. How can we supervise unreliable experts,
which have access to the truth but may not accurately report it, to give
answers that are systematically true and don't just superficially seem true,
when the supervisor can't tell the difference between the two on their own? In
this work, we show that debate between two unreliable experts can help a
non-expert judge more reliably identify the truth. We collect a dataset of
human-written debates on hard reading comprehension questions where the judge
has not read the source passage, only ever seeing expert arguments and short
quotes selectively revealed by 'expert' debaters who have access to the
passage. In our debates, one expert argues for the correct answer, and the
other for an incorrect answer. Comparing debate to a baseline we call
consultancy, where a single expert argues for only one answer which is correct
half of the time, we find that debate performs significantly better, with 84%
judge accuracy compared to consultancy's 74%. Debates are also more efficient,
being 68% of the length of consultancies. By comparing human to AI debaters, we
find evidence that with more skilled (in this case, human) debaters, the
performance of debate goes up but the performance of consultancy goes down. Our
error analysis also supports this trend, with 46% of errors in human debate
attributable to mistakes by the honest debater (which should go away with
increased skill); whereas 52% of errors in human consultancy are due to
debaters obfuscating the relevant evidence from the judge (which should become
worse with increased skill). Overall, these results show that debate is a
promising approach for supervising increasingly capable but potentially
unreliable AI systems.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08704" title="Abstract">arXiv:2311.08704</a> [<a href="/pdf/2311.08704" title="Download PDF">pdf</a>, <a href="/format/2311.08704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Follow Concept Annotation Guidelines? A Case  Study on Scientific and Financial Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fonseca%2C+M">Marcio Fonseca</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although large language models (LLMs) exhibit remarkable capacity to leverage
in-context demonstrations, it is still unclear to what extent they can learn
new concepts or facts from ground-truth labels. To address this question, we
examine the capacity of instruction-tuned LLMs to follow in-context concept
guidelines for sentence labeling tasks. We design guidelines that present
different types of factual and counterfactual concept definitions, which are
used as prompts for zero-shot sentence classification tasks. Our results show
that although concept definitions consistently help in task performance, only
the larger models (with 70B parameters or more) have limited ability to work
under counterfactual contexts. Importantly, only proprietary models such as
GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is
due to more sophisticated alignment methods. Finally, we find that
Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which
indicates that careful fine-tuning is more effective than increasing model
scale. Altogether, our simple evaluation method reveals significant gaps in
concept understanding between the most capable open-source language models and
the leading proprietary APIs.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08705" title="Abstract">arXiv:2311.08705</a> [<a href="/pdf/2311.08705" title="Download PDF">pdf</a>, <a href="/format/2311.08705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Robustness of Dialogue Summarization Models in the Presence  of Naturally Occurring Variations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Ankita Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Gunasekara%2C+C">Chulaka Gunasekara</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+H">Hui Wan</a>, 
<a href="/search/cs?searchtype=author&query=Ganhotra%2C+J">Jatin Ganhotra</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Sachindra Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Danilevsky%2C+M">Marina Danilevsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dialogue summarization task involves summarizing long conversations while
preserving the most salient information. Real-life dialogues often involve
naturally occurring variations (e.g., repetitions, hesitations) and existing
dialogue summarization models suffer from performance drop on such
conversations. In this study, we systematically investigate the impact of such
variations on state-of-the-art dialogue summarization models using publicly
available datasets. To simulate real-life variations, we introduce two types of
perturbations: utterance-level perturbations that modify individual utterances
with errors and language variations, and dialogue-level perturbations that add
non-informative exchanges (e.g., repetitions, greetings). We conduct our
analysis along three dimensions of robustness: consistency, saliency, and
faithfulness, which capture different aspects of the summarization model's
performance. We find that both fine-tuned and instruction-tuned models are
affected by input variations, with the latter being more susceptible,
particularly to dialogue-level perturbations. We also validate our findings via
human evaluation. Finally, we investigate if the robustness of fine-tuned
models can be improved by training them with a fraction of perturbed data and
observe that this approach is insufficient to address robustness challenges
with current models and thus warrants a more thorough investigation to identify
better solutions. Overall, our work highlights robustness challenges in
dialogue summarization and provides insights for future research.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08706" title="Abstract">arXiv:2311.08706</a> [<a href="/pdf/2311.08706" title="Download PDF">pdf</a>, <a href="/format/2311.08706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligned: A Platform-based Process for Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaotran%2C+E">Ethan Shaotran</a>, 
<a href="/search/cs?searchtype=author&query=Pesok%2C+I">Ido Pesok</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+S">Sam Jones</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+E">Emi Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures. For associated public report, see <a href="https://energize.ai/openai">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We are introducing Aligned, a platform for global governance and alignment of
frontier models, and eventually superintelligence. While previous efforts at
the major AI labs have attempted to gather inputs for alignment, these are
often conducted behind closed doors. We aim to set the foundation for a more
trustworthy, public-facing approach to safety: a constitutional committee
framework. Initial tests with 680 participants result in a 30-guideline
constitution with 93% overall support. We show the platform naturally scales,
instilling confidence and enjoyment from the community. We invite other AI labs
and teams to plug and play into the Aligned ecosystem.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08707" title="Abstract">arXiv:2311.08707</a> [<a href="/pdf/2311.08707" title="Download PDF">pdf</a>, <a href="/format/2311.08707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> K-BMPC: Derivative-based Koopman Bilinear Model Predictive Control For  Tractor-trailer Trajectory Tracking With Unknown Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zehao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jingchuan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Nonlinear dynamics bring difficulties to controller design for control-affine
systems such as tractor-trailer vehicles, especially when the parameters in
dynamics are unknown. To address this constraint, we propose a derivative-based
lifting function construction method, show that the corresponding infinite
dimensional Koopman bilinear model over the lifting function is equivalent to
the original control-affine system. Further, we analyze the propagation and
bounds of state prediction errors caused by the the truncation in derivative
order. The identified finite dimensional Koopman bilinear model would serve as
predictive model in next step. Koopman Bilinear Model Predictive control
(K-BMPC) is proposed to solve the trajectory tracking problem. We linearize the
bilinear model around the estimation of the lifted state and control input.
Then the bilinear Model Predictive Control problem is approximated by a
quadratic programming problem. Further, the estimation is updated at each
iteration until the convergence is reached. Moreover, we implement our
algorithm on a tractor-trailer dynamic system, taking into account the
longitudinal and side slip effects. The open-loop simulation shows the proposed
Koopman bilinear model captures the dynamics with unknown parameters and has
good prediction performance. Closed loop tracking results show the proposed
K-BMPC exhibits elevated tracking precision along with commendable
computational efficiency. The experimental results demonstrate the feasibility
of the proposed method.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08708" title="Abstract">arXiv:2311.08708</a> [<a href="/pdf/2311.08708" title="Download PDF">pdf</a>, <a href="/format/2311.08708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA  in the Indoor Environment via Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+Y+M">Yu Min Park</a>, 
<a href="/search/cs?searchtype=author&query=Tun%2C+Y+K">Yan Kyaw Tun</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+C+S">Choong Seon Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures, IEEE/IFIP Network Operations and Management Symposium (NOMS) 2024 submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The development of 6G/B5G wireless networks, which have requirements that go
beyond current 5G networks, is gaining interest from academic and industrial.
However, to increase 6G/B5G network quality, conventional cellular networks
that rely on terrestrial base stations are constrained geographically and
economically. Meanwhile, NOMA allows multiple users to share the same
resources, which improves the spectral efficiency of the system and has the
advantage of supporting a larger number of users. Additionally, by
intelligently manipulating the phase and amplitude of both the reflected and
transmitted signals, STAR-RISs can achieve improved coverage, increased
spectral efficiency, and enhanced communication reliability. However, STAR-RISs
must simultaneously optimize the Amplitude and Phase-shift corresponding to
reflection and transmission, which makes the existing terrestiral networks more
complicated and is considered a major challenging issue. Motivated by the
above, we study the joint user pairing for NOMA and beamforming design of
Multi-STAR-RISs in an indoor environment. Then, we formulate the optimization
problem with the objective of maximizing the total throughput of MUs by jointly
optimizing the decoding order, user pairing, active beamforming, and passive
beamforming. However, the formulated problem is a MINLP. To tackle this
challenge, we first introduce the decoding order for NOMA networks. Next, we
decompose the original problem into two subproblems namely: 1) MU pairing and
2) Beamforming optimization under the optimal decoding order. For the first
subproblem, we employ correlation-based K-means clustering to solve the user
pairing problem. Then, to jointly deal with beamforming vector optimizations,
we propose MAPPO, which can make quick decisions in the given environment owing
to its low complexity.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08711" title="Abstract">arXiv:2311.08711</a> [<a href="/pdf/2311.08711" title="Download PDF">pdf</a>, <a href="/format/2311.08711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dong-Ho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuwei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+M">Mengzhao Jia</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Barbieri%2C+F">Francesco Barbieri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Instruction tuning has remarkably advanced large language models (LLMs) in
understanding and responding to diverse human instructions. Despite the success
in high-resource languages, its application in lower-resource ones faces
challenges due to the imbalanced foundational abilities of LLMs across
different languages, stemming from the uneven language distribution in their
pre-training data. To tackle this issue, we propose pivot language guided
generation (PLUG), an approach that utilizes a high-resource language,
primarily English, as the pivot to enhance instruction tuning in lower-resource
languages. It trains the model to first process instructions in the pivot
language, and then produce responses in the target language. To evaluate our
approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4
languages (Chinese, Korean, Italian, and Spanish), each annotated by
professional translators. Our approach demonstrates a significant improvement
in the instruction-following abilities of LLMs by 29% on average, compared to
directly responding in the target language alone. Further experiments validate
the versatility of our approach by employing alternative pivot languages beyond
English to assist languages where LLMs exhibit lower proficiency.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08715" title="Abstract">arXiv:2311.08715</a> [<a href="/pdf/2311.08715" title="Download PDF">pdf</a>, <a href="/format/2311.08715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Geometry-based Trajectory Design for Multi-Purpose UAVs:  Package and Data Delivery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qin%2C+Y">Yujie Qin</a>, 
<a href="/search/eess?searchtype=author&query=Kishk%2C+M+A">Mustafa A. Kishk</a>, 
<a href="/search/eess?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 10.1109/TVT.2023.3323682
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">With the advancements achieved in drones' flexibility, low cost, and high
efficiency, they obtain huge application opportunities in various industries,
such as aerial delivery and future communication networks. However, the
increasing transportation needs and expansion of network capacity demands for
UAVs will cause aerial traffic conflicts in the future. To address this issue,
in this paper, we explore the idea of multi-purpose UAVs, which act as aerial
wireless communication data relays and means of aerial transportation
simultaneously to deliver data and packages at the same time. While UAVs
deliver the packages from warehouses to residential areas, we design their
trajectories which enable them to collect data from multiple Internet of Things
(IoT) clusters and forward the collected data to terrestrial base stations
(TBSs). To select the serving nearby IoT clusters, UAVs rank them based on
their priorities and distances. From the perspectives of data and package
delivery, respectively, we propose two algorithms that design the optimal UAVs
trajectory to maximize the transmitted data or minimize the round trip time.
Specifically, we use tools from stochastic geometry to model the locations of
IoT clusters and TBSs. Given the nature of random locations, the proposed
algorithm applies to general cases. Our numerical results show that
multi-purpose UAVs are practical and have great potential to enhance the
energy/time-efficiency of future networks.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08716" title="Abstract">arXiv:2311.08716</a> [<a href="/pdf/2311.08716" title="Download PDF">pdf</a>, <a href="/format/2311.08716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Federated Learning for Clients with Different Input Image Sizes  and Numbers of Output Categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nitta%2C+S">Shuhei Nitta</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Taiji Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Mulet%2C+A+R">Albert Rodr&#xed;guez Mulet</a>, 
<a href="/search/cs?searchtype=author&query=Yaguchi%2C+A">Atsushi Yaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Hirai%2C+R">Ryusuke Hirai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure, 2023 22nd International Conference on Machine Learning and Applications (ICMLA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Federated learning is a privacy-preserving training method which consists of
training from a plurality of clients but without sharing their confidential
data. However, previous work on federated learning do not explore suitable
neural network architectures for clients with different input images sizes and
different numbers of output categories. In this paper, we propose an effective
federated learning method named ScalableFL, where the depths and widths of the
local models for each client are adjusted according to the clients' input image
size and the numbers of output categories. In addition, we provide a new bound
for the generalization gap of federated learning. In particular, this bound
helps to explain the effectiveness of our scalable neural network approach. We
demonstrate the effectiveness of ScalableFL in several heterogeneous client
settings for both image classification and object detection tasks.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08718" title="Abstract">arXiv:2311.08718</a> [<a href="/pdf/2311.08718" title="Download PDF">pdf</a>, <a href="/format/2311.08718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decomposing Uncertainty for Large Language Models through Input  Clarification Ensembling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+B">Bairu Hou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yujian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kaizhi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Andreas%2C+J">Jacob Andreas</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shiyu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Uncertainty decomposition refers to the task of decomposing the total
uncertainty of a model into data (aleatoric) uncertainty, resulting from the
inherent complexity or ambiguity of the data, and model (epistemic)
uncertainty, resulting from the lack of knowledge in the model. Performing
uncertainty decomposition for large language models (LLMs) is an important step
toward improving the reliability, trustworthiness, and interpretability of
LLMs, but this research task is very challenging and remains unresolved. The
existing canonical method, Bayesian Neural Network (BNN), cannot be applied to
LLMs, because BNN requires training and ensembling multiple variants of models,
which is infeasible or prohibitively expensive for LLMs. In this paper, we
introduce an uncertainty decomposition framework for LLMs, called input
clarifications ensemble, which bypasses the need to train new models. Rather
than ensembling models with different parameters, our approach generates a set
of clarifications for the input, feeds them into the fixed LLMs, and ensembles
the corresponding predictions. We show that our framework shares a symmetric
decomposition structure with BNN. Empirical evaluations demonstrate that the
proposed framework provides accurate and reliable uncertainty quantification on
various tasks. Code will be made publicly available at
https://github.com/UCSB-NLP-Chang/llm_uncertainty .
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08719" title="Abstract">arXiv:2311.08719</a> [<a href="/pdf/2311.08719" title="Download PDF">pdf</a>, <a href="/format/2311.08719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term  Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoyan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Binbin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiqiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guannan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Memory-augmented Large Language Models (LLMs) have demonstrated remarkable
performance in long-term human-machine interactions, which basically relies on
iterative recalling and reasoning of history to generate high-quality
responses. However, such repeated recall-reason steps easily produce biased
thoughts, \textit{i.e.}, inconsistent reasoning results when recalling the same
history for different questions. On the contrary, humans can keep thoughts in
the memory and recall them without repeated reasoning. Motivated by this human
capability, we propose a novel memory mechanism called TiM (Think-in-Memory)
that enables LLMs to maintain an evolved memory for storing historical thoughts
along the conversation stream. The TiM framework consists of two crucial
stages: (1) before generating a response, a LLM agent recalls relevant thoughts
from memory, and (2) after generating a response, the LLM agent post-thinks and
incorporates both historical and new thoughts to update the memory. Thus, TiM
can eliminate the issue of repeated reasoning by saving the post-thinking
thoughts as the history. Besides, we formulate the basic principles to organize
the thoughts in memory based on the well-established operations,
(\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic
updates and evolution of the thoughts. Furthermore, we introduce
Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the
long-term conversations. We conduct qualitative and quantitative experiments on
real-world and simulated dialogues covering a wide range of topics,
demonstrating that equipping existing LLMs with TiM significantly enhances
their performance in generating responses for long-term interactions.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08721" title="Abstract">arXiv:2311.08721</a> [<a href="/pdf/2311.08721" title="Download PDF">pdf</a>, <a href="/format/2311.08721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robust Semantics-based Watermark for Large Language Model against  Paraphrasing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yingqian Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Large language models (LLMs) have show great ability in various natural
language tasks. However, there are concerns that LLMs are possible to be used
improperly or even illegally. To prevent the malicious usage of LLMs, detecting
LLM-generated text becomes crucial in the deployment of LLM applications.
Watermarking is an effective strategy to detect the LLM-generated content by
encoding a pre-defined secret watermark to facilitate the detection process.
However, the majority of existing watermark methods leverage the simple hashes
of precedent tokens to partition vocabulary. Such watermark can be easily
eliminated by paraphrase and correspondingly the detection effectiveness will
be greatly compromised. Thus, to enhance the robustness against paraphrase, we
propose a semantics-based watermark framework SemaMark. It leverages the
semantics as an alternative to simple hashes of tokens since the paraphrase
will likely preserve the semantic meaning of the sentences. Comprehensive
experiments are conducted to demonstrate the effectiveness and robustness of
SemaMark under different paraphrases.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08723" title="Abstract">arXiv:2311.08723</a> [<a href="/pdf/2311.08723" title="Download PDF">pdf</a>, <a href="/format/2311.08723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Token Prediction as Implicit Classification to Identify LLM-Generated  Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yutian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Hao Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+V">Vivian Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangze Li</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper introduces a novel approach for identifying the possible large
language models (LLMs) involved in text generation. Instead of adding an
additional classification layer to a base LM, we reframe the classification
task as a next-token prediction task and directly fine-tune the base LM to
perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the
backbone for our experiments. We compared our approach to the more direct
approach of utilizing hidden states for classification. Evaluation shows the
exceptional performance of our method in the text classification task,
highlighting its simplicity and efficiency. Furthermore, interpretability
studies on the features extracted by our model reveal its ability to
differentiate distinctive writing styles among various LLMs even in the absence
of an explicit classifier. We also collected a dataset named OpenLLMText,
containing approximately 340k text samples from human and LLMs, including
GPT3.5, PaLM, LLaMA, and GPT2.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08724" title="Abstract">arXiv:2311.08724</a> [<a href="/pdf/2311.08724" title="Download PDF">pdf</a>, <a href="/ps/2311.08724" title="Download PostScript">ps</a>, <a href="/format/2311.08724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Method for Text Entity Linking in Power Distribution Scheduling Oriented  to Power Distribution Network Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Che Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bing Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sizhe Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The proposed method for linking entities in power distribution dispatch texts
to a power distribution network knowledge graph is based on a deep
understanding of these networks. This method leverages the unique features of
entities in both the power distribution network's knowledge graph and the
dispatch texts, focusing on their semantic, phonetic, and syntactic
characteristics. An enhanced model, the Lexical Semantic Feature-based Skip
Convolutional Neural Network (LSF-SCNN), is utilized for effectively matching
dispatch text entities with those in the knowledge graph. The efficacy of this
model, compared to a control model, is evaluated through cross-validation
methods in real-world power distribution dispatch scenarios. The results
indicate that the LSF-SCNN model excels in accurately linking a variety of
entity types, demonstrating high overall accuracy in entity linking when the
process is conducted in English.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08725" title="Abstract">arXiv:2311.08725</a> [<a href="/pdf/2311.08725" title="Download PDF">pdf</a>, <a href="/ps/2311.08725" title="Download PostScript">ps</a>, <a href="/format/2311.08725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Theory of Liquidity Provisioning for Automated Market Makers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhaskara%2C+A">Adithya Bhaskara</a>, 
<a href="/search/cs?searchtype=author&query=Frongillo%2C+R">Rafael Frongillo</a>, 
<a href="/search/cs?searchtype=author&query=Papireddygari%2C+M">Maneesha Papireddygari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">In decentralized finance, it is common for automated market makers to
provision liquidity from external parties. The market maker rewards these
liquidity providers with a cut of the trading fees, in exchange for the risk
they take on. A handful of protocols for liquidity provisioning have been
proposed, such as Uniswap V2 and V3, with specific and sometimes complex rules
for collecting liquidity deposits, executing trades, and dividing up fees.
Beyond these examples, and a broader understanding of liquidity provisioning,
and particularly the design space from which one could choose a different
protocols, has been out of reach. In this work, we show that one can view
liquidity provisioning very broadly as the practice of running several market
makers "in parallel": each market maker provides its own liquidity, yet the
combined group can operate as a single coherent market. We prove that this
general protocol, when restricted to specific forms of the constituent market
makers, recovers Uniswap V2 and V3 as special cases. We then go on to propose a
new restriction which may have advantages over Uniswap V3. In the context of
prediction markets, where computation costs are less constrained, our general
protocol gives a maximally flexible way to provision liquidity. We conclude
with remarks about the nature of liquidity and fees in markets with more than 2
assets, and several open questions.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08726" title="Abstract">arXiv:2311.08726</a> [<a href="/pdf/2311.08726" title="Download PDF">pdf</a>, <a href="/format/2311.08726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Estimation on Sequential Labeling via Uncertainty  Transmission
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jianfeng He</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Linlin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+S">Shuo Lei</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chang-Tien Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Sequential labeling is a task predicting labels for each token in a sequence,
such as Named Entity Recognition (NER). NER tasks aim to extract entities and
predict their labels given a text, which is important in information
extraction. Although previous works have shown great progress in improving NER
performance, uncertainty estimation on NER (UE-NER) is still underexplored but
essential. This work focuses on UE-NER, which aims to estimate uncertainty
scores for the NER predictions. Previous uncertainty estimation models often
overlook two unique characteristics of NER: the connection between entities
(i.e., one entity embedding is learned based on the other ones) and wrong span
cases in the entity extraction subtask. Therefore, we propose a Sequential
Labeling Posterior Network (SLPN) to estimate uncertainty scores for the
extracted entities, considering uncertainty transmitted from other tokens.
Moreover, we have defined an evaluation strategy to address the specificity of
wrong-span cases. Our SLPN has achieved significant improvements on two
datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant
dataset.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08728" title="Abstract">arXiv:2311.08728</a> [<a href="/pdf/2311.08728" title="Download PDF">pdf</a>, <a href="/ps/2311.08728" title="Download PostScript">ps</a>, <a href="/format/2311.08728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Placement of Capacitor in Distribution System using Particle  Swarm Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haq%2C+I+U">Izhar Ul Haq</a> (School of Automation, Central South University, China)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 3 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In power systems, the incorporation of capacitors offers a wide range of
established advantages. These benefits encompass the enhancement of the systems
power factor, optimization of voltage profiles, increased capacity for current
flow through cables and transformers, and the mitigation of losses attributed
to the compensation of reactive power components. Different techniques have
been applied to enhance the performance of the distribution system by reducing
line losses. This paper focuses on reducing line losses through the optimal
placement and sizing of capacitors. Optimal capacitor placement is analysed
using load flow analysis with the Newton Raphson method. The placement of
capacitor optimization is related to the sensitivity of the buses, which
depends on the loss sensitivity factor. The optimal capacitor size is
determined using Particle Swarm Optimization (PSO). The analysis is conducted
using the IEEE 14 bus system in MATLAB. The results reveal that placing
capacitors at the most sensitive bus locations leads to a significant reduction
in line losses. Additionally, the optimal capacitor size has a substantial
impact on improving the voltage profile and the power loss is reduced by 21.02
percent through the proposed method.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08732" title="Abstract">arXiv:2311.08732</a> [<a href="/pdf/2311.08732" title="Download PDF">pdf</a>, <a href="/format/2311.08732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Emergency Decision-making with Knowledge Graphs and Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minze Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Zhenxiang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Weitong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+T">Tingxin Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chunli Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Emergency management urgently requires comprehensive knowledge while having a
high possibility to go beyond individuals' cognitive scope. Therefore,
artificial intelligence(AI) supported decision-making under that circumstance
is of vital importance. Recent emerging large language models (LLM) provide a
new direction for enhancing targeted machine intelligence. However, the
utilization of LLM directly would inevitably introduce unreliable output for
its inherent issue of hallucination and poor reasoning skills. In this work, we
develop a system called Enhancing Emergency decision-making with Knowledge
Graph and LLM (E-KELL), which provides evidence-based decision-making in
various emergency stages. The study constructs a structured emergency knowledge
graph and guides LLMs to reason over it via a prompt chain. In real-world
evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in
comprehensibility, accuracy, conciseness, and instructiveness from a group of
emergency commanders and firefighters, demonstrating a significant improvement
across various situations compared to baseline models. This work introduces a
novel approach to providing reliable emergency decision support.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08734" title="Abstract">arXiv:2311.08734</a> [<a href="/pdf/2311.08734" title="Download PDF">pdf</a>, <a href="/format/2311.08734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thread of Thought Unraveling Chaotic Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yucheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xiubo Geng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chongyang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+G">Guodong Long</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian-Guang Lou</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianbing Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have ushered in a transformative era in the
field of natural language processing, excelling in tasks related to text
comprehension and generation. Nevertheless, they encounter difficulties when
confronted with chaotic contexts (e.g., distractors rather than long irrelevant
context), leading to the inadvertent omission of certain details within the
chaotic context. In response to these challenges, we introduce the "Thread of
Thought" (ThoT) strategy, which draws inspiration from human cognitive
processes. ThoT systematically segments and analyzes extended contexts while
adeptly selecting pertinent information. This strategy serves as a versatile
"plug-and-play" module, seamlessly integrating with various LLMs and prompting
techniques. In the experiments, we utilize the PopQA and EntityQ datasets, as
well as a Multi-Turn Conversation Response dataset (MTCR) we collected, to
illustrate that ThoT significantly improves reasoning performance compared to
other prompting techniques.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08740" title="Abstract">arXiv:2311.08740</a> [<a href="/pdf/2311.08740" title="Download PDF">pdf</a>, <a href="/format/2311.08740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdVENTR: Autonomous Robot Navigation in Complex Outdoor Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weerakoon%2C+K">Kasun Weerakoon</a>, 
<a href="/search/cs?searchtype=author&query=Sathyamoorthy%2C+A+J">Adarsh Jagan Sathyamoorthy</a>, 
<a href="/search/cs?searchtype=author&query=Elnoor%2C+M">Mohamed Elnoor</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present a novel system, AdVENTR for autonomous robot navigation in
unstructured outdoor environments that consist of uneven and vegetated
terrains. Our approach is general and can enable both wheeled and legged robots
to handle outdoor terrain complexity including unevenness, surface properties
like poor traction, granularity, obstacle stiffness, etc. We use data from
sensors including RGB cameras, 3D Lidar, IMU, robot odometry, and pose
information with efficient learning-based perception and planning algorithms
that can execute on edge computing hardware. Our system uses a scene-aware
switching method to perceive the environment for navigation at any time instant
and dynamically switches between multiple perception algorithms. We test our
system in a variety of sloped, rocky, muddy, and densely vegetated terrains and
demonstrate its performance on Husky and Spot robots.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08744" title="Abstract">arXiv:2311.08744</a> [<a href="/pdf/2311.08744" title="Download PDF">pdf</a>, <a href="/format/2311.08744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Graph-Aware Diffusion Modeling for Collaborative Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yunqin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recovering masked feedback with neural models is a popular paradigm in
recommender systems. Seeing the success of diffusion models in solving
ill-posed inverse problems, we introduce a conditional diffusion framework for
collaborative filtering that iteratively reconstructs a user's hidden
preferences guided by its historical interactions. To better align with the
intrinsic characteristics of implicit feedback data, we implement forward
diffusion by applying synthetic smoothing filters to interaction signals on an
item-item graph. The resulting reverse diffusion can be interpreted as a
personalized process that gradually refines preference scores. Through graph
Fourier transform, we equivalently characterize this model as an anisotropic
Gaussian diffusion in the graph spectral domain, establishing both forward and
reverse formulations. Our model outperforms state-of-the-art methods by a large
margin on one dataset and yields competitive results on the others.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08745" title="Abstract">arXiv:2311.08745</a> [<a href="/pdf/2311.08745" title="Download PDF">pdf</a>, <a href="/format/2311.08745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Stochastic Gradient Descent to Smooth Nonconvex Functions:  Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sato%2C+N">Naoki Sato</a>, 
<a href="/search/cs?searchtype=author&query=Iiduka%2C+H">Hideaki Iiduka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The graduated optimization approach is a heuristic method for finding
globally optimal solutions for nonconvex functions and has been theoretically
analyzed in several studies. This paper defines a new family of nonconvex
functions for graduated optimization, discusses their sufficient conditions,
and provides a convergence analysis of the graduated optimization algorithm for
them. It shows that stochastic gradient descent (SGD) with mini-batch
stochastic gradients has the effect of smoothing the function, the degree of
which is determined by the learning rate and batch size. This finding provides
theoretical insights from a graduated optimization perspective on why large
batch sizes fall into sharp local minima, why decaying learning rates and
increasing batch sizes are superior to fixed learning rates and batch sizes,
and what the optimal learning rate scheduling is. To the best of our knowledge,
this is the first paper to provide a theoretical explanation for these aspects.
Moreover, a new graduated optimization framework that uses a decaying learning
rate and increasing batch size is analyzed and experimental results of image
classification that support our theoretical findings are reported.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08747" title="Abstract">arXiv:2311.08747</a> [<a href="/pdf/2311.08747" title="Download PDF">pdf</a>, <a href="/format/2311.08747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Dense Nested Attention Network Based on Transformer for  Infrared Small Target Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+C">Chun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jie Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yaqian Ning</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tianhua Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zechen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Q">Qun Hao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Infrared small target detection based on deep learning offers unique
advantages in separating small targets from complex and dynamic backgrounds.
However, the features of infrared small targets gradually weaken as the depth
of convolutional neural network (CNN) increases. To address this issue, we
propose a novel method for detecting infrared small targets called improved
dense nested attention network (IDNANet), which is based on the transformer
architecture. We preserve the dense nested structure of dense nested attention
network (DNANet) and introduce the Swin-transformer during feature extraction
stage to enhance the continuity of features. Furthermore, we integrate the
ACmix attention structure into the dense nested structure to enhance the
features of intermediate layers. Additionally, we design a weighted dice binary
cross-entropy (WD-BCE) loss function to mitigate the negative impact of
foreground-background imbalance in the samples. Moreover, we develop a dataset
specifically for infrared small targets, called BIT-SIRST. The dataset
comprises a significant amount of real-world targets and manually annotated
labels, as well as synthetic data and corresponding labels. We have evaluated
the effectiveness of our method through experiments conducted on public
datasets. In comparison to other state-of-the-art methods, our approach
outperforms in terms of probability of detection (P_d), false-alarm rate (F_a),
and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89 on the
NUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08756" title="Abstract">arXiv:2311.08756</a> [<a href="/pdf/2311.08756" title="Download PDF">pdf</a>, <a href="/format/2311.08756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Toeplitz Neural Network with Constant-time Inference  Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yiran Zhong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023. Yiran Zhong is the corresponding author. The source code is available at <a href="https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in
various sequence modeling tasks. They outperform commonly used
Transformer-based models while benefiting from log-linear space-time
complexities. On the other hand, State Space Models (SSMs) achieve lower
performance than TNNs in language modeling but offer the advantage of constant
inference complexity. In this paper, we aim to combine the strengths of TNNs
and SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to
achieve the same constant inference complexities as SSMs. To accomplish this,
we formulate the conversion process as an optimization problem and provide a
closed-form solution. We demonstrate how to transform the target equation into
a Vandermonde linear system problem, which can be efficiently solved using the
Discrete Fourier Transform (DFT). Notably, our method requires no training and
maintains numerical stability. It can be also applied to any LongConv-based
model. To assess its effectiveness, we conduct extensive experiments on
language modeling tasks across various settings. Additionally, we compare our
method to other gradient-descent solutions, highlighting the superior numerical
stability of our approach. The source code is available at
https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08757" title="Abstract">arXiv:2311.08757</a> [<a href="/pdf/2311.08757" title="Download PDF">pdf</a>, <a href="/format/2311.08757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scalable Two-Level Domain Decomposition Eigensolver for Periodic  Schr&#xf6;dinger Eigenstates in Anisotropically Expanding Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Theisen%2C+L">Lambert Theisen</a>, 
<a href="/search/math?searchtype=author&query=Stamm%2C+B">Benjamin Stamm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Accelerating iterative eigenvalue algorithms is often achieved by employing a
spectral shifting strategy. Unfortunately, improved shifting typically leads to
a smaller eigenvalue for the resulting shifted operator, which in turn results
in a high condition number of the underlying solution matrix, posing a major
challenge for iterative linear solvers. This paper introduces a two-level
domain decomposition preconditioner that addresses this issue for the linear
Schr\"odinger eigenvalue problem, even in the presence of a vanishing
eigenvalue gap in non-uniform, expanding domains. Since the quasi-optimal
shift, which is already available as the solution to a spectral cell problem,
is required for the eigenvalue solver, it is logical to also use its associated
eigenfunction as a generator to construct a coarse space. We analyze the
resulting two-level additive Schwarz preconditioner and obtain a condition
number bound that is independent of the domain's anisotropy, despite the need
for only one basis function per subdomain for the coarse solver. Several
numerical examples are presented to illustrate its flexibility and efficiency.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08759" title="Abstract">arXiv:2311.08759</a> [<a href="/pdf/2311.08759" title="Download PDF">pdf</a>, <a href="/format/2311.08759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yijie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianyi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jun Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The illumination of improperly exposed photographs has been widely corrected
using deep convolutional neural networks or Transformers. Despite with
promising performance, these methods usually suffer from large parameter
amounts and heavy computational FLOPs on high-resolution photographs. In this
paper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale
Linear Transformation (MSLT) networks under the multi-layer perception
architecture, which can process 4K-resolution sRGB images at 125
Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT
networks first decompose an input image into high and low frequency layers by
Laplacian pyramid techniques, and then sequentially correct different layers by
pixel-adaptive linear transformation, which is implemented by efficient
bilateral grid learning or 1x1 convolutions. Experiments on two benchmark
datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts
on photo exposure correction. Extensive ablation studies validate the
effectiveness of our contributions. The code is available at
https://github.com/Zhou-Yijie/MSLTNet.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08760" title="Abstract">arXiv:2311.08760</a> [<a href="/pdf/2311.08760" title="Download PDF">pdf</a>, <a href="/format/2311.08760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forms of Understanding of XAI-Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buschmeier%2C+H">Hendrik Buschmeier</a>, 
<a href="/search/cs?searchtype=author&query=Buhl%2C+H+M">Heike M. Buhl</a>, 
<a href="/search/cs?searchtype=author&query=Kern%2C+F">Friederike Kern</a>, 
<a href="/search/cs?searchtype=author&query=Grimminger%2C+A">Angela Grimminger</a>, 
<a href="/search/cs?searchtype=author&query=Beierling%2C+H">Helen Beierling</a>, 
<a href="/search/cs?searchtype=author&query=Fisher%2C+J">Josephine Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Gro%C3%9F%2C+A">Andr&#xe9; Gro&#xdf;</a>, 
<a href="/search/cs?searchtype=author&query=Horwath%2C+I">Ilona Horwath</a>, 
<a href="/search/cs?searchtype=author&query=Klowait%2C+N">Nils Klowait</a>, 
<a href="/search/cs?searchtype=author&query=Lazarov%2C+S">Stefan Lazarov</a>, 
<a href="/search/cs?searchtype=author&query=Lenke%2C+M">Michael Lenke</a>, 
<a href="/search/cs?searchtype=author&query=Lohmer%2C+V">Vivien Lohmer</a>, 
<a href="/search/cs?searchtype=author&query=Rohlfing%2C+K">Katharina Rohlfing</a>, 
<a href="/search/cs?searchtype=author&query=Scharlau%2C+I">Ingrid Scharlau</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Amit Singh</a>, 
<a href="/search/cs?searchtype=author&query=Terfloth%2C+L">Lutz Terfloth</a>, 
<a href="/search/cs?searchtype=author&query=Vollmer%2C+A">Anna-Lisa Vollmer</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wilmes%2C+A">Annedore Wilmes</a>, 
<a href="/search/cs?searchtype=author&query=Wrede%2C+B">Britta Wrede</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Explainability has become an important topic in computer science and
artificial intelligence, leading to a subfield called Explainable Artificial
Intelligence (XAI). The goal of providing or seeking explanations is to achieve
(better) 'understanding' on the part of the explainee. However, what it means
to 'understand' is still not clearly defined, and the concept itself is rarely
the subject of scientific investigation. This conceptual article aims to
present a model of forms of understanding in the context of XAI and beyond.
From an interdisciplinary perspective bringing together computer science,
linguistics, sociology, and psychology, a definition of understanding and its
forms, assessment, and dynamics during the process of giving everyday
explanations are explored. Two types of understanding are considered as
possible outcomes of explanations, namely enabledness, 'knowing how' to do or
decide something, and comprehension, 'knowing that' -- both in different
degrees (from shallow to deep). Explanations regularly start with shallow
understanding in a specific domain and can lead to deep comprehension and
enabledness of the explanandum, which we see as a prerequisite for human users
to gain agency. In this process, the increase of comprehension and enabledness
are highly interdependent. Against the background of this systematization,
special challenges of understanding in XAI are discussed.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08761" title="Abstract">arXiv:2311.08761</a> [<a href="/pdf/2311.08761" title="Download PDF">pdf</a>, <a href="/format/2311.08761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A unified framework for multiscale spectral generalized FEMs and  low-rank approximations to multiscale PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ma%2C+C">Chupeng Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work presents an abstract framework for the design, implementation, and
analysis of the multiscale spectral generalized finite element method
(MS-GFEM), a particular numerical multiscale method originally proposed in [I.
Babuska and R. Lipton, Multiscale Model.\;\,Simul., 9 (2011), pp.~373--406].
MS-GFEM is a partition of unity method employing optimal local approximation
spaces constructed from local spectral problems. We establish a general local
approximation theory demonstrating exponential convergence with respect to
local degrees of freedom under certain assumptions, with explicit dependence on
key problem parameters. Our framework applies to a broad class of multiscale
PDEs with $L^{\infty}$-coefficients in both continuous and discrete, finite
element settings, including highly indefinite problems (convection-dominated
diffusion, as well as the high-frequency Helmholtz, Maxwell and elastic wave
equations with impedance boundary conditions), and higher-order problems.
Notably, we prove a local convergence rate of $O(e^{-cn^{1/d}})$ for MS-GFEM
for all these problems, improving upon the $O(e^{-cn^{1/(d+1)}})$ rate shown by
Babuska and Lipton.
<br />Moreover, based on the abstract local approximation theory for MS-GFEM, we
establish a unified framework for showing low-rank approximations to multiscale
PDEs. This framework applies to the aforementioned problems, proving that the
associated Green's functions admit an $O(|\log\epsilon|^{d})$-term separable
approximation on well-separated domains with error $\epsilon&gt;0$. Our analysis
improves and generalizes the result in [M. Bebendorf and W. Hackbusch,
Numerische Mathematik, 95 (2003), pp.~1-28] where an
$O(|\log\epsilon|^{d+1})$-term separable approximation was proved for
Poisson-type problems.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08764" title="Abstract">arXiv:2311.08764</a> [<a href="/pdf/2311.08764" title="Download PDF">pdf</a>, <a href="/format/2311.08764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Past, Present and Future: A Self-Supervised Approach for Class  Incremental Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoshuang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhongyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+K">Ke Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shouhong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hongtao Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Class Incremental Learning (CIL) aims to handle the scenario where data of
novel classes occur continuously and sequentially. The model should recognize
the sequential novel classes while alleviating the catastrophic forgetting. In
the self-supervised manner, it becomes more challenging to avoid the conflict
between the feature embedding spaces of novel classes and old ones without any
class labels. To address the problem, we propose a self-supervised CIL
framework CPPF, meaning Combining Past, Present and Future. In detail, CPPF
consists of a prototype clustering module (PC), an embedding space reserving
module (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the
ESR modules reserve embedding space for subsequent phases at the prototype
level and the feature level respectively to prepare for knowledge learned in
the future. 2) The MTD module maintains the representations of the current
phase without the interference of past knowledge. One of the teacher networks
retains the representations of the past phases, and the other teacher network
distills relation information of the current phase to the student network.
Extensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our
proposed method boosts the performance of self-supervised class incremental
learning. We will release code in the near future.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08768" title="Abstract">arXiv:2311.08768</a> [<a href="/pdf/2311.08768" title="Download PDF">pdf</a>, <a href="/ps/2311.08768" title="Download PostScript">ps</a>, <a href="/format/2311.08768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three Conjectures on Unexpectedeness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sileno%2C+G">Giovanni Sileno</a>, 
<a href="/search/cs?searchtype=author&query=Dessalles%2C+J">Jean-Louis Dessalles</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Information Theory (cs.IT); Systems and Control (eess.SY)

</div>
<p class="mathjax">Unexpectedness is a central concept in Simplicity Theory, a theory of
cognition relating various inferential processes to the computation of
Kolmogorov complexities, rather than probabilities. Its predictive power has
been confirmed by several experiments with human subjects, yet its theoretical
basis remains largely unexplored: why does it work? This paper lays the
groundwork for three theoretical conjectures. First, unexpectedness can be seen
as a generalization of Bayes' rule. Second, the frequentist core of
unexpectedness can be connected to the function of tracking ergodic properties
of the world. Third, unexpectedness can be seen as constituent of various
measures of divergence between the entropy of the world (environment) and the
variety of the observer (system). The resulting framework hints to research
directions that go beyond the division between probabilistic and logical
approaches, potentially bringing new insights into the extraction of causal
relations, and into the role of descriptive mechanisms in learning.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08769" title="Abstract">arXiv:2311.08769</a> [<a href="/pdf/2311.08769" title="Download PDF">pdf</a>, <a href="/format/2311.08769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> adF: A Novel System for Measuring Web Fingerprinting through Ads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bermejo-Agueda%2C+M+A">Miguel A. Bermejo-Agueda</a> (1), 
<a href="/search/cs?searchtype=author&query=Callejo%2C+P">Patricia Callejo</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Cuevas%2C+R">Rub&#xe9;n Cuevas</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Cuevas%2C+%C3%81">&#xc1;ngel Cuevas</a> (1 and 2) ((1) Universidad Carlos III de Madrid, (2) uc3m-Santander Big Data Institute)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 figures, 4 tables; added keywords
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">This paper introduces adF, a novel system for analyzing the vulnerability of
different devices, Operating Systems (OSes), and browsers to web
fingerprinting. adF performs its measurements from code inserted in ads. We
have used our system in several ad campaigns that delivered 5,40 million ad
impressions. The collected data enable us to assess the vulnerability of
current desktop and mobile devices to web fingerprinting. Based on our results,
we estimate that 64% of desktop devices and 40% of mobile devices can be
uniquely fingerprinted with our web fingerprinting system. However, the
resilience to web fingerprinting varies significantly across browsers and
device types, with Chrome on desktops being the most vulnerable configuration.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08770" title="Abstract">arXiv:2311.08770</a> [<a href="/pdf/2311.08770" title="Download PDF">pdf</a>, <a href="/ps/2311.08770" title="Download PostScript">ps</a>, <a href="/format/2311.08770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A proof-of-concept online metadata catalogue service of Earth  observation datasets for human health research in exposomics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koh%2C+K">Keumseok Koh</a>, 
<a href="/search/cs?searchtype=author&query=Boulos%2C+M+N+K">Maged N. Kamel Boulos</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Gang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongsheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Iyyanki%2C+M+V">Muralikrishna V. Iyyanki</a>, 
<a href="/search/cs?searchtype=author&query=Bwambale%2C+B">Bosco Bwambale</a>, 
<a href="/search/cs?searchtype=author&query=Dewan%2C+A">Ashraf Dewan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Databases (cs.DB)

</div>
<p class="mathjax">This article describes research carried out during 2023 under an
International Society for Photogrammetry and Remote Sensing (ISPRS)-funded
project to develop and disseminate a metadata catalogue of Earth observation
data sources/products and types that are relevant to human health research in
exposomics, as a free service to interested researchers worldwide. The
proof-of-concept catalogue was informed by input from existing research
literature on the subject (desk research), as well as online communications
with, and relevant research publications collected from, a small panel (n = 5)
of select experts from the academia in three countries (China, UK and USA). It
has 90 metadata records of relevant Earth observation datasets (n = 40) and
associated health-focused research publications (n = 50). The project's online
portal offers a searchable version of the catalogue featuring a number of
search modes and filtering options. It is hoped future, more comprehensive
versions of this service will enable more researchers and studies to discover
and use remote sensing data about population-level exposures to disease
determinants (exposomic determinants of disease) in combination with other
relevant data to reveal fresh insights that could improve our understanding of
relevant diseases, and hence contribute to the development of better-optimized
prevention and management plans to tackle them.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08776" title="Abstract">arXiv:2311.08776</a> [<a href="/pdf/2311.08776" title="Download PDF">pdf</a>, <a href="/format/2311.08776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context Adaptive Cooperation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Albouy%2C+T">Timoth&#xe9; Albouy</a>, 
<a href="/search/cs?searchtype=author&query=Frey%2C+D">Davide Frey</a>, 
<a href="/search/cs?searchtype=author&query=Gestin%2C+M">Mathieu Gestin</a>, 
<a href="/search/cs?searchtype=author&query=Raynal%2C+M">Michel Raynal</a>, 
<a href="/search/cs?searchtype=author&query=Ta%C3%AFani%2C+F">Fran&#xe7;ois Ta&#xef;ani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Reliable broadcast and consensus are the two pillars that support a lot of
non-trivial fault-tolerant distributed middleware and fault-tolerant
distributed systems. While they have close definitions, they strongly differ in
the underlying assumptions needed to implement each of them. Reliable broadcast
can be implemented in asynchronous systems in the presence of crash or
Byzantine failures while Consensus cannot. This key difference stems from the
fact that consensus involves synchronization between multiple processes that
concurrently propose values, while reliable broadcast simply involves
delivering a message from a predefined sender. This paper strikes a balance
between these two agreement abstractions in the presence of Byzantine failures.
It proposes CAC, a novel agreement abstraction that enables multiple processes
to broadcast messages simultaneously, while guaranteeing that (despite
potential conflicts, asynchrony, and Byzantine behaviors) the non-faulty
processes will agree on messages deliveries. We show that this novel
abstraction can enable more efficient algorithms for a variety of applications
(such as money transfer where several people can share a same account). This is
obtained by focusing the need for synchronization only on the processes that
actually need to synchronize.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08778" title="Abstract">arXiv:2311.08778</a> [<a href="/pdf/2311.08778" title="Download PDF">pdf</a>, <a href="/format/2311.08778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gitor: Scalable Code Clone Detection by Building Global Sample
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shan%2C+J">Junjie Shan</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yueming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hairu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Code clone detection is about finding out similar code fragments, which has
drawn much attention in software engineering since it is important for software
maintenance and evolution. Researchers have proposed many techniques and tools
for source code clone detection, but current detection methods concentrate on
analyzing or processing code samples individually without exploring the
underlying connections among code samples. In this paper, we propose Gitor to
capture the underlying connections among different code samples. Specifically,
given a source code database, we first tokenize all code samples to extract the
pre-defined individual information. After obtaining all samples individual
information, we leverage them to build a large global sample graph where each
node is a code sample or a type of individual information. Then we apply a node
embedding technique on the global sample graph to extract all the samples
vector representations. After collecting all code samples vectors, we can
simply compare the similarity between any two samples to detect possible clone
pairs. More importantly, since the obtained vector of a sample is from a global
sample graph, we can combine it with its own code features to improve the code
clone detection performance. To demonstrate the effectiveness of Gitor, we
evaluate it on a widely used dataset namely BigCloneBench. Our experimental
results show that Gitor has higher accuracy in terms of code clone detection
and excellent execution time for inputs of various sizes compared to existing
state-of-the-art tools. Moreover, we also evaluate the combination of Gitor
with other traditional vector-based clone detection methods, the results show
that the use of Gitor enables them detect more code clones with higher F1.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08782" title="Abstract">arXiv:2311.08782</a> [<a href="/pdf/2311.08782" title="Download PDF">pdf</a>, <a href="/format/2311.08782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Semantic Graph Guided Data-Efficient Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wenxuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+L">Lincan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jingxuan Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Developing generalizable models that can effectively learn from limited data
and with minimal reliance on human supervision is a significant objective
within the machine learning community, particularly in the era of deep neural
networks. Therefore, to achieve data-efficient learning, researchers typically
explore approaches that can leverage more related or unlabeled data without
necessitating additional manual labeling efforts, such as Semi-Supervised
Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL
leverages unlabeled data in the training process, while TL enables the transfer
of expertise from related data distributions. DA broadens the dataset by
synthesizing new data from existing examples. However, the significance of
additional knowledge contained within labels has been largely overlooked in
research. In this paper, we propose a novel perspective on data efficiency that
involves exploiting the semantic information contained in the labels of the
available data. Specifically, we introduce a Language Semantic Graph (LSG)
which is constructed from labels manifest as natural language descriptions.
Upon this graph, an auxiliary graph neural network is trained to extract
high-level semantic relations and then used to guide the training of the
primary model, enabling more adequate utilization of label knowledge. Across
image, video, and audio modalities, we utilize the LSG method in both TL and
SSL scenarios and illustrate its versatility in significantly enhancing
performance compared to other data-efficient learning approaches. Additionally,
our in-depth analysis shows that the LSG method also expedites the training
process.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08783" title="Abstract">arXiv:2311.08783</a> [<a href="/pdf/2311.08783" title="Download PDF">pdf</a>, <a href="/format/2311.08783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly  Care Home
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paster%2C+S">Sveta Paster</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+K">Kantwon Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Briggs%2C+G">Gordon Briggs</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>, 
<a href="/search/cs?searchtype=author&query=Mirsky%2C+R">Reuth Mirsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This report is part of ICRA roboethics competition : <a href="https://competition.raiselab.ca/competition-details-2023_1/ethics-challenge/submitted-proposals/submission-1">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the projected surge in the elderly population, service robots offer a
promising avenue to enhance their well-being in elderly care homes. Such robots
will encounter complex scenarios which will require them to perform decisions
with ethical consequences. In this report, we propose to leverage the
Intelligent Disobedience framework in order to give the robot the ability to
perform a deliberation process over decisions with potential ethical
implications. We list the issues that this framework can assist with, define it
formally in the context of the specific elderly care home scenario, and
delineate the requirements for implementing an intelligently disobeying robot.
We conclude this report with some critical analysis and suggestions for future
work.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08786" title="Abstract">arXiv:2311.08786</a> [<a href="/pdf/2311.08786" title="Download PDF">pdf</a>, <a href="/format/2311.08786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HFORD: High-Fidelity and Occlusion-Robust De-identification for Face  Privacy Protection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mingrui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nannan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinbo Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With the popularity of smart devices and the development of computer vision
technology, concerns about face privacy protection are growing. The face
de-identification technique is a practical way to solve the identity protection
problem. The existing facial de-identification methods have revealed several
problems, including the impact on the realism of anonymized results when faced
with occlusions and the inability to maintain identity-irrelevant details in
anonymized results. We present a High-Fidelity and Occlusion-Robust
De-identification (HFORD) method to deal with these issues. This approach can
disentangle identities and attributes while preserving image-specific details
such as background, facial features (e.g., wrinkles), and lighting, even in
occluded scenes. To disentangle the latent codes in the GAN inversion space, we
introduce an Identity Disentanglement Module (IDM). This module selects the
latent codes that are closely related to the identity. It further separates the
latent codes into identity-related codes and attribute-related codes, enabling
the network to preserve attributes while only modifying the identity. To ensure
the preservation of image details and enhance the network's robustness to
occlusions, we propose an Attribute Retention Module (ARM). This module
adaptively preserves identity-irrelevant details and facial occlusions and
blends them into the generated results in a modulated manner. Extensive
experiments show that our method has higher quality, better detail fidelity,
and stronger occlusion robustness than other face de-identification methods.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08787" title="Abstract">arXiv:2311.08787</a> [<a href="/pdf/2311.08787" title="Download PDF">pdf</a>, <a href="/format/2311.08787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polygonal Cone Control Barrier Functions (PolyC2BF) for safe navigation  in cluttered environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tayal%2C+M">Manan Tayal</a>, 
<a href="/search/cs?searchtype=author&query=Kolathaya%2C+S">Shishir Kolathaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 Pages, 6 Figures. arXiv admin note: text overlap with <a href="/abs/2303.15871">arXiv:2303.15871</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In fields such as mining, search and rescue, and archaeological exploration,
ensuring real-time, collision-free navigation of robots in confined, cluttered
environments is imperative. Despite the value of established path planning
algorithms, they often face challenges in convergence rates and handling
dynamic infeasibilities. Alternative techniques like collision cones struggle
to accurately represent complex obstacle geometries. This paper introduces a
novel category of control barrier functions, known as Polygonal Cone Control
Barrier Function (PolyC2BF), which addresses overestimation and computational
complexity issues. The proposed PolyC2BF, formulated as a Quadratic Programming
(QP) problem, proves effective in facilitating collision-free movement of
multiple robots in complex environments. The efficacy of this approach is
further demonstrated through PyBullet simulations on quadruped (unicycle
model), and crazyflie 2.1 (quadrotor model) in cluttered environments.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08788" title="Abstract">arXiv:2311.08788</a> [<a href="/pdf/2311.08788" title="Download PDF">pdf</a>, <a href="/format/2311.08788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented  Instruction Tuning with Auxiliary Evaluation Aspects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Minqian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Ying Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+E">Eunah Cho</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vaibhav Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Ghanadan%2C+R">Reza Ghanadan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lifu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures, 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Natural Language Generation (NLG) typically involves evaluating the generated
text in various aspects (e.g., consistency and naturalness) to obtain a
comprehensive assessment. However, multi-aspect evaluation remains challenging
as it may require the evaluator to generalize to any given evaluation aspect
even if it's absent during training. In this paper, we introduce X-Eval, a
two-stage instruction tuning framework to evaluate the text in both seen and
unseen aspects customized by end users. X-Eval consists of two learning stages:
the vanilla instruction tuning stage that improves the model's ability to
follow evaluation instructions, and an enhanced instruction tuning stage that
exploits the connections between fine-grained evaluation aspects to better
assess text quality. To support the training of X-Eval, we collect
AspectInstruct, the first instruction tuning dataset tailored for multi-aspect
NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance
task diversity, we devise an augmentation strategy that converts human rating
annotations into diverse forms of NLG evaluation tasks, including scoring,
comparison, ranking, and Boolean question answering. Extensive experiments
across three essential categories of NLG tasks: dialogue generation,
summarization, and data-to-text coupled with 21 aspects in meta-evaluation,
demonstrate that our X-Eval enables even a lightweight language model to
achieve a comparable if not higher correlation with human judgments compared to
the state-of-the-art NLG evaluators, such as GPT-4.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08791" title="Abstract">arXiv:2311.08791</a> [<a href="/pdf/2311.08791" title="Download PDF">pdf</a>, <a href="/format/2311.08791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Direct Approach for Solving Cloud Computing Task Assignment with Soft  Deadlines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+G">Guang Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages,8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Job scheduling in cloud computing environments is a critical yet complex
problem. Cloud computing user job requirements are highly dynamic and
uncertain, while cloud computing resources are heterogeneous and constrained.
This paper studies the online resource allocation problem for elastic computing
jobs with soft deadlines in cloud computing environments. The main
contributions include: 1) Integer linear programming modeling is used to design
an auction time scheduling framework with three key modules - resource
allocation, evaluation, and operation, which can dynamically allocate resources
in closed loops. 2) Methods such as time-based single resource utilization
evaluation and weighted average evaluation are proposed to evaluate resource
usage efficiency. 3) Soft acceptance protocols are introduced to achieve
elastic online resource allocation. 4) The time complexity of the proposed
algorithms is analyzed and proven to be polynomial time, demonstrating
efficiency. 5) Modular design makes the framework extensible. This paper
provides a structured cloud computing auction framework as a reference for
building practical cloud resource management systems. Future work may explore
more complex models of random arrival and multi-dimensional resource
constraints, evaluate algorithm performance on real cloud workloads, and
further enhance system robustness, efficiency and fairness.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08793" title="Abstract">arXiv:2311.08793</a> [<a href="/pdf/2311.08793" title="Download PDF">pdf</a>, <a href="/format/2311.08793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> German FinBERT: A German Pre-trained Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scherrmann%2C+M">Moritz Scherrmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">This study presents German FinBERT, a novel pre-trained German language model
tailored for financial textual data. The model is trained through a
comprehensive pre-training process, leveraging a substantial corpus comprising
financial reports, ad-hoc announcements and news related to German companies.
The corpus size is comparable to the data sets commonly used for training
standard BERT models. I evaluate the performance of German FinBERT on
downstream tasks, specifically sentiment prediction, topic recognition and
question answering against generic German language models. My results
demonstrate improved performance on finance-specific data, indicating the
efficacy of German FinBERT in capturing domain-specific nuances. The presented
findings suggest that German FinBERT holds promise as a valuable tool for
financial text analysis, potentially benefiting various applications in the
financial domain.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08795" title="Abstract">arXiv:2311.08795</a> [<a href="/pdf/2311.08795" title="Download PDF">pdf</a>, <a href="/format/2311.08795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancements and Challenges in Object-Centric Process Mining: A  Systematic Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berti%2C+A">Alessandro Berti</a>, 
<a href="/search/cs?searchtype=author&query=Montali%2C+M">Marco Montali</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Aalst%2C+W+M+P">Wil M.P. van der Aalst</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Recent years have seen the emergence of object-centric process mining
techniques. Born as a response to the limitations of traditional process mining
in analyzing event data from prevalent information systems like CRM and ERP,
these techniques aim to tackle the deficiency, convergence, and divergence
issues seen in traditional event logs. Despite the promise, the adoption in
real-world process mining analyses remains limited. This paper embarks on a
comprehensive literature review of object-centric process mining, providing
insights into the current status of the discipline and its historical
trajectory.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08798" title="Abstract">arXiv:2311.08798</a> [<a href="/pdf/2311.08798" title="Download PDF">pdf</a>, <a href="/format/2311.08798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-GRL: An Empirical Assessment of Explainable GNN-DRL in B5G/6G Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezazadeh%2C+F">Farhad Rezazadeh</a>, 
<a href="/search/cs?searchtype=author&query=Barrachina-MuNoz%2C+S">Sergio Barrachina-MuNoz</a>, 
<a href="/search/cs?searchtype=author&query=Zeydan%2C+E">Engin Zeydan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Houbing Song</a>, 
<a href="/search/cs?searchtype=author&query=Subbalakshmi%2C+K+P">K.P. Subbalakshmi</a>, 
<a href="/search/cs?searchtype=author&query=Mangues-Bafalluy%2C+J">Josep Mangues-Bafalluy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The rapid development of artificial intelligence (AI) techniques has
triggered a revolution in beyond fifth-generation (B5G) and upcoming
sixth-generation (6G) mobile networks. Despite these advances, efficient
resource allocation in dynamic and complex networks remains a major challenge.
This paper presents an experimental implementation of deep reinforcement
learning (DRL) enhanced with graph neural networks (GNNs) on a real 5G testbed.
The method addresses the explainability of GNNs by evaluating the importance of
each edge in determining the model's output. The custom sampling functions feed
the data into the proposed GNN-driven Monte Carlo policy gradient (REINFORCE)
agent to optimize the gNodeB (gNB) radio resources according to the specific
traffic demands. The demo demonstrates real-time visualization of network
parameters and superior performance compared to benchmarks.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08799" title="Abstract">arXiv:2311.08799</a> [<a href="/pdf/2311.08799" title="Download PDF">pdf</a>, <a href="/format/2311.08799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target  Approaching in Robotic Eye Surgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhihao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Siyuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zapp%2C+D">Daniel Zapp</a>, 
<a href="/search/cs?searchtype=author&query=Maier%2C+M">Mathias Maier</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kai Huang</a>, 
<a href="/search/cs?searchtype=author&query=Navab%2C+N">Nassir Navab</a>, 
<a href="/search/cs?searchtype=author&query=Nasseri%2C+M+A">M. Ali Nasseri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Robotic ophthalmic surgery is an emerging technology to facilitate
high-precision interventions such as retina penetration in subretinal injection
and removal of floating tissues in retinal detachment depending on the input
imaging modalities such as microscopy and intraoperative OCT (iOCT). Although
iOCT is explored to locate the needle tip within its range-limited ROI, it is
still difficult to coordinate iOCT's motion with the needle, especially at the
initial target-approaching stage. Meanwhile, due to 2D perspective projection
and thus the loss of depth information, current image-based methods cannot
effectively estimate the needle tip's trajectory towards both retinal and
floating targets. To address this limitation, we propose to use the shadow
positions of the target and the instrument tip to estimate their relative depth
position and accordingly optimize the instrument tip's insertion trajectory
until the tip approaches targets within iOCT's scanning area. Our method
succeeds target approaching on a retina model, and achieves an average depth
error of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively
in the surgical simulator without damaging the retina.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08803" title="Abstract">arXiv:2311.08803</a> [<a href="/pdf/2311.08803" title="Download PDF">pdf</a>, <a href="/format/2311.08803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StrategyLLM: Large Language Models as Strategy Generators, Executors,  Optimizers, and Evaluators for Problem Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haiyun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Most existing chain-of-thought (CoT) prompting methods suffer from the issues
of generalizability and consistency, as they often rely on instance-specific
solutions that may not be applicable to other cases and lack task-level
consistency in their reasoning steps. To address these limitations, we propose
a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to
tackle various tasks. The framework improves generalizability by formulating
general problem-solving strategies and enhances consistency by producing
consistent solutions using these strategies. StrategyLLM employs four LLM-based
agents: strategy generator, executor, optimizer, and evaluator, working
together to generate, evaluate, and select promising strategies for a given
task automatically. The experimental results demonstrate that StrategyLLM
outperforms the competitive baseline CoT-SC that requires human-annotated
solutions on 13 datasets across 4 challenging tasks without human involvement,
including math reasoning (39.2% $\rightarrow$ 43.3%), commonsense reasoning
(70.3% $\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\rightarrow$ 62.0%),
and symbolic reasoning (30.0% $\rightarrow$ 79.2%).
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08804" title="Abstract">arXiv:2311.08804</a> [<a href="/pdf/2311.08804" title="Download PDF">pdf</a>, <a href="/format/2311.08804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel Capacity and Bounds In Mixed Gaussian-Impulsive Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+T">Tianfu Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Q">Qihang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoping Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaonan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Communication systems suffer from the mixed noise consisting of both
non-Gaussian impulsive noise (IN) and white Gaussian noise (WGN) in many
practical applications. However, there is little literature about the channel
capacity under mixed noise. In this paper, we prove the existence of the
capacity under p-th moment constraint and show that there are only finite mass
points in the capacity-achieving distribution. Moreover, we provide lower and
upper capacity bounds with closed forms. It is shown that the lower bounds can
degenerate to the well-known Shannon formula under special scenarios. In
addition, the capacity for specific modulations and the corresponding lower
bounds are discussed. Numerical results reveal that the capacity decreases when
the impulsiveness of the mixed noise becomes dominant and the obtained capacity
bounds are shown to be very tight.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08806" title="Abstract">arXiv:2311.08806</a> [<a href="/pdf/2311.08806" title="Download PDF">pdf</a>, <a href="/format/2311.08806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in  Spiking Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shanlin Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiyi Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As the third-generation neural network, the Spiking Neural Network (SNN) has
the advantages of low power consumption and high energy efficiency, making it
suitable for implementation on edge devices. More recently, the most advanced
SNN, Spikformer, combines the self-attention module from Transformer with SNN
to achieve remarkable performance. However, it adopts larger channel dimensions
in MLP layers, leading to an increased number of redundant model parameters. To
effectively decrease the computational complexity and weight parameters of the
model, we explore the Lottery Ticket Hypothesis (LTH) and discover a very
sparse ($\ge$90%) subnetwork that achieves comparable performance to the
original network. Furthermore, we also design a lightweight token selector
module, which can remove unimportant background information from images based
on the average spike firing rate of neurons, selecting only essential
foreground image tokens to participate in attention calculation. Based on that,
we present SparseSpikformer, a co-design framework aimed at achieving sparsity
in Spikformer through token and weight pruning techniques. Experimental results
demonstrate that our framework can significantly reduce 90% model parameters
and cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining
the accuracy of the original model.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08807" title="Abstract">arXiv:2311.08807</a> [<a href="/pdf/2311.08807" title="Download PDF">pdf</a>, <a href="/format/2311.08807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLP-Based Techniques for Cyber Threat Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arazzi%2C+M">Marco Arazzi</a>, 
<a href="/search/cs?searchtype=author&query=Arikkat%2C+D+R">Dincy R. Arikkat</a>, 
<a href="/search/cs?searchtype=author&query=Nicolazzo%2C+S">Serena Nicolazzo</a>, 
<a href="/search/cs?searchtype=author&query=Nocera%2C+A">Antonino Nocera</a>, 
<a href="/search/cs?searchtype=author&query=A.%2C+R+R+K">Rafidha Rehiman K. A.</a>, 
<a href="/search/cs?searchtype=author&query=P.%2C+V">Vinod P.</a>, 
<a href="/search/cs?searchtype=author&query=Conti%2C+M">Mauro Conti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In the digital era, threat actors employ sophisticated techniques for which,
often, digital traces in the form of textual data are available. Cyber Threat
Intelligence~(CTI) is related to all the solutions inherent to data collection,
processing, and analysis useful to understand a threat actor's targets and
attack behavior. Currently, CTI is assuming an always more crucial role in
identifying and mitigating threats and enabling proactive defense strategies.
In this context, NLP, an artificial intelligence branch, has emerged as a
powerful tool for enhancing threat intelligence capabilities. This survey paper
provides a comprehensive overview of NLP-based techniques applied in the
context of threat intelligence. It begins by describing the foundational
definitions and principles of CTI as a major tool for safeguarding digital
assets. It then undertakes a thorough examination of NLP-based techniques for
CTI data crawling from Web sources, CTI data analysis, Relation Extraction from
cybersecurity data, CTI sharing and collaboration, and security threats of CTI.
Finally, the challenges and limitations of NLP in threat intelligence are
exhaustively examined, including data quality issues and ethical
considerations. This survey draws a complete framework and serves as a valuable
resource for security professionals and researchers seeking to understand the
state-of-the-art NLP-based threat intelligence techniques and their potential
impact on cybersecurity.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08810" title="Abstract">arXiv:2311.08810</a> [<a href="/pdf/2311.08810" title="Download PDF">pdf</a>, <a href="/format/2311.08810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wireless Communications in Cavity: A Reconfigurable Boundary Modulation  based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dong%2C+X">Xuehui Dong</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/eess?searchtype=author&query=Lai%2C+B">Bokai Lai</a>, 
<a href="/search/eess?searchtype=author&query=Xiong%2C+R">Rujing Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Mi%2C+T">Tiebin Mi</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+R+C">Robert Caiming Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper explores the potential wireless communication applications of
Reconfigurable Intelligent Surfaces (RIS) in reverberant wave propagation
environments. Unlike in free space, we utilize the sensitivity to boundaries of
the enclosed electromagnetic (EM) field and the equivalent perturbation of
RISs. For the first time, we introduce the framework of reconfigurable boundary
modulation in the cavities . We have proposed a robust boundary modulation
scheme that exploits the continuity of object motion and the mutation of the
codebook switch, which achieves pulse position modulation (PPM) by
RIS-generated equivalent pulses for wireless communication in cavities. This
approach achieves around 2 Mbps bit rate in the prototype and demonstrates
strong resistance to channel's frequency selectivity resulting in an extremely
low bit error rate (BER).
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08811" title="Abstract">arXiv:2311.08811</a> [<a href="/pdf/2311.08811" title="Download PDF">pdf</a>, <a href="/format/2311.08811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlation-aware active learning for surgery video segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Marquez-Neila%2C+P">Pablo Marquez-Neila</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Mingyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Rafii-Tari%2C+H">Hedyeh Rafii-Tari</a>, 
<a href="/search/cs?searchtype=author&query=Sznitman%2C+R">Raphael Sznitman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024, 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semantic segmentation is a complex task that relies heavily on large amounts
of annotated image data. However, annotating such data can be time-consuming
and resource-intensive, especially in the medical domain. Active Learning (AL)
is a popular approach that can help to reduce this burden by iteratively
selecting images for annotation to improve the model performance. In the case
of video data, it is important to consider the model uncertainty and the
temporal nature of the sequences when selecting images for annotation. This
work proposes a novel AL strategy for surgery video segmentation, \COALSamp{},
COrrelation-aWare Active Learning. Our approach involves projecting images into
a latent space that has been fine-tuned using contrastive learning and then
selecting a fixed number of representative images from local clusters of video
frames. We demonstrate the effectiveness of this approach on two video datasets
of surgical instruments and three real-world video datasets. The datasets and
code will be made publicly available upon receiving necessary approvals.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08813" title="Abstract">arXiv:2311.08813</a> [<a href="/pdf/2311.08813" title="Download PDF">pdf</a>, <a href="/ps/2311.08813" title="Download PostScript">ps</a>, <a href="/format/2311.08813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comments on &quot;Dynamic Consensus Committee-Based for Secure Data Sharing  With Authorized Multi-Receiver Searchable Encryption&quot;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zi-Yuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tso%2C+R">Raylin Tso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Recently, Yang et al. introduced an efficient searchable encryption scheme
titled "Dynamic Consensus Committee-Based for Secure Data Sharing With
Authorized Multi-Receiver Searchable Encryption (DCC-SE)," published in IEEE
Transactions on Information Forensics and Security (DOI:
10.1109/TIFS.<a href="/abs/2023.33051">2023.33051</a>83). According to the authors, DCC-SE meets various
security requirements, especially the keyword trapdoor indistinguishability
against chosen keyword attacks (KT-IND-CKA). In this letter, however, we reveal
a significant vulnerability of DCC-SE: any users involved in the system can
execute attacks against KT-IND-CKA security. This flaw potentially results in
the unintended disclosure of sensitive keyword information related to the
documents. We present a detailed cryptanalysis on DCC-SE. In addition, to
address this vulnerability, we discuss the root cause and identify a flaw in
the security proof of DCC-SE. Subsequently, we provide a solution that
effectively addresses this concern without significantly increasing
computational overhead.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08815" title="Abstract">arXiv:2311.08815</a> [<a href="/pdf/2311.08815" title="Download PDF">pdf</a>, <a href="/format/2311.08815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Disentanglement by Leveraging Structure in Data  Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eastwood%2C+C">Cian Eastwood</a>, 
<a href="/search/cs?searchtype=author&query=von+K%C3%BCgelgen%2C+J">Julius von K&#xfc;gelgen</a>, 
<a href="/search/cs?searchtype=author&query=Ericsson%2C+L">Linus Ericsson</a>, 
<a href="/search/cs?searchtype=author&query=Bouchacourt%2C+D">Diane Bouchacourt</a>, 
<a href="/search/cs?searchtype=author&query=Vincent%2C+P">Pascal Vincent</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+M">Mark Ibrahim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Self-supervised representation learning often uses data augmentations to
induce some invariance to "style" attributes of the data. However, with
downstream tasks generally unknown at training time, it is difficult to deduce
a priori which attributes of the data are indeed "style" and can be safely
discarded. To address this, we introduce a more principled approach that seeks
to disentangle style features rather than discard them. The key idea is to add
multiple style embedding spaces where: (i) each is invariant to all-but-one
augmentation; and (ii) joint entropy is maximized. We formalize our structured
data-augmentation procedure from a causal latent-variable-model perspective,
and prove identifiability of both content and (multiple blocks of) style
variables. We empirically demonstrate the benefits of our approach on synthetic
datasets and then present promising but limited results on ImageNet.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08817" title="Abstract">arXiv:2311.08817</a> [<a href="/pdf/2311.08817" title="Download PDF">pdf</a>, <a href="/format/2311.08817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAP&#x27;s not dead yet: Uncovering true language model modes by conditioning  away degeneracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+D">Davis Yoshida</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+K">Kartik Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Gimpel%2C+K">Kevin Gimpel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">It has been widely observed that exact or approximate MAP (mode-seeking)
decoding from natural language generation (NLG) models consistently leads to
degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has
generally been attributed to either a fundamental inadequacy of modes in models
or weaknesses in language modeling. Contrastingly in this work, we emphasize
that degenerate modes can even occur in the absence of any model error, due to
contamination of the training data. Specifically, we show that mixing even a
tiny amount of low-entropy noise with a population text distribution can cause
the data distribution's mode to become degenerate, implying that any models
trained on it will be as well. As the unconditional mode of NLG models will
often be degenerate, we therefore propose to apply MAP decoding to the model's
distribution conditional on avoiding specific degeneracies. Using exact-search,
we empirically verify that the length-conditional modes of machine translation
models and language models are indeed more fluent and topical than their
unconditional modes. For the first time, we also share many examples of exact
modal sequences from these models, and from several variants of the LLaMA-7B
model. Notably, the modes of the LLaMA models are still degenerate, showing
that improvements in modeling have not fixed this issue. Because of the cost of
exact mode finding algorithms, we develop an approximate mode finding approach,
ACBS, which finds sequences that are both high-likelihood and high-quality. We
apply this approach to LLaMA-7B, a model which was not trained for instruction
following, and find that we are able to elicit reasonable outputs without any
finetuning.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08819" title="Abstract">arXiv:2311.08819</a> [<a href="/pdf/2311.08819" title="Download PDF">pdf</a>, <a href="/format/2311.08819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frequency Domain-based Dataset Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+D">Donghyeok Shin</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Seungjae Shin</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+I">Il-Chul Moon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper presents FreD, a novel parameterization method for dataset
distillation, which utilizes the frequency domain to distill a small-sized
synthetic dataset from a large-sized original dataset. Unlike conventional
approaches that focus on the spatial domain, FreD employs frequency-based
transforms to optimize the frequency representations of each data instance. By
leveraging the concentration of spatial domain information on specific
frequency components, FreD intelligently selects a subset of frequency
dimensions for optimization, leading to a significant reduction in the required
budget for synthesizing an instance. Through the selection of frequency
dimensions based on the explained variance, FreD demonstrates both theoretical
and empirical evidence of its ability to operate efficiently within a limited
budget, while better preserving the information of the original dataset
compared to conventional parameterization methods. Furthermore, based on the
orthogonal compatibility of FreD with existing methods, we confirm that FreD
consistently improves the performances of existing distillation methods over
the evaluation scenarios with different benchmark datasets. We release the code
at https://github.com/sdh0818/FreD.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08820" title="Abstract">arXiv:2311.08820</a> [<a href="/pdf/2311.08820" title="Download PDF">pdf</a>, <a href="/format/2311.08820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning with Model Predictive Control for Highway Ramp  Metering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Airaldi%2C+F">Filippo Airaldi</a>, 
<a href="/search/eess?searchtype=author&query=De+Schutter%2C+B">Bart De Schutter</a>, 
<a href="/search/eess?searchtype=author&query=Dabiri%2C+A">Azita Dabiri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 10 figures, 3 tables, submitted to IEEE Transactions on Intelligent Transportation Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the backdrop of an increasingly pressing need for effective urban and
highway transportation systems, this work explores the synergy between
model-based and learning-based strategies to enhance traffic flow management by
use of an innovative approach to the problem of highway ramp metering control
that embeds Reinforcement Learning techniques within the Model Predictive
Control framework. The control problem is formulated as an RL task by crafting
a suitable stage cost function that is representative of the traffic
conditions, variability in the control action, and violations of a
safety-critical constraint on the maximum number of vehicles in queue. An
MPC-based RL approach, which merges the advantages of the two paradigms in
order to overcome the shortcomings of each framework, is proposed to learn to
efficiently control an on-ramp and to satisfy its constraints despite
uncertainties in the system model and variable demands. Finally, simulations
are performed on a benchmark from the literature consisting of a small-scale
highway network. Results show that, starting from an MPC controller that has an
imprecise model and is poorly tuned, the proposed methodology is able to
effectively learn to improve the control policy such that congestion in the
network is reduced and constraints are satisfied, yielding an improved
performance compared to the initial controller.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08821" title="Abstract">arXiv:2311.08821</a> [<a href="/pdf/2311.08821" title="Download PDF">pdf</a>, <a href="/format/2311.08821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thermal Finite Element Modeling and Simulation of a Squirrel-Cage  Induction Machine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bergfried%2C+C">Christian Bergfried</a>, 
<a href="/search/cs?searchtype=author&query=Sp%C3%A4ck-Leigsnering%2C+Y">Yvonne Sp&#xe4;ck-Leigsnering</a>, 
<a href="/search/cs?searchtype=author&query=Seebacher%2C+R">Roland Seebacher</a>, 
<a href="/search/cs?searchtype=author&query=Eickhoff%2C+H">Heinrich Eickhoff</a>, 
<a href="/search/cs?searchtype=author&query=Muetze%2C+A">Annette Muetze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Finite element models of electrical machines allow insights in electrothermal
stresses which endanger the insulation system of the machine. This paper
presents a thermal finite element model of a 3.7 kW squirrel-cage induction
machine. The model resolves the conductors and the surrounding insulation
materials in the stator slots. A set of transient thermal scenarios is defined
and measured in the machine laboratory. These data are used to assess the
finite element model.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08829" title="Abstract">arXiv:2311.08829</a> [<a href="/pdf/2311.08829" title="Download PDF">pdf</a>, <a href="/format/2311.08829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autoencoder with Group-based Decoder and Multi-task Optimization for  Anomalous Sound Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yifan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongxing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Haoran Wei</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yanhua Long</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In industry, machine anomalous sound detection (ASD) is in great demand.
However, collecting enough abnormal samples is difficult due to the high cost,
which boosts the rapid development of unsupervised ASD algorithms. Autoencoder
(AE) based methods have been widely used for unsupervised ASD, but suffer from
problems including 'shortcut', poor anti-noise ability and sub-optimal quality
of features. To address these challenges, we propose a new AE-based framework
termed AEGM. Specifically, we first insert an auxiliary classifier into AE to
enhance ASD in a multi-task learning manner. Then, we design a group-based
decoder structure, accompanied by an adaptive loss function, to endow the model
with domain-specific knowledge. Results on the DCASE 2021 Task 2 development
set show that our methods achieve a relative improvement of 13.11% and 15.20%
respectively in average AUC over the official AE and MobileNetV2 across test
sets of seven machines.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08832" title="Abstract">arXiv:2311.08832</a> [<a href="/pdf/2311.08832" title="Download PDF">pdf</a>, <a href="/format/2311.08832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Links between Conversational Agent Design Challenges and  Interdisciplinary Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadek%2C+M">Malak Sadek</a>, 
<a href="/search/cs?searchtype=author&query=Mougenot%2C+C">C&#xe9;line Mougenot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent years have seen a steady rise in the popularity and use of
Conversational Agents (CA) for different applications, well before the more
immediate impact of large language models. This rise has been accompanied by an
extensive exploration and documentation of the challenges of designing and
creating conversational agents. Focusing on a recent scoping review of the
socio-technical challenges of CA creation, this opinion paper calls for an
examination of the extent to which interdisciplinary collaboration (IDC)
challenges might contribute towards socio-technical CA design challenges. The
paper proposes a taxonomy of CA design challenges using IDC as a lens, and
proposes practical strategies to overcome them which complement existing design
principles. The paper invites future work to empirically verify suggested
conceptual links and apply the proposed strategies within the space of CA
design to evaluate their effectiveness.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08834" title="Abstract">arXiv:2311.08834</a> [<a href="/pdf/2311.08834" title="Download PDF">pdf</a>, <a href="/ps/2311.08834" title="Download PostScript">ps</a>, <a href="/format/2311.08834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A* search algorithm for an optimal investment problem in vehicle-sharing  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+B+L">Ba Luat Le</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+L">Layla Martin</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+E">Emrah Demir</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+D+M">Duc Minh Vu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version of the conference paper which is accepted to be appear in the proceeding of the The 12th International Conference on Computational Data and Social Networks - SCONET2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We study an optimal investment problem that arises in the context of the
vehicle-sharing system. Given a set of locations to build stations, we need to
determine i) the sequence of stations to be built and the number of vehicles to
acquire in order to obtain the target state where all stations are built, and
ii) the number of vehicles to acquire and their allocation in order to maximize
the total profit returned by operating the system when some or all stations are
open. The profitability associated with operating open stations, measured over
a specific time period, is represented as a linear optimization problem applied
to a collection of open stations. With operating capital, the owner of the
system can open new stations. This property introduces a set-dependent aspect
to the duration required for opening a new station, and the optimal investment
problem can be viewed as a variant of the Traveling Salesman Problem (TSP) with
set-dependent cost. We propose an A* search algorithm to address this
particular variant of the TSP. Computational experiments highlight the benefits
of the proposed algorithm in comparison to the widely recognized Dijkstra
algorithm and propose future research to explore new possibilities and
applications for both exact and approximate A* algorithms.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08835" title="Abstract">arXiv:2311.08835</a> [<a href="/pdf/2311.08835" title="Download PDF">pdf</a>, <a href="/format/2311.08835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlation-guided Query-Dependency Calibration in Video Representation  Learning for Temporal Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moon%2C+W">WonJun Moon</a>, 
<a href="/search/cs?searchtype=author&query=Hyun%2C+S">Sangeek Hyun</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">SuBeen Lee</a>, 
<a href="/search/cs?searchtype=author&query=Heo%2C+J">Jae-Pil Heo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 14 figures, 14 tables, Code is available at <a href="https://github.com/wjun0830/CGDETR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent endeavors in video temporal grounding enforce strong cross-modal
interactions through attention mechanisms to overcome the modality gap between
video and text query. However, previous works treat all video clips equally
regardless of their semantic relevance with the text query in attention
modules. In this paper, our goal is to provide clues for query-associated video
clips within the crossmodal encoding process. With our Correlation-Guided
Detection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of
cross-modal interactions and how to exploit such degrees for prediction. First,
we design an adaptive cross-attention layer with dummy tokens. Dummy tokens
conditioned by text query take a portion of the attention weights, preventing
irrelevant video clips from being represented by the text query. Yet, not all
word tokens equally inherit the text query's correlation to video clips. Thus,
we further guide the cross-attention map by inferring the fine-grained
correlation between video clips and words. We enable this by learning a joint
embedding space for high-level concepts, i.e., moment and sentence level, and
inferring the clip-word correlation. Lastly, we use a moment-adaptive saliency
detector to exploit each video clip's degrees of text engagement. We validate
the superiority of CG-DETR with the state-of-the-art results on various
benchmarks for both moment retrieval and highlight detection. Codes are
available at https://github.com/wjun0830/CGDETR.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08836" title="Abstract">arXiv:2311.08836</a> [<a href="/pdf/2311.08836" title="Download PDF">pdf</a>, <a href="/format/2311.08836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Gender Bias in the Translation of Gender-Neutral Languages  into English
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rarrick%2C+S">Spencer Rarrick</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+R">Ranjita Naik</a>, 
<a href="/search/cs?searchtype=author&query=Poudel%2C+S">Sundar Poudel</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhary%2C+V">Vishal Chowdhary</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Machine Translation (MT) continues to improve in quality and adoption, yet
the inadvertent perpetuation of gender bias remains a significant concern.
Despite numerous studies into gender bias in translations from gender-neutral
languages such as Turkish into more strongly gendered languages like English,
there are no benchmarks for evaluating this phenomenon or for assessing
mitigation strategies. To address this gap, we introduce GATE X-E, an extension
to the GATE (Rarrick et al., 2023) corpus, that consists of human translations
from Turkish, Hungarian, Finnish, and Persian into English. Each translation is
accompanied by feminine, masculine, and neutral variants for each possible
gender interpretation. The dataset, which contains between 1250 and 1850
instances for each of the four language pairs, features natural sentences with
a wide range of sentence lengths and domains, challenging translation rewriters
on various linguistic phenomena. Additionally, we present an English gender
rewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We
open source our contributions to encourage further research on gender
debiasing.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08838" title="Abstract">arXiv:2311.08838</a> [<a href="/pdf/2311.08838" title="Download PDF">pdf</a>, <a href="/format/2311.08838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disinformation Capabilities of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vykopal%2C+I">Ivan Vykopal</a>, 
<a href="/search/cs?searchtype=author&query=Pikuliak%2C+M">Mat&#xfa;&#x161; Pikuliak</a>, 
<a href="/search/cs?searchtype=author&query=Srba%2C+I">Ivan Srba</a>, 
<a href="/search/cs?searchtype=author&query=Moro%2C+R">Robert Moro</a>, 
<a href="/search/cs?searchtype=author&query=Macko%2C+D">Dominik Macko</a>, 
<a href="/search/cs?searchtype=author&query=Bielikova%2C+M">Maria Bielikova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automated disinformation generation is often listed as one of the risks of
large language models (LLMs). The theoretical ability to flood the information
space with disinformation content might have dramatic consequences for
democratic societies around the world. This paper presents a comprehensive
study of the disinformation capabilities of the current generation of LLMs to
generate false news articles in English language. In our study, we evaluated
the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated
several aspects of the LLMs: how well they are at generating news articles, how
strongly they tend to agree or disagree with the disinformation narratives, how
often they generate safety warnings, etc. We also evaluated the abilities of
detection models to detect these articles as LLM-generated. We conclude that
LLMs are able to generate convincing news articles that agree with dangerous
disinformation narratives.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08840" title="Abstract">arXiv:2311.08840</a> [<a href="/pdf/2311.08840" title="Download PDF">pdf</a>, <a href="/format/2311.08840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An MRL-Based Design Solution for RIS-Assisted MU-MIMO Wireless System  under Time-Varying Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+M+A">Meng-Qian Alexander Wu</a>, 
<a href="/search/eess?searchtype=author&query=Sang%2C+T">Tzu-Hsien Sang</a>, 
<a href="/search/eess?searchtype=author&query=Schuhmacher%2C+L">Luisa Schuhmacher</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+M">Ming-Jie Guo</a>, 
<a href="/search/eess?searchtype=author&query=Hammoud%2C+K">Khodr Hammoud</a>, 
<a href="/search/eess?searchtype=author&query=Pollin%2C+S">Sofie Pollin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in proceedings of the 2023 IEEE Conference on Global Communications (GLOBECOM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Utilizing Deep Reinforcement Learning (DRL) for Reconfigurable Intelligent
Surface (RIS) assisted wireless communication has been extensively researched.
However, existing DRL methods either act as a simple optimizer or only solve
problems with concurrent Channel State Information (CSI) represented in the
training data set. Consequently, solutions for RIS-assisted wireless
communication systems under time-varying environments are relatively
unexplored. However, communication problems should be considered with realistic
assumptions; for instance, in scenarios where the channel is time-varying, the
policy obtained by reinforcement learning should be applicable for situations
where CSI is not well represented in the training data set. In this paper, we
apply Meta-Reinforcement Learning (MRL) to the joint optimization problem of
active beamforming at the Base Station (BS) and phase shift at the RIS,
motivated by MRL's ability to extend the DRL concept of solving one Markov
Decision Problem (MDP) to multiple MDPs. We provide simulation results to
compare the average sum rate of the proposed approach with those of selected
forerunners in the literature. Our approach improves the sum rate by more than
60% under time-varying CSI assumption while maintaining the advantages of
typical DRL-based solutions. Our study's results emphasize the possibility of
utilizing MRL-based designs in RIS-assisted wireless communication systems
while considering realistic environment assumptions.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08843" title="Abstract">arXiv:2311.08843</a> [<a href="/pdf/2311.08843" title="Download PDF">pdf</a>, <a href="/format/2311.08843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Video Relighting Using Casual Light Stage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J+M">Jun Myeong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Christman%2C+M">Max Christman</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+R">Roni Sengupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">In this paper, we develop a personalized video relighting algorithm that
produces high-quality and temporally consistent relit video under any pose,
expression, and lighting conditions in real-time. Existing relighting
algorithms typically rely either on publicly available synthetic data, which
yields poor relighting results, or instead on Light Stage data which is
inaccessible and is not publicly available. We show that by casually capturing
video of a user watching YouTube videos on a monitor we can train a
personalized algorithm capable of producing high-quality relighting under any
condition. Our key contribution is a novel neural relighting architecture that
effectively separates the intrinsic appearance features, geometry and
reflectance, from the source lighting and then combines it with the target
lighting to generate a relit image. This neural architecture enables smoothing
of intrinsic appearance features leading to temporally stable video relighting.
Both qualitative and quantitative evaluations show that our relighting
architecture improves portrait image relighting quality and temporal
consistency over state-of-the-art approaches on both casually captured Light
Stage at Your Desk (LSYD) data and Light Stage captured One Light At a Time
(OLAT) datasets.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08844" title="Abstract">arXiv:2311.08844</a> [<a href="/pdf/2311.08844" title="Download PDF">pdf</a>, <a href="/format/2311.08844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Violet: A Vision-Language Model for Arabic Image Captioning with Gemini  Decoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+A">Abdelrahman Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Alwajih%2C+F">Fakhraddin Alwajih</a>, 
<a href="/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B">El Moatez Billah Nagoudi</a>, 
<a href="/search/cs?searchtype=author&query=Inciarte%2C+A+A">Alcides Alcoba Inciarte</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ArabicNLP Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Although image captioning has a vast array of applications, it has not
reached its full potential in languages other than English. Arabic, for
instance, although the native language of more than 400 million people, remains
largely underrepresented in this area. This is due to the lack of labeled data
and powerful Arabic generative models. We alleviate this issue by presenting a
novel vision-language model dedicated to Arabic, dubbed \textit{Violet}. Our
model is based on a vision encoder and a Gemini text decoder that maintains
generation fluency while allowing fusion between the vision and language
components. To train our model, we introduce a new method for automatically
acquiring data from available English datasets. We also manually prepare a new
dataset for evaluation. \textit{Violet} performs sizeably better than our
baselines on all of our evaluation datasets. For example, it reaches a CIDEr
score of $61.2$ on our manually annotated dataset and achieves an improvement
of $13$ points on Flickr8k.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08849" title="Abstract">arXiv:2311.08849</a> [<a href="/pdf/2311.08849" title="Download PDF">pdf</a>, <a href="/format/2311.08849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient  Large-scale Multilingual Continued Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yihong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+P">Peiqin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pretraining multilingual language models from scratch requires considerable
computational resources and substantial training data. Therefore, a more
efficient method is to adapt existing pretrained language models (PLMs) to new
languages via vocabulary extension and continued pretraining. However, this
method usually randomly initializes the embeddings of new subwords and
introduces substantially more embedding parameters to the language model, thus
weakening the efficiency. To address these issues, we propose a novel
framework: \textbf{O}ne \textbf{F}or \textbf{A}ll (\textbf{\textsc{Ofa}}),
which wisely initializes the embeddings of unseen subwords from target
languages and thus can adapt a PLM to multiple languages efficiently and
effectively. \textsc{Ofa} takes advantage of external well-aligned multilingual
word embeddings and injects the alignment knowledge into the new embeddings. In
addition, \textsc{Ofa} applies matrix factorization and replaces the cumbersome
embeddings with two lower-dimensional matrices, which significantly reduces the
number of parameters while not sacrificing the performance. Through extensive
experiments, we show models initialized by \textsc{Ofa} are efficient and
outperform several baselines. \textsc{Ofa} not only accelerates the convergence
of continued pretraining, which is friendly to a limited computation budget,
but also improves the zero-shot crosslingual transfer on a wide range of
downstream tasks. We make our code and models publicly available.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08850" title="Abstract">arXiv:2311.08850</a> [<a href="/pdf/2311.08850" title="Download PDF">pdf</a>, <a href="/format/2311.08850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling the Output of a Generative Model by Latent Feature Vector  Shifting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belanec%2C+R">R&#xf3;bert Belanec</a>, 
<a href="/search/cs?searchtype=author&query=Lacko%2C+P">Peter Lacko</a>, 
<a href="/search/cs?searchtype=author&query=Malinovsk%C3%A1%2C+K">Krist&#xed;na Malinovsk&#xe1;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, presented on DISA2023 conference in Ko\v{s}ice
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 World Symposium on Digital Intelligence for Systems and
  Machines (DISA), Ko\v{s}ice, Slovakia, 2023, pp. 24-30
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">State-of-the-art generative models (e.g. StyleGAN3 \cite{karras2021alias})
often generate photorealistic images based on vectors sampled from their latent
space. However, the ability to control the output is limited. Here we present
our novel method for latent vector shifting for controlled output image
modification utilizing semantic features of the generated images. In our
approach we use a pre-trained model of StyleGAN3 that generates images of
realistic human faces in relatively high resolution. We complement the
generative model with a convolutional neural network classifier, namely
ResNet34, trained to classify the generated images with binary facial features
from the CelebA dataset. Our latent feature shifter is a neural network model
with a task to shift the latent vectors of a generative model into a specified
feature direction. We have trained latent feature shifter for multiple facial
features, and outperformed our baseline method in the number of generated
images with the desired feature. To train our latent feature shifter neural
network, we have designed a dataset of pairs of latent vectors with and without
a certain feature. Based on the evaluation, we conclude that our latent feature
shifter approach was successful in the controlled generation of the StyleGAN3
generator.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08851" title="Abstract">arXiv:2311.08851</a> [<a href="/pdf/2311.08851" title="Download PDF">pdf</a>, <a href="/format/2311.08851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentations in Deep Weight Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shamsian%2C+A">Aviv Shamsian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D+W">David W. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Navon%2C+A">Aviv Navon</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kofinas%2C+M">Miltiadis Kofinas</a>, 
<a href="/search/cs?searchtype=author&query=Achituve%2C+I">Idan Achituve</a>, 
<a href="/search/cs?searchtype=author&query=Valperga%2C+R">Riccardo Valperga</a>, 
<a href="/search/cs?searchtype=author&query=Burghouts%2C+G+J">Gertjan J. Burghouts</a>, 
<a href="/search/cs?searchtype=author&query=Gavves%2C+E">Efstratios Gavves</a>, 
<a href="/search/cs?searchtype=author&query=Snoek%2C+C+G+M">Cees G. M. Snoek</a>, 
<a href="/search/cs?searchtype=author&query=Fetaya%2C+E">Ethan Fetaya</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>, 
<a href="/search/cs?searchtype=author&query=Maron%2C+H">Haggai Maron</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 Workshop on Symmetry and Geometry in Neural Representations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Learning in weight spaces, where neural networks process the weights of other
deep neural networks, has emerged as a promising research direction with
applications in various fields, from analyzing and editing neural fields and
implicit neural representations, to network pruning and quantization. Recent
works designed architectures for effective learning in that space, which takes
into account its unique, permutation-equivariant, structure. Unfortunately, so
far these architectures suffer from severe overfitting and were shown to
benefit from large datasets. This poses a significant challenge because
generating data for this learning setup is laborious and time-consuming since
each data sample is a full set of network weights that has to be trained. In
this paper, we address this difficulty by investigating data augmentations for
weight spaces, a set of techniques that enable generating new data examples on
the fly without having to train additional input weight space elements. We
first review several recently proposed data augmentation schemes %that were
proposed recently and divide them into categories. We then introduce a novel
augmentation scheme based on the Mixup method. We evaluate the performance of
these techniques on existing benchmarks as well as new benchmarks we generate,
which can be valuable for future studies.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08852" title="Abstract">arXiv:2311.08852</a> [<a href="/pdf/2311.08852" title="Download PDF">pdf</a>, <a href="/ps/2311.08852" title="Download PostScript">ps</a>, <a href="/format/2311.08852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Performance of Industrial IoT Communication Technologies: A  Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behnke%2C+I">Ilja Behnke</a>, 
<a href="/search/cs?searchtype=author&query=Austad%2C+H">Henrik Austad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Internet of Things Journal 2023 | Journal article DOI: 10.1109/JIOT.2023.3332507
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">With the growing need for automation and the ongoing merge of OT and IT,
industrial networks have to transport a high amount of heterogeneous data with
mixed criticality such as control traffic, sensor data, and configuration
messages. Current advances in IT technologies furthermore enable a new set of
automation scenarios under the roof of Industry 4.0 and IIoT where industrial
networks now have to meet new requirements in flexibility and reliability. The
necessary real-time guarantees will place significant demands on the networks.
In this paper, we identify IIoT use cases and infer real-time requirements
along several axes before bridging the gap between real-time network
technologies and the identified scenarios. We review real-time networking
technologies and present peer-reviewed works from the past 5 years for
industrial environments. We investigate how these can be applied to
controllers, systems, and embedded devices. Finally, we discuss open challenges
for real-time communication technologies to enable the identified scenarios.
The review shows academic interest in the field of real-time communication
technologies but also highlights a lack of a fixed set of standards important
for trust in safety and reliability, especially where wireless technologies are
concerned.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08854" title="Abstract">arXiv:2311.08854</a> [<a href="/pdf/2311.08854" title="Download PDF">pdf</a>, <a href="/ps/2311.08854" title="Download PostScript">ps</a>, <a href="/format/2311.08854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Formalization of Finite Group Theory: Part III
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Russinoff%2C+D+M">David M. Russinoff</a> (Arm Inc.)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 33-49
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">This is the third and final installment of an exposition of an ACL2
formalization of finite group theory. Part I covers groups and subgroups,
cosets, normal subgroups, and quotient groups. Part II extends the theory in
the developmnent of group homomorphisms and direct products, which are applied
in a proof of the Fundamental Theorem of Finite Abelian Groups. The central
topics of the present paper are the symmetric groups and the Sylow theorems,
which pertain to subgroups of prime power order. Since these theorems are based
on the conjugation of subgroups, an example of a group action on a set, their
presentation is preceded by a comprehensive treatment of group actions. Our
final result is mainly an application of the Sylow theorems: after showing that
the alternating group of order 60 is simple (i.e., has no proper normal
subgroup), we prove that no group of non-prime order less than 60 is simple.
The combined content of the groups directory is a close approximation to that
of an advanced undergraduate course taught by the author in 1976.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08855" title="Abstract">arXiv:2311.08855</a> [<a href="/pdf/2311.08855" title="Download PDF">pdf</a>, <a href="/format/2311.08855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Case Study in Analytic Protocol Analysis in ACL2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+Hippel%2C+M">Max von Hippel</a> (Northeastern University), 
<a href="/search/cs?searchtype=author&query=Manolios%2C+P">Panagiotis Manolios</a> (Northeastern University), 
<a href="/search/cs?searchtype=author&query=McMillan%2C+K+L">Kenneth L. McMillan</a> (University of Texas at Austin), 
<a href="/search/cs?searchtype=author&query=Nita-Rotaru%2C+C">Cristina Nita-Rotaru</a> (Northeastern University), 
<a href="/search/cs?searchtype=author&query=Zuck%2C+L">Lenore Zuck</a> (University of Illinois Chicago)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 50-66
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Mathematical Software (cs.MS)

</div>
<p class="mathjax">When verifying computer systems we sometimes want to study their asymptotic
behaviors, i.e., how they behave in the long run. In such cases, we need real
analysis, the area of mathematics that deals with limits and the foundations of
calculus. In a prior work, we used real analysis in ACL2s to study the
asymptotic behavior of the RTO computation, commonly used in congestion control
algorithms across the Internet. One key component in our RTO computation
analysis was proving in ACL2s that for all alpha in [0, 1), the limit as n
approaches infinity of alpha raised to n is zero. Whereas the most obvious
proof strategy involves the logarithm, whose codomain includes irrationals, by
default ACL2 only supports rationals, which forced us to take a non-standard
approach. In this paper, we explore different approaches to proving the above
result in ACL2(r) and ACL2s, from the perspective of a relatively new user to
each. We also contextualize the theorem by showing how it allowed us to prove
important asymptotic properties of the RTO computation. Finally, we discuss
tradeoffs between the various proof strategies and directions for future
research.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08856" title="Abstract">arXiv:2311.08856</a> [<a href="/pdf/2311.08856" title="Download PDF">pdf</a>, <a href="/ps/2311.08856" title="Download PostScript">ps</a>, <a href="/format/2311.08856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advances in ACL2 Proof Debugging Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaufmann%2C+M">Matt Kaufmann</a> (UT Austin, retired), 
<a href="/search/cs?searchtype=author&query=Moore%2C+J+S">J Strother Moore</a> (UT Austin, retired)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 67-81
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO); Software Engineering (cs.SE)

</div>
<p class="mathjax">The experience of an ACL2 user generally includes many failed proof attempts.
A key to successful use of the ACL2 prover is the effective use of tools to
debug those failures. We focus on changes made after ACL2 Version 8.5: the
improved break-rewrite utility and the new utility, with-brr-data.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08857" title="Abstract">arXiv:2311.08857</a> [<a href="/pdf/2311.08857" title="Download PDF">pdf</a>, <a href="/format/2311.08857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Counterexample Generation and Theory Exploration to Suggest  Missing Hypotheses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gamboa%2C+R">Ruben Gamboa</a> (University of Wyoming, Kestrel Institute), 
<a href="/search/cs?searchtype=author&query=Manolios%2C+P">Panagiotis Manolios</a> (Northeastern University), 
<a href="/search/cs?searchtype=author&query=Smith%2C+E">Eric Smith</a> (Kestrel Institute), 
<a href="/search/cs?searchtype=author&query=Thompson%2C+K">Kyle Thompson</a> (University of California San Diego)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 82-93
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Newcomers to ACL2 are sometimes surprised that ACL2 rejects formulas that
they believe should be theorems, such as (REVERSE (REVERSE X)) = X. Experienced
ACL2 users will recognize that the theorem only holds for intended values of X,
and given ACL2's total logic, there are many counterexamples for which this
formula is simply not true. Counterexample generation (cgen) is a technique
that helps by giving the user a number of counterexamples (and also witnesses)
to the formula, e.g., letting the user know that the intended theorem is false
when X is equal to 10. In this paper we describe a tool called DrLA that goes
further by suggesting additional hypotheses that will make the theorem true. In
this case, for example, DrLA may suggest that X needs to be either a TRUE-LIST
or a STRING. The suggestions are discovered using the ideas of theory
exploration and subsumption from automated theorem proving.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08858" title="Abstract">arXiv:2311.08858</a> [<a href="/pdf/2311.08858" title="Download PDF">pdf</a>, <a href="/format/2311.08858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Verification of Zero-Knowledge Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coglio%2C+A">Alessandro Coglio</a> (Kestrel Institute and Aleo Systems Inc.), 
<a href="/search/cs?searchtype=author&query=McCarthy%2C+E">Eric McCarthy</a> (Kestrel Institute and Aleo Systems Inc.), 
<a href="/search/cs?searchtype=author&query=Smith%2C+E+W">Eric W. Smith</a> (Kestrel Institute)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 94-112
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Cryptography and Security (cs.CR); Symbolic Computation (cs.SC)

</div>
<p class="mathjax">Zero-knowledge circuits are sets of equality constraints over arithmetic
expressions interpreted in a prime field; they are used to encode computations
in cryptographic zero-knowledge proofs. We make the following contributions to
the problem of ensuring that a circuit correctly encodes a computation: a
formal framework for circuit correctness; an ACL2 library for prime fields; an
ACL2 model of the existing R1CS (Rank-1 Constraint Systems) formalism to
represent circuits, along with ACL2 and Axe tools to verify circuits of this
form; a novel PFCS (Prime Field Constraint Systems) formalism to represent
hierarchically structured circuits, along with an ACL2 model of it and ACL2
tools to verify circuits of this form in a compositional and scalable way;
verification of circuits, ranging from simple to complex; and discovery of bugs
and optimizations in existing zero-knowledge systems.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08859" title="Abstract">arXiv:2311.08859</a> [<a href="/pdf/2311.08859" title="Download PDF">pdf</a>, <a href="/format/2311.08859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verification of GossipSub in ACL2s
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ankit Kumar</a> (Northeastern University), 
<a href="/search/cs?searchtype=author&query=von+Hippel%2C+M">Max von Hippel</a> (Northeastern University), 
<a href="/search/cs?searchtype=author&query=Manolios%2C+P">Panagiotis Manolios</a> (Northeastern University), 
<a href="/search/cs?searchtype=author&query=Nita-Rotaru%2C+C">Cristina Nita-Rotaru</a> (Northeastern University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 113-132
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computers and Society (cs.CY); Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">GossipSub is a popular new peer-to-peer network protocol designed to
disseminate messages quickly and efficiently by allowing peers to forward the
full content of messages only to a dynamically selected subset of their
neighboring peers (mesh neighbors) while gossiping about messages they have
seen with the rest. Peers decide which of their neighbors to graft or prune
from their mesh locally and periodically using a score for each neighbor.
Scores are calculated using a score function that depends on mesh-specific
parameters, weights and counters relating to a peer's performance in the
network. Since a GossipSub network's performance ultimately depends on the
performance of its peers, an important question arises: Is the score
calculation mechanism effective in weeding out non-performing or even
intentionally misbehaving peers from meshes? We answered this question in the
negative in our companion paper by reasoning about GossipSub using our formal,
official and executable ACL2s model. Based on our findings, we synthesized and
simulated attacks against GossipSub which were confirmed by the developers of
GossipSub, FileCoin, and Eth2.0, and publicly disclosed in MITRE
CVE-2022-47547. In this paper, we present a detailed description of our model.
We discuss design decisions, security properties of GossipSub, reasoning about
the security properties in context of our model, attack generation and lessons
we learnt when writing it.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08860" title="Abstract">arXiv:2311.08860</a> [<a href="/pdf/2311.08860" title="Download PDF">pdf</a>, <a href="/ps/2311.08860" title="Download PostScript">ps</a>, <a href="/format/2311.08860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proving Calculational Proofs Correct
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Walter%2C+A+T">Andrew T. Walter</a> (Northeastern University), 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ankit Kumar</a> (Northeastern University), 
<a href="/search/cs?searchtype=author&query=Manolios%2C+P">Panagiotis Manolios</a> (Northeastern University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 133-150
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Teaching proofs is a crucial component of any undergraduate-level program
that covers formal reasoning. We have developed a calculational reasoning
format and refined it over several years of teaching a freshman-level course,
"Logic and Computation", to thousands of undergraduate students. In our
companion paper, we presented our calculational proof format, gave an overview
of the calculational proof checker (CPC) tool that we developed to help users
write and validate proofs, described some of the technical and implementation
details of CPC and provided several publicly available proofs written using our
format. In this paper, we dive deeper into the implementation details of CPC,
highlighting how proof validation works, which helps us argue that our proof
checking process is sound.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08861" title="Abstract">arXiv:2311.08861</a> [<a href="/pdf/2311.08861" title="Download PDF">pdf</a>, <a href="/format/2311.08861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACL2 Proofs of Nonlinear Inequalities with Imandra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Passmore%2C+G">Grant Passmore</a> (Imandra Inc.)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 151-160
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Symbolic Computation (cs.SC)

</div>
<p class="mathjax">We present a proof-producing integration of ACL2 and Imandra for proving
nonlinear inequalities. This leverages a new Imandra interface exposing its
nonlinear decision procedures. The reasoning takes place over the reals, but
the proofs produced are valid over the rationals and may be run in both ACL2
and ACL2(r). The ACL2 proofs Imandra constructs are extracted from
Positivstellensatz refutations, a real algebraic analogue of the
Nullstellensatz, and are found using convex optimization.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08862" title="Abstract">arXiv:2311.08862</a> [<a href="/pdf/2311.08862" title="Download PDF">pdf</a>, <a href="/ps/2311.08862" title="Download PostScript">ps</a>, <a href="/format/2311.08862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verification of a Rust Implementation of Knuth&#x27;s Dancing Links using  ACL2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hardin%2C+D+S">David S. Hardin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>. arXiv admin note: substantial text overlap with <a href="/abs/2205.11709">arXiv:2205.11709</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 161-174
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Data Structures and Algorithms (cs.DS); Programming Languages (cs.PL)

</div>
<p class="mathjax">Dancing Links connotes an optimization to a circular doubly-linked list data
structure implementation which provides for fast list element removal and
restoration. The Dancing Links optimization is used primarily in fast
algorithms to find exact covers, and has been popularized by Knuth in Volume 4B
of his seminal series The Art of Computer Programming. We describe an
implementation of the Dancing Links optimization in the Rust programming
language, as well as its formal verification using the ACL2 theorem prover.
Rust has garnered significant endorsement in the past few years as a modern,
memory-safe successor to C/C++ at companies such as Amazon, Google, and
Microsoft, and is being integrated into both the Linux and Windows operating
system kernels. Our interest in Rust stems from its potential as a
hardware/software co-assurance language, with application to critical systems.
We have crafted a Rust subset, inspired by Russinoff's Restricted Algorithmic C
(RAC), which we have imaginatively named Restricted Algorithmic Rust, or RAR.
In previous work, we described our initial implementation of a RAR toolchain,
wherein we simply transpile the RAR source into RAC. By so doing, we leverage a
number of existing hardware/software co-assurance tools with a minimum
investment of time and effort. In this paper, we describe the RAR Rust subset,
describe our improved prototype RAR toolchain, and detail the design and
verification of a circular doubly-linked list data structure employing the
Dancing Links optimization in RAR, with full proofs of functional correctness
accomplished using the ACL2 theorem prover.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08863" title="Abstract">arXiv:2311.08863</a> [<a href="/pdf/2311.08863" title="Download PDF">pdf</a>, <a href="/format/2311.08863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toulouse Hyperspectral Data Set: a benchmark data set to assess  semi-supervised spectral representation learning and pixel-wise  classification techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thoreau%2C+R">Romain Thoreau</a>, 
<a href="/search/cs?searchtype=author&query=Risser%2C+L">Laurent Risser</a>, 
<a href="/search/cs?searchtype=author&query=Achard%2C+V">V&#xe9;ronique Achard</a>, 
<a href="/search/cs?searchtype=author&query=Berthelot%2C+B">B&#xe9;atrice Berthelot</a>, 
<a href="/search/cs?searchtype=author&query=Briottet%2C+X">Xavier Briottet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Airborne hyperspectral images can be used to map the land cover in large
urban areas, thanks to their very high spatial and spectral resolutions on a
wide spectral domain. While the spectral dimension of hyperspectral images is
highly informative of the chemical composition of the land surface, the use of
state-of-the-art machine learning algorithms to map the land cover has been
dramatically limited by the availability of training data. To cope with the
scarcity of annotations, semi-supervised and self-supervised techniques have
lately raised a lot of interest in the community. Yet, the publicly available
hyperspectral data sets commonly used to benchmark machine learning models are
not totally suited to evaluate their generalization performances due to one or
several of the following properties: a limited geographical coverage (which
does not reflect the spectral diversity in metropolitan areas), a small number
of land cover classes and a lack of appropriate standard train / test splits
for semi-supervised and self-supervised learning. Therefore, we release in this
paper the Toulouse Hyperspectral Data Set that stands out from other data sets
in the above-mentioned respects in order to meet key issues in spectral
representation learning and classification over large-scale hyperspectral
images with very few labeled pixels. Besides, we discuss and experiment the
self-supervised task of Masked Autoencoders and establish a baseline for
pixel-wise classification based on a conventional autoencoder combined with a
Random Forest classifier achieving 82% overall accuracy and 74% F1 score. The
Toulouse Hyperspectral Data Set and our code are publicly available at
https://www.toulouse-hyperspectral-data-set.com and
https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08866" title="Abstract">arXiv:2311.08866</a> [<a href="/pdf/2311.08866" title="Download PDF">pdf</a>, <a href="/ps/2311.08866" title="Download PostScript">ps</a>, <a href="/format/2311.08866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Formalization of Finite Group Theory: Part II
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Russinoff%2C+D+M">David M. Russinoff</a> (Arm Inc.)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings ACL2-2023, <a href="/abs/2311.08373">arXiv:2311.08373</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 393, 2023, pp. 16-32
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">This is the second installment of an exposition of an ACL2 formalization of
finite group theory. The first, which was presented at the 2022 ACL2 workshop,
covered groups and subgroups, cosets, normal subgroups, and quotient groups,
culminating in a proof of Cauchy's Theorem: If the order of a group G is
divisible by a prime p, then G has an element of order p. This sequel addresses
homomorphisms, direct products, and the Fundamental Theorem of Finite Abelian
Groups: Every finite abelian group is isomorphic to the direct product of a
list of cyclic p-groups, the orders of which are unique up to permutation. This
theorem is a suitable application of ACL2 because of its extensive reliance on
recursion and induction as well as the constructive nature of the
factorization. The proof of uniqueness is especially challenging, requiring the
formalization of vague intuition that is commonly taken as self-evident.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08868" title="Abstract">arXiv:2311.08868</a> [<a href="/pdf/2311.08868" title="Download PDF">pdf</a>, <a href="/ps/2311.08868" title="Download PostScript">ps</a>, <a href="/format/2311.08868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Color Fault-Tolerant Spanners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petruschka%2C+A">Asaf Petruschka</a>, 
<a href="/search/cs?searchtype=author&query=Sapir%2C+S">Shay Sapir</a>, 
<a href="/search/cs?searchtype=author&query=Tzalik%2C+E">Elad Tzalik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ITCS 2024, shortened abstract for arxiv
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We initiate the study of spanners in arbitrarily vertex- or edge-colored
graphs (with no "legality" restrictions), that are resilient to failures of
entire color classes. When a color fails, all vertices/edges of that color
crash. An $f$-color fault-tolerant ($f$-CFT) $t$-spanner of an $n$-vertex
colored graph $G$ is a subgraph $H$ that preserves distances up to factor $t$,
even in the presence of at most $f$ color faults. This notion generalizes the
well-studied $f$-vertex/edge fault-tolerant ($f$-V/EFT) spanners. The size of
an $f$-V/EFT spanner crucially depends on the number $f$ of vertex/edge faults
to be tolerated. In the colored variants, even a single color fault can
correspond to an unbounded number of vertex/edge faults. The key conceptual
contribution of this work is in showing that the size (number of edges)
required by an $f$-CFT spanner is in fact comparable to its uncolored
counterpart, with no dependency on the size of color classes. We provide
optimal bounds on the size required by $f$-CFT spanners, revealing an
interesting phenomenon: while (individual) edge faults are "easier" than vertex
faults in terms of spanner size, edge-color faults are "harder" than
vertex-color faults. Our upper bounds are based on a generalization of the
blocking set technique of [Bodwin and Patel, PODC 2019] for analyzing the
(exponential-time) greedy algorithm for FT spanners. We complement them by
providing efficient constructions of CFT spanners with similar size guarantees,
based on the algorithm of [Dinitz and Robelle, PODC 2020].
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08870" title="Abstract">arXiv:2311.08870</a> [<a href="/pdf/2311.08870" title="Download PDF">pdf</a>, <a href="/format/2311.08870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Shot Federated Learning with Classifier-Guided Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingzhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+S">Shangchao Su</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+X">Xiangyang Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">One-shot federated learning (OSFL) has gained attention in recent years due
to its low communication cost. However, most of the existing methods require
auxiliary datasets or training generators, which hinders their practicality in
real-world scenarios. In this paper, we explore the novel opportunities that
diffusion models bring to OSFL and propose FedCADO, utilizing guidance from
client classifiers to generate data that complies with clients' distributions
and subsequently training the aggregated model on the server. Specifically, our
method involves targeted optimizations in two aspects. On one hand, we
conditionally edit the randomly sampled initial noises, embedding them with
specified semantics and distributions, resulting in a significant improvement
in both the quality and stability of generation. On the other hand, we employ
the BN statistics from the classifiers to provide detailed guidance during
generation. These tailored optimizations enable us to limitlessly generate
datasets, which closely resemble the distribution and quality of the original
client dataset. Our method effectively handles the heterogeneous client models
and the problems of non-IID features or labels. In terms of privacy protection,
our method avoids training any generator or transferring any auxiliary
information on clients, eliminating any additional privacy leakage risks.
Leveraging the extensive knowledge stored in the pre-trained diffusion model,
the synthetic datasets can assist us in surpassing the knowledge limitations of
the client samples, resulting in aggregation models that even outperform the
performance ceiling of centralized training in some cases, which is
convincingly demonstrated in the sufficient quantification and visualization
experiments conducted on three large-scale multi-domain image datasets.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08872" title="Abstract">arXiv:2311.08872</a> [<a href="/pdf/2311.08872" title="Download PDF">pdf</a>, <a href="/format/2311.08872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel Monte Carlo methods for the Dean-Kawasaki equation from  Fluctuating Hydrodynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cornalba%2C+F">Federico Cornalba</a>, 
<a href="/search/math?searchtype=author&query=Fischer%2C+J">Julian Fischer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Probability (math.PR)

</div>
<p class="mathjax">Stochastic PDEs of Fluctuating Hydrodynamics are a powerful tool for the
description of fluctuations in many-particle systems. In this paper, we develop
and analyze a Multilevel Monte Carlo (MLMC) scheme for the Dean-Kawasaki
equation, a pivotal representative of this class of SPDEs. We prove
analytically and demonstrate numerically that our MLMC scheme provides a
significant speed-up (with respect to a standard Monte Carlo method) in the
simulation of the Dean-Kawasaki equation. Specifically, we quantify how the
speed-up factor increases as the average particle density increases, and show
that sizeable speed-ups can be obtained even in regimes of low particle
density. Numerical simulations are provided in the two-dimensional case,
confirming our theoretical predictions.
<br />Our results are formulated entirely in terms of the law of distributions
rather than in terms of strong spatial norms: this crucially allows for MLMC
speed-ups altogether despite the Dean-Kawasaki equation being highly singular.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08874" title="Abstract">arXiv:2311.08874</a> [<a href="/pdf/2311.08874" title="Download PDF">pdf</a>, <a href="/format/2311.08874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Label Embedding -- Measuring classification difficulty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hechinger%2C+K">Katharina Hechinger</a>, 
<a href="/search/cs?searchtype=author&query=Koller%2C+C">Christoph Koller</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Kauermann%2C+G">G&#xf6;ran Kauermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Uncertainty quantification in machine learning is a timely and vast field of
research. In supervised learning, uncertainty can already occur in the very
first stage of the training process, the labelling step. In particular, this is
the case when not every instance can be unambiguously classified. The problem
occurs for classifying instances, where classes may overlap or instances can
not be clearly categorised. In other words, there is inevitable ambiguity in
the annotation step and not necessarily a 'ground truth'. We look exemplary at
the classification of satellite images. Each image is annotated independently
by multiple labellers and classified into local climate zones (LCZs). For each
instance we have multiple votes, leading to a distribution of labels rather
than a single value. The main idea of this work is that we do not assume a
ground truth label but embed the votes into a K-dimensional space, with K as
the number of possible categories. The embedding is derived from the voting
distribution in a Bayesian setup, modelled via a Dirichlet-Multinomial model.
We estimate the model and posteriors using a stochastic Expectation
Maximisation algorithm with Markov Chain Monte Carlo steps. While we focus on
the particular example of LCZ classification, the methods developed in this
paper readily extend to other situations where multiple annotators
independently label texts or images. We also apply our approach to two other
benchmark datasets for image classification to demonstrate this. Besides the
embeddings themselves, we can investigate the resulting correlation matrices,
which can be seen as generalised confusion matrices and reflect the semantic
similarities of the original classes very well for all three exemplary
datasets. The insights gained are valuable and can serve as general label
embedding if a single ground truth per observation cannot be guaranteed.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08876" title="Abstract">arXiv:2311.08876</a> [<a href="/pdf/2311.08876" title="Download PDF">pdf</a>, <a href="/format/2311.08876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aerial IRS with Robotic Anchoring Capabilities: A Novel Way for Adaptive  Coverage Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Friderikos%2C+V">Vasilis Friderikos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">It is widely accepted that integrating intelligent reflecting surfaces (IRSs)
with unmanned aerial vehicles (UAV) or drones can assist wireless networks in
improving network coverage and end user Quality of Service (QoS). However, the
critical constrain of drones is their very limited hovering/flying time. In
this paper we propose the concept of robotic aerial IRSs (RA-IRSs), which are
in essence drones that in addition to IRS embed an anchoring mechanism that
allows them to grasp in an energy neutral manner at tall urban landforms such
as lampposts. By doing so, RA-IRSs can completely eliminate the flying/hovering
energy consumption and can offer service for multiple hours or even days
(something not possible with UAV-mounted IRSs). Using that property we show how
RA-IRS can increase network performance by changing their anchoring location to
follow the spatio-temporal traffic demand. The proposed methodology, developed
through Integer Linear Programming (ILP) formulations offers a significant
Signal-to-Noise (SNR) gain in highly heterogeneous regions in terms of traffic
demand compared to fixed IRS; hence, addressing urban coverage discrepancies
effectively. Numerical simulations validate the superiority of RA-IRSs over
fixed terrestrial IRSs in terms of traffic serviceability, sustaining more than
2 times the traffic demand in areas experiencing high heterogeneity,
emphasizing their adaptability in improving coverage and QoS in complex urban
terrains.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08877" title="Abstract">arXiv:2311.08877</a> [<a href="/pdf/2311.08877" title="Download PDF">pdf</a>, <a href="/format/2311.08877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Llamas Know What GPTs Don&#x27;t Show: Surrogate Models for Confidence  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+V">Vaishnavi Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ananya Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">To maintain user trust, large language models (LLMs) should signal low
confidence on examples where they are incorrect, instead of misleading the
user. The standard approach of estimating confidence is to use the softmax
probabilities of these models, but as of November 2023, state-of-the-art LLMs
such as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We
first study eliciting confidence linguistically -- asking an LLM for its
confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4
averaged across 12 question-answering datasets -- 7% above a random baseline)
but leaves room for improvement. We then explore using a surrogate confidence
model -- using a model where we do have probabilities to evaluate the original
model's confidence in a given question. Surprisingly, even though these
probabilities come from a different and often weaker model, this method leads
to higher AUC than linguistic confidences on 9 out of 12 datasets. Our best
method composing linguistic confidences and surrogate model probabilities gives
state-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on
GPT-4).
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08879" title="Abstract">arXiv:2311.08879</a> [<a href="/pdf/2311.08879" title="Download PDF">pdf</a>, <a href="/format/2311.08879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voice App Developer Experiences with Alexa and Google Assistant:  Juggling Risks, Liability, and Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seymour%2C+W">William Seymour</a>, 
<a href="/search/cs?searchtype=author&query=Abdi%2C+N">Noura Abdi</a>, 
<a href="/search/cs?searchtype=author&query=Ramokapane%2C+K+M">Kopo M. Ramokapane</a>, 
<a href="/search/cs?searchtype=author&query=Edu%2C+J">Jide Edu</a>, 
<a href="/search/cs?searchtype=author&query=Suarez-Tangil%2C+G">Guillermo Suarez-Tangil</a>, 
<a href="/search/cs?searchtype=author&query=Such%2C+J">Jose Such</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at USENIX Security 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Voice applications (voice apps) are a key element in Voice Assistant
ecosystems such as Amazon Alexa and Google Assistant, as they provide
assistants with a wide range of capabilities that users can invoke with a voice
command. Most voice apps, however, are developed by third parties - i.e., not
by Amazon/Google - and they are included in the ecosystem through marketplaces
akin to smartphone app stores but with crucial differences, e.g., the voice app
code is not hosted by the marketplace and is not run on the local device.
Previous research has studied the security and privacy issues of voice apps in
the wild, finding evidence of bad practices by voice app developers. However,
developers' perspectives are yet to be explored.
<br />In this paper, we report a qualitative study of the experiences of voice app
developers and the challenges they face. Our findings suggest that: 1)
developers face several risks due to liability pushed on to them by the more
powerful voice assistant platforms, which are linked to negative privacy and
security outcomes on voice assistant platforms; and 2) there are key issues
around monetization, privacy, design, and testing rooted in problems with the
voice app certification process. We discuss the implications of our results for
voice app developers, platforms, regulators, and research on voice app
development and certification.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08880" title="Abstract">arXiv:2311.08880</a> [<a href="/pdf/2311.08880" title="Download PDF">pdf</a>, <a href="/format/2311.08880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motion Control of Two Mobile Robots under Allowable Collisions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+L">Li Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+W">Wei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xi-Ming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Junlin Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This letter investigates the motion control problem of two mobile robots
under allowable collisions. Here, the allowable collisions mean that the
collisions do not damage the mobile robots. The occurrence of the collisions is
discussed and the effects of the collisions on the mobile robots are analyzed
to develop a hybrid model of each mobile robot under allowable collisions.
Based on the effects of the collisions, we show the necessity of redesigning
the motion control strategy for mobile robots. Furthermore, impulsive control
techniques are applied to redesign the motion control strategy to guarantee the
task accomplishment for each mobile robot. Finally, an example is used to
illustrate the redesigned motion control strategy.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08881" title="Abstract">arXiv:2311.08881</a> [<a href="/pdf/2311.08881" title="Download PDF">pdf</a>, <a href="/ps/2311.08881" title="Download PostScript">ps</a>, <a href="/format/2311.08881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing 2-QuBit Gate Count for ZX-Calculus based Quantum Circuit  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Staudacher%2C+K">Korbinian Staudacher</a> (Ludwig-Maximilians-Universit&#xe4;t M&#xfc;nchen), 
<a href="/search/cs?searchtype=author&query=Guggemos%2C+T">Tobias Guggemos</a> (Ludwig-Maximilians-Universit&#xe4;t M&#xfc;nchen), 
<a href="/search/cs?searchtype=author&query=Grundner-Culemann%2C+S">Sophia Grundner-Culemann</a> (Ludwig-Maximilians-Universit&#xe4;t M&#xfc;nchen), 
<a href="/search/cs?searchtype=author&query=Gehrke%2C+W">Wolfgang Gehrke</a> (Universit&#xe4;t der Bundeswehr M&#xfc;nchen)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 29-45
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">In the near term, programming quantum computers will remain severely limited
by low quantum volumes. Therefore, it is desirable to implement quantum
circuits with the fewest resources possible. For the common Clifford+T
circuits, most research is focused on reducing the number of T gates, since
they are an order of magnitude more expensive than Clifford gates in quantum
error corrected encoding schemes. However, this optimization sometimes leads to
more 2-qubit gates, which, even though they are less expensive in terms of
fault-tolerance, contribute significantly to the overall circuit cost.
Approaches based on the ZX-calculus have recently gained some popularity in the
field, but reduction of 2-qubit gates is not their focus. In this work, we
present an alternative for improving 2-qubit gate count of a quantum circuit
with the ZX-calculus by using heuristics in ZX-diagram simplification. Our
approach maintains the good reduction of the T gate count provided by other
strategies based on ZX-calculus, thus serving as an extension for other
optimization algorithms. Our results show that combining the available
ZX-calculus-based optimizations with our algorithms can reduce the number of
2-qubit gates by as much as 40% compared to current approaches using
ZX-calculus. Additionally, we improve the results of the best currently
available optimization technique of Nam et. al for some circuits by up to 15%.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08882" title="Abstract">arXiv:2311.08882</a> [<a href="/pdf/2311.08882" title="Download PDF">pdf</a>, <a href="/ps/2311.08882" title="Download PostScript">ps</a>, <a href="/format/2311.08882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification of Causal Influences in Quantum Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Friend%2C+I">Isaac Friend</a> (University of Oxford), 
<a href="/search/cs?searchtype=author&query=Kissinger%2C+A">Aleks Kissinger</a> (University of Oxford)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 101-115
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Though the topic of causal inference is typically considered in the context
of classical statistical models, recent years have seen great interest in
extending causal inference techniques to quantum and generalized theories.
Causal identification is a type of causal inference problem concerned with
recovering from observational data and qualitative assumptions the causal
mechanisms generating the data, and hence the effects of hypothetical
interventions. A major obstacle to a theory of causal identification in the
quantum setting is the question of what should play the role of "observational
data," as any means of extracting data at a certain locus will almost certainly
disturb the system. Hence, one might think a priori that quantum measurements
are already too much like interventions, so that the problem of causal
identification trivializes. This is not the case. Fixing a limited class of
quantum instruments (namely the class of all projective measurements) to play
the role of "observations," we note that as in the classical setting, there
exist scenarios for which causal identification is not possible. We then
present sufficient conditions for quantum causal identification, starting with
a quantum analogue of the well-known "front-door criterion" and finishing with
a broader class of scenarios for which the effect of a single intervention is
identifiable. These results emerge from generalizing the process-theoretic
account of classical causal inference due to Jacobs, Kissinger, and Zanasi
beyond the setting of Markov categories, and thereby treating the classical and
quantum problems uniformly.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08883" title="Abstract">arXiv:2311.08883</a> [<a href="/pdf/2311.08883" title="Download PDF">pdf</a>, <a href="/format/2311.08883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Large Language Models to Learn from Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenkai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jirong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have shown incredible performance in completing
various real-world tasks. The current knowledge learning paradigm of LLMs is
mainly based on learning from examples, in which LLMs learn the internal rule
implicitly from a certain number of supervised examples. However, the learning
paradigm may not well learn those complicated rules, especially when the
training examples are limited. We are inspired that humans can learn the new
tasks or knowledge in another way by learning from rules. That is, humans can
grasp the new tasks or knowledge quickly and generalize well given only a
detailed rule and a few optional examples. Therefore, in this paper, we aim to
explore the feasibility of this new learning paradigm, which encodes the
rule-based knowledge into LLMs. We propose rule distillation, which first uses
the strong in-context abilities of LLMs to extract the knowledge from the
textual rules and then explicitly encode the knowledge into LLMs' parameters by
learning from the above in-context signals produced inside the model. Our
experiments show that making LLMs learn from rules by our method is much more
efficient than example-based learning in both the sample size and
generalization ability.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08884" title="Abstract">arXiv:2311.08884</a> [<a href="/pdf/2311.08884" title="Download PDF">pdf</a>, <a href="/format/2311.08884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CREPE Notes: A new method for segmenting pitch contours into discrete  notes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Riley%2C+X">Xavier Riley</a>, 
<a href="/search/cs?searchtype=author&query=Dixon%2C+S">Simon Dixon</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 20th Sound and Music Computing Conference. June
  15-17, 2023. Stockholm, Sweden
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Tracking the fundamental frequency (f0) of a monophonic instrumental
performance is effectively a solved problem with several solutions achieving
99% accuracy. However, the related task of automatic music transcription
requires a further processing step to segment an f0 contour into discrete
notes. This sub-task of note segmentation is necessary to enable a range of
applications including musicological analysis and symbolic music generation.
Building on CREPE, a state-of-the-art monophonic pitch tracking solution based
on a simple neural network, we propose a simple and effective method for
post-processing CREPE's output to achieve monophonic note segmentation. The
proposed method demonstrates state-of-the-art results on two challenging
datasets of monophonic instrumental music. Our approach also gives a 97%
reduction in the total number of parameters used when compared with other deep
learning based methods.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08886" title="Abstract">arXiv:2311.08886</a> [<a href="/pdf/2311.08886" title="Download PDF">pdf</a>, <a href="/format/2311.08886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIMB: Curriculum Learning for Infant-inspired Model Building
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinez%2C+R+D">Richard Diehl Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Goriely%2C+Z">Zebulon Goriely</a>, 
<a href="/search/cs?searchtype=author&query=McGovern%2C+H">Hope McGovern</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+C">Christopher Davis</a>, 
<a href="/search/cs?searchtype=author&query=Caines%2C+A">Andrew Caines</a>, 
<a href="/search/cs?searchtype=author&query=Buttery%2C+P">Paula Buttery</a>, 
<a href="/search/cs?searchtype=author&query=Beinborn%2C+L">Lisa Beinborn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We describe our team's contribution to the STRICT-SMALL track of the BabyLM
Challenge. The challenge requires training a language model from scratch using
only a relatively small training dataset of ten million words. We experiment
with three variants of cognitively-motivated curriculum learning and analyze
their effect on the performance of the model on linguistic evaluation tasks. In
the vocabulary curriculum, we analyze methods for constraining the vocabulary
in the early stages of training to simulate cognitively more plausible learning
curves. In the data curriculum experiments, we vary the order of the training
instances based on i) infant-inspired expectations and ii) the learning
behavior of the model. In the objective curriculum, we explore different
variations of combining the conventional masked language modeling task with a
more coarse-grained word class prediction task to reinforce linguistic
generalization capabilities. Our results did not yield consistent improvements
over our own non-curriculum learning baseline across a range of linguistic
benchmarks; however, we do find marginal gains on select tasks. Our analysis
highlights key takeaways for specific combinations of tasks and settings which
benefit from our proposed curricula. We moreover determine that careful
selection of model architecture, and training hyper-parameters yield
substantial improvements over the default baselines provided by the BabyLM
challenge.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08890" title="Abstract">arXiv:2311.08890</a> [<a href="/pdf/2311.08890" title="Download PDF">pdf</a>, <a href="/format/2311.08890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are legal but they are not: Making the case for a  powerful LegalLLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jayakumar%2C+T">Thanmay Jayakumar</a>, 
<a href="/search/cs?searchtype=author&query=Farooqui%2C+F">Fauzan Farooqui</a>, 
<a href="/search/cs?searchtype=author&query=Farooqui%2C+L">Luqman Farooqui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, Accepted at Natural Legal Language Processing Workshop, EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Realizing the recent advances in Natural Language Processing (NLP) to the
legal sector poses challenging problems such as extremely long sequence
lengths, specialized vocabulary that is usually only understood by legal
professionals, and high amounts of data imbalance. The recent surge of Large
Language Models (LLMs) has begun to provide new opportunities to apply NLP in
the legal domain due to their ability to handle lengthy, complex sequences.
Moreover, the emergence of domain-specific LLMs has displayed extremely
promising results on various tasks. In this study, we aim to quantify how
general LLMs perform in comparison to legal-domain models (be it an LLM or
otherwise). Specifically, we compare the zero-shot performance of three
general-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR
subset of the LexGLUE benchmark for contract provision classification. Although
the LLMs were not explicitly trained on legal data, we observe that they are
still able to classify the theme correctly in most cases. However, we find that
their mic-F1/mac-F1 performance is up to 19.2/26.8\% lesser than smaller models
fine-tuned on the legal domain, thus underscoring the need for more powerful
legal-domain LLMs.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08891" title="Abstract">arXiv:2311.08891</a> [<a href="/pdf/2311.08891" title="Download PDF">pdf</a>, <a href="/format/2311.08891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdapterShadow: Adapting Segment Anything Model for Shadow Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jie%2C+L">Leiping Jie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hui Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Segment anything model (SAM) has shown its spectacular performance in
segmenting universal objects, especially when elaborate prompts are provided.
However, the drawback of SAM is twofold. On the first hand, it fails to segment
specific targets, e.g., shadow images or lesions in medical images. On the
other hand, manually specifying prompts is extremely time-consuming. To
overcome the problems, we propose AdapterShadow, which adapts SAM model for
shadow detection. To adapt SAM for shadow images, trainable adapters are
inserted into the frozen image encoder of SAM, since the training of the full
SAM model is both time and memory consuming. Moreover, we introduce a novel
grid sampling method to generate dense point prompts, which helps to
automatically segment shadows without any manual interventions. Extensive
experiments are conducted on four widely used benchmark datasets to demonstrate
the superior performance of our proposed method. Codes will are publicly
available at https://github.com/LeipingJie/AdapterShadow.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08894" title="Abstract">arXiv:2311.08894</a> [<a href="/pdf/2311.08894" title="Download PDF">pdf</a>, <a href="/format/2311.08894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Transfer Learning with In-context Learning using Blackbox LLMs  for Zero-shot Knowledge Base Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patidar%2C+M">Mayur Patidar</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Avinash Singh</a>, 
<a href="/search/cs?searchtype=author&query=Sawhney%2C+R">Riya Sawhney</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+I">Indrajit Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We address the zero-shot transfer learning setting for the knowledge base
question answering (KBQA) problem, where a large volume of labeled training
data is available for the source domain, but no such labeled examples are
available for the target domain. Transfer learning for KBQA makes use of large
volumes of unlabeled data in the target in addition to the labeled data in the
source. More recently, few-shot in-context learning using Black-box Large
Language Models (BLLMs) has been adapted for KBQA without considering any
source domain data. In this work, we show how to meaningfully combine these two
paradigms for KBQA so that their benefits add up. Specifically, we preserve the
two stage retrieve-then-generate pipeline of supervised KBQA and introduce
interaction between in-context learning using BLLMs and transfer learning from
the source for both stages. In addition, we propose execution-guided
self-refinement using BLLMs, decoupled from the transfer setting. With the help
of experiments using benchmark datasets GrailQA as the source and WebQSP as the
target, we show that the proposed combination brings significant improvements
to both stages and also outperforms by a large margin state-of-the-art
supervised KBQA models trained on the source. We also show that in the
in-domain setting, the proposed BLLM augmentation significantly outperforms
state-of-the-art supervised models, when the volume of labeled data is limited,
and also outperforms these marginally even when using the entire large training
dataset.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08896" title="Abstract">arXiv:2311.08896</a> [<a href="/pdf/2311.08896" title="Download PDF">pdf</a>, <a href="/ps/2311.08896" title="Download PostScript">ps</a>, <a href="/format/2311.08896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HELLaMA: LLaMA-based Table to Text Generation by Highlighting the  Important Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Junyi Bian</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+X">Xiaolei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+W">Wuhe Zou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mengzuo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weidong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large models have demonstrated significant progress across various domains,
particularly in tasks related to text generation. In the domain of Table to
Text, many Large Language Model (LLM)-based methods currently resort to
modifying prompts to invoke public APIs, incurring potential costs and
information leaks. With the advent of open-source large models, fine-tuning
LLMs has become feasible. In this study, we conducted parameter-efficient
fine-tuning on the LLaMA2 model. Distinguishing itself from previous
fine-tuning-based table-to-text methods, our approach involves injecting
reasoning information into the input by emphasizing table-specific row data.
Our model consists of two modules: 1) a table reasoner that identifies relevant
row evidence, and 2) a table summarizer that generates sentences based on the
highlighted table. To facilitate this, we propose a search strategy to
construct reasoning labels for training the table reasoner. On both the FetaQA
and QTSumm datasets, our approach achieved state-of-the-art results.
Additionally, we observed that highlighting input tables significantly enhances
the model's performance and provides valuable interpretability.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08902" title="Abstract">arXiv:2311.08902</a> [<a href="/pdf/2311.08902" title="Download PDF">pdf</a>, <a href="/format/2311.08902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Importance of Step-wise Embeddings for Heterogeneous Clinical  Time-Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuznetsova%2C+R">Rita Kuznetsova</a>, 
<a href="/search/cs?searchtype=author&query=Pace%2C+A">Aliz&#xe9;e Pace</a>, 
<a href="/search/cs?searchtype=author&query=Burger%2C+M">Manuel Burger</a>, 
<a href="/search/cs?searchtype=author&query=Y%C3%A8che%2C+H">Hugo Y&#xe8;che</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A4tsch%2C+G">Gunnar R&#xe4;tsch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Machine Learning for Health (ML4H) 2023 in Proceedings of Machine Learning Research 225
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent advances in deep learning architectures for sequence modeling have not
fully transferred to tasks handling time-series from electronic health records.
In particular, in problems related to the Intensive Care Unit (ICU), the
state-of-the-art remains to tackle sequence classification in a tabular manner
with tree-based methods. Recent findings in deep learning for tabular data are
now surpassing these classical methods by better handling the severe
heterogeneity of data input features. Given the similar level of feature
heterogeneity exhibited by ICU time-series and motivated by these findings, we
explore these novel methods' impact on clinical sequence modeling tasks. By
jointly using such advances in deep learning for tabular data, our primary
objective is to underscore the importance of step-wise embeddings in
time-series modeling, which remain unexplored in machine learning methods for
clinical data. On a variety of clinically relevant tasks from two large-scale
ICU datasets, MIMIC-III and HiRID, our work provides an exhaustive analysis of
state-of-the-art methods for tabular time-series as time-step embedding models,
showing overall performance improvement. In particular, we evidence the
importance of feature grouping in clinical time-series, with significant
performance gains when considering features within predefined semantic groups
in the step-wise embedding module.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08903" title="Abstract">arXiv:2311.08903</a> [<a href="/pdf/2311.08903" title="Download PDF">pdf</a>, <a href="/format/2311.08903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost Sharing under Private Costs and Connection Control on Directed  Acyclic Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dengji Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Sizhe Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We consider a cost sharing problem on a weighted directed acyclic graph (DAG)
with a source node to which all the other nodes want to connect. The cost
(weight) of each edge is private information reported by multiple contractors,
and among them, only one contractor is selected as the builder. All the nodes
except for the source need to share the total cost of the used edges. However,
they may block others' connections to the source by strategically cutting their
outgoing edges to reduce their cost share, which may increase the total cost of
connectivity. To minimize the total cost of connectivity, we design a cost
sharing mechanism to incentivize each node to offer all its outgoing edges and
each contractor to report all the edges' weights truthfully, and show the
properties of the proposed mechanism. In addition, our mechanism outperforms
the two benchmark mechanisms.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08904" title="Abstract">arXiv:2311.08904</a> [<a href="/pdf/2311.08904" title="Download PDF">pdf</a>, <a href="/ps/2311.08904" title="Download PostScript">ps</a>, <a href="/format/2311.08904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Efficient Design of Satellite-Terrestrial Computing in 6G  Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Q">Qiao Qi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we investigate the issue of satellite-terrestrial computing in
the sixth generation (6G) wireless networks, where multiple terrestrial base
stations (BSs) and low earth orbit (LEO) satellites collaboratively provide
edge computing services to ground user equipments (GUEs) and space user
equipments (SUEs) over the world. In particular, we design a complete process
of satellite-terrestrial computing in terms of communication and computing
according to the characteristics of 6G wireless networks. In order to minimize
the weighted total energy consumption while ensuring delay requirements of
computing tasks, an energy-efficient satellite-terrestrial computing algorithm
is put forward by jointly optimizing offloading selection, beamforming design
and resource allocation. Finally, both theoretical analysis and simulation
results confirm fast convergence and superior performance of the proposed
algorithm for satellite-terrestrial computing in 6G wireless networks.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08907" title="Abstract">arXiv:2311.08907</a> [<a href="/pdf/2311.08907" title="Download PDF">pdf</a>, <a href="/format/2311.08907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive space-time model order reduction with dual-weighted residual  (MORe DWR) error control for poroelasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fischer%2C+H">Hendrik Fischer</a>, 
<a href="/search/math?searchtype=author&query=Roth%2C+J">Julian Roth</a>, 
<a href="/search/math?searchtype=author&query=Chamoin%2C+L">Ludovic Chamoin</a>, 
<a href="/search/math?searchtype=author&query=Fau%2C+A">Amelie Fau</a>, 
<a href="/search/math?searchtype=author&query=Wheeler%2C+M+F">Mary F. Wheeler</a>, 
<a href="/search/math?searchtype=author&query=Wick%2C+T">Thomas Wick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 9 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work, the space-time MORe DWR (Model Order Reduction with
Dual-Weighted Residual error estimates) framework is extended and further
developed for single-phase flow problems in porous media. Specifically, our
problem statement is the Biot system which consists of vector-valued
displacements (geomechanics) coupled to a Darcy flow pressure equation. The
MORe DWR method introduces a goal-oriented adaptive incremental proper
orthogonal decomposition (POD) based-reduced-order model (ROM). The error in
the reduced goal functional is estimated during the simulation, and the POD
basis is enriched on-the-fly if the estimate exceeds a given threshold. This
results in a reduction of the total number of full-order-model solves for the
simulation of the porous medium, a robust estimation of the quantity of
interest and well-suited reduced bases for the problem at hand. We apply a
space-time Galerkin discretization with Taylor-Hood elements in space and a
discontinuous Galerkin method with piecewise constant functions in time. The
latter is well-known to be similar to the backward Euler scheme. We demonstrate
the efficiency of our method on the well-known two-dimensional Mandel benchmark
and a three-dimensional footing problem.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08909" title="Abstract">arXiv:2311.08909</a> [<a href="/pdf/2311.08909" title="Download PDF">pdf</a>, <a href="/format/2311.08909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DLAS: An Exploration and Assessment of the Deep Learning Acceleration  Stack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gibson%2C+P">Perry Gibson</a>, 
<a href="/search/cs?searchtype=author&query=Cano%2C+J">Jos&#xe9; Cano</a>, 
<a href="/search/cs?searchtype=author&query=Crowley%2C+E+J">Elliot J. Crowley</a>, 
<a href="/search/cs?searchtype=author&query=Storkey%2C+A">Amos Storkey</a>, 
<a href="/search/cs?searchtype=author&query=O%27Boyle%2C+M">Michael O&#x27;Boyle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Performance (cs.PF)

</div>
<p class="mathjax">Deep Neural Networks (DNNs) are extremely computationally demanding, which
presents a large barrier to their deployment on resource-constrained devices.
Since such devices are where many emerging deep learning applications lie
(e.g., drones, vision-based medical technology), significant bodies of work
from both the machine learning and systems communities have attempted to
provide optimizations to accelerate DNNs. To help unify these two perspectives,
in this paper we combine machine learning and systems techniques within the
Deep Learning Acceleration Stack (DLAS), and demonstrate how these layers can
be tightly dependent on each other with an across-stack perturbation study. We
evaluate the impact on accuracy and inference time when varying different
parameters of DLAS across two datasets, seven popular DNN architectures, four
DNN compression techniques, three algorithmic primitives with sparse and dense
variants, untuned and auto-scheduled code generation, and four hardware
platforms. Our evaluation highlights how perturbations across DLAS parameters
can cause significant variation and across-stack interactions. The highest
level observation from our evaluation is that the model size, accuracy, and
inference time are not guaranteed to be correlated. Overall we make 13 key
observations, including that speedups provided by compression techniques are
very hardware dependent, and that compiler auto-tuning can significantly alter
what the best algorithm to use for a given configuration is. With DLAS, we aim
to provide a reference framework to aid machine learning and systems
practitioners in reasoning about the context in which their respective DNN
acceleration solutions exist in. With our evaluation strongly motivating the
need for co-design, we believe that DLAS can be a valuable concept for
exploring the next generation of co-designed accelerated deep learning
solutions.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08910" title="Abstract">arXiv:2311.08910</a> [<a href="/pdf/2311.08910" title="Download PDF">pdf</a>, <a href="/format/2311.08910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Feedback-Enhanced Transformer for Image Forgery Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haochen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+G">Gang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xianglin Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Blind detection of the forged regions in digital images is an effective
authentication means to counter the malicious use of local image editing
techniques. Existing encoder-decoder forensic networks overlook the fact that
detecting complex and subtle tampered regions typically requires more feedback
information. In this paper, we propose a Progressive FeedbACk-enhanced
Transformer (ProFact) network to achieve coarse-to-fine image forgery
localization. Specifically, the coarse localization map generated by an initial
branch network is adaptively fed back to the early transformer encoder layers
for enhancing the representation of positive features while suppressing
interference factors. The cascaded transformer network, combined with a
contextual spatial pyramid module, is designed to refine discriminative
forensic features for improving the forgery localization accuracy and
reliability. Furthermore, we present an effective strategy to automatically
generate large-scale forged image samples close to real-world forensic
scenarios, especially in realistic and coherent processing. Leveraging on such
samples, a progressive and cost-effective two-stage training protocol is
applied to the ProFact network. The extensive experimental results on nine
public forensic datasets show that our proposed localizer greatly outperforms
the state-of-the-art on the generalization ability and robustness of image
forgery localization. Code will be publicly available at
https://github.com/multimediaFor/ProFact.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08911" title="Abstract">arXiv:2311.08911</a> [<a href="/pdf/2311.08911" title="Download PDF">pdf</a>, <a href="/format/2311.08911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connection Incentives in Cost Sharing Mechanisms with Budgets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dengji Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Sizhe Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2201.05976">arXiv:2201.05976</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">In a cost sharing problem on a weighted undirected graph, all other nodes
want to connect to the source node for some service. Each edge has a cost
denoted by a weight and all the connected nodes should share the total cost for
the connectivity. The goal of the existing solutions (e.g. folk solution and
cycle-complete solution) is to design cost sharing rules with nice properties,
e.g. budget balance and cost monotonicity. However, they did not consider the
cases that each non-source node has a budget which is the maximum it can pay
for its cost share and may cut its adjacent edges to reduce its cost share. In
this paper, we design two cost sharing mechanisms taking into account the
nodes' budgets and incentivizing all nodes to report all their adjacent edges
so that we can minimize the total cost for the connectivity.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08914" title="Abstract">arXiv:2311.08914</a> [<a href="/pdf/2311.08914" title="Download PDF">pdf</a>, <a href="/format/2311.08914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficiently Escaping Saddle Points for Non-Convex Policy Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khorasani%2C+S">Sadegh Khorasani</a>, 
<a href="/search/cs?searchtype=author&query=Salehkaleybar%2C+S">Saber Salehkaleybar</a>, 
<a href="/search/cs?searchtype=author&query=Kiyavash%2C+N">Negar Kiyavash</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>, 
<a href="/search/cs?searchtype=author&query=Grossglauser%2C+M">Matthias Grossglauser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Policy gradient (PG) is widely used in reinforcement learning due to its
scalability and good performance. In recent years, several variance-reduced PG
methods have been proposed with a theoretical guarantee of converging to an
approximate first-order stationary point (FOSP) with the sample complexity of
$O(\epsilon^{-3})$. However, FOSPs could be bad local optima or saddle points.
Moreover, these algorithms often use importance sampling (IS) weights which
could impair the statistical effectiveness of variance reduction. In this
paper, we propose a variance-reduced second-order method that uses second-order
information in the form of Hessian vector products (HVP) and converges to an
approximate second-order stationary point (SOSP) with sample complexity of
$\tilde{O}(\epsilon^{-3})$. This rate improves the best-known sample complexity
for achieving approximate SOSPs by a factor of $O(\epsilon^{-0.5})$. Moreover,
the proposed variance reduction technique bypasses IS weights by using HVP
terms. Our experimental results show that the proposed algorithm outperforms
the state of the art and is more robust to changes in random seeds.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08919" title="Abstract">arXiv:2311.08919</a> [<a href="/pdf/2311.08919" title="Download PDF">pdf</a>, <a href="/format/2311.08919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FCS-HGNN: Flexible Multi-type Community Search in Heterogeneous  Information Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guoxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+F">Fangda Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Peiying Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Community Search (CS), a crucial task in network science, has attracted
considerable interest owing to its prowess in unveiling personalized
communities, thereby finding applications across diverse domains. Existing
research primarily focuses on traditional homogeneous networks, which cannot be
directly applied to heterogeneous information networks (HINs). However,
existing research also has some limitations. For instance, either they solely
focus on single-type or multi-type community search, which severely lacking
flexibility, or they require users to specify meta-paths or predefined
community structures, which poses significant challenges for users who are
unfamiliar with community search and HINs. In this paper, we propose an
innovative method, FCS-HGNN, that can flexibly identify either single-type or
multi-type communities in HINs based on user preferences. We propose the
heterogeneous information transformer to handle node heterogeneity, and the
edge-semantic attention mechanism to address edge heterogeneity. This not only
considers the varying contributions of edges when identifying different
communities, but also expertly circumvents the challenges presented by
meta-paths, thereby elegantly unifying the single-type and multi-type community
search problems. Moreover, to enhance the applicability on large-scale graphs,
we propose the neighbor sampling and depth-based heuristic search strategies,
resulting in LS-FCS-HGNN. This algorithm significantly improves training and
query efficiency while maintaining outstanding community effectiveness. We
conducted extensive experiments on five real-world large-scale HINs, and the
results demonstrated that the effectiveness and efficiency of our proposed
method, which significantly outperforms state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08921" title="Abstract">arXiv:2311.08921</a> [<a href="/pdf/2311.08921" title="Download PDF">pdf</a>, <a href="/format/2311.08921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Improving for Zero-Shot Named Entity Recognition with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tingyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongwei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Exploring the application of powerful large language models (LLMs) on the
fundamental named entity recognition (NER) task has drawn much attention
recently. This work aims to investigate the possibilities of pushing the
boundary of zero-shot NER with LLM via a training-free self-improving strategy.
We propose a self-improving framework, which utilize an unlabeled corpus to
stimulate the self-learning ability of LLMs on NER. First, we use LLM to make
predictions on the unlabeled corpus and obtain the self-annotated data. Second,
we explore various strategies to select reliable samples from the
self-annotated dataset as demonstrations, considering the similarity, diversity
and reliability of demonstrations. Finally, we conduct inference for the test
query via in-context learning with the selected self-annotated demonstrations.
Through comprehensive experimental analysis, our study yielded the following
findings: (1) The self-improving framework further pushes the boundary of
zero-shot NER with LLMs, and achieves an obvious performance improvement; (2)
Iterative self-improving or naively increasing the size of unlabeled corpus
does not guarantee improvements; (3) There might still be space for improvement
via more advanced strategy for reliable entity selection.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08923" title="Abstract">arXiv:2311.08923</a> [<a href="/pdf/2311.08923" title="Download PDF">pdf</a>, <a href="/format/2311.08923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Activation Maximization and Generative Adversarial Training  to Recognize and Explain Patterns in Natural Areas in Satellite Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emam%2C+A">Ahmed Emam</a>, 
<a href="/search/cs?searchtype=author&query=Stomberg%2C+T+T">Timo T. Stomberg</a>, 
<a href="/search/cs?searchtype=author&query=Roscher%2C+R">Ribana Roscher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Natural protected areas are vital for biodiversity, climate change
mitigation, and supporting ecological processes. Despite their significance,
comprehensive mapping is hindered by a lack of understanding of their
characteristics and a missing land cover class definition. This paper aims to
advance the explanation of the designating patterns forming protected and wild
areas. To this end, we propose a novel framework that uses activation
maximization and a generative adversarial model. With this, we aim to generate
satellite images that, in combination with domain knowledge, are capable of
offering complete and valid explanations for the spatial and spectral patterns
that define the natural authenticity of these regions. Our proposed framework
produces more precise attribution maps pinpointing the designating patterns
forming the natural authenticity of protected areas. Our approach fosters our
understanding of the ecological integrity of the protected natural areas and
may contribute to future monitoring and preservation efforts.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08925" title="Abstract">arXiv:2311.08925</a> [<a href="/pdf/2311.08925" title="Download PDF">pdf</a>, <a href="/format/2311.08925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Octal Annular Ring-Shaped Planar Monopole Antenna For WiFi And  Unlicensed Ultra Wideband Frequency Range Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mangal%2C+J">Jai Mangal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 8 figures, letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Our paper presents the design of a unique annular ring-shaped planar monopole
antenna with octal geometry intended for a broad spectrum of frequency
applications. Utilizing FR4 epoxy for the substrate and copper material for the
top patch, the antenna measures 39 mm $\times$ 30 mm $\times$ 1.6 mm. It
exhibits resonance at 6.8 GHz, with a return loss of -49.01 dB. The antenna
demonstrates a broad frequency range from 2.1 GHz - 13.1 GHz, resulting in an
overall -10 dB bandwidth of 11 GHz. At the resonating frequency of 6.8 GHz, the
antenna accomplished a total gain of 3.01 dBi and a peak gain of 5.87 dBi at
12.4 GHz. Additionally, it attains a high radiation efficiency of 95.26\%. The
annular patch of the antenna helps distribute the current uniformly at the
boundaries of the patch. This helps achieve better current distribution and
wider -10 dB bandwidth. This antenna is a versatile replacement for multiple
antennas catering to various frequency bands.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08927" title="Abstract">arXiv:2311.08927</a> [<a href="/pdf/2311.08927" title="Download PDF">pdf</a>, <a href="/ps/2311.08927" title="Download PostScript">ps</a>, <a href="/format/2311.08927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introducing CHAD -- An ADM1 Solver for Direct Linking to Lagrangian CFD  Software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Prashant Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhenghao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Dabiri%2C+S">Soroush Dabiri</a>, 
<a href="/search/cs?searchtype=author&query=Rauch%2C+N">Nikolaus Rauch</a>, 
<a href="/search/cs?searchtype=author&query=Rauch%2C+W">Wolfgang Rauch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Standard methods for modeling anaerobic digestion processes assume
homogeneous conditions inside the tank and thus suffer from the negligence of
hydrodynamics. In this work, we present the software toolbox Coupled
Hydrodynamics and Anaerobic Digestion (CHAD), a novel parallelized solver that
is capable of utilizing CFD results as the basis for Anaerobic digestion model
No.1 (ADMno1) simulations. CHAD uses a particle-based Lagrangian CFD solver
i.e., DualSPHysics (DSPH) as input and provides for a parallelized, C++ code
implementation of the standard ADMno1. This paper demonstrates a conceptual and
numerical verification of the toolbox and outlines the future pathway to
enhance the approach.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08931" title="Abstract">arXiv:2311.08931</a> [<a href="/pdf/2311.08931" title="Download PDF">pdf</a>, <a href="/format/2311.08931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural-Based Uncertainty in Deep Learning Across Anatomical Scales:  Analysis in White Matter Lesion Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Molchanova%2C+N">Nataliia Molchanova</a>, 
<a href="/search/cs?searchtype=author&query=Raina%2C+V">Vatsal Raina</a>, 
<a href="/search/cs?searchtype=author&query=Malinin%2C+A">Andrey Malinin</a>, 
<a href="/search/cs?searchtype=author&query=La+Rosa%2C+F">Francesco La Rosa</a>, 
<a href="/search/cs?searchtype=author&query=Depeursinge%2C+A">Adrien Depeursinge</a>, 
<a href="/search/cs?searchtype=author&query=Gales%2C+M">Mark Gales</a>, 
<a href="/search/cs?searchtype=author&query=Granziera%2C+C">Cristina Granziera</a>, 
<a href="/search/cs?searchtype=author&query=Muller%2C+H">Henning Muller</a>, 
<a href="/search/cs?searchtype=author&query=Graziani%2C+M">Mara Graziani</a>, 
<a href="/search/cs?searchtype=author&query=Cuadra%2C+M+B">Meritxell Bach Cuadra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to the journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper explores uncertainty quantification (UQ) as an indicator of the
trustworthiness of automated deep-learning (DL) tools in the context of white
matter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of
multiple sclerosis (MS) patients. Our study focuses on two principal aspects of
uncertainty in structured output segmentation tasks. Firstly, we postulate that
a good uncertainty measure should indicate predictions likely to be incorrect
with high uncertainty values. Second, we investigate the merit of quantifying
uncertainty at different anatomical scales (voxel, lesion, or patient). We
hypothesize that uncertainty at each scale is related to specific types of
errors. Our study aims to confirm this relationship by conducting separate
analyses for in-domain and out-of-domain settings. Our primary methodological
contributions are (i) the development of novel measures for quantifying
uncertainty at lesion and patient scales, derived from structural prediction
discrepancies, and (ii) the extension of an error retention curve analysis
framework to facilitate the evaluation of UQ performance at both lesion and
patient scales. The results from a multi-centric MRI dataset of 172 patients
demonstrate that our proposed measures more effectively capture model errors at
the lesion and patient scales compared to measures that average voxel-scale
uncertainty values. We provide the UQ protocols code at
https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08934" title="Abstract">arXiv:2311.08934</a> [<a href="/pdf/2311.08934" title="Download PDF">pdf</a>, <a href="/ps/2311.08934" title="Download PostScript">ps</a>, <a href="/format/2311.08934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Shamir &amp; Additive Secret Sharing to Improve Efficiency of SMC  Primitives Against Malicious Adversaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goss%2C+K">Kenneth Goss</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1810.01571">arXiv:1810.01571</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Secure multi-party computation provides a wide array of protocols for
mutually distrustful parties be able to securely evaluate functions of private
inputs. Within recent years, many such protocols have been proposed
representing a plethora of strategies to securely and efficiently handle such
computation. These protocols have become increasingly efficient, but their
performance still is impractical in many settings. We propose new approaches to
some of these problems which are either more efficient than previous works
within the same security models or offer better security guarantees with
comparable efficiency. The goals of this research are to improve efficiency and
security of secure multi-party protocols and explore the application of such
approaches to novel threat scenarios. Some of the novel optimizations employed
are dynamically switching domains of shared secrets, asymmetric computations,
and advantageous functional transformations, among others. Specifically, this
work presents a novel combination of Shamir and Additive secret sharing to be
used in parallel which allows for the transformation of efficient protocols
secure against passive adversaries to be secure against active adversaries.
From this set of primitives we propose the construction of a comparison
protocol which can be implemented under that approach with a complexity which
is more efficient than other recent works for common domains of interest.
Finally, we present a system which addresses a critical security threat for the
protection and obfuscation of information which may be of high consequence.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08935" title="Abstract">arXiv:2311.08935</a> [<a href="/pdf/2311.08935" title="Download PDF">pdf</a>, <a href="/format/2311.08935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supported Trust Region Optimization for Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yixiu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongchang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline reinforcement learning suffers from the out-of-distribution issue and
extrapolation error. Most policy constraint methods regularize the density of
the trained policy towards the behavior policy, which is too restrictive in
most cases. We propose Supported Trust Region optimization (STR) which performs
trust region policy optimization with the policy constrained within the support
of the behavior policy, enjoying the less restrictive support constraint. We
show that, when assuming no approximation and sampling error, STR guarantees
strict policy improvement until convergence to the optimal support-constrained
policy in the dataset. Further with both errors incorporated, STR still
guarantees safe policy improvement for each step. Empirical results validate
the theory of STR and demonstrate its state-of-the-art performance on MuJoCo
locomotion domains and much more challenging AntMaze domains.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08936" title="Abstract">arXiv:2311.08936</a> [<a href="/pdf/2311.08936" title="Download PDF">pdf</a>, <a href="/format/2311.08936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confident Naturalness Explanation (CNE): A Framework to Explain and  Assess Patterns Forming Naturalness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emam%2C+A">Ahmed Emam</a>, 
<a href="/search/cs?searchtype=author&query=Farag%2C+M">Mohamed Farag</a>, 
<a href="/search/cs?searchtype=author&query=Roscher%2C+R">Ribana Roscher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Protected natural areas are regions that have been minimally affected by
human activities such as urbanization, agriculture, and other human
interventions. To better understand and map the naturalness of these areas,
machine learning models can be used to analyze satellite imagery. Specifically,
explainable machine learning methods show promise in uncovering patterns that
contribute to the concept of naturalness within these protected environments.
Additionally, addressing the uncertainty inherent in machine learning models is
crucial for a comprehensive understanding of this concept. However, existing
approaches have limitations. They either fail to provide explanations that are
both valid and objective or struggle to offer a quantitative metric that
accurately measures the contribution of specific patterns to naturalness, along
with the associated confidence. In this paper, we propose a novel framework
called the Confident Naturalness Explanation (CNE) framework. This framework
combines explainable machine learning and uncertainty quantification to assess
and explain naturalness. We introduce a new quantitative metric that describes
the confident contribution of patterns to the concept of naturalness.
Furthermore, we generate an uncertainty-aware segmentation mask for each input
sample, highlighting areas where the model lacks knowledge. To demonstrate the
effectiveness of our framework, we apply it to a study site in Fennoscandia
using two open-source satellite datasets.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08941" title="Abstract">arXiv:2311.08941</a> [<a href="/pdf/2311.08941" title="Download PDF">pdf</a>, <a href="/format/2311.08941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning over Description Logic-based Contexts with Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Poulis%2C+A">Angelos Poulis</a>, 
<a href="/search/cs?searchtype=author&query=Tsalapati%2C+E">Eleni Tsalapati</a>, 
<a href="/search/cs?searchtype=author&query=Koubarakis%2C+M">Manolis Koubarakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">One way that the current state of the art measures the reasoning ability of
transformer-based models is by evaluating accuracy in downstream tasks like
logical question answering or proof generation over synthetic contexts
expressed in natural language. However, most of the contexts used are in
practice very simple; in most cases, they are generated from short first-order
logic sentences with only a few logical operators and quantifiers. In this
work, we seek to answer the question how well a transformer-based model will
perform reasoning over expressive contexts. For this purpose, we construct a
synthetic natural language question-answering dataset, generated by description
logic knowledge bases. For the generation of the knowledge bases, we use the
expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K
examples, and increases in two dimensions: i) reasoning depth, and ii) length
of sentences. We show that the performance of our DeBERTa-based model,
DELTA$_M$, is marginally affected when the reasoning depth is increased and it
is not affected at all when the length of the sentences is increasing. We also
evaluate the generalization ability of the model on reasoning depths unseen at
training, both increasing and decreasing, revealing interesting insights into
the model's adaptive generalization abilities.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08942" title="Abstract">arXiv:2311.08942</a> [<a href="/pdf/2311.08942" title="Download PDF">pdf</a>, <a href="/format/2311.08942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thermally-Resilient Soft Gripper for Space Debris Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ruiz%2C+F">Fernando Ruiz</a>, 
<a href="/search/eess?searchtype=author&query=Arrue%2C+B">Begona Arrue</a>, 
<a href="/search/eess?searchtype=author&query=Ollero%2C+A">Anibal Ollero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Space debris poses a significant and growing threat to orbital operations,
demanding urgent solutions. Soft manipulators, with their adaptability to
various shapes and sizes, present a promising approach to mitigate this concern
and facilitate orbital maintenance tasks. Challenges such as radiation, vacuum,
and microgravity are significant, but the predominant issue is ensuring these
devices operate effectively in the extreme temperature swings from -180{\deg}C
to over 200{\deg}C. The majority of soft materials become brittle and hard due
to crystallization at cryogenic temperatures or undergo drastic shifts in their
elasticity near their melting points. This work pioneers experiments using
liquid nitrogen to simulate cryogenic conditions and heat guns for elevated
temperatures. It derives insights into the behavior of these materials, leading
to the design of a soft gripper tailored for space debris removal in LEO
orbits. The multi-layered design leverages the properties of thermoplastic
polyurethane at low infill rates for lightweight inherent flexibility, silicone
rubber ensuring structural integrity, PTFE (Teflon) for unparalleled thermal
stability, and aerogel for insulation. The nylon-crafted tendon-driven
mechanism incorporated uses molybdenum disulfide grease as a lubrication layer
for cryogenic temperatures. The insights from this experiments and the modeling
of the temperature-driven property alterations are pivotal for the advancement
of soft manipulators tailored for on-orbit operations.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08943" title="Abstract">arXiv:2311.08943</a> [<a href="/pdf/2311.08943" title="Download PDF">pdf</a>, <a href="/format/2311.08943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety, Trust, and Ethics Considerations for Human-AI Teaming in  Aerospace Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hobbs%2C+K+L">Kerianne L. Hobbs</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bernard Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Designing a safe, trusted, and ethical AI may be practically impossible;
however, designing AI with safe, trusted, and ethical use in mind is possible
and necessary in safety and mission-critical domains like aerospace. Safe,
trusted, and ethical use of AI are often used interchangeably; however, a
system can be safely used but not trusted or ethical, have a trusted use that
is not safe or ethical, and have an ethical use that is not safe or trusted.
This manuscript serves as a primer to illuminate the nuanced differences
between these concepts, with a specific focus on applications of Human-AI
teaming in aerospace system control, where humans may be in, on, or
out-of-the-loop of decision-making.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08946" title="Abstract">arXiv:2311.08946</a> [<a href="/pdf/2311.08946" title="Download PDF">pdf</a>, <a href="/ps/2311.08946" title="Download PostScript">ps</a>, <a href="/format/2311.08946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An explicit substructuring method for overlapping domain decomposition  based on stochastic calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mor%C3%B3n-Vidal%2C+J">Jorge Mor&#xf3;n-Vidal</a>, 
<a href="/search/math?searchtype=author&query=Bernal%2C+F">Francisco Bernal</a>, 
<a href="/search/math?searchtype=author&query=Suzuki%2C+A">Atsushi Suzuki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">In a recent paper [{\em F. Bernal, J. Mor\'on-Vidal and J.A. Acebr\'on,
Comp.$\&amp;$ Math. App. 146:294-308 (2023)}] an hybrid supercomputing algorithm
for elliptic equations has been put forward. The idea is that the interfacial
nodal solutions solve a linear system, whose coefficients are expectations of
functionals of stochastic differential equations confined within patches of
about subdomain size. Compared to standard substructuring techniques such as
the Schur complement method for the skeleton, the hybrid approach renders an
explicit and sparse shrunken matrix -- hence suitable for being substructured
again. The ultimate goal is to push strong scalability beyond the state of the
art, by leveraging the scope for parallelisation of stochastic calculus. Here,
we present a major revamping of that framework, based on the insight of
embedding the domain in a cover of overlapping circles (in two dimensions).
This allows for efficient Fourier interpolation along the interfaces (now
circumferences) and -- crucially -- for the evaluation of most of the
interfacial system entries as the solution of small boundary value problems on
a circle. This is both extremely efficient (as they can be solved in parallel
and by the pseudospectral method) and free of Monte Carlo error. Stochastic
numerics are only needed on the relatively few circles intersecting the domain
boundary. In sum, the new formulation is significantly faster, simpler and more
accurate, while retaining all of the advantageous properties of PDDSparse.
Numerical experiments are included for the purpose of illustration.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08951" title="Abstract">arXiv:2311.08951</a> [<a href="/pdf/2311.08951" title="Download PDF">pdf</a>, <a href="/ps/2311.08951" title="Download PostScript">ps</a>, <a href="/format/2311.08951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corrections to &quot;Universal Densities Exist for Every Finite Reference  Measure&quot;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%C4%99bowski%2C+%C5%81">&#x141;ukasz D&#x119;bowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, no figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In the article "Universal Densities Exist for Every Finite Reference Measure"
(IEEE Trans. Inform. Theory, vol. 69, no. 8, pp. 5277--5288, 2023) we neglected
to mention relevant contributions of Boris Ryabko. We cited a source by him
that contains a construction of the universal density that we claimed to be our
own invention without checking the source after drafting the article. Our
article was motivated by a preprint by Feutrill and Roughan, about which we had
learned when reviewing the PhD thesis by Andrew Feutrill. Whereas we were not
allowed to contact Feutrill and Roughan besides the review form, we developed
some ideas of theirs further, ignoring that we stepped into the area previously
researched by Ryabko. Our published results exceed those by Ryabko but the
article should have been refocused to report Ryabko's contributions. In this
note, we detail our citation mistakes.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08952" title="Abstract">arXiv:2311.08952</a> [<a href="/pdf/2311.08952" title="Download PDF">pdf</a>, <a href="/format/2311.08952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel concept for Titan robotic exploration based on soft morphing  aerial robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruiz%2C+F">Fernando Ruiz</a>, 
<a href="/search/cs?searchtype=author&query=Arrue%2C+B">Begona Arrue</a>, 
<a href="/search/cs?searchtype=author&query=Ollero%2C+A">Anibal Ollero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at International Astronautical Congress 2023 (Baku, Azerbaiyan)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This work introduces a novel approach for Titan exploration based on soft
morphing aerial robots leveraging the use of flexible adaptive materials. The
controlled deformation of the multirotor arms, actuated by a combination of a
pneumatic system and a tendon mechanism, provides the explorer robot with the
ability to perform full-body perching and land on rocky, irregular, or uneven
terrains, thus unlocking new exploration horizons. In addition, after landing,
they can be used for efficient sampling as tendon-driven continuum
manipulators, with the pneumatic system drawing in the samples. The proposed
arms enable the drone to cover long distances in Titan's atmosphere
efficiently, by directing rotor thrust without rotating the body, reducing the
aerodynamic drag. Given that the exploration concept is envisioned as a
rotorcraft planetary lander, the robot's folding features enable over a 30$\%$
reduction in the hypersonic aeroshell's diameter. Building on this folding
capability, the arms can morph partially in flight to navigate tight spaces. As
for propulsion, the rotor design, justified through CFD simulations, utilizes a
ducted fan configuration tailored for Titan's high Reynolds numbers. The rotors
are integrated within the robot's deformable materials, facilitating smooth
interactions with the environment. The research spotlights exploration
simulations in the Gazebo environment, focusing on the Sotra-Patera cryovolcano
region, a location with potential to clarify Titan's unique methane cycle and
its Earth-like features. This work addresses one of the primary challenges of
the concept by testing the behavior of small-scale deformable arms under
conditions mimicking those of Titan. Groundbreaking experiments with liquid
nitrogen at cryogenic temperatures were conducted on various materials, with
Teflon (PTFE) at low infill rates (15-30%) emerging as a promising option.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08955" title="Abstract">arXiv:2311.08955</a> [<a href="/pdf/2311.08955" title="Download PDF">pdf</a>, <a href="/ps/2311.08955" title="Download PostScript">ps</a>, <a href="/format/2311.08955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianjun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zebin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Liang Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Fusion-based hyperspectral image (HSI) super-resolution aims to produce a
high-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a
high-spatial-resolution multispectral image. Such a HSI super-resolution
process can be modeled as an inverse problem, where the prior knowledge is
essential for obtaining the desired solution. Motivated by the success of
diffusion models, we propose a novel spectral diffusion prior for fusion-based
HSI super-resolution. Specifically, we first investigate the spectrum
generation problem and design a spectral diffusion model to model the spectral
data distribution. Then, in the framework of maximum a posteriori, we keep the
transition information between every two neighboring states during the reverse
generative process, and thereby embed the knowledge of trained spectral
diffusion model into the fusion problem in the form of a regularization term.
At last, we treat each generation step of the final optimization problem as its
subproblem, and employ the Adam to solve these subproblems in a reverse
sequence. Experimental results conducted on both synthetic and real datasets
demonstrate the effectiveness of the proposed approach. The code of the
proposed approach will be available on https://github.com/liuofficial/SDP.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08957" title="Abstract">arXiv:2311.08957</a> [<a href="/pdf/2311.08957" title="Download PDF">pdf</a>, <a href="/format/2311.08957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in  Social Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbo%2C+G+A">Giulio Antonio Abbo</a>, 
<a href="/search/cs?searchtype=author&query=Belpaeme%2C+T">Tony Belpaeme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In the rapidly evolving landscape of human-computer interaction, the
integration of vision capabilities into conversational agents stands as a
crucial advancement. This paper presents an initial implementation of a
dialogue manager that leverages the latest progress in Large Language Models
(e.g., GPT-4, IDEFICS) to enhance the traditional text-based prompts with
real-time visual input. LLMs are used to interpret both textual prompts and
visual stimuli, creating a more contextually aware conversational agent. The
system's prompt engineering, incorporating dialogue with summarisation of the
images, ensures a balance between context preservation and computational
efficiency. Six interactions with a Furhat robot powered by this system are
reported, illustrating and discussing the results obtained. By implementing
this vision-enabled dialogue system, the paper envisions a future where
conversational agents seamlessly blend textual and visual modalities, enabling
richer, more context-aware dialogues.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08961" title="Abstract">arXiv:2311.08961</a> [<a href="/pdf/2311.08961" title="Download PDF">pdf</a>, <a href="/format/2311.08961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DBJoules: An Energy Measurement Tool for Database Management Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lella%2C+H+S">Hemasri Sai Lella</a>, 
<a href="/search/cs?searchtype=author&query=Manasa%2C+K">Kurra Manasa</a>, 
<a href="/search/cs?searchtype=author&query=Chattaraj%2C+R">Rajrupa Chattaraj</a>, 
<a href="/search/cs?searchtype=author&query=Chimalakonda%2C+S">Sridhar Chimalakonda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In the rapidly evolving landscape of modern data-driven technologies,
software relies on large datasets and constant data center operations using
various database systems to support computation-intensive tasks. As energy
consumption in software systems becomes a growing concern, selecting the right
database from energy-efficiency perspective is also critical. To address this,
we introduce \textbf{\textit{DBJoules}}, a tool that measures the energy
consumption of activities in database systems. \textit{DBJoules} supports
energy measurement of CRUD operations for four popular databases. Through
evaluations on two widely-used datasets, we identify disparities of 7\% to 38\%
in the energy consumption of these databases. Hence, the goal is to raise
developer awareness about the effect of running queries in different databases
from an energy consumption perspective, enabling them to select appropriate
database for sustainable usage. The tool's demonstration is available at
\url{https://youtu.be/D1MTZum0jok} and related artifacts at
\url{https://rishalab.github.io/DBJoules/}.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08966" title="Abstract">arXiv:2311.08966</a> [<a href="/pdf/2311.08966" title="Download PDF">pdf</a>, <a href="/format/2311.08966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Large-scale Deep Biasing with Phoneme Features and Text-only  Data in Streaming Transducer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Jin Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zejun Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Deep biasing for the Transducer can improve the recognition performance of
rare words or contextual entities, which is essential in practical
applications, especially for streaming Automatic Speech Recognition (ASR).
However, deep biasing with large-scale rare words remains challenging, as the
performance drops significantly when more distractors exist and there are words
with similar grapheme sequences in the bias list. In this paper, we combine the
phoneme and textual information of rare words in Transducers to distinguish
words with similar pronunciation or spelling. Moreover, the introduction of
training with text-only data containing more rare words benefits large-scale
deep biasing. The experiments on the LibriSpeech corpus demonstrate that the
proposed method achieves state-of-the-art performance on rare word error rate
for different scales and levels of bias lists.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08967" title="Abstract">arXiv:2311.08967</a> [<a href="/pdf/2311.08967" title="Download PDF">pdf</a>, <a href="/format/2311.08967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homomorphic Polynomial Public Key Cryptography for Quantum-secure  Digital Signature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+R">Randy Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Perepechaenko%2C+M">Maria Perepechaenko</a>, 
<a href="/search/cs?searchtype=author&query=Sayed%2C+M">Mahmoud Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+D">Dafu Lou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In their 2022 study, Kuang et al. introduced Multivariable Polynomial Public
Key (MPPK) cryptography, leveraging the inversion relationship between
multiplication and division for quantum-safe public key systems. They extended
MPPK into Homomorphic Polynomial Public Key (HPPK), employing homomorphic
encryption for large hidden ring operations. Originally designed for key
encapsulation (KEM), HPPK's security relies on homomorphic encryption of public
polynomials. This paper expands HPPK KEM to a digital signature scheme, facing
challenges due to the distinct nature of verification compared to decryption.
To adapt HPPK KEM to digital signatures, the authors introduce an extension of
the Barrett reduction algorithm, transforming modular multiplications into
divisions in the verification equation over a prime field. The extended
algorithm non-linearly embeds the signature into public polynomial
coefficients, addressing vulnerabilities in earlier MPPK DS schemes. Security
analysis demonstrates exponential complexity for private key recovery and
forged signature attacks, considering ring bit length twice that of the prime
field size.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08968" title="Abstract">arXiv:2311.08968</a> [<a href="/pdf/2311.08968" title="Download PDF">pdf</a>, <a href="/format/2311.08968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Linear Relational Concepts in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chanin%2C+D">David Chanin</a>, 
<a href="/search/cs?searchtype=author&query=Hunter%2C+A">Anthony Hunter</a>, 
<a href="/search/cs?searchtype=author&query=Camburu%2C+O">Oana-Maria Camburu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformer language models (LMs) have been shown to represent concepts as
directions in the latent space of hidden activations. However, for any given
human-interpretable concept, how can we find its direction in the latent space?
We present a technique called linear relational concepts (LRC) for finding
concept directions corresponding to human-interpretable concepts at a given
hidden layer in a transformer LM by first modeling the relation between subject
and object as a linear relational embedding (LRE). While the LRE work was
mainly presented as an exercise in understanding model representations, we find
that inverting the LRE while using earlier object layers results in a powerful
technique to find concept directions that both work well as a classifier and
causally influence model outputs.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08969" title="Abstract">arXiv:2311.08969</a> [<a href="/pdf/2311.08969" title="Download PDF">pdf</a>, <a href="/format/2311.08969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PDU-set Scheduling Algorithm for XR Traffic in Multi-Service 5G-Advanced  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paymard%2C+P">Pouria Paymard</a>, 
<a href="/search/cs?searchtype=author&query=Paris%2C+S">Stefano Paris</a>, 
<a href="/search/cs?searchtype=author&query=Amiri%2C+A">Abolfazl Amiri</a>, 
<a href="/search/cs?searchtype=author&query=Kolding%2C+T+E">Troels E. Kolding</a>, 
<a href="/search/cs?searchtype=author&query=Moya%2C+F+S">Fernando Sanchez Moya</a>, 
<a href="/search/cs?searchtype=author&query=Pedersen%2C+K+I">Klaus I. Pedersen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In this paper, we investigate a dynamic packet scheduling algorithm designed
to enhance the eXtended Reality (XR) capacity of fifth-generation (5G)-Advanced
networks with multiple cells, multiple users, and multiple services. The
scheduler exploits the newly defined protocol data unit (PDU)-set information
for XR traffic flows to enhance its quality-of-service awareness. To evaluate
the performance of the proposed solution, advanced dynamic system-level
simulations are conducted. The findings reveal that the proposed scheduler
offers a notable improvement in increasing XR capacity up to 45%, while keeping
the same enhanced mobile broadband (eMBB) cell throughput as compared to the
well-known baseline schedulers.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08972" title="Abstract">arXiv:2311.08972</a> [<a href="/pdf/2311.08972" title="Download PDF">pdf</a>, <a href="/format/2311.08972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised approaches based on optimal transport and convex analysis  for inverse problems in imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carioni%2C+M">Marcello Carioni</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Subhadip Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H+Y">Hong Ye Tan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Junqi Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Unsupervised deep learning approaches have recently become one of the crucial
research areas in imaging owing to their ability to learn expressive and
powerful reconstruction operators even when paired high-quality training data
is scarcely available. In this chapter, we review theoretically principled
unsupervised learning schemes for solving imaging inverse problems, with a
particular focus on methods rooted in optimal transport and convex analysis. We
begin by reviewing the optimal transport-based unsupervised approaches such as
the cycle-consistency-based models and learned adversarial regularization
methods, which have clear probabilistic interpretations. Subsequently, we give
an overview of a recent line of works on provably convergent learned
optimization algorithms applied to accelerate the solution of imaging inverse
problems, alongside their dedicated unsupervised training schemes. We also
survey a number of provably convergent plug-and-play algorithms (based on
gradient-step deep denoisers), which are among the most important and widely
applied unsupervised approaches for imaging problems. At the end of this
survey, we provide an overview of a few related unsupervised learning
frameworks that complement our focused schemes. Together with a detailed
survey, we provide an overview of the key mathematical results that underlie
the methods reviewed in the chapter to keep our discussion self-contained.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08979" title="Abstract">arXiv:2311.08979</a> [<a href="/pdf/2311.08979" title="Download PDF">pdf</a>, <a href="/format/2311.08979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multimodal Dataset of 21,412 Recorded Nights for Sleep and Respiratory  Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diament%2C+A">Alon Diament</a> (1), 
<a href="/search/cs?searchtype=author&query=Gorodetski%2C+M">Maria Gorodetski</a> (1), 
<a href="/search/cs?searchtype=author&query=Jankelow%2C+A">Adam Jankelow</a> (1), 
<a href="/search/cs?searchtype=author&query=Keshet%2C+A">Ayya Keshet</a> (2), 
<a href="/search/cs?searchtype=author&query=Shor%2C+T">Tal Shor</a> (1), 
<a href="/search/cs?searchtype=author&query=Weissglas-Volkov%2C+D">Daphna Weissglas-Volkov</a> (1), 
<a href="/search/cs?searchtype=author&query=Rossman%2C+H">Hagai Rossman</a> (1), 
<a href="/search/cs?searchtype=author&query=Segal%2C+E">Eran Segal</a> (2) ((1) Pheno.AI, Tel-Aviv, Israel, (2) Weizmann Institute of Science, Rehovot, Israel)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This study introduces a novel, rich dataset obtained from home sleep apnea
tests using the FDA-approved WatchPAT-300 device, collected from 7,077
participants over 21,412 nights. The dataset comprises three levels of sleep
data: raw multi-channel time-series from sensors, annotated sleep events, and
computed summary statistics, which include 447 features related to sleep
architecture, sleep apnea, and heart rate variability (HRV). We present
reference values for Apnea/Hypopnea Index (AHI), sleep efficiency, Wake After
Sleep Onset (WASO), and HRV sample entropy, stratified by age and sex.
Moreover, we demonstrate that the dataset improves the predictive capability
for various health related traits, including body composition, bone density,
blood sugar levels and cardiovascular health. These results illustrate the
dataset's potential to advance sleep research, personalized healthcare, and
machine learning applications in biomedicine.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08981" title="Abstract">arXiv:2311.08981</a> [<a href="/pdf/2311.08981" title="Download PDF">pdf</a>, <a href="/format/2311.08981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speculative Contrastive Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hongyi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Keming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have shown extraordinary performance in various
language tasks, but high computational requirements hinder their widespread
deployment. Speculative decoding, which uses amateur models to predict the
generation of expert models, has been proposed as a way to accelerate LLM
inference. However, speculative decoding focuses on acceleration instead of
making the best use of the token distribution from amateur models. We proposed
Speculative Contrastive Decoding (SCD), an accelerated decoding method
leveraging the natural contrast between expert and amateur models in
speculative decoding. Comprehensive evaluations on four benchmarks show that
SCD can achieve similar acceleration factors as speculative decoding while
further improving the generation quality as the contrastive decoding. The
analysis of token probabilities further demonstrates the compatibility between
speculative and contrastive decoding. Overall, SCD provides an effective
approach to enhance the decoding quality of LLMs while saving computational
resources.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08982" title="Abstract">arXiv:2311.08982</a> [<a href="/pdf/2311.08982" title="Download PDF">pdf</a>, <a href="/format/2311.08982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SentAlign: Accurate and Scalable Sentence Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steingr%C3%ADmsson%2C+S">Stein&#xfe;&#xf3;r Steingr&#xed;msson</a>, 
<a href="/search/cs?searchtype=author&query=Loftsson%2C+H">Hrafn Loftsson</a>, 
<a href="/search/cs?searchtype=author&query=Way%2C+A">Andy Way</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 System Demonstration paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present SentAlign, an accurate sentence alignment tool designed to handle
very large parallel document pairs. Given user-defined parameters, the
alignment algorithm evaluates all possible alignment paths in fairly large
documents of thousands of sentences and uses a divide-and-conquer approach to
align documents containing tens of thousands of sentences. The scoring function
is based on LaBSE bilingual sentence representations. SentAlign outperforms
five other sentence alignment tools when evaluated on two different evaluation
sets, German-French and English-Icelandic, and on a downstream machine
translation task.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08983" title="Abstract">arXiv:2311.08983</a> [<a href="/pdf/2311.08983" title="Download PDF">pdf</a>, <a href="/format/2311.08983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge Accelerated Robot Navigation with Hierarchical Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+R">Ruihua Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Fei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Eldar%2C+Y+C">Yonina C. Eldar</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengzhong Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 14 figures, 1 table, submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Low-cost autonomous robots suffer from limited onboard computing power,
resulting in excessive computation time when navigating in cluttered
environments. This paper presents Edge Accelerated Robot Navigation, or EARN
for short, to achieve real-time collision avoidance by adopting hierarchical
motion planning (HMP). In contrast to existing local or edge motion planning
solutions that ignore the interdependency between low-level motion planning and
high-level resource allocation, EARN adopts model predictive switching (MPS)
that maximizes the expected switching gain w.r.t. robot states and actions
under computation and communication resource constraints. As such, each robot
can dynamically switch between a point-mass motion planner executed locally to
guarantee safety (e.g., path-following) and a full-shape motion planner
executed non-locally to guarantee efficiency (e.g., overtaking). The crux to
EARN is a two-time scale integrated decision-planning algorithm based on
bilevel mixed-integer optimization, and a fast conditional collision avoidance
algorithm based on penalty dual decomposition. We validate the performance of
EARN in indoor simulation, outdoor simulation, and real-world environments.
Experiments show that EARN achieves significantly smaller navigation time and
collision ratios than state-of-the-art navigation approaches.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08987" title="Abstract">arXiv:2311.08987</a> [<a href="/pdf/2311.08987" title="Download PDF">pdf</a>, <a href="/ps/2311.08987" title="Download PostScript">ps</a>, <a href="/format/2311.08987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proceedings Fifth International Workshop on Formal Methods for  Autonomous Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farrell%2C+M">Marie Farrell</a> (University of Manchester, UK), 
<a href="/search/cs?searchtype=author&query=Luckcuck%2C+M">Matt Luckcuck</a> (University of Nottingham, UK), 
<a href="/search/cs?searchtype=author&query=Gleirscher%2C+M">Mario Gleirscher</a> (University of Bremen, Germany), 
<a href="/search/cs?searchtype=author&query=Schwammberger%2C+M">Maike Schwammberger</a> (Karlsruhe Institute of Technology, Germany)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This EPTCS volume contains the proceedings for the Fifth International
Workshop on Formal Methods for Autonomous Systems (FMAS 2023), which was held
on the 15th and 16th of November 2023. FMAS 2023 was co-located with 18th
International Conference on integrated Formal Methods (iFM) (iFM'22), organised
by Leiden Institute of Advanced Computer Science of Leiden University. The
workshop itself was held at Scheltema Leiden, a renovated 19th Century blanket
factory alongside the canal.
<br />FMAS 2023 received 25 submissions. We received 11 regular papers, 3
experience reports, 6 research previews, and 5 vision papers. The researchers
who submitted papers to FMAS 2023 were from institutions in: Australia, Canada,
Colombia, France, Germany, Ireland, Italy, the Netherlands, Sweden, the United
Kingdom, and the United States of America. Increasing our number of submissions
for the third year in a row is an encouraging sign that FMAS has established
itself as a reputable publication venue for research on the formal modelling
and verification of autonomous systems. After each paper was reviewed by three
members of our Programme Committee we accepted a total of 15 papers: 8 long
papers and 7 short papers.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08988" title="Abstract">arXiv:2311.08988</a> [<a href="/pdf/2311.08988" title="Download PDF">pdf</a>, <a href="/format/2311.08988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counting Small Induced Subgraphs with Edge-monotone Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%C3%B6ring%2C+S">Simon D&#xf6;ring</a>, 
<a href="/search/cs?searchtype=author&query=Marx%2C+D">D&#xe1;niel Marx</a>, 
<a href="/search/cs?searchtype=author&query=Wellnitz%2C+P">Philip Wellnitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study the parameterized complexity of #IndSub($\Phi$), where given a graph
$G$ and an integer $k$, the task is to count the number of induced subgraphs on
$k$ vertices that satisfy the graph property $\Phi$. Focke and Roth [STOC 2022]
completely characterized the complexity for each $\Phi$ that is a hereditary
property (that is, closed under vertex deletions): #IndSub($\Phi$) is
#W[1]-hard except in the degenerate cases when every graph satisfies $\Phi$ or
only finitely many graphs satisfy $\Phi$. We complement this result with a
classification for each $\Phi$ that is edge monotone (that is, closed under
edge deletions): #IndSub($\Phi$) is #W[1]-hard except in the degenerate case
when there are only finitely many integers $k$ such that $\Phi$ is nontrivial
on $k$-vertex graphs. Our result generalizes earlier results for specific
properties $\Phi$ that are related to the connectivity or density of the graph.
<br />Further, we extend the #W[1]-hardness result by a lower bound which shows
that #IndSub($\Phi$) cannot be solved in time $f(k) \cdot |V(G)|^{o(\sqrt{\log
k/\log\log k})}$ for any function $f$, unless the Exponential-Time Hypothesis
(ETH) fails. For many natural properties, we obtain even a tight bound $f(k)
\cdot |V(G)|^{o(k)}$; for example, this is the case for every property $\Phi$
that is nontrivial on $k$-vertex graphs for each $k$ greater than some $k_0$.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08989" title="Abstract">arXiv:2311.08989</a> [<a href="/pdf/2311.08989" title="Download PDF">pdf</a>, <a href="/format/2311.08989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMF-Aware Power Control for Massive MIMO: Cell-Free versus Cellular  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liesegang%2C+S">Sergi Liesegang</a>, 
<a href="/search/cs?searchtype=author&query=Buzzi%2C+S">Stefano Buzzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The impressive growth of wireless data networks has recently led to increased
attention to the issue of electromagnetic pollution. Specific absorption rates
and incident power densities have become popular indicators for measuring
electromagnetic field (EMF) exposure. This paper tackles the problem of power
control in user-centric cell-free massive multiple-input-multiple-output
(CF-mMIMO) systems under EMF constraints. Specifically, the power allocation
maximizing the minimum data rate across users is derived for both the uplink
and the downlink under EMF constraints. The developed solution is also applied
to a cellular mMIMO system and compared to other benchmark strategies.
Simulation results prove that EMF safety restrictions can be easily met without
jeopardizing the minimum data rate, that the CF-mMIMO outperforms the
multi-cell massive MIMO deployment, and that the proposed power control
strategy greatly improves the system fairness.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08992" title="Abstract">arXiv:2311.08992</a> [<a href="/pdf/2311.08992" title="Download PDF">pdf</a>, <a href="/ps/2311.08992" title="Download PostScript">ps</a>, <a href="/format/2311.08992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lifting iso-dual algebraic geometry codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chara%2C+M">Mar&#xed;a Chara</a>, 
<a href="/search/cs?searchtype=author&query=Podest%C3%A1%2C+R">Ricardo Podest&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Quoos%2C+L">Luciane Quoos</a>, 
<a href="/search/cs?searchtype=author&query=Toledano%2C+R">Ricardo Toledano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Number Theory (math.NT)

</div>
<p class="mathjax">In this work we investigate the problem of producing iso-dual algebraic
geometry (AG) codes over a finite field $\mathbb{F}_q$ with $q$ elements. Given
a finite separable extension $\mathcal{M}/\mathcal{F}$ of function fields and
an iso-dual AG-code $\mathcal{C}$ defined over $\mathcal{F}$, we provide a
general method to lift the code $\mathcal{C}$ to another iso-dual AG-code
$\tilde{\mathcal{C}}$ defined over $\mathcal{M}$ under some assumptions on the
parity of the involved different exponents. We apply this method to lift
iso-dual AG-codes over the rational function field to elementary abelian
$p$-extensions, like the maximal function fields defined by the Hermitian,
Suzuki, and one covered by the $GGS$ function field. We also obtain long binary
and ternary iso-dual AG-codes defined over cyclotomic extensions.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08993" title="Abstract">arXiv:2311.08993</a> [<a href="/pdf/2311.08993" title="Download PDF">pdf</a>, <a href="/format/2311.08993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When does In-context Learning Fall Short and Why? A Study on  Specification-Heavy Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaozhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianhui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weikai Li</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yunjia Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zimu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhili Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kaisheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In-context learning (ICL) has become the default method for using large
language models (LLMs), making the exploration of its limitations and
understanding the underlying causes crucial. In this paper, we find that ICL
falls short of handling specification-heavy tasks, which are tasks with
complicated and extensive task specifications, requiring several hours for
ordinary humans to master, such as traditional information extraction tasks.
The performance of ICL on these tasks mostly cannot reach half of the
state-of-the-art results. To explore the reasons behind this failure, we
conduct comprehensive experiments on 18 specification-heavy tasks with various
LLMs and identify three primary reasons: inability to specifically understand
context, misalignment in task schema comprehension with humans, and inadequate
long-text understanding ability. Furthermore, we demonstrate that through
fine-tuning, LLMs can achieve decent performance on these tasks, indicating
that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback
of existing alignment methods that renders LLMs incapable of handling
complicated specification-heavy tasks via ICL. To substantiate this, we perform
dedicated instruction tuning on LLMs for these tasks and observe a notable
improvement. We hope the analyses in this paper could facilitate advancements
in alignment methods enabling LLMs to meet more sophisticated human demands.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08995" title="Abstract">arXiv:2311.08995</a> [<a href="/pdf/2311.08995" title="Download PDF">pdf</a>, <a href="/ps/2311.08995" title="Download PostScript">ps</a>, <a href="/format/2311.08995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple but Effective Unsupervised Classification for Specified Domain  Images: A Case Study on Fungi Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=liu%2C+Z">Zhaocong liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fa Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Huanxi Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoyan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chichun Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">High-quality labeled datasets are essential for deep learning. Traditional
manual annotation methods are not only costly and inefficient but also pose
challenges in specialized domains where expert knowledge is needed.
Self-supervised methods, despite leveraging unlabeled data for feature
extraction, still require hundreds or thousands of labeled instances to guide
the model for effective specialized image classification. Current unsupervised
learning methods offer automatic classification without prior annotation but
often compromise on accuracy. As a result, efficiently procuring high-quality
labeled datasets remains a pressing challenge for specialized domain images
devoid of annotated data. Addressing this, an unsupervised classification
method with three key ideas is introduced: 1) dual-step feature dimensionality
reduction using a pre-trained model and manifold learning, 2) a voting
mechanism from multiple clustering algorithms, and 3) post-hoc instead of prior
manual annotation. This approach outperforms supervised methods in
classification accuracy, as demonstrated with fungal image data, achieving
94.1% and 96.7% on public and private datasets respectively. The proposed
unsupervised classification method reduces dependency on pre-annotated
datasets, enabling a closed-loop for data classification. The simplicity and
ease of use of this method will also bring convenience to researchers in
various fields in building datasets, promoting AI applications for images in
specialized domains.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08996" title="Abstract">arXiv:2311.08996</a> [<a href="/pdf/2311.08996" title="Download PDF">pdf</a>, <a href="/format/2311.08996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel Estimation for mmWave MIMO using sub-6 GHz Out-of-Band  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasic%2C+F">Faruk Pasic</a>, 
<a href="/search/cs?searchtype=author&query=Hofer%2C+M">Markus Hofer</a>, 
<a href="/search/cs?searchtype=author&query=Mussbah%2C+M">Mariam Mussbah</a>, 
<a href="/search/cs?searchtype=author&query=Caban%2C+S">Sebastian Caban</a>, 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+S">Stefan Schwarz</a>, 
<a href="/search/cs?searchtype=author&query=Zemen%2C+T">Thomas Zemen</a>, 
<a href="/search/cs?searchtype=author&query=Mecklenbr%C3%A4uker%2C+C+F">Christoph F. Mecklenbr&#xe4;uker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Future wireless multiple-input multiple-output (MIMO) communication systems
will employ sub-6 GHz and millimeter wave (mmWave) frequency bands working
cooperatively. Establishing a MIMO communication link usually relies on
estimating channel state information (CSI) which is difficult to acquire at
mmWave frequencies due to a low signal-to-noise ratio (SNR). In this paper, we
propose three novel methods to estimate mmWave MIMO channels using out-of-band
information obtained from the sub-6GHz band. We compare the proposed channel
estimation methods with a conventional one utilizing only in-band information.
Simulation results show that the proposed methods outperform the conventional
mmWave channel estimation method in terms of achievable spectral efficiency,
especially at low SNR and high K-factor.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08998" title="Abstract">arXiv:2311.08998</a> [<a href="/pdf/2311.08998" title="Download PDF">pdf</a>, <a href="/format/2311.08998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Low Rank Approach to Minimize Sensor-to-Actuator Communication in  Finite Horizon Output Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Aspeel%2C+A">Antoine Aspeel</a>, 
<a href="/search/eess?searchtype=author&query=Nylof%2C+J">Jakob Nylof</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J+S">Jing Shuang Li</a>, 
<a href="/search/eess?searchtype=author&query=Ozay%2C+N">Necmiye Ozay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Many modern controllers are composed of different components that communicate
in real-time over some network with limited resources. In this work, we are
interested in designing a controller that can be implemented with a minimum
number of sensor-to-actuator messages, while satisfying safety constraints over
a finite horizon. For finite horizon problems, a linear time-varying controller
with memory can be represented as a block-lower-triangular matrix. We show that
the rank of this matrix exactly captures the minimum number of messages needed
to be sent from the sensors to actuators to implement such a controller.
Moreover, we introduce a novel matrix factorization called causal factorization
that gives the required implementation. Finally, we show that the rank of the
controller is the same as the rank of the Youla parameter, enabling the Youla
parametrization (or analogous parametrizations) to be used to design the
controller, which reduces the overall design problem into a rank minimization
one over a convex set. Finally, convex relaxations for rank are used to
demonstrate that our approach leads to 20-50% less messages on a simulation
than a benchmark method.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08999" title="Abstract">arXiv:2311.08999</a> [<a href="/pdf/2311.08999" title="Download PDF">pdf</a>, <a href="/ps/2311.08999" title="Download PostScript">ps</a>, <a href="/format/2311.08999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging AI for Natural Disaster Management : Takeaways From The  Moroccan Earthquake
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hackathon%2C+M+S">Morocco Solidarity Hackathon</a> (Organizers, Speakers, Mentors and Participant teams)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The devastating 6.8-magnitude earthquake in Al Haouz, Morocco in 2023
prompted critical reflections on global disaster management strategies,
resulting in a post-disaster hackathon, using artificial intelligence (AI) to
improve disaster preparedness, response, and recovery. This paper provides (i)
a comprehensive literature review, (ii) an overview of winning projects, (iii)
key insights and challenges, namely real-time open-source data, data scarcity,
and interdisciplinary collaboration barriers, and (iv) a community-call for
further action.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09000" title="Abstract">arXiv:2311.09000</a> [<a href="/pdf/2311.09000" title="Download PDF">pdf</a>, <a href="/format/2311.09000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and  Correction of LLM Output
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+R+G">Revanth Gangi Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Mujahid%2C+Z+M">Zain Muhammad Mujahid</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Arnav Arora</a>, 
<a href="/search/cs?searchtype=author&query=Rubashevskii%2C+A">Aleksandr Rubashevskii</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+J">Jiahui Geng</a>, 
<a href="/search/cs?searchtype=author&query=Afzal%2C+O+M">Osama Mohammed Afzal</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Borenstein%2C+N">Nadav Borenstein</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+A">Aditya Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The increased use of large language models (LLMs) across a variety of
real-world applications calls for mechanisms to verify the factual accuracy of
their outputs. In this work, we present a holistic end-to-end solution for
annotating the factuality of LLM-generated responses, which encompasses a
multi-stage annotation scheme designed to yield detailed labels concerning the
verifiability and factual inconsistencies found in LLM outputs. We design and
build an annotation tool to speed up the labelling procedure and ease the
workload of raters. It allows flexible incorporation of automatic results in
any stage, e.g. automatically-retrieved evidence. We further construct an
open-domain document-level factuality benchmark in three-level granularity:
claim, sentence and document. Preliminary experiments show that FacTool,
FactScore and Perplexity.ai are struggling to identify false claims with the
best F1=0.53. Annotation tool, benchmark and code are available at
https://github.com/yuxiaw/Factcheck-GPT.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09004" title="Abstract">arXiv:2311.09004</a> [<a href="/pdf/2311.09004" title="Download PDF">pdf</a>, <a href="/format/2311.09004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incremental Object-Based Novelty Detection with Feedback Loop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caldarella%2C+S">Simone Caldarella</a>, 
<a href="/search/cs?searchtype=author&query=Ricci%2C+E">Elisa Ricci</a>, 
<a href="/search/cs?searchtype=author&query=Aljundi%2C+R">Rahaf Aljundi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Object-based Novelty Detection (ND) aims to identify unknown objects that do
not belong to classes seen during training by an object detection model. The
task is particularly crucial in real-world applications, as it allows to avoid
potentially harmful behaviours, e.g. as in the case of object detection models
adopted in a self-driving car or in an autonomous robot. Traditional approaches
to ND focus on one time offline post processing of the pretrained object
detection output, leaving no possibility to improve the model robustness after
training and discarding the abundant amount of out-of-distribution data
encountered during deployment.
<br />In this work, we propose a novel framework for object-based ND, assuming that
human feedback can be requested on the predicted output and later incorporated
to refine the ND model without negatively affecting the main object detection
performance. This refinement operation is repeated whenever new feedback is
available. To tackle this new formulation of the problem for object detection,
we propose a lightweight ND module attached on top of a pre-trained object
detection model, which is incrementally updated through a feedback loop. We
also propose a new benchmark to evaluate methods on this new setting and test
extensively our ND approach against baselines, showing increased robustness and
a successful incorporation of the received feedback.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09006" title="Abstract">arXiv:2311.09006</a> [<a href="/pdf/2311.09006" title="Download PDF">pdf</a>, <a href="/format/2311.09006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Similarity is Not Enough to Explain Language Model Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yauney%2C+G">Gregory Yauney</a>, 
<a href="/search/cs?searchtype=author&query=Reif%2C+E">Emily Reif</a>, 
<a href="/search/cs?searchtype=author&query=Mimno%2C+D">David Mimno</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models achieve high performance on many but not all downstream
tasks. The interaction between pretraining data and task data is commonly
assumed to determine this variance: a task with data that is more similar to a
model's pretraining data is assumed to be easier for that model. We test
whether distributional and example-specific similarity measures (embedding-,
token- and model-based) correlate with language model performance through a
large-scale comparison of the Pile and C4 pretraining datasets with downstream
benchmarks. Similarity correlates with performance for multilingual datasets,
but in other benchmarks, we surprisingly find that similarity metrics are not
correlated with accuracy or even each other. This suggests that the
relationship between pretraining data and downstream tasks is more complex than
often assumed.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09008" title="Abstract">arXiv:2311.09008</a> [<a href="/pdf/2311.09008" title="Download PDF">pdf</a>, <a href="/format/2311.09008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and  Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Libo Qin</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Wenbo Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+L">Lizi Liao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Min Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">End-to-end task-oriented dialogue (EToD) can directly generate responses in
an end-to-end fashion without modular training, which attracts escalating
popularity. The advancement of deep neural networks, especially the successful
use of large pre-trained models, has further led to significant progress in
EToD research in recent years. In this paper, we present a thorough review and
provide a unified perspective to summarize existing approaches as well as
recent trends to advance the development of EToD research. The contributions of
this paper can be summarized: (1) \textbf{\textit{First survey}}: to our
knowledge, we take the first step to present a thorough survey of this research
field; (2) \textbf{\textit{New taxonomy}}: we first introduce a unified
perspective for EToD, including (i) \textit{Modularly EToD} and (ii)
\textit{Fully EToD}; (3) \textbf{\textit{New Frontiers}}: we discuss some
potential frontier areas as well as the corresponding challenges, hoping to
spur breakthrough research in EToD field; (4) \textbf{\textit{Abundant
resources}}: we build a public website\footnote{We collect the related papers,
baseline projects, and leaderboards for the community at
\url{https://etods.net/}.}, where EToD researchers could directly access the
recent progress. We hope this work can serve as a thorough reference for the
EToD research community.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09011" title="Abstract">arXiv:2311.09011</a> [<a href="/pdf/2311.09011" title="Download PDF">pdf</a>, <a href="/format/2311.09011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmic Cheap Talk
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Babichenko%2C+Y">Yakov Babichenko</a>, 
<a href="/search/cs?searchtype=author&query=Talgam-Cohen%2C+I">Inbal Talgam-Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zabarnyi%2C+K">Konstantin Zabarnyi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">The literature on strategic communication originated with the influential
cheap talk model, which precedes the Bayesian persuasion model by three
decades. This model describes an interaction between two agents: sender and
receiver. The sender knows some state of the world which the receiver does not
know, and tries to influence the receiver's action by communicating a cheap
talk message to the receiver.
<br />This paper initiates the algorithmic study of cheap talk in a finite
environment (i.e., a finite number of states and receiver's possible actions).
We first prove that approximating the sender-optimal or the welfare-maximizing
cheap talk equilibrium up to a certain additive constant or multiplicative
factor is NP-hard. Fortunately, we identify three naturally-restricted cases
that admit efficient algorithms for finding a sender-optimal equilibrium. These
include a state-independent sender's utility structure, a constant number of
states or a receiver having only two actions.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09014" title="Abstract">arXiv:2311.09014</a> [<a href="/pdf/2311.09014" title="Download PDF">pdf</a>, <a href="/format/2311.09014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Attacks to Reward Machine-based Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nodari%2C+L">Lorenzo Nodari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Thesis Supervisor: Prof. Federico Cerutti (Universit\`a degli Studi di Brescia, IT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In recent years, Reward Machines (RMs) have stood out as a simple yet
effective automata-based formalism for exposing and exploiting task structure
in reinforcement learning settings. Despite their relevance, little to no
attention has been directed to the study of their security implications and
robustness to adversarial scenarios, likely due to their recent appearance in
the literature. With my thesis, I aim to provide the first analysis of the
security of RM-based reinforcement learning techniques, with the hope of
motivating further research in the field, and I propose and evaluate a novel
class of attacks on RM-based techniques: blinding attacks.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09016" title="Abstract">arXiv:2311.09016</a> [<a href="/pdf/2311.09016" title="Download PDF">pdf</a>, <a href="/ps/2311.09016" title="Download PostScript">ps</a>, <a href="/format/2311.09016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Chromatic Number of Kneser Hypergraphs via Consensus Division
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haviv%2C+I">Ishay Haviv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM); Algebraic Topology (math.AT); Combinatorics (math.CO)

</div>
<p class="mathjax">We show that the Consensus Division theorem implies lower bounds on the
chromatic number of Kneser hypergraphs, offering a novel proof for a result of
Alon, Frankl, and Lov\'{a}sz (Trans. Amer. Math. Soc., 1986) and for its
generalization by Kriz (Trans. Amer. Math. Soc., 1992). Our approach is applied
to study the computational complexity of the total search problem Kneser$^p$,
which given a succinct representation of a coloring of a $p$-uniform Kneser
hypergraph with fewer colors than its chromatic number, asks to find a
monochromatic hyperedge. We prove that for every prime $p$, the Kneser$^p$
problem with an extended access to the input coloring is efficiently reducible
to a quite weak approximation of the Consensus Division problem with $p$
shares. In particular, for $p=2$, the problem is efficiently reducible to any
non-trivial approximation of the Consensus Halving problem on normalized
monotone functions. We further show that for every prime $p$, the Kneser$^p$
problem lies in the complexity class $\mathsf{PPA}$-$p$. As an application, we
establish limitations on the complexity of the Kneser$^p$ problem, restricted
to colorings with a bounded number of colors.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09017" title="Abstract">arXiv:2311.09017</a> [<a href="/pdf/2311.09017" title="Download PDF">pdf</a>, <a href="/ps/2311.09017" title="Download PostScript">ps</a>, <a href="/format/2311.09017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semidefinite programs simulate approximate message passing robustly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ivkov%2C+M">Misha Ivkov</a>, 
<a href="/search/cs?searchtype=author&query=Schramm%2C+T">Tselil Schramm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">Approximate message passing (AMP) is a family of iterative algorithms that
generalize matrix power iteration. AMP algorithms are known to optimally solve
many average-case optimization problems. In this paper, we show that a large
class of AMP algorithms can be simulated in polynomial time by \emph{local
statistics hierarchy} semidefinite programs (SDPs), even when an unknown
principal minor of measure $1/\mathrm{polylog}(\mathrm{dimension})$ is
adversarially corrupted. Ours are the first robust guarantees for many of these
problems. Further, our results offer an interesting counterpoint to strong
lower bounds against less constrained SDP relaxations for average-case
max-cut-gain (a.k.a. "optimizing the Sherrington-Kirkpatrick Hamiltonian") and
other problems.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09018" title="Abstract">arXiv:2311.09018</a> [<a href="/pdf/2311.09018" title="Download PDF">pdf</a>, <a href="/ps/2311.09018" title="Download PostScript">ps</a>, <a href="/format/2311.09018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Foundation of Distributionally Robust Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+N">Nian Si</a>, 
<a href="/search/cs?searchtype=author&query=Blanchet%2C+J">Jose Blanchet</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhengyuan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Motivated by the need for a robust policy in the face of environment shifts
between training and the deployment, we contribute to the theoretical
foundation of distributionally robust reinforcement learning (DRRL). This is
accomplished through a comprehensive modeling framework centered around
distributionally robust Markov decision processes (DRMDPs). This framework
obliges the decision maker to choose an optimal policy under the worst-case
distributional shift orchestrated by an adversary. By unifying and extending
existing formulations, we rigorously construct DRMDPs that embraces various
modeling attributes for both the decision maker and the adversary. These
attributes include adaptability granularity, exploring history-dependent,
Markov, and Markov time-homogeneous decision maker and adversary dynamics.
Additionally, we delve into the flexibility of shifts induced by the adversary,
examining SA and S-rectangularity. Within this DRMDP framework, we investigate
conditions for the existence or absence of the dynamic programming principle
(DPP). From an algorithmic standpoint, the existence of DPP holds significant
implications, as the vast majority of existing data and computationally
efficiency RL algorithms are reliant on the DPP. To study its existence, we
comprehensively examine combinations of controller and adversary attributes,
providing streamlined proofs grounded in a unified methodology. We also offer
counterexamples for settings in which a DPP with full generality is absent.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09020" title="Abstract">arXiv:2311.09020</a> [<a href="/pdf/2311.09020" title="Download PDF">pdf</a>, <a href="/format/2311.09020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining Explanation: An Empirical Study on Explanation in Code  Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Widyasari%2C+R">Ratnadira Widyasari</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Ting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bouraffa%2C+A">Abir Bouraffa</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Code review is an important process for quality assurance in software
development. For an effective code review, the reviewers must explain their
feedback to enable the authors of the code change to act on them. However, the
explanation needs may differ among developers, who may require different types
of explanations. It is therefore crucial to understand what kind of
explanations reviewers usually use in code reviews. To the best of our
knowledge, no study published to date has analyzed the types of explanations
used in code review. In this study, we present the first analysis of
explanations in useful code reviews. We extracted a set of code reviews based
on their usefulness and labeled them based on whether they contained an
explanation, a solution, or both a proposed solution and an explanation
thereof.
<br />Based on our analysis, we found that a significant portion of the code review
comments (46%) only include solutions without providing an explanation. We
further investigated the remaining 54% of code review comments containing an
explanation and conducted an open card sorting to categorize the reviewers'
explanations. We distilled seven distinct categories of explanations based on
the expression forms developers used. Then, we utilize large language models,
specifically ChatGPT, to assist developers in getting a code review explanation
that suits their preferences. Specifically, we created prompts to transform a
code review explanation into a specific type of explanation. Our evaluation
results show that ChatGPT correctly generated the specified type of explanation
in 88/90 cases and that 89/90 of the cases have the correct explanation.
Overall, our study provides insights into the types of explanations that
developers use in code review and showcases how ChatGPT can be leveraged during
the code review process to generate a specific type of explanation.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09022" title="Abstract">arXiv:2311.09022</a> [<a href="/pdf/2311.09022" title="Download PDF">pdf</a>, <a href="/format/2311.09022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Potential of Large Language Models in Computational  Argumentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guizhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Liying Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+L+A">Luu Anh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Computational argumentation has become an essential tool in various fields,
including artificial intelligence, law, and public policy. It is an emerging
research field in natural language processing (NLP) that attracts increasing
attention. Research on computational argumentation mainly involves two types of
tasks: argument mining and argument generation. As large language models (LLMs)
have demonstrated strong abilities in understanding context and generating
natural language, it is worthwhile to evaluate the performance of LLMs on
various computational argumentation tasks. This work aims to embark on an
assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under
zero-shot and few-shot settings within the realm of computational
argumentation. We organize existing tasks into 6 main classes and standardise
the format of 14 open-sourced datasets. In addition, we present a new benchmark
dataset on counter speech generation, that aims to holistically evaluate the
end-to-end performance of LLMs on argument mining and argument generation.
Extensive experiments show that LLMs exhibit commendable performance across
most of these datasets, demonstrating their capabilities in the field of
argumentation. We also highlight the limitations in evaluating computational
argumentation and provide suggestions for future research directions in this
field.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09024" title="Abstract">arXiv:2311.09024</a> [<a href="/pdf/2311.09024" title="Download PDF">pdf</a>, <a href="/format/2311.09024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Certification of Vision-Language Models Using Incremental  Randomized Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nirala%2C+A+K">A K Nirala</a> (1), 
<a href="/search/cs?searchtype=author&query=Joshi%2C+A">A Joshi</a> (2), 
<a href="/search/cs?searchtype=author&query=Hegde%2C+C">C Hegde</a> (2), 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">S Sarkar</a> (1) ((1) Iowa State University, (2) New York University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A key benefit of deep vision-language models such as CLIP is that they enable
zero-shot open vocabulary classification; the user has the ability to define
novel class labels via natural language prompts at inference time. However,
while CLIP-based zero-shot classifiers have demonstrated competitive
performance across a range of domain shifts, they remain highly vulnerable to
adversarial attacks. Therefore, ensuring the robustness of such models is
crucial for their reliable deployment in the wild.
<br />In this work, we introduce Open Vocabulary Certification (OVC), a fast
certification method designed for open-vocabulary models like CLIP via
randomized smoothing techniques. Given a base "training" set of prompts and
their corresponding certified CLIP classifiers, OVC relies on the observation
that a classifier with a novel prompt can be viewed as a perturbed version of
nearby classifiers in the base training set. Therefore, OVC can rapidly certify
the novel classifier using a variation of incremental randomized smoothing. By
using a caching trick, we achieve approximately two orders of magnitude
acceleration in the certification process for novel prompts. To achieve further
(heuristic) speedups, OVC approximates the embedding space at a given input
using a multivariate normal distribution bypassing the need for sampling via
forward passes through the vision backbone. We demonstrate the effectiveness of
OVC on through experimental evaluation using multiple vision-language backbones
on the CIFAR-10 and ImageNet test datasets.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09027" title="Abstract">arXiv:2311.09027</a> [<a href="/pdf/2311.09027" title="Download PDF">pdf</a>, <a href="/format/2311.09027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Robustness of Intelligence-Driven Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nodari%2C+L">Lorenzo Nodari</a>, 
<a href="/search/cs?searchtype=author&query=Cerutti%2C+F">Federico Cerutti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at IEEE TechDefense 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Robustness to noise is of utmost importance in reinforcement learning
systems, particularly in military contexts where high stakes and uncertain
environments prevail. Noise and uncertainty are inherent features of military
operations, arising from factors such as incomplete information, adversarial
actions, or unpredictable battlefield conditions. In RL, noise can critically
impact decision-making, mission success, and the safety of personnel. Reward
machines offer a powerful tool to express complex reward structures in RL
tasks, enabling the design of tailored reinforcement signals that align with
mission objectives. This paper considers the problem of the robustness of
intelligence-driven reinforcement learning based on reward machines. The
preliminary results presented suggest the need for further research in
evidential reasoning and learning to harden current state-of-the-art
reinforcement learning approaches before being mission-critical-ready.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09028" title="Abstract">arXiv:2311.09028</a> [<a href="/pdf/2311.09028" title="Download PDF">pdf</a>, <a href="/format/2311.09028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Sensing, Communication, and Power Transfer: Multiuser  Beamforming Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Ziqin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Guangxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaibin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Shuguang Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In the sixth-generation (6G) networks, massive low-power devices are expected
to sense environment and deliver tremendous data. To enhance the radio resource
efficiency, the integrated sensing and communication (ISAC) technique exploits
the sensing and communication functionalities of signals, while the
simultaneous wireless information and power transfer (SWIPT) techniques
utilizes the same signals as the carriers for both information and power
delivery. The further combination of ISAC and SWIPT leads to the advanced
technology namely integrated sensing, communication, and power transfer
(ISCPT). In this paper, a multi-user multiple-input multiple-output (MIMO)
ISCPT system is considered, where a base station equipped with multiple
antennas transmits messages to multiple information receivers (IRs), transfers
power to multiple energy receivers (ERs), and senses a target simultaneously.
The sensing target can be regarded as a point or an extended surface. When the
locations of IRs and ERs are separated, the MIMO beamforming designs are
optimized to improve the sensing performance while meeting the communication
and power transfer requirements. The resultant non-convex optimization problems
are solved based on a series of techniques including Schur complement
transformation and rank reduction. Moreover, when the IRs and ERs are
co-located, the power splitting factors are jointly optimized together with the
beamformers to balance the performance of communication and power transfer. To
better understand the performance of ISCPT, the target positioning problem is
further investigated. Simulations are conducted to verify the effectiveness of
our proposed designs, which also reveal a performance tradeoff among sensing,
communication, and power transfer.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09029" title="Abstract">arXiv:2311.09029</a> [<a href="/pdf/2311.09029" title="Download PDF">pdf</a>, <a href="/format/2311.09029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Annotated 3D Geometric Learning for Smeared Points Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Miaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+D">Daniel Morris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is accepted at WACV2024(<a href="https://wacv2024.thecvf.com/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">There has been significant progress in improving the accuracy and quality of
consumer-level dense depth sensors. Nevertheless, there remains a common depth
pixel artifact which we call smeared points. These are points not on any 3D
surface and typically occur as interpolations between foreground and background
objects. As they cause fictitious surfaces, these points have the potential to
harm applications dependent on the depth maps. Statistical outlier removal
methods fare poorly in removing these points as they tend also to remove actual
surface points. Trained network-based point removal faces difficulty in
obtaining sufficient annotated data. To address this, we propose a fully
self-annotated method to train a smeared point removal classifier. Our approach
relies on gathering 3D geometric evidence from multiple perspectives to
automatically detect and annotate smeared points and valid points. To validate
the effectiveness of our method, we present a new benchmark dataset: the Real
Azure-Kinect dataset. Experimental results and ablation studies show that our
method outperforms traditional filters and other self-annotated methods. Our
work is publicly available at
https://github.com/wangmiaowei/wacv2024_smearedremover.git.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09031" title="Abstract">arXiv:2311.09031</a> [<a href="/pdf/2311.09031" title="Download PDF">pdf</a>, <a href="/format/2311.09031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Sensing, Communication, and Power Transfer: From Theory to  Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zidong Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Guangxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuanming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaibin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">To support the development of internet-of-things applications, an enormous
population of low-power devices are expected to be incorporated in wireless
networks performing sensing and communication tasks. As a key technology for
improving the data collection efficiency, integrated sensing and communication
(ISAC) enables simultaneous data transmission and radar sensing by reusing the
same radio signals. In addition to information carriers, wireless signals can
also serve as energy delivers, which enables simultaneous wireless information
and power transfer (SWIPT). To improve the energy and spectrum efficiency, the
advantages of ISAC and SWIPT are expected to be exploited, leading to the
emerging technology of integrating sensing, communication, and power transfer
(ISCPT). In this article, a timely overview of ISCPT is provided with the
description of the fundamentals, the characterization of the theoretical
boundary, the discussion on the key technologies, and the demonstration of the
implementation platform.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09032" title="Abstract">arXiv:2311.09032</a> [<a href="/pdf/2311.09032" title="Download PDF">pdf</a>, <a href="/format/2311.09032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nahida: In-Band Distributed Tracing with eBPF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wanqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pengfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huxing Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
<p class="mathjax">Microservices are commonly used in modern cloud-native applications to
achieve agility. However, the complexity of service dependencies in large-scale
microservices systems can lead to anomaly propagation, making fault
troubleshooting a challenge. To address this issue, distributed tracing systems
have been proposed to trace complete request execution paths, enabling
developers to troubleshoot anomalous services. However, existing distributed
tracing systems have limitations such as invasive instrumentation, trace loss,
or inaccurate trace correlation. To overcome these limitations, we propose a
new tracing system based on eBPF (extended Berkeley Packet Filter), named
Nahida, that can track complete requests in the kernel without intrusion,
regardless of programming language or implementation. Our evaluation results
show that Nahida can track over 92% of requests with stable accuracy, even
under the high concurrency of user requests, while the state-of-the-art
non-invasive approaches can not track any of the requests. Importantly, Nahida
can track requests served by a multi-threaded application that none of the
existing invasive tracing systems can handle by instrumenting tracing codes
into libraries. Moreover, the overhead introduced by Nahida is negligible,
increasing service latency by only 1.55%-2.1%. Overall, Nahida provides an
effective and non-invasive solution for distributed tracing.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09033" title="Abstract">arXiv:2311.09033</a> [<a href="/pdf/2311.09033" title="Download PDF">pdf</a>, <a href="/format/2311.09033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MELA: Multilingual Evaluation of Linguistic Acceptability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziyin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yikang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weifang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Junyu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hai Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent benchmarks for Large Language Models (LLMs) have mostly focused on
application-driven tasks such as complex reasoning and code generation, and
this has led to a scarcity in purely linguistic evaluation of LLMs. Against
this background, we introduce Multilingual Evaluation of Linguistic
Acceptability -- MELA, the first multilingual benchmark on linguistic
acceptability with 48K samples covering 10 languages from a diverse set of
language families. We establish baselines of commonly used LLMs along with
supervised models, and conduct cross-lingual transfer and multi-task learning
experiments with XLM-R. In pursuit of multilingual interpretability, we analyze
the weights of fine-tuned XLM-R to explore the possibility of identifying
transfer difficulty between languages. Our results show that ChatGPT benefits
much from in-context examples but still lags behind fine-tuned XLM-R, while the
performance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting.
Cross-lingual and multi-task learning experiments show that unlike semantic
tasks, in-language training data is crucial in acceptability judgements.
Results in layerwise probing indicate that the upper layers of XLM-R become a
task-specific but language-agnostic region for multilingual acceptability
judgment. We also introduce the concept of conflicting weight, which could be a
potential indicator for the difficulty of cross-lingual transfer between
languages. Our data will be available at https://github.com/sjtu-compling/MELA.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09040" title="Abstract">arXiv:2311.09040</a> [<a href="/pdf/2311.09040" title="Download PDF">pdf</a>, <a href="/format/2311.09040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Health Video Consumption Behaviors on Social Media: Activities,  Challenges, and Characteristics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 8 figures, submitted to Computer Supported Collaborative Work (CSCW 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Many people now watch health videos, such as diet, exercise, mental health,
COVID-19, and chronic disease videos, on social media. Most existing studies
focused on video creators, leaving the motivations and practices of viewers
underexplored. We interviewed 18 participants, surveyed 121 respondents, and
derived a model characterizing consumers' video consumption practices on social
media. The practices include five main activities: deciding to watch videos
driven by various motivations, accessing videos on social media through a
socio-technical ecosystem, watching videos to meet informational, emotional,
and entertainment needs, evaluating the credibility and interestingness of
videos, and using videos to achieve health goals. The five activities do not
necessarily proceed in a linear fashion; rather, their arrangement is
situational, depending on individuals' motivations and their social and
technological environments. We further identified challenges that consumers
face while consuming health videos on social media and discussed design
implications and directions for future research.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09047" title="Abstract">arXiv:2311.09047</a> [<a href="/pdf/2311.09047" title="Download PDF">pdf</a>, <a href="/format/2311.09047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 6G Non-Terrestrial Networks Enabled Low-Altitude Economy: Opportunities  and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yihang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Guangxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jing Deng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Q">Qingjiang Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The unprecedented development of non-terrestrial networks (NTN) utilizes the
low-altitude airspace for commercial and social flying activities. The
integration of NTN and terres- trial networks leads to the emergence of
low-altitude economy (LAE). A series of LAE application scenarios are enabled
by the sensing, communication, and transportation functionalities of the
aircrafts. The prerequisite technologies supporting LAE are introduced in this
paper, including the network coverage and aircrafts detection. The LAE
functionalities assisted by aircrafts with respect to sensing and communication
are then summarized, including the terrestrial and non-terrestrial targets
sensing, ubiquitous coverage, relaying, and traffic offloading. Finally,
several future directions are identified, including aircrafts collaboration,
energy efficiency, and artificial intelligence enabled LAE.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09048" title="Abstract">arXiv:2311.09048</a> [<a href="/pdf/2311.09048" title="Download PDF">pdf</a>, <a href="/format/2311.09048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRASP: A novel benchmark for evaluating language GRounding And Situated  Physics understanding in multimodal language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jassim%2C+S">Serwan Jassim</a>, 
<a href="/search/cs?searchtype=author&query=Holubar%2C+M">Mario Holubar</a>, 
<a href="/search/cs?searchtype=author&query=Richter%2C+A">Annika Richter</a>, 
<a href="/search/cs?searchtype=author&query=Wolff%2C+C">Cornelius Wolff</a>, 
<a href="/search/cs?searchtype=author&query=Ohmer%2C+X">Xenia Ohmer</a>, 
<a href="/search/cs?searchtype=author&query=Bruni%2C+E">Elia Bruni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper presents GRASP, a novel benchmark to evaluate the language
grounding and physical understanding capabilities of video-based multimodal
large language models (LLMs). This evaluation is accomplished via a two-tier
approach leveraging Unity simulations. The initial level tests for language
grounding by assessing a model's ability to relate simple textual descriptions
with visual information. The second level evaluates the model's understanding
of 'Intuitive Physics' principles, such as object permanence and continuity. In
addition to releasing the benchmark, we use it to evaluate several
state-of-the-art multimodal LLMs. Our evaluation reveals significant
shortcomings in current models' language grounding and intuitive physics. These
identified limitations underline the importance of benchmarks like GRASP to
monitor the progress of future models in developing these competencies.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09049" title="Abstract">arXiv:2311.09049</a> [<a href="/pdf/2311.09049" title="Download PDF">pdf</a>, <a href="/format/2311.09049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Large Language Models by Integrating Collaborative Semantics  for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Bowen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yupeng Hou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hongyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recently, large language models (LLMs) have shown great potential in
recommender systems, either improving existing recommendation models or serving
as the backbone. However, there exists a large semantic gap between LLMs and
recommender systems, since items to be recommended are often indexed by
discrete identifiers (item ID) out of the LLM's vocabulary. In essence, LLMs
capture language semantics while recommender systems imply collaborative
semantics, making it difficult to sufficiently leverage the model capacity of
LLMs for recommendation. To address this challenge, in this paper, we propose a
new LLM-based recommendation model called LC-Rec, which can better integrate
language and collaborative semantics for recommender systems. Our approach can
directly generate items from the entire item set for recommendation, without
relying on candidate items. Specifically, we make two major contributions in
our approach. For item indexing, we design a learning-based vector quantization
method with uniform semantic mapping, which can assign meaningful and
non-conflicting IDs (called item indices) for items. For alignment tuning, we
propose a series of specially designed tuning tasks to enhance the integration
of collaborative semantics in LLMs. Our fine-tuning tasks enforce LLMs to
deeply integrate language and collaborative semantics (characterized by the
learned item indices), so as to achieve an effective adaptation to recommender
systems. Extensive experiments demonstrate the effectiveness of our method,
showing that our approach can outperform a number of competitive baselines
including traditional recommenders and existing LLM-based recommenders. Our
code is available at https://github.com/RUCAIBox/LC-Rec/.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09050" title="Abstract">arXiv:2311.09050</a> [<a href="/pdf/2311.09050" title="Download PDF">pdf</a>, <a href="/format/2311.09050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Zero-shot Visual Question Answering via Large Language Models  with Reasoning Question Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+W">Wei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weining Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Zero-shot Visual Question Answering (VQA) is a prominent vision-language task
that examines both the visual and textual understanding capability of systems
in the absence of training data. Recently, by converting the images into
captions, information across multi-modalities is bridged and Large Language
Models (LLMs) can apply their strong zero-shot generalization capability to
unseen questions. To design ideal prompts for solving VQA via LLMs, several
studies have explored different strategies to select or generate
question-answer pairs as the exemplar prompts, which guide LLMs to answer the
current questions effectively. However, they totally ignore the role of
question prompts. The original questions in VQA tasks usually encounter
ellipses and ambiguity which require intermediate reasoning. To this end, we
present Reasoning Question Prompts for VQA tasks, which can further activate
the potential of LLMs in zero-shot scenarios. Specifically, for each question,
we first generate self-contained questions as reasoning question prompts via an
unsupervised question edition module considering sentence fluency, semantic
integrity and syntactic invariance. Each reasoning question prompt clearly
indicates the intent of the original question. This results in a set of
candidate answers. Then, the candidate answers associated with their confidence
scores acting as answer heuristics are fed into LLMs and produce the final
answer. We evaluate reasoning question prompts on three VQA challenges,
experimental results demonstrate that they can significantly improve the
results of LLMs on zero-shot setting and outperform existing state-of-the-art
zero-shot methods on three out of four data sets. Our source code is publicly
released at \url{https://github.com/ECNU-DASE-NLP/RQP}.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09051" title="Abstract">arXiv:2311.09051</a> [<a href="/pdf/2311.09051" title="Download PDF">pdf</a>, <a href="/format/2311.09051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional Finite Element curl\,div Complexes and Application to  Quad Curl Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+X">Xuehai Huang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The paper addresses the challenge of constructing conforming finite element
spaces for high-order differential operators in high dimensions, with a focus
on the $\textrm{curl\,div}$ operator in three dimensions. Tangential-normal
continuity is introduced in order to develop distributional finite element
$\textrm{curl\,div}$ complexes. The spaces constructed are applied to
discretize a quad curl problem, demonstrating optimal order of convergence.
Furthermore, a hybridization technique is proposed, demonstrating its
equivalence to nonconforming finite elements and weak Galerkin methods.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09052" title="Abstract">arXiv:2311.09052</a> [<a href="/pdf/2311.09052" title="Download PDF">pdf</a>, <a href="/format/2311.09052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network-Level Integrated Sensing and Communication: Interference  Management and BS Coordination Using Stochastic Geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+K">Kaitao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Masouros%2C+C">Christos Masouros</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangji Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures. This work has been submitted to the IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this work, we study integrated sensing and communication (ISAC) networks
with the aim of effectively balancing sensing and communication (S&amp;C)
performance at the network level. Focusing on monostatic sensing, the tool of
stochastic geometry is exploited to capture the S&amp;C performance, which
facilitates us to illuminate key cooperative dependencies in the ISAC network
and optimize key network-level parameters. Based on the derived tractable
expression of area spectral efficiency (ASE), we formulate the optimization
problem to maximize the network performance from the view point of two joint
S&amp;C metrics. Towards this end, we further jointly optimize the cooperative BS
cluster sizes for S&amp;C and the serving/probing numbers of users/targets to
achieve a flexible tradeoff between S&amp;C at the network level. It is verified
that interference nulling can effectively improve the average data rate and
radar information rate. Surprisingly, the optimal communication tradeoff for
the case of the ASE maximization tends to employ all spacial resources towards
multiplexing and diversity gain, without interference nulling. By contrast, for
the sensing objectives, resource allocation tends to eliminate certain
interference especially when the antenna resources are sufficient, because the
inter-cell interference becomes a more dominant factor affecting sensing
performance. Furthermore, we prove that the ratio of the optimal number of
users and the number of transmit antennas is a constant value when the
communication performance is optimal. Simulation results demonstrate that the
proposed cooperative ISAC scheme achieves a substantial gain in S&amp;C performance
at the network level.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09053" title="Abstract">arXiv:2311.09053</a> [<a href="/pdf/2311.09053" title="Download PDF">pdf</a>, <a href="/format/2311.09053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Knowledge Editing in Language Models via Relation Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yifan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaoyan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huanhuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+F">Fangyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+Y">Yixuan Weng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+R">Ran Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Knowledge Editing (KE) for modifying factual knowledge in Large Language
Models (LLMs) has been receiving increasing attention. However, existing
knowledge editing methods are entity-centric, and it is unclear whether this
approach is suitable for a relation-centric perspective. To address this gap,
this paper constructs a new benchmark named RaKE, which focuses on Relation
based Knowledge Editing. In this paper, we establish a suite of innovative
metrics for evaluation and conduct comprehensive experiments involving various
knowledge editing baselines. We notice that existing knowledge editing methods
exhibit the potential difficulty in their ability to edit relations. Therefore,
we further explore the role of relations in factual triplets within the
transformer. Our research results confirm that knowledge related to relations
is not only stored in the FFN network but also in the attention layers. This
provides experimental support for future relation-based knowledge editing
methods.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09056" title="Abstract">arXiv:2311.09056</a> [<a href="/pdf/2311.09056" title="Download PDF">pdf</a>, <a href="/format/2311.09056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Range-Visual-Inertial Sensor Fusion for Micro Aerial Vehicle  Localization and Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goudar%2C+A">Abhishek Goudar</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenda Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Schoellig%2C+A+P">Angela P. Schoellig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We propose a fixed-lag smoother-based sensor fusion architecture to leverage
the complementary benefits of range-based sensors and visual-inertial odometry
(VIO) for localization. We use two fixed-lag smoothers (FLS) to decouple
accurate state estimation and high-rate pose generation for closed-loop
control. The first FLS combines ultrawideband (UWB)-based range measurements
and VIO to estimate the robot trajectory and any systematic biases that affect
the range measurements in cluttered environments. The second FLS estimates
smooth corrections to VIO to generate pose estimates at a high rate for online
control. The proposed method is lightweight and can run on a computationally
constrained micro-aerial vehicle (MAV). We validate our approach through
closed-loop flight tests involving dynamic trajectories in multiple real-world
cluttered indoor environments. Our method achieves
decimeter-to-sub-decimeter-level positioning accuracy using off-the-shelf
sensors and decimeter-level tracking accuracy with minimally-tuned open-source
controllers.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09058" title="Abstract">arXiv:2311.09058</a> [<a href="/pdf/2311.09058" title="Download PDF">pdf</a>, <a href="/format/2311.09058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Horizons in Parameter Regularization: A Constraint Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franke%2C+J+K+H">J&#xf6;rg K.H. Franke</a>, 
<a href="/search/cs?searchtype=author&query=Hefenbrock%2C+M">Michael Hefenbrock</a>, 
<a href="/search/cs?searchtype=author&query=Koehler%2C+G">Gregor Koehler</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This work presents constrained parameter regularization (CPR), an alternative
to traditional weight decay. Instead of applying a constant penalty uniformly
to all parameters, we enforce an upper bound on a statistical measure (e.g.,
the L$_2$-norm) of individual parameter groups. This reformulates learning as a
constrained optimization problem. To solve this, we utilize an adaptation of
the augmented Lagrangian method. Our approach allows for varying regularization
strengths across different parameter groups, removing the need for explicit
penalty coefficients in the regularization terms. CPR only requires two
hyperparameters and introduces no measurable runtime overhead. We offer
empirical evidence of CPR's effectiveness through experiments in the "grokking"
phenomenon, image classification, and language modeling. Our findings show that
CPR can counteract the effects of grokking, and it consistently matches or
surpasses the performance of traditional weight decay.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09060" title="Abstract">arXiv:2311.09060</a> [<a href="/pdf/2311.09060" title="Download PDF">pdf</a>, <a href="/format/2311.09060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Localization Methods Actually Localize Memorized Data in LLMs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+T">Ting-Yun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Thomason%2C+J">Jesse Thomason</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) can memorize many pretrained sequences verbatim.
This paper studies if we can locate a small set of neurons in LLMs responsible
for memorizing a given sequence. While the concept of localization is often
mentioned in prior work, methods for localization have never been
systematically and directly evaluated; we address this with two benchmarking
approaches. In our INJ Benchmark, we actively inject a piece of new information
into a small subset of LLM weights and measure whether localization methods can
identify these "ground truth" weights. In the DEL Benchmark, we study
localization of pretrained data that LLMs have already memorized; while this
setting lacks ground truth, we can still evaluate localization by measuring
whether dropping out located neurons erases a memorized sequence from the
model. We evaluate five localization methods on our two benchmarks, and both
show similar rankings. All methods exhibit promising localization ability,
especially for pruning-based methods, though the neurons they identify are not
necessarily specific to a single memorized sequence.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09061" title="Abstract">arXiv:2311.09061</a> [<a href="/pdf/2311.09061" title="Download PDF">pdf</a>, <a href="/format/2311.09061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic cable harness layout routing in a customizable 3D environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karlsson%2C+T">T. Karlsson</a>, 
<a href="/search/cs?searchtype=author&query=%C3%85blad%2C+E">E. &#xc5;blad</a>, 
<a href="/search/cs?searchtype=author&query=Hermansson%2C+T">T. Hermansson</a>, 
<a href="/search/cs?searchtype=author&query=Carlson%2C+J+S">J. S. Carlson</a>, 
<a href="/search/cs?searchtype=author&query=Tenf%C3%A4lt%2C+G">G. Tenf&#xe4;lt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, submitted for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Designing cable harnesses can be time-consuming and complex due to many
design and manufacturing aspects and rules. Automating the design process can
help to fulfil these rules, speed up the process, and optimize the design. To
accommodate this, we formulate a harness routing optimization problem to
minimize cable lengths, maximize bundling by rewarding shared paths, and
optimize the cables' spatial location with respect to case-specific information
of the routing environment, e.g., zones to avoid. A deterministic and
computationally effective cable harness routing algorithm has been developed to
solve the routing problem and is used to generate a set of cable harness
topology candidates and approximate the Pareto front. Our approach was tested
against a stochastic and an exact solver and our routing algorithm generated
objective function values better than the stochastic approach and close to the
exact solver. Our algorithm was able to find solutions, some of them being
proven to be near-optimal, for three industrial-sized 3D cases within
reasonable time (in magnitude of seconds to minutes) and the computation times
were comparable to those of the stochastic approach.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09062" title="Abstract">arXiv:2311.09062</a> [<a href="/pdf/2311.09062" title="Download PDF">pdf</a>, <a href="/ps/2311.09062" title="Download PostScript">ps</a>, <a href="/format/2311.09062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brain Functional Connectivity under Teleoperation Latency: a fNIRS Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Vann%2C+W">William Vann</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jing Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Human Factors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Objective: This study aims to understand the cognitive impact of latency in
teleoperation and the related mitigation methods, using functional
Near-Infrared Spectroscopy (fNIRS) to analyze functional connectivity.
Background: Latency between command, execution, and feedback in teleoperation
can impair performance and affect operators mental state. The neural
underpinnings of these effects are not well understood. Method: A human subject
experiment (n = 41) of a simulated remote robot manipulation task was
performed. Three conditions were tested: no latency, with visual and haptic
latency, with visual latency and no haptic latency. fNIRS and performance data
were recorded and analyzed. Results: The presence of latency in teleoperation
significantly increased functional connectivity within and between prefrontal
and motor cortexes. Maintaining visual latency while providing real-time haptic
feedback reduced the average functional connectivity in all cortical networks
and showed a significantly different connectivity ratio within prefrontal and
motor cortical networks. The performance results showed the worst performance
in the all-delayed condition and best performance in no latency condition,
which echoes the neural activity patterns. Conclusion: The study provides
neurological evidence that latency in teleoperation increases cognitive load,
anxiety, and challenges in motion planning and control. Real-time haptic
feedback, however, positively influences neural pathways related to cognition,
decision-making, and sensorimotor processes. Application: This research can
inform the design of ergonomic teleoperation systems that mitigate the effects
of latency.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09064" title="Abstract">arXiv:2311.09064</a> [<a href="/pdf/2311.09064" title="Download PDF">pdf</a>, <a href="/format/2311.09064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imagine the Unseen World: A Benchmark for Systematic Generalization in  Visual World Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yeongbin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gautam Singh</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Junyeong Park</a>, 
<a href="/search/cs?searchtype=author&query=Gulcehre%2C+C">Caglar Gulcehre</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungjin Ahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NeurIPS 2023. The first two authors contributed equally. To download the benchmark, visit <a href="https://systematic-visual-imagination.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Systematic compositionality, or the ability to adapt to novel situations by
creating a mental model of the world using reusable pieces of knowledge,
remains a significant challenge in machine learning. While there has been
considerable progress in the language domain, efforts towards systematic visual
imagination, or envisioning the dynamical implications of a visual observation,
are in their infancy. We introduce the Systematic Visual Imagination Benchmark
(SVIB), the first benchmark designed to address this problem head-on. SVIB
offers a novel framework for a minimal world modeling problem, where models are
evaluated based on their ability to generate one-step image-to-image
transformations under a latent world dynamics. The framework provides benefits
such as the possibility to jointly optimize for systematic perception and
imagination, a range of difficulty levels, and the ability to control the
fraction of possible factor combinations used during training. We provide a
comprehensive evaluation of various baseline models on SVIB, offering insight
into the current state-of-the-art in systematic visual imagination. We hope
that this benchmark will help advance visual systematic compositionality.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09066" title="Abstract">arXiv:2311.09066</a> [<a href="/pdf/2311.09066" title="Download PDF">pdf</a>, <a href="/format/2311.09066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Self-Disclosures of Use, Misuse and Addiction in  Community-based Social Media Posts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenghao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+T">Tuhin Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Hochstatter%2C+K+R">Karli R Hochstatter</a>, 
<a href="/search/cs?searchtype=author&query=Slavin%2C+M+N">Melissa N Slavin</a>, 
<a href="/search/cs?searchtype=author&query=El-Bassel%2C+N">Nabila El-Bassel</a>, 
<a href="/search/cs?searchtype=author&query=Muresan%2C+S">Smaranda Muresan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the last decade, the United States has lost more than 500,000 people from
an overdose involving prescription and illicit opioids
(https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a national
public health emergency (USDHHS, 2017). To more effectively prevent
unintentional opioid overdoses, medical practitioners require robust and timely
tools that can effectively identify at-risk patients. Community-based social
media platforms such as Reddit allow self-disclosure for users to discuss
otherwise sensitive drug-related behaviors, often acting as indicators for
opioid use disorder. Towards this, we present a moderate size corpus of 2500
opioid-related posts from various subreddits spanning 6 different phases of
opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For
every post, we annotate span-level extractive explanations and crucially study
their role both in annotation quality and model development. We evaluate
several state-of-the-art models in a supervised, few-shot, or zero-shot
setting. Experimental results and error analysis show that identifying the
phases of opioid use disorder is highly contextual and challenging. However, we
find that using explanations during modeling leads to a significant boost in
classification accuracy demonstrating their beneficial role in a high-stakes
domain such as studying the opioid use disorder continuum. The dataset will be
made available for research on Github in the formal version.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09068" title="Abstract">arXiv:2311.09068</a> [<a href="/pdf/2311.09068" title="Download PDF">pdf</a>, <a href="/format/2311.09068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Fair Division from Bandit Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamada%2C+H">Hakuei Yamada</a>, 
<a href="/search/cs?searchtype=author&query=Komiyama%2C+J">Junpei Komiyama</a>, 
<a href="/search/cs?searchtype=author&query=Abe%2C+K">Kenshi Abe</a>, 
<a href="/search/cs?searchtype=author&query=Iwasaki%2C+A">Atsushi Iwasaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">This work addresses learning online fair division under uncertainty, where a
central planner sequentially allocates items without precise knowledge of
agents' values or utilities. Departing from conventional online algorithm, the
planner here relies on noisy, estimated values obtained after allocating items.
We introduce wrapper algorithms utilizing \textit{dual averaging}, enabling
gradual learning of both the type distribution of arriving items and agents'
values through bandit feedback. This approach enables the algorithms to
asymptotically achieve optimal Nash social welfare in linear Fisher markets
with agents having additive utilities. We establish regret bounds in Nash
social welfare and empirically validate the superior performance of our
proposed algorithms across synthetic and empirical datasets.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09069" title="Abstract">arXiv:2311.09069</a> [<a href="/pdf/2311.09069" title="Download PDF">pdf</a>, <a href="/format/2311.09069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Well Do Large Language Models Truly Ground?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunji Lee</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+S">Sejune Joo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Chaeeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Joel Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Doyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=On%2C+K">Kyoung-Woon On</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reliance on the inherent knowledge of Large Language Models (LLMs) can cause
issues such as hallucinations, lack of control, and difficulties in integrating
variable knowledge. To mitigate this, LLMs can be probed to generate responses
by grounding on external context, often given as input (knowledge-augmented
models). Yet, previous research is often confined to a narrow view of the term
"grounding", often only focusing on whether the response contains the correct
answer or not, which does not ensure the reliability of the entire response. To
address this limitation, we introduce a strict definition of grounding: a model
is considered truly grounded when its responses (1) fully utilize necessary
knowledge from the provided context, and (2) don't exceed the knowledge within
the contexts. We introduce a new dataset and a grounding metric to assess this
new definition and perform experiments across 13 LLMs of different sizes and
training methods to provide insights into the factors that influence grounding
performance. Our findings contribute to a better understanding of how to
improve grounding capabilities and suggest an area of improvement toward more
reliable and controllable LLM applications.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09071" title="Abstract">arXiv:2311.09071</a> [<a href="/pdf/2311.09071" title="Download PDF">pdf</a>, <a href="/format/2311.09071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Multilingual is Multilingual LLM?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+F">Fei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs), trained predominantly on extensive English
data, often exhibit limitations when applied to other languages. Current
research is primarily focused on enhancing the multilingual capabilities of
these models by employing various tuning strategies. Despite their
effectiveness in certain languages, the understanding of the multilingual
abilities of LLMs remains incomplete. This study endeavors to evaluate the
multilingual capacity of LLMs by conducting an exhaustive analysis across 101
languages, and classifies languages with similar characteristics into four
distinct quadrants. By delving into each quadrant, we shed light on the
rationale behind their categorization and offer actionable guidelines for
tuning these languages. Extensive experiments reveal that existing LLMs possess
multilingual capabilities that surpass our expectations, and we can
significantly improve the multilingual performance of LLMs by focusing on these
distinct attributes present in each quadrant.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09075" title="Abstract">arXiv:2311.09075</a> [<a href="/pdf/2311.09075" title="Download PDF">pdf</a>, <a href="/format/2311.09075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-stabilizing Byzantine Multivalued Consensus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duvignau%2C+R">Romaric Duvignau</a>, 
<a href="/search/cs?searchtype=author&query=Raynal%2C+M">Michel Raynal</a>, 
<a href="/search/cs?searchtype=author&query=Schiller%2C+E+M">Elad Michael Schiller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2110.08592">arXiv:2110.08592</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Consensus, abstracting a myriad of problems in which processes have to agree
on a single value, is one of the most celebrated problems of fault-tolerant
distributed computing. Consensus applications include fundamental services for
the environments of the Cloud and Blockchain, and in such challenging
environments, malicious behaviors are often modeled as adversarial Byzantine
faults.
<br />At OPODIS 2010, Mostefaoui and Raynal (in short MR) presented a
Byzantine-tolerant solution to consensus in which the decided value cannot be a
value proposed only by Byzantine processes. MR has optimal resilience coping
with up to t &lt; n/3 Byzantine nodes over n processes. MR provides this
multivalued consensus object (which accepts proposals taken from a finite set
of values) assuming the availability of a single Binary consensus object (which
accepts proposals taken from the set {0,1}).
<br />This work, which focuses on multivalued consensus, aims at the design of an
even more robust solution than MR. Our proposal expands MR's fault-model with
self-stabilization, a vigorous notion of fault-tolerance. In addition to
tolerating Byzantine, self-stabilizing systems can automatically recover after
the occurrence of arbitrary transient-faults. These faults represent any
violation of the assumptions according to which the system was designed to
operate (provided that the algorithm code remains intact).
<br />To the best of our knowledge, we propose the first self-stabilizing solution
for intrusion-tolerant multivalued consensus for asynchronous message-passing
systems prone to Byzantine failures. Our solution has a O(t) stabilization time
from arbitrary transient faults.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09077" title="Abstract">arXiv:2311.09077</a> [<a href="/pdf/2311.09077" title="Download PDF">pdf</a>, <a href="/format/2311.09077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking NeRF: Representing the Real-World Geometry by a Discontinuous  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+Z">Zhanfeng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gang Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A crucial reason for the success of existing NeRF-based methods is to build a
neural density field for the geometry representation via multiple perceptron
layers (MLPs). MLPs are continuous functions, however, real geometry or density
field is frequently discontinuous at the interface between the air and the
surface. Such a contrary brings the problem of unfaithful geometry
representation. To this end, this paper proposes spiking NeRF, which leverages
spiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural
Network (SNN) framework to build a discontinuous density field for faithful
geometry representation. Specifically, we first demonstrate the reason why
continuous density fields will bring inaccuracy. Then, we propose to use the
spiking neurons to build a discontinuous density field. We conduct
comprehensive analysis for the problem of existing spiking neuron models and
then provide the numerical relationship between the parameter of spiking neuron
and the theoretical accuracy of geometry, Based on this, we propose a bounded
spiking neuron to build the discontinuous density field. Our results achieve
SOTA performance. Our code and data will be released to the public.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09084" title="Abstract">arXiv:2311.09084</a> [<a href="/pdf/2311.09084" title="Download PDF">pdf</a>, <a href="/format/2311.09084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Transformer Learning with Proximity Data Generation for  Text-Based Person Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hefeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhibin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianshui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Liang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE T-CSVT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Given a descriptive text query, text-based person search (TBPS) aims to
retrieve the best-matched target person from an image gallery. Such a
cross-modal retrieval task is quite challenging due to significant modality
gap, fine-grained differences and insufficiency of annotated data. To better
align the two modalities, most existing works focus on introducing
sophisticated network structures and auxiliary tasks, which are complex and
hard to implement. In this paper, we propose a simple yet effective dual
Transformer model for text-based person search. By exploiting a hardness-aware
contrastive learning strategy, our model achieves state-of-the-art performance
without any special design for local feature alignment or side information.
Moreover, we propose a proximity data generation (PDG) module to automatically
produce more diverse data for cross-modal training. The PDG module first
introduces an automatic generation algorithm based on a text-to-image diffusion
model, which generates new text-image pair samples in the proximity space of
original ones. Then it combines approximate text generation and feature-level
mixup during training to further strengthen the data diversity. The PDG module
can largely guarantee the reasonability of the generated samples that are
directly used for training without any human inspection for noise rejection. It
improves the performance of our model significantly, providing a feasible
solution to the data insufficiency problem faced by such fine-grained
visual-linguistic tasks. Extensive experiments on two popular datasets of the
TBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach
outperforms state-of-the-art approaches evidently, e.g., improving by 3.88%,
4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be
available at https://github.com/HCPLab-SYSU/PersonSearch-CTLG
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09086" title="Abstract">arXiv:2311.09086</a> [<a href="/pdf/2311.09086" title="Download PDF">pdf</a>, <a href="/format/2311.09086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Uli Dataset: An Exercise in Experience Led Annotation of oGBV
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Arnav Arora</a>, 
<a href="/search/cs?searchtype=author&query=Jinadoss%2C+M">Maha Jinadoss</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+C">Cheshta Arora</a>, 
<a href="/search/cs?searchtype=author&query=George%2C+D">Denny George</a>, 
<a href="/search/cs?searchtype=author&query=Brindaalakshmi">Brindaalakshmi</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+H+D">Haseena Dawood Khan</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+K">Kirti Rawat</a>, Div, 
<a href="/search/cs?searchtype=author&query=Ritash">Ritash</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+S">Seema Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+S">Shivani Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Shora%2C+S+R">Shehla Rashid Shora</a>, 
<a href="/search/cs?searchtype=author&query=Raut%2C+R">Rie Raut</a>, 
<a href="/search/cs?searchtype=author&query=Pawar%2C+S">Sumit Pawar</a>, 
<a href="/search/cs?searchtype=author&query=Paithane%2C+A">Apurva Paithane</a>, 
<a href="/search/cs?searchtype=author&query=Sonia">Sonia</a>, 
<a href="/search/cs?searchtype=author&query=Vivek">Vivek</a>, 
<a href="/search/cs?searchtype=author&query=Priscilla%2C+D">Dharini Priscilla</a>, 
<a href="/search/cs?searchtype=author&query=Khairunnisha">Khairunnisha</a>, 
<a href="/search/cs?searchtype=author&query=Banu%2C+G">Grace Banu</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+A">Ambika Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Thakker%2C+R">Rishav Thakker</a>, 
<a href="/search/cs?searchtype=author&query=Korra%2C+R+D">Rahul Dev Korra</a>, 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+A">Aatman Vaidya</a>, 
<a href="/search/cs?searchtype=author&query=Prabhakar%2C+T">Tarunima Prabhakar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Online gender based violence has grown concomitantly with adoption of the
internet and social media. Its effects are worse in the Global majority where
many users use social media in languages other than English. The scale and
volume of conversations on the internet has necessitated the need for automated
detection of hate speech, and more specifically gendered abuse. There is,
however, a lack of language specific and contextual data to build such
automated tools. In this paper we present a dataset on gendered abuse in three
languages- Hindi, Tamil and Indian English. The dataset comprises of tweets
annotated along three questions pertaining to the experience of gender abuse,
by experts who identify as women or a member of the LGBTQIA community in South
Asia. Through this dataset we demonstrate a participatory approach to creating
datasets that drive AI systems.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09088" title="Abstract">arXiv:2311.09088</a> [<a href="/pdf/2311.09088" title="Download PDF">pdf</a>, <a href="/format/2311.09088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-ML: Collaborative Machine Learning Model Building for Developing  Dataset Design Practices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tseng%2C+T">Tiffany Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Davidson%2C+M+J">Matt J. Davidson</a>, 
<a href="/search/cs?searchtype=author&query=Morales-Navarro%2C+L">Luis Morales-Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J+K">Jennifer King Chen</a>, 
<a href="/search/cs?searchtype=author&query=Delaney%2C+V">Victoria Delaney</a>, 
<a href="/search/cs?searchtype=author&query=Leibowitz%2C+M">Mark Leibowitz</a>, 
<a href="/search/cs?searchtype=author&query=Beason%2C+J">Jazbo Beason</a>, 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+R+B">R. Benjamin Shapiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Machine learning (ML) models are fundamentally shaped by data, and building
inclusive ML systems requires significant considerations around how to design
representative datasets. Yet, few novice-oriented ML modeling tools are
designed to foster hands-on learning of dataset design practices, including how
to design for data diversity and inspect for data quality.
<br />To this end, we outline a set of four data design practices (DDPs) for
designing inclusive ML models and share how we designed a tablet-based
application called Co-ML to foster learning of DDPs through a collaborative ML
model building experience. With Co-ML, beginners can build image classifiers
through a distributed experience where data is synchronized across multiple
devices, enabling multiple users to iteratively refine ML datasets in
discussion and coordination with their peers.
<br />We deployed Co-ML in a 2-week-long educational AIML Summer Camp, where youth
ages 13-18 worked in groups to build custom ML-powered mobile applications. Our
analysis reveals how multi-user model building with Co-ML, in the context of
student-driven projects created during the summer camp, supported development
of DDPs involving incorporating data diversity, evaluating model performance,
and inspecting for data quality. Additionally, we found that students' attempts
to improve model performance often prioritized learnability over class balance.
Through this work, we highlight how the combination of collaboration, model
testing interfaces, and student-driven projects can empower learners to
actively engage in exploring the role of data in ML systems.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09090" title="Abstract">arXiv:2311.09090</a> [<a href="/pdf/2311.09090" title="Download PDF">pdf</a>, <a href="/format/2311.09090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Bias Probing: Fairness Benchmarking for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manerba%2C+M+M">Marta Marchiori Manerba</a>, 
<a href="/search/cs?searchtype=author&query=Sta%C5%84czak%2C+K">Karolina Sta&#x144;czak</a>, 
<a href="/search/cs?searchtype=author&query=Guidotti%2C+R">Riccardo Guidotti</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models have been shown to encode a variety of social biases,
which carries the risk of downstream harms. While the impact of these biases
has been recognized, prior methods for bias evaluation have been limited to
binary association tests on small datasets, offering a constrained view of the
nature of societal biases within language models. In this paper, we propose an
original framework for probing language models for societal biases. We collect
a probing dataset to analyze language models' general associations, as well as
along the axes of societal categories, identities, and stereotypes. To this
end, we leverage a novel perplexity-based fairness score. We curate a
large-scale benchmarking dataset addressing drawbacks and limitations of
existing fairness collections, expanding to a variety of different identities
and stereotypes. When comparing our methodology with prior work, we demonstrate
that biases within language models are more nuanced than previously
acknowledged. In agreement with recent findings, we find that larger model
variants exhibit a higher degree of bias. Moreover, we expose how identities
expressing different religions lead to the most pronounced disparate treatments
across all models.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09093" title="Abstract">arXiv:2311.09093</a> [<a href="/pdf/2311.09093" title="Download PDF">pdf</a>, <a href="/format/2311.09093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of Computer Vision in Autonomous Vehicles: Methods,  Challenges and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xingshuai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cappuccio%2C+M+L">Massimiliano L. Cappuccio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Autonomous vehicle refers to a vehicle capable of perceiving its surrounding
environment and driving with little or no human driver input. The perception
system is a fundamental component which enables the autonomous vehicle to
collect data and extract relevant information from the environment to drive
safely. Benefit from the recent advances in computer vision, the perception
task can be achieved by using sensors, such as camera, LiDAR, radar, and
ultrasonic sensor. This paper reviews publications on computer vision and
autonomous driving that are published during the last ten years. In particular,
we first investigate the development of autonomous driving systems and
summarize these systems that are developed by the major automotive
manufacturers from different countries. Second, we investigate the sensors and
benchmark data sets that are commonly utilized for autonomous driving. Then, a
comprehensive overview of computer vision applications for autonomous driving
such as depth estimation, object detection, lane detection, and traffic sign
recognition are discussed. Additionally, we review public opinions and concerns
on autonomous vehicles. Based on the discussion, we analyze the current
technological challenges that autonomous vehicles meet with. Finally, we
present our insights and point out some promising directions for future
research. This paper will help the reader to understand autonomous vehicles
from the perspectives of academia and industry.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09094" title="Abstract">arXiv:2311.09094</a> [<a href="/pdf/2311.09094" title="Download PDF">pdf</a>, <a href="/format/2311.09094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can MusicGen Create Training Data for MIR Tasks?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kroher%2C+N">Nadine Kroher</a>, 
<a href="/search/cs?searchtype=author&query=Cuesta%2C+H">Helena Cuesta</a>, 
<a href="/search/cs?searchtype=author&query=Pikrakis%2C+A">Aggelos Pikrakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended abstract presented at the Late-Breaking / Demo Session of the International Society for Music Information Retrieval Conference (ISMIR) 2023 (Milan, Italy)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We are investigating the broader concept of using AI-based generative music
systems to generate training data for Music Information Retrieval (MIR) tasks.
To kick off this line of work, we ran an initial experiment in which we trained
a genre classifier on a fully artificial music dataset created with MusicGen.
We constructed over 50 000 genre- conditioned textual descriptions and
generated a collection of music excerpts that covers five musical genres. Our
preliminary results show that the proposed model can learn genre-specific
characteristics from artificial music tracks that generalise well to real-world
music recordings.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09095" title="Abstract">arXiv:2311.09095</a> [<a href="/pdf/2311.09095" title="Download PDF">pdf</a>, <a href="/format/2311.09095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Graph Decompositions and Combinatorial Boolean Matrix Multiplication  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abboud%2C+A">Amir Abboud</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+N">Nick Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Kelley%2C+Z">Zander Kelley</a>, 
<a href="/search/cs?searchtype=author&query=Lovett%2C+S">Shachar Lovett</a>, 
<a href="/search/cs?searchtype=author&query=Meka%2C+R">Raghu Meka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We revisit the fundamental Boolean Matrix Multiplication (BMM) problem. With
the invention of algebraic fast matrix multiplication over 50 years ago, it
also became known that BMM can be solved in truly subcubic $O(n^\omega)$ time,
where $\omega&lt;3$; much work has gone into bringing $\omega$ closer to $2$.
Since then, a parallel line of work has sought comparably fast combinatorial
algorithms but with limited success. The naive $O(n^3)$-time algorithm was
initially improved by a $\log^2{n}$ factor [Arlazarov et al.; RAS'70], then by
$\log^{2.25}{n}$ [Bansal and Williams; FOCS'09], then by $\log^3{n}$ [Chan;
SODA'15], and finally by $\log^4{n}$ [Yu; ICALP'15].
<br />We design a combinatorial algorithm for BMM running in time $n^3 /
2^{\Omega(\sqrt[7]{\log n})}$ -- a speed-up over cubic time that is stronger
than any poly-log factor. This comes tantalizingly close to refuting the
conjecture from the 90s that truly subcubic combinatorial algorithms for BMM
are impossible. This popular conjecture is the basis for dozens of fine-grained
hardness results.
<br />Our main technical contribution is a new regularity decomposition theorem for
Boolean matrices (or equivalently, bipartite graphs) under a notion of
regularity that was recently introduced and analyzed analytically in the
context of communication complexity [Kelley, Lovett, Meka; arXiv'23], and is
related to a similar notion from the recent work on $3$-term arithmetic
progression free sets [Kelley, Meka; FOCS'23].
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09096" title="Abstract">arXiv:2311.09096</a> [<a href="/pdf/2311.09096" title="Download PDF">pdf</a>, <a href="/format/2311.09096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Large Language Models Against Jailbreaking Attacks Through  Goal Prioritization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhexin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+P">Pei Ke</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) continue to advance in their capabilities, yet
this progress is accompanied by a growing array of safety risks. While
significant attention has been dedicated to exploiting weaknesses in LLMs
through jailbreaking attacks, there remains a paucity of exploration into
defending against these attacks. We point out a pivotal factor contributing to
the success of jailbreaks: the inherent conflict between the goals of being
helpful and ensuring safety. To counter jailbreaking attacks, we propose to
integrate goal prioritization at both training and inference stages.
Implementing goal prioritization during inference substantially diminishes the
Attack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to
2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising
general performance. Furthermore, integrating the concept of goal
prioritization into the training phase reduces the ASR from 71.0% to 6.6% for
LLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are
included during training, our approach slashes the ASR by half, decreasing it
from 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs
face greater safety risks, they also possess a greater capacity to be steered
towards defending against such attacks. We hope our work could contribute to
the comprehension of jailbreaking attacks and defenses, and shed light on the
relationship between LLMs' capability and safety. Our code will be available at
\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09101" title="Abstract">arXiv:2311.09101</a> [<a href="/pdf/2311.09101" title="Download PDF">pdf</a>, <a href="/format/2311.09101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards A Unified View of Answer Calibration for Multi-Step Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shumin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Oo%2C+N">Nay Oo</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have
broadened the scope for improving multi-step reasoning capabilities. Usually,
answer calibration strategies such as step-level or path-level calibration play
a vital role in multi-step reasoning. While effective, there remains a
significant gap in our understanding of the key factors that drive their
success. In this paper, we break down the design of recent answer calibration
strategies and present a unified view which establishes connections between
them. We then conduct a thorough evaluation on these strategies from a unified
view, systematically scrutinizing step-level and path-level answer calibration
across multiple paths. Our study holds the potential to illuminate key insights
for optimizing multi-step reasoning with answer calibration.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09103" title="Abstract">arXiv:2311.09103</a> [<a href="/pdf/2311.09103" title="Download PDF">pdf</a>, <a href="/ps/2311.09103" title="Download PostScript">ps</a>, <a href="/format/2311.09103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guided Scale Space Radon Transform for linear structures detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goumeidane%2C+A+B">Aicha Baya Goumeidane</a>, 
<a href="/search/cs?searchtype=author&query=Ziou%2C+D">Djemel Ziou</a>, 
<a href="/search/cs?searchtype=author&query=Nacereddine%2C+N">Nafaa Nacereddine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Using integral transforms to the end of lines detection in images with
complex background, makes the detection a hard task needing additional
processing to manage the detection. As an integral transform, the Scale Space
Radon Transform (SSRT) suffers from such drawbacks, even with its great
abilities for thick lines detection. In this work, we propose a method to
address this issue for automatic detection of thick linear structures in gray
scale and binary images using the SSRT, whatever the image background content.
This method involves the calculated Hessian orientations of the investigated
image while computing its SSRT, in such a way that linear structures are
emphasized in the SSRT space. As a consequence, the subsequent maxima detection
in the SSRT space is done on a modified transform space freed from unwanted
parts and, consequently, from irrelevant peaks that usually drown the peaks
representing lines. Besides, highlighting the linear structure in the SSRT
space permitting, thus, to efficiently detect lines of different thickness in
synthetic and real images, the experiments show also the method robustness
against noise and complex background.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09104" title="Abstract">arXiv:2311.09104</a> [<a href="/pdf/2311.09104" title="Download PDF">pdf</a>, <a href="/format/2311.09104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-view and Cross-pose Completion for 3D Human Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Armando%2C+M">Matthieu Armando</a>, 
<a href="/search/cs?searchtype=author&query=Galaaoui%2C+S">Salma Galaaoui</a>, 
<a href="/search/cs?searchtype=author&query=Baradel%2C+F">Fabien Baradel</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+T">Thomas Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Leroy%2C+V">Vincent Leroy</a>, 
<a href="/search/cs?searchtype=author&query=Br%C3%A9gier%2C+R">Romain Br&#xe9;gier</a>, 
<a href="/search/cs?searchtype=author&query=Weinzaepfel%2C+P">Philippe Weinzaepfel</a>, 
<a href="/search/cs?searchtype=author&query=Rogez%2C+G">Gr&#xe9;gory Rogez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Human perception and understanding is a major domain of computer vision
which, like many other vision subdomains recently, stands to gain from the use
of large models pre-trained on large datasets. We hypothesize that the most
common pre-training strategy of relying on general purpose, object-centric
image datasets such as ImageNet, is limited by an important domain shift. On
the other hand, collecting domain specific ground truth such as 2D or 3D labels
does not scale well. Therefore, we propose a pre-training approach based on
self-supervised learning that works on human-centric data using only images.
Our method uses pairs of images of humans: the first is partially masked and
the model is trained to reconstruct the masked parts given the visible ones and
a second image. It relies on both stereoscopic (cross-view) pairs, and temporal
(cross-pose) pairs taken from videos, in order to learn priors about 3D as well
as human motion. We pre-train a model for body-centric tasks and one for
hand-centric tasks. With a generic transformer architecture, these models
outperform existing self-supervised pre-training methods on a wide set of
human-centric downstream tasks, and obtain state-of-the-art performance for
instance when fine-tuning for model-based and model-free human mesh recovery.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09105" title="Abstract">arXiv:2311.09105</a> [<a href="/pdf/2311.09105" title="Download PDF">pdf</a>, <a href="/format/2311.09105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding  Dataset with Event Argument Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaozhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yong Guan</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kaisheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianhui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ruobing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Understanding events in texts is a core objective of natural language
understanding, which requires detecting event occurrences, extracting event
arguments, and analyzing inter-event relationships. However, due to the
annotation challenges brought by task complexity, a large-scale dataset
covering the full process of event understanding has long been absent. In this
paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event
argument annotations, making the first all-in-one dataset supporting event
detection, event argument extraction (EAE), and event relation extraction. As
an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive
schema covering 162 event types and 612 argument roles, all with expert-written
definitions and examples; (2) a large data scale, containing 98,591 events and
290,613 arguments obtained with laborious human annotation; (3) the exhaustive
annotation supporting all task variants of EAE, which annotates both entity and
non-entity event arguments in document level. Experiments indicate that
MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary
large language models (LLMs). Furthermore, to demonstrate the benefits of an
all-in-one dataset, we preliminarily explore a potential application, future
event prediction, with LLMs. MAVEN-Arg and our code can be obtained from
https://github.com/THU-KEG/MAVEN-Argument.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09106" title="Abstract">arXiv:2311.09106</a> [<a href="/pdf/2311.09106" title="Download PDF">pdf</a>, <a href="/format/2311.09106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;We Demand Justice!&quot;: Towards Grounding Political Text in Social Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pujari%2C+R">Rajkumar Pujari</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chengfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Goldwasser%2C+D">Dan Goldwasser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Was accepted to and withdrawn from Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Social media discourse from US politicians frequently consists of 'seemingly
similar language used by opposing sides of the political spectrum'. But often,
it translates to starkly contrasting real-world actions. For instance, "We need
to keep our students safe from mass shootings" may signal either "arming
teachers to stop the shooter" or "banning guns to reduce mass shootings"
depending on who says it and their political stance on the issue. In this
paper, we define and characterize the context that is required to fully
understand such ambiguous statements in a computational setting and ground them
in real-world entities, actions, and attitudes. To that end, we propose two
challenging datasets that require an understanding of the real-world context of
the text to be solved effectively. We benchmark these datasets against
baselines built upon large pre-trained models such as BERT, RoBERTa, GPT-3,
etc. Additionally, we develop and benchmark more structured baselines building
upon existing 'Discourse Contextualization Framework' and 'Political Actor
Representation' models. We perform analysis of the datasets and baseline
predictions to obtain further insights into the pragmatic language
understanding challenges posed by the proposed social grounding tasks.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09109" title="Abstract">arXiv:2311.09109</a> [<a href="/pdf/2311.09109" title="Download PDF">pdf</a>, <a href="/format/2311.09109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge  Graph Completion?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sakai%2C+Y">Yusuke Sakai</a>, 
<a href="/search/cs?searchtype=author&query=Kamigaito%2C+H">Hidetaka Kamigaito</a>, 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+K">Katsuhiko Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+T">Taro Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge graphs (KGs) consist of links that describe relationships between
entities. Due to the difficulty of manually enumerating all relationships
between entities, automatically completing them is essential for KGs. Knowledge
Graph Completion (KGC) is a task that infers unseen relationships between
entities in a KG. Traditional embedding-based KGC methods, such as RESCAL,
TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using
only the knowledge from training data. In contrast, the recent Pre-trained
Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training.
Therefore, PLM-based KGC can estimate missing links between entities by reusing
memorized knowledge from pre-training without inference. This approach is
problematic because building KGC models aims to infer unseen links between
entities. However, conventional evaluations in KGC do not consider inference
and memorization abilities separately. Thus, a PLM-based KGC method, which
achieves high performance in current KGC evaluations, may be ineffective in
practical applications. To address this issue, we analyze whether PLM-based KGC
methods make inferences or merely access memorized knowledge. For this purpose,
we propose a method for constructing synthetic datasets specified in this
analysis and conclude that PLMs acquire the inference abilities required for
KGC through pre-training, even though the performance improvements mostly come
from textual information of entities and relations.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09111" title="Abstract">arXiv:2311.09111</a> [<a href="/pdf/2311.09111" title="Download PDF">pdf</a>, <a href="/ps/2311.09111" title="Download PostScript">ps</a>, <a href="/format/2311.09111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Compression with Side Information at the Decoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vippathalla%2C+P+K">Praneeth Kumar Vippathalla</a>, 
<a href="/search/cs?searchtype=author&query=Badiu%2C+M">Mihai-Alin Badiu</a>, 
<a href="/search/cs?searchtype=author&query=Coon%2C+J+P">Justin P. Coon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 2 figures, submitted to the IEEE Journal on Selected Areas in Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this paper, we study the problem of graph compression with side
information at the decoder. The focus is on the situation when an unlabelled
graph (which is also referred to as a structure) is to be compressed or is
available as side information. For correlated Erd\H{o}s-R\'enyi weighted random
graphs, we give a precise characterization of the smallest rate at which a
labelled graph or its structure can be compressed with aid of a correlated
labelled graph or its structure at the decoder. We approach this problem by
using the entropy-spectrum framework and establish some convergence results for
conditional distributions involving structures, which play a key role in the
construction of an optimal encoding and decoding scheme. Our proof essentially
uses the fact that, in the considered correlated Erd\H{o}s-R\'enyi model, the
structure retains most of the information about the labelled graph.
Furthermore, we consider the case of unweighted graphs and present how the
optimal decoding can be done using the notion of graph alignment.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09114" title="Abstract">arXiv:2311.09114</a> [<a href="/pdf/2311.09114" title="Download PDF">pdf</a>, <a href="/format/2311.09114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ever: Mitigating Hallucination in Large Language Models through  Real-Time Verification and Rectification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Haoqiang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Juntong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating fluent text. However, they often encounter the challenge of
generating inaccurate or hallucinated content. This issue is common in both
non-retrieval-based generation and retrieval-augmented generation approaches,
and existing post-hoc rectification methods may not address the accumulated
hallucination errors that may be caused by the "snowballing" issue, especially
in reasoning tasks. To tackle these challenges, we introduce a novel approach
called Real-time Verification and Rectification (Ever). Instead of waiting
until the end of the generation process to rectify hallucinations, Ever employs
a real-time, step-wise generation and hallucination rectification strategy. The
primary objective is to detect and rectify hallucinations as they occur during
the text generation process. When compared to both retrieval-based and
non-retrieval-based baselines, Ever demonstrates a significant improvement in
generating trustworthy and factually accurate text across a diverse range of
tasks, including short-form QA, biography generation, and multi-hop reasoning.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09115" title="Abstract">arXiv:2311.09115</a> [<a href="/pdf/2311.09115" title="Download PDF">pdf</a>, <a href="/format/2311.09115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hemker%2C+K">Konstantin Hemker</a>, 
<a href="/search/cs?searchtype=author&query=Smidjievski%2C+N">Nikola Smidjievski</a>, 
<a href="/search/cs?searchtype=author&query=Jamnik%2C+M">Mateja Jamnik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages body, 5 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Technological advances in medical data collection such as high-resolution
histopathology and high-throughput genomic sequencing have contributed to the
rising requirement for multi-modal biomedical modelling, specifically for
image, tabular, and graph data. Most multi-modal deep learning approaches use
modality-specific architectures that are trained separately and cannot capture
the crucial cross-modal information that motivates the integration of different
data sources. This paper presents the Hybrid Early-fusion Attention Learning
Network (HEALNet): a flexible multi-modal fusion architecture, which a)
preserves modality-specific structural information, b) captures the cross-modal
interactions and structural information in a shared latent space, c) can
effectively handle missing modalities during training and inference, and d)
enables intuitive model inspection by learning on the raw data input instead of
opaque embeddings. We conduct multi-modal survival analysis on Whole Slide
Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas
(TCGA). HEALNet achieves state-of-the-art performance, substantially improving
over both uni-modal and recent multi-modal baselines, whilst being robust in
scenarios with missing modalities.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09117" title="Abstract">arXiv:2311.09117</a> [<a href="/pdf/2311.09117" title="Download PDF">pdf</a>, <a href="/format/2311.09117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R-Spin: Efficient Speaker and Noise-invariant Representation Learning  with Acoustic Pieces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Heng-Jui Chang</a>, 
<a href="/search/cs?searchtype=author&query=Glass%2C+J">James Glass</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper introduces Robust Spin (R-Spin), a data-efficient self-supervised
fine-tuning framework for speaker and noise-invariant speech representations by
learning discrete acoustic units with speaker-invariant clustering (Spin).
R-Spin resolves Spin's issues and enhances content representations by learning
to predict acoustic pieces. R-Spin offers a 12X reduction in computational
resources compared to previous state-of-the-art methods while outperforming
them in severely distorted speech scenarios. This paper provides detailed
analyses to show how discrete units contribute to speech encoder training and
improving robustness in diverse acoustic environments.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09118" title="Abstract">arXiv:2311.09118</a> [<a href="/pdf/2311.09118" title="Download PDF">pdf</a>, <a href="/format/2311.09118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WildlifeDatasets: An open-source toolkit for animal re-identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C4%8Cerm%C3%A1k%2C+V">Vojt&#x11b;ch &#x10c;erm&#xe1;k</a>, 
<a href="/search/cs?searchtype=author&query=Picek%2C+L">Lukas Picek</a>, 
<a href="/search/cs?searchtype=author&query=Adam%2C+L">Luk&#xe1;&#x161; Adam</a>, 
<a href="/search/cs?searchtype=author&query=Papafitsoros%2C+K">Kostas Papafitsoros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we present WildlifeDatasets
(https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source
toolkit intended primarily for ecologists and computer-vision /
machine-learning researchers. The WildlifeDatasets is written in Python, allows
straightforward access to publicly available wildlife datasets, and provides a
wide variety of methods for dataset pre-processing, performance analysis, and
model fine-tuning. We showcase the toolkit in various scenarios and baseline
experiments, including, to the best of our knowledge, the most comprehensive
experimental comparison of datasets and methods for wildlife re-identification,
including both local descriptors and deep learning approaches. Furthermore, we
provide the first-ever foundation model for individual re-identification within
a wide range of species - MegaDescriptor - that provides state-of-the-art
performance on animal re-identification datasets and outperforms other
pre-trained models such as CLIP and DINOv2 by a significant margin. To make the
model available to the general public and to allow easy integration with any
existing wildlife monitoring applications, we provide multiple MegaDescriptor
flavors (i.e., Small, Medium, and Large) through the HuggingFace hub
(https://huggingface.co/BVRA).
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09119" title="Abstract">arXiv:2311.09119</a> [<a href="/pdf/2311.09119" title="Download PDF">pdf</a>, <a href="/ps/2311.09119" title="Download PostScript">ps</a>, <a href="/format/2311.09119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A high-order local discontinuous Galerkin method for the $p$-Laplace  equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Y">Yan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 36 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We study the high-order local discontinuous Galerkin (LDG) method for the
$p$-Laplace equation. We reformulate our spatial discretization as an
equivalent convex minimization problem and use a preconditioned gradient
descent method as the nonlinear solver. For the first time, a weighted
preconditioner that provides $hk$-independent convergence is applied in the LDG
setting. For polynomial order $k \geqslant 1$, we rigorously establish the
solvability of our scheme and provide a priori error estimates in a
mesh-dependent energy norm. Our error estimates are under a different and
non-equivalent distance from existing LDG results. For arbitrarily high-order
polynomials under the assumption that the exact solution has enough regularity,
the error estimates demonstrate the potential for high-order accuracy. Our
numerical results exhibit the desired convergence speed facilitated by the
preconditioner, and we observe best convergence rates in gradient variables in
alignment with linear LDG, and optimal rates in the primal variable when $1 &lt; p
\leqslant 2$.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09122" title="Abstract">arXiv:2311.09122</a> [<a href="/pdf/2311.09122" title="Download PDF">pdf</a>, <a href="/format/2311.09122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal NER: A Gold-Standard Multilingual Named Entity Recognition  Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mayhew%2C+S">Stephen Mayhew</a>, 
<a href="/search/cs?searchtype=author&query=Blevins%2C+T">Terra Blevins</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0uppa%2C+M">Marek &#x160;uppa</a>, 
<a href="/search/cs?searchtype=author&query=Gonen%2C+H">Hila Gonen</a>, 
<a href="/search/cs?searchtype=author&query=Imperial%2C+J+M">Joseph Marvin Imperial</a>, 
<a href="/search/cs?searchtype=author&query=Karlsson%2C+B+F">B&#xf6;rje F. Karlsson</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+P">Peiqin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ljube%C5%A1i%C4%87%2C+N">Nikola Ljube&#x161;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Miranda%2C+L">LJ Miranda</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Riabi%2C+A">Arij Riabi</a>, 
<a href="/search/cs?searchtype=author&query=Pinter%2C+Y">Yuval Pinter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce Universal NER (UNER), an open, community-driven project to
develop gold-standard NER benchmarks in many languages. The overarching goal of
UNER is to provide high-quality, cross-lingually consistent annotations to
facilitate and standardize multilingual NER research. UNER v1 contains 18
datasets annotated with named entities in a cross-lingual consistent schema
across 12 diverse languages. In this paper, we detail the dataset creation and
composition of UNER; we also provide initial modeling baselines on both
in-language and cross-lingual learning settings. We release the data, code, and
fitted models to the public.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09127" title="Abstract">arXiv:2311.09127</a> [<a href="/pdf/2311.09127" title="Download PDF">pdf</a>, <a href="/format/2311.09127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuanwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing work on jailbreak Multimodal Large Language Models (MLLMs) has
focused primarily on adversarial examples in model inputs, with less attention
to vulnerabilities in model APIs. To fill the research gap, we carry out the
following work: 1) We discover a system prompt leakage vulnerability in GPT-4V.
Through carefully designed dialogue, we successfully steal the internal system
prompts of GPT-4V. This finding indicates potential exploitable security risks
in MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM
jailbreaking attack method termed SASP (Self-Adversarial Attack via System
Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to
search for potential jailbreak prompts leveraging stolen system prompts.
Furthermore, in pursuit of better performance, we also add human modification
based on GPT-4's analysis, which further improves the attack success rate to
98.7\%; 3) We evaluated the effect of modifying system prompts to defend
against jailbreaking attacks. Results show that appropriately designed system
prompts can significantly reduce jailbreak success rates. Overall, our work
provides new insights into enhancing MLLM security, demonstrating the important
role of system prompts in jailbreaking, which could be leveraged to greatly
facilitate jailbreak success rates while also holding the potential for
defending against jailbreaks.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09128" title="Abstract">arXiv:2311.09128</a> [<a href="/pdf/2311.09128" title="Download PDF">pdf</a>, <a href="/format/2311.09128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Detection of Phase Transitions with Multi-Task  Learning-by-Confusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arnold%2C+J">Julian Arnold</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4fer%2C+F">Frank Sch&#xe4;fer</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B6rch%2C+N">Niels L&#xf6;rch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, Machine Learning and the Physical Sciences Workshop, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistical Mechanics (cond-mat.stat-mech)

</div>
<p class="mathjax">Machine learning has been successfully used to study phase transitions. One
of the most popular approaches to identifying critical points from data without
prior knowledge of the underlying phases is the learning-by-confusion scheme.
As input, it requires system samples drawn from a grid of the parameter whose
change is associated with potential phase transitions. Up to now, the scheme
required training a distinct binary classifier for each possible splitting of
the grid into two sides, resulting in a computational cost that scales linearly
with the number of grid points. In this work, we propose and showcase an
alternative implementation that only requires the training of a single
multi-class classifier. Ideally, such multi-task learning eliminates the
scaling with respect to the number of grid points. In applications to the Ising
model and an image dataset generated with Stable Diffusion, we find significant
speedups that closely correspond to the ideal case, with only minor deviations.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09130" title="Abstract">arXiv:2311.09130</a> [<a href="/pdf/2311.09130" title="Download PDF">pdf</a>, <a href="/format/2311.09130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Meme-ing: Measuring Linguistic Variation in Memes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+N">Naitian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jurgens%2C+D">David Jurgens</a>, 
<a href="/search/cs?searchtype=author&query=Bamman%2C+D">David Bamman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Much work in the space of NLP has used computational methods to explore
sociolinguistic variation in text. In this paper, we argue that memes, as
multimodal forms of language comprised of visual templates and text, also
exhibit meaningful social variation. We construct a computational pipeline to
cluster individual instances of memes into templates and semantic variables,
taking advantage of their multimodal structure in doing so. We apply this
method to a large collection of meme images from Reddit and make available the
resulting \textsc{SemanticMemes} dataset of 3.8M images clustered by their
semantic function. We use these clusters to analyze linguistic variation in
memes, discovering not only that socially meaningful variation in meme usage
exists between subreddits, but that patterns of meme innovation and
acculturation within these communities align with previous findings on written
language.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09132" title="Abstract">arXiv:2311.09132</a> [<a href="/pdf/2311.09132" title="Download PDF">pdf</a>, <a href="/format/2311.09132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Neural Machine Translation Models: Human Feedback in Training  and Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramos%2C+M+M">Miguel Moura Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+P">Patrick Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=Farinhas%2C+A">Ant&#xf3;nio Farinhas</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+A+F+T">Andr&#xe9; F. T. Martins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, work-in-progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Reinforcement learning from human feedback (RLHF) is a recent technique to
improve the quality of the text generated by a language model, making it closer
to what humans would generate. A core ingredient in RLHF's success in aligning
and improving large language models (LLMs) is its reward model, trained using
human feedback on model outputs. In machine translation (MT), where metrics
trained from human annotations can readily be used as reward models, recent
methods using minimum Bayes risk decoding and reranking have succeeded in
improving the final quality of translation. In this study, we comprehensively
explore and compare techniques for integrating quality metrics as reward models
into the MT pipeline. This includes using the reward model for data filtering,
during the training phase through RL, and at inference time by employing
reranking techniques, and we assess the effects of combining these in a unified
approach. Our experimental results, conducted across multiple translation
tasks, underscore the crucial role of effective data filtering, based on
estimated quality, in harnessing the full potential of RL in enhancing MT
quality. Furthermore, our findings demonstrate the effectiveness of combining
RL training with reranking techniques, showcasing substantial improvements in
translation quality.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09133" title="Abstract">arXiv:2311.09133</a> [<a href="/pdf/2311.09133" title="Download PDF">pdf</a>, <a href="/ps/2311.09133" title="Download PostScript">ps</a>, <a href="/format/2311.09133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Text Classification Techniques in Legal Document Review:  Locating Rationales without Using Human Annotated Training Text Snippets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+C">Christian Mahoney</a>, 
<a href="/search/cs?searchtype=author&query=Gronvall%2C+P">Peter Gronvall</a>, 
<a href="/search/cs?searchtype=author&query=Huber-Fliflet%2C+N">Nathaniel Huber-Fliflet</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianping Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1912.09501">arXiv:1912.09501</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">US corporations regularly spend millions of dollars reviewing
electronically-stored documents in legal matters. Recently, attorneys apply
text classification to efficiently cull massive volumes of data to identify
responsive documents for use in these matters. While text classification is
regularly used to reduce the discovery costs of legal matters, it also faces a
perception challenge: amongst lawyers, this technology is sometimes looked upon
as a "black box". Put simply, no extra information is provided for attorneys to
understand why documents are classified as responsive. In recent years,
explainable machine learning has emerged as an active research area. In an
explainable machine learning system, predictions or decisions made by a machine
learning model are human understandable. In legal 'document review' scenarios,
a document is responsive, because one or more of its small text snippets are
deemed responsive. In these scenarios, if these responsive snippets can be
located, then attorneys could easily evaluate the model's document
classification decisions - this is especially important in the field of
responsible AI. Our prior research identified that predictive models created
using annotated training text snippets improved the precision of a model when
compared to a model created using all of a set of documents' text as training.
While interesting, manually annotating training text snippets is not generally
practical during a legal document review. However, small increases in precision
can drastically decrease the cost of large document reviews. Automating the
identification of training text snippets without human review could then make
the application of training text snippet-based models a practical approach.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09134" title="Abstract">arXiv:2311.09134</a> [<a href="/pdf/2311.09134" title="Download PDF">pdf</a>, <a href="/format/2311.09134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable and Effective Generative Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Hansi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Chen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bowen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Sarwar%2C+S+M">Sheikh Muhammad Sarwar</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tianxin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zamani%2C+H">Hamed Zamani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recent research has shown that transformer networks can be used as
differentiable search indexes by representing each document as a sequences of
document ID tokens. These generative retrieval models cast the retrieval
problem to a document ID generation problem for each given query. Despite their
elegant design, existing generative retrieval models only perform well on
artificially-constructed and small-scale collections. This has led to serious
skepticism in the research community on their real-world impact. This paper
represents an important milestone in generative retrieval research by showing,
for the first time, that generative retrieval models can be trained to perform
effectively on large-scale standard retrieval benchmarks. For doing so, we
propose RIPOR- an optimization framework for generative retrieval that can be
adopted by any encoder-decoder architecture. RIPOR is designed based on two
often-overlooked fundamental design considerations in generative retrieval.
First, given the sequential decoding nature of document ID generation,
assigning accurate relevance scores to documents based on the whole document ID
sequence is not sufficient. To address this issue, RIPOR introduces a novel
prefix-oriented ranking optimization algorithm. Second, initial document IDs
should be constructed based on relevance associations between queries and
documents, instead of the syntactic and semantic information in the documents.
RIPOR addresses this issue using a relevance-based document ID construction
approach that quantizes relevance-based representations learned for documents.
Evaluation on MSMARCO and TREC Deep Learning Track reveals that RIPOR surpasses
state-of-the-art generative retrieval models by a large margin (e.g., 30.5% MRR
improvements on MS MARCO Dev Set), and perform better on par with popular dense
retrieval models.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09136" title="Abstract">arXiv:2311.09136</a> [<a href="/pdf/2311.09136" title="Download PDF">pdf</a>, <a href="/format/2311.09136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RRescue: Ranking LLM Responses to Enhance Reasoning Over Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yikun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Effectively using a given context is paramount for large language models. A
context window can include task specifications, retrieved documents, previous
conversations, and even model self-reflections, functioning similarly to
episodic memory. While efforts are being made to expand the context window,
studies indicate that LLMs do not use their context optimally for response
generation. In this paper, we present a novel approach to optimize LLMs using
ranking metrics, which teaches LLMs to rank a collection of
contextually-grounded candidate responses. Rather than a traditional full
ordering, we advocate for a partial ordering. This is because achieving
consensus on the perfect order for system responses can be challenging. Our
partial ordering is more robust, less sensitive to noise, and can be acquired
through human labelers, heuristic functions, or model distillation. We test our
system's improved contextual understanding using the latest benchmarks,
including a new multi-document question answering dataset. We conduct ablation
studies to understand crucial factors, such as how to gather candidate
responses, determine their most suitable order, and balance supervised
fine-tuning with ranking metrics. Our approach, named RRescue, suggests a
promising avenue for enhancing LLMs' contextual understanding via response
ranking.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09137" title="Abstract">arXiv:2311.09137</a> [<a href="/pdf/2311.09137" title="Download PDF">pdf</a>, <a href="/format/2311.09137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal prediction models for medication safety monitoring: The diagnosis  of vancomycin-induced acute kidney injury
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kom%2C+I+Y">Izak Yasrebi-de Kom</a>, 
<a href="/search/cs?searchtype=author&query=Klopotowska%2C+J">Joanna Klopotowska</a>, 
<a href="/search/cs?searchtype=author&query=Dongelmans%2C+D">Dave Dongelmans</a>, 
<a href="/search/cs?searchtype=author&query=De+Keizer%2C+N">Nicolette De Keizer</a>, 
<a href="/search/cs?searchtype=author&query=Jager%2C+K">Kitty Jager</a>, 
<a href="/search/cs?searchtype=author&query=Abu-Hanna%2C+A">Ameen Abu-Hanna</a>, 
<a href="/search/cs?searchtype=author&query=Cin%C3%A0%2C+G">Giovanni Cin&#xe0;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">The current best practice approach for the retrospective diagnosis of adverse
drug events (ADEs) in hospitalized patients relies on a full patient chart
review and a formal causality assessment by multiple medical experts. This
evaluation serves to qualitatively estimate the probability of causation (PC);
the probability that a drug was a necessary cause of an adverse event. This
practice is manual, resource intensive and prone to human biases, and may thus
benefit from data-driven decision support. Here, we pioneer a causal modeling
approach using observational data to estimate a lower bound of the PC
(PC$_{low}$). This method includes two key causal inference components: (1) the
target trial emulation framework and (2) estimation of individualized treatment
effects using machine learning. We apply our method to the clinically relevant
use-case of vancomycin-induced acute kidney injury in intensive care patients,
and compare our causal model-based PC$_{low}$ estimates to qualitative
estimates of the PC provided by a medical expert. Important limitations and
potential improvements are discussed, and we conclude that future improved
causal models could provide essential data-driven support for medication safety
monitoring in hospitalized patients.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09141" title="Abstract">arXiv:2311.09141</a> [<a href="/pdf/2311.09141" title="Download PDF">pdf</a>, <a href="/ps/2311.09141" title="Download PostScript">ps</a>, <a href="/format/2311.09141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prophet Inequalities Require Only a Constant Number of Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cristi%2C+A">Andr&#xe9;s Cristi</a>, 
<a href="/search/cs?searchtype=author&query=Ziliotto%2C+B">Bruno Ziliotto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In a prophet inequality problem, $n$ independent random variables are
presented to a gambler one by one. The gambler decides when to stop the
sequence and obtains the most recent value as reward. We evaluate a stopping
rule by the worst-case ratio between its expected reward and the expectation of
the maximum variable. In the classic setting, the order is fixed, and the
optimal ratio is known to be 1/2. Three variants of this problem have been
extensively studied: the prophet-secretary model, where variables arrive in
uniformly random order; the free-order model, where the gambler chooses the
arrival order; and the i.i.d. model, where the distributions are all the same,
rendering the arrival order irrelevant.
<br />Most of the literature assumes that distributions are known to the gambler.
Recent work has considered the question of what is achievable when the gambler
has access only to a few samples per distribution. Surprisingly, in the
fixed-order case, a single sample from each distribution is enough to
approximate the optimal ratio, but this is not the case in any of the three
variants.
<br />We provide a unified proof that for all three variants of the problem, a
constant number of samples (independent of n) for each distribution is good
enough to approximate the optimal ratios.
<br />Prior to our work, this was known to be the case only in the i.i.d. variant.
We complement our result showing that our algorithms can be implemented in
polynomial time.
<br />A key ingredient in our proof is an existential result based on a minimax
argument, which states that there must exist an algorithm that attains the
optimal ratio and does not rely on the knowledge of the upper tail of the
distributions. A second key ingredient is a refined sample-based version of a
decomposition of the instance into "small" and "large" variables, first
introduced by Liu et al. [EC'21].
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09142" title="Abstract">arXiv:2311.09142</a> [<a href="/pdf/2311.09142" title="Download PDF">pdf</a>, <a href="/format/2311.09142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine-learning parameter tracking with partial state observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Z">Zheng-Meng Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Moradi%2C+M">Mohammadamin Moradi</a>, 
<a href="/search/cs?searchtype=author&query=Glaz%2C+B">Bryan Glaz</a>, 
<a href="/search/cs?searchtype=author&query=Haile%2C+M">Mulugeta Haile</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Ying-Cheng Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS); Chaotic Dynamics (nlin.CD); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Complex and nonlinear dynamical systems often involve parameters that change
with time, accurate tracking of which is essential to tasks such as state
estimation, prediction, and control. Existing machine-learning methods require
full state observation of the underlying system and tacitly assume adiabatic
changes in the parameter. Formulating an inverse problem and exploiting
reservoir computing, we develop a model-free and fully data-driven framework to
accurately track time-varying parameters from partial state observation in real
time. In particular, with training data from a subset of the dynamical
variables of the system for a small number of known parameter values, the
framework is able to accurately predict the parameter variations in time. Low-
and high-dimensional, Markovian and non-Markovian nonlinear dynamical systems
are used to demonstrate the power of the machine-learning based
parameter-tracking framework. Pertinent issues affecting the tracking
performance are addressed.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09144" title="Abstract">arXiv:2311.09144</a> [<a href="/pdf/2311.09144" title="Download PDF">pdf</a>, <a href="/format/2311.09144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grounding or Guesswork? Large Language Models are Presumptive Grounders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaikh%2C+O">Omar Shaikh</a>, 
<a href="/search/cs?searchtype=author&query=Gligori%C4%87%2C+K">Kristina Gligori&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Khetan%2C+A">Ashna Khetan</a>, 
<a href="/search/cs?searchtype=author&query=Gerstgrasser%2C+M">Matthias Gerstgrasser</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jurafsky%2C+D">Dan Jurafsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Effective conversation requires common ground: a shared understanding between
the participants. Common ground, however, does not emerge spontaneously in
conversation. Speakers and listeners work together to both identify and
construct a shared basis while avoiding misunderstanding. To accomplish
grounding, humans rely on a range of dialogue acts, like clarification (What do
you mean?) and acknowledgment (I understand.). In domains like teaching and
emotional support, carefully constructing grounding prevents misunderstanding.
However, it is unclear whether large language models (LLMs) leverage these
dialogue acts in constructing common ground. To this end, we curate a set of
grounding acts and propose corresponding metrics that quantify attempted
grounding. We study whether LLMs use these grounding acts, simulating them
taking turns from several dialogue datasets, and comparing the results to
humans. We find that current LLMs are presumptive grounders, biased towards
assuming common ground without using grounding acts. To understand the roots of
this behavior, we examine the role of instruction tuning and reinforcement
learning with human feedback (RLHF), finding that RLHF leads to less grounding.
Altogether, our work highlights the need for more research investigating
grounding in human-AI interaction.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09145" title="Abstract">arXiv:2311.09145</a> [<a href="/pdf/2311.09145" title="Download PDF">pdf</a>, <a href="/format/2311.09145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Agnostic Explainable Selective Regression via Uncertainty  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pugnana%2C+A">Andrea Pugnana</a>, 
<a href="/search/cs?searchtype=author&query=Mougan%2C+C">Carlos Mougan</a>, 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+D+S">Dan Saattrup Nielsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">With the wide adoption of machine learning techniques, requirements have
evolved beyond sheer high performance, often requiring models to be
trustworthy. A common approach to increase the trustworthiness of such systems
is to allow them to refrain from predicting. Such a framework is known as
selective prediction. While selective prediction for classification tasks has
been widely analyzed, the problem of selective regression is understudied. This
paper presents a novel approach to selective regression that utilizes
model-agnostic non-parametric uncertainty estimation. Our proposed framework
showcases superior performance compared to state-of-the-art selective
regressors, as demonstrated through comprehensive benchmarking on 69 datasets.
Finally, we use explainable AI techniques to gain an understanding of the
drivers behind selective regression. We implement our selective regression
method in the open-source Python package doubt and release the code used to
reproduce our experiments.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09149" title="Abstract">arXiv:2311.09149</a> [<a href="/pdf/2311.09149" title="Download PDF">pdf</a>, <a href="/format/2311.09149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Knowledge Question Answering via Abstract Reasoning Induction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongfang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we tackle the significant challenge of temporal knowledge
reasoning in Large Language Models (LLMs), an area where such models frequently
encounter difficulties. These difficulties often result in the generation of
misleading or incorrect information, primarily due to their limited capacity to
process evolving factual knowledge and complex temporal logic. In response, we
propose a novel, constructivism-based approach that advocates for a paradigm
shift in LLM learning towards an active, ongoing process of knowledge synthesis
and customization. At the heart of our proposal is the Abstract Reasoning
Induction ARI framework, which divides temporal reasoning into two distinct
phases: Knowledge-agnostic and Knowledge-based. This division aims to reduce
instances of hallucinations and improve LLMs' capacity for integrating abstract
methodologies derived from historical data. Our approach achieves remarkable
improvements, with relative gains of 29.7\% and 9.27\% on two temporal QA
datasets, underscoring its efficacy in advancing temporal reasoning in LLMs.
The code will be released at https://github.com/czy1999/ARI.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09154" title="Abstract">arXiv:2311.09154</a> [<a href="/pdf/2311.09154" title="Download PDF">pdf</a>, <a href="/format/2311.09154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenhong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hongkun Hao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiwei He</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yunze Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yumeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hanxu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yiran Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hongyuan Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We are currently in an era of fierce competition among various large language
models (LLMs) continuously pushing the boundaries of benchmark performance.
However, genuinely assessing the capabilities of these LLMs has become a
challenging and critical issue due to potential data contamination, and it
wastes dozens of time and effort for researchers and engineers to download and
try those contaminated models. To save our precious time, we propose a novel
and useful method, Clean-Eval, which mitigates the issue of data contamination
and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to
paraphrase and back-translate the contaminated data into a candidate set,
generating expressions with the same meaning but in different surface forms. A
semantic detector is then used to filter the generated low-quality samples to
narrow down this candidate set. The best candidate is finally selected from
this set based on the BLEURT score. According to human assessment, this best
candidate is semantically similar to the original contamination data but
expressed differently. All candidates can form a new benchmark to evaluate the
model. Our experiments illustrate that Clean-Eval substantially restores the
actual evaluation results on contaminated LLMs under both few-shot learning and
fine-tuning scenarios.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09165" title="Abstract">arXiv:2311.09165</a> [<a href="/pdf/2311.09165" title="Download PDF">pdf</a>, <a href="/format/2311.09165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approaching adverse event detection utilizing transformers on clinical  time-series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fredriksen%2C+H">Helge Fredriksen</a> (1), 
<a href="/search/cs?searchtype=author&query=Burman%2C+P+J">Per Joel Burman</a> (2), 
<a href="/search/cs?searchtype=author&query=Woldaregay%2C+A">Ashenafi Woldaregay</a> (2), 
<a href="/search/cs?searchtype=author&query=Mikalsen%2C+K+%C3%98">Karl &#xd8;yvind Mikalsen</a> (2), 
<a href="/search/cs?searchtype=author&query=Nymo%2C+S">St&#xe5;le Nymo</a> (3) ((1) UiT - The Arctic University of Norway, (2) The Norwegian Centre for Clinical Artificial Intelligence, (3) Nordland Hospital Trust)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Patients being admitted to a hospital will most often be associated with a
certain clinical development during their stay. However, there is always a risk
of patients being subject to the wrong diagnosis or to a certain treatment not
pertaining to the desired effect, potentially leading to adverse events. Our
research aims to develop an anomaly detection system for identifying deviations
from expected clinical trajectories. To address this goal we analyzed 16 months
of vital sign recordings obtained from the Nordland Hospital Trust (NHT). We
employed an self-supervised framework based on the STraTS transformer
architecture to represent the time series data in a latent space. These
representations were then subjected to various clustering techniques to explore
potential patient phenotypes based on their clinical progress. While our
preliminary results from this ongoing research are promising, they underscore
the importance of enhancing the dataset with additional demographic information
from patients. This additional data will be crucial for a more comprehensive
evaluation of the method's performance.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09168" title="Abstract">arXiv:2311.09168</a> [<a href="/pdf/2311.09168" title="Download PDF">pdf</a>, <a href="/format/2311.09168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Neighbor Search using Commodity Hardware Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandarapu%2C+D">Durga Mandarapu</a>, 
<a href="/search/cs?searchtype=author&query=Nagarajan%2C+V">Vani Nagarajan</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+M">Milind Kulkarni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)

</div>
<p class="mathjax">Tree-based Nearest Neighbor Search (NNS) is hard to parallelize on GPUs.
However, newer Nvidia GPUs are equipped with Ray Tracing (RT) cores that can
build a spatial tree called Bounding Volume Hierarchy (BVH) to accelerate
graphics rendering. Recent work proposed using RT cores to implement NNS, but
they all have a hardware-imposed constraint on the type of distance metric,
which is the Euclidean distance. We propose and implement two approaches for
generalized distance computations: filter-refine, and monotone transformation,
each of which allows non-euclidean nearest neighbor queries to be performed in
terms of Euclidean distances. We find that our reductions improve the time
taken to perform distance computations during the search, thereby improving the
overall performance of the NNS.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09172" title="Abstract">arXiv:2311.09172</a> [<a href="/pdf/2311.09172" title="Download PDF">pdf</a>, <a href="/format/2311.09172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing AmBC Systems with Deep Learning for Joint Channel Estimation  and Signal Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zargari%2C+S">S. Zargari</a>, 
<a href="/search/cs?searchtype=author&query=Hakimi%2C+A">A. Hakimi</a>, 
<a href="/search/cs?searchtype=author&query=Tellambura%2C+C">C. Tellambura</a>, 
<a href="/search/cs?searchtype=author&query=Maaref%2C+A">A. Maaref</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The era of ubiquitous, affordable wireless connectivity has opened doors to
countless practical applications. In this context, ambient backscatter
communication (AmBC) stands out, utilizing passive tags to establish
connections with readers by harnessing reflected ambient radio frequency (RF)
signals. However, conventional data detectors face limitations due to their
inadequate knowledge of channel and RF-source parameters. To address this
challenge, we propose an innovative approach using a deep neural network (DNN)
for channel state estimation (CSI) and signal detection within AmBC systems.
Unlike traditional methods that separate CSI estimation and data detection, our
approach leverages a DNN to implicitly estimate CSI and simultaneously detect
data. The DNN model, trained offline using simulated data derived from channel
statistics, excels in online data recovery, ensuring robust performance in
practical scenarios. Comprehensive evaluations validate the superiority of our
proposed DNN method over traditional detectors, particularly in terms of bit
error rate (BER). In high signal-to-noise ratio (SNR) conditions, our method
exhibits an impressive approximately 20% improvement in BER performance
compared to the maximum likelihood (ML) approach. These results underscore the
effectiveness of our developed approach for AmBC channel estimation and signal
detection. In summary, our method outperforms traditional detectors, bolstering
the reliability and efficiency of AmBC systems, even in challenging channel
conditions.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09173" title="Abstract">arXiv:2311.09173</a> [<a href="/pdf/2311.09173" title="Download PDF">pdf</a>, <a href="/ps/2311.09173" title="Download PostScript">ps</a>, <a href="/format/2311.09173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Theory for Societal Digital Transformation: The Case of Digital  Global Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Braa%2C+J">Jorn Braa</a>, 
<a href="/search/cs?searchtype=author&query=Sahay%2C+S">Sundeep Sahay</a>, 
<a href="/search/cs?searchtype=author&query=Monteiro%2C+E">Eric Monteiro</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of the AIS, 24(6), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">With societal challenges, including but not limited to human development,
equity, social justice, and climate change, societal-level digital
transformation (SDT) is of imminent relevance and theoretical interest. While
building on local-level efforts, societal-level transformation is a nonlinear
extension of the local level. Unfortunately, academic discourse on digital
transformation has largely left SDT unaccounted for. Drawing on more than 25
years of intensive, interventionist research engagement with the digital
transformation of public healthcare information management and delivery in more
than 80 countries in the Global South, we contribute to theorizing SDT in the
form of a design theory consisting of six interconnected design principles.
These design principles articulate the interplay and tensions of accommodating
over time increased diversity and flexibility in digital solutions, while
simultaneously connecting local, national, and regional/ global efforts.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09174" title="Abstract">arXiv:2311.09174</a> [<a href="/pdf/2311.09174" title="Download PDF">pdf</a>, <a href="/format/2311.09174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AbsPyramid: Benchmarking the Abstraction Ability of Language Models with  a Unified Entailment Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haochen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tianqing Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sehyun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Cognitive research indicates that abstraction ability is essential in human
intelligence, which remains under-explored in language models. In this paper,
we present AbsPyramid, a unified entailment graph of 221K textual descriptions
of abstraction knowledge. While existing resources only touch nouns or verbs
within simplified events or specific domains, AbsPyramid collects abstract
knowledge for three components of diverse events to comprehensively evaluate
the abstraction ability of language models in the open domain. Experimental
results demonstrate that current LLMs face challenges comprehending abstraction
knowledge in zero-shot and few-shot settings. By training on our rich
abstraction knowledge, we find LLMs can acquire basic abstraction abilities and
generalize to unseen events. In the meantime, we empirically show that our
benchmark is comprehensive to enhance LLMs across two previous abstraction
tasks.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09175" title="Abstract">arXiv:2311.09175</a> [<a href="/pdf/2311.09175" title="Download PDF">pdf</a>, <a href="/format/2311.09175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword  Generation for Zero-Shot Neural Rankers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minghan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Honglei Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+K">Kai Hui</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jimmy Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jagerman%2C+R">Rolf Jagerman</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuanhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Query expansion has been proved to be effective in improving recall and
precision of first-stage retrievers, and yet its influence on a complicated,
state-of-the-art cross-encoder ranker remains under-explored. We first show
that directly applying the expansion techniques in the current literature to
state-of-the-art neural rankers can result in deteriorated zero-shot
performance. To this end, we propose GFF, a pipeline that includes a large
language model and a neural ranker, to Generate, Filter, and Fuse query
expansions more effectively in order to improve the zero-shot ranking metrics
such as nDCG@10. Specifically, GFF first calls an instruction-following
language model to generate query-related keywords through a reasoning chain.
Leveraging self-consistency and reciprocal rank weighting, GFF further filters
and combines the ranking results of each expanded query dynamically. By
utilizing this pipeline, we show that GFF can improve the zero-shot nDCG@10 on
BEIR and TREC DL 2019/2020. We also analyze different modelling choices in the
GFF pipeline and shed light on the future directions in query expansion for
zero-shot neural rankers.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09178" title="Abstract">arXiv:2311.09178</a> [<a href="/pdf/2311.09178" title="Download PDF">pdf</a>, <a href="/format/2311.09178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hussein%2C+D">Dareen Hussein</a>, 
<a href="/search/cs?searchtype=author&query=Eraqi%2C+H">Hesham Eraqi</a>, 
<a href="/search/cs?searchtype=author&query=Fahmy%2C+I">Israa Fahmy</a>, 
<a href="/search/cs?searchtype=author&query=Sulaiman%2C+M">Marwah Sulaiman</a>, 
<a href="/search/cs?searchtype=author&query=Barakat%2C+M">Mohammed Barakat</a>, 
<a href="/search/cs?searchtype=author&query=El-Naggar%2C+M">Mohammed El-Naggar</a>, 
<a href="/search/cs?searchtype=author&query=Youssef%2C+M">Moustafa Youssef</a>, 
<a href="/search/cs?searchtype=author&query=Shehabeldin%2C+Z">Zahraa Shehabeldin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, video super resolution (VSR) has become a very impactful task in
the area of Computer Vision due to its various applications. In this paper, we
propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for
VSR in an attempt to generate temporally coherent solutions while preserving
spatial details. RBPGAN integrates two state-of-the-art models to get the best
in both worlds without compromising the accuracy of produced video. The
generator of the model is inspired by RBPN system, while the discriminator is
inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal
consistency over time. Our contribution together results in a model that
outperforms earlier work in terms of temporally consistent details, as we will
demonstrate qualitatively and quantitatively using different datasets.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09179" title="Abstract">arXiv:2311.09179</a> [<a href="/pdf/2311.09179" title="Download PDF">pdf</a>, <a href="/format/2311.09179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SiRA: Sparse Mixture of Low Rank Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wichers%2C+N">Nevan Wichers</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chu-Cheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+L">Lei Shu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Han Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Canoee Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Liangchen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jindong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lei Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Parameter Efficient Tuning has been an prominent approach to adapt the Large
Language Model to downstream tasks. Most previous works considers adding the
dense trainable parameters, where all parameters are used to adapt certain
task. We found this less effective empirically using the example of LoRA that
introducing more trainable parameters does not help. Motivated by this we
investigate the importance of leveraging "sparse" computation and propose SiRA:
sparse mixture of low rank adaption. SiRA leverages the Sparse Mixture of
Expert(SMoE) to boost the performance of LoRA. Specifically it enforces the top
$k$ experts routing with a capacity limit restricting the maximum number of
tokens each expert can process. We propose a novel and simple expert dropout on
top of gating network to reduce the over-fitting issue. Through extensive
experiments, we verify SiRA performs better than LoRA and other mixture of
expert approaches across different single tasks and multitask settings.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09180" title="Abstract">arXiv:2311.09180</a> [<a href="/pdf/2311.09180" title="Download PDF">pdf</a>, <a href="/format/2311.09180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEARL: Personalizing Large Language Model Writing Assistants with  Generation-Calibrated Retrievers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mysore%2C+S">Sheshera Mysore</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhuoran Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+M">Mengting Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Longqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Menezes%2C+S">Steve Menezes</a>, 
<a href="/search/cs?searchtype=author&query=Baghaee%2C+T">Tina Baghaee</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+E+B">Emmanuel Barajas Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Neville%2C+J">Jennifer Neville</a>, 
<a href="/search/cs?searchtype=author&query=Safavi%2C+T">Tara Safavi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print, work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Powerful large language models have facilitated the development of writing
assistants that promise to significantly improve the quality and efficiency of
composition and communication. However, a barrier to effective assistance is
the lack of personalization in LLM outputs to the author's communication style
and specialized knowledge. In this paper, we address this challenge by
proposing PEARL, a retrieval-augmented LLM writing assistant personalized with
a generation-calibrated retriever. Our retriever is trained to select historic
user-authored documents for prompt augmentation, such that they are likely to
best personalize LLM generations for a user request. We propose two key
novelties for training our retriever: 1) A training data selection method that
identifies user requests likely to benefit from personalization and documents
that provide that benefit; and 2) A scale-calibrating KL-divergence objective
that ensures that our retriever closely tracks the benefit of a document for
personalized generation. We demonstrate the effectiveness of PEARL in
generating personalized workplace social media posts and Reddit comments.
Finally, we showcase the potential of a generation-calibrated retriever to
double as a performance predictor and further improve low-quality generations
via LLM chaining.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09182" title="Abstract">arXiv:2311.09182</a> [<a href="/pdf/2311.09182" title="Download PDF">pdf</a>, <a href="/format/2311.09182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ContraDoc: Understanding Self-Contradictions in Documents with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jierui Li</a>, 
<a href="/search/cs?searchtype=author&query=Raheja%2C+V">Vipul Raheja</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dhruv Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In recent times, large language models (LLMs) have shown impressive
performance on various document-level tasks such as document classification,
summarization, and question-answering. However, research on understanding their
capabilities on the task of self-contradictions in long documents has been very
limited. In this work, we introduce ContraDoc, the first human-annotated
dataset to study self-contradictions in long documents across multiple domains,
varying document lengths, self-contradictions types, and scope. We then analyze
the current capabilities of four state-of-the-art open-source and commercially
available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4
performs the best and can outperform humans on this task, we find that it is
still unreliable and struggles with self-contradictions that require more
nuance and context. We release the dataset and all the code associated with the
experiments.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09184" title="Abstract">arXiv:2311.09184</a> [<a href="/pdf/2311.09184" title="Download PDF">pdf</a>, <a href="/format/2311.09184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Generation and Evaluation Capabilities of Large Language  Models for Instruction Controllable Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fabbri%2C+A+R">Alexander R. Fabbri</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiawen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Simeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+D">Dragomir Radev</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chien-Sheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GitHub Repo: <a href="https://github.com/yale-nlp/InstruSum">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While large language models (LLMs) already achieve strong performance on
standard generic summarization benchmarks, their performance on more complex
summarization task settings is less studied. Therefore, we benchmark LLMs on
instruction controllable text summarization, where the model input consists of
both a source article and a natural language requirement for the desired
summary characteristics. To this end, we curate an evaluation-only dataset for
this task setting and conduct human evaluation on 5 LLM-based summarization
systems. We then benchmark LLM-based automatic evaluation for this task with 4
different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods
in total. Our study reveals that instruction controllable text summarization
remains a challenging task for LLMs, since (1) all LLMs evaluated still make
factual and other types of errors in their summaries; (2) all LLM-based
evaluation methods cannot achieve a strong alignment with human annotators when
judging the quality of candidate summaries; (3) different LLMs show large
performance gaps in summary generation and evaluation. We make our collected
benchmark, InstruSum, publicly available to facilitate future research in this
direction.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09185" title="Abstract">arXiv:2311.09185</a> [<a href="/pdf/2311.09185" title="Download PDF">pdf</a>, <a href="/format/2311.09185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified incremental nonlinear controller for the transition control of a  hybrid dual-axis tilting rotor quad-plane
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mancinelli%2C+A">Alessandro Mancinelli</a>, 
<a href="/search/eess?searchtype=author&query=Remes%2C+B+D+W">Bart D.W. Remes</a>, 
<a href="/search/eess?searchtype=author&query=de+Croon%2C+G+C+H+E">Guido C.H.E. de Croon</a>, 
<a href="/search/eess?searchtype=author&query=Smeur%2C+E+J+J">Ewoud J.J. Smeur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Overactuated Tilt Rotor Unmanned Aerial Vehicles are renowned for exceptional
wind resistance and a broad operational range, which poses complex control
challenges due to non-affine dynamics. Traditional solutions employ multi-state
switched logic controllers for transitions. Our study introduces a novel
unified incremental nonlinear controller for overactuated dual-axis tilting
rotor quad-planes, seamlessly managing pitch, roll, and physical actuator
commands. The control allocation problem is addressed using a SQP iterative
optimization algorithm, well-suited for nonlinear actuator effectiveness in
thrust vectoring vehicles. The controller design integrates desired roll and
pitch angle inputs. These desired attitude angles are autonomously managed by
the controller and then conveyed to the vehicle during slow airspeed phases,
when the vehicle maintains its 6 DOF. We incorporate an AoA protection logic to
prevent wing stall and a yaw rate reference model for coordinated turns. Flight
tests confirm the controller's effectiveness in transitioning from hovering to
forward flight, achieving desired vertical and lateral accelerations, and
reverting to hovering.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09188" title="Abstract">arXiv:2311.09188</a> [<a href="/pdf/2311.09188" title="Download PDF">pdf</a>, <a href="/format/2311.09188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Verifiable Text Generation with Symbolic References
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hennigen%2C+L+T">Lucas Torroba Hennigen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shannon Shen</a>, 
<a href="/search/cs?searchtype=author&query=Nrusimha%2C+A">Aniruddha Nrusimha</a>, 
<a href="/search/cs?searchtype=author&query=Gapp%2C+B">Bernhard Gapp</a>, 
<a href="/search/cs?searchtype=author&query=Sontag%2C+D">David Sontag</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 4 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated an impressive ability to
synthesize plausible and fluent text. However they remain vulnerable to
hallucinations, and thus their outputs generally require manual human
verification for high-stakes applications, which can be time-consuming and
difficult. This paper proposes symbolically grounded generation (SymGen) as a
simple approach for enabling easier validation of an LLM's output. SymGen
prompts an LLM to interleave its regular output text with explicit symbolic
references to fields present in some conditioning data (e.g., a table in JSON
format). The references can be used to display the provenance of different
spans of text in the generation, reducing the effort required for manual
verification. Across data-to-text and question answering experiments, we find
that LLMs are able to directly output text that makes use of symbolic
references while maintaining fluency and accuracy.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09189" title="Abstract">arXiv:2311.09189</a> [<a href="/pdf/2311.09189" title="Download PDF">pdf</a>, <a href="/format/2311.09189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for  Mental Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Haoan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mengyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K+Q">Kenny Q. Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, there has been a growing interest in utilizing large language
models (LLMs) in mental health research, with studies showcasing their
remarkable capabilities, such as disease detection. However, there is currently
a lack of a comprehensive benchmark for evaluating the capability of LLMs in
this domain. Therefore, we address this gap by introducing the first
comprehensive benchmark tailored to the unique characteristics of the mental
health domain. This benchmark encompasses a total of six sub-tasks, covering
three dimensions, to systematically assess the capabilities of LLMs in the
realm of mental health. We have designed corresponding concise prompts for each
sub-task. And we comprehensively evaluate a total of eight advanced LLMs using
our benchmark. Experiment results not only demonstrate significant room for
improvement in current LLMs concerning mental health but also unveil potential
directions for future model optimization.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09190" title="Abstract">arXiv:2311.09190</a> [<a href="/pdf/2311.09190" title="Download PDF">pdf</a>, <a href="/ps/2311.09190" title="Download PostScript">ps</a>, <a href="/format/2311.09190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Computation of the Gaussian Rate-Distortion-Perception Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Serra%2C+G">Giuseppe Serra</a>, 
<a href="/search/cs?searchtype=author&query=Stavrou%2C+P+A">Photios A. Stavrou</a>, 
<a href="/search/cs?searchtype=author&query=Kountouris%2C+M">Marios Kountouris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted for journal publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">In this paper, we study the computation of the rate-distortion-perception
function (RDPF) for a multivariate Gaussian source under mean squared error
(MSE) distortion and, respectively, Kullback-Leibler divergence, geometric
Jensen-Shannon divergence, squared Hellinger distance, and squared
Wasserstein-2 distance perception metrics. To this end, we first characterize
the analytical bounds of the scalar Gaussian RDPF for the aforementioned
divergence functions, also providing the RDPF-achieving forward "test-channel"
realization. Focusing on the multivariate case, we establish that, for
tensorizable distortion and perception metrics, the optimal solution resides on
the vector space spanned by the eigenvector of the source covariance matrix.
Consequently, the multivariate optimization problem can be expressed as a
function of the scalar Gaussian RDPFs of the source marginals, constrained by
global distortion and perception levels. Leveraging this characterization, we
design an alternating minimization scheme based on the block nonlinear
Gauss-Seidel method, which optimally solves the problem while identifying the
Gaussian RDPF-achieving realization. Furthermore, the associated algorithmic
embodiment is provided, as well as the convergence and the rate of convergence
characterization. Lastly, for the "perfect realism" regime, the analytical
solution for the multivariate Gaussian RDPF is obtained. We corroborate our
results with numerical simulations and draw connections to existing results.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09191" title="Abstract">arXiv:2311.09191</a> [<a href="/pdf/2311.09191" title="Download PDF">pdf</a>, <a href="/format/2311.09191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Aligned CLIP for Few-shot Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gondal%2C+M+W">Muhammad Waleed Gondal</a>, 
<a href="/search/cs?searchtype=author&query=Gast%2C+J">Jochen Gast</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz%2C+I+A">Inigo Alonso Ruiz</a>, 
<a href="/search/cs?searchtype=author&query=Droste%2C+R">Richard Droste</a>, 
<a href="/search/cs?searchtype=author&query=Macri%2C+T">Tommaso Macri</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Suren Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Staudigl%2C+L">Luitpold Staudigl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large vision-language representation learning models like CLIP have
demonstrated impressive performance for zero-shot transfer to downstream tasks
while largely benefiting from inter-modal (image-text) alignment via
contrastive objectives. This downstream performance can further be enhanced by
full-scale fine-tuning which is often compute intensive, requires large
labelled data, and can reduce out-of-distribution (OOD) robustness.
Furthermore, sole reliance on inter-modal alignment might overlook the rich
information embedded within each individual modality. In this work, we
introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain
Aligned CLIP (DAC), which improves both intra-modal (image-image) and
inter-modal alignment on target distributions without fine-tuning the main
model. For intra-modal alignment, we introduce a lightweight adapter that is
specifically trained with an intra-modal contrastive objective. To improve
inter-modal alignment, we introduce a simple framework to modulate the
precomputed class text embeddings. The proposed few-shot fine-tuning framework
is computationally efficient, robust to distribution shifts, and does not alter
CLIP's parameters. We study the effectiveness of DAC by benchmarking on 11
widely used image classification tasks with consistent improvements in 16-shot
classification upon strong baselines by about 2.3% and demonstrate competitive
performance on 4 OOD robustness benchmarks.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09193" title="Abstract">arXiv:2311.09193</a> [<a href="/pdf/2311.09193" title="Download PDF">pdf</a>, <a href="/format/2311.09193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yifan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengchuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wenhan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Oguz%2C+B">Barlas Oguz</a>, 
<a href="/search/cs?searchtype=author&query=Gee%2C+J+C">James C. Gee</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Yixin Nie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The study explores the effectiveness of the Chain-of-Thought approach, known
for its proficiency in language tasks by breaking them down into sub-tasks and
intermediate steps, in improving vision-language tasks that demand
sophisticated perception and reasoning. We present the "Description then
Decision" strategy, which is inspired by how humans process signals. This
strategy significantly improves probing task performance by 50%, establishing
the groundwork for future research on reasoning paradigms in complex
vision-language tasks.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09194" title="Abstract">arXiv:2311.09194</a> [<a href="/pdf/2311.09194" title="Download PDF">pdf</a>, <a href="/format/2311.09194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Priming Demonstrates Abstract Grammatical Representations in  Multilingual Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michaelov%2C+J+A">James A. Michaelov</a>, 
<a href="/search/cs?searchtype=author&query=Arnett%2C+C">Catherine Arnett</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+T+A">Tyler A. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Bergen%2C+B+K">Benjamin K. Bergen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Abstract grammatical knowledge - of parts of speech and grammatical patterns
- is key to the capacity for linguistic generalization in humans. But how
abstract is grammatical knowledge in large language models? In the human
literature, compelling evidence for grammatical abstraction comes from
structural priming. A sentence that shares the same grammatical structure as a
preceding sentence is processed and produced more readily. Because confounds
exist when using stimuli in a single language, evidence of abstraction is even
more compelling from crosslingual structural priming, where use of a syntactic
structure in one language primes an analogous structure in another language. We
measure crosslingual structural priming in large language models, comparing
model behavior to human experimental results from eight crosslingual
experiments covering six languages, and four monolingual structural priming
experiments in three non-English languages. We find evidence for abstract
monolingual and crosslingual grammatical representations in the models that
function similarly to those found in humans. These results demonstrate that
grammatical representations in multilingual language models are not only
similar across languages, but they can causally influence text produced in
different languages.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09195" title="Abstract">arXiv:2311.09195</a> [<a href="/pdf/2311.09195" title="Download PDF">pdf</a>, <a href="/format/2311.09195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Curriculum Generation for Autonomous Reinforcement  Learning without Task-Specific Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sang-Hyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Seung-Woo Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">A significant bottleneck in applying current reinforcement learning
algorithms to real-world scenarios is the need to reset the environment between
every episode. This reset process demands substantial human intervention,
making it difficult for the agent to learn continuously and autonomously.
Several recent works have introduced autonomous reinforcement learning (ARL)
algorithms that generate curricula for jointly training reset and forward
policies. While their curricula can reduce the number of required manual resets
by taking into account the agent's learning progress, they rely on
task-specific knowledge, such as predefined initial states or reset reward
functions. In this paper, we propose a novel ARL algorithm that can generate a
curriculum adaptive to the agent's learning progress without task-specific
knowledge. Our curriculum empowers the agent to autonomously reset to diverse
and informative initial states. To achieve this, we introduce a success
discriminator that estimates the success probability from each initial state
when the agent follows the forward policy. The success discriminator is trained
with relabeled transitions in a self-supervised manner. Our experimental
results demonstrate that our ARL algorithm can generate an adaptive curriculum
and enable the agent to efficiently bootstrap to solve sparse-reward maze
navigation tasks, outperforming baselines with significantly fewer manual
resets.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09196" title="Abstract">arXiv:2311.09196</a> [<a href="/pdf/2311.09196" title="Download PDF">pdf</a>, <a href="/format/2311.09196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding polarised communities and tracking information diffusion on  Twitter: The Irish Abortion Referendum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pena%2C+C">Caroline Pena</a>, 
<a href="/search/cs?searchtype=author&query=MacCarron%2C+P">P&#xe1;draig MacCarron</a>, 
<a href="/search/cs?searchtype=author&query=O%27Sullivan%2C+D+J+P">David J.P. O&#x27;Sullivan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 4 appendices, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">The analysis of social networks enables the understanding of social
interactions, polarisation of ideas, and the spread of information and
therefore plays an important role in society. We use Twitter data - as it is a
popular venue for the expression of opinion and dissemination of information -
to identify opposing sides of a debate and, importantly, to observe how
information spreads between these groups in our current polarised climate.
<br />To achieve this, we collected over 688,000 Tweets from the Irish Abortion
Referendum of 2018 to build a conversation network from users mentions with
sentiment-based homophily. From this network, community detection methods allow
us to isolate yes- or no-aligned supporters with high accuracy (90.9%). We
supplement this by tracking how information cascades spread via over 31,000
retweet-cascades. We found that very little information spread between
polarised communities. This provides a valuable methodology for extracting and
studying information diffusion on large networks by isolating ideologically
polarised groups and exploring the propagation of information within and
between these groups.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09197" title="Abstract">arXiv:2311.09197</a> [<a href="/pdf/2311.09197" title="Download PDF">pdf</a>, <a href="/ps/2311.09197" title="Download PostScript">ps</a>, <a href="/format/2311.09197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Approach to Learning Ising Models: Beyond Independence and  Bounded Width
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaitonde%2C+J">Jason Gaitonde</a>, 
<a href="/search/cs?searchtype=author&query=Mossel%2C+E">Elchanan Mossel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">We revisit the problem of efficiently learning the underlying parameters of
Ising models from data. Current algorithmic approaches achieve essentially
optimal sample complexity when given i.i.d. samples from the stationary measure
and the underlying model satisfies "width" bounds on the total $\ell_1$
interaction involving each node. We show that a simple existing approach based
on node-wise logistic regression provably succeeds at recovering the underlying
model in several new settings where these assumptions are violated:
<br />(1) Given dynamically generated data from a wide variety of local Markov
chains, like block or round-robin dynamics, logistic regression recovers the
parameters with optimal sample complexity up to $\log\log n$ factors. This
generalizes the specialized algorithm of Bresler, Gamarnik, and Shah [IEEE
Trans. Inf. Theory'18] for structure recovery in bounded degree graphs from
Glauber dynamics.
<br />(2) For the Sherrington-Kirkpatrick model of spin glasses, given
$\mathsf{poly}(n)$ independent samples, logistic regression recovers the
parameters in most of the known high-temperature regime via a simple reduction
to weaker structural properties of the measure. This improves on recent work of
Anari, Jain, Koehler, Pham, and Vuong [ArXiv'23] which gives distribution
learning at higher temperature.
<br />(3) As a simple byproduct of our techniques, logistic regression achieves an
exponential improvement in learning from samples in the M-regime of data
considered by Dutt, Lokhov, Vuffray, and Misra [ICML'21] as well as novel
guarantees for learning from the adversarial Glauber dynamics of Chin, Moitra,
Mossel, and Sandon [ArXiv'23].
<br />Our approach thus significantly generalizes the elegant analysis of Wu,
Sanghavi, and Dimakis [Neurips'19] without any algorithmic modification.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09198" title="Abstract">arXiv:2311.09198</a> [<a href="/pdf/2311.09198" title="Download PDF">pdf</a>, <a href="/format/2311.09198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Never Lost in the Middle: Improving Large Language Models via Attention  Strengthening Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Junqing%2C+H">He Junqing</a>, 
<a href="/search/cs?searchtype=author&query=Kunhao%2C+P">Pan Kunhao</a>, 
<a href="/search/cs?searchtype=author&query=Xiaoqun%2C+D">Dong Xiaoqun</a>, 
<a href="/search/cs?searchtype=author&query=Zhuoyang%2C+S">Song Zhuoyang</a>, 
<a href="/search/cs?searchtype=author&query=Yibo%2C+L">Liu Yibo</a>, 
<a href="/search/cs?searchtype=author&query=Yuxin%2C+L">Liang Yuxin</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+W">Wang Hao</a>, 
<a href="/search/cs?searchtype=author&query=Qianguo%2C+S">Sun Qianguo</a>, 
<a href="/search/cs?searchtype=author&query=Songxin%2C+Z">Zhang Songxin</a>, 
<a href="/search/cs?searchtype=author&query=Zejian%2C+X">Xie Zejian</a>, 
<a href="/search/cs?searchtype=author&query=Jiaxing%2C+Z">Zhang Jiaxing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While large language models (LLMs) are equipped with longer text input
capabilities than before, they are struggling to seek correct information in
long contexts. The "lost in the middle" problem challenges most LLMs, referring
to the dramatic decline in accuracy when correct information is located in the
middle. To overcome this crucial issue, this paper proposes to enhance the
information searching and reflection ability of LLMs in long contexts via
specially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).
Following these tasks, our model excels in focusing more precisely on the
desired information. Experimental results show substantial improvement in
Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%
absolute gain in shuffled settings, by 21.5% in passage retrieval task. We
release our model, Ziya-Reader to promote related research in the community.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09204" title="Abstract">arXiv:2311.09204</a> [<a href="/pdf/2311.09204" title="Download PDF">pdf</a>, <a href="/format/2311.09204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion-Eval: Integrating Evaluators with LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+L">Lei Shu</a>, 
<a href="/search/cs?searchtype=author&query=Wichers%2C+N">Nevan Wichers</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Liangchen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jindong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lei Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Evaluating Large Language Models (LLMs) is a complex task, especially
considering the intricacies of natural language understanding and the
expectations for high-level reasoning. Traditional evaluations typically lean
on human-based, model-based, or automatic-metrics-based paradigms, each with
its own advantages and shortcomings. We introduce "Fusion-Eval", a system that
employs LLMs not solely for direct evaluations, but to skillfully integrate
insights from diverse evaluators. This gives Fusion-Eval flexibility, enabling
it to work effectively across diverse tasks and make optimal use of multiple
references. In testing on the SummEval dataset, Fusion-Eval achieved a Spearman
correlation of 0.96, outperforming other evaluators. The success of Fusion-Eval
underscores the potential of LLMs to produce evaluations that closely align
human perspectives, setting a new standard in the field of LLM evaluation.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09205" title="Abstract">arXiv:2311.09205</a> [<a href="/pdf/2311.09205" title="Download PDF">pdf</a>, <a href="/format/2311.09205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Is Multilinguality a Curse? Language Modeling for 250 High- and  Low-Resource Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+T+A">Tyler A. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Arnett%2C+C">Catherine Arnett</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhuowen Tu</a>, 
<a href="/search/cs?searchtype=author&query=Bergen%2C+B+K">Benjamin K. Bergen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multilingual language models are widely used to extend NLP systems to
low-resource languages. However, concrete evidence for the effects of
multilinguality on language modeling performance in individual languages
remains scarce. Here, we pre-train over 10,000 monolingual and multilingual
language models for over 250 languages, including multiple language families
that are under-studied in NLP. We assess how language modeling performance in
each language varies as a function of (1) monolingual dataset size, (2) added
multilingual dataset size, (3) linguistic similarity of the added languages,
and (4) model size (up to 45M parameters). We find that in moderation, adding
multilingual data improves low-resource language modeling performance, similar
to increasing low-resource dataset sizes by up to 33%. Improvements depend on
the syntactic similarity of the added multilingual data, with marginal
additional effects of vocabulary overlap. However, high-resource languages
consistently perform worse in multilingual pre-training scenarios. As dataset
sizes increase, adding multilingual data begins to hurt performance for both
low-resource and high-resource languages, likely due to limited model capacity
(the "curse of multilinguality"). These results suggest that massively
multilingual pre-training may not be optimal for any languages involved, but
that more targeted models can significantly improve performance.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09206" title="Abstract">arXiv:2311.09206</a> [<a href="/pdf/2311.09206" title="Download PDF">pdf</a>, <a href="/format/2311.09206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TableLlama: Towards Open Large Generalist Models for Tables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianshu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifei Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Semi-structured tables are ubiquitous. There has been a variety of tasks that
aim to automatically interpret, augment, and query tables. Current methods
often require pretraining on tables or special model architecture design, are
restricted to specific table types, or have simplifying assumptions about
tables and tasks. This paper makes the first step towards developing
open-source large language models (LLMs) as generalists for a diversity of
table-based tasks. Towards that end, we construct TableInstruct, a new dataset
with a variety of realistic tables and tasks, for instruction tuning and
evaluating LLMs. We further develop the first open-source generalist model for
tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the
long context challenge. We experiment under both in-domain setting and
out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves
comparable or better performance than the SOTA for each task, despite the
latter often has task-specific design. On 6 out-of-domain datasets, it achieves
6-48 absolute point gains compared with the base model, showing that training
on TableInstruct enhances the model's generalizability. We will open-source our
dataset and trained model to boost future work on developing open generalist
models for tables.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09210" title="Abstract">arXiv:2311.09210</a> [<a href="/pdf/2311.09210" title="Download PDF">pdf</a>, <a href="/format/2311.09210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaoman Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kaixin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Retrieval-augmented language models (RALMs) represent a substantial
advancement in the capabilities of large language models, notably in reducing
factual hallucination by leveraging external knowledge sources. However, the
reliability of the retrieved information is not always guaranteed. The
retrieval of irrelevant data can lead to misguided responses, and potentially
causing the model to overlook its inherent knowledge, even when it possesses
adequate information to address the query. Moreover, standard RALMs often
struggle to assess whether they possess adequate knowledge, both intrinsic and
retrieved, to provide an accurate answer. In situations where knowledge is
lacking, these systems should ideally respond with "unknown" when the answer is
unattainable. In response to these challenges, we introduces Chain-of-Noting
(CoN), a novel approach aimed at improving the robustness of RALMs in facing
noisy, irrelevant documents and in handling unknown scenarios. The core idea of
CoN is to generate sequential reading notes for retrieved documents, enabling a
thorough evaluation of their relevance to the given question and integrating
this information to formulate the final answer. We employed ChatGPT to create
training data for CoN, which was subsequently trained on an LLaMa-2 7B model.
Our experiments across four open-domain QA benchmarks show that RALMs equipped
with CoN significantly outperform standard RALMs. Notably, CoN achieves an
average improvement of +7.9 in EM score given entirely noisy retrieved
documents and +10.5 in rejection rates for real-time questions that fall
outside the pre-training knowledge scope.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09211" title="Abstract">arXiv:2311.09211</a> [<a href="/pdf/2311.09211" title="Download PDF">pdf</a>, <a href="/ps/2311.09211" title="Download PostScript">ps</a>, <a href="/format/2311.09211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digitally reproducing the artistic style of XVI century artist Antonio  Campelo in Alegoria Prudencia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+J+F">Joao Fradinho Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+J+M">Joao Madeiras Pereira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">In this work, the artistic style of the sixteenth century Portuguese artist
Ant\'onio Campelo in Alegoria \`a Prud\^encia is analyzed in order to create a
computational tool that allows one to transform any 3D digital sculpture model
into an image that resembles the modeled style. From this analysis the problem
is divided into two parts: detection and drawing of contour lines and the
shading of the scene. Several techniques from Non Photorealistic Rendering
(NPR) and from Photorealistic Rendering that can resolve the problem are
presented and, based on this study, a possible solution is presented. Each
modeled rendering component is then analyzed using image based methods against
the proposed artistic style and parameters are adjusted for a closer match. In
the final stage a group of people was asked to answer a questionnaire where the
similarity between the renderings of different objects and the original style
was classified according to their personal opinion. One of our findings is that
although the source 3D objects cannot be readily found for a direct comparison,
nor can the paper medium with centuries old damage be the same, the comparison
of sub -parts of both images of the same topology was still possible validating
our method and discarding other styles from the comparison.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09212" title="Abstract">arXiv:2311.09212</a> [<a href="/pdf/2311.09212" title="Download PDF">pdf</a>, <a href="/format/2311.09212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllable Text Summarization: Unraveling Challenges, Approaches, and  Prospects -- A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Urlana%2C+A">Ashok Urlana</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+P">Pruthwik Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+T">Tathagato Roy</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+R">Rahul Mishra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generic text summarization approaches often fail to address the specific
intent and needs of individual users. Recently, scholarly attention has turned
to the development of summarization methods that are more closely tailored and
controlled to align with specific objectives and user needs. While a growing
corpus of research is devoted towards a more controllable summarization, there
is no comprehensive survey available that thoroughly explores the diverse
controllable aspects or attributes employed in this context, delves into the
associated challenges, and investigates the existing solutions. In this survey,
we formalize the Controllable Text Summarization (CTS) task, categorize
controllable aspects according to their shared characteristics and objectives,
and present a thorough examination of existing methods and datasets within each
category. Moreover, based on our findings, we uncover limitations and research
gaps, while also delving into potential solutions and future directions for
CTS.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09213" title="Abstract">arXiv:2311.09213</a> [<a href="/pdf/2311.09213" title="Download PDF">pdf</a>, <a href="/format/2311.09213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRIM: GRaph-based Interactive narrative visualization for gaMes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leandro%2C+J">Jorge Leandro</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+S">Sudha Rao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Michael Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weijia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jojic%2C+N">Nebosja Jojic</a>, 
<a href="/search/cs?searchtype=author&query=Brockett%2C+C">Chris Brockett</a>, 
<a href="/search/cs?searchtype=author&query=Dolan%2C+B">Bill Dolan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The
narratives of these may take years to write and typically involve a large
creative team. In this work, we demonstrate the potential of large generative
text models to assist this process. \textbf{GRIM}, a prototype
\textbf{GR}aph-based \textbf{I}nteractive narrative visualization system for
ga\textbf{M}es, generates a rich narrative graph with branching storylines that
match a high-level narrative description and constraints provided by the
designer. Game designers can interactively edit the graph by automatically
generating new sub-graphs that fit the edits within the original narrative and
constraints. We illustrate the use of \textbf{GRIM} in conjunction with GPT-4,
generating branching narratives for four well-known stories with different
contextual constraints.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09214" title="Abstract">arXiv:2311.09214</a> [<a href="/pdf/2311.09214" title="Download PDF">pdf</a>, <a href="/format/2311.09214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind&#x27;s Mirror: Distilling Self-Evaluation Capability and Comprehensive  Thinking from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weize Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guocong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bang Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongxia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jintai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have achieved remarkable advancements in the
field of natural language processing. However, the sheer scale and
computational demands of these models present formidable challenges when
considering their practical deployment in resource-constrained contexts. While
techniques such as chain-of-thought (CoT) distillation have displayed promise
in distilling LLMs into small language models (SLMs), there is a risk that
distilled SLMs may still carry over flawed reasoning or hallucinations
inherited from their LLM counterparts. To address these issues, we propose a
twofold methodology: First, we introduce a novel method for distilling the
self-evaluation capability inherent in LLMs into SLMs, which aims to mitigate
the adverse effects of erroneous reasoning and reduce hallucinations. Second,
we advocate for a comprehensive distillation process that incorporates multiple
distinct chain-of-thought and self-evaluation paradigms and ensures a more
holistic and robust knowledge transfer into SLMs. Experiments on three NLP
benchmarks demonstrate that our method significantly improves the performance
of distilled SLMs and sheds light on the path towards developing smaller models
closely aligned with human cognition.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09215" title="Abstract">arXiv:2311.09215</a> [<a href="/pdf/2311.09215" title="Download PDF">pdf</a>, <a href="/format/2311.09215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vishniakov%2C+K">Kirill Vishniakov</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqiang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Modern computer vision offers a great variety of models to practitioners, and
selecting a model from multiple options for specific applications can be
challenging. Conventionally, competing model architectures and training
protocols are compared by their classification accuracy on ImageNet. However,
this single metric does not fully capture performance nuances critical for
specialized tasks. In this work, we conduct an in-depth comparative analysis of
model behaviors beyond ImageNet accuracy, for both ConvNet and Vision
Transformer architectures, each across supervised and CLIP training paradigms.
Although our selected models have similar ImageNet accuracies and compute
requirements, we find that they differ in many other aspects: types of
mistakes, output calibration, transferability, and feature invariance, among
others. This diversity in model characteristics, not captured by traditional
metrics, highlights the need for more nuanced analysis when choosing among
different models. Our code is available at
https://github.com/kirill-vish/Beyond-INet.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09216" title="Abstract">arXiv:2311.09216</a> [<a href="/pdf/2311.09216" title="Download PDF">pdf</a>, <a href="/format/2311.09216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Translation capabilities of Large Language Models involving  English and Indian Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mujadia%2C+V">Vandan Mujadia</a>, 
<a href="/search/cs?searchtype=author&query=Urlana%2C+A">Ashok Urlana</a>, 
<a href="/search/cs?searchtype=author&query=Bhaskar%2C+Y">Yash Bhaskar</a>, 
<a href="/search/cs?searchtype=author&query=Pavani%2C+P+A">Penumalla Aditya Pavani</a>, 
<a href="/search/cs?searchtype=author&query=Shravya%2C+K">Kukkapalli Shravya</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+P">Parameswari Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+D+M">Dipti Misra Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative Large Language Models (LLMs) have achieved remarkable advancements
in various NLP tasks. In this work, our aim is to explore the multilingual
capabilities of large language models by using machine translation as a task
involving English and 22 Indian languages. We first investigate the translation
capabilities of raw large language models, followed by exploring the in-context
learning capabilities of the same raw models. We fine-tune these large language
models using parameter efficient fine-tuning methods such as LoRA and
additionally with full fine-tuning. Through our study, we have identified the
best performing large language model for the translation task involving LLMs,
which is based on LLaMA.
<br />Our results demonstrate significant progress, with average BLEU scores of
13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99,
42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for
English to Indian languages on IN22 (conversational), IN22 (general),
flores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for
Indian languages to English, we achieved average BLEU scores of 14.03, 16.65,
16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51,
and 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational),
IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets.
Overall, our findings highlight the potential and strength of large language
models for machine translation capabilities, including for languages that are
currently underrepresented in LLMs.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09217" title="Abstract">arXiv:2311.09217</a> [<a href="/pdf/2311.09217" title="Download PDF">pdf</a>, <a href="/format/2311.09217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yinghao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Hao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+F">Fujun Luan</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+S">Sai Bi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiahao Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zifan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sunkavalli%2C+K">Kalyan Sunkavalli</a>, 
<a href="/search/cs?searchtype=author&query=Wetzstein%2C+G">Gordon Wetzstein</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zexiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://justimyhxu.github.io/projects/dmv3d/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose \textbf{DMV3D}, a novel 3D generation approach that uses a
transformer-based 3D large reconstruction model to denoise multi-view
diffusion. Our reconstruction model incorporates a triplane NeRF representation
and can denoise noisy multi-view images via NeRF reconstruction and rendering,
achieving single-stage 3D generation in $\sim$30s on single A100 GPU. We train
\textbf{DMV3D} on large-scale multi-view image datasets of highly diverse
objects using only image reconstruction losses, without accessing 3D assets. We
demonstrate state-of-the-art results for the single-image reconstruction
problem where probabilistic modeling of unseen object parts is required for
generating diverse reconstructions with sharp textures. We also show
high-quality text-to-3D generation results outperforming previous 3D diffusion
models. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09221" title="Abstract">arXiv:2311.09221</a> [<a href="/pdf/2311.09221" title="Download PDF">pdf</a>, <a href="/format/2311.09221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-Image 3D Human Digitization with Shape-Guided Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AlBahar%2C+B">Badour AlBahar</a>, 
<a href="/search/cs?searchtype=author&query=Saito%2C+S">Shunsuke Saito</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+H">Hung-Yu Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Changil Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kopf%2C+J">Johannes Kopf</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jia-Bin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023. Project website: <a href="https://human-sgd.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present an approach to generate a 360-degree view of a person with a
consistent, high-resolution appearance from a single input image. NeRF and its
variants typically require videos or images from different viewpoints. Most
existing approaches taking monocular input either rely on ground-truth 3D scans
for supervision or lack 3D consistency. While recent 3D generative models show
promise of 3D consistent human digitization, these approaches do not generalize
well to diverse clothing appearances, and the results lack photorealism. Unlike
existing work, we utilize high-capacity 2D diffusion models pretrained for
general image synthesis tasks as an appearance prior of clothed humans. To
achieve better 3D consistency while retaining the input identity, we
progressively synthesize multiple views of the human in the input image by
inpainting missing regions with shape-guided diffusion conditioned on
silhouette and surface normal. We then fuse these synthesized multi-view images
via inverse rendering to obtain a fully textured high-resolution 3D mesh of the
given person. Experiments show that our approach outperforms prior methods and
achieves photorealistic 360-degree synthesis of a wide range of clothed humans
with complex textures from a single image.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Thu, 16 Nov 23</h3>
<dl>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.01514" title="Abstract">arXiv:2205.01514</a> (cross-list from quant-ph) [<a href="/pdf/2205.01514" title="Download PDF">pdf</a>, <a href="/format/2205.01514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tunable Quantum Neural Networks in the QPAC-Learning Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ngoc%2C+V+P">Viet Pham Ngoc</a> (Imperial College London), 
<a href="/search/quant-ph?searchtype=author&query=Tuckey%2C+D">David Tuckey</a> (Imperial College London), 
<a href="/search/quant-ph?searchtype=author&query=Wiklicky%2C+H">Herbert Wiklicky</a> (Imperial College London)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 221-235
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we investigate the performances of tunable quantum neural
networks in the Quantum Probably Approximately Correct (QPAC) learning
framework. Tunable neural networks are quantum circuits made of
multi-controlled X gates. By tuning the set of controls these circuits are able
to approximate any Boolean functions. This architecture is particularly suited
to be used in the QPAC-learning framework as it can handle the superposition
produced by the oracle. In order to tune the network so that it can approximate
a target concept, we have devised and implemented an algorithm based on
amplitude amplification. The numerical results show that this approach can
efficiently learn concepts from a simple class.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03814" title="Abstract">arXiv:2311.03814</a> (cross-list from econ.TH) [<a href="/pdf/2311.03814" title="Download PDF">pdf</a>, <a href="/format/2311.03814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ultimatum game: regret or fairness?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Aleksanyan%2C+L+H">Lida H. Aleksanyan</a>, 
<a href="/search/econ?searchtype=author&query=Allahverdyan%2C+A+E">Armen E. Allahverdyan</a>, 
<a href="/search/econ?searchtype=author&query=Bardakhchyan%2C+V+G">Vardan G. Bardakhchyan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">In the ultimatum game, the challenge is to explain why responders reject
non-zero offers thereby defying classical rationality. Fairness and related
notions have been the main explanations so far. We explain this rejection
behavior via the following principle: if the responder regrets less about
losing the offer than the proposer regrets not offering the best option, the
offer is rejected. This principle qualifies as a rational punishing behavior
and it replaces the experimentally falsified classical rationality (the subgame
perfect Nash equilibrium) that leads to accepting any non-zero offer. The
principle is implemented via the transitive regret theory for probabilistic
lotteries. The expected utility implementation is a limiting case of this. We
show that several experimental results normally prescribed to fairness and
intent-recognition can be given an alternative explanation via rational
punishment; e.g. the comparison between "fair" and "superfair", the behavior
under raising the stakes etc. Hence we also propose experiments that can
distinguish these two scenarios (fairness versus regret-based punishment). They
assume different utilities for the proposer and responder. We focus on the
mini-ultimatum version of the game and also show how it can emerge from a more
general setup of the game.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08417" title="Abstract">arXiv:2311.08417</a> (cross-list from eess.IV) [<a href="/pdf/2311.08417" title="Download PDF">pdf</a>, <a href="/format/2311.08417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image complexity based fMRI-BOLD visual network categorization across  visual datasets using topological descriptors and deep-hybrid learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bhattacharya%2C+D">Debanjali Bhattacharya</a>, 
<a href="/search/eess?searchtype=author&query=Sinha%2C+N">Neelam Sinha</a>, 
<a href="/search/eess?searchtype=author&query=R.%2C+Y">Yashwanth R.</a>, 
<a href="/search/eess?searchtype=author&query=Chattopadhyay%2C+A">Amit Chattopadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Signal Processing (eess.SP); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">This study proposes a new approach that investigates differences in
topological characteristics of visual networks, which are constructed using
fMRI BOLD time-series corresponding to visual datasets of COCO, ImageNet, and
SUN. A publicly available BOLD5000 dataset is utilized that contains fMRI scans
while viewing 5254 images of diverse complexities. The objective of this study
is to examine how network topology differs in response to distinct visual
stimuli from these visual datasets. To achieve this, 0- and 1-dimensional
persistence diagrams are computed for each visual network representing COCO,
ImageNet, and SUN. For extracting suitable features from topological
persistence diagrams, K-means clustering is executed. The extracted K-means
cluster features are fed to a novel deep-hybrid model that yields accuracy in
the range of 90%-95% in classifying these visual networks. To understand
vision, this type of visual network categorization across visual datasets is
important as it captures differences in BOLD signals while perceiving images
with different contexts and complexities. Furthermore, distinctive topological
patterns of visual network associated with each dataset, as revealed from this
study, could potentially lead to the development of future neuroimaging
biomarkers for diagnosing visual processing disorders like visual agnosia or
prosopagnosia, and tracking changes in visual cognition over time.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08421" title="Abstract">arXiv:2311.08421</a> (cross-list from physics.ao-ph) [<a href="/pdf/2311.08421" title="Download PDF">pdf</a>, <a href="/format/2311.08421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surrogate Neural Networks to Estimate Parametric Sensitivity of Ocean  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sun%2C+Y">Yixuan Sun</a>, 
<a href="/search/physics?searchtype=author&query=Cucuzzella%2C+E">Elizabeth Cucuzzella</a>, 
<a href="/search/physics?searchtype=author&query=Brus%2C+S">Steven Brus</a>, 
<a href="/search/physics?searchtype=author&query=Narayanan%2C+S+H+K">Sri Hari Krishna Narayanan</a>, 
<a href="/search/physics?searchtype=author&query=Nadiga%2C+B">Balu Nadiga</a>, 
<a href="/search/physics?searchtype=author&query=Van+Roekel%2C+L">Luke Van Roekel</a>, 
<a href="/search/physics?searchtype=author&query=H%C3%BCckelheim%2C+J">Jan H&#xfc;ckelheim</a>, 
<a href="/search/physics?searchtype=author&query=Madireddy%2C+S">Sandeep Madireddy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Modeling is crucial to understanding the effect of greenhouse gases, warming,
and ice sheet melting on the ocean. At the same time, ocean processes affect
phenomena such as hurricanes and droughts. Parameters in the models that cannot
be physically measured have a significant effect on the model output. For an
idealized ocean model, we generated perturbed parameter ensemble data and
trained surrogate neural network models. The neural surrogates accurately
predicted the one-step forward dynamics, of which we then computed the
parametric sensitivity.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08426" title="Abstract">arXiv:2311.08426</a> (cross-list from eess.IV) [<a href="/pdf/2311.08426" title="Download PDF">pdf</a>, <a href="/ps/2311.08426" title="Download PostScript">ps</a>, <a href="/format/2311.08426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Contact Breathing Rate Detection Using Optical Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Maxwell%2C+R">Robyn Maxwell</a>, 
<a href="/search/eess?searchtype=author&query=Hanley%2C+T">Timothy Hanley</a>, 
<a href="/search/eess?searchtype=author&query=Golden%2C+D">Dara Golden</a>, 
<a href="/search/eess?searchtype=author&query=Andonie%2C+A">Adara Andonie</a>, 
<a href="/search/eess?searchtype=author&query=Lemley%2C+J">Joseph Lemley</a>, 
<a href="/search/eess?searchtype=author&query=Parsi%2C+A">Ashkan Parsi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of Irish Machine Vision and Image Processing Conference 2023 (IMVIP2023), Galway, Ireland, August 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Breathing rate is a vital health metric that is an invaluable indicator of
the overall health of a person. In recent years, the non-contact measurement of
health signals such as breathing rate has been a huge area of development, with
a wide range of applications from telemedicine to driver monitoring systems.
This paper presents an investigation into a method of non-contact breathing
rate detection using a motion detection algorithm, optical flow. Optical flow
is used to successfully measure breathing rate by tracking the motion of
specific points on the body. In this study, the success of optical flow when
using different sets of points is evaluated. Testing shows that both chest and
facial movement can be used to determine breathing rate but to different
degrees of success. The chest generates very accurate signals, with an RMSE of
0.63 on the tested videos. Facial points can also generate reliable signals
when there is minimal head movement but are much more vulnerable to noise
caused by head/body movements. These findings highlight the potential of
optical flow as a non-invasive method for breathing rate detection and
emphasize the importance of selecting appropriate points to optimize accuracy.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08428" title="Abstract">arXiv:2311.08428</a> (cross-list from q-bio.QM) [<a href="/pdf/2311.08428" title="Download PDF">pdf</a>, <a href="/format/2311.08428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Phenotyping of Non-Alcoholic Fatty Liver Disease Patients with  Genetic Factors for Insights into the Complex Disease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Priya%2C+T+S">Tahmina Sultana Priya</a>, 
<a href="/search/q-bio?searchtype=author&query=Leng%2C+F">Fan Leng</a>, 
<a href="/search/q-bio?searchtype=author&query=Luehrs%2C+A+C">Anthony C. Luehrs</a>, 
<a href="/search/q-bio?searchtype=author&query=Klee%2C+E+W">Eric W. Klee</a>, 
<a href="/search/q-bio?searchtype=author&query=Allen%2C+A+M">Alina M. Allen</a>, 
<a href="/search/q-bio?searchtype=author&query=Lazaridis%2C+K+N">Konstantinos N. Lazaridis</a>, 
<a href="/search/q-bio?searchtype=author&query=Danfeng">Danfeng</a> (Daphne)Yao, 
<a href="/search/q-bio?searchtype=author&query=Tian%2C+S">Shulan Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Non-alcoholic fatty liver disease (NAFLD) is a prevalent chronic liver
disorder characterized by the excessive accumulation of fat in the liver in
individuals who do not consume significant amounts of alcohol, including risk
factors like obesity, insulin resistance, type 2 diabetes, etc. We aim to
identify subgroups of NAFLD patients based on demographic, clinical, and
genetic characteristics for precision medicine. The genomic and phenotypic data
(3,408 cases and 4,739 controls) for this study were gathered from participants
in Mayo Clinic Tapestry Study (IRB#19-000001) and their electric health
records, including their demographic, clinical, and comorbidity data, and the
genotype information through whole exome sequencing performed at Helix using
the Exome+$^\circledR$ Assay according to standard procedure
$\href{https://www.helix.com/}{(www.helix.com)}$. Factors highly relevant to
NAFLD were determined by the chi-square test and stepwise backward-forward
regression model. Latent class analysis (LCA) was performed on NAFLD cases
using significant indicator variables to identify subgroups. The optimal
clustering revealed 5 latent subgroups from 2,013 NAFLD patients (mean age 60.6
years and 62.1% women), while a polygenic risk score based on 6
single-nucleotide polymorphism (SNP) variants and disease outcomes were used to
analyze the subgroups. The groups are characterized by metabolic syndrome,
obesity, different comorbidities, psychoneurological factors, and genetic
factors. Odds ratios were utilized to compare the risk of complex diseases,
such as fibrosis, cirrhosis, and hepatocellular carcinoma (HCC), as well as
liver failure between the clusters. Cluster 2 has a significantly higher
complex disease outcome compared to other clusters. $$\\$$ Keywords: Fatty
liver disease; Polygenic risk score; Precision medicine; Deep phenotyping;
NAFLD comorbidities; Latent class analysis.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08433" title="Abstract">arXiv:2311.08433</a> (cross-list from q-bio.QM) [<a href="/pdf/2311.08433" title="Download PDF">pdf</a>, <a href="/ps/2311.08433" title="Download PostScript">ps</a>, <a href="/format/2311.08433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clinical Characteristics and Laboratory Biomarkers in ICU-admitted  Septic Patients with and without Bacteremia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Baek%2C+S">Sangwon Baek</a>, 
<a href="/search/q-bio?searchtype=author&query=Lee%2C+S+J">Seung Jun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figure, 2 tables, accepted for poster presentation at ASM Microbe 2022 and oral presentation at ECCMID 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">Few studies have investigated the diagnostic utilities of biomarkers for
predicting bacteremia among septic patients admitted to intensive care units
(ICU). Therefore, this study evaluated the prediction power of laboratory
biomarkers to utilize those markers with high performance to optimize the
predictive model for bacteremia. This retrospective cross-sectional study was
conducted at the ICU department of Gyeongsang National University Changwon
Hospital in 2019. Adult patients qualifying SEPSIS-3 (increase in sequential
organ failure score greater than or equal to 2) criteria with at least two sets
of blood culture were selected. Collected data was initially analyzed
independently to identify the significant predictors, which was then used to
build the multivariable logistic regression (MLR) model. A total of 218
patients with 48 cases of true bacteremia were analyzed in this research. Both
CRP and PCT showed a substantial area under the curve (AUC) value for
discriminating bacteremia among septic patients (0.757 and 0.845,
respectively). To further enhance the predictive accuracy, we combined PCT,
bilirubin, neutrophil lymphocyte ratio (NLR), platelets, lactic acid,
erythrocyte sedimentation rate (ESR), and Glasgow Coma Scale (GCS) score to
build the predictive model with an AUC of 0.907 (95% CI, 0.843 to 0.956). In
addition, a high association between bacteremia and mortality rate was
discovered through the survival analysis (0.004). While PCT is certainly a
useful index for distinguishing patients with and without bacteremia by itself,
our MLR model indicates that the accuracy of bacteremia prediction
substantially improves by the combined use of PCT, bilirubin, NLR, platelets,
lactic acid, ESR, and GCS score.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08439" title="Abstract">arXiv:2311.08439</a> (cross-list from eess.IV) [<a href="/pdf/2311.08439" title="Download PDF">pdf</a>, <a href="/format/2311.08439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Approach for Comprehensive Analysis of Various Spectral and  Tissue Doppler Echocardiography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jeon%2C+J">Jaeik Jeon</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+J">Jiyeon Kim</a>, 
<a href="/search/eess?searchtype=author&query=Jang%2C+Y">Yeonggul Jang</a>, 
<a href="/search/eess?searchtype=author&query=Yoon%2C+Y+E">Yeonyee E. Yoon</a>, 
<a href="/search/eess?searchtype=author&query=Jeong%2C+D">Dawun Jeong</a>, 
<a href="/search/eess?searchtype=author&query=Hong%2C+Y">Youngtaek Hong</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+S">Seung-Ah Lee</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+H">Hyuk-Jae Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Doppler echocardiography offers critical insights into cardiac function and
phases by quantifying blood flow velocities and evaluating myocardial motion.
However, previous methods for automating Doppler analysis, ranging from initial
signal processing techniques to advanced deep learning approaches, have been
constrained by their reliance on electrocardiogram (ECG) data and their
inability to process Doppler views collectively. We introduce a novel unified
framework using a convolutional neural network for comprehensive analysis of
spectral and tissue Doppler echocardiography images that combines automatic
measurements and end-diastole (ED) detection into a singular method. The
network automatically recognizes key features across various Doppler views,
with novel Doppler shape embedding and anti-aliasing modules enhancing
interpretation and ensuring consistent analysis. Empirical results indicate a
consistent outperformance in performance metrics, including dice similarity
coefficients (DSC) and intersection over union (IoU). The proposed framework
demonstrates strong agreement with clinicians in Doppler automatic measurements
and competitive performance in ED detection.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08442" title="Abstract">arXiv:2311.08442</a> (cross-list from math.ST) [<a href="/pdf/2311.08442" title="Download PDF">pdf</a>, <a href="/format/2311.08442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean-field variational inference with the TAP free energy: Geometric and  statistical properties in linear models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Celentano%2C+M">Michael Celentano</a>, 
<a href="/search/math?searchtype=author&query=Fan%2C+Z">Zhou Fan</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+L">Licong Lin</a>, 
<a href="/search/math?searchtype=author&query=Mei%2C+S">Song Mei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 79 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study mean-field variational inference in a Bayesian linear model when the
sample size n is comparable to the dimension p. In high dimensions, the common
approach of minimizing a Kullback-Leibler divergence from the posterior
distribution, or maximizing an evidence lower bound, may deviate from the true
posterior mean and underestimate posterior uncertainty. We study instead
minimization of the TAP free energy, showing in a high-dimensional asymptotic
framework that it has a local minimizer which provides a consistent estimate of
the posterior marginals and may be used for correctly calibrated posterior
inference. Geometrically, we show that the landscape of the TAP free energy is
strongly convex in an extensive neighborhood of this local minimizer, which
under certain general conditions can be found by an Approximate Message Passing
(AMP) algorithm. We then exhibit an efficient algorithm that linearly converges
to the minimizer within this local neighborhood. In settings where it is
conjectured that no efficient algorithm can find this local neighborhood, we
prove analogous geometric properties for a local minimizer of the TAP free
energy reachable by AMP, and show that posterior inference based on this
minimizer remains correctly calibrated.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08460" title="Abstract">arXiv:2311.08460</a> (cross-list from astro-ph.GA) [<a href="/pdf/2311.08460" title="Download PDF">pdf</a>, <a href="/format/2311.08460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surrogate Modeling for Computationally Expensive Simulations of  Supernovae in High-Resolution Galaxy Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Hirashima%2C+K">Keiya Hirashima</a>, 
<a href="/search/astro-ph?searchtype=author&query=Moriwaki%2C+K">Kana Moriwaki</a>, 
<a href="/search/astro-ph?searchtype=author&query=Fujii%2C+M+S">Michiko S. Fujii</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hirai%2C+Y">Yutaka Hirai</a>, 
<a href="/search/astro-ph?searchtype=author&query=Saitoh%2C+T+R">Takayuki R. Saitoh</a>, 
<a href="/search/astro-ph?searchtype=author&query=Makino%2C+J">Junichiro Makino</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ho%2C+S">Shirley Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures, Accepted for the NeurIPS 2023 AI4Science Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Astrophysics of Galaxies (astro-ph.GA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Some stars are known to explode at the end of their lives, called supernovae
(SNe). The substantial amount of matter and energy that SNe release provides
significant feedback to star formation and gas dynamics in a galaxy. SNe
release a substantial amount of matter and energy to the interstellar medium,
resulting in significant feedback to star formation and gas dynamics in a
galaxy. While such feedback has a crucial role in galaxy formation and
evolution, in simulations of galaxy formation, it has only been implemented
using simple {\it sub-grid models} instead of numerically solving the evolution
of gas elements around SNe in detail due to a lack of resolution. We develop a
method combining machine learning and Gibbs sampling to predict how a supernova
(SN) affects the surrounding gas. The fidelity of our model in the thermal
energy and momentum distribution outperforms the low-resolution SN simulations.
Our method can replace the SN sub-grid models and help properly simulate
un-resolved SN feedback in galaxy formation simulations. We find that employing
our new approach reduces the necessary computational cost to $\sim$ 1 percent
compared to directly resolving SN feedback.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08493" title="Abstract">arXiv:2311.08493</a> (cross-list from eess.IV) [<a href="/pdf/2311.08493" title="Download PDF">pdf</a>, <a href="/format/2311.08493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance of Machine Learning Classification in Mammography Images  using BI-RADS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gunawardhana%2C+M">Malitha Gunawardhana</a>, 
<a href="/search/eess?searchtype=author&query=Zolek%2C+N">Norbert Zolek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This research aims to investigate the classification accuracy of various
state-of-the-art image classification models across different categories of
breast ultrasound images, as defined by the Breast Imaging Reporting and Data
System (BI-RADS). To achieve this, we have utilized a comprehensively assembled
dataset of 2,945 mammographic images sourced from 1,540 patients. In order to
conduct a thorough analysis, we employed six advanced classification
architectures, including VGG19 \cite{simonyan2014very}, ResNet50
\cite{he2016deep}, GoogleNet \cite{szegedy2015going}, ConvNext
\cite{liu2022convnet}, EfficientNet \cite{tan2019efficientnet}, and Vision
Transformers (ViT) \cite{dosovitskiy2020image}, instead of traditional machine
learning models. We evaluate models in three different settings: full
fine-tuning, linear evaluation and training from scratch. Our findings
demonstrate the effectiveness and capability of our Computer-Aided Diagnosis
(CAD) system, with a remarkable accuracy of 76.39\% and an F1 score of 67.94\%
in the full fine-tuning setting. Our findings indicate the potential for
enhanced diagnostic accuracy in the field of breast imaging, providing a solid
foundation for future endeavors aiming to improve the precision and reliability
of CAD systems in medical imaging.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08502" title="Abstract">arXiv:2311.08502</a> (cross-list from quant-ph) [<a href="/pdf/2311.08502" title="Download PDF">pdf</a>, <a href="/format/2311.08502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Quantum Eigensolver with Constraints (VQEC): Solving  Constrained Optimization Problems via VQE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Le%2C+T+V">Thinh Viet Le</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kekatos%2C+V">Vassilis Kekatos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 13 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Variational quantum approaches have shown great promise in finding
near-optimal solutions to computationally challenging tasks. Nonetheless,
enforcing constraints in a disciplined fashion has been largely unexplored. To
address this gap, this work proposes a hybrid quantum-classical algorithmic
paradigm termed VQEC that extends the celebrated VQE to handle optimization
with constraints. As with the standard VQE, the vector of optimization
variables is captured by the state of a variational quantum circuit (VQC). To
deal with constraints, VQEC optimizes a Lagrangian function classically over
both the VQC parameters as well as the dual variables associated with
constraints. To comply with the quantum setup, variables are updated via a
perturbed primal-dual method leveraging the parameter shift rule. Among a wide
gamut of potential applications, we showcase how VQEC can approximately solve
quadratically-constrained binary optimization (QCBO) problems, find stochastic
binary policies satisfying quadratic constraints on the average and in
probability, and solve large-scale linear programs (LP) over the probability
simplex. Under an assumption on the error for the VQC to approximate an
arbitrary probability mass function (PMF), we provide bounds on the optimality
gap attained by a VQC. Numerical tests on a quantum simulator investigate the
effect of various parameters and corroborate that VQEC can generate
high-quality solutions.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08504" title="Abstract">arXiv:2311.08504</a> (cross-list from stat.ML) [<a href="/pdf/2311.08504" title="Download PDF">pdf</a>, <a href="/ps/2311.08504" title="Download PostScript">ps</a>, <a href="/format/2311.08504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On semi-supervised estimation using exponential tilt mixture models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tian%2C+Y">Ye Tian</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+X">Xinwei Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Tan%2C+Z">Zhiqiang Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Consider a semi-supervised setting with a labeled dataset of binary responses
and predictors and an unlabeled dataset with only the predictors. Logistic
regression is equivalent to an exponential tilt model in the labeled
population. For semi-supervised estimation, we develop further analysis and
understanding of a statistical approach using exponential tilt mixture (ETM)
models and maximum nonparametric likelihood estimation, while allowing that the
class proportions may differ between the unlabeled and labeled data. We derive
asymptotic properties of ETM-based estimation and demonstrate improved
efficiency over supervised logistic regression in a random sampling setup and
an outcome-stratified sampling setup previously used. Moreover, we reconcile
such efficiency improvement with the existing semiparametric efficiency theory
when the class proportions in the unlabeled and labeled data are restricted to
be the same. We also provide a simulation study to numerically illustrate our
theoretical findings.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08524" title="Abstract">arXiv:2311.08524</a> (cross-list from eess.IV) [<a href="/pdf/2311.08524" title="Download PDF">pdf</a>, <a href="/format/2311.08524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-dataset domain adaptation for the classification COVID-19 using  chest computed tomography images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ouni%2C+R">Ridha Ouni</a>, 
<a href="/search/eess?searchtype=author&query=Alhichri%2C+H">Haikel Alhichri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Detecting COVID-19 patients using Computed Tomography (CT) images of the
lungs is an active area of research. Datasets of CT images from COVID-19
patients are becoming available. Deep learning (DL) solutions and in particular
Convolutional Neural Networks (CNN) have achieved impressive results for the
classification of COVID-19 CT images, but only when the training and testing
take place within the same dataset. Work on the cross-dataset problem is still
limited and the achieved results are low. Our work tackles the cross-dataset
problem through a Domain Adaptation (DA) technique with deep learning. Our
proposed solution, COVID19-DANet, is based on pre-trained CNN backbone for
feature extraction. For this task, we select the pre-trained Efficientnet-B3
CNN because it has achieved impressive classification accuracy in previous
work. The backbone CNN is followed by a prototypical layer which is a concept
borrowed from prototypical networks in few-shot learning (FSL). It computes a
cosine distance between given samples and the class prototypes and then
converts them to class probabilities using the Softmax function. To train the
COVID19-DANet model, we propose a combined loss function that is composed of
the standard cross-entropy loss for class discrimination and another entropy
loss computed over the unlabelled target set only. This so-called unlabelled
target entropy loss is minimized and maximized in an alternative fashion, to
reach the two objectives of class discrimination and domain invariance.
COVID19-DANet is tested under four cross-dataset scenarios using the
SARS-CoV-2-CT and COVID19-CT datasets and has achieved encouraging results
compared to recent work in the literature.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08532" title="Abstract">arXiv:2311.08532</a> (cross-list from econ.TH) [<a href="/pdf/2311.08532" title="Download PDF">pdf</a>, <a href="/ps/2311.08532" title="Download PostScript">ps</a>, <a href="/format/2311.08532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crowdsearch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Gersbach%2C+H">Hans Gersbach</a>, 
<a href="/search/econ?searchtype=author&query=Mamageishvili%2C+A">Akaki Mamageishvili</a>, 
<a href="/search/econ?searchtype=author&query=Pitsuwan%2C+F">Fikri Pitsuwan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2304.00077">arXiv:2304.00077</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">A common economic process is crowdsearch, wherein a group of agents is
invited to search for a valuable physical or virtual object, e.g. creating and
patenting an invention, solving an open scientific problem, or identifying
vulnerabilities in software. We study a binary model of crowdsearch in which
agents have different abilities to find the object. We characterize the types
of equilibria and identify which type of crowd maximizes the likelihood of
finding the object. Sometimes, however, an unlimited crowd is not sufficient to
guarantee that the object is found. It even can happen that inviting more
agents lowers the probability of finding the object. We characterize the
optimal prize and show that offering only one prize (winner-takes-all)
maximizes the probability of finding the object but is not necessarily optimal
for the crowdsearch designer.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08537" title="Abstract">arXiv:2311.08537</a> (cross-list from math.AP) [<a href="/pdf/2311.08537" title="Download PDF">pdf</a>, <a href="/format/2311.08537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Curve Shortening Flow for Curves of Finite Total (Absolute)  Curvature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guidotti%2C+P">Patrick Guidotti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We revisit the well-known Curve Shortening Flow for immersed curves in the
$d$-dimensional Euclidean space. We exploit a fundamental structure of the
problem to derive a new global construction of a solution, that is, a
construction that is valid for all times and is insensitive to singularities.
The construction is characterized by discretization in time and the
approximant, while still exhibiting the possibile formation of finitely many
singularities at a finite set of singular times, exists globally and is well
behaved and simpler to analyze. A solution of the CSF is obtained in the limit.
Estimates for a natural (geometric) norm involving length and total absolute
curvature allow passage to the limit. Many classical qualitative results about
the flow can be recovered by exploiting the simplicity of the approximant and
new ones can be proved. The construction also suggests a numerical procedure
for the computation of the flow which proves very effective as demonstrated by
a series of numerical experiments scattered throughout the paper.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08543" title="Abstract">arXiv:2311.08543</a> (cross-list from eess.SP) [<a href="/pdf/2311.08543" title="Download PDF">pdf</a>, <a href="/format/2311.08543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jiarui Xu</a>, 
<a href="/search/eess?searchtype=author&query=Said%2C+K">Karim Said</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+L">Lizhong Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+L">Lingjia Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, journal submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Orthogonal time frequency space (OTFS) is a promising modulation scheme for
wireless communication in high-mobility scenarios. Recently, a reservoir
computing (RC) based approach has been introduced for online subframe-based
symbol detection in the OTFS system, where only a limited number of
over-the-air (OTA) pilot symbols are utilized for training. However, this
approach does not leverage the domain knowledge specific to the OTFS system.
This paper introduces a novel two-dimensional RC (2D-RC) method that
incorporates the structural knowledge of the OTFS system into the design for
online symbol detection on a subframe basis. Specifically, as the channel
response acts as a two-dimensional (2D) operation over the transmitted
information symbols in the delay-Doppler (DD) domain, the 2D-RC is designed to
have a 2D structure to equalize the channel. With the introduced architecture,
the 2D-RC can benefit from the predictable channel representation in the DD
domain. Moreover, unlike the previous work that requires multiple RCs to learn
the channel feature, the 2D-RC only requires a single neural network for
detection. Experimental results demonstrate the effectiveness of the 2D-RC
approach across different OTFS system variants and modulation orders.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08544" title="Abstract">arXiv:2311.08544</a> (cross-list from q-bio.NC) [<a href="/pdf/2311.08544" title="Download PDF">pdf</a>, <a href="/format/2311.08544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JOSA: Joint surface-based registration and atlas construction of brain  geometry and function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Tuckute%2C+G">Greta Tuckute</a>, 
<a href="/search/q-bio?searchtype=author&query=Fedorenko%2C+E">Evelina Fedorenko</a>, 
<a href="/search/q-bio?searchtype=author&query=Edlow%2C+B+L">Brian L. Edlow</a>, 
<a href="/search/q-bio?searchtype=author&query=Dalca%2C+A+V">Adrian V. Dalca</a>, 
<a href="/search/q-bio?searchtype=author&query=Fischl%2C+B">Bruce Fischl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A. V. Dalca and B. Fischl are co-senior authors with equal contribution. arXiv admin note: text overlap with <a href="/abs/2303.01592">arXiv:2303.01592</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Surface-based cortical registration is an important topic in medical image
analysis and facilitates many downstream applications. Current approaches for
cortical registration are mainly driven by geometric features, such as sulcal
depth and curvature, and often assume that registration of folding patterns
leads to alignment of brain function. However, functional variability of
anatomically corresponding areas across subjects has been widely reported,
particularly in higher-order cognitive areas. In this work, we present JOSA, a
novel cortical registration framework that jointly models the mismatch between
geometry and function while simultaneously learning an unbiased
population-specific atlas. Using a semi-supervised training strategy, JOSA
achieves superior registration performance in both geometry and function to the
state-of-the-art methods but without requiring functional data at inference.
This learning framework can be extended to any auxiliary data to guide
spherical registration that is available during training but is difficult or
impossible to obtain during inference, such as parcellations, architectonic
identity, transcriptomic information, and molecular profiles. By recognizing
the mismatch between geometry and function, JOSA provides new insights into the
future development of registration methods using joint analysis of the brain
structure and function.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08548" title="Abstract">arXiv:2311.08548</a> (cross-list from eess.SP) [<a href="/pdf/2311.08548" title="Download PDF">pdf</a>, <a href="/format/2311.08548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology of Surface Electromyogram Signals: Hand Gesture Decoding on  Riemannian Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gowda%2C+H+T">Harshavardhana T. Gowda</a>, 
<a href="/search/eess?searchtype=author&query=Miller%2C+L+M">Lee M. Miller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Decoding gestures from the upper limb using noninvasive surface
electromyogram (sEMG) signals is of keen interest for the rehabilitation of
amputees, artificial supernumerary limb augmentation, gestural control of
computers, and virtual/augmented realities. We show that sEMG signals recorded
across an array of sensor electrodes in multiple spatial locations around the
forearm evince a rich geometric pattern of global motor unit (MU) activity that
can be leveraged to distinguish different hand gestures. We demonstrate a
simple technique to analyze spatial patterns of muscle MU activity within a
temporal window and show that distinct gestures can be classified in both
supervised and unsupervised manners. Specifically, we construct symmetric
positive definite (SPD) covariance matrices to represent the spatial
distribution of MU activity in a time window of interest, calculated as
pairwise covariance of electrical signals measured across different electrodes.
This allows us to understand and manipulate multivariate sEMG timeseries on a
more natural subspace -the Riemannian manifold. Furthermore, it directly
addresses signal variability across individuals and sessions, which remains a
major challenge in the field. sEMG signals measured at a single electrode lack
contextual information such as how various anatomical and physiological factors
influence the signals and how their combined effect alters the evident
interaction among neighboring muscles. As we show here, analyzing spatial
patterns using covariance matrices on Riemannian manifolds allows us to
robustly model complex interactions across spatially distributed MUs and
provides a flexible and transparent framework to quantify differences in sEMG
signals across individuals. The proposed method is novel in the study of sEMG
signals and its performance exceeds the current benchmarks while maintaining
exceptional computational efficiency.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08549" title="Abstract">arXiv:2311.08549</a> (cross-list from stat.ML) [<a href="/pdf/2311.08549" title="Download PDF">pdf</a>, <a href="/format/2311.08549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Manifold learning in Wasserstein space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hamm%2C+K">Keaton Hamm</a>, 
<a href="/search/stat?searchtype=author&query=Moosm%C3%BCller%2C+C">Caroline Moosm&#xfc;ller</a>, 
<a href="/search/stat?searchtype=author&query=Schmitzer%2C+B">Bernhard Schmitzer</a>, 
<a href="/search/stat?searchtype=author&query=Thorpe%2C+M">Matthew Thorpe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Differential Geometry (math.DG)

</div>
<p class="mathjax">This paper aims at building the theoretical foundations for manifold learning
algorithms in the space of absolutely continuous probability measures on a
compact and convex subset of $\mathbb{R}^d$, metrized with the Wasserstein-2
distance $W$. We begin by introducing a natural construction of submanifolds
$\Lambda$ of probability measures equipped with metric $W_\Lambda$, the
geodesic restriction of $W$ to $\Lambda$. In contrast to other constructions,
these submanifolds are not necessarily flat, but still allow for local
linearizations in a similar fashion to Riemannian submanifolds of
$\mathbb{R}^d$. We then show how the latent manifold structure of
$(\Lambda,W_{\Lambda})$ can be learned from samples $\{\lambda_i\}_{i=1}^N$ of
$\Lambda$ and pairwise extrinsic Wasserstein distances $W$ only. In particular,
we show that the metric space $(\Lambda,W_{\Lambda})$ can be asymptotically
recovered in the sense of Gromov--Wasserstein from a graph with nodes
$\{\lambda_i\}_{i=1}^N$ and edge weights $W(\lambda_i,\lambda_j)$. In addition,
we demonstrate how the tangent space at a sample $\lambda$ can be
asymptotically recovered via spectral analysis of a suitable "covariance
operator" using optimal transport maps from $\lambda$ to sufficiently close and
diverse samples $\{\lambda_i\}_{i=1}^N$. The paper closes with some explicit
constructions of submanifolds $\Lambda$ and numerical examples on the recovery
of tangent spaces through spectral analysis.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08558" title="Abstract">arXiv:2311.08558</a> (cross-list from astro-ph.CO) [<a href="/pdf/2311.08558" title="Download PDF">pdf</a>, <a href="/format/2311.08558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic reconstruction of Dark Matter fields from biased tracers  using diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Park%2C+C+F">Core Francisco Park</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ono%2C+V">Victoria Ono</a>, 
<a href="/search/astro-ph?searchtype=author&query=Mudur%2C+N">Nayantara Mudur</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ni%2C+Y">Yueying Ni</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cuesta-Lazaro%2C+C">Carolina Cuesta-Lazaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Astrophysics of Galaxies (astro-ph.GA); Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Galaxies are biased tracers of the underlying cosmic web, which is dominated
by dark matter components that cannot be directly observed. The relationship
between dark matter density fields and galaxy distributions can be sensitive to
assumptions in cosmology and astrophysical processes embedded in the galaxy
formation models, that remain uncertain in many aspects. Based on
state-of-the-art galaxy formation simulation suites with varied cosmological
parameters and sub-grid astrophysics, we develop a diffusion generative model
to predict the unbiased posterior distribution of the underlying dark matter
fields from the given stellar mass fields, while being able to marginalize over
the uncertainties in cosmology and galaxy formation.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08560" title="Abstract">arXiv:2311.08560</a> (cross-list from math.CO) [<a href="/pdf/2311.08560" title="Download PDF">pdf</a>, <a href="/ps/2311.08560" title="Download PostScript">ps</a>, <a href="/format/2311.08560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Colouring of Binomial Random Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eide%2C+A">Austin Eide</a>, 
<a href="/search/math?searchtype=author&query=Pra%C5%82at%2C+P">Pawe&#x142; Pra&#x142;at</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We investigate the linear chromatic number $\chi_{\text{lin}}(G(n,p))$ of the
binomial random graph $G(n,p)$ on $n$ vertices in which each edge appears
independently with probability $p=p(n)$. For dense random graphs ($np \to
\infty$ as $n \to \infty$), we show that asymptotically almost surely
$\chi_{\text{lin}}(G(n,p)) \ge n (1 - O( (np)^{-1/2} ) ) = n(1-o(1))$.
Understanding the order of the linear chromatic number for subcritical random
graphs ($np &lt; 1$) and critical ones ($np=1$) is relatively easy. However,
supercritical sparse random graphs ($np = c$ for some constant $c &gt; 1$) remain
to be investigated.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08570" title="Abstract">arXiv:2311.08570</a> (cross-list from math.OC) [<a href="/pdf/2311.08570" title="Download PDF">pdf</a>, <a href="/ps/2311.08570" title="Download PostScript">ps</a>, <a href="/format/2311.08570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxation strength for multilinear optimization: McCormick strikes back
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Schutte%2C+E">Emily Schutte</a>, 
<a href="/search/math?searchtype=author&query=Walter%2C+M">Matthias Walter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">We consider linear relaxations for multilinear optimization problems. In a
recent paper, Khajavirad proved that the extended flower relaxation is at least
as strong as the relaxation of any recursive McCormick linearization
(Operations Research Letters 51 (2023) 146-152). In this paper we extend the
result to more general linearizations, and present a simpler proof. Moreover,
we complement Khajavirad's result by showing that the intersection of the
relaxations of such linearizations and the extended flower relaxation are
equally strong.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08585" title="Abstract">arXiv:2311.08585</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2311.08585" title="Download PDF">pdf</a>, <a href="/format/2311.08585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised segmentation of irradiation$\unicode{x2010}$induced  order$\unicode{x2010}$disorder phase transitions in electron microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Ter-Petrosyan%2C+A+H">Arman H Ter-Petrosyan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Bilbrey%2C+J+A">Jenna A Bilbrey</a>, 
<a href="/search/cond-mat?searchtype=author&query=Doty%2C+C+M">Christina M Doty</a>, 
<a href="/search/cond-mat?searchtype=author&query=Matthews%2C+B+E">Bethany E Matthews</a>, 
<a href="/search/cond-mat?searchtype=author&query=Wang%2C+L">Le Wang</a>, 
<a href="/search/cond-mat?searchtype=author&query=Du%2C+Y">Yingge Du</a>, 
<a href="/search/cond-mat?searchtype=author&query=Lang%2C+E">Eric Lang</a>, 
<a href="/search/cond-mat?searchtype=author&query=Hattar%2C+K">Khalid Hattar</a>, 
<a href="/search/cond-mat?searchtype=author&query=Spurgeon%2C+S+R">Steven R Spurgeon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures. Accepted to Machine Learning and the Physical Sciences Workshop, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">We present a method for the unsupervised segmentation of electron microscopy
images, which are powerful descriptors of materials and chemical systems.
Images are oversegmented into overlapping chips, and similarity graphs are
generated from embeddings extracted from a domain$\unicode{x2010}$pretrained
convolutional neural network (CNN). The Louvain method for community detection
is then applied to perform segmentation. The graph representation provides an
intuitive way of presenting the relationship between chips and communities. We
demonstrate our method to track irradiation$\unicode{x2010}$induced amorphous
fronts in thin films used for catalysis and electronics. This method has
potential for "on$\unicode{x2010}$the$\unicode{x2010}$fly" segmentation to
guide emerging automated electron microscopes.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08615" title="Abstract">arXiv:2311.08615</a> (cross-list from math.OC) [<a href="/pdf/2311.08615" title="Download PDF">pdf</a>, <a href="/format/2311.08615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Uniform Smoothness for Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Berahas%2C+A+S">Albert S. Berahas</a>, 
<a href="/search/math?searchtype=author&query=Roberts%2C+L">Lindon Roberts</a>, 
<a href="/search/math?searchtype=author&query=Roosta%2C+F">Fred Roosta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The analysis of gradient descent-type methods typically relies on the
Lipschitz continuity of the objective gradient. This generally requires an
expensive hyperparameter tuning process to appropriately calibrate a stepsize
for a given problem. In this work we introduce a local first-order smoothness
oracle (LFSO) which generalizes the Lipschitz continuous gradients smoothness
condition and is applicable to any twice-differentiable function. We show that
this oracle can encode all relevant problem information for tuning stepsizes
for a suitably modified gradient descent method and give global and local
convergence results. We also show that LFSOs in this modified first-order
method can yield global linear convergence rates for non-strongly convex
problems with extremely flat minima, and thus improve over the lower bound on
rates achievable by general (accelerated) first-order methods.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08630" title="Abstract">arXiv:2311.08630</a> (cross-list from eess.AS) [<a href="/pdf/2311.08630" title="Download PDF">pdf</a>, <a href="/format/2311.08630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-channel Conversational Speaker Separation via Neural Diarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Taherian%2C+H">Hassan Taherian</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+D">DeLiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">When dealing with overlapped speech, the performance of automatic speech
recognition (ASR) systems substantially degrades as they are designed for
single-talker speech. To enhance ASR performance in conversational or meeting
environments, continuous speaker separation (CSS) is commonly employed.
However, CSS requires a short separation window to avoid many speakers inside
the window and sequential grouping of discontinuous speech segments. To address
these limitations, we introduce a new multi-channel framework called "speaker
separation via neural diarization" (SSND) for meeting environments. Our
approach utilizes an end-to-end diarization system to identify the speech
activity of each individual speaker. By leveraging estimated speaker
boundaries, we generate a sequence of embeddings, which in turn facilitate the
assignment of speakers to the outputs of a multi-talker separation model. SSND
addresses the permutation ambiguity issue of talker-independent speaker
separation during the diarization phase through location-based training, rather
than during the separation process. This unique approach allows multiple
non-overlapped speakers to be assigned to the same output stream, making it
possible to efficiently process long segments-a task impossible with CSS.
Additionally, SSND is naturally suitable for speaker-attributed ASR. We
evaluate our proposed diarization and separation methods on the open LibriCSS
dataset, advancing state-of-the-art diarization and ASR results by a large
margin.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08636" title="Abstract">arXiv:2311.08636</a> (cross-list from stat.ML) [<a href="/pdf/2311.08636" title="Download PDF">pdf</a>, <a href="/format/2311.08636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised low-rank semi-nonnegative matrix factorization with frequency  regularization for forecasting spatio-temporal data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kim%2C+K">Keunsu Kim</a>, 
<a href="/search/stat?searchtype=author&query=Lyu%2C+H">Hanbaek Lyu</a>, 
<a href="/search/stat?searchtype=author&query=Kim%2C+J">Jinsu Kim</a>, 
<a href="/search/stat?searchtype=author&query=Jung%2C+J">Jae-Hun Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a novel methodology for forecasting spatio-temporal data using
supervised semi-nonnegative matrix factorization (SSNMF) with frequency
regularization. Matrix factorization is employed to decompose spatio-temporal
data into spatial and temporal components. To improve clarity in the temporal
patterns, we introduce a nonnegativity constraint on the time domain along with
regularization in the frequency domain. Specifically, regularization in the
frequency domain involves selecting features in the frequency space, making an
interpretation in the frequency domain more convenient. We propose two methods
in the frequency domain: soft and hard regularizations, and provide convergence
guarantees to first-order stationary points of the corresponding constrained
optimization problem. While our primary motivation stems from geophysical data
analysis based on GRACE (Gravity Recovery and Climate Experiment) data, our
methodology has the potential for wider application. Consequently, when
applying our methodology to GRACE data, we find that the results with the
proposed methodology are comparable to previous research in the field of
geophysical sciences but offer clearer interpretability.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08653" title="Abstract">arXiv:2311.08653</a> (cross-list from quant-ph) [<a href="/pdf/2311.08653" title="Download PDF">pdf</a>, <a href="/ps/2311.08653" title="Download PostScript">ps</a>, <a href="/format/2311.08653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Locally Recoverable Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Golowich%2C+L">Louis Golowich</a>, 
<a href="/search/quant-ph?searchtype=author&query=Guruswami%2C+V">Venkatesan Guruswami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Classical locally recoverable codes, which permit highly efficient recovery
from localized errors as well as global recovery from larger errors, provide
some of the most useful codes for distributed data storage in practice. In this
paper, we initiate the study of quantum locally recoverable codes (qLRCs). In
the long term, like their classical counterparts, such qLRCs may be used for
large-scale quantum data storage. Our results also have concrete implications
for quantum LDPC codes, which are applicable to near-term quantum
error-correction.
<br />After defining quantum local recoverability, we provide an explicit
construction of qLRCs based on the classical LRCs of Tamo and Barg (2014),
which we show have (1) a close-to-optimal rate-distance tradeoff (i.e. near the
Singleton bound), (2) an efficient decoder, and (3) permit good spatial
locality in a physical implementation. Although the analysis is significantly
more involved than in the classical case, we obtain close-to-optimal parameters
by introducing a "folded" version of our quantum Tamo-Barg (qTB) codes, which
we then analyze using a combination of algebraic techniques. We furthermore
present and analyze two additional constructions using more basic techniques,
namely random qLRCs, and qLRCs from AEL distance amplification. Each of these
constructions has some advantages, but neither achieves all 3 properties of our
folded qTB codes described above.
<br />We complement these constructions with Singleton-like bounds that show our
qLRC constructions achieve close-to-optimal parameters. We also apply these
results to obtain Singleton-like bounds for qLDPC codes, which to the best of
our knowledge are novel. We then show that even the weakest form of a stronger
locality property called local correctability, which permits more robust local
recovery and is achieved by certain classical codes, is impossible quantumly.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08661" title="Abstract">arXiv:2311.08661</a> (cross-list from stat.ML) [<a href="/pdf/2311.08661" title="Download PDF">pdf</a>, <a href="/format/2311.08661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Neural Network Identification of Limnonectes Species and New Class  Detection Using Image Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+L">Li Xu</a>, 
<a href="/search/stat?searchtype=author&query=Hong%2C+Y">Yili Hong</a>, 
<a href="/search/stat?searchtype=author&query=Smith%2C+E+P">Eric P. Smith</a>, 
<a href="/search/stat?searchtype=author&query=McLeod%2C+D+S">David S. McLeod</a>, 
<a href="/search/stat?searchtype=author&query=Deng%2C+X">Xinwei Deng</a>, 
<a href="/search/stat?searchtype=author&query=Freeman%2C+L+J">Laura J. Freeman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 11 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">As is true of many complex tasks, the work of discovering, describing, and
understanding the diversity of life on Earth (viz., biological systematics and
taxonomy) requires many tools. Some of this work can be accomplished as it has
been done in the past, but some aspects present us with challenges which
traditional knowledge and tools cannot adequately resolve. One such challenge
is presented by species complexes in which the morphological similarities among
the group members make it difficult to reliably identify known species and
detect new ones. We address this challenge by developing new tools using the
principles of machine learning to resolve two specific questions related to
species complexes. The first question is formulated as a classification problem
in statistics and machine learning and the second question is an
out-of-distribution (OOD) detection problem. We apply these tools to a species
complex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex)
and employ a morphological character (hind limb skin texture) traditionally
treated qualitatively in a quantitative and objective manner. We demonstrate
that deep neural networks can successfully automate the classification of an
image into a known species group for which it has been trained. We further
demonstrate that the algorithm can successfully classify an image into a new
class if the image does not belong to the existing classes. Additionally, we
use the larger MNIST dataset to test the performance of our OOD detection
algorithm. We finish our paper with some concluding remarks regarding the
application of these methods to species complexes and our efforts to document
true biodiversity. This paper has online supplementary materials.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08663" title="Abstract">arXiv:2311.08663</a> (cross-list from physics.soc-ph) [<a href="/pdf/2311.08663" title="Download PDF">pdf</a>, <a href="/ps/2311.08663" title="Download PostScript">ps</a>, <a href="/format/2311.08663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influence maximization in multilayer networks based on adaptive coupling  degree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zhang%2C+S">Su-Su Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Xie%2C+M">Ming Xie</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+C">Chuang Liu</a>, 
<a href="/search/physics?searchtype=author&query=Zhan%2C+X">Xiu-Xiu Zhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Influence Maximization(IM) aims to identify highly influential nodes to
maximize influence spread in a network. Previous research on the IM problem has
mainly concentrated on single-layer networks, disregarding the comprehension of
the coupling structure that is inherent in multilayer networks. To solve the IM
problem in multilayer networks, we first propose an independent cascade model
(MIC) in a multilayer network where propagation occurs simultaneously across
different layers. Consequently, a heuristic algorithm, i.e., Adaptive Coupling
Degree (ACD), which selects seed nodes with high spread influence and a low
degree of overlap of influence, is proposed to identify seed nodes for IM in a
multilayer network. By conducting experiments based on MIC, we have
demonstrated that our proposed method is superior to the baselines in terms of
influence spread and time cost in 6 synthetic and 4 real-world multilayer
networks.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08689" title="Abstract">arXiv:2311.08689</a> (cross-list from eess.SP) [<a href="/pdf/2311.08689" title="Download PDF">pdf</a>, <a href="/format/2311.08689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low Complexity High Speed Deep Neural Network Augmented Wireless Channel  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=haq%2C+S+A+u">Syed Asrar ul haq</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+V">Varun Singh</a>, 
<a href="/search/eess?searchtype=author&query=Tanaji%2C+B+T">Bhanu Teja Tanaji</a>, 
<a href="/search/eess?searchtype=author&query=Darak%2C+S">Sumit Darak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">The channel estimation (CE) in wireless receivers is one of the most critical
and computationally complex signal processing operations. Recently, various
works have shown that the deep learning (DL) based CE outperforms conventional
minimum mean square error (MMSE) based CE, and it is hardware-friendly.
However, DL-based CE has higher complexity and latency than popularly used
least square (LS) based CE. In this work, we propose a novel low complexity
high-speed Deep Neural Network-Augmented Least Square (LC-LSDNN) algorithm for
IEEE 802.11p wireless physical layer and efficiently implement it on Zynq
system on chip (ZSoC). The novelty of the LC-LSDNN is to use different DNNs for
real and imaginary values of received complex symbols. This helps reduce the
size of DL by 59% and optimize the critical path, allowing it to operate at 60%
higher clock frequency. We also explore three different architectures for
MMSE-based CE. We show that LC-LSDNN significantly outperforms MMSE and
state-of-the-art DL-based CE for a wide range of signal-to-noise ratios (SNR)
and different wireless channels. Also, it is computationally efficient, with
around 50% lower resources than existing DL-based CE.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08703" title="Abstract">arXiv:2311.08703</a> (cross-list from q-bio.NC) [<a href="/pdf/2311.08703" title="Download PDF">pdf</a>, <a href="/format/2311.08703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Nap on Performance in Different Working Memory Tasks Using EEG
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Shin%2C+G">Gi-Hwan Shin</a>, 
<a href="/search/q-bio?searchtype=author&query=Kweon%2C+Y">Young-Seok Kweon</a>, 
<a href="/search/q-bio?searchtype=author&query=Kwak%2C+H">Heon-Gyu Kwak</a>, 
<a href="/search/q-bio?searchtype=author&query=Jo%2C+H">Ha-Na Jo</a>, 
<a href="/search/q-bio?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 12th IEEE International Winter Conference on Brain-Computer Interface
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Electroencephalography (EEG) has been widely used to study the relationship
between naps and working memory, yet the effects of naps on distinct working
memory tasks remain unclear. Here, participants performed word-pair and
visuospatial working memory tasks pre- and post-nap sessions. We found marked
differences in accuracy and reaction time between tasks performed pre- and
post-nap. In order to identify the impact of naps on performance in each
working memory task, we employed clustering to classify participants as high-
or low-performers. Analysis of sleep architecture revealed significant
variations in sleep onset latency and rapid eye movement (REM) proportion. In
addition, the two groups exhibited prominent differences, especially in the
delta power of the Non-REM 3 stage linked to memory. Our results emphasize the
interplay between nap-related neural activity and working memory, underlining
specific EEG markers associated with cognitive performance.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08727" title="Abstract">arXiv:2311.08727</a> (cross-list from math.CO) [<a href="/pdf/2311.08727" title="Download PDF">pdf</a>, <a href="/format/2311.08727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hierarchy of Hereditary Sorting Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jel%C3%ADnek%2C+V">V&#xed;t Jel&#xed;nek</a>, 
<a href="/search/math?searchtype=author&query=Opler%2C+M">Michal Opler</a>, 
<a href="/search/math?searchtype=author&query=Pek%C3%A1rek%2C+J">Jakub Pek&#xe1;rek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We consider the following general model of a sorting procedure: we fix a
hereditary permutation class $\mathcal{C}$, which corresponds to the operations
that the procedure is allowed to perform in a single step. The input of sorting
is a permutation $\pi$ of the set $[n]=\{1,2,\dotsc,n\}$, i.e., a sequence
where each element of $[n]$ appears once. In every step, the sorting procedure
picks a permutation $\sigma$ of length $n$ from $\mathcal{C}$, and rearranges
the current permutation of numbers by composing it with $\sigma$. The goal is
to transform the input $\pi$ into the sorted sequence $1,2,\dotsc,n$ in as few
steps as possible.
<br />This model of sorting captures not only classical sorting algorithms, like
insertion sort or bubble sort, but also sorting by series of devices, like
stacks or parallel queues, as well as sorting by block operations commonly
considered, e.g., in the context of genome rearrangement.
<br />Our goal is to describe the possible asymptotic behavior of the worst-case
number of steps needed when sorting with a hereditary permutation class. As the
main result, we show that any hereditary permutation class $\mathcal{C}$ falls
into one of five distinct categories. Disregarding the classes that cannot sort
all permutations, the number of steps needed to sort any permutation of $[n]$
with $\mathcal{C}$ is either $\Theta(n^2)$, a function between $O(n)$ and
$\Omega(\sqrt{n})$, a function betwee $O(\log^2 n)$ and $\Omega(\log n), or
$1$, and for each of these cases we provide a structural characterization of
the corresponding hereditary classes.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08735" title="Abstract">arXiv:2311.08735</a> (cross-list from q-bio.NC) [<a href="/pdf/2311.08735" title="Download PDF">pdf</a>, <a href="/format/2311.08735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neurophysiological Response Based on Auditory Sense for Brain Modulation  Using Monaural Beat
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Jo%2C+H">Ha-Na Jo</a>, 
<a href="/search/q-bio?searchtype=author&query=Kweon%2C+Y">Young-Seok Kweon</a>, 
<a href="/search/q-bio?searchtype=author&query=Shin%2C+G">Gi-Hwan Shin</a>, 
<a href="/search/q-bio?searchtype=author&query=Kwak%2C+H">Heon-Gyu Kwak</a>, 
<a href="/search/q-bio?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMBC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Brain modulation is a modification process of brain activity through external
stimulations. However, which condition can induce the activation is still
unclear. Therefore, we aimed to identify brain activation conditions using 40
Hz monaural beat (MB). Under this stimulation, auditory sense status which is
determined by frequency and power range is the condition to consider. Hence, we
designed five sessions to compare; no stimulation, audible (AB), inaudible in
frequency, inaudible in power, and inaudible in frequency and power. Ten
healthy participants underwent each stimulation session for ten minutes with
electroencephalogram (EEG) recording. For analysis, we calculated the power
spectral density (PSD) of EEG for each session and compared them in frequency,
time, and five brain regions. As a result, we observed the prominent power peak
at 40 Hz in only AB. The induced EEG amplitude increase started at one minute
and increased until the end of the session. These results of AB had significant
differences in frontal, central, temporal, parietal, and occipital regions
compared to other stimulations. From the statistical analysis, the PSD of the
right temporal region was significantly higher than the left. We figure out the
role that the auditory sense is important to lead brain activation. These
findings help to understand the neurophysiological principle and effects of
auditory stimulation.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08746" title="Abstract">arXiv:2311.08746</a> (cross-list from eess.IV) [<a href="/pdf/2311.08746" title="Download PDF">pdf</a>, <a href="/format/2311.08746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Diffusion Model Based Quality Enhancement Method for HEVC Compressed  Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/eess?searchtype=author&query=Qi%2C+H">Honggang Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Video post-processing methods can improve the quality of compressed videos at
the decoder side. Most of the existing methods need to train corresponding
models for compressed videos with different quantization parameters to improve
the quality of compressed videos. However, in most cases, the quantization
parameters of the decoded video are unknown. This makes existing methods have
their limitations in improving video quality. To tackle this problem, this work
proposes a diffusion model based post-processing method for compressed videos.
The proposed method first estimates the feature vectors of the compressed video
and then uses the estimated feature vectors as the prior information for the
quality enhancement model to adaptively enhance the quality of compressed video
with different quantization parameters. Experimental results show that the
quality enhancement results of our proposed method on mixed datasets are
superior to existing methods.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08755" title="Abstract">arXiv:2311.08755</a> (cross-list from eess.SP) [<a href="/pdf/2311.08755" title="Download PDF">pdf</a>, <a href="/format/2311.08755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Environment-independent mmWave Fall Detection with Interacting Multiple  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yu%2C+X">Xuyao Yu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiazhao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+W">Wenchao Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The ageing society brings attention to daily elderly care through sensing
technologies. The future smart home is expected to enable in-home daily
monitoring, such as fall detection, for seniors in a non-invasive,
non-cooperative, and non-contact manner. The mmWave radar is a promising
candidate technology for its privacy-preserving and non-contact manner.
However, existing solutions suffer from low accuracy and robustness due to
environment dependent features. In this paper, we present FADE
(\underline{FA}ll \underline{DE}tection), a practical fall detection radar
system with enhanced accuracy and robustness in real-world scenarios. The key
enabler underlying FADE is an interacting multiple model (IMM) state estimator
that can extract environment-independent features for highly accurate and
instantaneous fall detection. Furthermore, we proposed a robust multiple-user
tracking system to deal with noises from the environment and other human
bodies. We deployed our algorithm on low computing power and low power
consumption system-on-chip (SoC) composed of data front end, DSP, and ARM
processor, and tested its performance in real-world. The experiment shows that
the accuracy of fall detection is up to 95\%.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08774" title="Abstract">arXiv:2311.08774</a> (cross-list from eess.IV) [<a href="/pdf/2311.08774" title="Download PDF">pdf</a>, <a href="/format/2311.08774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage Joint Transductive and Inductive learning for Nuclei  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ali%2C+H">Hesham Ali</a>, 
<a href="/search/eess?searchtype=author&query=Tondji%2C+I">Idriss Tondji</a>, 
<a href="/search/eess?searchtype=author&query=Siam%2C+M">Mennatullah Siam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">AI-assisted nuclei segmentation in histopathological images is a crucial task
in the diagnosis and treatment of cancer diseases. It decreases the time
required to manually screen microscopic tissue images and can resolve the
conflict between pathologists during diagnosis. Deep Learning has proven useful
in such a task. However, lack of labeled data is a significant barrier for deep
learning-based approaches. In this study, we propose a novel approach to nuclei
segmentation that leverages the available labelled and unlabelled data. The
proposed method combines the strengths of both transductive and inductive
learning, which have been previously attempted separately, into a single
framework. Inductive learning aims at approximating the general function and
generalizing to unseen test data, while transductive learning has the potential
of leveraging the unlabelled test data to improve the classification. To the
best of our knowledge, this is the first study to propose such a hybrid
approach for medical image segmentation. Moreover, we propose a novel two-stage
transductive inference scheme. We evaluate our approach on MoNuSeg benchmark to
demonstrate the efficacy and potential of our method.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08816" title="Abstract">arXiv:2311.08816</a> (cross-list from eess.IV) [<a href="/pdf/2311.08816" title="Download PDF">pdf</a>, <a href="/format/2311.08816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Target-oriented Domain Adaptation for Infrared Image Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yongsong Huang</a>, 
<a href="/search/eess?searchtype=author&query=Miyazaki%2C+T">Tomo Miyazaki</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xiaofeng Liu</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+Y">Yafei Dong</a>, 
<a href="/search/eess?searchtype=author&query=Omachi%2C+S">Shinichiro Omachi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent efforts have explored leveraging visible light images to enrich
texture details in infrared (IR) super-resolution. However, this direct
adaptation approach often becomes a double-edged sword, as it improves texture
at the cost of introducing noise and blurring artifacts. To address these
challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN),
an innovative framework specifically engineered for robust IR super-resolution
model adaptation. DASRGAN operates on the synergy of two key components: 1)
Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and
2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer.
Specifically, TOA uniquely integrates a specialized discriminator,
incorporating a prior extraction branch, and employs a Sobel-guided adversarial
loss to align texture distributions effectively. Concurrently, NOA utilizes a
noise adversarial loss to distinctly separate the generative and Gaussian noise
pattern distributions during adversarial training. Our extensive experiments
confirm DASRGAN's superiority. Comparative analyses against leading methods
across multiple benchmarks and upsampling factors reveal that DASRGAN sets new
state-of-the-art performance standards. Code are available at
\url{https://github.com/yongsongH/DASRGAN}.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08826" title="Abstract">arXiv:2311.08826</a> (cross-list from math.PR) [<a href="/pdf/2311.08826" title="Download PDF">pdf</a>, <a href="/format/2311.08826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-stage Euler-Maruyama methods for backward stochastic differential  equations driven by continuous-time Markov chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kaneko%2C+A">Akihiro Kaneko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Numerical Analysis (math.NA); Mathematical Finance (q-fin.MF)

</div>
<p class="mathjax">Numerical methods for computing the solutions of Markov backward stochastic
differential equations (BSDEs) driven by continuous-time Markov chains (CTMCs)
are explored. The main contributions of this paper are as follows: (1) we
observe that Euler-Maruyama temporal discretization methods for solving Markov
BSDEs driven by CTMCs are equivalent to exponential integrators for solving the
associated systems of ordinary differential equations (ODEs); (2) we introduce
multi-stage Euler-Maruyama methods for effectively solving "stiff" Markov BSDEs
driven by CTMCs; these BSDEs typically arise from the spatial discretization of
Markov BSDEs driven by Brownian motion; (3) we propose a multilevel spatial
discretization method on sparse grids that efficiently approximates
high-dimensional Markov BSDEs driven by Brownian motion with a combination of
multiple Markov BSDEs driven by CTMCs on grids with different resolutions. We
also illustrate the effectiveness of the presented methods with a number of
numerical experiments in which we treat nonlinear BSDEs arising from option
pricing problems in finance.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08833" title="Abstract">arXiv:2311.08833</a> (cross-list from eess.SP) [<a href="/pdf/2311.08833" title="Download PDF">pdf</a>, <a href="/ps/2311.08833" title="Download PostScript">ps</a>, <a href="/format/2311.08833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase retrieval with semi-algebraic and ReLU neural network priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bendory%2C+T">Tamir Bendory</a>, 
<a href="/search/eess?searchtype=author&query=Dym%2C+N">Nadav Dym</a>, 
<a href="/search/eess?searchtype=author&query=Edidin%2C+D">Dan Edidin</a>, 
<a href="/search/eess?searchtype=author&query=Suresh%2C+A">Arun Suresh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The key ingredient to retrieving a signal from its Fourier magnitudes,
namely, to solve the phase retrieval problem, is an effective prior on the
sought signal. In this paper, we study the phase retrieval problem under the
prior that the signal lies in a semi-algebraic set. This is a very general
prior as semi-algebraic sets include linear models, sparse models, and ReLU
neural network generative models. The latter is the main motivation of this
paper, due to the remarkable success of deep generative models in a variety of
imaging tasks, including phase retrieval. We prove that almost all signals in
R^N can be determined from their Fourier magnitudes, up to a sign, if they lie
in a (generic) semi-algebraic set of dimension N/2. The same is true for all
signals if the semi-algebraic set is of dimension N/4. We also generalize these
results to the problem of signal recovery from the second moment in
multi-reference alignment models with multiplicity free representations of
compact groups. This general result is then used to derive improved sample
complexity bounds for recovering band-limited functions on the sphere from
their noisy copies, each acted upon by a random element of SO(3).
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08845" title="Abstract">arXiv:2311.08845</a> (cross-list from math.ST) [<a href="/pdf/2311.08845" title="Download PDF">pdf</a>, <a href="/ps/2311.08845" title="Download PostScript">ps</a>, <a href="/format/2311.08845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical learning by sparse deep neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Abramovich%2C+F">Felix Abramovich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider a deep neural network estimator based on empirical risk
minimization with l_1-regularization. We derive a general bound for its excess
risk in regression and classification (including multiclass), and prove that it
is adaptively nearly-minimax (up to log-factors) simultaneously across the
entire range of various function classes.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08878" title="Abstract">arXiv:2311.08878</a> (cross-list from eess.AS) [<a href="/pdf/2311.08878" title="Download PDF">pdf</a>, <a href="/format/2311.08878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-objective Non-intrusive Hearing-aid Speech Assessment Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chiang%2C+H">Hsin-Tien Chiang</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+S">Szu-Wei Fu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hsin-Min Wang</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>, 
<a href="/search/eess?searchtype=author&query=Hansen%2C+J+H+L">John H. L. Hansen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Without the need for a clean reference, non-intrusive speech assessment
methods have caught great attention for objective evaluations. While deep
learning models have been used to develop non-intrusive speech assessment
methods with promising results, there is limited research on hearing-impaired
subjects. This study proposes a multi-objective non-intrusive hearing-aid
speech assessment model, called HASA-Net Large, which predicts speech quality
and intelligibility scores based on input speech signals and specified
hearing-loss patterns. Our experiments showed the utilization of pre-trained
SSL models leads to a significant boost in speech quality and intelligibility
predictions compared to using spectrograms as input. Additionally, we examined
three distinct fine-tuning approaches that resulted in further performance
improvements. Furthermore, we demonstrated that incorporating SSL models
resulted in greater transferability to OOD dataset. Finally, this study
introduces HASA-Net Large, which is a non-invasive approach for evaluating
speech quality and intelligibility. HASA-Net Large utilizes raw waveforms and
hearing-loss patterns to accurately predict speech quality and intelligibility
levels for individuals with normal and impaired hearing and demonstrates
superior prediction performance and transferability.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08908" title="Abstract">arXiv:2311.08908</a> (cross-list from stat.ME) [<a href="/pdf/2311.08908" title="Download PDF">pdf</a>, <a href="/format/2311.08908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Brain MRI Image Classification with SIBOW-SVM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zeng%2C+L">Liyun Zeng</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+H+H">Hao Helen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The majority of primary Central Nervous System (CNS) tumors in the brain are
among the most aggressive diseases affecting humans. Early detection of brain
tumor types, whether benign or malignant, glial or non-glial, is critical for
cancer prevention and treatment, ultimately improving human life expectancy.
Magnetic Resonance Imaging (MRI) stands as the most effective technique to
detect brain tumors by generating comprehensive brain images through scans.
However, human examination can be error-prone and inefficient due to the
complexity, size, and location variability of brain tumors. Recently, automated
classification techniques using machine learning (ML) methods, such as
Convolutional Neural Network (CNN), have demonstrated significantly higher
accuracy than manual screening, while maintaining low computational costs.
Nonetheless, deep learning-based image classification methods, including CNN,
face challenges in estimating class probabilities without proper model
calibration. In this paper, we propose a novel brain tumor image classification
method, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with
SIFT feature extraction and weighted Support Vector Machines (wSVMs). This new
approach effectively captures hidden image features, enabling the
differentiation of various tumor types and accurate label predictions.
Additionally, the SIBOW-SVM is able to estimate the probabilities of images
belonging to each class, thereby providing high-confidence classification
decisions. We have also developed scalable and parallelable algorithms to
facilitate the practical implementation of SIBOW-SVM for massive images. As a
benchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI
images containing four classes: glioma, meningioma, pituitary, and normal. Our
results show that the new method outperforms state-of-the-art methods,
including CNN.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08933" title="Abstract">arXiv:2311.08933</a> (cross-list from eess.SP) [<a href="/pdf/2311.08933" title="Download PDF">pdf</a>, <a href="/ps/2311.08933" title="Download PostScript">ps</a>, <a href="/format/2311.08933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Implementation of a Hybrid Wireless Power and Communication  System for Medical Implants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khaleghi%2C+A">A. Khaleghi</a>, 
<a href="/search/eess?searchtype=author&query=Hasanvand%2C+A">A. Hasanvand</a>, 
<a href="/search/eess?searchtype=author&query=Balasingham%2C+I">I. Balasingham</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE MTT-S International Microwave Biomedical Conference
  (IMBioC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Data collection and analysis from multiple implant nodes in humans can
provide targeted medicine and treatment strategies that can prevent many
chronic diseases. This data can be collected for a long time and processed
using artificial intelligence (AI) techniques in a medical network for early
detection and prevention of diseases. Additionally, machine learning (ML)
algorithms can be applied for the analysis of big data for health monitoring of
the population. Wireless powering, sensing, and communication are essential
parts of future wireless implants that aim to achieve the aforementioned goals.
In this paper, we present the technical development of a wireless implant that
is powered by radio frequency (RF) at 401 MHz, with the sensor data being
communicated to an on-body reader. The implant communication is based on two
simultaneous wireless links: RF backscatter for implant-to-on-body
communication and a galvanic link for intra-body implant-to-implant
connectivity. It is demonstrated that RF powering, using the proposed compact
antennas, can provide an efficient and integrable system for powering up to an
8 cm depth inside body tissues. Furthermore, the same antennas are utilized for
backscatter and galvanic communication.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08945" title="Abstract">arXiv:2311.08945</a> (cross-list from math.OC) [<a href="/pdf/2311.08945" title="Download PDF">pdf</a>, <a href="/format/2311.08945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Single-Loop Algorithm for Decentralized Bilevel Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dong%2C+Y">Youran Dong</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+S">Shiqian Ma</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+J">Junfeng Yang</a>, 
<a href="/search/math?searchtype=author&query=Yin%2C+C">Chao Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Bilevel optimization has received more and more attention recently due to its
wide applications in machine learning. In this paper, we consider bilevel
optimization in decentralized networks. In particular, we propose a novel
single-loop algorithm for solving decentralized bilevel optimization with
strongly convex lower level problem. Our algorithm is fully single-loop and
does not require heavy matrix-vector multiplications when approximating the
hypergradient. Moreover, unlike existing methods for decentralized bilevel
optimization and federated bilevel optimization, our algorithm does not require
any gradient heterogeneity assumption. Our analysis shows that the proposed
algorithm achieves the best known convergence rate for bilevel optimization
algorithms.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08949" title="Abstract">arXiv:2311.08949</a> (cross-list from eess.IV) [<a href="/pdf/2311.08949" title="Download PDF">pdf</a>, <a href="/format/2311.08949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Volume Corrected Mitotic Index Calculation Through  Annotation-Free Deep Learning using Immunohistochemistry as Reference  Standard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ammeling%2C+J">Jonas Ammeling</a>, 
<a href="/search/eess?searchtype=author&query=Hecker%2C+M">Moritz Hecker</a>, 
<a href="/search/eess?searchtype=author&query=Ganz%2C+J">Jonathan Ganz</a>, 
<a href="/search/eess?searchtype=author&query=Donovan%2C+T+A">Taryn A. Donovan</a>, 
<a href="/search/eess?searchtype=author&query=Bertram%2C+C+A">Christof A. Bertram</a>, 
<a href="/search/eess?searchtype=author&query=Breininger%2C+K">Katharina Breininger</a>, 
<a href="/search/eess?searchtype=author&query=Aubreville%2C+M">Marc Aubreville</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The volume-corrected mitotic index (M/V-Index) was shown to provide
prognostic value in invasive breast carcinomas. However, despite its prognostic
significance, it is not established as the standard method for assessing
aggressive biological behaviour, due to the high additional workload associated
with determining the epithelial proportion. In this work, we show that using a
deep learning pipeline solely trained with an annotation-free,
immunohistochemistry-based approach, provides accurate estimations of
epithelial segmentation in canine breast carcinomas. We compare our automatic
framework with the manually annotated M/V-Index in a study with three
board-certified pathologists. Our results indicate that the deep learning-based
pipeline shows expert-level performance, while providing time efficiency and
reproducibility.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08978" title="Abstract">arXiv:2311.08978</a> (cross-list from astro-ph.EP) [<a href="/pdf/2311.08978" title="Download PDF">pdf</a>, <a href="/format/2311.08978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probability of Collision of satellites and space debris for short-term  encounters: Rederivation and fast-to-compute upper and lower bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Ferreira%2C+R">Ricardo Ferreira</a>, 
<a href="/search/astro-ph?searchtype=author&query=Soares%2C+C">Cl&#xe1;udia Soares</a>, 
<a href="/search/astro-ph?searchtype=author&query=Guimar%C3%A3es%2C+M">Marta Guimar&#xe3;es</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">The proliferation of space debris in LEO has become a major concern for the
space industry. With the growing interest in space exploration, the prediction
of potential collisions between objects in orbit has become a crucial issue. It
is estimated that, in orbit, there are millions of fragments a few millimeters
in size and thousands of inoperative satellites and discarded rocket stages.
Given the high speeds that these fragments can reach, even fragments a few
millimeters in size can cause fractures in a satellite's hull or put a serious
crack in the window of a space shuttle. The conventional method proposed by
Akella and Alfriend in 2000 remains widely used to estimate the probability of
collision in short-term encounters. Given the small period of time, it is
assumed that, during the encounter: (1) trajectories are represented by
straight lines with constant velocity; (2) there is no velocity uncertainty and
the position exhibits a stationary distribution throughout the encounter; and
(3) position uncertainties are independent and represented by Gaussian
distributions. This study introduces a novel derivation based on first
principles that naturally allows for tight and fast upper and lower bounds for
the probability of collision. We tested implementations of both probability and
bound computations with the original and our formulation on a real CDM dataset
used in ESA's Collision Avoidance Challenge. Our approach reduces the
calculation of the probability to two one-dimensional integrals and has the
potential to significantly reduce the processing time compared to the
traditional method, from 80% to nearly real-time.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08990" title="Abstract">arXiv:2311.08990</a> (cross-list from quant-ph) [<a href="/pdf/2311.08990" title="Download PDF">pdf</a>, <a href="/format/2311.08990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> sQUlearn $\unicode{x2013}$ A Python Library for Quantum Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kreplin%2C+D+A">David A. Kreplin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Willmann%2C+M">Moritz Willmann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Schnabel%2C+J">Jan Schnabel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Rapp%2C+F">Frederic Rapp</a>, 
<a href="/search/quant-ph?searchtype=author&query=Roth%2C+M">Marco Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10+5 pages, 5+3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">sQUlearn introduces a user-friendly, NISQ-ready Python library for quantum
machine learning (QML), designed for seamless integration with classical
machine learning tools like scikit-learn. The library's dual-layer architecture
serves both QML researchers and practitioners, enabling efficient prototyping,
experimentation, and pipelining. sQUlearn provides a comprehensive toolset that
includes both quantum kernel methods and quantum neural networks, along with
features like customizable data encoding strategies, automated execution
handling, and specialized kernel regularization techniques. By focusing on
NISQ-compatibility and end-to-end automation, sQUlearn aims to bridge the gap
between current quantum computing capabilities and practical machine learning
applications.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09003" title="Abstract">arXiv:2311.09003</a> (cross-list from math.PR) [<a href="/pdf/2311.09003" title="Download PDF">pdf</a>, <a href="/ps/2311.09003" title="Download PostScript">ps</a>, <a href="/format/2311.09003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming under isoperimetry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lytras%2C+I">Iosif Lytras</a>, 
<a href="/search/math?searchtype=author&query=Sabanis%2C+S">Sotirios Sabanis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Numerical Analysis (math.NA); Optimization and Control (math.OC); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this article we propose a novel taming Langevin-based scheme called
$\mathbf{sTULA}$ to sample from distributions with superlinearly growing
log-gradient which also satisfy a Log-Sobolev inequality. We derive
non-asymptotic convergence bounds in $KL$ and consequently total variation and
Wasserstein-$2$ distance from the target measure. Non-asymptotic convergence
guarantees are provided for the performance of the new algorithm as an
optimizer. Finally, some theoretical results on isoperimertic inequalities for
distributions with superlinearly growing gradients are provided. Key findings
are a Log-Sobolev inequality with constant independent of the dimension, in the
presence of a higher order regularization and a Poincare inequality with
constant independent of temperature and dimension under a novel non-convex
theoretical framework.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09010" title="Abstract">arXiv:2311.09010</a> (cross-list from quant-ph) [<a href="/pdf/2311.09010" title="Download PDF">pdf</a>, <a href="/ps/2311.09010" title="Download PostScript">ps</a>, <a href="/format/2311.09010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of sum-of-squares relaxations for the quantum rotor model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Rao%2C+S">Sujit Rao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, submitted to QIP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The noncommutative sum-of-squares (ncSoS) hierarchy was introduced by
Navascu\'{e}s-Pironio-Ac\'{i}n as a sequence of semidefinite programming
relaxations for approximating values of noncommutative polynomial optimization
problems, which were originally intended to generalize quantum values of
nonlocal games. Recent work has started to analyze the hierarchy for
approximating ground energies of local Hamiltonians, initially through rounding
algorithms which output product states for degree-2 ncSoS applied to Quantum
Max-Cut. Some rounding methods are known which output entangled states, but
they use degree-4 ncSoS. Based on this, Hwang-Neeman-Parekh-Thompson-Wright
conjectured that degree-2 ncSoS cannot beat product state approximations for
Quantum Max-Cut and gave a partial proof relying on a conjectural
generalization of Borrell's inequality. In this work we consider a family of
Hamiltonians (called the quantum rotor model in condensed matter literature or
lattice $O(k)$ vector model in quantum field theory) with infinite-dimensional
local Hilbert space $L^{2}(S^{k - 1})$, and show that a degree-2 ncSoS
relaxation approximates the ground state energy better than any product state.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09015" title="Abstract">arXiv:2311.09015</a> (cross-list from stat.ME) [<a href="/pdf/2311.09015" title="Download PDF">pdf</a>, <a href="/format/2311.09015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification and Estimation for Nonignorable Missing Data: A Data  Fusion Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+Z">Zixiao Wang</a>, 
<a href="/search/stat?searchtype=author&query=Ghassami%2C+A">AmirEmad Ghassami</a>, 
<a href="/search/stat?searchtype=author&query=Shpitser%2C+I">Ilya Shpitser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We consider the task of identifying and estimating a parameter of interest in
settings where data is missing not at random (MNAR). In general, such
parameters are not identified without strong assumptions on the missing data
model. In this paper, we take an alternative approach and introduce a method
inspired by data fusion, where information in an MNAR dataset is augmented by
information in an auxiliary dataset subject to missingness at random (MAR). We
show that even if the parameter of interest cannot be identified given either
dataset alone, it can be identified given pooled data, under two complementary
sets of assumptions. We derive an inverse probability weighted (IPW) estimator
for identified parameters, and evaluate the performance of our estimation
strategies via simulation studies.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09019" title="Abstract">arXiv:2311.09019</a> (cross-list from math.OC) [<a href="/pdf/2311.09019" title="Download PDF">pdf</a>, <a href="/ps/2311.09019" title="Download PostScript">ps</a>, <a href="/format/2311.09019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed-Loop Identification of Stabilized Models Using Dual Input-Output  Parameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+R">Ran Chen</a>, 
<a href="/search/math?searchtype=author&query=Srivastava%2C+A">Amber Srivastava</a>, 
<a href="/search/math?searchtype=author&query=Yin%2C+M">Mingzhou Yin</a>, 
<a href="/search/math?searchtype=author&query=Smith%2C+R+S">Roy S. Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper introduces a dual input-output parameterization (dual IOP) for the
identification of linear time-invariant systems from closed-loop data. It draws
inspiration from the recent input-output parameterization developed to
synthesize a stabilizing controller. The controller is parameterized in terms
of closed-loop transfer functions, from the external disturbances to the input
and output of the system, constrained to lie in a given subspace. Analogously,
the dual IOP method parameterizes the unknown plant with analogous closed-loop
transfer functions, also referred to as dual parameters. In this case, these
closed-loop transfer functions are constrained to lie in an affine subspace
guaranteeing that the identified plant is \emph{stabilized} by the known
controller. Compared with existing closed-loop identification techniques
guaranteeing closed-loop stability, such as the dual Youla parameterization,
the dual IOP neither requires a doubly-coprime factorization of the controller
nor a nominal plant that is stabilized by the controller. The dual IOP does not
depend on the order and the state-space realization of the controller either,
as in the dual system-level parameterization. Simulation shows that the dual
IOP outperforms the existing benchmark methods.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09030" title="Abstract">arXiv:2311.09030</a> (cross-list from eess.AS) [<a href="/pdf/2311.09030" title="Download PDF">pdf</a>, <a href="/ps/2311.09030" title="Download PostScript">ps</a>, <a href="/format/2311.09030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-based soundscape analysis: Jointly identifying sound sources and  predicting annoyance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hou%2C+Y">Yuanbo Hou</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+Q">Qiaoqiao Ren</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Huizhong Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Mitchell%2C+A">Andrew Mitchell</a>, 
<a href="/search/eess?searchtype=author&query=Aletta%2C+F">Francesco Aletta</a>, 
<a href="/search/eess?searchtype=author&query=Kang%2C+J">Jian Kang</a>, 
<a href="/search/eess?searchtype=author&query=Botteldooren%2C+D">Dick Botteldooren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The Journal of the Acoustical Society of America, 154 (5), 3145
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Journal of the Acoustical Society of America, 154, 3145 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Soundscape studies typically attempt to capture the perception and
understanding of sonic environments by surveying users. However, for long-term
monitoring or assessing interventions, sound-signal-based approaches are
required. To this end, most previous research focused on psycho-acoustic
quantities or automatic sound recognition. Few attempts were made to include
appraisal (e.g., in circumplex frameworks). This paper proposes an artificial
intelligence (AI)-based dual-branch convolutional neural network with
cross-attention-based fusion (DCNN-CaF) to analyze automatic soundscape
characterization, including sound recognition and appraisal. Using the DeLTA
dataset containing human-annotated sound source labels and perceived annoyance,
the DCNN-CaF is proposed to perform sound source classification (SSC) and
human-perceived annoyance rating prediction (ARP). Experimental findings
indicate that (1) the proposed DCNN-CaF using loudness and Mel features
outperforms the DCNN-CaF using only one of them. (2) The proposed DCNN-CaF with
cross-attention fusion outperforms other typical AI-based models and
soundscape-related traditional machine learning methods on the SSC and ARP
tasks. (3) Correlation analysis reveals that the relationship between sound
sources and annoyance is similar for humans and the proposed AI-based DCNN-CaF
model. (4) Generalization tests show that the proposed model's ARP in the
presence of model-unknown sound sources is consistent with expert expectations
and can explain previous findings from the literature on sound-scape
augmentation.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09042" title="Abstract">arXiv:2311.09042</a> (cross-list from math.CO) [<a href="/pdf/2311.09042" title="Download PDF">pdf</a>, <a href="/ps/2311.09042" title="Download PostScript">ps</a>, <a href="/format/2311.09042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A necessary and sufficient condition for the existence of a properly  coloured $f$-factor in an edge-coloured graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=%C4%8Cada%2C+R">Roman &#x10c;ada</a>, 
<a href="/search/math?searchtype=author&query=Furuya%2C+M">Michitaka Furuya</a>, 
<a href="/search/math?searchtype=author&query=Kimura%2C+K">Kenji Kimura</a>, 
<a href="/search/math?searchtype=author&query=Ozeki%2C+K">Kenta Ozeki</a>, 
<a href="/search/math?searchtype=author&query=Purcell%2C+C">Christopher Purcell</a>, 
<a href="/search/math?searchtype=author&query=Yashima%2C+T">Takamasa Yashima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The main result of this paper is an edge-coloured version of Tutte's
$f$-factor theorem. We give a necessary and sufficient condition for an
edge-coloured graph $G^c$ to have a properly coloured $f$-factor. We state and
prove our result in terms of an auxiliary graph $G_f^c$ which has a 1-factor if
and only if $G^c$ has a properly coloured $f$-factor; this is analogous to the
"short proof" of the $f$-factor theorem given by Tutte in 1954. An alternative
statement, analogous to the original $f$-factor theorem, is also given. We show
that our theorem generalises the $f$-factor theorem; that is, the former
implies the latter. We consider other properties of edge-coloured graphs, and
show that similar results are unlikely for $f$-factors with rainbow components
and distance-$d$-coloured $f$-factors, even when $d=2$ and the number of
colours used is asymptotically minimal.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09065" title="Abstract">arXiv:2311.09065</a> (cross-list from math.OC) [<a href="/pdf/2311.09065" title="Download PDF">pdf</a>, <a href="/format/2311.09065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems  with Convex Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dahal%2C+H">Hari Dahal</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Y">Yangyang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We give a damped proximal augmented Lagrangian method (DPALM) for solving
problems with a weakly-convex objective and convex linear/nonlinear
constraints. Instead of taking a full stepsize, DPALM adopts a damped dual
stepsize to ensure the boundedness of dual iterates. We show that DPALM can
produce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterations
if each DPALM subproblem is solved to a proper accuracy. In addition, we
establish overall iteration complexity of DPALM when the objective is either a
regularized smooth function or in a regularized compositional form. For the
former case, DPALM achieves the complexity of
$\widetilde{\mathcal{O}}\left(\varepsilon^{-2.5} \right)$ to produce an
$\varepsilon$-KKT point by applying an accelerated proximal gradient (APG)
method to each DPALM subproblem. For the latter case, the complexity of DPALM
is $\widetilde{\mathcal{O}}\left(\varepsilon^{-3} \right)$ to produce a near
$\varepsilon$-KKT point by using an APG to solve a Moreau-envelope smoothed
version of each subproblem. Our outer iteration complexity and the overall
complexity either generalize existing best ones from unconstrained or
linear-constrained problems to convex-constrained ones, or improve over the
best-known results on solving the same-structured problems. Furthermore,
numerical experiments on linearly/quadratically constrained non-convex
quadratic programs and linear-constrained robust nonlinear least squares are
conducted to demonstrate the empirical efficiency of the proposed DPALM over
several state-of-the art methods.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09072" title="Abstract">arXiv:2311.09072</a> (cross-list from math.CO) [<a href="/pdf/2311.09072" title="Download PDF">pdf</a>, <a href="/ps/2311.09072" title="Download PostScript">ps</a>, <a href="/format/2311.09072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Trees to Polynomials and Back Again: New Capacity Bounds with  Applications to TSP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gurvits%2C+L">Leonid Gurvits</a>, 
<a href="/search/math?searchtype=author&query=Klein%2C+N">Nathan Klein</a>, 
<a href="/search/math?searchtype=author&query=Leake%2C+J">Jonathan Leake</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Probability (math.PR)

</div>
<p class="mathjax">We give simply exponential lower bounds on the probabilities of a given
strongly Rayleigh distribution, depending only on its expectation. This
resolves a weak version of a problem left open by Karlin-Klein-Oveis Gharan in
their recent breakthrough work on metric TSP, and this resolution leads to a
minor improvement of their approximation factor for metric TSP. Our results
also allow for a more streamlined analysis of the algorithm.
<br />To achieve these new bounds, we build upon the work of Gurvits-Leake on the
use of the productization technique for bounding the capacity of a real stable
polynomial. This technique allows one to reduce certain inequalities for real
stable polynomials to products of affine linear forms, which have an underlying
matrix structure. In this paper, we push this technique further by
characterizing the worst-case polynomials via bipartitioned forests. This rigid
combinatorial structure yields a clean induction argument, which implies our
stronger bounds.
<br />In general, we believe the results of this paper will lead to further
improvement and simplification of the analysis of various combinatorial and
probabilistic bounds and algorithms.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09123" title="Abstract">arXiv:2311.09123</a> (cross-list from math.OC) [<a href="/pdf/2311.09123" title="Download PDF">pdf</a>, <a href="/format/2311.09123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence analysis of a primal-dual optimization-by-continuation  algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Loris%2C+I">Ignace Loris</a>, 
<a href="/search/math?searchtype=author&query=Rebegoldi%2C+S">Simone Rebegoldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We present a numerical iterative optimization algorithm for the minimization
of a cost function consisting of a linear combination of three convex terms,
one of which is differentiable, a second one is prox-simple and the third one
is the composition of a linear map and a prox-simple function. The algorithm's
special feature lies in its ability to approximate, in a single iteration run,
the minimizers of the cost function for many different values of the parameters
determining the relative weight of the three terms in the cost function. A
proof of convergence of the algorithm, based on an inexact variable metric
approach, is also provided. As a special case, one recovers a generalization of
the primal-dual algorithm of Chambolle and Pock, and also of the
proximal-gradient algorithm. Finally, we show how it is related to a
primal-dual iterative algorithm based on inexact proximal evaluations of the
non-smooth terms of the cost function.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09200" title="Abstract">arXiv:2311.09200</a> (cross-list from stat.ML) [<a href="/pdf/2311.09200" title="Download PDF">pdf</a>, <a href="/format/2311.09200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bridges%2C+R+A">Robert A. Bridges</a>, 
<a href="/search/stat?searchtype=author&query=Tombs%2C+V+J">Vandy J. Tombs</a>, 
<a href="/search/stat?searchtype=author&query=Stanley%2C+C+B">Christopher B. Stanley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">In this pioneering work we formulate ExpM+NF, a method for training machine
learning (ML) on private data with pre-specified differentially privacy
guarantee $\varepsilon&gt;0, \delta=0$, by using the Exponential Mechanism (ExpM)
and an auxiliary Normalizing Flow (NF). We articulate theoretical benefits of
ExpM+NF over Differentially Private Stochastic Gradient Descent (DPSGD), the
state-of-the-art (SOTA) and de facto method for differentially private ML, and
we empirically test ExpM+NF against DPSGD using the SOTA implementation (Opacus
with PRV accounting) in multiple classification tasks on the Adult Dataset
(census data) and MIMIC-III Dataset (electronic healthcare records) using
Logistic Regression and GRU-D, a deep learning recurrent neural network with
~20K-100K parameters. In all experiments, ExpM+NF achieves greater than 93% of
the non-private training accuracy (AUC) for $\varepsilon \in [1\mathrm{e}{-3},
1]$, exhibiting greater accuracy (higher AUC) and privacy (lower $\varepsilon$
with $\delta=0$) than DPSGD. Differentially private ML generally considers
$\varepsilon \in [1,10]$ to maintain reasonable accuracy; hence, ExpM+NF's
ability to provide strong accuracy for orders of magnitude better privacy
(smaller $\varepsilon$) substantially pushes what is currently possible in
differentially private ML. Training time results are presented showing ExpM+NF
is comparable to (slightly faster) than DPSGD. Code for these experiments will
be provided after review. Limitations and future directions are provided.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Thu, 16 Nov 23</h3>
<dl>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.04103" title="Abstract">arXiv:2003.04103</a> (replaced) [<a href="/pdf/2003.04103" title="Download PDF">pdf</a>, <a href="/ps/2003.04103" title="Download PostScript">ps</a>, <a href="/format/2003.04103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible numerical optimization with ensmallen
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Curtin%2C+R+R">Ryan R. Curtin</a>, 
<a href="/search/cs?searchtype=author&query=Edel%2C+M">Marcus Edel</a>, 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+R+G">Rahul Ganesh Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Basak%2C+S">Suryoday Basak</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Z">Zhihao Lou</a>, 
<a href="/search/cs?searchtype=author&query=Sanderson%2C+C">Conrad Sanderson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://ensmallen.org/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2004.09283" title="Abstract">arXiv:2004.09283</a> (replaced) [<a href="/pdf/2004.09283" title="Download PDF">pdf</a>, <a href="/ps/2004.09283" title="Download PostScript">ps</a>, <a href="/format/2004.09283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A fast algorithm for computing Bell polynomials based on index  break-downs using prime factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Taghavian%2C+H">Hamed Taghavian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Numerical Analysis (math.NA); Number Theory (math.NT)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.08357" title="Abstract">arXiv:2007.08357</a> (replaced) [<a href="/pdf/2007.08357" title="Download PDF">pdf</a>, <a href="/format/2007.08357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Substring Complexity in Sublinear Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernardini%2C+G">Giulia Bernardini</a>, 
<a href="/search/cs?searchtype=author&query=Fici%2C+G">Gabriele Fici</a>, 
<a href="/search/cs?searchtype=author&query=Gawrychowski%2C+P">Pawe&#x142; Gawrychowski</a>, 
<a href="/search/cs?searchtype=author&query=Pissis%2C+S+P">Solon P. Pissis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ISAAC 2023. Abstract abridged to satisfy arXiv requirements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.05306" title="Abstract">arXiv:2104.05306</a> (replaced) [<a href="/e-print/2104.05306" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WLFC: Write Less in Flash-based Cache
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chaos Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianshun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Need revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.10293" title="Abstract">arXiv:2105.10293</a> (replaced) [<a href="/pdf/2105.10293" title="Download PDF">pdf</a>, <a href="/format/2105.10293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision Questions for Probabilistic Automata on Small Alphabets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bell%2C+P+C">Paul C. Bell</a>, 
<a href="/search/cs?searchtype=author&query=Semukhin%2C+P">Pavel Semukhin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final updated journal pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.14201" title="Abstract">arXiv:2105.14201</a> (replaced) [<a href="/pdf/2105.14201" title="Download PDF">pdf</a>, <a href="/format/2105.14201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CNTLS: A Benchmark Dataset for Abstractive or Extractive Chinese  Timeline Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qianren Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiazheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.05410" title="Abstract">arXiv:2106.05410</a> (replaced) [<a href="/pdf/2106.05410" title="Download PDF">pdf</a>, <a href="/format/2106.05410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DASVDD: Deep Autoencoding Support Vector Data Descriptor for Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hojjati%2C+H">Hadi Hojjati</a>, 
<a href="/search/cs?searchtype=author&query=Armanfard%2C+N">Narges Armanfard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Knowledge and Data Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.03427" title="Abstract">arXiv:2107.03427</a> (replaced) [<a href="/pdf/2107.03427" title="Download PDF">pdf</a>, <a href="/format/2107.03427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Two-Sided Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravindranath%2C+S+S">Sai Srivatsa Ravindranath</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhe Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shira Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jonathan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kominers%2C+S+D">Scott D. Kominers</a>, 
<a href="/search/cs?searchtype=author&query=Parkes%2C+D+C">David C. Parkes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.00206" title="Abstract">arXiv:2108.00206</a> (replaced) [<a href="/pdf/2108.00206" title="Download PDF">pdf</a>, <a href="/ps/2108.00206" title="Download PostScript">ps</a>, <a href="/format/2108.00206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified analysis of finite-size error for periodic Hartree-Fock and  second order M&#xf8;ller-Plesset perturbation theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Xing%2C+X">Xin Xing</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+X">Xiaoxu Li</a>, 
<a href="/search/physics?searchtype=author&query=Lin%2C+L">Lin Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.05976" title="Abstract">arXiv:2201.05976</a> (replaced) [<a href="/e-print/2201.05976" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost Sharing for Connectivity with Budget
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dengji Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Sizhe Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The content are totally reorganized in a new paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.11986" title="Abstract">arXiv:2201.11986</a> (replaced) [<a href="/pdf/2201.11986" title="Download PDF">pdf</a>, <a href="/format/2201.11986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient Masked Averaging for Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tenison%2C+I">Irene Tenison</a>, 
<a href="/search/cs?searchtype=author&query=Sreeramadas%2C+S+A">Sai Aravind Sreeramadas</a>, 
<a href="/search/cs?searchtype=author&query=Mugunthan%2C+V">Vaikkunth Mugunthan</a>, 
<a href="/search/cs?searchtype=author&query=Oyallon%2C+E">Edouard Oyallon</a>, 
<a href="/search/cs?searchtype=author&query=Rish%2C+I">Irina Rish</a>, 
<a href="/search/cs?searchtype=author&query=Belilovsky%2C+E">Eugene Belilovsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.12985" title="Abstract">arXiv:2204.12985</a> (replaced) [<a href="/pdf/2204.12985" title="Download PDF">pdf</a>, <a href="/format/2204.12985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Linear Optics via String Diagrams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=de+Felice%2C+G">Giovanni de Felice</a> (Quantinuum), 
<a href="/search/quant-ph?searchtype=author&query=Coecke%2C+B">Bob Coecke</a> (Quantinuum)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 83-100
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.13039" title="Abstract">arXiv:2204.13039</a> (replaced) [<a href="/pdf/2204.13039" title="Download PDF">pdf</a>, <a href="/ps/2204.13039" title="Download PostScript">ps</a>, <a href="/format/2204.13039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Biset-Enriched Categorical Model for Proto-Quipper with Dynamic  Lifting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+P">Peng Fu</a> (Dalhousie University), 
<a href="/search/cs?searchtype=author&query=Kishida%2C+K">Kohei Kishida</a> (University of Illinois at Urbana-Champaign), 
<a href="/search/cs?searchtype=author&query=Ross%2C+N+J">Neil J. Ross</a> (Dalhousie University), 
<a href="/search/cs?searchtype=author&query=Selinger%2C+P">Peter Selinger</a> (Dalhousie University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 302-342
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Category Theory (math.CT); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.03279" title="Abstract">arXiv:2205.03279</a> (replaced) [<a href="/pdf/2205.03279" title="Download PDF">pdf</a>, <a href="/format/2205.03279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Control and Majorization of Optimal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lefebvre%2C+T">Tom Lefebvre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.01251" title="Abstract">arXiv:2206.01251</a> (replaced) [<a href="/pdf/2206.01251" title="Download PDF">pdf</a>, <a href="/format/2206.01251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Representation Expressiveness and Learnability to Evaluate  Self-Supervised Learning Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuchen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Baratin%2C+A">Aristide Baratin</a>, 
<a href="/search/cs?searchtype=author&query=Laroche%2C+R">Romain Laroche</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+A">Aaron Courville</a>, 
<a href="/search/cs?searchtype=author&query=Sordoni%2C+A">Alessandro Sordoni</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> TMLR 2023 -- Transactions of Machine Learning Research, 11/2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.03532" title="Abstract">arXiv:2206.03532</a> (replaced) [<a href="/pdf/2206.03532" title="Download PDF">pdf</a>, <a href="/format/2206.03532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Q# as a Quantum Algorithmic Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singhal%2C+K">Kartik Singhal</a> (University of Chicago), 
<a href="/search/cs?searchtype=author&query=Hietala%2C+K">Kesha Hietala</a> (University of Maryland), 
<a href="/search/cs?searchtype=author&query=Marshall%2C+S">Sarah Marshall</a> (Microsoft Quantum), 
<a href="/search/cs?searchtype=author&query=Rand%2C+R">Robert Rand</a> (University of Chicago)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 170-191
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Emerging Technologies (cs.ET); Logic in Computer Science (cs.LO); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.04814" title="Abstract">arXiv:2206.04814</a> (replaced) [<a href="/pdf/2206.04814" title="Download PDF">pdf</a>, <a href="/ps/2206.04814" title="Download PostScript">ps</a>, <a href="/format/2206.04814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Properties of Partial Quantum Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Andr%C3%A9s-Mart%C3%AD%C3%82-nez%2C+P">Pablo Andr&#xe9;s-Mart&#xed;&#xc2;-nez</a> (Quantinuum), 
<a href="/search/quant-ph?searchtype=author&query=Heunen%2C+C">Chris Heunen</a> (University of Edinburgh), 
<a href="/search/quant-ph?searchtype=author&query=Kaarsgaard%2C+R">Robin Kaarsgaard</a> (University of Southern Denmark)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 192-207
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO); Category Theory (math.CT); Operator Algebras (math.OA)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.06807" title="Abstract">arXiv:2206.06807</a> (replaced) [<a href="/pdf/2206.06807" title="Download PDF">pdf</a>, <a href="/format/2206.06807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Causal Structure of Semantic Ambiguities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Daphne Wang</a> (University College London), 
<a href="/search/cs?searchtype=author&query=Sadrzadeh%2C+M">Mehrnoosh Sadrzadeh</a> (University College London)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 208-220
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04053" title="Abstract">arXiv:2207.04053</a> (replaced) [<a href="/pdf/2207.04053" title="Download PDF">pdf</a>, <a href="/format/2207.04053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Need and Applicability of Causality for Fair Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Binkyt%C4%97%2C+R">R&#x16b;ta Binkyt&#x117;</a>, 
<a href="/search/cs?searchtype=author&query=Grozdanovski%2C+L">Ljupcho Grozdanovski</a>, 
<a href="/search/cs?searchtype=author&query=Zhioua%2C+S">Sami Zhioua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.05832" title="Abstract">arXiv:2207.05832</a> (replaced) [<a href="/pdf/2207.05832" title="Download PDF">pdf</a>, <a href="/ps/2207.05832" title="Download PostScript">ps</a>, <a href="/format/2207.05832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum de Finetti Theorems as Categorical Limits, and Limits of State  Spaces of C*-algebras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Staton%2C+S">Sam Staton</a> (University of Oxford), 
<a href="/search/quant-ph?searchtype=author&query=Summers%2C+N">Ned Summers</a> (University of Oxford)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 400-414
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.06025" title="Abstract">arXiv:2207.06025</a> (replaced) [<a href="/pdf/2207.06025" title="Download PDF">pdf</a>, <a href="/format/2207.06025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> URANUS: Radio Frequency Tracking, Classification and Identification of  Unmanned Aircraft Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lof%C3%B9%2C+D">Domenico Lof&#xf9;</a>, 
<a href="/search/cs?searchtype=author&query=Di+Gennaro%2C+P">Pietro Di Gennaro</a>, 
<a href="/search/cs?searchtype=author&query=Tedeschi%2C+P">Pietro Tedeschi</a>, 
<a href="/search/cs?searchtype=author&query=Di+Noia%2C+T">Tommaso Di Noia</a>, 
<a href="/search/cs?searchtype=author&query=Di+Sciascio%2C+E">Eugenio Di Sciascio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09858" title="Abstract">arXiv:2207.09858</a> (replaced) [<a href="/pdf/2207.09858" title="Download PDF">pdf</a>, <a href="/ps/2207.09858" title="Download PostScript">ps</a>, <a href="/format/2207.09858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenHPF: General Healthcare Predictive Framework with Multi-task  Multi-source Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hur%2C+K">Kyunghoon Hur</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Jungwoo Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jiyoun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M+J">Min Jae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+E">Eunbyeol Cho</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Seong-Eun Moon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Hak Kim</a>, 
<a href="/search/cs?searchtype=author&query=Atallah%2C+L">Louis Atallah</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Journal of Biomedical and Health Informatics
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Journal of Biomedical and Health Informatics 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13081" title="Abstract">arXiv:2207.13081</a> (replaced) [<a href="/pdf/2207.13081" title="Download PDF">pdf</a>, <a href="/format/2207.13081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Future-Dependent Value-Based Off-Policy Evaluation in POMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uehara%2C+M">Masatoshi Uehara</a>, 
<a href="/search/cs?searchtype=author&query=Kiyohara%2C+H">Haruka Kiyohara</a>, 
<a href="/search/cs?searchtype=author&query=Bennett%2C+A">Andrew Bennett</a>, 
<a href="/search/cs?searchtype=author&query=Chernozhukov%2C+V">Victor Chernozhukov</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kallus%2C+N">Nathan Kallus</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chengchun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was accepted in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.02474" title="Abstract">arXiv:2208.02474</a> (replaced) [<a href="/pdf/2208.02474" title="Download PDF">pdf</a>, <a href="/format/2208.02474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFARnet: deep learning for target detection with constant false alarm  rate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diskin%2C+T">Tzvi Diskin</a>, 
<a href="/search/cs?searchtype=author&query=Beer%2C+Y">Yiftach Beer</a>, 
<a href="/search/cs?searchtype=author&query=Okun%2C+U">Uri Okun</a>, 
<a href="/search/cs?searchtype=author&query=Wiesel%2C+A">Ami Wiesel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2206.05747">arXiv:2206.05747</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.02945" title="Abstract">arXiv:2208.02945</a> (replaced) [<a href="/pdf/2208.02945" title="Download PDF">pdf</a>, <a href="/format/2208.02945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Beam Alignment for Mobile MIMO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Naguib%2C+M">Mohamed Naguib</a>, 
<a href="/search/eess?searchtype=author&query=Shabara%2C+Y">Yahia Shabara</a>, 
<a href="/search/eess?searchtype=author&query=Koksal%2C+C+E">Can Emre Koksal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is accepted to be published by IEEE Transactions on Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07853" title="Abstract">arXiv:2208.07853</a> (replaced) [<a href="/pdf/2208.07853" title="Download PDF">pdf</a>, <a href="/format/2208.07853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Appearance Models for Image Segmentation via Tensor  Factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neto%2C+J+F+S+R">Jeova Farias Sales Rocha Neto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.14407" title="Abstract">arXiv:2208.14407</a> (replaced) [<a href="/pdf/2208.14407" title="Download PDF">pdf</a>, <a href="/format/2208.14407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Model-Based Reinforcement Learning From Abstracted  Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Starre%2C+R+A+N">Rolf A. N. Starre</a>, 
<a href="/search/cs?searchtype=author&query=Loog%2C+M">Marco Loog</a>, 
<a href="/search/cs?searchtype=author&query=Congeduti%2C+E">Elena Congeduti</a>, 
<a href="/search/cs?searchtype=author&query=Oliehoek%2C+F+A">Frans A. Oliehoek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 2 figures, published in Transactions on Machine Learning Research (TMLR) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00805" title="Abstract">arXiv:2210.00805</a> (replaced) [<a href="/pdf/2210.00805" title="Download PDF">pdf</a>, <a href="/format/2210.00805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limitations of neural network training due to numerical instability of  backpropagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karner%2C+C">Clemens Karner</a>, 
<a href="/search/cs?searchtype=author&query=Kazeev%2C+V">Vladimir Kazeev</a>, 
<a href="/search/cs?searchtype=author&query=Petersen%2C+P+C">Philipp Christian Petersen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Functional Analysis (math.FA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02998" title="Abstract">arXiv:2210.02998</a> (replaced) [<a href="/pdf/2210.02998" title="Download PDF">pdf</a>, <a href="/format/2210.02998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ThoraX-PriorNet: A Novel Attention-Based Architecture Using Anatomical  Prior Probability Maps for Thoracic Disease Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hossain%2C+M+I">Md. Iqbal Hossain</a>, 
<a href="/search/eess?searchtype=author&query=Zunaed%2C+M">Mohammad Zunaed</a>, 
<a href="/search/eess?searchtype=author&query=Ahmed%2C+M+K">Md. Kawsar Ahmed</a>, 
<a href="/search/eess?searchtype=author&query=Hossain%2C+S+M+J">S. M. Jawwad Hossain</a>, 
<a href="/search/eess?searchtype=author&query=Hasan%2C+A">Anwarul Hasan</a>, 
<a href="/search/eess?searchtype=author&query=Hasan%2C+T">Taufiq Hasan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13034" title="Abstract">arXiv:2210.13034</a> (replaced) [<a href="/pdf/2210.13034" title="Download PDF">pdf</a>, <a href="/format/2210.13034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Vectors: Subspace Representations for Set Operations of  Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ishibashi%2C+Y">Yoichi Ishibashi</a>, 
<a href="/search/cs?searchtype=author&query=Yokoi%2C+S">Sho Yokoi</a>, 
<a href="/search/cs?searchtype=author&query=Sudoh%2C+K">Katsuhito Sudoh</a>, 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+S">Satoshi Nakamura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06891" title="Abstract">arXiv:2211.06891</a> (replaced) [<a href="/pdf/2211.06891" title="Download PDF">pdf</a>, <a href="/format/2211.06891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Residual Degradation Learning Unfolding Framework with Mixing Priors  across Spectral and Spatial for Compressive Spectral Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dong%2C+Y">Yubo Dong</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+D">Dahua Gao</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+T">Tian Qiu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yuyan Li</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+M">Minxi Yang</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+G">Guangming Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10851" title="Abstract">arXiv:2211.10851</a> (replaced) [<a href="/pdf/2211.10851" title="Download PDF">pdf</a>, <a href="/format/2211.10851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward is not Necessary: How to Create a Modular &amp; Compositional  Self-Preserving Agent for Life-Long Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ringstrom%2C+T+J">Thomas J. Ringstrom</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11483" title="Abstract">arXiv:2211.11483</a> (replaced) [<a href="/pdf/2211.11483" title="Download PDF">pdf</a>, <a href="/format/2211.11483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deanthropomorphising NLP: Can a Language Model Be Conscious?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shardlow%2C+M">Matthew Shardlow</a>, 
<a href="/search/cs?searchtype=author&query=Przyby%C5%82a%2C+P">Piotr Przyby&#x142;a</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14400" title="Abstract">arXiv:2211.14400</a> (replaced) [<a href="/pdf/2211.14400" title="Download PDF">pdf</a>, <a href="/ps/2211.14400" title="Download PostScript">ps</a>, <a href="/format/2211.14400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and  Besov Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Siegel%2C+J+W">Jonathan W. Siegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10368" title="Abstract">arXiv:2212.10368</a> (replaced) [<a href="/pdf/2212.10368" title="Download PDF">pdf</a>, <a href="/format/2212.10368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Event Modeling: Self-Supervised Pretraining for Event Cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klenk%2C+S">Simon Klenk</a>, 
<a href="/search/cs?searchtype=author&query=Bonello%2C+D">David Bonello</a>, 
<a href="/search/cs?searchtype=author&query=Koestler%2C+L">Lukas Koestler</a>, 
<a href="/search/cs?searchtype=author&query=Araslanov%2C+N">Nikita Araslanov</a>, 
<a href="/search/cs?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11223" title="Abstract">arXiv:2212.11223</a> (replaced) [<a href="/pdf/2212.11223" title="Download PDF">pdf</a>, <a href="/format/2212.11223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speedup and efficiency of computational parallelization: A unifying  approach and asymptotic analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schryen%2C+G">Guido Schryen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.12322" title="Abstract">arXiv:2212.12322</a> (replaced) [<a href="/pdf/2212.12322" title="Download PDF">pdf</a>, <a href="/format/2212.12322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infrared Image Super-Resolution: Systematic Review, and Future Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yongsong Huang</a>, 
<a href="/search/eess?searchtype=author&query=Miyazaki%2C+T">Tomo Miyazaki</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xiaofeng Liu</a>, 
<a href="/search/eess?searchtype=author&query=Omachi%2C+S">Shinichiro Omachi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE TNNLS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.12431" title="Abstract">arXiv:2212.12431</a> (replaced) [<a href="/pdf/2212.12431" title="Download PDF">pdf</a>, <a href="/ps/2212.12431" title="Download PostScript">ps</a>, <a href="/format/2212.12431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algebra of L-banded Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+S">Shunqi Huang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+L">Lei Liu</a>, 
<a href="/search/eess?searchtype=author&query=Kurkoski%2C+B+M">Brian M. Kurkoski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT); Rings and Algebras (math.RA)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00732" title="Abstract">arXiv:2301.00732</a> (replaced) [<a href="/pdf/2301.00732" title="Download PDF">pdf</a>, <a href="/ps/2301.00732" title="Download PostScript">ps</a>, <a href="/format/2301.00732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved NP-Hardness of Approximation for Orthogonality Dimension and  Minrank
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chawin%2C+D">Dror Chawin</a>, 
<a href="/search/cs?searchtype=author&query=Haviv%2C+I">Ishay Haviv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM); Information Theory (cs.IT); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02469" title="Abstract">arXiv:2301.02469</a> (replaced) [<a href="/pdf/2301.02469" title="Download PDF">pdf</a>, <a href="/format/2301.02469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cox Point Processes for Multi-Altitude LEO Satellite Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Choi%2C+C">Chang-Sik Choi</a>, 
<a href="/search/eess?searchtype=author&query=Baccelli%2C+F">Fran&#xe7;ois Baccelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04764" title="Abstract">arXiv:2301.04764</a> (replaced) [<a href="/pdf/2301.04764" title="Download PDF">pdf</a>, <a href="/format/2301.04764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Inexact Hypergradients for Bilevel Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ehrhardt%2C+M+J">Matthias J. Ehrhardt</a>, 
<a href="/search/math?searchtype=author&query=Roberts%2C+L">Lindon Roberts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IMA Journal of Applied Mathematics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11723" title="Abstract">arXiv:2301.11723</a> (replaced) [<a href="/pdf/2301.11723" title="Download PDF">pdf</a>, <a href="/format/2301.11723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Program Dependence Net and On-demand Slicing for Property Verification  of Concurrent System and Software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zhijun Ding</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Cong He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12554" title="Abstract">arXiv:2301.12554</a> (replaced) [<a href="/pdf/2301.12554" title="Download PDF">pdf</a>, <a href="/format/2301.12554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive  Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yatong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+B+G">Brendon G. Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+A">Aerin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sojoudi%2C+S">Somayeh Sojoudi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01994" title="Abstract">arXiv:2302.01994</a> (replaced) [<a href="/pdf/2302.01994" title="Download PDF">pdf</a>, <a href="/format/2302.01994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of a fully discretized FDM-FEM scheme for solving  thermo-elastic-damage coupled nonlinear PDE systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Parvizi%2C+M">Maryam Parvizi</a>, 
<a href="/search/math?searchtype=author&query=Khodadadian%2C+A">Amirreza Khodadadian</a>, 
<a href="/search/math?searchtype=author&query=Wick%2C+T">Thomas Wick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02831" title="Abstract">arXiv:2302.02831</a> (replaced) [<a href="/pdf/2302.02831" title="Download PDF">pdf</a>, <a href="/ps/2302.02831" title="Download PostScript">ps</a>, <a href="/format/2302.02831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform Cyclic Group Factorizations of Finite Groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kanai%2C+K">Kazuki Kanai</a>, 
<a href="/search/math?searchtype=author&query=Miyamoto%2C+K">Kengo Miyamoto</a>, 
<a href="/search/math?searchtype=author&query=Nuida%2C+K">Koji Nuida</a>, 
<a href="/search/math?searchtype=author&query=Shinagawa%2C+K">Kazumasa Shinagawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages. To appear in Communications in Algebra
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Group Theory (math.GR)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05763" title="Abstract">arXiv:2302.05763</a> (replaced) [<a href="/pdf/2302.05763" title="Download PDF">pdf</a>, <a href="/format/2302.05763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Multi-User Activity Recognition through Facilitated Training  Data and Deep Learning for Human-Robot Collaboration Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Semeraro%2C+F">Francesco Semeraro</a>, 
<a href="/search/cs?searchtype=author&query=Carberry%2C+J">Jon Carberry</a>, 
<a href="/search/cs?searchtype=author&query=Cangelosi%2C+A">Angelo Cangelosi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the accepted manuscript. Please see published version at <a href="https://ieeexplore.ieee.org/document/10191782">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 International Joint Conference on Neural Networks (IJCNN),
  1-9
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08490" title="Abstract">arXiv:2302.08490</a> (replaced) [<a href="/pdf/2302.08490" title="Download PDF">pdf</a>, <a href="/format/2302.08490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensorial parametric model order reduction of nonlinear dynamical  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mamonov%2C+A+V">Alexander V. Mamonov</a>, 
<a href="/search/math?searchtype=author&query=Olshanskii%2C+M+A">Maxim A. Olshanskii</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01350" title="Abstract">arXiv:2303.01350</a> (replaced) [<a href="/pdf/2303.01350" title="Download PDF">pdf</a>, <a href="/format/2303.01350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Securing Verified IO Programs Against Unverified Code in F*
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andrici%2C+C">Cezar-Constantin Andrici</a>, 
<a href="/search/cs?searchtype=author&query=Ciobaca%2C+S">Stefan Ciobaca</a>, 
<a href="/search/cs?searchtype=author&query=Hritcu%2C+C">Catalin Hritcu</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+G">Guido Mart&#xed;nez</a>, 
<a href="/search/cs?searchtype=author&query=Rivas%2C+E">Exequiel Rivas</a>, 
<a href="/search/cs?searchtype=author&query=Tanter%2C+%C3%89">&#xc9;ric Tanter</a>, 
<a href="/search/cs?searchtype=author&query=Winterhalter%2C+T">Th&#xe9;o Winterhalter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> POPL'24 camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01676" title="Abstract">arXiv:2303.01676</a> (replaced) [<a href="/pdf/2303.01676" title="Download PDF">pdf</a>, <a href="/format/2303.01676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> eViper: A Scalable Platform for Untethered Modular Soft Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hsin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhiwu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Prakhar Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Afridi%2C+W">Wali Afridi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Ben Kim</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+S">Sigurd Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+N">Naveen Verma</a>, 
<a href="/search/cs?searchtype=author&query=Sturm%2C+J+C">James C. Sturm</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minjie Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 21 figures, accepted by IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07208" title="Abstract">arXiv:2303.07208</a> (replaced) [<a href="/pdf/2303.07208" title="Download PDF">pdf</a>, <a href="/format/2303.07208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social network analysis of manga: similarities to real-world social  networks and trends over decades
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sugishita%2C+K">Kashin Sugishita</a>, 
<a href="/search/physics?searchtype=author&query=Masuda%2C+N">Naoki Masuda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures, 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Network Science 8, 79 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14516" title="Abstract">arXiv:2303.14516</a> (replaced) [<a href="/pdf/2303.14516" title="Download PDF">pdf</a>, <a href="/format/2303.14516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OVeNet: Offset Vector Network for Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alexandropoulos%2C+S">Stamatis Alexandropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Sakaridis%2C+C">Christos Sakaridis</a>, 
<a href="/search/cs?searchtype=author&query=Maragos%2C+P">Petros Maragos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17403" title="Abstract">arXiv:2303.17403</a> (replaced) [<a href="/pdf/2303.17403" title="Download PDF">pdf</a>, <a href="/format/2303.17403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix Product Belief Propagation for reweighted stochastic dynamics  over graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Crotti%2C+S">Stefano Crotti</a>, 
<a href="/search/cond-mat?searchtype=author&query=Braunstein%2C+A">Alfredo Braunstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures, 1 table, appendix
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PNAS, 120 (47) e2307935120 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Mechanics (cond-mat.stat-mech)</span>; Social and Information Networks (cs.SI); Populations and Evolution (q-bio.PE)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03292" title="Abstract">arXiv:2304.03292</a> (replaced) [<a href="/pdf/2304.03292" title="Download PDF">pdf</a>, <a href="/format/2304.03292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SE-shapelets: Semi-supervised Clustering of Time Series Using  Representative Shapelets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+B">Borui Cai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Guangyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuiqiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yong Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+C">Chi-Hung Chi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03571" title="Abstract">arXiv:2304.03571</a> (replaced) [<a href="/pdf/2304.03571" title="Download PDF">pdf</a>, <a href="/format/2304.03571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $&#x3b2;$-Variational autoencoders and transformers for reduced-order  modelling of fluid flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Solera-Rico%2C+A">Alberto Solera-Rico</a> (1 and 2), 
<a href="/search/physics?searchtype=author&query=Vila%2C+C+S">Carlos Sanmiguel Vila</a> (1 and 2), 
<a href="/search/physics?searchtype=author&query=G%C3%B3mez%2C+M+A">M. A. G&#xf3;mez</a> (2), 
<a href="/search/physics?searchtype=author&query=Wang%2C+Y">Yuning Wang</a> (4), 
<a href="/search/physics?searchtype=author&query=Almashjary%2C+A">Abdulrahman Almashjary</a> (3), 
<a href="/search/physics?searchtype=author&query=Dawson%2C+S+T+M">Scott T. M. Dawson</a> (3), 
<a href="/search/physics?searchtype=author&query=Vinuesa%2C+R">Ricardo Vinuesa</a> (4) (1: Aerospace Engineering Research Group, Universidad Carlos III de Madrid, Legan&#xe9;s, Spain 2: Subdirectorate General of Terrestrial Systems, Spanish National Institute for Aerospace Technology (INTA), San Mart&#xed;n de la Vega, Spain 3: Mechanical, Materials, and Aerospace Engineering Department, Illinois Institute of Technology, Chicago, USA 4: FLOW, Engineering Mechanics, KTH Royal Institute of Technology, Stockholm, Sweden)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13217" title="Abstract">arXiv:2304.13217</a> (replaced) [<a href="/pdf/2304.13217" title="Download PDF">pdf</a>, <a href="/format/2304.13217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfiguration of the Union of Arborescences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+Y">Yusuke Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Mahara%2C+R">Ryoga Mahara</a>, 
<a href="/search/cs?searchtype=author&query=Schwarcz%2C+T">Tam&#xe1;s Schwarcz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03111" title="Abstract">arXiv:2305.03111</a> (replaced) [<a href="/pdf/2305.03111" title="Download PDF">pdf</a>, <a href="/format/2305.03111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLM Already Serve as A Database Interface? A BIg Bench for  Large-Scale Database Grounded Text-to-SQLs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+B">Binyuan Hui</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+G">Ge Qu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiaxi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Binhua Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bailin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bowen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+R">Rongyu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+R">Ruiying Geng</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+N">Nan Huo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuanhe Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenhao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C+C">Kevin C.C. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Reynold Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03129" title="Abstract">arXiv:2305.03129</a> (replaced) [<a href="/pdf/2305.03129" title="Download PDF">pdf</a>, <a href="/format/2305.03129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Programming-by-Demonstration for Long-Horizon Robot Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patton%2C+N">Noah Patton</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+K">Kia Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Missula%2C+M">Meghana Missula</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Dillig%2C+I">I&#x15f;il Dillig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 Pages, Extended Version of POPL 2024 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04978" title="Abstract">arXiv:2305.04978</a> (replaced) [<a href="/pdf/2305.04978" title="Download PDF">pdf</a>, <a href="/format/2305.04978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Howard%2C+P">Phillip Howard</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junlin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lal%2C+V">Vasudev Lal</a>, 
<a href="/search/cs?searchtype=author&query=Singer%2C+G">Gadi Singer</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Swayamdipta%2C+S">Swabha Swayamdipta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06295" title="Abstract">arXiv:2305.06295</a> (replaced) [<a href="/pdf/2305.06295" title="Download PDF">pdf</a>, <a href="/format/2305.06295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extracting Diagnosis Pathways from Electronic Health Records Using Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muyama%2C+L">Lillian Muyama</a>, 
<a href="/search/cs?searchtype=author&query=Neuraz%2C+A">Antoine Neuraz</a>, 
<a href="/search/cs?searchtype=author&query=Coulet%2C+A">Adrien Coulet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06348" title="Abstract">arXiv:2305.06348</a> (replaced) [<a href="/pdf/2305.06348" title="Download PDF">pdf</a>, <a href="/ps/2305.06348" title="Download PostScript">ps</a>, <a href="/format/2305.06348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised learning with probabilistic morphisms and kernel mean  embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=L%C3%AA%2C+H+V">H&#xf4;ng V&#xe2;n L&#xea;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V5: 51 p., minor correction, Corollary 6.13(2) added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Category Theory (math.CT); Functional Analysis (math.FA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08703" title="Abstract">arXiv:2305.08703</a> (replaced) [<a href="/pdf/2305.08703" title="Download PDF">pdf</a>, <a href="/format/2305.08703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Schema-adaptable Knowledge Graph Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Hongbin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+H">Honghao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09863" title="Abstract">arXiv:2305.09863</a> (replaced) [<a href="/pdf/2305.09863" title="Download PDF">pdf</a>, <a href="/format/2305.09863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining black box text modules in natural language with language  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+C">Chandan Singh</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+A+R">Aliyah R. Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Antonello%2C+R">Richard Antonello</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shailee Jain</a>, 
<a href="/search/cs?searchtype=author&query=Huth%2C+A+G">Alexander G. Huth</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10722" title="Abstract">arXiv:2305.10722</a> (replaced) [<a href="/pdf/2305.10722" title="Download PDF">pdf</a>, <a href="/format/2305.10722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discriminative Diffusion Models as Few-shot Vision and Language Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuehai He</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Weixi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tsu-Jui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Jampani%2C+V">Varun Jampani</a>, 
<a href="/search/cs?searchtype=author&query=Akula%2C+A">Arjun Akula</a>, 
<a href="/search/cs?searchtype=author&query=Narayana%2C+P">Pradyumna Narayana</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Sugato Basu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+E">Xin Eric Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11531" title="Abstract">arXiv:2305.11531</a> (replaced) [<a href="/pdf/2305.11531" title="Download PDF">pdf</a>, <a href="/format/2305.11531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing to new geometries with Geometry-Aware Autoregressive Models  (GAAMs) for fast calorimeter simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Liu%2C+J">Junze Liu</a>, 
<a href="/search/physics?searchtype=author&query=Ghosh%2C+A">Aishik Ghosh</a>, 
<a href="/search/physics?searchtype=author&query=Smith%2C+D">Dylan Smith</a>, 
<a href="/search/physics?searchtype=author&query=Baldi%2C+P">Pierre Baldi</a>, 
<a href="/search/physics?searchtype=author&query=Whiteson%2C+D">Daniel Whiteson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); High Energy Physics - Phenomenology (hep-ph); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12057" title="Abstract">arXiv:2305.12057</a> (replaced) [<a href="/pdf/2305.12057" title="Download PDF">pdf</a>, <a href="/format/2305.12057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurate Knowledge Distillation with n-best Reranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Setiawan%2C+H">Hendra Setiawan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12102" title="Abstract">arXiv:2305.12102</a> (replaced) [<a href="/pdf/2305.12102" title="Download PDF">pdf</a>, <a href="/format/2305.12102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Embedding: Battle-Tested Feature Representations for Web-Scale  ML Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coleman%2C+B">Benjamin Coleman</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wang-Cheng Kang</a>, 
<a href="/search/cs?searchtype=author&query=Fahrbach%2C+M">Matthew Fahrbach</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruoxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lichan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D+Z">Derek Zhiyuan Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS'23 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12308" title="Abstract">arXiv:2305.12308</a> (replaced) [<a href="/pdf/2305.12308" title="Download PDF">pdf</a>, <a href="/format/2305.12308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIMO Evolution toward 6G: End-User-Centric Collaborative MIMO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsai%2C+L">Lung-Sheng Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Shih%2C+S">Shang-Ling Shih</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+P">Pei-Kai Liao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Chao-Kai Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 1 table. This work has been accepted in IEEE Communications Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12437" title="Abstract">arXiv:2305.12437</a> (replaced) [<a href="/pdf/2305.12437" title="Download PDF">pdf</a>, <a href="/format/2305.12437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PLAR: Prompt Learning for Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+R">Ruiqi Xian</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+T">Tianrui Guan</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12704" title="Abstract">arXiv:2305.12704</a> (replaced) [<a href="/pdf/2305.12704" title="Download PDF">pdf</a>, <a href="/format/2305.12704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rotation-Constrained Cross-View Feature Fusion for Multi-View  Appearance-based Gaze Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hisadome%2C+Y">Yoichiro Hisadome</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jiawei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Sugano%2C+Y">Yusuke Sugano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV2024. The code will be available at <a href="https://github.com/ut-vision/Rot-MVGaze">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13062" title="Abstract">arXiv:2305.13062</a> (replaced) [<a href="/pdf/2305.13062" title="Download PDF">pdf</a>, <a href="/format/2305.13062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT4Table: Can Large Language Models Understand Structured Table Data? A  Benchmark and Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yuan Sui</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mengyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shi Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted as a full paper at WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14332" title="Abstract">arXiv:2305.14332</a> (replaced) [<a href="/pdf/2305.14332" title="Download PDF">pdf</a>, <a href="/format/2305.14332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating and Modeling Attribution for Cross-Lingual Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muller%2C+B">Benjamin Muller</a>, 
<a href="/search/cs?searchtype=author&query=Wieting%2C+J">John Wieting</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+J+H">Jonathan H. Clark</a>, 
<a href="/search/cs?searchtype=author&query=Kwiatkowski%2C+T">Tom Kwiatkowski</a>, 
<a href="/search/cs?searchtype=author&query=Ruder%2C+S">Sebastian Ruder</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+L+B">Livio Baldini Soares</a>, 
<a href="/search/cs?searchtype=author&query=Aharoni%2C+R">Roee Aharoni</a>, 
<a href="/search/cs?searchtype=author&query=Herzig%2C+J">Jonathan Herzig</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a long paper at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14336" title="Abstract">arXiv:2305.14336</a> (replaced) [<a href="/pdf/2305.14336" title="Download PDF">pdf</a>, <a href="/format/2305.14336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Schema-Driven Information Extraction from Heterogeneous Tables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+F">Fan Bai</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Junmo Kang</a>, 
<a href="/search/cs?searchtype=author&query=Stanovsky%2C+G">Gabriel Stanovsky</a>, 
<a href="/search/cs?searchtype=author&query=Freitag%2C+D">Dayne Freitag</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14463" title="Abstract">arXiv:2305.14463</a> (replaced) [<a href="/pdf/2305.14463" title="Download PDF">pdf</a>, <a href="/format/2305.14463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain  Readability Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naous%2C+T">Tarek Naous</a>, 
<a href="/search/cs?searchtype=author&query=Ryan%2C+M+J">Michael J. Ryan</a>, 
<a href="/search/cs?searchtype=author&query=Lavrouk%2C+A">Anton Lavrouk</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+M">Mohit Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We have added French and Russian as two new languages to the corpus
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14489" title="Abstract">arXiv:2305.14489</a> (replaced) [<a href="/pdf/2305.14489" title="Download PDF">pdf</a>, <a href="/format/2305.14489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Robust Coreference Resolvers?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+N+T">Nghia T. Le</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14493" title="Abstract">arXiv:2305.14493</a> (replaced) [<a href="/pdf/2305.14493" title="Download PDF">pdf</a>, <a href="/format/2305.14493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do prompt positions really matter?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Junyu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Middleton%2C+S+E">Stuart E. Middleton</a>, 
<a href="/search/cs?searchtype=author&query=Niranjan%2C+M">Mahesan Niranjan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14613" title="Abstract">arXiv:2305.14613</a> (replaced) [<a href="/pdf/2305.14613" title="Download PDF">pdf</a>, <a href="/format/2305.14613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selectively Answering Ambiguous Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cole%2C+J+R">Jeremy R. Cole</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M+J+Q">Michael J.Q. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gillick%2C+D">Daniel Gillick</a>, 
<a href="/search/cs?searchtype=author&query=Eisenschlos%2C+J+M">Julian Martin Eisenschlos</a>, 
<a href="/search/cs?searchtype=author&query=Dhingra%2C+B">Bhuwan Dhingra</a>, 
<a href="/search/cs?searchtype=author&query=Eisenstein%2C+J">Jacob Eisenstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EMNLP 2023. 9 pages, 5 figures, 2 pages of appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14658" title="Abstract">arXiv:2305.14658</a> (replaced) [<a href="/pdf/2305.14658" title="Download PDF">pdf</a>, <a href="/format/2305.14658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluate What You Can&#x27;t Evaluate: Unassessable Quality for Generated  Response
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongkang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Daling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14964" title="Abstract">arXiv:2305.14964</a> (replaced) [<a href="/pdf/2305.14964" title="Download PDF">pdf</a>, <a href="/format/2305.14964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Multidimensional Political Incivility on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pendzel%2C+S">Sagi Pendzel</a>, 
<a href="/search/cs?searchtype=author&query=Lotan%2C+N">Nir Lotan</a>, 
<a href="/search/cs?searchtype=author&query=Zoizner%2C+A">Alon Zoizner</a>, 
<a href="/search/cs?searchtype=author&query=Minkov%2C+E">Einat Minkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17170" title="Abstract">arXiv:2305.17170</a> (replaced) [<a href="/pdf/2305.17170" title="Download PDF">pdf</a>, <a href="/format/2305.17170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Bounds for Learning with Vector-Valued Random Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lanthaler%2C+S">Samuel Lanthaler</a>, 
<a href="/search/stat?searchtype=author&query=Nelsen%2C+N+H">Nicholas H. Nelsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 1 table, 3 figures. NeurIPS 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17208" title="Abstract">arXiv:2305.17208</a> (replaced) [<a href="/pdf/2305.17208" title="Download PDF">pdf</a>, <a href="/format/2305.17208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Categorical Representation Language and Computational System for  Knowledge-Based Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aguinaldo%2C+A">Angeline Aguinaldo</a>, 
<a href="/search/cs?searchtype=author&query=Patterson%2C+E">Evan Patterson</a>, 
<a href="/search/cs?searchtype=author&query=Fairbanks%2C+J">James Fairbanks</a>, 
<a href="/search/cs?searchtype=author&query=Regli%2C+W">William Regli</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz%2C+J">Jaime Ruiz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO); Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18497" title="Abstract">arXiv:2305.18497</a> (replaced) [<a href="/pdf/2305.18497" title="Download PDF">pdf</a>, <a href="/format/2305.18497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Learning via Prediction Consensus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+D">Dongyang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Mendler-D%C3%BCnner%2C+C">Celestine Mendler-D&#xfc;nner</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19011" title="Abstract">arXiv:2305.19011</a> (replaced) [<a href="/pdf/2305.19011" title="Download PDF">pdf</a>, <a href="/format/2305.19011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yu-Hsiang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+H">Huang-Yu Chen</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Hsu%2C+W">Winston Hsu</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19179" title="Abstract">arXiv:2305.19179</a> (replaced) [<a href="/pdf/2305.19179" title="Download PDF">pdf</a>, <a href="/ps/2305.19179" title="Download PostScript">ps</a>, <a href="/format/2305.19179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Quasi-Newton and Anderson Acceleration Framework with Explicit  Global (Accelerated) Convergence Rates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Scieur%2C+D">Damien Scieur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03530" title="Abstract">arXiv:2306.03530</a> (replaced) [<a href="/pdf/2306.03530" title="Download PDF">pdf</a>, <a href="/format/2306.03530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RLtools: A Fast, Portable Deep Reinforcement Learning Library for  Continuous Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eschmann%2C+J">Jonas Eschmann</a>, 
<a href="/search/cs?searchtype=author&query=Albani%2C+D">Dario Albani</a>, 
<a href="/search/cs?searchtype=author&query=Loianno%2C+G">Giuseppe Loianno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://rl.tools">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03621" title="Abstract">arXiv:2306.03621</a> (replaced) [<a href="/pdf/2306.03621" title="Download PDF">pdf</a>, <a href="/format/2306.03621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pathwidth vs cocircumference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bria%C5%84ski%2C+M">Marcin Bria&#x144;ski</a>, 
<a href="/search/math?searchtype=author&query=Joret%2C+G">Gwena&#xeb;l Joret</a>, 
<a href="/search/math?searchtype=author&query=Seweryn%2C+M+T">Micha&#x142; T. Seweryn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: revised following the referees' comments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06190" title="Abstract">arXiv:2306.06190</a> (replaced) [<a href="/pdf/2306.06190" title="Download PDF">pdf</a>, <a href="/format/2306.06190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $FastDoc$: Domain-Specific Fast Pre-training Technique using  Document-Level Metadata and Taxonomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nandy%2C+A">Abhilash Nandy</a>, 
<a href="/search/cs?searchtype=author&query=Kapadnis%2C+M+N">Manav Nitin Kapadnis</a>, 
<a href="/search/cs?searchtype=author&query=Patnaik%2C+S">Sohan Patnaik</a>, 
<a href="/search/cs?searchtype=author&query=Butala%2C+Y+P">Yash Parag Butala</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+P">Pawan Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+N">Niloy Ganguly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09012" title="Abstract">arXiv:2306.09012</a> (replaced) [<a href="/pdf/2306.09012" title="Download PDF">pdf</a>, <a href="/format/2306.09012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Yes, we CANN: Constrained Approximate Nearest Neighbors for local  feature-based visual localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aiger%2C+D">Dror Aiger</a>, 
<a href="/search/cs?searchtype=author&query=Araujo%2C+A">Andr&#xe9; Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Lynen%2C+S">Simon Lynen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09690" title="Abstract">arXiv:2306.09690</a> (replaced) [<a href="/pdf/2306.09690" title="Download PDF">pdf</a>, <a href="/format/2306.09690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Physiological and Psychological Responses in Virtual  Reality and Flat Screen Gaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vatsal%2C+R">Ritik Vatsal</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Shrivatsa Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Thareja%2C+R">Rushil Thareja</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+M">Mrinmoy Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+O">Ojaswa Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+J">Jainendra Shukla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE Transactions on Affective Computing for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11281" title="Abstract">arXiv:2306.11281</a> (replaced) [<a href="/pdf/2306.11281" title="Download PDF">pdf</a>, <a href="/format/2306.11281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Characterizing Domain Counterfactuals For Invertible Latent  Causal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zeyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+R">Ruqi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Kulinski%2C+S">Sean Kulinski</a>, 
<a href="/search/cs?searchtype=author&query=Kocaoglu%2C+M">Murat Kocaoglu</a>, 
<a href="/search/cs?searchtype=author&query=Inouye%2C+D+I">David I. Inouye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11971" title="Abstract">arXiv:2306.11971</a> (replaced) [<a href="/pdf/2306.11971" title="Download PDF">pdf</a>, <a href="/format/2306.11971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdCraft: An Advanced Reinforcement Learning Benchmark Environment for  Search Engine Marketing Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gomrokchi%2C+M">Maziar Gomrokchi</a>, 
<a href="/search/cs?searchtype=author&query=Levin%2C+O">Owen Levin</a>, 
<a href="/search/cs?searchtype=author&query=Roach%2C+J">Jeffrey Roach</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+J">Jonah White</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12370" title="Abstract">arXiv:2306.12370</a> (replaced) [<a href="/pdf/2306.12370" title="Download PDF">pdf</a>, <a href="/format/2306.12370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PriorBand: Practical Hyperparameter Optimization in the Age of Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mallik%2C+N">Neeratyoy Mallik</a>, 
<a href="/search/cs?searchtype=author&query=Bergman%2C+E">Edward Bergman</a>, 
<a href="/search/cs?searchtype=author&query=Hvarfner%2C+C">Carl Hvarfner</a>, 
<a href="/search/cs?searchtype=author&query=Stoll%2C+D">Danny Stoll</a>, 
<a href="/search/cs?searchtype=author&query=Janowski%2C+M">Maciej Janowski</a>, 
<a href="/search/cs?searchtype=author&query=Lindauer%2C+M">Marius Lindauer</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+L">Luigi Nardi</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13333" title="Abstract">arXiv:2306.13333</a> (replaced) [<a href="/pdf/2306.13333" title="Download PDF">pdf</a>, <a href="/format/2306.13333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Optimization for HVAC Systems in Multi-VAV Open Offices: A Deep  Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xiwen Chen</a>, 
<a href="/search/eess?searchtype=author&query=Vital%2C+N">Natan Vital</a>, 
<a href="/search/eess?searchtype=author&query=Duffy%2C+E">Edward.Duffy</a>, 
<a href="/search/eess?searchtype=author&query=Razi%2C+A">Abolfazl Razi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14703" title="Abstract">arXiv:2306.14703</a> (replaced) [<a href="/pdf/2306.14703" title="Download PDF">pdf</a>, <a href="/ps/2306.14703" title="Download PostScript">ps</a>, <a href="/format/2306.14703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repetition and recurrence times in the case of a stretched exponential  growth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%C4%99bowski%2C+%C5%81">&#x141;ukasz D&#x119;bowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16248" title="Abstract">arXiv:2306.16248</a> (replaced) [<a href="/pdf/2306.16248" title="Download PDF">pdf</a>, <a href="/format/2306.16248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent SDEs on Homogeneous Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Sebastian Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Graf%2C+F">Florian Graf</a>, 
<a href="/search/cs?searchtype=author&query=Kwitt%2C+R">Roland Kwitt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17108" title="Abstract">arXiv:2306.17108</a> (replaced) [<a href="/pdf/2306.17108" title="Download PDF">pdf</a>, <a href="/format/2306.17108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ManimML: Communicating Machine Learning Architectures with Animation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Helbling%2C+A">Alec Helbling</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+D+H">Duen Horng Chau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Winner of the Best Poster Award at IEEE VIS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17833" title="Abstract">arXiv:2306.17833</a> (replaced) [<a href="/pdf/2306.17833" title="Download PDF">pdf</a>, <a href="/format/2306.17833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resetting the Optimizer in Deep RL: An Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asadi%2C+K">Kavosh Asadi</a>, 
<a href="/search/cs?searchtype=author&query=Fakoor%2C+R">Rasool Fakoor</a>, 
<a href="/search/cs?searchtype=author&query=Sabach%2C+S">Shoham Sabach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00146" title="Abstract">arXiv:2307.00146</a> (replaced) [<a href="/pdf/2307.00146" title="Download PDF">pdf</a>, <a href="/format/2307.00146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bluefish: A Relational Framework for Graphic Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pollock%2C+J">Josh Pollock</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+C">Catherine Mei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Grace Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jackson%2C+D">Daniel Jackson</a>, 
<a href="/search/cs?searchtype=author&query=Satyanarayan%2C+A">Arvind Satyanarayan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Human-Computer Interaction (cs.HC); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02185" title="Abstract">arXiv:2307.02185</a> (replaced) [<a href="/pdf/2307.02185" title="Download PDF">pdf</a>, <a href="/format/2307.02185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Citation: A Key to Building Responsible and Accountable Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06507" title="Abstract">arXiv:2307.06507</a> (replaced) [<a href="/pdf/2307.06507" title="Download PDF">pdf</a>, <a href="/format/2307.06507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Nonalcoholic Fatty Liver Disease Classification Performance  With Latent Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hardy%2C+R">Romain Hardy</a>, 
<a href="/search/cs?searchtype=author&query=Klepich%2C+J">Joe Klepich</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+R">Ryan Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+S">Steve Hall</a>, 
<a href="/search/cs?searchtype=author&query=Villareal%2C+J">Jericho Villareal</a>, 
<a href="/search/cs?searchtype=author&query=Ilin%2C+C">Cornelia Ilin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07374" title="Abstract">arXiv:2307.07374</a> (replaced) [<a href="/pdf/2307.07374" title="Download PDF">pdf</a>, <a href="/ps/2307.07374" title="Download PostScript">ps</a>, <a href="/format/2307.07374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Budget Selection in a Competitive Autobidding World
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yiding Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lucier%2C+B">Brendan Lucier</a>, 
<a href="/search/cs?searchtype=author&query=Slivkins%2C+A">Aleksandrs Slivkins</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07705" title="Abstract">arXiv:2307.07705</a> (replaced) [<a href="/pdf/2307.07705" title="Download PDF">pdf</a>, <a href="/format/2307.07705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPET: Effective Parameter-Efficient Tuning for Compressed Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuxiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08423" title="Abstract">arXiv:2307.08423</a> (replaced) [<a href="/pdf/2307.08423" title="Download PDF">pdf</a>, <a href="/format/2307.08423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Intelligence for Science in Quantum, Atomistic, and Continuum  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Limei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Helwig%2C+J">Jacob Helwig</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Youzhi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Cong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yaochen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Meng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuchao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+K">Keqiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+K">Keir Adams</a>, 
<a href="/search/cs?searchtype=author&query=Weiler%2C+M">Maurice Weiler</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiner Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tianfan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yucheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">YuQing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Strasser%2C+A">Alex Strasser</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shenglong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuanqi Du</a>, 
<a href="/search/cs?searchtype=author&query=Saxton%2C+A">Alexandra Saxton</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Hongyi Ling</a>, 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+H">Hannah Lawrence</a>, 
<a href="/search/cs?searchtype=author&query=St%C3%A4rk%2C+H">Hannes St&#xe4;rk</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+S">Shurui Gui</a>, 
<a href="/search/cs?searchtype=author&query=Edwards%2C+C">Carl Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+N">Nicholas Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ladera%2C+A">Adriana Ladera</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tailin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hofgard%2C+E+F">Elyssa F. Hofgard</a>, 
<a href="/search/cs?searchtype=author&query=Tehrani%2C+A+M">Aria Mansouri Tehrani</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Daigavane%2C+A">Ameya Daigavane</a>, 
<a href="/search/cs?searchtype=author&query=Bohde%2C+M">Montgomery Bohde</a>, 
<a href="/search/cs?searchtype=author&query=Kurtin%2C+J">Jerry Kurtin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+T">Tuong Phung</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minkai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+C+K">Chaitanya K. Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+S+V">Simon V. Mathis</a>, 
<a href="/search/cs?searchtype=author&query=Azizzadenesheli%2C+K">Kamyar Azizzadenesheli</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+A">Ada Fang</a>, 
<a href="/search/cs?searchtype=author&query=Aspuru-Guzik%2C+A">Al&#xe1;n Aspuru-Guzik</a>, 
<a href="/search/cs?searchtype=author&query=Bekkers%2C+E">Erik Bekkers</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Zitnik%2C+M">Marinka Zitnik</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnnemann%2C+S">Stephan G&#xfc;nnemann</a>, 
<a href="/search/cs?searchtype=author&query=Leskovec%2C+J">Jure Leskovec</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Barzilay%2C+R">Regina Barzilay</a>,  et al. (6 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08433" title="Abstract">arXiv:2307.08433</a> (replaced) [<a href="/pdf/2307.08433" title="Download PDF">pdf</a>, <a href="/format/2307.08433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From random-walks to graph-sprints: a low-latency node embedding  framework on continuous-time dynamic graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eddin%2C+A+N">Ahmad Naser Eddin</a>, 
<a href="/search/cs?searchtype=author&query=Bono%2C+J">Jacopo Bono</a>, 
<a href="/search/cs?searchtype=author&query=Apar%C3%ADcio%2C+D">David Apar&#xed;cio</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+H">Hugo Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Ascens%C3%A3o%2C+J">Jo&#xe3;o Ascens&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+P">Pedro Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Bizarro%2C+P">Pedro Bizarro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09145" title="Abstract">arXiv:2307.09145</a> (replaced) [<a href="/pdf/2307.09145" title="Download PDF">pdf</a>, <a href="/ps/2307.09145" title="Download PostScript">ps</a>, <a href="/format/2307.09145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomial Time and Dependent Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atkey%2C+R">Robert Atkey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of published POPL version, with full typing rules
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10894" title="Abstract">arXiv:2307.10894</a> (replaced) [<a href="/pdf/2307.10894" title="Download PDF">pdf</a>, <a href="/format/2307.10894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Motion Generation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wentao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoxuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ro%2C+D">Dongwoo Ro</a>, 
<a href="/search/cs?searchtype=author&query=Ci%2C+H">Hai Ci</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinlu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiaxin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Feng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qi Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TPAMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13390" title="Abstract">arXiv:2307.13390</a> (replaced) [<a href="/pdf/2307.13390" title="Download PDF">pdf</a>, <a href="/format/2307.13390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Explanation via Search in Gaussian Mixture Distributed  Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Broelemann%2C+K">Klaus Broelemann</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+G">Gjergji Kasneci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> XAI workshop of IJCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15053" title="Abstract">arXiv:2307.15053</a> (replaced) [<a href="/pdf/2307.15053" title="Download PDF">pdf</a>, <a href="/format/2307.15053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation  Metric for Top-$n$ Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeunen%2C+O">Olivier Jeunen</a>, 
<a href="/search/cs?searchtype=author&query=Potapov%2C+I">Ivan Potapov</a>, 
<a href="/search/cs?searchtype=author&query=Ustimenko%2C+A">Aleksei Ustimenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15280" title="Abstract">arXiv:2307.15280</a> (replaced) [<a href="/pdf/2307.15280" title="Download PDF">pdf</a>, <a href="/format/2307.15280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active RIS-Assisted MIMO-OFDM System: Analyses and Prototype  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chian%2C+D">De-Ming Chian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng-Ji Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yu-Chen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Chao-Kai Wen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chi-Hung Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fu-Kang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kai-Kit Wong</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+C">Chan-Byoung Chae</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, 1 table, accepted by IEEE Communications Letters, for demo video see: <a href="https://www.youtube.com/watch?v=3R6eZXizwns">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15573" title="Abstract">arXiv:2307.15573</a> (replaced) [<a href="/pdf/2307.15573" title="Download PDF">pdf</a>, <a href="/ps/2307.15573" title="Download PostScript">ps</a>, <a href="/format/2307.15573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three remarks on $\mathbf{W_2}$ graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Feghali%2C+C">Carl Feghali</a>, 
<a href="/search/math?searchtype=author&query=Marin%2C+M">Malory Marin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00755" title="Abstract">arXiv:2308.00755</a> (replaced) [<a href="/pdf/2308.00755" title="Download PDF">pdf</a>, <a href="/format/2308.00755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Bias Amplification Paradox in Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seshadri%2C+P">Preethi Seshadri</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sameer Singh</a>, 
<a href="/search/cs?searchtype=author&query=Elazar%2C+Y">Yanai Elazar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02487" title="Abstract">arXiv:2308.02487</a> (replaced) [<a href="/pdf/2308.02487" title="Download PDF">pdf</a>, <a href="/format/2308.02487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen  Convolutional CLIP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qihang Yu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Ju He</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xueqing Deng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaohui Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang-Chieh Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera ready. code and model available at <a href="https://github.com/bytedance/fc-clip">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10103" title="Abstract">arXiv:2308.10103</a> (replaced) [<a href="/pdf/2308.10103" title="Download PDF">pdf</a>, <a href="/format/2308.10103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASPIRE: Language-Guided Augmentation for Robust Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Sreyan Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Evuru%2C+C+K+R">Chandra Kiran Reddy Evuru</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sonal Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Tyagi%2C+U">Utkarsh Tyagi</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sakshi Singh</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S">Sanjoy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10238" title="Abstract">arXiv:2308.10238</a> (replaced) [<a href="/pdf/2308.10238" title="Download PDF">pdf</a>, <a href="/format/2308.10238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thompson Sampling for Real-Valued Combinatorial Pure Exploration of  Multi-Armed Bandit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+S">Shintaro Nakamura</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11809" title="Abstract">arXiv:2308.11809</a> (replaced) [<a href="/pdf/2308.11809" title="Download PDF">pdf</a>, <a href="/format/2308.11809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressive probabilistic sampling in recurrent neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+S">Shirui Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Jiang%2C+L+P">Linxing Preston Jiang</a>, 
<a href="/search/q-bio?searchtype=author&query=Rao%2C+R+P+N">Rajesh P. N. Rao</a>, 
<a href="/search/q-bio?searchtype=author&query=Shea-Brown%2C+E">Eric Shea-Brown</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12259" title="Abstract">arXiv:2308.12259</a> (replaced) [<a href="/pdf/2308.12259" title="Download PDF">pdf</a>, <a href="/format/2308.12259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Identification of Parametric Governing Equations of  Dynamical Systems Using the Signed Cumulative Distribution Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rubaiyat%2C+A+H+M">Abu Hasnat Mohammad Rubaiyat</a>, 
<a href="/search/eess?searchtype=author&query=Thai%2C+D+H">Duy H. Thai</a>, 
<a href="/search/eess?searchtype=author&query=Nichols%2C+J+M">Jonathan M. Nichols</a>, 
<a href="/search/eess?searchtype=author&query=Hutchinson%2C+M+N">Meredith N. Hutchinson</a>, 
<a href="/search/eess?searchtype=author&query=Wallen%2C+S+P">Samuel P. Wallen</a>, 
<a href="/search/eess?searchtype=author&query=Naify%2C+C+J">Christina J. Naify</a>, 
<a href="/search/eess?searchtype=author&query=Geib%2C+N">Nathan Geib</a>, 
<a href="/search/eess?searchtype=author&query=Haberman%2C+M+R">Michael R. Haberman</a>, 
<a href="/search/eess?searchtype=author&query=Rohde%2C+G+K">Gustavo K. Rohde</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14119" title="Abstract">arXiv:2308.14119</a> (replaced) [<a href="/pdf/2308.14119" title="Download PDF">pdf</a>, <a href="/format/2308.14119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Learning in the Few-Shot Zero-Shot Scenario
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fluss%2C+N">Noam Fluss</a>, 
<a href="/search/cs?searchtype=author&query=Hacohen%2C+G">Guy Hacohen</a>, 
<a href="/search/cs?searchtype=author&query=Weinshall%2C+D">Daphna Weinshall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15452" title="Abstract">arXiv:2308.15452</a> (replaced) [<a href="/pdf/2308.15452" title="Download PDF">pdf</a>, <a href="/format/2308.15452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Do Program-of-Thoughts Work for Reasoning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Z">Zhen Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yinuo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shumin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guozhou Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01065" title="Abstract">arXiv:2309.01065</a> (replaced) [<a href="/pdf/2309.01065" title="Download PDF">pdf</a>, <a href="/format/2309.01065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Mobile-Edge AI-Generated Everything (AIGX) Services by Prompt  Engineering: Fundamental, Framework, and Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinqiu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiawen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Shuguang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xuemin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Ping Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02781" title="Abstract">arXiv:2309.02781</a> (replaced) [<a href="/pdf/2309.02781" title="Download PDF">pdf</a>, <a href="/format/2309.02781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Contact-aware Feedback CPG System for Learning-based Soft  Snake Robot Locomotion Controllers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Onal%2C+C+D">Cagdas D. Onal</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 21 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05310" title="Abstract">arXiv:2309.05310</a> (replaced) [<a href="/pdf/2309.05310" title="Download PDF">pdf</a>, <a href="/format/2309.05310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ImitationNet: Unsupervised Human-to-Robot Motion Retargeting via Shared  Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yashuai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Mascaro%2C+E+V">Esteve Valls Mascaro</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongheui Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Humanoids 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06038" title="Abstract">arXiv:2309.06038</a> (replaced) [<a href="/pdf/2309.06038" title="Download PDF">pdf</a>, <a href="/format/2309.06038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraspGF: Learning Score-based Grasping Primitive for Human-assisting  Dexterous Grasping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mingdong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Y">Yunchong Gan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07781" title="Abstract">arXiv:2309.07781</a> (replaced) [<a href="/pdf/2309.07781" title="Download PDF">pdf</a>, <a href="/format/2309.07781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deductive Verification Infrastructure for Probabilistic Programs  (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schr%C3%B6er%2C+P">Philipp Schr&#xf6;er</a>, 
<a href="/search/cs?searchtype=author&query=Batz%2C+K">Kevin Batz</a>, 
<a href="/search/cs?searchtype=author&query=Kaminski%2C+B+L">Benjamin Lucien Kaminski</a>, 
<a href="/search/cs?searchtype=author&query=Katoen%2C+J">Joost-Pieter Katoen</a>, 
<a href="/search/cs?searchtype=author&query=Matheja%2C+C">Christoph Matheja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the extended version of the the publication at OOPSLA 2023 (<a href="https://doi.org/10.1145/3622870">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07944" title="Abstract">arXiv:2309.07944</a> (replaced) [<a href="/pdf/2309.07944" title="Download PDF">pdf</a>, <a href="/format/2309.07944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-to-Image Models for Counterfactual Explanations: a Black-Box  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeanneret%2C+G">Guillaume Jeanneret</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+L">Lo&#xef;c Simon</a>, 
<a href="/search/cs?searchtype=author&query=Jurie%2C+F">Fr&#xe9;d&#xe9;ric Jurie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024 Camera ready + supplementary material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08534" title="Abstract">arXiv:2309.08534</a> (replaced) [<a href="/pdf/2309.08534" title="Download PDF">pdf</a>, <a href="/format/2309.08534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Last-layer Retraining for Group Robustness with Fewer  Annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=LaBonte%2C+T">Tyler LaBonte</a>, 
<a href="/search/cs?searchtype=author&query=Muthukumar%2C+V">Vidya Muthukumar</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Abhishek Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12265" title="Abstract">arXiv:2309.12265</a> (replaced) [<a href="/pdf/2309.12265" title="Download PDF">pdf</a>, <a href="/ps/2309.12265" title="Download PostScript">ps</a>, <a href="/format/2309.12265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-sharing in Parking Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Elder%2C+J">Jennifer Elder</a>, 
<a href="/search/math?searchtype=author&query=Harris%2C+P+E">Pamela E. Harris</a>, 
<a href="/search/math?searchtype=author&query=Kretschmann%2C+J">Jan Kretschmann</a>, 
<a href="/search/math?searchtype=author&query=Mori%2C+J+C+M">J. Carlos Mart&#xed;nez Mori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12632" title="Abstract">arXiv:2309.12632</a> (replaced) [<a href="/pdf/2309.12632" title="Download PDF">pdf</a>, <a href="/format/2309.12632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Deep Learning Classification Results Obtained on CT Scans Fair and  Interpretable?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ashames%2C+M+M+A">Mohamad M.A. Ashames</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+A">Ahmet Demir</a>, 
<a href="/search/cs?searchtype=author&query=Gerek%2C+O+N">Omer N. Gerek</a>, 
<a href="/search/cs?searchtype=author&query=Fidan%2C+M">Mehmet Fidan</a>, 
<a href="/search/cs?searchtype=author&query=Gulmezoglu%2C+M+B">M. Bilginer Gulmezoglu</a>, 
<a href="/search/cs?searchtype=author&query=Ergin%2C+S">Semih Ergin</a>, 
<a href="/search/cs?searchtype=author&query=Koc%2C+M">Mehmet Koc</a>, 
<a href="/search/cs?searchtype=author&query=Barkana%2C+A">Atalay Barkana</a>, 
<a href="/search/cs?searchtype=author&query=Calisir%2C+C">Cuneyt Calisir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14136" title="Abstract">arXiv:2309.14136</a> (replaced) [<a href="/pdf/2309.14136" title="Download PDF">pdf</a>, <a href="/format/2309.14136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Image Residual Learning for Scaling Deeper Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Guoxi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Hongtao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Bors%2C+A+G">Adrian G. Bors</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15647" title="Abstract">arXiv:2309.15647</a> (replaced) [<a href="/pdf/2309.15647" title="Download PDF">pdf</a>, <a href="/ps/2309.15647" title="Download PostScript">ps</a>, <a href="/format/2309.15647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Black-Box Identity Testing of Noncommutative Rational Formulas in  Deterministic Quasipolynomial Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arvind%2C+V">V. Arvind</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+A">Abhranil Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+P">Partha Mukhopadhyay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A white-box quasi-NC RIT algorithm has been added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17370" title="Abstract">arXiv:2309.17370</a> (replaced) [<a href="/pdf/2309.17370" title="Download PDF">pdf</a>, <a href="/format/2309.17370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based Neural Weather Prediction for Limited Area Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oskarsson%2C+J">Joel Oskarsson</a>, 
<a href="/search/cs?searchtype=author&query=Landelius%2C+T">Tomas Landelius</a>, 
<a href="/search/cs?searchtype=author&query=Lindsten%2C+F">Fredrik Lindsten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 27 figures. Accepted to the Tackling Climate Change with Machine Learning workshop at NeurIPS 2023. Code available at: <a href="https://github.com/joeloskarsson/neural-lam">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00239" title="Abstract">arXiv:2310.00239</a> (replaced) [<a href="/pdf/2310.00239" title="Download PDF">pdf</a>, <a href="/format/2310.00239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaptNet: Policy Adaptation for Physics-Based Character Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+K">Kaixiang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Andrews%2C+S">Sheldon Andrews</a>, 
<a href="/search/cs?searchtype=author&query=Kry%2C+P+G">Paul G. Kry</a>, 
<a href="/search/cs?searchtype=author&query=Neff%2C+M">Michael Neff</a>, 
<a href="/search/cs?searchtype=author&query=McGuire%2C+M">Morgan McGuire</a>, 
<a href="/search/cs?searchtype=author&query=Karamouzas%2C+I">Ioannis Karamouzas</a>, 
<a href="/search/cs?searchtype=author&query=Zordan%2C+V">Victor Zordan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023. Video: <a href="https://youtu.be/WxmJSCNFb28.">this https URL</a> Website: <a href="https://motion-lab.github.io/AdaptNet">this https URL</a>, <a href="https://pei-xu.github.io/AdaptNet">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Transactions on Graphics 42, 6, Article 112.1522 (December
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01258" title="Abstract">arXiv:2310.01258</a> (replaced) [<a href="/pdf/2310.01258" title="Download PDF">pdf</a>, <a href="/format/2310.01258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=van+Rozendaal%2C+T">Ties van Rozendaal</a>, 
<a href="/search/eess?searchtype=author&query=Singhal%2C+T">Tushar Singhal</a>, 
<a href="/search/eess?searchtype=author&query=Le%2C+H">Hoang Le</a>, 
<a href="/search/eess?searchtype=author&query=Sautiere%2C+G">Guillaume Sautiere</a>, 
<a href="/search/eess?searchtype=author&query=Said%2C+A">Amir Said</a>, 
<a href="/search/eess?searchtype=author&query=Buska%2C+K">Krishna Buska</a>, 
<a href="/search/eess?searchtype=author&query=Raha%2C+A">Anjuman Raha</a>, 
<a href="/search/eess?searchtype=author&query=Kalatzis%2C+D">Dimitris Kalatzis</a>, 
<a href="/search/eess?searchtype=author&query=Mehta%2C+H">Hitarth Mehta</a>, 
<a href="/search/eess?searchtype=author&query=Mayer%2C+F">Frank Mayer</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Nagel%2C+M">Markus Nagel</a>, 
<a href="/search/eess?searchtype=author&query=Wiggers%2C+A">Auke Wiggers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Matches version published at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02457" title="Abstract">arXiv:2310.02457</a> (replaced) [<a href="/pdf/2310.02457" title="Download PDF">pdf</a>, <a href="/format/2310.02457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Empty Signifier Problem: Towards Clearer Paradigms for  Operationalising &quot;Alignment&quot; in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirk%2C+H+R">Hannah Rose Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Vidgen%2C+B">Bertie Vidgen</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ttger%2C+P">Paul R&#xf6;ttger</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+S+A">Scott A. Hale</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Socially Responsible Language Modelling Research (SoLaR) @ NeurIPs 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02571" title="Abstract">arXiv:2310.02571</a> (replaced) [<a href="/pdf/2310.02571" title="Download PDF">pdf</a>, <a href="/ps/2310.02571" title="Download PostScript">ps</a>, <a href="/format/2310.02571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The role of local bounds on neighborhoods in the network for scale-free  state synchronization of multi-agent systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Stoorvogel%2C+A+A">Anton A. Stoorvogel</a>, 
<a href="/search/eess?searchtype=author&query=Saberi%2C+A">Ali Saberi</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhenwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was submitted to IJRNC on Aug. 3, 2023 and resubmitted on Nov. 16, 2023. Now, it is under review at the second round
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02971" title="Abstract">arXiv:2310.02971</a> (replaced) [<a href="/pdf/2310.02971" title="Download PDF">pdf</a>, <a href="/format/2310.02971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+M">Ming-Hsin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yun-Ping Lin</a>, 
<a href="/search/eess?searchtype=author&query=Hsu%2C+J+N">Jing Neng Hsu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+P+K">Paul Kuo-Ming Huang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+C">Chien-yu Huang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shang-Wen Li</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05982" title="Abstract">arXiv:2310.05982</a> (replaced) [<a href="/pdf/2310.05982" title="Download PDF">pdf</a>, <a href="/ps/2310.05982" title="Download PostScript">ps</a>, <a href="/format/2310.05982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On matrix rank function over bounded arithmetics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ken%2C+E">Eitetsu Ken</a>, 
<a href="/search/cs?searchtype=author&query=Kuroda%2C+S">Satoru Kuroda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages, section 4 added for readability, typos modified, acknowledgement revised, results unchanged, no figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07250" title="Abstract">arXiv:2310.07250</a> (replaced) [<a href="/e-print/2310.07250" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Missing MRI Sequences from Available Modalities using  Generative Adversarial Networks in BraTS Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Hamamci%2C+I+E">Ibrahim Ethem Hamamci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Wrong paper submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07570" title="Abstract">arXiv:2310.07570</a> (replaced) [<a href="/pdf/2310.07570" title="Download PDF">pdf</a>, <a href="/format/2310.07570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT for Computational Topology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+J">Jian Liu</a>, 
<a href="/search/math?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/math?searchtype=author&query=Wei%2C+G">Guo-Wei Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08130" title="Abstract">arXiv:2310.08130</a> (replaced) [<a href="/pdf/2310.08130" title="Download PDF">pdf</a>, <a href="/format/2310.08130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-grained Conversational Decoding via Isotropic and Proximal Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuxuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiling Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09336" title="Abstract">arXiv:2310.09336</a> (replaced) [<a href="/pdf/2310.09336" title="Download PDF">pdf</a>, <a href="/format/2310.09336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Abilities Emerge Multiplicatively: Exploring Diffusion  Models on a Synthetic Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okawa%2C+M">Maya Okawa</a>, 
<a href="/search/cs?searchtype=author&query=Lubana%2C+E+S">Ekdeep Singh Lubana</a>, 
<a href="/search/cs?searchtype=author&query=Dick%2C+R+P">Robert P. Dick</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Hidenori Tanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10191" title="Abstract">arXiv:2310.10191</a> (replaced) [<a href="/pdf/2310.10191" title="Download PDF">pdf</a>, <a href="/format/2310.10191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VIBE: Topic-Driven Temporal Adaptation for Twitter Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11305" title="Abstract">arXiv:2310.11305</a> (replaced) [<a href="/pdf/2310.11305" title="Download PDF">pdf</a>, <a href="/format/2310.11305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello,  and Atari Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Ti-Rong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guei%2C+H">Hung Guei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Po-Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+P">Pei-Chiun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T+H">Ting Han Wei</a>, 
<a href="/search/cs?searchtype=author&query=Shih%2C+C">Chung-Chin Shih</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yun-Jui Tsai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Games, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12342" title="Abstract">arXiv:2310.12342</a> (replaced) [<a href="/pdf/2310.12342" title="Download PDF">pdf</a>, <a href="/format/2310.12342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliminating Reasoning via Inferring with Planning: A New Framework to  Guide LLMs&#x27; Non-linear Thinking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+Y">Yongqi Tong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sizhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Simeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12545" title="Abstract">arXiv:2310.12545</a> (replaced) [<a href="/pdf/2310.12545" title="Download PDF">pdf</a>, <a href="/format/2310.12545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel Picard algorithm for general semilinear parabolic PDEs with  gradient-dependent nonlinearities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Neufeld%2C+A">Ariel Neufeld</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+S">Sizhou Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12963" title="Abstract">arXiv:2310.12963</a> (replaced) [<a href="/pdf/2310.12963" title="Download PDF">pdf</a>, <a href="/format/2310.12963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoMix: Automatically Mixing Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madaan%2C+A">Aman Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+P">Pranjal Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+A">Ankit Anand</a>, 
<a href="/search/cs?searchtype=author&query=Potharaju%2C+S+P">Srividya Pranavi Potharaju</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aditya Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rajagopal%2C+D">Dheeraj Rajagopal</a>, 
<a href="/search/cs?searchtype=author&query=Kappaganthu%2C+K">Karthik Kappaganthu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Upadhyay%2C+S">Shyam Upadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>, 
<a href="/search/cs?searchtype=author&query=Faruqui%2C+M">Manaal Faruqui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally. Work started and partly done during Aman's internship at Google. This version adds results on mixing 3 models, and will be presented at the workshop on robustness of zero/few-shot learning in foundation models, Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13007" title="Abstract">arXiv:2310.13007</a> (replaced) [<a href="/pdf/2310.13007" title="Download PDF">pdf</a>, <a href="/format/2310.13007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Critical Survey on Fairness Benefits of XAI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deck%2C+L">Luca Deck</a>, 
<a href="/search/cs?searchtype=author&query=Schoeffer%2C+J">Jakob Schoeffer</a>, 
<a href="/search/cs?searchtype=author&query=De-Arteaga%2C+M">Maria De-Arteaga</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChl%2C+N">Niklas K&#xfc;hl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13913" title="Abstract">arXiv:2310.13913</a> (replaced) [<a href="/pdf/2310.13913" title="Download PDF">pdf</a>, <a href="/format/2310.13913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-Training on Large-Scale Generated Docking Conformations with  HelixDock to Unlock the Potential of Protein-ligand Structure Prediction  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lihang Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Donglong He</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xianbin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shanzhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaonan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+H">Hua Chai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fan Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jingzhou He</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yonghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xiaomin Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14085" title="Abstract">arXiv:2310.14085</a> (replaced) [<a href="/pdf/2310.14085" title="Download PDF">pdf</a>, <a href="/ps/2310.14085" title="Download PostScript">ps</a>, <a href="/format/2310.14085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and  Exp-Concave Games with Gradient Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tianyi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhengyuan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Operations Research; 47 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14088" title="Abstract">arXiv:2310.14088</a> (replaced) [<a href="/pdf/2310.14088" title="Download PDF">pdf</a>, <a href="/format/2310.14088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark  for Language Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zexue He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+A">An Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E+Y">Eric Y. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Gentili%2C+A">Amilcare Gentili</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chun-Nan Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023. Camera-ready version: updated IRB, added more evaluation results on LLMs such as GPT4, LLaMa2, and LLaMa2-chat
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14421" title="Abstract">arXiv:2310.14421</a> (replaced) [<a href="/pdf/2310.14421" title="Download PDF">pdf</a>, <a href="/format/2310.14421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On existence, uniqueness and scalability of adversarial robustness  measures for AI classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Horenko%2C+I">Illia Horenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14931" title="Abstract">arXiv:2310.14931</a> (replaced) [<a href="/pdf/2310.14931" title="Download PDF">pdf</a>, <a href="/ps/2310.14931" title="Download PostScript">ps</a>, <a href="/format/2310.14931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological Derivative for Shallow Water Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ngom%2C+M+G">Mame Gor Ngom</a>, 
<a href="/search/math?searchtype=author&query=Faye%2C+I">Ibrahima Faye</a>, 
<a href="/search/math?searchtype=author&query=Seck%2C+D">Diaraf Seck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15612" title="Abstract">arXiv:2310.15612</a> (replaced) [<a href="/pdf/2310.15612" title="Download PDF">pdf</a>, <a href="/format/2310.15612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Translation for Nko: Tools, Corpora and Baseline Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doumbouya%2C+M+K+B">Moussa Koulako Bala Doumbouya</a>, 
<a href="/search/cs?searchtype=author&query=Dian%C3%A9%2C+B+M">Baba Mamadi Dian&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Ciss%C3%A9%2C+S+F">Solo Farabado Ciss&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Dian%C3%A9%2C+D">Djibrila Dian&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Sow%2C+A">Abdoulaye Sow</a>, 
<a href="/search/cs?searchtype=author&query=Doumbouya%2C+S+M">S&#xe9;r&#xe9; Moussa Doumbouya</a>, 
<a href="/search/cs?searchtype=author&query=Bangoura%2C+D">Daouda Bangoura</a>, 
<a href="/search/cs?searchtype=author&query=Bayo%2C+F+M">Fod&#xe9; Moriba Bayo</a>, 
<a href="/search/cs?searchtype=author&query=Cond%C3%A9%2C+I+S+2">Ibrahima Sory 2. Cond&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Dian%C3%A9%2C+K+M">Kalo Mory Dian&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Piech%2C+C">Chris Piech</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C">Christopher Manning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15681" title="Abstract">arXiv:2310.15681</a> (replaced) [<a href="/pdf/2310.15681" title="Download PDF">pdf</a>, <a href="/format/2310.15681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed  Bandit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+S">Shintaro Nakamura</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17658" title="Abstract">arXiv:2310.17658</a> (replaced) [<a href="/pdf/2310.17658" title="Download PDF">pdf</a>, <a href="/format/2310.17658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Channel Independent strategy optimal for Time Series Forecasting?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peiwen%2C+Y">Yuan Peiwen</a>, 
<a href="/search/cs?searchtype=author&query=Changsheng%2C+Z">Zhu Changsheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17842" title="Abstract">arXiv:2310.17842</a> (replaced) [<a href="/pdf/2310.17842" title="Download PDF">pdf</a>, <a href="/format/2310.17842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What You See Is What You Detect: Towards better Object Densification in  3D detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pasandi%2C+M+M">Morteza Mousa Pasandi</a>, 
<a href="/search/cs?searchtype=author&query=Laganiere%2C+R">Robert Laganiere</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18301" title="Abstract">arXiv:2310.18301</a> (replaced) [<a href="/pdf/2310.18301" title="Download PDF">pdf</a>, <a href="/format/2310.18301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Motion Planning for Autonomous Vehicles with Joint  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Veer%2C+S">Sushant Veer</a>, 
<a href="/search/cs?searchtype=author&query=Karkus%2C+P">Peter Karkus</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18306" title="Abstract">arXiv:2310.18306</a> (replaced) [<a href="/pdf/2310.18306" title="Download PDF">pdf</a>, <a href="/format/2310.18306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised and Penalized Baseline Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Andries%2C+E">Erik Andries</a>, 
<a href="/search/stat?searchtype=author&query=Nikzad-Langerodi%2C+R">Ramin Nikzad-Langerodi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages; 9 figures; 2 tables; fixed typos; additional sanity checks for grammar and syntax; streamlined text and made minor cosmetic changes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19387" title="Abstract">arXiv:2310.19387</a> (replaced) [<a href="/pdf/2310.19387" title="Download PDF">pdf</a>, <a href="/format/2310.19387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Othello is Solved
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takizawa%2C+H">Hiroki Takizawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Typos corrected; related work, discussion, and references added; English writing revised; results unchanged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20091" title="Abstract">arXiv:2310.20091</a> (replaced) [<a href="/pdf/2310.20091" title="Download PDF">pdf</a>, <a href="/format/2310.20091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Density-based User Representation through Gaussian Process Regression  for Multi-interest Personalized Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haolun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Meshi%2C+O">Ofer Meshi</a>, 
<a href="/search/cs?searchtype=author&query=Zoghi%2C+M">Masrour Zoghi</a>, 
<a href="/search/cs?searchtype=author&query=Diaz%2C+F">Fernando Diaz</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Boutilier%2C+C">Craig Boutilier</a>, 
<a href="/search/cs?searchtype=author&query=Karimzadehgan%2C+M">Maryam Karimzadehgan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00423" title="Abstract">arXiv:2311.00423</a> (replaced) [<a href="/pdf/2311.00423" title="Download PDF">pdf</a>, <a href="/format/2311.00423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMRec: Large Language Models with Graph Augmentation for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xubin Ren</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiabin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qinyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+L">Lixin Su</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Suqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WSDM 2024 Oral Presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00684" title="Abstract">arXiv:2311.00684</a> (replaced) [<a href="/pdf/2311.00684" title="Download PDF">pdf</a>, <a href="/format/2311.00684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention Alignment and Flexible Positional Embeddings Improve  Transformer Length Extrapolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+T">Ta-Chung Chi</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+T">Ting-Han Fan</a>, 
<a href="/search/cs?searchtype=author&query=Rudnicky%2C+A+I">Alexander I. Rudnicky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00735" title="Abstract">arXiv:2311.00735</a> (replaced) [<a href="/pdf/2311.00735" title="Download PDF">pdf</a>, <a href="/ps/2311.00735" title="Download PostScript">ps</a>, <a href="/format/2311.00735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PET Tracer Conversion among Brain PET via Variable Augmented Invertible  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+B">Bohui Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xubiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Pengfei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shirui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xinchong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangsong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaoyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weirui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiegen Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02762" title="Abstract">arXiv:2311.02762</a> (replaced) [<a href="/e-print/2311.02762" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Sparse 3D Convolution Network with VDB
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fangjun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+A">Anyong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Sifakis%2C+E">Eftychios Sifakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Unauthorized publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02869" title="Abstract">arXiv:2311.02869</a> (replaced) [<a href="/pdf/2311.02869" title="Download PDF">pdf</a>, <a href="/format/2311.02869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight equivariant interaction graph neural network for accurate  and efficient interatomic potential and force predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziduo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Q">Qiujie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+Y">Calvin Yu-Chian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lei Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03217" title="Abstract">arXiv:2311.03217</a> (replaced) [<a href="/pdf/2311.03217" title="Download PDF">pdf</a>, <a href="/format/2311.03217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Transformers to Improve Breast Cancer Classification and Risk  Assessment with Multi-modal and Longitudinal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shen%2C+Y">Yiqiu Shen</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+J">Jungkyu Park</a>, 
<a href="/search/eess?searchtype=author&query=Yeung%2C+F">Frank Yeung</a>, 
<a href="/search/eess?searchtype=author&query=Goldberg%2C+E">Eliana Goldberg</a>, 
<a href="/search/eess?searchtype=author&query=Heacock%2C+L">Laura Heacock</a>, 
<a href="/search/eess?searchtype=author&query=Shamout%2C+F">Farah Shamout</a>, 
<a href="/search/eess?searchtype=author&query=Geras%2C+K+J">Krzysztof J. Geras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ML4H 2023 Findings Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04037" title="Abstract">arXiv:2311.04037</a> (replaced) [<a href="/pdf/2311.04037" title="Download PDF">pdf</a>, <a href="/format/2311.04037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Discovery Under Local Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Binkyt%C4%97%2C+R">R&#x16b;ta Binkyt&#x117;</a>, 
<a href="/search/cs?searchtype=author&query=Pinz%C3%B3n%2C+C">Carlos Pinz&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Lesty%C3%A1n%2C+S">Szilvia Lesty&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+K">Kangsoo Jung</a>, 
<a href="/search/cs?searchtype=author&query=Arcolezi%2C+H+H">H&#xe9;ber H. Arcolezi</a>, 
<a href="/search/cs?searchtype=author&query=Palamidessi%2C+C">Catuscia Palamidessi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04246" title="Abstract">arXiv:2311.04246</a> (replaced) [<a href="/pdf/2311.04246" title="Download PDF">pdf</a>, <a href="/format/2311.04246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ADFactory: An Effective Framework for Generalizing Optical Flow with  Nerf
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Han Ling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04302" title="Abstract">arXiv:2311.04302</a> (replaced) [<a href="/pdf/2311.04302" title="Download PDF">pdf</a>, <a href="/format/2311.04302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Hard is Weak-Memory Testing?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Soham Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+S">Shankaranarayanan Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+U">Umang Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Pavlogiannis%2C+A">Andreas Pavlogiannis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04378" title="Abstract">arXiv:2311.04378</a> (replaced) [<a href="/pdf/2311.04378" title="Download PDF">pdf</a>, <a href="/format/2311.04378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Watermarks in the Sand: Impossibility of Strong Watermarking for  Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanlin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Edelman%2C+B+L">Benjamin L. Edelman</a>, 
<a href="/search/cs?searchtype=author&query=Francati%2C+D">Danilo Francati</a>, 
<a href="/search/cs?searchtype=author&query=Venturi%2C+D">Daniele Venturi</a>, 
<a href="/search/cs?searchtype=author&query=Ateniese%2C+G">Giuseppe Ateniese</a>, 
<a href="/search/cs?searchtype=author&query=Barak%2C+B">Boaz Barak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Blog post: <a href="https://www.harvard.edu/kempner-institute/2023/11/09/watermarking-in-the-sand/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04658" title="Abstract">arXiv:2311.04658</a> (replaced) [<a href="/pdf/2311.04658" title="Download PDF">pdf</a>, <a href="/format/2311.04658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mass Adoption of NATs: Survey and experiments on carrier-grade NATs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kanaris%2C+O">Orestis Kanaris</a>, 
<a href="/search/cs?searchtype=author&query=Pouwelse%2C+J">Johan Pouwelse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04818" title="Abstract">arXiv:2311.04818</a> (replaced) [<a href="/pdf/2311.04818" title="Download PDF">pdf</a>, <a href="/format/2311.04818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Silo Federated Learning Across Divergent Domains with Iterative  Parameter Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorbett%2C+M">Matt Gorbett</a>, 
<a href="/search/cs?searchtype=author&query=Shirazi%2C+H">Hossein Shirazi</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+I">Indrakshi Ray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at IEEE Big Data 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05836" title="Abstract">arXiv:2311.05836</a> (replaced) [<a href="/pdf/2311.05836" title="Download PDF">pdf</a>, <a href="/format/2311.05836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-aware Single View Volumetric Rendering for Medical Neural  Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hu%2C+J">Jing Hu</a>, 
<a href="/search/eess?searchtype=author&query=Fan%2C+Q">Qinrui Fan</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+S">Shu Hu</a>, 
<a href="/search/eess?searchtype=author&query=Lyu%2C+S">Siwei Lyu</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+X">Xi Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05922" title="Abstract">arXiv:2311.05922</a> (replaced) [<a href="/e-print/2311.05922" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation  Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xilai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An error example is in Table 14 on Page 18. Need to carefully correct and evaluate the error
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06281" title="Abstract">arXiv:2311.06281</a> (replaced) [<a href="/pdf/2311.06281" title="Download PDF">pdf</a>, <a href="/format/2311.06281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Parallelization of an Ubiquitous Sequential Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heinsen%2C+F+A">Franz A. Heinsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source code for replicating our results is available online at <a href="https://github.com/glassroom/heinsen_sequence">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06851" title="Abstract">arXiv:2311.06851</a> (replaced) [<a href="/pdf/2311.06851" title="Download PDF">pdf</a>, <a href="/format/2311.06851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Textual Normalization for Hate Speech Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A+T">Anh Thi-Hoang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+H">Dung Ha Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+N+T">Nguyet Thi Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+K+T">Khanh Thanh-Duy Ho</a>, 
<a href="/search/cs?searchtype=author&query=Van+Nguyen%2C+K">Kiet Van Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06928" title="Abstract">arXiv:2311.06928</a> (replaced) [<a href="/pdf/2311.06928" title="Download PDF">pdf</a>, <a href="/format/2311.06928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention for Causal Relationship Discovery from Biological Neural  Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Ziyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Tabassum%2C+A">Anika Tabassum</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Shruti Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+L">Lu Mi</a>, 
<a href="/search/cs?searchtype=author&query=Kutz%2C+J+N">J. Nathan Kutz</a>, 
<a href="/search/cs?searchtype=author&query=Shea-Brown%2C+E">Eric Shea-Brown</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+S">Seung-Hwan Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the NeurIPS 2023 Workshop on Causal Representation Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07007" title="Abstract">arXiv:2311.07007</a> (replaced) [<a href="/pdf/2311.07007" title="Download PDF">pdf</a>, <a href="/ps/2311.07007" title="Download PostScript">ps</a>, <a href="/format/2311.07007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizations of Minimal Expected Length Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Congero%2C+S">Spencer Congero</a>, 
<a href="/search/cs?searchtype=author&query=Zeger%2C+K">Kenneth Zeger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07009" title="Abstract">arXiv:2311.07009</a> (replaced) [<a href="/pdf/2311.07009" title="Download PDF">pdf</a>, <a href="/ps/2311.07009" title="Download PostScript">ps</a>, <a href="/format/2311.07009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Competitive Advantage of Huffman and Shannon-Fano Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Congero%2C+S">Spencer Congero</a>, 
<a href="/search/cs?searchtype=author&query=Zeger%2C+K">Kenneth Zeger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07042" title="Abstract">arXiv:2311.07042</a> (replaced) [<a href="/pdf/2311.07042" title="Download PDF">pdf</a>, <a href="/format/2311.07042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Vocabulary Video Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuerong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yujia Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07049" title="Abstract">arXiv:2311.07049</a> (replaced) [<a href="/pdf/2311.07049" title="Download PDF">pdf</a>, <a href="/ps/2311.07049" title="Download PostScript">ps</a>, <a href="/format/2311.07049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clifford Algebra-Based Iterated Extended Kalman Filter with Application  to Low-Cost INS/GNSS Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ouyang%2C+W">Wei Ouyang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yutian Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yuanxin Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07068" title="Abstract">arXiv:2311.07068</a> (replaced) [<a href="/pdf/2311.07068" title="Download PDF">pdf</a>, <a href="/format/2311.07068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shannon Theory for Wireless Communication in a Resonant Chamber
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Amritpal Singh</a>, 
<a href="/search/cs?searchtype=author&query=Marzetta%2C+T">Thomas Marzetta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 13 figures. To be published in IEEE Journal on Selected Areas in Communications Special Issue on Electromagnetic Signal and Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP); Classical Physics (physics.class-ph)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07079" title="Abstract">arXiv:2311.07079</a> (replaced) [<a href="/pdf/2311.07079" title="Download PDF">pdf</a>, <a href="/format/2311.07079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample Dominance Aware Framework via Non-Parametric Estimation for  Spontaneous Brain-Computer Interface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byeong-Hoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+B">Byoung-Hee Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07310" title="Abstract">arXiv:2311.07310</a> (replaced) [<a href="/pdf/2311.07310" title="Download PDF">pdf</a>, <a href="/format/2311.07310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Optimization on Quantum Hardware: Feasibility for a Process  Industry Use Case
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nenno%2C+D+M">Dennis Michael Nenno</a>, 
<a href="/search/math?searchtype=author&query=Caspari%2C+A">Adrian Caspari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07435" title="Abstract">arXiv:2311.07435</a> (replaced) [<a href="/pdf/2311.07435" title="Download PDF">pdf</a>, <a href="/format/2311.07435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectories and Platoon-forming Algorithm for Intersections with  Heterogeneous Autonomous Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Joshi%2C+P+C">P. C. Joshi</a>, 
<a href="/search/eess?searchtype=author&query=Boon%2C+M+A+A">M. A. A. Boon</a>, 
<a href="/search/eess?searchtype=author&query=Borst%2C+S+C">S. C. Borst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 20 figures. 3D Animations included as ancillary files
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07636" title="Abstract">arXiv:2311.07636</a> (replaced) [<a href="/pdf/2311.07636" title="Download PDF">pdf</a>, <a href="/format/2311.07636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-based Multi-task Learning for Base Editor Outcome Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Mollaysa%2C+A">Amina Mollaysa</a>, 
<a href="/search/q-bio?searchtype=author&query=Allam%2C+A">Ahmed Allam</a>, 
<a href="/search/q-bio?searchtype=author&query=Krauthammer%2C+M">Michael Krauthammer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 15 pages. arXiv admin note: substantial text overlap with <a href="/abs/2310.02919">arXiv:2310.02919</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07772" title="Abstract">arXiv:2311.07772</a> (replaced) [<a href="/pdf/2311.07772" title="Download PDF">pdf</a>, <a href="/format/2311.07772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-context Learning and Gradient Descent Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Natan%2C+T+B">Tomer Bar Natan</a>, 
<a href="/search/cs?searchtype=author&query=Deutch%2C+G">Gilad Deutch</a>, 
<a href="/search/cs?searchtype=author&query=Magar%2C+N">Nadav Magar</a>, 
<a href="/search/cs?searchtype=author&query=Dar%2C+G">Guy Dar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07861" title="Abstract">arXiv:2311.07861</a> (replaced) [<a href="/pdf/2311.07861" title="Download PDF">pdf</a>, <a href="/format/2311.07861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of the TREC 2023 Product Product Search Track
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Campos%2C+D">Daniel Campos</a>, 
<a href="/search/cs?searchtype=author&query=Kallumadi%2C+S">Surya Kallumadi</a>, 
<a href="/search/cs?searchtype=author&query=Rosset%2C+C">Corby Rosset</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+C+X">Cheng Xiang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Magnani%2C+A">Alessandro Magnani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 11 tables - TREC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07878" title="Abstract">arXiv:2311.07878</a> (replaced) [<a href="/pdf/2311.07878" title="Download PDF">pdf</a>, <a href="/format/2311.07878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating LLMs on Document-Based QA: Exact Answer Selection and  Numerical Extraction using Cogtale dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasool%2C+Z">Zafaryab Rasool</a>, 
<a href="/search/cs?searchtype=author&query=Barnett%2C+S">Scott Barnett</a>, 
<a href="/search/cs?searchtype=author&query=Kurniawan%2C+S">Stefanus Kurniawan</a>, 
<a href="/search/cs?searchtype=author&query=Balugo%2C+S">Sherwin Balugo</a>, 
<a href="/search/cs?searchtype=author&query=Vasa%2C+R">Rajesh Vasa</a>, 
<a href="/search/cs?searchtype=author&query=Chesser%2C+C">Courtney Chesser</a>, 
<a href="/search/cs?searchtype=author&query=Bahar-Fuchs%2C+A">Alex Bahar-Fuchs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07955" title="Abstract">arXiv:2311.07955</a> (replaced) [<a href="/pdf/2311.07955" title="Download PDF">pdf</a>, <a href="/format/2311.07955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle  Imagery: Review and Experimental Comparisons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chenjie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R+W">Ryan Wen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Jingxiang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruobin Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08060" title="Abstract">arXiv:2311.08060</a> (replaced) [<a href="/pdf/2311.08060" title="Download PDF">pdf</a>, <a href="/format/2311.08060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All Byzantine Agreement Problems are Expensive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Civit%2C+P">Pierre Civit</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+S">Seth Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Komatovic%2C+J">Jovan Komatovic</a>, 
<a href="/search/cs?searchtype=author&query=Paramonov%2C+A">Anton Paramonov</a>, 
<a href="/search/cs?searchtype=author&query=Vidigueira%2C+M">Manuel Vidigueira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08189" title="Abstract">arXiv:2311.08189</a> (replaced) [<a href="/pdf/2311.08189" title="Download PDF">pdf</a>, <a href="/format/2311.08189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Science: Novel Dataset and Benchmark for Cross-Modality  Scientific Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiwei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Karlsson%2C+B+F">B&#xf6;rje F. Karlsson</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Okumura%2C+M">Manabu Okumura</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chin-Yew Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08329" title="Abstract">arXiv:2311.08329</a> (replaced) [<a href="/pdf/2311.08329" title="Download PDF">pdf</a>, <a href="/format/2311.08329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KTRL+F: Knowledge-Augmented In-Document Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+H">Hanseok Oh</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Haebin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+M">Miyoung Ko</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunji Lee</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08330" title="Abstract">arXiv:2311.08330</a> (replaced) [<a href="/pdf/2311.08330" title="Download PDF">pdf</a>, <a href="/format/2311.08330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative De-Quantization for Neural Speech Codec via Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+H">Haici Yang</a>, 
<a href="/search/eess?searchtype=author&query=Jang%2C+I">Inseon Jang</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+M">Minje Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08360" title="Abstract">arXiv:2311.08360</a> (replaced) [<a href="/pdf/2311.08360" title="Download PDF">pdf</a>, <a href="/format/2311.08360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Transient Nature of Emergent In-Context Learning in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Aaditya K. Singh</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S+C+Y">Stephanie C.Y. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Moskovitz%2C+T">Ted Moskovitz</a>, 
<a href="/search/cs?searchtype=author&query=Grant%2C+E">Erin Grant</a>, 
<a href="/search/cs?searchtype=author&query=Saxe%2C+A+M">Andrew M. Saxe</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+F">Felix Hill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08379" title="Abstract">arXiv:2311.08379</a> (replaced) [<a href="/pdf/2311.08379" title="Download PDF">pdf</a>, <a href="/format/2311.08379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scheming AIs: Will AIs fake alignment during training in order to get  power?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carlsmith%2C+J">Joe Carlsmith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 127 pages, 8 figures. Revised to correct typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08385" title="Abstract">arXiv:2311.08385</a> (replaced) [<a href="/pdf/2311.08385" title="Download PDF">pdf</a>, <a href="/format/2311.08385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChOiRe: Characterizing and Predicting Human Opinions with Chain of  Opinion Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+X+L">Xuan Long Do</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M">Min-Yen Kan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item364">Cross-lists</a></li>
<li><a href="#item422">Replacements</a></li>
</ul>
<small>[ total of 616 entries:  <b>1-616</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2311">2311</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
