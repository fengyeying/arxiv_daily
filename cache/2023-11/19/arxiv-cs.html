<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Wed 15 Nov 23  to  Thu 16 Nov 23, announced Fri, 17 Nov 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item410">Cross-lists</a></li>
<li><a href="#item451">Replacements</a></li>
</ul>
<small>[ total of 637 entries:  <b>1-637</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Fri, 17 Nov 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09224" title="Abstract">arXiv:2311.09224</a> [<a href="/pdf/2311.09224" title="Download PDF">pdf</a>, <a href="/ps/2311.09224" title="Download PostScript">ps</a>, <a href="/format/2311.09224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Cybersecurity Crisis of Artificial Intelligence: Unrestrained  Adoption and Natural Language-Based Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsamados%2C+A">Andreas Tsamados</a>, 
<a href="/search/cs?searchtype=author&query=Floridi%2C+L">Luciano Floridi</a>, 
<a href="/search/cs?searchtype=author&query=Taddeo%2C+M">Mariarosaria Taddeo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">The widespread integration of autoregressive-large language models (AR-LLMs),
such as ChatGPT, across established applications, like search engines, has
introduced critical vulnerabilities with uniquely scalable characteristics. In
this commentary, we analyse these vulnerabilities, their dependence on natural
language as a vector of attack, and their challenges to cybersecurity best
practices. We offer recommendations designed to mitigate these challenges.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09225" title="Abstract">arXiv:2311.09225</a> [<a href="/pdf/2311.09225" title="Download PDF">pdf</a>, <a href="/format/2311.09225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Driving using Spiking Neural Networks on Dynamic Vision  Sensor Data: A Case Study of Traffic Light Change Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuelei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Neural and Evolutionary Computing (cs.NE); Robotics (cs.RO); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Autonomous driving is a challenging task that has gained broad attention from
both academia and industry. Current solutions using convolutional neural
networks require large amounts of computational resources, leading to high
power consumption. Spiking neural networks (SNNs) provide an alternative
computation model to process information and make decisions. This biologically
plausible model has the advantage of low latency and energy efficiency. Recent
work using SNNs for autonomous driving mostly focused on simple tasks like lane
keeping in simplified simulation environments. This project studies SNNs on
photo-realistic driving scenes in the CARLA simulator, which is an important
step toward using SNNs on real vehicles. The efficacy and generalizability of
the method will be investigated.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09226" title="Abstract">arXiv:2311.09226</a> [<a href="/pdf/2311.09226" title="Download PDF">pdf</a>, <a href="/ps/2311.09226" title="Download PostScript">ps</a>, <a href="/format/2311.09226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The value creation potential of digital humans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zirar%2C+A">Araz Zirar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">'Digital humans' are digital reproductions of humans powered by artificial
intelligence (AI) and capable of communicating and forming emotional bonds. The
value creation potential of digital humans is overlooked due to the limitations
of digital human technologies. This article explores the value creation
potential and the value realisation limitations of digital humans. The analysis
is based on a review of 62 articles retrieved from the Web of Science database.
The analysis suggests that digital humans have the potential to alleviate
labour and skill shortages, reduce the natural human element in high-risk
tasks, avoid design errors, improve the ergonomics of products and workplaces,
and provide guidance and emotional support, all of which will benefit natural
humans in the workplace. However, technical limits, evolving understanding of
digital humans, the social significance and acceptance of digital humans,
ethical considerations, and the adjustment of legal tradition limit the value
realisation. This review suggests that digital humans' perceived usefulness and
ease of development determine organisations' willingness to utilise this
technology. Overcoming the limitations, which still involve engineering
challenges and a change in how they are perceived, will positively affect
realising the value potential of digital humans in organisations.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09227" title="Abstract">arXiv:2311.09227</a> [<a href="/pdf/2311.09227" title="Download PDF">pdf</a>, <a href="/format/2311.09227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Sourcing Highly Capable Foundation Models: An evaluation of risks,  benefits, and alternative methods for pursuing open-source objectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seger%2C+E">Elizabeth Seger</a>, 
<a href="/search/cs?searchtype=author&query=Dreksler%2C+N">Noemi Dreksler</a>, 
<a href="/search/cs?searchtype=author&query=Moulange%2C+R">Richard Moulange</a>, 
<a href="/search/cs?searchtype=author&query=Dardaman%2C+E">Emily Dardaman</a>, 
<a href="/search/cs?searchtype=author&query=Schuett%2C+J">Jonas Schuett</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+K">K. Wei</a>, 
<a href="/search/cs?searchtype=author&query=Winter%2C+C">Christoph Winter</a>, 
<a href="/search/cs?searchtype=author&query=Arnold%2C+M">Mackenzie Arnold</a>, 
<a href="/search/cs?searchtype=author&query=h%C3%89igeartaigh%2C+S+%C3%93">Se&#xe1;n &#xd3; h&#xc9;igeartaigh</a>, 
<a href="/search/cs?searchtype=author&query=Korinek%2C+A">Anton Korinek</a>, 
<a href="/search/cs?searchtype=author&query=Anderljung%2C+M">Markus Anderljung</a>, 
<a href="/search/cs?searchtype=author&query=Bucknall%2C+B">Ben Bucknall</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Alan Chan</a>, 
<a href="/search/cs?searchtype=author&query=Stafford%2C+E">Eoghan Stafford</a>, 
<a href="/search/cs?searchtype=author&query=Koessler%2C+L">Leonie Koessler</a>, 
<a href="/search/cs?searchtype=author&query=Ovadya%2C+A">Aviv Ovadya</a>, 
<a href="/search/cs?searchtype=author&query=Garfinkel%2C+B">Ben Garfinkel</a>, 
<a href="/search/cs?searchtype=author&query=Bluemke%2C+E">Emma Bluemke</a>, 
<a href="/search/cs?searchtype=author&query=Aird%2C+M">Michael Aird</a>, 
<a href="/search/cs?searchtype=author&query=Levermore%2C+P">Patrick Levermore</a>, 
<a href="/search/cs?searchtype=author&query=Hazell%2C+J">Julian Hazell</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Official release at <a href="https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">Recent decisions by leading AI labs to either open-source their models or to
restrict access to their models has sparked debate about whether, and how,
increasingly capable AI models should be shared. Open-sourcing in AI typically
refers to making model architecture and weights freely and publicly accessible
for anyone to modify, study, build on, and use. This offers advantages such as
enabling external oversight, accelerating progress, and decentralizing control
over AI development and use. However, it also presents a growing potential for
misuse and unintended consequences. This paper offers an examination of the
risks and benefits of open-sourcing highly capable foundation models. While
open-sourcing has historically provided substantial net benefits for most
software and AI development processes, we argue that for some highly capable
foundation models likely to be developed in the near future, open-sourcing may
pose sufficiently extreme risks to outweigh the benefits. In such a case,
highly capable foundation models should not be open-sourced, at least not
initially. Alternative strategies, including non-open-source model sharing
options, are explored. The paper concludes with recommendations for developers,
standard-setting bodies, and governments for establishing safe and responsible
model sharing practices and preserving open-source benefits where safe.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09228" title="Abstract">arXiv:2311.09228</a> [<a href="/pdf/2311.09228" title="Download PDF">pdf</a>, <a href="/format/2311.09228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Starting Deep Learning Homework Earlier Improve Grades?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raff%2C+E">Edward Raff</a>, 
<a href="/search/cs?searchtype=author&query=Matuszek%2C+C">Cynthia Matuszek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in AI for AI Education, co-located with ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Intuitively, students who start a homework assignment earlier and spend more
time on it should receive better grades on the assignment. However, existing
literature on the impact of time spent on homework is not clear-cut and comes
mostly from K-12 education. It is not clear that these prior studies can inform
coursework in deep learning due to differences in demographics, as well as the
computational time needed for assignments to be completed. We study this
problem in a post-hoc study of three semesters of a deep learning course at the
University of Maryland, Baltimore County (UMBC), and develop a hierarchical
Bayesian model to help make principled conclusions about the impact on student
success given an approximate measure of the total time spent on the homework,
and how early they submitted the assignment. Our results show that both
submitting early and spending more time positively relate with final grade.
Surprisingly, the value of an additional day of work is apparently equal across
students, even when some require less total time to complete an assignment.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09230" title="Abstract">arXiv:2311.09230</a> [<a href="/pdf/2311.09230" title="Download PDF">pdf</a>, <a href="/ps/2311.09230" title="Download PostScript">ps</a>, <a href="/format/2311.09230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating and Improving Value Judgments in AI: A Scenario-Based Study  on Large Language Models&#x27; Depiction of Social Conventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+J">Jaeyoun You</a>, 
<a href="/search/cs?searchtype=author&query=Suh%2C+B">Bongwon Suh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure, 2 tables, The 18th International AAAI Conference on Web and Social Media (ICWSM 2024) Accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The adoption of generative AI technologies is swiftly expanding. Services
employing both linguistic and mul-timodal models are evolving, offering users
increasingly precise responses. Consequently, human reliance on these
technologies is expected to grow rapidly. With the premise that people will be
impacted by the output of AI, we explored approaches to help AI output produce
better results. Initially, we evaluated how contemporary AI services
competitively meet user needs, then examined society's depiction as mirrored by
Large Language Models (LLMs). We did a query experiment, querying about social
conventions in various countries and eliciting a one-word response. We compared
the LLMs' value judgments with public data and suggested an model of
decision-making in value-conflicting scenarios which could be adopted for
future machine value judgments. This paper advocates for a practical approach
to using AI as a tool for investigating other remote worlds. This re-search has
significance in implicitly rejecting the notion of AI making value judgments
and instead arguing a more critical perspective on the environment that defers
judgmental capabilities to individuals. We anticipate this study will empower
anyone, regardless of their capacity, to receive safe and accurate value
judgment-based out-puts effectively.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09231" title="Abstract">arXiv:2311.09231</a> [<a href="/pdf/2311.09231" title="Download PDF">pdf</a>, <a href="/format/2311.09231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Key Factors Affecting European Reactions to AI in European Full and  Flawed Democracies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+L">Long Pham</a>, 
<a href="/search/cs?searchtype=author&query=O%27Sullivan%2C+B">Barry O&#x27;Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+T+T">Tai Tan Mai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IJCAI DemorcAI 2023 - The 2nd International Workshop on Democracy and AI in conjunction with IJCAI 2023, Macau
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study examines the key factors that affect European reactions to
artificial intelligence (AI) in the context of both full and flawed democracies
in Europe. Analysing a dataset of 4,006 respondents, categorised into full
democracies and flawed democracies based on the Democracy Index developed by
the Economist Intelligence Unit (EIU), this research identifies crucial factors
that shape European attitudes toward AI in these two types of democracies. The
analysis reveals noteworthy findings. Firstly, it is observed that flawed
democracies tend to exhibit higher levels of trust in government entities
compared to their counterparts in full democracies. Additionally, individuals
residing in flawed democracies demonstrate a more positive attitude toward AI
when compared to respondents from full democracies. However, the study finds no
significant difference in AI awareness between the two types of democracies,
indicating a similar level of general knowledge about AI technologies among
European citizens. Moreover, the study reveals that trust in AI measures,
specifically "Trust AI Solution", does not significantly vary between full and
flawed democracies. This suggests that despite the differences in democratic
quality, both types of democracies have similar levels of confidence in AI
solutions.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09232" title="Abstract">arXiv:2311.09232</a> [<a href="/pdf/2311.09232" title="Download PDF">pdf</a>, <a href="/ps/2311.09232" title="Download PostScript">ps</a>, <a href="/format/2311.09232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Attracts Employees to Work Onsite in Times of Increased Remote  Working?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smite%2C+D">Darja Smite</a>, 
<a href="/search/cs?searchtype=author&query=Klotins%2C+E">Eriks Klotins</a>, 
<a href="/search/cs?searchtype=author&query=Moe%2C+N+B">Nils Brede Moe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 table, 2 figures. Submitted to IEEE Software
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">COVID-19 pandemic has irreversibly changed the attitude towards office
presence. While previously remote workers were met with skepticism and
distrust, today the same applies to companies prohibiting remote working.
Albeit many workspaces are half empty. In this paper, we offer insights into
the role of the office, corporate policies and actions regarding remote work in
eight companies: Ericsson, Knowit, SpareBank 1 Utvikling, Spotify, Storebrand,
Telenor, Company-X, Company-Y, and their sites in Sweden, Norway and the UK.
Our findings are twofold. First, we found that companies indeed struggle with
office presence and a large share of corporate space (35-67%) is underutilized.
Second, we found that the main motivator for office presence is Connection and
community, followed by Material offerings, Preference and Duty. Finally, we
summarize actionable advice to promote onsite work, which is likely to help
many other companies to rejuvenate life in their offices.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09233" title="Abstract">arXiv:2311.09233</a> [<a href="/pdf/2311.09233" title="Download PDF">pdf</a>, <a href="/format/2311.09233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Packing: from Visual Sensing to Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Juzhan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Minglun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Ruizhen Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Graphics (cs.GR); Robotics (cs.RO)

</div>
<p class="mathjax">We present a novel learning framework to solve the transport-and-packing
(TAP) problem in 3D. It constitutes a full solution pipeline from partial
observations of input objects via RGBD sensing and recognition to final box
placement, via robotic motion planning, to arrive at a compact packing in a
target container. The technical core of our method is a neural network for TAP,
trained via reinforcement learning (RL), to solve the NP-hard combinatorial
optimization problem. Our network simultaneously selects an object to pack and
determines the final packing location, based on a judicious encoding of the
continuously evolving states of partially observed source objects and available
spaces in the target container, using separate encoders both enabled with
attention mechanisms. The encoded feature vectors are employed to compute the
matching scores and feasibility masks of different pairings of box selection
and available space configuration for packing strategy optimization. Extensive
experiments, including ablation studies and physical packing execution by a
real robot (Universal Robot UR5e), are conducted to evaluate our method in
terms of its design choices, scalability, generalizability, and comparisons to
baselines, including the most recent RL-based TAP solution. We also contribute
the first benchmark for TAP which covers a variety of input settings and
difficulty levels.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09234" title="Abstract">arXiv:2311.09234</a> [<a href="/pdf/2311.09234" title="Download PDF">pdf</a>, <a href="/format/2311.09234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Genetic Algorithms for Evolution of QWOP Gaits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jones%2C+Z">Zachary Jones</a>, 
<a href="/search/cs?searchtype=author&query=Al-Saad%2C+M">Mohammad Al-Saad</a>, 
<a href="/search/cs?searchtype=author&query=Vavishta%2C+A">Ankush Vavishta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">QWOP is a browser-based, 2-dimensional flash game in which the player
controls an Olympic sprinter competing in a simulated 100-meter race. The goal
of the game is to advance the runner to the end of the 100-meter race as
quickly as possible using the Q, W, O, and P keys, which control the muscles in
the sprinters legs. Despite the game simple controls and straightforward goal,
it is renowned for its difficulty and unintuitive gameplay. In this paper, we
attempt to automatically discover effective QWOP gaits. We describe a
programmatic interface developed to play the game, and we introduce several
variants of a genetic algorithm tailored to solve this problem. We present
experimental results on the effectiveness of various representations,
initialization strategies, evolution paradigms, and parameter control
mechanisms.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09235" title="Abstract">arXiv:2311.09235</a> [<a href="/pdf/2311.09235" title="Download PDF">pdf</a>, <a href="/format/2311.09235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Diffusion for Materials Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengjiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">KwangHwan Cho</a>, 
<a href="/search/cs?searchtype=author&query=Merchant%2C+A">Amil Merchant</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Schuurmans%2C+D">Dale Schuurmans</a>, 
<a href="/search/cs?searchtype=author&query=Mordatch%2C+I">Igor Mordatch</a>, 
<a href="/search/cs?searchtype=author&query=Cubuk%2C+E+D">Ekin Dogus Cubuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://unified-materials.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative models trained on internet-scale data are capable of generating
novel and realistic texts, images, and videos. A natural next question is
whether these models can advance science, for example by generating novel
stable materials. Traditionally, models with explicit structures (e.g., graphs)
have been used in modeling structural relationships in scientific data (e.g.,
atoms and bonds in crystals), but generating structures can be difficult to
scale to large and complex systems. Another challenge in generating materials
is the mismatch between standard generative modeling metrics and downstream
applications. For instance, common metrics such as the reconstruction error do
not correlate well with the downstream goal of discovering stable materials. In
this work, we tackle the scalability challenge by developing a unified crystal
representation that can represent any crystal structure (UniMat), followed by
training a diffusion probabilistic model on these UniMat representations. Our
empirical results suggest that despite the lack of explicit structure modeling,
UniMat can generate high fidelity crystal structures from larger and more
complex chemical systems, outperforming previous graph-based approaches under
various generative modeling metrics. To better connect the generation quality
of materials to downstream applications, such as discovering novel stable
materials, we propose additional metrics for evaluating generative models of
materials, including per-composition formation energy and stability with
respect to convex hulls through decomposition energy from Density Function
Theory (DFT). Lastly, we show that conditional generation with UniMat can scale
to previously established crystal datasets with up to millions of crystals
structures, outperforming random structure search (the current leading method
for structure discovery) in discovering new stable materials.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09237" title="Abstract">arXiv:2311.09237</a> [<a href="/pdf/2311.09237" title="Download PDF">pdf</a>, <a href="/format/2311.09237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Innovative Tool for Uploading/Scraping Large Image Datasets on Social  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arceri%2C+N+F">Nicol&#xf2; Fabio Arceri</a>, 
<a href="/search/cs?searchtype=author&query=Giudice%2C+O">Oliver Giudice</a>, 
<a href="/search/cs?searchtype=author&query=Battiato%2C+S">Sebastiano Battiato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, presented at 2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Nowadays, people can retrieve and share digital information in an
increasingly easy and fast fashion through the well-known digital platforms,
including sensitive data, inappropriate or illegal content, and, in general,
information that might serve as probative evidence in court. Consequently, to
assess forensics issues, we need to figure out how to trace back to the posting
chain of a digital evidence (e.g., a picture, an audio) throughout the involved
platforms -- this is what Digital (also Forensics) Ballistics basically deals
with. With the entry of Machine Learning as a tool of the trade in many
research areas, the need for vast amounts of data has been dramatically
increasing over the last few years. However, collecting or simply find the
"right" datasets that properly enables data-driven research studies can turn
out to be not trivial in some cases, if not extremely challenging, especially
when it comes with highly specialized tasks, such as creating datasets analyzed
to detect the source media platform of a given digital media. In this paper we
propose an automated approach by means of a digital tool that we created on
purpose. The tool is capable of automatically uploading an entire image dataset
to the desired digital platform and then downloading all the uploaded pictures,
thus shortening the overall time required to output the final dataset to be
analyzed.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09238" title="Abstract">arXiv:2311.09238</a> [<a href="/pdf/2311.09238" title="Download PDF">pdf</a>, <a href="/format/2311.09238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Ultra-Low-Power Remote Health Monitoring: An Optimal and Adaptive  Compressed Sensing Framework for Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pagan%2C+J">J. Pagan</a>, 
<a href="/search/cs?searchtype=author&query=Fallahzadeh%2C+R">R. Fallahzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Pedram%2C+M">M. Pedram</a>, 
<a href="/search/cs?searchtype=author&query=Risco-Mart%C3%ADn%2C+J+L">Jos&#xe9; L. Risco-Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Moya%2C+J+M">J. M. Moya</a>, 
<a href="/search/cs?searchtype=author&query=Ayala%2C+J+L">J. L. Ayala</a>, 
<a href="/search/cs?searchtype=author&query=Ghasemzadeh%2C+H">H. Ghasemzadeh</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Mobile Computing, 18(3), pp. 658-673, 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Activity recognition, as an important component of behavioral monitoring and
intervention, has attracted enormous attention, especially in Mobile Cloud
Computing (MCC) and Remote Health Monitoring (RHM) paradigms. While recently
resource constrained wearable devices have been gaining popularity, their
battery life is limited and constrained by the frequent wireless transmission
of data to more computationally powerful back-ends. This paper proposes an
ultra-low power activity recognition system using a novel adaptive compressed
sensing technique that aims to minimize transmission costs. Coarse-grained
on-body sensor localization and unsupervised clustering modules are devised to
autonomously reconfigure the compressed sensing module for further power
saving. We perform a thorough heuristic optimization using Grammatical
Evolution (GE) to ensure minimal computation overhead of the proposed
methodology. Our evaluation on a real-world dataset and a low power wearable
sensing node demonstrates that our approach can reduce the energy consumption
of the wireless data transmission up to $81.2\%$ and $61.5\%$, with up to
$60.6\%$ and $35.0\%$ overall power savings in comparison with baseline and a
naive state-of-the-art approaches, respectively. These solutions lead to an
average activity recognition accuracy of $89.0\%$ -- only $4.8\%$ less than the
baseline accuracy -- while having a negligible energy overhead of on-node
computation.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09240" title="Abstract">arXiv:2311.09240</a> [<a href="/pdf/2311.09240" title="Download PDF">pdf</a>, <a href="/format/2311.09240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Devil in the Landscapes: Inferring Epidemic Exposure Risks from Street  View Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhenyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Y">Yanxin Xi</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+T">Tong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ACM SIGSPATIAL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Built environment supports all the daily activities and shapes our health.
Leveraging informative street view imagery, previous research has established
the profound correlation between the built environment and chronic,
non-communicable diseases; however, predicting the exposure risk of infectious
diseases remains largely unexplored. The person-to-person contacts and
interactions contribute to the complexity of infectious disease, which is
inherently different from non-communicable diseases. Besides, the complex
relationships between street view imagery and epidemic exposure also hinder
accurate predictions. To address these problems, we construct a regional
mobility graph informed by the gravity model, based on which we propose a
transmission-aware graph convolutional network (GCN) to capture disease
transmission patterns arising from human mobility. Experiments show that the
proposed model significantly outperforms baseline models by 8.54% in weighted
F1, shedding light on a low-cost, scalable approach to assess epidemic exposure
risks from street view imagery.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09241" title="Abstract">arXiv:2311.09241</a> [<a href="/pdf/2311.09241" title="Download PDF">pdf</a>, <a href="/format/2311.09241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain of Images for Intuitively Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fanxu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haotong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiding Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The human brain is naturally equipped to comprehend and interpret visual
information rapidly. When confronted with complex problems or concepts, we use
flowcharts, sketches, and diagrams to aid our thought process. Leveraging this
inherent ability can significantly enhance logical reasoning. However, current
Large Language Models (LLMs) do not utilize such visual intuition to help their
thinking. Even the most advanced version language models (e.g., GPT-4V and
LLaVA) merely align images into textual space, which means their reasoning
processes remain purely verbal. To mitigate such limitations, we present a
Chain of Images (CoI) approach, which can convert complex language reasoning
problems to simple pattern recognition by generating a series of images as
intermediate representations. Furthermore, we have developed a CoI evaluation
dataset encompassing 15 distinct domains where images can intuitively aid
problem-solving. Based on this dataset, we aim to construct a benchmark to
assess the capability of future multimodal large-scale models to leverage
images for reasoning. In supporting our CoI reasoning, we introduce a symbolic
multimodal large language model (SyMLLM) that generates images strictly based
on language instructions and accepts both text and image as input. Experiments
on Geometry, Chess and Common Sense tasks sourced from the CoI evaluation
dataset show that CoI improves performance significantly over the pure-language
Chain of Thoughts (CoT) baselines. The code is available at
https://github.com/GraphPKU/CoI.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09242" title="Abstract">arXiv:2311.09242</a> [<a href="/pdf/2311.09242" title="Download PDF">pdf</a>, <a href="/ps/2311.09242" title="Download PostScript">ps</a>, <a href="/format/2311.09242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping Eye Vergence Angle to the Depth of Real and Virtual Objects as  an Objective Measure of Depth Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arefin%2C+M+S">Mohammed Safayet Arefin</a>, 
<a href="/search/cs?searchtype=author&query=Swan%2C+J+E">J. Edward Swan II</a>, 
<a href="/search/cs?searchtype=author&query=Hoffing%2C+R+C">Russell Cohen Hoffing</a>, 
<a href="/search/cs?searchtype=author&query=Thurman%2C+S">Steven Thurman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal of Vision, Extended Reality and Vision Science Special Issue (Under Review)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Recently, extended reality (XR) displays including augmented reality (AR) and
virtual reality (VR) have integrated eye tracking capabilities, which could
enable novel ways of interacting with XR content. The vergence angle of the
eyes constantly changes according to the distance of fixated objects. Here we
measured vergence angle for eye fixations on real and simulated target objects
in three different environments: real objects in the real-world (real), virtual
objects in the real-world (AR), and virtual objects in the virtual world (VR)
using gaze data from an eye-tracking device. In a repeated-measures design with
13 participants, Gaze-measured Vergence Angle (GVA) was measured while
participants fixated on targets at varying distances. As expected, results
showed a significant main effect of target depth such that increasing GVA was
associated with closer targets. However, there were consistent individual
differences in baseline GVA. When these individual differences were controlled
for, there was a small but statistically-significant main effect of environment
(real, AR, VR). Importantly, GVA was stable with respect to the starting depth
of previously fixated targets and invariant to directionality (convergence vs.
divergence). In addition, GVA proved to be a more veridical depth estimate than
subjective depth judgements.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09243" title="Abstract">arXiv:2311.09243</a> [<a href="/pdf/2311.09243" title="Download PDF">pdf</a>, <a href="/ps/2311.09243" title="Download PostScript">ps</a>, <a href="/format/2311.09243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Efficacy of Interactive Language Therapy Based on LLM for  High-Functioning Autistic Adolescent Psychological Counseling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y">Yujin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Mingeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seojin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+O">Oyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+R+D">Ryan Donghan Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yoonha Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+D">Dohyun Lim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study investigates the efficacy of Large Language Models (LLMs) in
interactive language therapy for high-functioning autistic adolescents. With
the rapid advancement of artificial intelligence, particularly in natural
language processing, LLMs present a novel opportunity to augment traditional
psychological counseling methods. This research primarily focuses on evaluating
the LLM's ability to engage in empathetic, adaptable, and contextually
appropriate interactions within a therapeutic setting. A comprehensive
evaluation was conducted by a panel of clinical psychologists and psychiatrists
using a specially developed scorecard. The assessment covered various aspects
of the LLM's performance, including empathy, communication skills,
adaptability, engagement, and the ability to establish a therapeutic alliance.
The study avoided direct testing with patients, prioritizing privacy and
ethical considerations, and instead relied on simulated scenarios to gauge the
LLM's effectiveness. The results indicate that LLMs hold significant promise as
supportive tools in therapy, demonstrating strengths in empathetic engagement
and adaptability in conversation. However, challenges in achieving the depth of
personalization and emotional understanding characteristic of human therapists
were noted. The study also highlights the importance of ethical considerations
in the application of AI in therapeutic contexts. This research provides
valuable insights into the potential and limitations of using LLMs in
psychological counseling for autistic adolescents. It lays the groundwork for
future explorations into AI's role in mental health care, emphasizing the need
for ongoing development to enhance the capabilities of these models in
therapeutic settings.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09244" title="Abstract">arXiv:2311.09244</a> [<a href="/pdf/2311.09244" title="Download PDF">pdf</a>, <a href="/ps/2311.09244" title="Download PostScript">ps</a>, <a href="/format/2311.09244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality Evaluation of Projection-Based VR Displays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pape%2C+D">Dave Pape</a>, 
<a href="/search/cs?searchtype=author&query=Sandin%2C+D">Dan Sandin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, 4th International Immersive Projection Technology Workshop (Ames, Iowa, 19-20 June 2000)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We present a collection of heuristics and simple tests for evaluating the
quality of a projection-based virtual reality display. A typical VR system
includes numerous potential sources of error. By understanding the
characteristics of a correctly working system, and the types of errors that are
likely to occur, users can quickly determine if their display is inaccurate and
what components may need correction.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09245" title="Abstract">arXiv:2311.09245</a> [<a href="/pdf/2311.09245" title="Download PDF">pdf</a>, <a href="/format/2311.09245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Affine Invariance in Continuous-Domain Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohaddes%2C+A">Ali Mohaddes</a>, 
<a href="/search/cs?searchtype=author&query=Lederer%2C+J">Johannes Lederer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">The notion of group invariance helps neural networks in recognizing patterns
and features under geometric transformations. Indeed, it has been shown that
group invariance can largely improve deep learning performances in practice,
where such transformations are very common. This research studies affine
invariance on continuous-domain convolutional neural networks. Despite other
research considering isometric invariance or similarity invariance, we focus on
the full structure of affine transforms generated by the generalized linear
group $\mathrm{GL}_2(\mathbb{R})$. We introduce a new criterion to assess the
similarity of two input signals under affine transformations. Then, unlike
conventional methods that involve solving complex optimization problems on the
Lie group $G_2$, we analyze the convolution of lifted signals and compute the
corresponding integration over $G_2$. In sum, our research could eventually
extend the scope of geometrical transformations that practical deep-learning
pipelines can handle.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09246" title="Abstract">arXiv:2311.09246</a> [<a href="/pdf/2311.09246" title="Download PDF">pdf</a>, <a href="/format/2311.09246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smell of Fire Increases Behavioural Realism in Virtual Reality: A Case  Study on a Recreated MGM Grand Hotel Fire
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+H">Humayun Khan</a>, 
<a href="/search/cs?searchtype=author&query=Nilsson%2C+D">Daniel Nilsson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE International Symposium on Mixed and Augmented Reality (ISMAR) 2023, 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Virtual reality allows creating highly immersive visual and auditory
experiences, making users feel physically present in the environment. This
makes it an ideal platform to simulate dangerous scenarios, including fire
evacuation, and study human behaviour without exposing users to harmful
elements. However, human perception of the surroundings is based on the
integration of multiple sensory cues (visual, auditory, tactile, or/and
olfactory) present in the environment. When some of the sensory stimuli are
missing in the virtual experience, it can break the illusion of being there in
the environment and could lead to actions that deviate from normal behaviour.
In this work, we added an olfactory cue in a well-documented historic hotel
fire scenario that was recreated in VR, and examined the effects of the
olfactory cue on human behaviour. We conducted a between subject study on 40
naive participants. Our results show that the addition of the olfactory cue
could increase behavioural realism. We found that 80% of the studied actions
for the VR with olfactory cue condition matched the ones performed by the
survivors. In comparison, only 40% of the participants' actions for VR only
condition were similar to the survivors.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09247" title="Abstract">arXiv:2311.09247</a> [<a href="/pdf/2311.09247" title="Download PDF">pdf</a>, <a href="/format/2311.09247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+M">Melanie Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Palmarini%2C+A+B">Alessandro B. Palmarini</a>, 
<a href="/search/cs?searchtype=author&query=Moskvichev%2C+A">Arseny Moskvichev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09248" title="Abstract">arXiv:2311.09248</a> [<a href="/pdf/2311.09248" title="Download PDF">pdf</a>, <a href="/format/2311.09248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Home Goal Feature Model -- A guide to support Smart Homes for  Ageing in Place
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Logothetis%2C+I">Irini Logothetis</a>, 
<a href="/search/cs?searchtype=author&query=Rani%2C+P">Priya Rani</a>, 
<a href="/search/cs?searchtype=author&query=Sivasothy%2C+S">Shangeetha Sivasothy</a>, 
<a href="/search/cs?searchtype=author&query=Vasa%2C+R">Rajesh Vasa</a>, 
<a href="/search/cs?searchtype=author&query=Mouzakis%2C+K">Kon Mouzakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Smart technologies are significant in supporting ageing in place for elderly.
Leveraging Artificial Intelligence (AI) and Machine Learning (ML), it provides
peace of mind, enabling the elderly to continue living independently. Elderly
use smart technologies for entertainment and social interactions, this can be
extended to provide safety and monitor health and environmental conditions,
detect emergencies and notify informal and formal caregivers when care is
needed. This paper provides an overview of the smart home technologies
commercially available to support ageing in place, the advantages and
challenges of smart home technologies, and their usability from elderlys
perspective. Synthesizing prior knowledge, we created a structured Smart Home
Goal Feature Model (SHGFM) to resolve heuristic approaches used by the Subject
Matter Experts (SMEs) at aged care facilities and healthcare researchers in
adapting smart homes. The SHGFM provides SMEs the ability to (i) establish
goals and (ii) identify features to set up strategies to design, develop and
deploy smart homes for the elderly based on personalised needs. Our model
provides guidance to healthcare researchers and aged care industries to set up
smart homes based on the needs of elderly, by defining a set of goals at
different levels mapped to a different set of features.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09251" title="Abstract">arXiv:2311.09251</a> [<a href="/pdf/2311.09251" title="Download PDF">pdf</a>, <a href="/format/2311.09251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple and Powerful Framework for Stable Dynamic Network Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davis%2C+E">Ed Davis</a>, 
<a href="/search/cs?searchtype=author&query=Gallagher%2C+I">Ian Gallagher</a>, 
<a href="/search/cs?searchtype=author&query=Lawson%2C+D+J">Daniel John Lawson</a>, 
<a href="/search/cs?searchtype=author&query=Rubin-Delanchy%2C+P">Patrick Rubin-Delanchy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we address the problem of dynamic network embedding, that is,
representing the nodes of a dynamic network as evolving vectors within a
low-dimensional space. While the field of static network embedding is wide and
established, the field of dynamic network embedding is comparatively in its
infancy. We propose that a wide class of established static network embedding
methods can be used to produce interpretable and powerful dynamic network
embeddings when they are applied to the dilated unfolded adjacency matrix. We
provide a theoretical guarantee that, regardless of embedding dimension, these
unfolded methods will produce stable embeddings, meaning that nodes with
identical latent behaviour will be exchangeable, regardless of their position
in time or space. We additionally define a hypothesis testing framework which
can be used to evaluate the quality of a dynamic network embedding by testing
for planted structure in simulated networks. Using this, we demonstrate that,
even in trivial cases, unstable methods are often either conservative or encode
incorrect structure. In contrast, we demonstrate that our suite of stable
unfolded methods are not only more interpretable but also more powerful in
comparison to their unstable counterparts.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09252" title="Abstract">arXiv:2311.09252</a> [<a href="/pdf/2311.09252" title="Download PDF">pdf</a>, <a href="/ps/2311.09252" title="Download PostScript">ps</a>, <a href="/format/2311.09252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In the Red(dit): Social Media and Stock Prices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baker%2C+J">James Baker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL)

</div>
<p class="mathjax">Spearheaded by retail traders on the website reddit, the GameStop short
squeeze of early 2021 shows that social media embeds information that
correlates with market movements. This paper seeks to examine this relationship
by using daily frequencies of classified comments and buzzwords as additional
factors in a Fama-French three factor model. Comments are classified using an
unsupervised clustering method, while past studies have used pretrained models
that are not specific to the domains being studied.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09254" title="Abstract">arXiv:2311.09254</a> [<a href="/pdf/2311.09254" title="Download PDF">pdf</a>, <a href="/format/2311.09254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bayesian Agent-Based Framework for Argument Exchange Across Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Assaad%2C+L">Leon Assaad</a>, 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+R">Rafael Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Jalalimanesh%2C+A">Ammar Jalalimanesh</a>, 
<a href="/search/cs?searchtype=author&query=Phillips%2C+K">Kirsty Phillips</a>, 
<a href="/search/cs?searchtype=author&query=Schoeppl%2C+L">Leon Schoeppl</a>, 
<a href="/search/cs?searchtype=author&query=Hahn%2C+U">Ulrike Hahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">In this paper, we introduce a new framework for modelling the exchange of
multiple arguments across agents in a social network. To date, most modelling
work concerned with opinion dynamics, testimony, or communication across social
networks has involved only the simulated exchange of a single opinion or single
claim. By contrast, real-world debate involves the provision of numerous
individual arguments relevant to such an opinion. This may include arguments
both for and against, and arguments varying in strength. This prompts the need
for appropriate aggregation rules for combining diverse evidence as well as
rules for communication. Here, we draw on the Bayesian framework to create an
agent-based modelling environment that allows the study of belief dynamics
across complex domains characterised by Bayesian Networks. Initial case studies
illustrate the scope of the framework.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09256" title="Abstract">arXiv:2311.09256</a> [<a href="/pdf/2311.09256" title="Download PDF">pdf</a>, <a href="/format/2311.09256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reading Between the Mud: A Challenging Motorcycle Racer Number Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tyo%2C+J">Jacob Tyo</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+Y">Youngseog Chung</a>, 
<a href="/search/cs?searchtype=author&query=Olarinre%2C+M">Motolani Olarinre</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper introduces the off-road motorcycle Racer number Dataset (RnD), a
new challenging dataset for optical character recognition (OCR) research. RnD
contains 2,411 images from professional motorsports photographers that depict
motorcycle racers in off-road competitions. The images exhibit a wide variety
of factors that make OCR difficult, including mud occlusions, motion blur,
non-standard fonts, glare, complex backgrounds, etc. The dataset has 5,578
manually annotated bounding boxes around visible motorcycle numbers, along with
transcribed digits and letters. Our experiments benchmark leading OCR
algorithms and reveal an end-to-end F1 score of only 0.527 on RnD, even after
fine-tuning. Analysis of performance on different occlusion types shows mud as
the primary challenge, degrading accuracy substantially compared to normal
conditions. But the models struggle with other factors including glare, blur,
shadows, and dust. Analysis exposes substantial room for improvement and
highlights failure cases of existing models. RnD represents a valuable new
benchmark to drive innovation in real-world OCR capabilities. The authors hope
the community will build upon this dataset and baseline experiments to make
progress on the open problem of robustly recognizing text in unconstrained
natural environments. The dataset is available at
https://github.com/JacobTyo/SwinTextSpotter.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09257" title="Abstract">arXiv:2311.09257</a> [<a href="/pdf/2311.09257" title="Download PDF">pdf</a>, <a href="/format/2311.09257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UFOGen: You Forward Once Large Scale Text-to-Image Generation via  Diffusion GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yanwu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhisheng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+T">Tingbo Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-image diffusion models have demonstrated remarkable capabilities in
transforming textual prompts into coherent images, yet the computational cost
of their inference remains a persistent challenge. To address this issue, we
present UFOGen, a novel generative model designed for ultra-fast, one-step
text-to-image synthesis. In contrast to conventional approaches that focus on
improving samplers or employing distillation techniques for diffusion models,
UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN
objective. Leveraging a newly introduced diffusion-GAN objective and
initialization with pre-trained diffusion models, UFOGen excels in efficiently
generating high-quality images conditioned on textual descriptions in a single
step. Beyond traditional text-to-image generation, UFOGen showcases versatility
in applications. Notably, UFOGen stands among the pioneering models enabling
one-step text-to-image generation and diverse downstream tasks, presenting a
significant advancement in the landscape of efficient generative models.
\blfootnote{*Work done as a student researcher of Google, $\dagger$ indicates
equal contribution.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09262" title="Abstract">arXiv:2311.09262</a> [<a href="/pdf/2311.09262" title="Download PDF">pdf</a>, <a href="/format/2311.09262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling the Potential Impacts of Papers into Diffusion,  Conformity, and Contribution Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhikai Xue</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guoxiu He</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhuoren Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yangyang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Star Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to JASIST
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The potential impact of an academic paper is determined by various factors,
including its popularity and contribution. Existing models usually estimate
original citation counts based on static graphs and fail to differentiate
values from nuanced perspectives. In this study, we propose a novel graph
neural network to Disentangle the Potential impacts of Papers into Diffusion,
Conformity, and Contribution values (called DPPDCC). Given a target paper,
DPPDCC encodes temporal and structural features within the constructed dynamic
heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize
the importance of comparative and co-cited/citing information between papers
and aggregate snapshots evolutionarily. To unravel popularity, we contrast
augmented graphs to extract the essence of diffusion and predict the
accumulated citation binning to model conformity. We further apply orthogonal
constraints to encourage distinct modeling of each perspective and preserve the
inherent value of contribution. To evaluate models' generalization for papers
published at various times, we reformulate the problem by partitioning data
based on specific time points to mirror real-world conditions. Extensive
experimental results on three datasets demonstrate that DPPDCC significantly
outperforms baselines for previously, freshly, and immediately published
papers. Further analyses confirm its robust capabilities. We will make our
datasets and codes publicly available.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09263" title="Abstract">arXiv:2311.09263</a> [<a href="/pdf/2311.09263" title="Download PDF">pdf</a>, <a href="/format/2311.09263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-ICL: In-Context Learning without Human Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinghan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shuming Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In the era of Large Language Models (LLMs), human-computer interaction has
evolved towards natural language, offering unprecedented flexibility. Despite
this, LLMs are heavily reliant on well-structured prompts to function
efficiently within the realm of In-Context Learning. Vanilla In-Context
Learning relies on human-provided contexts, such as labeled examples, explicit
instructions, or other guiding mechanisms that shape the model's outputs. To
address this challenge, our study presents a universal framework named
Automatic In-Context Learning. Upon receiving a user's request, we ask the
model to independently generate examples, including labels, instructions, or
reasoning pathways. The model then leverages this self-produced context to
tackle the given problem. Our approach is universally adaptable and can be
implemented in any setting where vanilla In-Context Learning is applicable. We
demonstrate that our method yields strong performance across a range of tasks,
standing up well when compared to existing methods.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09264" title="Abstract">arXiv:2311.09264</a> [<a href="/pdf/2311.09264" title="Download PDF">pdf</a>, <a href="/format/2311.09264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-domain feature disentanglement for interpretable modeling of tumor  microenvironment impact on drug response
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+J">Jia Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">High-throughput screening technology has facilitated the generation of
large-scale drug responses across hundreds of cancer cell lines. However, there
exists significant discrepancy between in vitro cell lines and actual tumors in
vivo in terms of their response to drug treatments, because of tumors comprise
of complex cellular compositions and histopathology structure, known as tumor
microenvironment (TME), which greatly influences the drug cytotoxicity against
tumor cells. To date, no study has focused on modeling the impact of the TME on
clinical drug response. This paper proposed a domain adaptation network for
feature disentanglement to separate representations of cancer cells and TME of
a tumor in patients. Two denoising autoencoders were separately used to extract
features from cell lines (source domain) and tumors (target domain) for partial
domain alignment and feature decoupling. The specific encoder was enforced to
extract information only about TME. Moreover, to ensure generalizability to
novel drugs, we applied a graph attention network to learn the latent
representation of drugs, allowing us to linearly model the drug perturbation on
cellular state in latent space. We calibrated our model on a benchmark dataset
and demonstrated its superior performance in predicting clinical drug response
and dissecting the influence of the TME on drug efficacy.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09265" title="Abstract">arXiv:2311.09265</a> [<a href="/pdf/2311.09265" title="Download PDF">pdf</a>, <a href="/format/2311.09265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+Z">Zhongjie Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weining Qian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingyi Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With the emergence of diffusion models and rapid development in image
processing, it has become effortless to generate fancy images in tasks such as
style transfer and image editing. However, these impressive image processing
approaches face consistency issues in video processing. In this paper, we
propose a powerful model-free toolkit called FastBlend to address the
consistency problem for video processing. Based on a patch matching algorithm,
we design two inference modes, including blending and interpolation. In the
blending mode, FastBlend eliminates video flicker by blending the frames within
a sliding window. Moreover, we optimize both computational efficiency and video
quality according to different application scenarios. In the interpolation
mode, given one or more keyframes rendered by diffusion models, FastBlend can
render the whole video. Since FastBlend does not modify the generation process
of diffusion models, it exhibits excellent compatibility. Extensive experiments
have demonstrated the effectiveness of FastBlend. In the blending mode,
FastBlend outperforms existing methods for video deflickering and video
synthesis. In the interpolation mode, FastBlend surpasses video interpolation
and model-based video processing approaches. The source codes have been
released on GitHub.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09266" title="Abstract">arXiv:2311.09266</a> [<a href="/pdf/2311.09266" title="Download PDF">pdf</a>, <a href="/format/2311.09266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarially Robust Spiking Neural Networks Through Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%96zdenizci%2C+O">Ozan &#xd6;zdenizci</a>, 
<a href="/search/cs?searchtype=author&query=Legenstein%2C+R">Robert Legenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Spiking neural networks (SNNs) provide an energy-efficient alternative to a
variety of artificial neural network (ANN) based AI applications. As the
progress in neuromorphic computing with SNNs expands their use in applications,
the problem of adversarial robustness of SNNs becomes more pronounced. To the
contrary of the widely explored end-to-end adversarial training based
solutions, we address the limited progress in scalable robust SNN training
methods by proposing an adversarially robust ANN-to-SNN conversion algorithm.
Our method provides an efficient approach to embrace various computationally
demanding robust learning objectives that have been proposed for ANNs. During a
post-conversion robust finetuning phase, our method adversarially optimizes
both layer-wise firing thresholds and synaptic connectivity weights of the SNN
to maintain transferred robustness gains from the pre-trained ANN. We perform
experimental evaluations in numerous adaptive adversarial settings that account
for the spike-based operation dynamics of SNNs, and show that our approach
yields a scalable state-of-the-art solution for adversarially robust deep SNNs
with low-latency.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09267" title="Abstract">arXiv:2311.09267</a> [<a href="/pdf/2311.09267" title="Download PDF">pdf</a>, <a href="/format/2311.09267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuroscience inspired scientific machine learning (Part-1): Variable  spiking neuron for regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Shailesh Garg</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souvik Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Redundant information transfer in a neural network can increase the
complexity of the deep learning model, thus increasing its power consumption.
We introduce in this paper a novel spiking neuron, termed Variable Spiking
Neuron (VSN), which can reduce the redundant firing using lessons from
biological neuron inspired Leaky Integrate and Fire Spiking Neurons (LIF-SN).
The proposed VSN blends LIF-SN and artificial neurons. It garners the advantage
of intermittent firing from the LIF-SN and utilizes the advantage of continuous
activation from the artificial neuron. This property of the proposed VSN makes
it suitable for regression tasks, which is a weak point for the vanilla spiking
neurons, all while keeping the energy budget low. The proposed VSN is tested
against both classification and regression tasks. The results produced advocate
favorably towards the efficacy of the proposed spiking neuron, particularly for
regression tasks.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09269" title="Abstract">arXiv:2311.09269</a> [<a href="/pdf/2311.09269" title="Download PDF">pdf</a>, <a href="/format/2311.09269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+E">En-Te Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+W">Wei-Jie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Ding-Tao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+L">Long Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing Object Pose Estimation (OPE) methods for stacked scenarios are not
robust to changes in object scale. This paper proposes a new 6DoF OPE network
(NormNet) for different scale objects in stacked scenarios. Specifically, each
object's scale is first learned with point-wise regression. Then, all objects
in the stacked scenario are normalized into the same scale through semantic
segmentation and affine transformation. Finally, they are fed into a shared
pose estimator to recover their 6D poses. In addition, we introduce a new
Sim-to-Real transfer pipeline, combining style transfer and domain
randomization. This improves the NormNet's performance on real data even if we
only train it on synthetic data. Extensive experiments demonstrate that the
proposed method achieves state-of-the-art performance on public benchmarks and
the MultiScale dataset we constructed. The real-world experiments show that our
method can robustly estimate the 6D pose of objects at different scales.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09270" title="Abstract">arXiv:2311.09270</a> [<a href="/pdf/2311.09270" title="Download PDF">pdf</a>, <a href="/format/2311.09270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedCode: Communication-Efficient Federated Learning via Transferring  Codebooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalilian%2C+S">Saeed Khalilian</a>, 
<a href="/search/cs?searchtype=author&query=Tsouvalas%2C+V">Vasileios Tsouvalas</a>, 
<a href="/search/cs?searchtype=author&query=Ozcelebi%2C+T">Tanir Ozcelebi</a>, 
<a href="/search/cs?searchtype=author&query=Meratnia%2C+N">Nirvana Meratnia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) is a distributed machine learning paradigm that
enables learning models from decentralized local data. While FL offers
appealing properties for clients' data privacy, it imposes high communication
burdens for exchanging model weights between a server and the clients. Existing
approaches rely on model compression techniques, such as pruning and weight
clustering to tackle this. However, transmitting the entire set of weight
updates at each federated round, even in a compressed format, limits the
potential for a substantial reduction in communication volume. We propose
FedCode where clients transmit only codebooks, i.e., the cluster centers of
updated model weight values. To ensure a smooth learning curve and proper
calibration of clusters between the server and the clients, FedCode
periodically transfers model weights after multiple rounds of solely
communicating codebooks. This results in a significant reduction in
communication volume between clients and the server in both directions, without
imposing significant computational overhead on the clients or leading to major
performance degradation of the models. We evaluate the effectiveness of FedCode
using various publicly available datasets with ResNet-20 and MobileNet backbone
model architectures. Our evaluations demonstrate a 12.2-fold data transmission
reduction on average while maintaining a comparable model performance with an
average accuracy loss of 1.3% compared to FedAvg. Further validation of FedCode
performance under non-IID data distributions showcased an average accuracy loss
of 2.0% compared to FedAvg while achieving approximately a 12.7-fold data
transmission reduction.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09271" title="Abstract">arXiv:2311.09271</a> [<a href="/pdf/2311.09271" title="Download PDF">pdf</a>, <a href="/format/2311.09271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empathetic User-Centric Chatbot for Emotional Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yanting Pan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yixuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yuchen Niu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper explores the intersection of Otome Culture and artificial
intelligence, particularly focusing on how Otome-oriented games fulfill the
emotional needs of young women. These games, which are deeply rooted in a
subcultural understanding of love, provide players with feelings of
satisfaction, companionship, and protection through carefully crafted narrative
structures and character development. With the proliferation of Large Language
Models (LLMs), there is an opportunity to transcend traditional static game
narratives and create dynamic, emotionally responsive interactions. We present
a case study of Tears of Themis, where we have integrated LLM technology to
enhance the interactive experience. Our approach involves augmenting existing
game narratives with a Question and Answer (QA) system, enriched through data
augmentation and emotional enhancement techniques, resulting in a chatbot that
offers realistic and supportive companionship.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09272" title="Abstract">arXiv:2311.09272</a> [<a href="/pdf/2311.09272" title="Download PDF">pdf</a>, <a href="/format/2311.09272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear time Evidence Accumulation Clustering with KMeans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Candel%2C+G">Ga&#xeb;lle Candel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, proofread not completed
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Among ensemble clustering methods, Evidence Accumulation Clustering is one of
the simplest technics. In this approach, a co-association (CA) matrix
representing the co-clustering frequency is built and then clustered to extract
consensus clusters. Compared to other approaches, this one is simple as there
is no need to find matches between clusters obtained from two different
partitionings. Nevertheless, this method suffers from computational issues, as
it requires to compute and store a matrix of size n x n, where n is the number
of items. Due to the quadratic cost, this approach is reserved for small
datasets. This work describes a trick which mimic the behavior of average
linkage clustering. We found a way of computing efficiently the density of a
partitioning, reducing the cost from a quadratic to linear complexity.
Additionally, we proved that the k-means maximizes naturally the density. We
performed experiments on several benchmark datasets where we compared the
k-means and the bisecting version to other state-of-the-art consensus
algorithms. The k-means results are comparable to the best state of the art in
terms of NMI while keeping the computational cost low. Additionally, the
k-means led to the best results in terms of density. These results provide
evidence that consensus clustering can be solved with simple algorithms.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09273" title="Abstract">arXiv:2311.09273</a> [<a href="/pdf/2311.09273" title="Download PDF">pdf</a>, <a href="/format/2311.09273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-vehicle Sensing and Data Analysis for Older Drivers with Mild  Cognitive Impairment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moshfeghi%2C+S">Sonia Moshfeghi</a>, 
<a href="/search/cs?searchtype=author&query=Jan%2C+M+T">Muhammad Tanveer Jan</a>, 
<a href="/search/cs?searchtype=author&query=Conniff%2C+J">Joshua Conniff</a>, 
<a href="/search/cs?searchtype=author&query=Ghoreishi%2C+S+G+A">Seyedeh Gol Ara Ghoreishi</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Jinwoo Jang</a>, 
<a href="/search/cs?searchtype=author&query=Furht%2C+B">Borko Furht</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kwangsoo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Rosselli%2C+M">Monica Rosselli</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+D">David Newman</a>, 
<a href="/search/cs?searchtype=author&query=Tappen%2C+R">Ruth Tappen</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+D">Dana Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, IEEE HONET Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Driving is a complex daily activity indicating age and disease related
cognitive declines. Therefore, deficits in driving performance compared with
ones without mild cognitive impairment (MCI) can reflect changes in cognitive
functioning. There is increasing evidence that unobtrusive monitoring of older
adults driving performance in a daily-life setting may allow us to detect
subtle early changes in cognition. The objectives of this paper include
designing low-cost in-vehicle sensing hardware capable of obtaining
high-precision positioning and telematics data, identifying important
indicators for early changes in cognition, and detecting early-warning signs of
cognitive impairment in a truly normal, day-to-day driving condition with
machine learning approaches. Our statistical analysis comparing drivers with
MCI to those without reveals that those with MCI exhibit smoother and safer
driving patterns. This suggests that drivers with MCI are cognizant of their
condition and tend to avoid erratic driving behaviors. Furthermore, our Random
Forest models identified the number of night trips, number of trips, and
education as the most influential factors in our data evaluation.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09274" title="Abstract">arXiv:2311.09274</a> [<a href="/pdf/2311.09274" title="Download PDF">pdf</a>, <a href="/format/2311.09274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructing interpretable principal curve using Neural ODEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guangzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bingxian Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">The study of high dimensional data sets often rely on their low dimensional
projections that preserve the local geometry of the original space. While
numerous methods have been developed to summarize this space as variations of
tree-like structures, they are usually non-parametric and "static" in nature.
As data may come from systems that are dynamical such as a differentiating
cell, a static, non-parametric characterization of the space may not be the
most appropriate. Here, we developed a framework, the principal flow, that is
capable of characterizing the space in a dynamical manner. The principal flow,
defined using neural ODEs, directs motion of a particle through the space,
where the trajectory of the particle resembles the principal curve of the
dataset. We illustrate that our framework can be used to characterize shapes of
various complexities, and is flexible to incorporate summaries of relaxation
dynamics.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09275" title="Abstract">arXiv:2311.09275</a> [<a href="/pdf/2311.09275" title="Download PDF">pdf</a>, <a href="/ps/2311.09275" title="Download PostScript">ps</a>, <a href="/format/2311.09275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Sparse Ising Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zick%2C+K+M">Kenneth M. Zick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn)

</div>
<p class="mathjax">Sparse Ising problems can be found in application areas such as logistics,
condensed matter physics and training of deep Boltzmann networks, but can be
very difficult to tackle with high efficiency and accuracy. This report
presents new data demonstrating significantly higher performance on some
longstanding benchmark problems with up to 20,000 variables. The data come from
a new heuristic algorithm tested on the large sparse instances from the Gset
benchmark suite. Relative to leading reported combinations of speed and
accuracy (e.g., from Toshiba's Simulated Bifurcation Machine and Breakout Local
Search), a proof-of-concept implementation reached targets 2-4 orders of
magnitude faster. For two instances (G72 and G77) the new algorithm discovered
a better solution than all previously reported values. Solution bitstrings
confirming these two best solutions are provided. The data suggest exciting
possibilities for pushing the sparse Ising performance frontier to potentially
strengthen algorithm portfolios, AI toolkits and decision-making systems.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09276" title="Abstract">arXiv:2311.09276</a> [<a href="/pdf/2311.09276" title="Download PDF">pdf</a>, <a href="/format/2311.09276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Citizen Science for Flood Extent Detection using Machine  Learning Benchmark Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramasubramanian%2C+M">Muthukumaran Ramasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Gurung%2C+I">Iksha Gurung</a>, 
<a href="/search/cs?searchtype=author&query=Gahlot%2C+S">Shubhankar Gahlot</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4nsch%2C+R">Ronny H&#xe4;nsch</a>, 
<a href="/search/cs?searchtype=author&query=Molthan%2C+A+L">Andrew L. Molthan</a>, 
<a href="/search/cs?searchtype=author&query=Maskey%2C+M">Manil Maskey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages in AGU format, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Accurate detection of inundated water extents during flooding events is
crucial in emergency response decisions and aids in recovery efforts. Satellite
Remote Sensing data provides a global framework for detecting flooding extents.
Specifically, Sentinel-1 C-Band Synthetic Aperture Radar (SAR) imagery has
proven to be useful in detecting water bodies due to low backscatter of water
features in both co-polarized and cross-polarized SAR imagery. However,
increased backscatter can be observed in certain flooded regions such as
presence of infrastructure and trees - rendering simple methods such as pixel
intensity thresholding and time-series differencing inadequate. Machine
Learning techniques has been leveraged to precisely capture flood extents in
flooded areas with bumps in backscatter but needs high amounts of labelled data
to work desirably. Hence, we created a labeled known water body extent and
flooded area extents during known flooding events covering about 36,000 sq.
kilometers of regions within mainland U.S and Bangladesh. Further, We also
leveraged citizen science by open-sourcing the dataset and hosting an open
competition based on the dataset to rapidly prototype flood extent detection
using community generated models. In this paper we present the information
about the dataset, the data processing pipeline, a baseline model and the
details about the competition, along with discussion on winning approaches. We
believe the dataset adds to already existing datasets based on Sentinel-1C SAR
data and leads to more robust modeling of flood extents. We also hope the
results from the competition pushes the research in flood extent detection
further.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09277" title="Abstract">arXiv:2311.09277</a> [<a href="/pdf/2311.09277" title="Download PDF">pdf</a>, <a href="/format/2311.09277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Chain-of-Thought Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chia%2C+Y+K">Yew Ken Chia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guizhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+L+A">Luu Anh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the success of chain of thought in enhancing language model
reasoning, the underlying process remains less well understood. Although
logically sound reasoning appears inherently crucial for chain of thought,
prior studies surprisingly reveal minimal impact when using invalid
demonstrations instead. Furthermore, the conventional chain of thought does not
inform language models on what mistakes to avoid, which potentially leads to
more errors. Hence, inspired by how humans can learn from both positive and
negative examples, we propose contrastive chain of thought to enhance language
model reasoning. Compared to the conventional chain of thought, our approach
provides both valid and invalid reasoning demonstrations, to guide the model to
reason step-by-step while reducing reasoning mistakes. To improve
generalization, we introduce an automatic method to construct contrastive
demonstrations. Our experiments on reasoning benchmarks demonstrate that
contrastive chain of thought can serve as a general enhancement of
chain-of-thought prompting.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09278" title="Abstract">arXiv:2311.09278</a> [<a href="/pdf/2311.09278" title="Download PDF">pdf</a>, <a href="/format/2311.09278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbol-LLM: Towards Foundational Symbol-centric Interface For Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fangzhi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qiushi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Siyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+F">Fei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qika Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have greatly propelled the progress in natural
language(NL)-centric tasks based on NL interface. However, the NL form is not
enough for world knowledge. Current works focus on this question by injecting
specific symbolic knowledge into LLM, which ignore two critical challenges: the
interrelations between various symbols and the balance between symbolic-centric
and NL-centric capabilities. In this work, we tackle these challenges from both
a data and framework perspective and introduce Symbol-LLM series models. First,
we collect 34 symbolic tasks, covering ~20 different forms, which are unified
to capture symbol interrelations. Then, a two-stage tuning framework succeeds
in injecting symbolic knowledge without loss of the generality ability.
Extensive experiments on both symbol- and NL-centric tasks demonstrate the
balanced and superior performances of Symbol-LLM series models.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09308" title="Abstract">arXiv:2311.09308</a> [<a href="/pdf/2311.09308" title="Download PDF">pdf</a>, <a href="/format/2311.09308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divergences between Language Models and Human Brains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuchen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+E">Emmy Liu</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Wehbe%2C+L">Leila Wehbe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Do machines and humans process language in similar ways? A recent line of
research has hinted in the affirmative, demonstrating that human brain signals
can be effectively predicted using the internal representations of language
models (LMs). This is thought to reflect shared computational principles
between LMs and human language processing. However, there are also clear
differences in how LMs and humans acquire and use language, even if the final
task they are performing is the same. Despite this, there is little work
exploring systematic differences between human and machine language processing
using brain data. To address this question, we examine the differences between
LM representations and the human brain's responses to language, specifically by
examining a dataset of Magnetoencephalography (MEG) responses to a written
narrative. In doing so we identify three phenomena that, in prior work, LMs
have been found to not capture well: emotional understanding, figurative
language processing, and physical commonsense. By fine-tuning LMs on datasets
related to these phenomena, we observe that fine-tuned LMs show improved
alignment with human brain responses across these tasks. Our study implies that
the observed divergences between LMs and human brains may stem from LMs'
inadequate representation of these specific types of knowledge.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09319" title="Abstract">arXiv:2311.09319</a> [<a href="/pdf/2311.09319" title="Download PDF">pdf</a>, <a href="/format/2311.09319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spoken Word2Vec: A Perspective And Some Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayeed%2C+M+A">Mohammad Amaan Sayeed</a>, 
<a href="/search/cs?searchtype=author&query=Aldarmaki%2C+H">Hanan Aldarmaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Text word embeddings that encode distributional semantic features work by
modeling contextual similarities of frequently occurring words. Acoustic word
embeddings, on the other hand, typically encode low-level phonetic
similarities. Semantic embeddings for spoken words have been previously
explored using similar algorithms to Word2Vec, but the resulting vectors still
mainly encoded phonetic rather than semantic features. In this paper, we
examine the assumptions and architectures used in previous works and show
experimentally how Word2Vec algorithms fail to encode distributional semantics
when the input units are acoustically correlated. In addition, previous works
relied on the simplifying assumptions of perfect word segmentation and
clustering by word type. Given these conditions, a trivial solution identical
to text-based embeddings has been overlooked. We follow this simpler path using
automatic word type clustering and examine the effects on the resulting
embeddings, highlighting the true challenges in this task.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09325" title="Abstract">arXiv:2311.09325</a> [<a href="/pdf/2311.09325" title="Download PDF">pdf</a>, <a href="/format/2311.09325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving fit to human reading times via temperature-scaled surprisal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tong Liu</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0krjanec%2C+I">Iza &#x160;krjanec</a>, 
<a href="/search/cs?searchtype=author&query=Demberg%2C+V">Vera Demberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Past studies have provided broad support for that words with lower
predictability (i.e., higher surprisal) require more time for comprehension by
using large language models (LLMs) to simulate humans' cognitive load. In
general, these studies have implicitly assumed that the probability scores from
LLMs are accurate, ignoring the discrepancies between human cognition and LLMs
from this standpoint. Inspired by the concept of probability calibration, we
are the first work to focus on the probability distribution for human reading
simulation. We propose to use temperature-scaled surprisal, a surprisal
calculated by shaped probability, to be the predictor of human reading times.
Our results across three corpora consistently revealed that such a surprisal
can drastically improve the prediction of reading times. Setting the
temperature to be approximately 2.5 across all models and datasets can yield up
to an 89% of increase in delta log-likelihood in our setting. We also propose a
calibration metric to quantify the possible human-likeness bias. Further
analysis was done and provided insights into this phenomenon.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09327" title="Abstract">arXiv:2311.09327</a> [<a href="/pdf/2311.09327" title="Download PDF">pdf</a>, <a href="/ps/2311.09327" title="Download PostScript">ps</a>, <a href="/format/2311.09327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survey of Rigid Body Simulation with Extended Position Based Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seabra%2C+M+L+N">Miguel Luis Nunes Seabra</a>, 
<a href="/search/cs?searchtype=author&query=Lopes%2C+D+S">Daniel Sim&#xf5;es Lopes</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+J+M">Jo&#xe3;o Madeiras Pereira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Interactive real-time rigid body simulation is a crucial tool in any modern
game engine or 3D authoring tool. The quest for fast, robust and accurate
simulations is ever evolving. PBRBD (Position Based Rigid Body Dynamics), a
recent expansion of PBD (Position Based Dynamics), is a novel approach for this
issue. This work aims at providing a comprehensible state-of-the art comparison
between Position Based methods and other real-time simulation methods used for
rigid body dynamics using a custom 3D physics engine for benchmarking and
comparing PBRBD (Position Based Rigid Body Dynamics), and some variants, with
state-of-the-art simulators commonly used in the gaming industry, PhysX and
Havok. Showing that PBRBD can produce simulations that are accurate and stable,
excelling at maintaining stable energy levels, and allowing for a variety of
constraints, but is limited in its handling of stable stacks of rigid bodies.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09329" title="Abstract">arXiv:2311.09329</a> [<a href="/pdf/2311.09329" title="Download PDF">pdf</a>, <a href="/format/2311.09329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Analysis of Machine Learning Models for Early Detection of  Hospital-Acquired Infections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harvey%2C+E">Ethan Harvey</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Junzi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+E">Erina Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Samadani%2C+A">Ali Samadani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As more and more infection-specific machine learning models are developed and
planned for clinical deployment, simultaneously running predictions from
different models may provide overlapping or even conflicting information. It is
important to understand the concordance and behavior of parallel models in
deployment. In this study, we focus on two models for the early detection of
hospital-acquired infections (HAIs): 1) the Infection Risk Index (IRI) and 2)
the Ventilator-Associated Pneumonia (VAP) prediction model. The IRI model was
built to predict all HAIs, whereas the VAP model identifies patients at risk of
developing ventilator-associated pneumonia. These models could make important
improvements in patient outcomes and hospital management of infections through
early detection of infections and in turn, enable early interventions. The two
models vary in terms of infection label definition, cohort selection, and
prediction schema. In this work, we present a comparative analysis between the
two models to characterize concordances and confusions in predicting HAIs by
these models. The learnings from this study will provide important findings for
how to deploy multiple concurrent disease-specific models in the future.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09332" title="Abstract">arXiv:2311.09332</a> [<a href="/pdf/2311.09332" title="Download PDF">pdf</a>, <a href="/format/2311.09332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Weighting Strategies for WENO Schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Barreto%2C+D">Daniel Barreto</a>, 
<a href="/search/math?searchtype=author&query=de+R.+Borges%2C+R+B">Rafael B. de R. Borges</a>, 
<a href="/search/math?searchtype=author&query=Costa%2C+B">Bruno Costa</a>, 
<a href="/search/math?searchtype=author&query=Santos%2C+S+d">Silvaneo dos Santos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this article, we propose a modified convex combination of the polynomial
reconstructions of odd-order WENO schemes to maintain the central substencil
prevalence over the lateral ones in all parts of the solution. New "centered"
versions of the classical WENO-Z and its less dissipative counterpart, WENO-Z+,
are defined through very simple modifications of the classical nonlinear
weights and show significantly superior numerical properties; for instance, a
well-known dispersion error for long-term runs is fixed, along with decreased
dissipation and better shock-capturing abilities. Moreover, the proposed
centered version of WENO-Z+ has no ad-hoc parameters and no dependence on the
powers of the grid size. All the new schemes are thoroughly analyzed concerning
convergence at critical points, adding to the discussion on the relevance of
such convergence to the numerical simulation of typical hyperbolic conservation
laws problems. Nonlinear spectral analysis confirms the enhancement achieved by
the new schemes over the standard ones.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09333" title="Abstract">arXiv:2311.09333</a> [<a href="/pdf/2311.09333" title="Download PDF">pdf</a>, <a href="/ps/2311.09333" title="Download PostScript">ps</a>, <a href="/format/2311.09333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Data Augmentation with CTGAN for Smart Manufacturing:  Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper  Production
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khosravi%2C+H">Hamed Khosravi</a>, 
<a href="/search/cs?searchtype=author&query=Farhadpour%2C+S">Sarah Farhadpour</a>, 
<a href="/search/cs?searchtype=author&query=Grandhi%2C+M">Manikanta Grandhi</a>, 
<a href="/search/cs?searchtype=author&query=Raihan%2C+A+S">Ahmed Shoyeb Raihan</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Srinjoy Das</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+I">Imtiaz Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A significant challenge for predictive maintenance in the pulp-and-paper
industry is the infrequency of paper breaks during the production process. In
this article, operational data is analyzed from a paper manufacturing machine
in which paper breaks are relatively rare but have a high economic impact.
Utilizing a dataset comprising 18,398 instances derived from a quality
assurance protocol, we address the scarcity of break events (124 cases) that
pose a challenge for machine learning predictive models. With the help of
Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority
Oversampling Technique (SMOTE), we implement a novel data augmentation
framework. This method ensures that the synthetic data mirrors the distribution
of the real operational data but also seeks to enhance the performance metrics
of predictive modeling. Before and after the data augmentation, we evaluate
three different machine learning algorithms-Decision Trees (DT), Random Forest
(RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our
study achieved significant improvements in predictive maintenance performance
metrics. The efficacy of CTGAN in addressing data scarcity was evident, with
the models' detection of machine breaks (Class 1) improving by over 30% for
Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression.
With this methodological advancement, this study contributes to industrial
quality control and maintenance scheduling by addressing rare event prediction
in manufacturing processes.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09335" title="Abstract">arXiv:2311.09335</a> [<a href="/pdf/2311.09335" title="Download PDF">pdf</a>, <a href="/format/2311.09335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large  Language Models for Abstractive Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chrysostomou%2C+G">George Chrysostomou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhixue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+M">Miles Williams</a>, 
<a href="/search/cs?searchtype=author&query=Aletras%2C+N">Nikolaos Aletras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite their remarkable performance on abstractive summarization, large
language models (LLMs) face two significant challenges: their considerable size
and tendency to hallucinate. Hallucinations are concerning because they erode
the reliability of LLMs and raise safety issues. Pruning is a technique that
reduces model size by removing redundant weights to create sparse models that
enable more efficient inference. Pruned models yield comparable performance to
their counterpart full-sized models, making them ideal alternatives when
operating on a limited budget. However, the effect that pruning has upon
hallucinations in abstractive summarization with LLMs has yet to be explored.
In this paper, we provide an extensive empirical study on the hallucinations
produced by pruned models across three standard summarization tasks, two
pruning approaches, three instruction-tuned LLMs, and three hallucination
evaluation metrics. Surprisingly, we find that pruned LLMs hallucinate less
compared to their full-sized counterparts. Our follow-up analysis suggests that
pruned models tend to depend more on the source input and less on their
parametric knowledge from pre-training for generation. This greater dependency
on the source input leads to a higher lexical overlap between generated content
and the source input, which can be a reason for the reduction in
hallucinations.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09336" title="Abstract">arXiv:2311.09336</a> [<a href="/pdf/2311.09336" title="Download PDF">pdf</a>, <a href="/format/2311.09336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained  Actionable Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenda Xu</a>, 
<a href="/search/cs?searchtype=author&query=Deutsch%2C+D">Daniel Deutsch</a>, 
<a href="/search/cs?searchtype=author&query=Finkelstein%2C+M">Mara Finkelstein</a>, 
<a href="/search/cs?searchtype=author&query=Juraska%2C+J">Juraj Juraska</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Biao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhongtao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Freitag%2C+M">Markus Freitag</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent improvements in text generation have leveraged human feedback to
improve the quality of the generated output. However, human feedback is not
always available, especially during inference. In this work, we propose an
inference time optimization method FITO to use fine-grained actionable feedback
in the form of error type, error location and severity level that are predicted
by a learned error pinpoint model for iterative refinement. FITO starts with an
initial output, then iteratively incorporates the feedback via a refinement
model that generates an improved output conditioned on the feedback. Given the
uncertainty of consistent refined samples at iterative steps, we formulate
iterative refinement into a local search problem and develop a simulated
annealing based algorithm that balances exploration of the search space and
optimization for output quality. We conduct experiments on three text
generation tasks, including machine translation, long-form question answering
(QA) and topical summarization. We observe 0.8 and 0.7 MetricX gain on
Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at
long form QA and topic summarization respectively, with a single iteration of
refinement. With our simulated annealing algorithm, we see further quality
improvements, including up to 1.7 MetricX improvements over the baseline
approach.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09338" title="Abstract">arXiv:2311.09338</a> [<a href="/pdf/2311.09338" title="Download PDF">pdf</a>, <a href="/format/2311.09338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenges for Predictive Modeling with Neural Network Techniques using  Error-Prone Dietary Intake Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spicker%2C+D">Dylan Spicker</a>, 
<a href="/search/cs?searchtype=author&query=Nazemi%2C+A">Amir Nazemi</a>, 
<a href="/search/cs?searchtype=author&query=Hutchinson%2C+J">Joy Hutchinson</a>, 
<a href="/search/cs?searchtype=author&query=Fieguth%2C+P">Paul Fieguth</a>, 
<a href="/search/cs?searchtype=author&query=Kirkpatrick%2C+S+I">Sharon I. Kirkpatrick</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+M">Michael Wallace</a>, 
<a href="/search/cs?searchtype=author&query=Dodd%2C+K+W">Kevin W. Dodd</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Dietary intake data are routinely drawn upon to explore diet-health
relationships. However, these data are often subject to measurement error,
distorting the true relationships. Beyond measurement error, there are likely
complex synergistic and sometimes antagonistic interactions between different
dietary components, complicating the relationships between diet and health
outcomes. Flexible models are required to capture the nuance that these complex
interactions introduce. This complexity makes research on diet-health
relationships an appealing candidate for the application of machine learning
techniques, and in particular, neural networks. Neural networks are
computational models that are able to capture highly complex, nonlinear
relationships so long as sufficient data are available. While these models have
been applied in many domains, the impacts of measurement error on the
performance of predictive modeling has not been systematically investigated.
However, dietary intake data are typically collected using self-report methods
and are prone to large amounts of measurement error. In this work, we
demonstrate the ways in which measurement error erodes the performance of
neural networks, and illustrate the care that is required for leveraging these
models in the presence of error. We demonstrate the role that sample size and
replicate measurements play on model performance, indicate a motivation for the
investigation of transformations to additivity, and illustrate the caution
required to prevent model overfitting. While the past performance of neural
networks across various domains make them an attractive candidate for examining
diet-health relationships, our work demonstrates that substantial care and
further methodological development are both required to observe increased
predictive performance when applying these techniques, compared to more
traditional statistical procedures.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09342" title="Abstract">arXiv:2311.09342</a> [<a href="/pdf/2311.09342" title="Download PDF">pdf</a>, <a href="/format/2311.09342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Distinguishability of Anomalies as Physical Faults or Actuation  Cyberattacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Roy%2C+T">Tanushree Roy</a>, 
<a href="/search/eess?searchtype=author&query=Dey%2C+S">Satadru Dey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is currently submitted to ASME
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Increased automation has created an impetus to integrate infrastructure with
wide-spread connectivity in order to improve efficiency, sustainability,
autonomy, and security. Nonetheless, this reliance on connectivity and the
inevitability of complexity in this system increases the vulnerabilities to
physical faults or degradation and external cyber-threats. However, strategies
to counteract faults and cyberattacks would be widely different and thus it is
vital to not only detect but also to identify the nature of the anomaly that is
present in these systems. In this work, we propose a mathematical framework to
distinguish between physical faults and cyberattack using a sliding mode based
unknown input observer. Finally, we present simulation case studies to
distinguish between physical faults and cyberattacks using the proposed
Distinguishability metric and criterion. The simulation results show that the
proposed framework successfully distinguishes between faults and cyberattacks.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09343" title="Abstract">arXiv:2311.09343</a> [<a href="/pdf/2311.09343" title="Download PDF">pdf</a>, <a href="/ps/2311.09343" title="Download PostScript">ps</a>, <a href="/format/2311.09343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Well-being in isolation: Exploring artistic immersive virtual  environments in a simulated lunar habitat to alleviate asthenia symptoms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pochwatko%2C+G">Grzegorz Pochwatko</a>, 
<a href="/search/cs?searchtype=author&query=Kopec%2C+W">Wieslaw Kopec</a>, 
<a href="/search/cs?searchtype=author&query=Swidrak%2C+J">Justyna Swidrak</a>, 
<a href="/search/cs?searchtype=author&query=Jaskulska%2C+A">Anna Jaskulska</a>, 
<a href="/search/cs?searchtype=author&query=Skorupska%2C+K+H">Kinga H. Skorupska</a>, 
<a href="/search/cs?searchtype=author&query=Karpowicz%2C+B">Barbara Karpowicz</a>, 
<a href="/search/cs?searchtype=author&query=Mas%C5%82yk%2C+R">Rafa&#x142; Mas&#x142;yk</a>, 
<a href="/search/cs?searchtype=author&query=Grzeszczuk%2C+M">Maciej Grzeszczuk</a>, 
<a href="/search/cs?searchtype=author&query=Barnes%2C+S">Steven Barnes</a>, 
<a href="/search/cs?searchtype=author&query=Borkiewicz%2C+P">Paulina Borkiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Kobyli%C5%84ski%2C+P">Pawe&#x142; Kobyli&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Pabi%C5%9B-Orzeszyna%2C+M">Micha&#x142; Pabi&#x15b;-Orzeszyna</a>, 
<a href="/search/cs?searchtype=author&query=Balas%2C+R">Robert Balas</a>, 
<a href="/search/cs?searchtype=author&query=Lazarek%2C+J">Jagoda Lazarek</a>, 
<a href="/search/cs?searchtype=author&query=Dufresne%2C+F">Florian Dufresne</a>, 
<a href="/search/cs?searchtype=author&query=Bensch%2C+L">Leonie Bensch</a>, 
<a href="/search/cs?searchtype=author&query=Nilsson%2C+T">Tommy Nilsson</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE International Symposium on Mixed and
  Augmented Reality ISMAR, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Revived interest in lunar and planetary exploration is heralding a new era
for human spaceflight, characterized by frequent strain on astronaut's mental
well-being, which stems from increased exposure to isolated, confined, and
extreme (ICE) conditions. Whilst Immersive Virtual Reality (IVR) has been
employed to facilitate self-help interventions to mitigate challenges caused by
isolated environments in several domains, its applicability in support of
future space expeditions remains largely unexplored. To address this
limitation, we administered the use of distinct IVR environments to crew
members (n=5) partaking in a simulated lunar habitat study. Utilizing a
Bayesian approach to scrutinize small group data, we discovered a significant
relationship between IVR usage and a reduction in perceived stress-related
symptoms, particularly those associated with asthenia (syndrome often linked to
chronic fatigue and weakness; a condition characterized by feelings of energy
depletion or exhaustion that can be amplified in ICE conditions). The
reductions were most prominent with the use of interactive virtual
environments. The 'Aesthetic Realities' - virtual environments conceived as art
exhibits - received exceptional praise from our participants. These
environments mark a fascinating convergence of art and science, holding promise
to mitigate effects related to isolation in spaceflight training and beyond.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09344" title="Abstract">arXiv:2311.09344</a> [<a href="/pdf/2311.09344" title="Download PDF">pdf</a>, <a href="/format/2311.09344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language and Task Arithmetic with Parameter-Efficient Layers for  Zero-Shot Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chronopoulou%2C+A">Alexandra Chronopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Pfeiffer%2C+J">Jonas Pfeiffer</a>, 
<a href="/search/cs?searchtype=author&query=Maynez%2C+J">Joshua Maynez</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ruder%2C+S">Sebastian Ruder</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Priyanka Agrawal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Parameter-efficient fine-tuning (PEFT) using labeled task data can
significantly improve the performance of large language models (LLMs) on the
downstream task. However, there are 7000 languages in the world and many of
these languages lack labeled data for real-world language generation tasks. In
this paper, we propose to improve zero-shot cross-lingual transfer by composing
language or task specialized parameters. Our method composes language and task
PEFT modules via element-wise arithmetic operations to leverage unlabeled data
and English labeled data. We extend our approach to cases where labeled data
from more languages is available and propose to arithmetically compose PEFT
modules trained on languages related to the target. Empirical results on
summarization demonstrate that our method is an effective strategy that obtains
consistent gains using minimal training of PEFT modules.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09346" title="Abstract">arXiv:2311.09346</a> [<a href="/pdf/2311.09346" title="Download PDF">pdf</a>, <a href="/format/2311.09346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud  Registration Under Large Geometric and Temporal Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shengyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Savarese%2C+S">Silvio Savarese</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+K">Konrad Schindler</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>, 
<a href="/search/cs?searchtype=author&query=Armeni%2C+I">Iro Armeni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 29 figures. For the project page, see <a href="http://nothing-stands-still.com">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Building 3D geometric maps of man-made spaces is a well-established and
active field that is fundamental to computer vision and robotics. However,
considering the evolving nature of built environments, it is essential to
question the capabilities of current mapping efforts in handling temporal
changes. In addition, spatiotemporal mapping holds significant potential for
achieving sustainability and circularity goals. Existing mapping approaches
focus on small changes, such as object relocation or self-driving car
operation; in all cases where the main structure of the scene remains fixed.
Consequently, these approaches fail to address more radical changes in the
structure of the built environment, such as geometry and topology. To this end,
we introduce the Nothing Stands Still (NSS) benchmark, which focuses on the
spatiotemporal registration of 3D scenes undergoing large spatial and temporal
change, ultimately creating one coherent spatiotemporal map. Specifically, the
benchmark involves registering two or more partial 3D point clouds (fragments)
from the same scene but captured from different spatiotemporal views. In
addition to the standard pairwise registration, we assess the multi-way
registration of multiple fragments that belong to any temporal stage. As part
of NSS, we introduce a dataset of 3D point clouds recurrently captured in
large-scale building indoor environments that are under construction or
renovation. The NSS benchmark presents three scenarios of increasing
difficulty, to quantify the generalization ability of point cloud registration
methods over space (within one building and across buildings) and time. We
conduct extensive evaluations of state-of-the-art methods on NSS. The results
demonstrate the necessity for novel methods specifically designed to handle
large spatiotemporal changes. The homepage of our benchmark is at
<a href="http://nothing-stands-still.com.">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09348" title="Abstract">arXiv:2311.09348</a> [<a href="/pdf/2311.09348" title="Download PDF">pdf</a>, <a href="/format/2311.09348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Research Trends in Computer Science: A Network Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalhor%2C+G">Ghazal Kalhor</a>, 
<a href="/search/cs?searchtype=author&query=Bahrak%2C+B">Behnam Bahrak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Nowadays, computer science (CS) has emerged as a dominant force in numerous
research areas both within and beyond its own discipline. However, despite its
significant impact on scholarly space, only a limited number of studies have
been conducted to analyze the research trends and relationships within computer
science. In this study, we collected information on fields and subfields from
over 2,000 research articles published in the 2022 proceedings of the top
Association for Computing Machinery (ACM) conferences spanning various research
fields. Through a network approach, we investigated the interconnections
between CS fields and subfields to evaluate their interdisciplinarity and
multidisciplinarity. Our findings indicate that computing methodologies and
privacy and security stand out as the most interdisciplinary fields, while
human-centered computing exhibits the highest frequency among the papers.
Furthermore, we discovered that machine learning emerges as the most
interdisciplinary and multidisciplinary subfield within computer science. These
results offer valuable insights for universities seeking to foster
interdisciplinary research opportunities for their students.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09349" title="Abstract">arXiv:2311.09349</a> [<a href="/pdf/2311.09349" title="Download PDF">pdf</a>, <a href="/format/2311.09349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI-Based Probabilistic Constellation Shaping With Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Letafati%2C+M">Mehdi Letafati</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Samad Ali</a>, 
<a href="/search/cs?searchtype=author&query=Latva-aho%2C+M">Matti Latva-aho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2309.08688">arXiv:2309.08688</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion models are at the vanguard of generative AI research with renowned
solutions such as ImageGen by Google Brain and DALL.E 3 by OpenAI.
Nevertheless, the potential merits of diffusion models for communication
engineering applications are not fully understood yet. In this paper, we aim to
unleash the power of generative AI for PHY design of constellation symbols in
communication systems. Although the geometry of constellations is predetermined
according to networking standards, e.g., quadrature amplitude modulation (QAM),
probabilistic shaping can design the probability of occurrence (generation) of
constellation symbols. This can help improve the information rate and decoding
performance of communication systems. We exploit the ``denoise-and-generate''
characteristics of denoising diffusion probabilistic models (DDPM) for
probabilistic constellation shaping. The key idea is to learn generating
constellation symbols out of noise, ``mimicking'' the way the receiver performs
symbol reconstruction. This way, we make the constellation symbols sent by the
transmitter, and what is inferred (reconstructed) at the receiver become as
similar as possible, resulting in as few mismatches as possible. Our results
show that the generative AI-based scheme outperforms deep neural network
(DNN)-based benchmark and uniform shaping, while providing network resilience
as well as robust out-of-distribution performance under low-SNR regimes and
non-Gaussian assumptions. Numerical evaluations highlight 30% improvement in
terms of cosine similarity and a threefold improvement in terms of mutual
information compared to DNN-based approach for 64-QAM geometry.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09350" title="Abstract">arXiv:2311.09350</a> [<a href="/pdf/2311.09350" title="Download PDF">pdf</a>, <a href="/format/2311.09350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizable Imitation Learning Through Pre-Trained Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+W">Wei-Di Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hogan%2C+F">Francois Hogan</a>, 
<a href="/search/cs?searchtype=author&query=Meger%2C+D">David Meger</a>, 
<a href="/search/cs?searchtype=author&query=Dudek%2C+G">Gregory Dudek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper we leverage self-supervised vision transformer models and their
emergent semantic abilities to improve the generalization abilities of
imitation learning policies. We introduce BC-ViT, an imitation learning
algorithm that leverages rich DINO pre-trained Visual Transformer (ViT)
patch-level embeddings to obtain better generalization when learning through
demonstrations. Our learner sees the world by clustering appearance features
into semantic concepts, forming stable keypoints that generalize across a wide
range of appearance variations and object types. We show that this
representation enables generalized behaviour by evaluating imitation learning
across a diverse dataset of object manipulation tasks. Our method, data and
evaluation approach are made available to facilitate further study of
generalization in Imitation Learners.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09353" title="Abstract">arXiv:2311.09353</a> [<a href="/pdf/2311.09353" title="Download PDF">pdf</a>, <a href="/format/2311.09353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible and Adaptive Manufacturing by Complementing Knowledge  Representation, Reasoning and Planning with Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mayr%2C+M">Matthias Mayr</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+F">Faseeh Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+V">Volker Krueger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 2 figures. Presented at the IROS 2023 Workshop on Robotics &amp; AI in Future Factory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper describes a novel approach to adaptive manufacturing in the
context of small batch production and customization. It focuses on integrating
task-level planning and reasoning with reinforcement learning (RL) in the
SkiROS2 skill-based robot control platform. This integration enhances the
efficiency and adaptability of robotic systems in manufacturing, enabling them
to adjust to task variations and learn from interaction data. The paper
highlights the architecture of SkiROS2, particularly its world model, skill
libraries, and task management. It demonstrates how combining RL with robotic
manipulators can learn and improve the execution of industrial tasks. It
advocates a multi-objective learning model that eases the learning problem
design. The approach can incorporate user priors or previous experiences to
accelerate learning and increase safety.
<br />Spotlight video: https://youtu.be/H5PmZl2rRbs?si=8wmZ-gbwuSJRxe3S&amp;t=1422
<br />SkiROS2 code: https://github.com/RVMI/skiros2
<br />SkiROS2 talk at ROSCon: https://vimeo.<a href="/abs/com/8790018">com/8790018</a>25/2a0e9d5412
<br />SkiREIL code: https://github.com/matthias-mayr/SkiREIL
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09355" title="Abstract">arXiv:2311.09355</a> [<a href="/pdf/2311.09355" title="Download PDF">pdf</a>, <a href="/format/2311.09355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Threats in Stable Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cilloni%2C+T">Thomas Cilloni</a>, 
<a href="/search/cs?searchtype=author&query=Fleming%2C+C">Charles Fleming</a>, 
<a href="/search/cs?searchtype=author&query=Walter%2C+C">Charles Walter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a novel approach to membership inference attacks (MIA)
targeting stable diffusion computer vision models, specifically focusing on the
highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract
sensitive information about a model's training data, posing significant privacy
concerns. Despite its advancements in image synthesis, our research reveals
privacy vulnerabilities in the stable diffusion models' outputs. Exploiting
this information, we devise a black-box MIA that only needs to query the victim
model repeatedly. Our methodology involves observing the output of a stable
diffusion model at different generative epochs and training a classification
model to distinguish when a series of intermediates originated from a training
sample or not. We propose numerous ways to measure the membership features and
discuss what works best. The attack's efficacy is assessed using the ROC AUC
method, demonstrating a 60\% success rate in inferring membership information.
This paper contributes to the growing body of research on privacy and security
in machine learning, highlighting the need for robust defenses against MIAs.
Our findings prompt a reevaluation of the privacy implications of stable
diffusion models, urging practitioners and developers to implement enhanced
security measures to safeguard against such attacks.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09356" title="Abstract">arXiv:2311.09356</a> [<a href="/pdf/2311.09356" title="Download PDF">pdf</a>, <a href="/format/2311.09356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LePaRD: A Large-Scale Dataset of Judges Citing Precedents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahari%2C+R">Robert Mahari</a>, 
<a href="/search/cs?searchtype=author&query=Stammbach%2C+D">Dominik Stammbach</a>, 
<a href="/search/cs?searchtype=author&query=Ash%2C+E">Elliott Ash</a>, 
<a href="/search/cs?searchtype=author&query=Pentland%2C+A+%60">Alex `Sandy&#x27; Pentland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present the Legal Passage Retrieval Dataset LePaRD. LePaRD is a massive
collection of U.S. federal judicial citations to precedent in context. The
dataset aims to facilitate work on legal passage prediction, a challenging
practice-oriented legal retrieval and reasoning task. Legal passage prediction
seeks to predict relevant passages from precedential court decisions given the
context of a legal argument. We extensively evaluate various retrieval
approaches on LePaRD, and find that classification appears to work best.
However, we note that legal precedent prediction is a difficult task, and there
remains significant room for improvement. We hope that by publishing LePaRD, we
will encourage others to engage with a legal NLP task that promises to help
expand access to justice by reducing the burden associated with legal research.
A subset of the LePaRD dataset is freely available and the whole dataset will
be released upon publication.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09358" title="Abstract">arXiv:2311.09358</a> [<a href="/pdf/2311.09358" title="Download PDF">pdf</a>, <a href="/format/2311.09358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical evaluation of Uncertainty Quantification in  Retrieval-Augmented Language Models for Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wagle%2C+S">Sridevi Wagle</a>, 
<a href="/search/cs?searchtype=author&query=Munikoti%2C+S">Sai Munikoti</a>, 
<a href="/search/cs?searchtype=author&query=Acharya%2C+A">Anurag Acharya</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+S">Sara Smith</a>, 
<a href="/search/cs?searchtype=author&query=Horawalavithana%2C+S">Sameera Horawalavithana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have shown remarkable achievements in natural
language processing tasks, producing high-quality outputs. However, LLMs still
exhibit limitations, including the generation of factually incorrect
information. In safety-critical applications, it is important to assess the
confidence of LLM-generated content to make informed decisions. Retrieval
Augmented Language Models (RALMs) is relatively a new area of research in NLP.
RALMs offer potential benefits for scientific NLP tasks, as retrieved
documents, can serve as evidence to support model-generated content. This
inclusion of evidence enhances trustworthiness, as users can verify and explore
the retrieved documents to validate model outputs. Quantifying uncertainty in
RALM generations further improves trustworthiness, with retrieved text and
confidence scores contributing to a comprehensive and reliable model for
scientific applications. However, there is limited to no research on UQ for
RALMs, particularly in scientific contexts. This study aims to address this gap
by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific
tasks. This research investigates how uncertainty scores vary when scientific
knowledge is incorporated as pretraining and retrieval data and explores the
relationship between uncertainty scores and the accuracy of model-generated
outputs. We observe that an existing RALM finetuned with scientific knowledge
as the retrieval data tends to be more confident in generating predictions
compared to the model pretrained only with scientific knowledge. We also found
that RALMs are overconfident in their predictions, making inaccurate
predictions more confidently than accurate ones. Scientific knowledge provided
either as pretraining or retrieval corpus does not help alleviate this issue.
We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09359" title="Abstract">arXiv:2311.09359</a> [<a href="/pdf/2311.09359" title="Download PDF">pdf</a>, <a href="/format/2311.09359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Computation Algorithms for Maximum Matching: New Lower Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behnezhad%2C+S">Soheil Behnezhad</a>, 
<a href="/search/cs?searchtype=author&query=Roghani%2C+M">Mohammad Roghani</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+A">Aviad Rubinstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study local computation algorithms (LCA) for maximum matching. An LCA does
not return its output entirely, but reveals parts of it upon query. For
matchings, each query is a vertex $v$; the LCA should return whether $v$ is
matched -- and if so to which neighbor -- while spending a small time per
query.
<br />In this paper, we prove that any LCA that computes a matching that is at most
an additive of $\epsilon n$ smaller than the maximum matching in $n$-vertex
graphs of maximum degree $\Delta$ must take at least
$\Delta^{\Omega(1/\varepsilon)}$ time. This comes close to the existing upper
bounds that take $(\Delta/\epsilon)^{O(1/\epsilon^2)} polylog(n)$ time.
<br />In terms of sublinear time algorithms, our techniques imply that any
algorithm that estimates the size of maximum matching up to an additive error
of $\epsilon n$ must take $\Delta^{\Omega(1/\epsilon)}$ time. This negatively
resolves a decade old open problem of the area (see Open Problem 39 of
sublinear.info) on whether such estimates can be achieved in
$poly(\Delta/\epsilon)$ time.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09361" title="Abstract">arXiv:2311.09361</a> [<a href="/pdf/2311.09361" title="Download PDF">pdf</a>, <a href="/format/2311.09361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination  Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gardner%2C+J+A+D">James A. D. Gardner</a>, 
<a href="/search/cs?searchtype=author&query=Egger%2C+B">Bernhard Egger</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+W+A+P">William A. P. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Repo - <a href="https://github.com/JADGardner/ns_reni.">this https URL</a> arXiv admin note: substantial text overlap with <a href="/abs/2206.03858">arXiv:2206.03858</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Inverse rendering is an ill-posed problem. Previous work has sought to
resolve this by focussing on priors for object or scene shape or appearance. In
this work, we instead focus on a prior for natural illuminations. Current
methods rely on spherical harmonic lighting or other generic representations
and, at best, a simplistic prior on the parameters. This results in limitations
for the inverse setting in terms of the expressivity of the illumination
conditions, especially when taking specular reflections into account. We
propose a conditional neural field representation based on a variational
auto-decoder and a transformer decoder. We extend Vector Neurons to build
equivariance directly into our architecture, and leveraging insights from depth
estimation through a scale-invariant loss function, we enable the accurate
representation of High Dynamic Range (HDR) images. The result is a compact,
rotation-equivariant HDR neural illumination model capable of capturing
complex, high-frequency features in natural environment maps. Training our
model on a curated dataset of 1.6K HDR environment maps of natural scenes, we
compare it against traditional representations, demonstrate its applicability
for an inverse rendering task and show environment map completion from partial
observations. We share our PyTorch implementation, dataset and trained models
at https://github.com/JADGardner/ns_reni
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09363" title="Abstract">arXiv:2311.09363</a> [<a href="/pdf/2311.09363" title="Download PDF">pdf</a>, <a href="/format/2311.09363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Emergent Audio Classification Ability of ASR  Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Rao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liusie%2C+A">Adian Liusie</a>, 
<a href="/search/cs?searchtype=author&query=Gales%2C+M+J+F">Mark J. F. Gales</a>, 
<a href="/search/cs?searchtype=author&query=Knill%2C+K+M">Kate M. Knill</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text and vision foundation models can perform many tasks in a zero-shot
setting, a desirable property that enables these systems to be applied in
general and low-resource settings. However, there has been significantly less
work on the zero-shot abilities of ASR foundation models, with these systems
typically fine-tuned to specific tasks or constrained to applications that
match their training criterion and data annotation. In this work we investigate
the ability of Whisper and MMS, ASR foundation models trained primarily for
speech recognition, to perform zero-shot audio classification. We use simple
template-based text prompts at the decoder and use the resulting decoding
probabilities to generate zero-shot predictions. Without training the model on
extra data or adding any new parameters, we demonstrate that Whisper shows
promising zero-shot classification performance on a range of 8
audio-classification datasets, outperforming existing state-of-the-art
zero-shot baseline's accuracy by an average of 9%. One important step to unlock
the emergent ability is debiasing, where a simple unsupervised reweighting
method of the class probabilities yields consistent significant performance
gains. We further show that performance increases with model size, implying
that as ASR foundation models scale up, they may exhibit improved zero-shot
performance.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09366" title="Abstract">arXiv:2311.09366</a> [<a href="/pdf/2311.09366" title="Download PDF">pdf</a>, <a href="/format/2311.09366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph  Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCusker%2C+J">Jamie McCusker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While the potential of Open Information Extraction (Open IE) for Knowledge
Graph Construction (KGC) may seem promising, we find that the alignment of Open
IE extraction results with existing knowledge graphs to be inadequate. The
advent of Large Language Models (LLMs), especially the commercially available
OpenAI models, have reset expectations for what is possible with deep learning
models and have created a new field called prompt engineering. We investigate
the use of GPT models and prompt engineering for knowledge graph construction
with the Wikidata knowledge graph to address a similar problem to Open IE,
which we call Open Knowledge Extraction (OKE) using an approach we call the
Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the
entity linking task essential to construction of real world knowledge graphs.
We merge the CaRB benchmark scoring approach with data from the TekGen dataset
for the LOKE task. We then show that a well engineered prompt, paired with a
naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's
OpenIE 4 implementation on the OKE task, although it over-generates triples
compared to the reference set due to overall triple scarcity in the TekGen set.
Through an analysis of entity linkability in the CaRB dataset, as well as
outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver"
TekGen triples show that the task is significantly different in content from
OIE, if not structure. Through this analysis and a qualitative analysis of
sentence extractions via all methods, we found that LOKE-GPT extractions are of
high utility for the KGC task and suitable for use in semi-automated extraction
settings.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09367" title="Abstract">arXiv:2311.09367</a> [<a href="/pdf/2311.09367" title="Download PDF">pdf</a>, <a href="/format/2311.09367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Online User Aggression: Content Detection and Behavioural  Analysis on Social Media Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mane%2C+S">Swapnil Mane</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Suman Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rajesh Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The rise of social media platforms has led to an increase in cyber-aggressive
behavior, encompassing a broad spectrum of hostile behavior, including
cyberbullying, online harassment, and the dissemination of offensive and hate
speech. These behaviors have been associated with significant societal
consequences, ranging from online anonymity to real-world outcomes such as
depression, suicidal tendencies, and, in some instances, offline violence.
Recognizing the societal risks associated with unchecked aggressive content,
this paper delves into the field of Aggression Content Detection and Behavioral
Analysis of Aggressive Users, aiming to bridge the gap between disparate
studies. In this paper, we analyzed the diversity of definitions and proposed a
unified cyber-aggression definition. We examine the comprehensive process of
Aggression Content Detection, spanning from dataset creation, feature selection
and extraction, and detection algorithm development. Further, we review studies
on Behavioral Analysis of Aggression that explore the influencing factors,
consequences, and patterns associated with cyber-aggressive behavior. This
systematic literature review is a cross-examination of content detection and
behavioral analysis in the realm of cyber-aggression. The integrated
investigation reveals the effectiveness of incorporating sociological insights
into computational techniques for preventing cyber-aggressive behavior.
Finally, the paper concludes by identifying research gaps and encouraging
further progress in the unified domain of socio-computational aggressive
behavior analysis.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09370" title="Abstract">arXiv:2311.09370</a> [<a href="/pdf/2311.09370" title="Download PDF">pdf</a>, <a href="/ps/2311.09370" title="Download PostScript">ps</a>, <a href="/format/2311.09370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On correlation bounds against polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ivanov%2C+P">Peter Ivanov</a>, 
<a href="/search/cs?searchtype=author&query=Pavlovic%2C+L">Liam Pavlovic</a>, 
<a href="/search/cs?searchtype=author&query=Viola%2C+E">Emanuele Viola</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CCC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">We study the fundamental challenge of exhibiting explicit functions that have
small correlation with low-degree polynomials over $\mathbb{F}_{2}$. Our main
contributions include:
<br />1. In STOC 2020, CHHLZ introduced a new technique to prove correlation
bounds. Using their technique they established new correlation bounds for
low-degree polynomials. They conjectured that their technique generalizes to
higher degree polynomials as well. We give a counterexample to their
conjecture, in fact ruling out weaker parameters and showing what they prove is
essentially the best possible.
<br />2. We propose a new approach for proving correlation bounds with the central
"mod functions", consisting of two steps: (I) the polynomials that maximize
correlation are symmetric and (II) symmetric polynomials have small
correlation. Contrary to related results in the literature, we conjecture that
(I) is true. We argue this approach is not affected by existing "barrier
results".
<br />3. We prove our conjecture for quadratic polynomials. Specifically, we
determine the maximum possible correlation between quadratic polynomials modulo
2 and the functions $(x_{1},\dots,x_{n})\to z^{\sum x_{i}}$ for any $z$ on the
complex unit circle; and show that it is achieved by symmetric polynomials. To
obtain our results we develop a new proof technique: we express correlation in
terms of directional derivatives and analyze it by slowly restricting the
direction.
<br />4. We make partial progress on the conjecture for cubic polynomials, in
particular proving tight correlation bounds for cubic polynomials whose
degree-3 part is symmetric.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09376" title="Abstract">arXiv:2311.09376</a> [<a href="/pdf/2311.09376" title="Download PDF">pdf</a>, <a href="/format/2311.09376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DISTA: Denoising Spiking Transformer with intrinsic plasticity and  spatiotemporal attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Boxun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+H">Hejia Geng</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yuxuan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Among the array of neural network architectures, the Vision Transformer (ViT)
stands out as a prominent choice, acclaimed for its exceptional expressiveness
and consistent high performance in various vision applications. Recently, the
emerging Spiking ViT approach has endeavored to harness spiking neurons, paving
the way for a more brain-inspired transformer architecture that thrives in
ultra-low power operations on dedicated neuromorphic hardware. Nevertheless,
this approach remains confined to spatial self-attention and doesn't fully
unlock the potential of spiking neural networks. We introduce DISTA, a
Denoising Spiking Transformer with Intrinsic Plasticity and SpatioTemporal
Attention, designed to maximize the spatiotemporal computational prowess of
spiking neurons, particularly for vision applications. DISTA explores two types
of spatiotemporal attentions: intrinsic neuron-level attention and
network-level attention with explicit memory. Additionally, DISTA incorporates
an efficient nonlinear denoising mechanism to quell the noise inherent in
computed spatiotemporal attention maps, thereby resulting in further
performance gains. Our DISTA transformer undergoes joint training involving
synaptic plasticity (i.e., weight tuning) and intrinsic plasticity (i.e.,
membrane time constant tuning) and delivers state-of-the-art performances
across several static image and dynamic neuromorphic datasets. With only 6 time
steps, DISTA achieves remarkable top-1 accuracy on CIFAR10 (96.26%) and
CIFAR100 (79.15%), as well as 79.1% on CIFAR10-DVS using 10 time steps.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09378" title="Abstract">arXiv:2311.09378</a> [<a href="/pdf/2311.09378" title="Download PDF">pdf</a>, <a href="/format/2311.09378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Predicts Interpersonal Affect? Preliminary Analyses from  Retrospective Evaluations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parreira%2C+M+T">Maria Teresa Parreira</a>, 
<a href="/search/cs?searchtype=author&query=Sack%2C+M+J">Michael J. Sack</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+M">Malte Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Late-Breaking Report in 2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN). arXiv admin note: text overlap with <a href="/abs/2306.16629">arXiv:2306.16629</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">While the field of affective computing has contributed to greatly improving
the seamlessness of human-robot interactions, the focus has primarily been on
the emotional processing of the self, rather than the perception of the other.
To address this gap, in a user study with 30 participant dyads, we collected
the users' retrospective ratings of the interpersonal perception of the other
interactant, after a short interaction. We made use of CORAE, a novel web-based
open-source tool for COntinuous Retrospective Affect Evaluation. In this work,
we analyze how these interpersonal ratings correlate with different aspects of
the interaction, namely personality traits, participation balance, and
sentiment analysis. Notably, we discovered that conversational imbalance has a
significant effect on the retrospective ratings, among other findings. By
employing these analyses and methodologies, we lay the groundwork for enhanced
human-robot interactions, wherein affect is understood as a highly dynamic and
context-dependent outcome of interaction history.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09379" title="Abstract">arXiv:2311.09379</a> [<a href="/pdf/2311.09379" title="Download PDF">pdf</a>, <a href="/format/2311.09379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Characteristic Mapping Method for Vlasov-Poisson with Extreme  Resolution Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Krah%2C+P">Philipp Krah</a>, 
<a href="/search/math?searchtype=author&query=Yin%2C+X">Xi-Yuan Yin</a>, 
<a href="/search/math?searchtype=author&query=Bergmann%2C+J">Julius Bergmann</a>, 
<a href="/search/math?searchtype=author&query=Nave%2C+J">Jean-Christophe Nave</a>, 
<a href="/search/math?searchtype=author&query=Schneider%2C+K">Kai Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint has not been reviewed yet. Code available: <a href="https://github.com/orgs/CharacteristicMappingMethod/repositories">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph); Plasma Physics (physics.plasm-ph)

</div>
<p class="mathjax">We propose an efficient semi-Lagrangian characteristic mapping method for
solving the one+one-dimensional Vlasov-Poisson equations with high precision on
a coarse grid. The flow map is evolved numerically and exponential resolution
in linear time is obtained. Global third-order convergence in space and time is
shown and conservation properties are assessed. For benchmarking, we consider
linear and nonlinear Landau damping and the two-stream instability. We compare
the results with a Fourier pseudo-spectral method. The extreme fine-scale
resolution features are illustrated showing the method's capabilities to
efficiently treat filamentation in fusion plasma simulations.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09383" title="Abstract">arXiv:2311.09383</a> [<a href="/pdf/2311.09383" title="Download PDF">pdf</a>, <a href="/format/2311.09383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-form Question Answering: An Iterative Planning-Retrieval-Generation  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akash%2C+P+S">Pritom Saha Akash</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K+K">Kashob Kumar Roy</a>, 
<a href="/search/cs?searchtype=author&query=Popa%2C+L">Lucian Popa</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Long-form question answering (LFQA) poses a challenge as it involves
generating detailed answers in the form of paragraphs, which go beyond simple
yes/no responses or short factual answers. While existing QA models excel in
questions with concise answers, LFQA requires handling multiple topics and
their intricate relationships, demanding comprehensive explanations. Previous
attempts at LFQA focused on generating long-form answers by utilizing relevant
contexts from a corpus, relying solely on the question itself. However, they
overlooked the possibility that the question alone might not provide sufficient
information to identify the relevant contexts. Additionally, generating
detailed long-form answers often entails aggregating knowledge from diverse
sources. To address these limitations, we propose an LFQA model with iterative
Planning, Retrieval, and Generation. This iterative process continues until a
complete answer is generated for the given question. From an extensive
experiment on both an open domain and a technical domain QA dataset, we find
that our model outperforms the state-of-the-art models on various textual and
factual metrics for the LFQA task.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09386" title="Abstract">arXiv:2311.09386</a> [<a href="/pdf/2311.09386" title="Download PDF">pdf</a>, <a href="/format/2311.09386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yaghooti%2C+B">Bahram Yaghooti</a>, 
<a href="/search/cs?searchtype=author&query=Raviv%2C+N">Netanel Raviv</a>, 
<a href="/search/cs?searchtype=author&query=Sinopoli%2C+B">Bruno Sinopoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Linear feature extraction at the presence of nonlinear dependencies among the
data is a fundamental challenge in unsupervised learning. We propose using a
Probabilistic Gram-Schmidt (PGS) type orthogonalization process in order to
detect and map out redundant dimensions. Specifically, by applying the PGS
process over any family of functions which presumably captures the nonlinear
dependencies in the data, we construct a series of covariance matrices that can
either be used to remove those dependencies from the principal components, or
to identify new large-variance directions. In the former case, we prove that
under certain assumptions the resulting algorithms detect and remove nonlinear
dependencies whenever those dependencies lie in the linear span of the chosen
function family. In the latter, we provide information-theoretic guarantees in
terms of entropy reduction. Both proposed methods extract linear features from
the data while removing nonlinear redundancies. We provide simulation results
on synthetic and real-world datasets which show improved performance over PCA
and state-of-the-art linear feature extraction algorithms, both in terms of
variance maximization of the extracted features, and in terms of improved
performance of classification algorithms.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09387" title="Abstract">arXiv:2311.09387</a> [<a href="/pdf/2311.09387" title="Download PDF">pdf</a>, <a href="/format/2311.09387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Banach-Tarski Embeddings and Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maher%2C+J">Joshua Maher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We introduce a new construction of embeddings of arbitrary recursive data
structures into high dimensional vectors. These embeddings provide an
interpretable model for the latent state vectors of transformers. We
demonstrate that these embeddings can be decoded to the original data structure
when the embedding dimension is sufficiently large. This decoding algorithm has
a natural implementation as a transformer. We also show that these embedding
vectors can be manipulated directly to perform computations on the underlying
data without decoding. As an example we present an algorithm that constructs
the embedded parse tree of an embedded token sequence using only vector
operations in embedding space.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09389" title="Abstract">arXiv:2311.09389</a> [<a href="/pdf/2311.09389" title="Download PDF">pdf</a>, <a href="/format/2311.09389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural machine translation for automated feedback on children&#x27;s  early-stage writing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jensen%2C+J+V">Jonas Vestergaard Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Jordahn%2C+M">Mikkel Jordahn</a>, 
<a href="/search/cs?searchtype=author&query=Andersen%2C+M+R">Michael Riis Andersen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure, 1 table, to be published in the proceedings of the Northern Lights Deep Learning Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we address the problem of assessing and constructing feedback
for early-stage writing automatically using machine learning. Early-stage
writing is typically vastly different from conventional writing due to phonetic
spelling and lack of proper grammar, punctuation, spacing etc. Consequently,
early-stage writing is highly non-trivial to analyze using common linguistic
metrics. We propose to use sequence-to-sequence models for "translating"
early-stage writing by students into "conventional" writing, which allows the
translated text to be analyzed using linguistic metrics. Furthermore, we
propose a novel robust likelihood to mitigate the effect of noise in the
dataset. We investigate the proposed methods using a set of numerical
experiments and demonstrate that the conventional text can be predicted with
high accuracy.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09390" title="Abstract">arXiv:2311.09390</a> [<a href="/pdf/2311.09390" title="Download PDF">pdf</a>, <a href="/format/2311.09390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Nalin Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Du%C5%A1ek%2C+O">Ond&#x159;ej Du&#x161;ek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Linguistic entrainment, or alignment, represents a phenomenon where
linguistic patterns employed by conversational participants converge to one
another. While alignment has been shown to produce a more natural user
experience, most dialogue systems do not have any provisions for it. In this
work, we introduce methods for achieving dialogue alignment in a GPT-2-based
end-to-end dialogue system through the utilization of shared vocabulary. We
experiment with training instance weighting, alignment-specific loss, and
additional conditioning to generate responses that align with the user. By
comparing different entrainment techniques on the MultiWOZ dataset, we
demonstrate that all three approaches produce significantly better-aligned
results than the baseline, as confirmed by both automated and manual evaluation
metrics.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09393" title="Abstract">arXiv:2311.09393</a> [<a href="/pdf/2311.09393" title="Download PDF">pdf</a>, <a href="/format/2311.09393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taypsi: Static Enforcement of Privacy Policies for Policy-Agnostic  Oblivious Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qianchuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Delaware%2C+B">Benjamin Delaware</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Secure multiparty computation (MPC) techniques enable multiple parties to
compute joint functions over their private data without sharing that data to
other parties, typically by employing powerful cryptographic protocols to
protect individual's data. One challenge when writing such functions is that
most MPC languages force users to intermix programmatic and privacy concerns in
a single application, making it difficult to change or audit a program's
underlying privacy policy. Existing policy-agnostic MPC languages rely on
run-time / dynamic enforcement to decouple privacy requirements from program
logic. Unfortunately, the resulting overhead makes it difficult to scale MPC
applications that manipulate structured data. This work proposes to eliminate
this overhead by instead transforming programs to semantically equivalent
versions that statically enforce user-provided privacy policies. We have
implemented this approach in a new MPC language, called Taypsi; our
experimental evaluation demonstrates that the resulting system features
considerable performance improvements on a variety of MPC applications
involving structured data and complex privacy polices.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09394" title="Abstract">arXiv:2311.09394</a> [<a href="/pdf/2311.09394" title="Download PDF">pdf</a>, <a href="/format/2311.09394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GWP-ASan: Sampling-Based Detection of Memory-Safety Bugs in Production
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Serebryany%2C+K">Kostya Serebryany</a>, 
<a href="/search/cs?searchtype=author&query=Kennelly%2C+C">Chris Kennelly</a>, 
<a href="/search/cs?searchtype=author&query=Phillips%2C+M">Mitch Phillips</a>, 
<a href="/search/cs?searchtype=author&query=Denton%2C+M">Matt Denton</a>, 
<a href="/search/cs?searchtype=author&query=Elver%2C+M">Marco Elver</a>, 
<a href="/search/cs?searchtype=author&query=Potapenko%2C+A">Alexander Potapenko</a>, 
<a href="/search/cs?searchtype=author&query=Morehouse%2C+M">Matt Morehouse</a>, 
<a href="/search/cs?searchtype=author&query=Tsyrklevich%2C+V">Vlad Tsyrklevich</a>, 
<a href="/search/cs?searchtype=author&query=Holler%2C+C">Christian Holler</a>, 
<a href="/search/cs?searchtype=author&query=Lettner%2C+J">Julian Lettner</a>, 
<a href="/search/cs?searchtype=author&query=Kilzer%2C+D">David Kilzer</a>, 
<a href="/search/cs?searchtype=author&query=Brandt%2C+L">Lander Brandt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Despite the recent advances in pre-production bug detection,
heap-use-after-free and heap-buffer-overflow bugs remain the primary problem
for security, reliability, and developer productivity for applications written
in C or C++, across all major software ecosystems. Memory-safe languages solve
this problem when they are used, but the existing code bases consisting of
billions of lines of C and C++ continue to grow, and we need additional bug
detection mechanisms.
<br />This paper describes a family of tools that detect these two classes of
memory-safety bugs, while running in production, at near-zero overhead. These
tools combine page-granular guarded allocation and low-rate sampling. In other
words, we added an "if" statement to a 36-year-old idea and made it work at
scale.
<br />We describe the basic algorithm, several of its variants and implementations,
and the results of multi-year deployments across mobile, desktop, and server
applications.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09401" title="Abstract">arXiv:2311.09401</a> [<a href="/pdf/2311.09401" title="Download PDF">pdf</a>, <a href="/format/2311.09401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoCo-Transfer: Investigating out-of-distribution contrastive learning  for limited-data domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuwen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Helen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Medical imaging data is often siloed within hospitals, limiting the amount of
data available for specialized model development. With limited in-domain data,
one might hope to leverage larger datasets from related domains. In this paper,
we analyze the benefit of transferring self-supervised contrastive
representations from moment contrast (MoCo) pretraining on out-of-distribution
data to settings with limited data. We consider two X-ray datasets which image
different parts of the body, and compare transferring from each other to
transferring from ImageNet. We find that depending on quantity of labeled and
unlabeled data, contrastive pretraining on larger out-of-distribution datasets
can perform nearly as well or better than MoCo pretraining in-domain, and
pretraining on related domains leads to higher performance than if one were to
use the ImageNet pretrained weights. Finally, we provide a preliminary way of
quantifying similarity between datasets.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09402" title="Abstract">arXiv:2311.09402</a> [<a href="/pdf/2311.09402" title="Download PDF">pdf</a>, <a href="/format/2311.09402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetically Enhanced: Unveiling Synthetic Data&#x27;s Potential in Medical  Imaging Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khosravi%2C+B">Bardia Khosravi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Frank Li</a>, 
<a href="/search/cs?searchtype=author&query=Dapamede%2C+T">Theo Dapamede</a>, 
<a href="/search/cs?searchtype=author&query=Rouzrokh%2C+P">Pouria Rouzrokh</a>, 
<a href="/search/cs?searchtype=author&query=Gamble%2C+C+U">Cooper U. Gamble</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+H+M">Hari M. Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Wyles%2C+C+C">Cody C. Wyles</a>, 
<a href="/search/cs?searchtype=author&query=Sellergren%2C+A+B">Andrew B. Sellergren</a>, 
<a href="/search/cs?searchtype=author&query=Purkayastha%2C+S">Saptarshi Purkayastha</a>, 
<a href="/search/cs?searchtype=author&query=Erickson%2C+B+J">Bradley J. Erickson</a>, 
<a href="/search/cs?searchtype=author&query=Gichoya%2C+J+W">Judy W. Gichoya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Chest X-rays (CXR) are the most common medical imaging study and are used to
diagnose multiple medical conditions. This study examines the impact of
synthetic data supplementation, using diffusion models, on the performance of
deep learning (DL) classifiers for CXR analysis. We employed three datasets:
CheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising
diffusion probabilistic models (DDPMs) to generate synthetic frontal
radiographs. Our approach ensured that synthetic images mirrored the
demographic and pathological traits of the original data. Evaluating the
classifiers' performance on internal and external datasets revealed that
synthetic data supplementation enhances model accuracy, particularly in
detecting less prevalent pathologies. Furthermore, models trained on synthetic
data alone approached the performance of those trained on real data. This
suggests that synthetic data can potentially compensate for real data shortages
in training robust DL models. However, despite promising outcomes, the
superiority of real data persists.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09404" title="Abstract">arXiv:2311.09404</a> [<a href="/pdf/2311.09404" title="Download PDF">pdf</a>, <a href="/format/2311.09404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To Translate or Not to Translate: A Systematic Investigation of  Translation-Based Cross-Lingual Transfer to Low-Resource Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ebing%2C+B">Benedikt Ebing</a>, 
<a href="/search/cs?searchtype=author&query=Glava%C5%A1%2C+G">Goran Glava&#x161;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Perfect machine translation (MT) would render cross-lingual transfer (XLT) by
means of multilingual language models (LMs) superfluous. Given, on one hand,
the large body of work on improving XLT with multilingual LMs and, on the other
hand, recent advances in massively multilingual MT, in this work, we
systematically evaluate existing and propose new translation-based XLT
approaches for transfer to low-resource languages. We show that all
translation-based approaches dramatically outperform zero-shot XLT with
multilingual LMs, rendering the approach that combines the round-trip
translation of the source-language training data with the translation of the
target-language test instances the most effective. We next show that one can
obtain further empirical gains by adding reliable translations to other
high-resource languages to the training data. Moreover, we propose an effective
translation-based XLT strategy even for languages not supported by the MT
system. Finally, we show that model selection for XLT based on target-language
validation data obtained with MT outperforms model selection based on the
source-language data. We hope that our findings encourage adoption of more
robust translation-based baselines in XLT research.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09406" title="Abstract">arXiv:2311.09406</a> [<a href="/pdf/2311.09406" title="Download PDF">pdf</a>, <a href="/format/2311.09406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alternatives to the Scaled Dot Product for Attention in the Transformer  Neural Network Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernhard%2C+J">James Bernhard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">The transformer neural network architecture uses a form of attention in which
the dot product of query and key is divided by the square root of the key
dimension before applying softmax. This scaling of the dot product is designed
to avoid the absolute value of the dot products becoming so large that applying
softmax leads to vanishing gradients. In this paper, we propose some
alternative scalings, including dividing the dot product instead by the sum of
the key lengths before applying softmax. We use simulated keys and queries to
show that in many situations this appears to be more effective at avoiding
regions where applying softmax leads to vanishing gradients.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09410" title="Abstract">arXiv:2311.09410</a> [<a href="/pdf/2311.09410" title="Download PDF">pdf</a>, <a href="/format/2311.09410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Large Language Models contradict humans? Large Language Models&#x27;  Sycophantic Behaviour
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ranaldi%2C+L">Leonardo Ranaldi</a>, 
<a href="/search/cs?searchtype=author&query=Pucci%2C+G">Giulia Pucci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have been demonstrating the ability to solve
complex tasks by delivering answers that are positively evaluated by humans due
in part to the intensive use of human feedback that refines responses. However,
the suggestibility transmitted through human feedback increases the inclination
to produce responses that correspond to the user's beliefs or misleading
prompts as opposed to true facts, a behaviour known as sycophancy. This
phenomenon decreases the bias, robustness, and, consequently, their
reliability.
<br />In this paper, we shed light on the suggestibility of LLMs to sycophantic
behaviour, demonstrating these tendencies via human-influenced prompts over
different tasks. Our investigation reveals that LLMs show sycophantic
tendencies when responding to queries involving subjective opinions and
statements that should elicit a contrary response based on facts, demonstrating
a lack of robustness.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09422" title="Abstract">arXiv:2311.09422</a> [<a href="/pdf/2311.09422" title="Download PDF">pdf</a>, <a href="/format/2311.09422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting generalization performance with correctness discriminators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuekun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Koller%2C+A">Alexander Koller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The ability to predict an NLP model's accuracy on unseen, potentially
out-of-distribution data is a prerequisite for trustworthiness. We present a
novel model that establishes upper and lower bounds on the accuracy, without
requiring gold labels for the unseen data. We achieve this by training a
discriminator which predicts whether the output of a given sequence-to-sequence
model is correct or not. We show across a variety of tagging, parsing, and
semantic parsing tasks that the gold accuracy is reliably between the predicted
upper and lower bounds, and that these bounds are remarkably close together.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09424" title="Abstract">arXiv:2311.09424</a> [<a href="/pdf/2311.09424" title="Download PDF">pdf</a>, <a href="/format/2311.09424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Spine Geometry and Scoliosis from DXA Scans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jamaludin%2C+A">Amir Jamaludin</a>, 
<a href="/search/cs?searchtype=author&query=Kadir%2C+T">Timor Kadir</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+E">Emma Clark</a>, 
<a href="/search/cs?searchtype=author&query=Zisserman%2C+A">Andrew Zisserman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CSI@MICCAI 2019 Submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Our objective in this paper is to estimate spine curvature in DXA scans. To
this end we first train a neural network to predict the middle spine curve in
the scan, and then use an integral-based method to determine the curvature
along the spine curve. We use the curvature to compare to the standard angle
scoliosis measure obtained using the DXA Scoliosis Method (DSM). The
performance improves over the prior work of Jamaludin et al. 2018. We show that
the maximum curvature can be used as a scoring function for ordering the
severity of spinal deformation.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09425" title="Abstract">arXiv:2311.09425</a> [<a href="/pdf/2311.09425" title="Download PDF">pdf</a>, <a href="/format/2311.09425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust and conservative dynamical low-rank methods for the Vlasov  equation via a novel macro-micro decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Coughlin%2C+J">Jack Coughlin</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+J">Jingwei Hu</a>, 
<a href="/search/math?searchtype=author&query=Shumlak%2C+U">Uri Shumlak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Dynamical low-rank (DLR) approximation has gained interest in recent years as
a viable solution to the curse of dimensionality in the numerical solution of
kinetic equations including the Boltzmann and Vlasov equations. These methods
include the projector-splitting and Basis-update &amp; Galerkin DLR integrators,
and have shown promise at greatly improving the computational efficiency of
kinetic solutions. However, this often comes at the cost of conservation of
charge, current and energy. In this work we show how a novel macro-micro
decomposition may be used to separate the distribution function into two
components, one of which carries the conserved quantities, and the other of
which is orthogonal to them. We apply DLR approximation to the latter, and
thereby achieve a clean and extensible approach to a conservative DLR scheme
which retains the computational advantages of the base scheme. Moreover, our
decomposition is compatible with the projector-splitting integrator, and can
therefore access second-order accuracy in time via a Strang splitting scheme.
We describe a first-order integrator which can exactly conserve charge and
either current or energy, as well as a second-order accurate integrator which
exactly conserves charge and energy. To highlight the flexibility of the
proposed macro-micro decomposition, we implement a pair of velocity space
discretizations, and verify the claimed accuracy and conservation properties on
a suite of plasma benchmark problems.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09428" title="Abstract">arXiv:2311.09428</a> [<a href="/pdf/2311.09428" title="Download PDF">pdf</a>, <a href="/format/2311.09428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yueqing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Payani%2C+A">Ali Payani</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+K">Kai Shu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">This work investigates the potential of undermining both fairness and
detection performance in abusive language detection. In a dynamic and complex
digital world, it is crucial to investigate the vulnerabilities of these
detection models to adversarial fairness attacks to improve their fairness
robustness. We propose a simple yet effective framework FABLE that leverages
backdoor attacks as they allow targeted control over the fairness and detection
performance. FABLE explores three types of trigger designs (i.e., rare,
artificial, and natural triggers) and novel sampling strategies. Specifically,
the adversary can inject triggers into samples in the minority group with the
favored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored
outcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the
effectiveness of FABLE attacking fairness and utility in abusive language
detection.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09431" title="Abstract">arXiv:2311.09431</a> [<a href="/pdf/2311.09431" title="Download PDF">pdf</a>, <a href="/format/2311.09431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Striped Attention: Faster Ring Attention for Causal Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brandon%2C+W">William Brandon</a>, 
<a href="/search/cs?searchtype=author&query=Nrusimha%2C+A">Aniruddha Nrusimha</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kevin Qian</a>, 
<a href="/search/cs?searchtype=author&query=Ankner%2C+Z">Zachary Ankner</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+T">Tian Jin</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhiye Song</a>, 
<a href="/search/cs?searchtype=author&query=Ragan-Kelley%2C+J">Jonathan Ragan-Kelley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">To help address the growing demand for ever-longer sequence lengths in
transformer models, Liu et al. recently proposed Ring Attention, an exact
attention algorithm capable of overcoming per-device memory bottle- necks by
distributing self-attention across multiple devices. In this paper, we study
the performance characteristics of Ring Attention in the important special case
of causal transformer models, and identify a key workload imbal- ance due to
triangular structure of causal attention computations. We propose a simple
extension to Ring Attention, which we call Striped Attention to fix this
imbalance. Instead of devices having contiguous subsequences, each device has a
subset of tokens distributed uniformly throughout the sequence, which we
demonstrate leads to more even workloads. In experiments running Striped
Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x
end-to-end throughput improvements over the original Ring Attention algorithm
on causal transformer training at a sequence length of 256k. Furthermore, on 16
TPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of
786k. We release the code for our experiments as open source
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09433" title="Abstract">arXiv:2311.09433</a> [<a href="/pdf/2311.09433" title="Download PDF">pdf</a>, <a href="/format/2311.09433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backdoor Activation Attack: Attack Large Language Models using  Activation Steering for Safety-Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+K">Kai Shu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">To ensure AI safety, instruction-tuned Large Language Models (LLMs) are
specifically trained to ensure alignment, which refers to making models behave
in accordance with human intentions. While these models have demonstrated
commendable results on various safety benchmarks, the vulnerability of their
safety alignment has not been extensively studied. This is particularly
troubling given the potential harm that LLMs can inflict. Existing attack
methods on LLMs often rely on poisoned training data or the injection of
malicious prompts. These approaches compromise the stealthiness and
generalizability of the attacks, making them susceptible to detection.
Additionally, these models often demand substantial computational resources for
implementation, making them less practical for real-world applications. In this
work, we introduce a novel attack framework, called Backdoor Activation Attack,
which injects trojan steering vectors into the activation layers of LLMs. These
malicious steering vectors can be triggered at inference time to steer the
models toward attacker-desired behaviors by manipulating their activations. In
particular, the steering vectors are generated by taking the difference between
benign and malicious activations. Then, the most effective steering vector is
selected and added to the forward passes of the LLMs. Our experiment results on
four primary alignment tasks show that our proposed method is highly effective
and adds little or no overhead to attack efficiency. Additionally, we discuss
potential countermeasures against such activation attacks. Our code and data
are available at https://email-haoran-for-link. Warning: this paper contains
content that can be offensive or upsetting.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09438" title="Abstract">arXiv:2311.09438</a> [<a href="/pdf/2311.09438" title="Download PDF">pdf</a>, <a href="/format/2311.09438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Labeled Interactive Topic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seelman%2C+K">Kyle Seelman</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mozhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J">Jordan Boyd-Graber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Topic models help users understand large document collections; however, topic
models do not always find the ``right'' topics. While classical probabilistic
and anchor-based topic models have interactive variants to guide models toward
better topics, such interactions are not available for neural topic models such
as the embedded topic model (\abr{etm}). We correct this lacuna by adding an
intuitive interaction to neural topic models: users can label a topic with a
word, and topics are updated so that the topic words are close to the label.
This allows a user to refine topics based on their information need. While,
interactivity is intuitive for \abr{etm}, we extend this framework to work with
other neural topic models as well. We develop an interactive interface which
allows users to interact and relabel topic models as they see fit. We evaluate
our method through a human study, where users can relabel topics to find
relevant documents. Using our method, user labeling improves document rank
scores, helping to find more relevant documents to a given query when compared
to no user labeling.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09439" title="Abstract">arXiv:2311.09439</a> [<a href="/pdf/2311.09439" title="Download PDF">pdf</a>, <a href="/format/2311.09439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Hyperplanes for Multi-Agent Collision Avoidance in Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palafox%2C+F">Fernando Palafox</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fridovich-Keil%2C+D">David Fridovich-Keil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">A core challenge of multi-robot interactions is collision avoidance among
robots with potentially conflicting objectives. We propose a game-theoretic
method for collision avoidance based on rotating hyperplane constraints. These
constraints ensure collision avoidance by defining separating hyperplanes that
rotate around a keep-out zone centered on certain robots. Since it is
challenging to select the parameters that define a hyperplane without
introducing infeasibilities, we propose to learn them from an expert trajectory
i.e., one collected by recording human operators. To do so, we solve for the
parameters whose corresponding equilibrium trajectory best matches the expert
trajectory. We validate our method by learning hyperplane parameters from noisy
expert trajectories and demonstrate the generalizability of the learned
parameters to scenarios with more robots and previously unseen initial
conditions.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09440" title="Abstract">arXiv:2311.09440</a> [<a href="/pdf/2311.09440" title="Download PDF">pdf</a>, <a href="/format/2311.09440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Relevance of Blockchain Evaluations on Bare Metal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lebedev%2C+A">Andrei Lebedev</a>, 
<a href="/search/cs?searchtype=author&query=Gramoli%2C+V">Vincent Gramoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF)

</div>
<p class="mathjax">In this paper, we present the first bare metal comparison of modern
blockchains, including Algorand, Avalanche, Diem, Ethereum, Quorum and Solana.
This evaluation was conducted with the recent Diablo benchmark suite, a
framework to evaluate the performance of different blockchains on the same
ground. By tuning network delays in our controlled environment we were able to
reproduce performance trends obtained in geo-distributed settings, hence
demonstrating the relevance of bare metal evaluations to better understand
blockchain performance.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09441" title="Abstract">arXiv:2311.09441</a> [<a href="/pdf/2311.09441" title="Download PDF">pdf</a>, <a href="/format/2311.09441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Privacy-Energy Consumption Tradeoff for Split Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joohyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Seif%2C+M">Mohamed Seif</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jungchan Cho</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Split Federated Learning (SFL) has recently emerged as a promising
distributed learning technology, leveraging the strengths of both federated
learning and split learning. It emphasizes the advantages of rapid convergence
while addressing privacy concerns. As a result, this innovation has received
significant attention from both industry and academia. However, since the model
is split at a specific layer, known as a cut layer, into both client-side and
server-side models for the SFL, the choice of the cut layer in SFL can have a
substantial impact on the energy consumption of clients and their privacy, as
it influences the training burden and the output of the client-side models.
Moreover, the design challenge of determining the cut layer is highly
intricate, primarily due to the inherent heterogeneity in the computing and
networking capabilities of clients. In this article, we provide a comprehensive
overview of the SFL process and conduct a thorough analysis of energy
consumption and privacy. This analysis takes into account the influence of
various system parameters on the cut layer selection strategy. Additionally, we
provide an illustrative example of the cut layer selection, aiming to minimize
the risk of clients from reconstructing the raw data at the server while
sustaining energy consumption within the required energy budget, which involve
trade-offs. Finally, we address open challenges in this field including their
applications to 6G technology. These directions represent promising avenues for
future research and development.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09442" title="Abstract">arXiv:2311.09442</a> [<a href="/pdf/2311.09442" title="Download PDF">pdf</a>, <a href="/format/2311.09442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local spline refinement driven by fault jump estimates for scattered  data approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bracco%2C+C">Cesare Bracco</a>, 
<a href="/search/math?searchtype=author&query=Giannelli%2C+C">Carlotta Giannelli</a>, 
<a href="/search/math?searchtype=author&query=Patrizi%2C+F">Francesco Patrizi</a>, 
<a href="/search/math?searchtype=author&query=Sestini%2C+A">Alessandra Sestini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a new fault (jump) indicator to guide local refinement in surface
approximation schemes with adaptive spline constructions. The proposed approach
is based on the idea that, since discontinuities in the data should naturally
correspond to sharp variations in the reconstructed surface, the location and
size of jumps detected in the input point cloud should drive the mesh
refinement algorithm. To exploit the possibility of inserting local meshlines
in one or the other coordinate direction, as suggested by the jump estimates,
we propose a quasi-interpolation (QI) scheme based on locally refined B-splines
(LR B-splines). Particular attention is devoted to the construction of the
local operator of the LR B-spline QI scheme, which properly adapts the spline
approximation according to the nature and density of the scattered data
configuration. A selection of numerical examples outlines the performance of
the method on synthetic and real datasets characterized by different
geographical features.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09443" title="Abstract">arXiv:2311.09443</a> [<a href="/pdf/2311.09443" title="Download PDF">pdf</a>, <a href="/format/2311.09443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sheppard%2C+B">Brooklyn Sheppard</a>, 
<a href="/search/cs?searchtype=author&query=Richter%2C+A">Anna Richter</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A">Allison Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+E+A">Elizabeth Allyn Smith</a>, 
<a href="/search/cs?searchtype=author&query=Kneese%2C+T">Tamara Kneese</a>, 
<a href="/search/cs?searchtype=author&query=Pelletier%2C+C">Carolyne Pelletier</a>, 
<a href="/search/cs?searchtype=author&query=Baldini%2C+I">Ioana Baldini</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yue Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Using novel approaches to dataset development, the Biasly dataset captures
the nuance and subtlety of misogyny in ways that are unique within the
literature. Built in collaboration with multi-disciplinary experts and
annotators themselves, the dataset contains annotations of movie subtitles,
capturing colloquial expressions of misogyny in North American film. The
dataset can be used for a range of NLP tasks, including classification,
severity score regression, and text generation for rewrites. In this paper, we
discuss the methodology used, analyze the annotations obtained, and provide
baselines using common NLP algorithms in the context of misogyny detection and
mitigation. We hope this work will promote AI for social good in NLP for bias
detection, explanation, and removal.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09445" title="Abstract">arXiv:2311.09445</a> [<a href="/pdf/2311.09445" title="Download PDF">pdf</a>, <a href="/format/2311.09445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Software-Hardware Co-Optimized Toolkit for Deep Reinforcement Learning  on Heterogeneous Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yuan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Kinsner%2C+M">Michael Kinsner</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+D">Deshanand Singh</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+M+A">Mahesh A Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+V">Viktor Prasanna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IPDPS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Deep Reinforcement Learning (DRL) is vital in various AI applications. DRL
algorithms comprise diverse compute kernels, which may not be simultaneously
optimized using a homogeneous architecture. However, even with available
heterogeneous architectures, optimizing DRL performance remains a challenge due
to the complexity of hardware and programming models employed in modern data
centers. To address this, we introduce PEARL, a toolkit for composing parallel
DRL systems on heterogeneous platforms consisting of general-purpose processors
(CPUs) and accelerators (GPUs, FPGAs). Our innovations include: 1. A general
training protocol agnostic of the underlying hardware, enabling portable
implementations across various processors and accelerators. 2. Incorporation of
DRL-specific scheduling optimizations within the protocol, facilitating
parallelized training and enhancing the overall system performance. 3.
High-level API for productive development using the toolkit. 4. Automatic
optimization of DRL task-to-device assignments through performance estimation,
supporting various optimization metrics including throughput and power
efficiency.
<br />We showcase our toolkit through experimentation with two widely used DRL
algorithms, DQN and DDPG, on two diverse heterogeneous platforms. The generated
implementations outperform state-of-the-art libraries for CPU-GPU platforms by
throughput improvements of up to 2.1$\times$ and power efficiency improvements
of up to 3.4$\times$.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09447" title="Abstract">arXiv:2311.09447</a> [<a href="/pdf/2311.09447" title="Download PDF">pdf</a>, <a href="/format/2311.09447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Trustworthy are Open-Source LLMs? An Assessment under Malicious  Demonstrations Shows their Vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+L">Lingbo Mo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boshi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid progress in open-source Large Language Models (LLMs) is
significantly driving AI development forward. However, there is still a limited
understanding of their trustworthiness. Deploying these models at scale without
sufficient trustworthiness can pose significant risks, highlighting the need to
uncover these issues promptly. In this work, we conduct an assessment of
open-source LLMs on trustworthiness, scrutinizing them across eight different
aspects including toxicity, stereotypes, ethics, hallucination, fairness,
sycophancy, privacy, and robustness against adversarial demonstrations. We
propose an enhanced Chain of Utterances-based (CoU) prompting strategy by
incorporating meticulously crafted malicious demonstrations for trustworthiness
attack. Our extensive experiments encompass recent and representative series of
open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The
empirical outcomes underscore the efficacy of our attack strategy across
diverse aspects. More interestingly, our result analysis reveals that models
with superior performance in general NLP tasks do not always have greater
trustworthiness; in fact, larger models can be more vulnerable to attacks.
Additionally, models that have undergone instruction tuning, focusing on
instruction following, tend to be more susceptible, although fine-tuning LLMs
for safety alignment proves effective in mitigating adversarial trustworthiness
attacks.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09449" title="Abstract">arXiv:2311.09449</a> [<a href="/pdf/2311.09449" title="Download PDF">pdf</a>, <a href="/format/2311.09449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HAL 9000: Skynet&#x27;s Risk Manager
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freitas%2C+T">Tadeu Freitas</a>, 
<a href="/search/cs?searchtype=author&query=Neto%2C+M">M&#xe1;rio Neto</a>, 
<a href="/search/cs?searchtype=author&query=Dutra%2C+I">In&#xea;s Dutra</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+J">Jo&#xe3;o Soares</a>, 
<a href="/search/cs?searchtype=author&query=Correia%2C+M">Manuel Correia</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+R">Rolando Martins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Operating Systems (cs.OS)

</div>
<p class="mathjax">Intrusion Tolerant Systems (ITSs) are a necessary component for
cyber-services/infrastructures. Additionally, as cyberattacks follow a
multi-domain attack surface, a similar defensive approach should be applied,
namely, the use of an evolving multi-disciplinary solution that combines ITS,
cybersecurity and Artificial Intelligence (AI). With the increased popularity
of AI solutions, due to Big Data use-case scenarios and decision support and
automation scenarios, new opportunities to apply Machine Learning (ML)
algorithms have emerged, namely ITS empowerment. Using ML algorithms, an ITS
can augment its intrusion tolerance capability, by learning from previous
attacks and from known vulnerabilities. As such, this work's contribution is
twofold: (1) an ITS architecture (Skynet) based on the state-of-the-art and
incorporates new components to increase its intrusion tolerance capability and
its adaptability to new adversaries; (2) an improved Risk Manager design that
leverages AI to improve ITSs by automatically assessing OS risks to intrusions,
and advise with safer configurations. One of the reasons that intrusions are
successful is due to bad configurations or slow adaptability to new threats.
This can be caused by the dependency that systems have for human intervention.
One of the characteristics in Skynet and HAL 9000 design is the removal of
human intervention. Being fully automatized lowers the chance of successful
intrusions caused by human error. Our experiments using Skynet, shows that HAL
is able to choose 15% safer configurations than the state-of-the-art risk
manager.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09452" title="Abstract">arXiv:2311.09452</a> [<a href="/pdf/2311.09452" title="Download PDF">pdf</a>, <a href="/format/2311.09452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Close the Gates to an Inhuman Future: How and why we should choose to  not develop superhuman general-purpose artificial intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aguirre%2C+A">Anthony Aguirre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In the coming years, humanity may irreversibly cross a threshold by creating
superhuman general-purpose artificial intelligence. This would present many
unprecedented risks and is likely to be uncontrollable in several ways. We can
choose not to do so, starting by instituting hard limits on the computation
that can be used to train and run neural networks. With these limits in place,
AI research and industry can work on making AI that humans can understand and
control, and from which we can reap enormous benefit.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09456" title="Abstract">arXiv:2311.09456</a> [<a href="/pdf/2311.09456" title="Download PDF">pdf</a>, <a href="/format/2311.09456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepMartNet -- A Martingale Based Deep Neural Network Learning Method  for Dirichlet BVP and Eigenvalue Problems of Elliptic PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cai%2C+W">Wei Cai</a>, 
<a href="/search/math?searchtype=author&query=He%2C+A">Andrew He</a>, 
<a href="/search/math?searchtype=author&query=Margolis%2C+D">Daniel Margolis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we propose DeepMartNet - a Martingale based deep neural
network learning method for solving Dirichlet boundary value problems (BVPs)
and eigenvalue problems for elliptic partial differential equations (PDEs) in
high dimensions. The method is based on Varadhan's Martingale problem
formulation for the BVP/eigenvalue problems where a loss function enforcing the
Martingale property for the PDE solution is used for efficient optimization by
sampling the stochastic processes associated with elliptic operators. High
dimensional numerical results for BVPs of the Poisson-Boltzmann equation and
eigenvalue problems of a Fokker-Planck equation demonstrate the capability of
the proposed DeepMartNet learning method for solving high dimensional PDE
problems.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09458" title="Abstract">arXiv:2311.09458</a> [<a href="/pdf/2311.09458" title="Download PDF">pdf</a>, <a href="/format/2311.09458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of  Lexical Overlap in Train and Test Reference Summaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choubey%2C+P+K">Prafulla Kumar Choubey</a>, 
<a href="/search/cs?searchtype=author&query=Fabbri%2C+A+R">Alexander R. Fabbri</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chien-Sheng Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023-Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Ideal summarization models should generalize to novel summary-worthy content
without remembering reference training summaries by rote. However, a single
average performance score on the entire test set is inadequate in determining
such model competencies. We propose a fine-grained evaluation protocol by
partitioning a test set based on the lexical similarity of reference test
summaries with training summaries. We observe up to a 5x (1.2x) difference in
ROUGE-2 (entity recall) scores between the subsets with the lowest and highest
similarity. Next, we show that such training repetitions also make a model
vulnerable to rote learning, reproducing data artifacts such as factual errors,
especially when reference test summaries are lexically close to training
summaries. Consequently, we propose to limit lexical repetitions in training
summaries during both supervised fine-tuning and likelihood calibration stages
to improve the performance on novel test cases while retaining average
performance. Our automatic and human evaluations on novel test subsets and
recent news articles show that limiting lexical repetitions in training
summaries can prevent rote learning and improve generalization.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09459" title="Abstract">arXiv:2311.09459</a> [<a href="/pdf/2311.09459" title="Download PDF">pdf</a>, <a href="/format/2311.09459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Convex Optimal Value Functions For POSGs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cunha%2C+R+F">Rafael F. Cunha</a>, 
<a href="/search/cs?searchtype=author&query=Castellini%2C+J">Jacopo Castellini</a>, 
<a href="/search/cs?searchtype=author&query=Peralez%2C+J">Johan Peralez</a>, 
<a href="/search/cs?searchtype=author&query=Dibangoye%2C+J+S">Jilles S. Dibangoye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Currently under review at JAIR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Multi-agent planning and reinforcement learning can be challenging when
agents cannot see the state of the world or communicate with each other due to
communication costs, latency, or noise. Partially Observable Stochastic Games
(POSGs) provide a mathematical framework for modelling such scenarios. This
paper aims to improve the efficiency of planning and reinforcement learning
algorithms for POSGs by identifying the underlying structure of optimal
state-value functions. The approach involves reformulating the original game
from the perspective of a trusted third party who plans on behalf of the agents
simultaneously. From this viewpoint, the original POSGs can be viewed as Markov
games where states are occupancy states, \ie posterior probability
distributions over the hidden states of the world and the stream of actions and
observations that agents have experienced so far. This study mainly proves that
the optimal state-value function is a convex function of occupancy states
expressed on an appropriate basis in all zero-sum, common-payoff, and
Stackelberg POSGs.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09460" title="Abstract">arXiv:2311.09460</a> [<a href="/pdf/2311.09460" title="Download PDF">pdf</a>, <a href="/format/2311.09460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Streaming Ellipsoidal Rounding for General Convex Polytopes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makarychev%2C+Y">Yury Makarychev</a>, 
<a href="/search/cs?searchtype=author&query=Manoj%2C+N+S">Naren Sarayu Manoj</a>, 
<a href="/search/cs?searchtype=author&query=Ovsiankin%2C+M">Max Ovsiankin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">We give near-optimal algorithms for computing an ellipsoidal rounding of a
convex polytope whose vertices are given in a stream. The approximation factor
is linear in the dimension (as in John's theorem) and only loses an excess
logarithmic factor in the aspect ratio of the polytope. Our algorithms are
nearly optimal in two senses: first, their runtimes nearly match those of the
most efficient known algorithms for the offline version of the problem. Second,
their approximation factors nearly match a lower bound we show against a
natural class of geometric streaming algorithms. In contrast to existing works
in the streaming setting that compute ellipsoidal roundings only for centrally
symmetric convex polytopes, our algorithms apply to general convex polytopes.
We also show how to use our algorithms to construct coresets from a stream of
points that approximately preserve both the ellipsoidal rounding and the convex
hull of the original set of points.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09462" title="Abstract">arXiv:2311.09462</a> [<a href="/pdf/2311.09462" title="Download PDF">pdf</a>, <a href="/format/2311.09462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Software-Defined Virtual Synchronous Condenser
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Zimin Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yifan Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Kocewiak%2C+%C5%81">&#x141;ukasz Kocewiak</a>, 
<a href="/search/eess?searchtype=author&query=Chandrashekhara%2C+D+K">Divya Kurthakoti Chandrashekhara</a>, 
<a href="/search/eess?searchtype=author&query=Picherit%2C+M">Marie-Lou Picherit</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+Z">Zefan Tang</a>, 
<a href="/search/eess?searchtype=author&query=Bowes%2C+K+B">Kenneth B. Bowes</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guangya Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Synchronous condensers (SCs) play important roles in integrating wind energy
into relatively weak power grids. However, the design of SCs usually depends on
specific application requirements and may not be adaptive enough to the
frequently-changing grid conditions caused by the transition from conventional
to renewable power generation. This paper devises a software-defined virtual
synchronous condenser (SDViSC) method to address the challenges. Our
contributions are fourfold: 1) design of a virtual synchronous condenser (ViSC)
to enable full converter wind turbines to provide built-in SC functionalities;
2) engineering SDViSCs to transfer hardware-based ViSC controllers into
software services, where a Tustin transformation-based software-defined control
algorithm guarantees accurate tracking of fast dynamics under limited
communication bandwidth; 3) a software-defined networking-enhanced SDViSC
communication scheme to allow enhanced communication reliability and reduced
communication bandwidth occupation; and 4) Prototype of SDViSC on our
real-time, cyber-in-the-loop digital twin of large-wind-farm in an RTDS
environment. Extensive test results validate the excellent performance of
SDViSC to support reliable and resilient operations of wind farms under various
physical and cyber conditions.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09465" title="Abstract">arXiv:2311.09465</a> [<a href="/pdf/2311.09465" title="Download PDF">pdf</a>, <a href="/ps/2311.09465" title="Download PostScript">ps</a>, <a href="/format/2311.09465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new stabilized time-spectral finite element solver for fast simulation  of blood flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Esmaily%2C+M">Mahdi Esmaily</a>, 
<a href="/search/math?searchtype=author&query=Jia%2C+D">Dongjie Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> *Correspondence: me399@cornell.edu
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">The increasing application of cardiorespiratory simulations for diagnosis and
surgical planning necessitates the development of computational methods
significantly faster than the current technology. To achieve this objective, we
leverage the time-periodic nature of these flows by discretizing equations in
the frequency domain instead of the time domain. This approach markedly reduces
the size of the discrete problem and, consequently, the simulation cost. With
this motivation, we introduce a finite element method for simulating
time-periodic flows that are physically stable. The proposed time-spectral
method is formulated by augmenting the baseline Galerkin's method with a
least-squares penalty term. This penalty term is weighted by a
positive-definite stabilization tensor, computed by solving an eigenvalue
problem that involves the contraction of the velocity convolution matrix with
the element metric tensor. The outcome is a formally stable residual-based
method that emulates the standard time method when simulating steady flows.
Consequently, it preserves the appealing properties of the standard method,
including stability in strong convection and the convenient use of equal-order
interpolation functions for velocity and pressure, among other benefits. This
method is tested on a patient-specific Fontan model at nominal Reynolds and
Womersley numbers of 500 and 10, respectively, demonstrating its ability to
replicate conventional time simulation results using as few as 7 modes at 11\%
of the computational cost. Owing to its higher local-to-processor computation
density, the proposed method also exhibits improved parallel scalability,
thereby enabling efficient utilization of computational resources for the rapid
simulation of time-critical applications.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09466" title="Abstract">arXiv:2311.09466</a> [<a href="/pdf/2311.09466" title="Download PDF">pdf</a>, <a href="/format/2311.09466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft Matching Distance: A metric on neural representations that captures  single-neuron tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khosla%2C+M">Meenakshi Khosla</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+A+H">Alex H. Williams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
<p class="mathjax">Common measures of neural representational (dis)similarity are designed to be
insensitive to rotations and reflections of the neural activation space.
Motivated by the premise that the tuning of individual units may be important,
there has been recent interest in developing stricter notions of
representational (dis)similarity that require neurons to be individually
matched across networks. When two networks have the same size (i.e. same number
of neurons), a distance metric can be formulated by optimizing over neuron
index permutations to maximize tuning curve alignment. However, it is not clear
how to generalize this metric to measure distances between networks with
different sizes. Here, we leverage a connection to optimal transport theory to
derive a natural generalization based on "soft" permutations. The resulting
metric is symmetric, satisfies the triangle inequality, and can be interpreted
as a Wasserstein distance between two empirical distributions. Further, our
proposed metric avoids counter-intuitive outcomes suffered by alternative
approaches, and captures complementary geometric insights into neural
representations that are entirely missed by rotation-invariant metrics.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09467" title="Abstract">arXiv:2311.09467</a> [<a href="/pdf/2311.09467" title="Download PDF">pdf</a>, <a href="/format/2311.09467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think While You Write: Hypothesis Verification Promotes Faithful  Knowledge-to-Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yifu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Embar%2C+V">Varun Embar</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Benjamin Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neural knowledge-to-text generation models often struggle to faithfully
generate descriptions for the input facts: they may produce hallucinations that
contradict the given facts, or describe facts not present in the input. To
reduce hallucinations, we propose a novel decoding method, TWEAK (Think While
Effectively Articulating Knowledge). TWEAK treats the generated sequences at
each decoding step and its future sequences as hypotheses, and ranks each
generation candidate based on how well their corresponding hypotheses support
the input facts using a Hypothesis Verification Model (HVM). We first
demonstrate the effectiveness of TWEAK by using a Natural Language Inference
(NLI) model as the HVM and report improved faithfulness with minimal impact on
the quality. We then replace the NLI model with our task-specific HVM trained
with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which
pairs input facts with their faithful and hallucinated descriptions with the
hallucinated spans marked. The new HVM improves the faithfulness and the
quality further and runs faster. Overall the best TWEAK variants improve on
average 2.22/7.17 points on faithfulness measured by FactKB over WebNLG and
TekGen/GenWiki, respectively, with only 0.14/0.32 points degradation on quality
measured by BERTScore over the same datasets. Since TWEAK is a decoding-only
approach, it can be integrated with any neural generative model without
retraining.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09468" title="Abstract">arXiv:2311.09468</a> [<a href="/pdf/2311.09468" title="Download PDF">pdf</a>, <a href="/format/2311.09468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Carbon Dioxide Emission Minimized Virtual Machine (VM) Placement in  Cloud-Fog Network Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bessalah%2C+T">Tarek Bessalah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Cloud computing has provided economies of scale, savings, and efficiency for
both individual consumers and enterprises. Its key advantage is its ability to
handle increasing amounts of data and provide functionality that gives users
the ability to scale their computing resources, including processing, data
storage, and networking capabilities. Virtual Machines (VM), enabled via
virtualization technology, allow cloud service providers to deliver their
services to users. This, however, results in increasing carbon dioxide
emissions from increased energy use. This paper introduces a Mixed-Integer
Linear Programming (MILP) model that investigates the VM placement, focusing on
the British Telecom (BT) network topology, in a cloud-fog network architecture
when renewable energy sources are introduced in the fog layer located near
traffic-producing sources. VMs can be placed on nodes hosted on the core,
metro, and access (fog) layers. We first investigate the effect of varying
traffic on IP over WDM power consumption in the backbone network and the number
of optical carrier signals to serve the traffic over a period of time. We later
extend the model to consider the CO2-minimized optimal virtual machine
placement given the sporadic traffic quantity, and the consideration of solar
renewable energy sources placed in data centers located in the access (fog)
layer throughout the day, imposed on the VM and the minimum workload
requirement of the VM to maintain a service level agreement (SLA).
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09469" title="Abstract">arXiv:2311.09469</a> [<a href="/pdf/2311.09469" title="Download PDF">pdf</a>, <a href="/format/2311.09469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M+J+Q">Michael J.Q. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Eunsol Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Resolving ambiguities through interaction is a hallmark of natural language,
and modeling this behavior is a core challenge in crafting AI assistants. In
this work, we study such behavior in LMs by proposing a task-agnostic framework
for resolving ambiguity by asking users clarifying questions. Our framework
breaks down this objective into three subtasks: (1) determining when
clarification is needed, (2) determining what clarifying question to ask, and
(3) responding accurately with the new information gathered through
clarification. We evaluate systems across three NLP applications: question
answering, machine translation and natural language inference. For the first
subtask, we present a novel uncertainty estimation approach, intent-sim, that
determines the utility of querying for clarification by estimating the entropy
over user intents. Our method consistently outperforms existing uncertainty
estimation approaches at identifying predictions that will benefit from
clarification. When only allowed to ask for clarification on 10% of examples,
our system is able to double the performance gains over randomly selecting
examples to clarify. Furthermore, we find that intent-sim is robust,
demonstrating improvements across a wide range of NLP tasks and LMs. Together,
our work lays foundation for studying clarifying interactions with LMs.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09473" title="Abstract">arXiv:2311.09473</a> [<a href="/pdf/2311.09473" title="Download PDF">pdf</a>, <a href="/format/2311.09473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JAB: Joint Adversarial Prompting and Belief Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehrabi%2C+N">Ninareh Mehrabi</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+P">Palash Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishna%2C+A">Anil Ramakrishna</a>, 
<a href="/search/cs?searchtype=author&query=Dhamala%2C+J">Jwala Dhamala</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Shalini Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Zemel%2C+R">Richard Zemel</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Galstyan%2C+A">Aram Galstyan</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rahul Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">With the recent surge of language models in different applications, attention
to safety and robustness of these models has gained significant importance.
Here we introduce a joint framework in which we simultaneously probe and
improve the robustness of a black-box target model via adversarial prompting
and belief augmentation using iterative feedback loops. This framework utilizes
an automated red teaming approach to probe the target model, along with a
belief augmenter to generate instructions for the target model to improve its
robustness to those adversarial probes. Importantly, the adversarial model and
the belief generator leverage the feedback from past interactions to improve
the effectiveness of the adversarial prompts and beliefs, respectively. In our
experiments, we demonstrate that such a framework can reduce toxic content
generation both in dynamic cases where an adversary directly interacts with a
target model and static cases where we use a static benchmark dataset to
evaluate our model.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09476" title="Abstract">arXiv:2311.09476</a> [<a href="/pdf/2311.09476" title="Download PDF">pdf</a>, <a href="/format/2311.09476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARES: An Automated Evaluation Framework for Retrieval-Augmented  Generation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saad-Falcon%2C+J">Jon Saad-Falcon</a>, 
<a href="/search/cs?searchtype=author&query=Khattab%2C+O">Omar Khattab</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Zaharia%2C+M">Matei Zaharia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Evaluating retrieval-augmented generation (RAG) systems traditionally relies
on hand annotations for input queries, passages to retrieve, and responses to
generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating
RAG systems along the dimensions of context relevance, answer faithfulness, and
answer relevance. Using synthetic training data, ARES finetunes lightweight LM
judges to assess the quality of individual RAG components. To mitigate
potential prediction errors, ARES utilizes a small set of human-annotated
datapoints for prediction-powered inference (PPI). Across six different
knowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG
systems while using a few hundred human annotations during evaluation.
Furthermore, ARES judges remain effective across domain shifts, proving
accurate even after changing the type of queries and/or documents used in the
evaluated RAG systems. We make our datasets and code for replication and
deployment available at https://github.com/stanford-futuredata/ARES.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09480" title="Abstract">arXiv:2311.09480</a> [<a href="/pdf/2311.09480" title="Download PDF">pdf</a>, <a href="/format/2311.09480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Show Your Work with Confidence: Confidence Bands for Tuning Curves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lourie%2C+N">Nicholas Lourie</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">He He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">The choice of hyperparameters greatly impacts performance in natural language
processing. Often, it is hard to tell if a method is better than another or
just better tuned. Tuning curves fix this ambiguity by accounting for tuning
effort. Specifically, they plot validation performance as a function of the
number of hyperparameter choices tried so far. While several estimators exist
for these curves, it is common to use point estimates, which we show fail
silently and give contradictory results when given too little data.
<br />Beyond point estimates, confidence bands are necessary to rigorously
establish the relationship between different approaches. We present the first
method to construct valid confidence bands for tuning curves. The bands are
exact, simultaneous, and distribution-free, thus they provide a robust basis
for comparing methods.
<br />Empirical analysis shows that while bootstrap confidence bands, which serve
as a baseline, fail to approximate their target confidence, ours achieve it
exactly. We validate our design with ablations, analyze the effect of sample
size, and provide guidance on comparing models with our method. To promote
confident comparisons in future work, we release a library implementing the
method at https://github.com/nalourie/opda .
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09481" title="Abstract">arXiv:2311.09481</a> [<a href="/pdf/2311.09481" title="Download PDF">pdf</a>, <a href="/format/2311.09481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Jargon Identification for Enhanced Interdisciplinary  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yue Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J+C">Joseph Chee Chang</a>, 
<a href="/search/cs?searchtype=author&query=Antoniak%2C+M">Maria Antoniak</a>, 
<a href="/search/cs?searchtype=author&query=Bransom%2C+E">Erin Bransom</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+T">Trevor Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L+L">Lucy Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=August%2C+T">Tal August</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Scientific jargon can impede researchers when they read materials from other
domains. Current methods of jargon identification mainly use corpus-level
familiarity indicators (e.g., Simple Wikipedia represents plain language).
However, researchers' familiarity of a term can vary greatly based on their own
background. We collect a dataset of over 10K term familiarity annotations from
11 computer science researchers for terms drawn from 100 paper abstracts.
Analysis of this data reveals that jargon familiarity and information needs
vary widely across annotators, even within the same sub-domain (e.g., NLP). We
investigate features representing individual, sub-domain, and domain knowledge
to predict individual jargon familiarity. We compare supervised and
prompt-based approaches, finding that prompt-based methods including personal
publications yields the highest accuracy, though zero-shot prompting provides a
strong baseline. This research offers insight into features and methods to
integrate personal data into scientific jargon identification.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09482" title="Abstract">arXiv:2311.09482</a> [<a href="/pdf/2311.09482" title="Download PDF">pdf</a>, <a href="/format/2311.09482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Conformal Prediction for STL Runtime Verification under  Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Y">Yiqi Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Hoxha%2C+B">Bardh Hoxha</a>, 
<a href="/search/eess?searchtype=author&query=Fainekos%2C+G">Georgios Fainekos</a>, 
<a href="/search/eess?searchtype=author&query=Deshmukh%2C+J+V">Jyotirmoy V. Deshmukh</a>, 
<a href="/search/eess?searchtype=author&query=Lindemann%2C+L">Lars Lindemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Logic in Computer Science (cs.LO); Robotics (cs.RO)

</div>
<p class="mathjax">Cyber-physical systems (CPS) designed in simulators behave differently in the
real-world. Once they are deployed in the real-world, we would hence like to
predict system failures during runtime. We propose robust predictive runtime
verification (RPRV) algorithms under signal temporal logic (STL) tasks for
general stochastic CPS. The RPRV problem faces several challenges: (1) there
may not be sufficient data of the behavior of the deployed CPS, (2) predictive
models are based on a distribution over system trajectories encountered during
the design phase, i.e., there may be a distribution shift during deployment. To
address these challenges, we assume to know an upper bound on the statistical
distance (in terms of an f-divergence) between the distributions at deployment
and design time, and we utilize techniques based on robust conformal
prediction. Motivated by our results in [1], we construct an accurate and an
interpretable RPRV algorithm. We use a trajectory prediction model to estimate
the system behavior at runtime and robust conformal prediction to obtain
probabilistic guarantees by accounting for distribution shifts. We precisely
quantify the relationship between calibration data, desired confidence, and
permissible distribution shift. To the best of our knowledge, these are the
first statistically valid algorithms under distribution shift in this setting.
We empirically validate our algorithms on a Franka manipulator within the
NVIDIA Isaac sim environment.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09483" title="Abstract">arXiv:2311.09483</a> [<a href="/pdf/2311.09483" title="Download PDF">pdf</a>, <a href="/format/2311.09483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Interventions with User-Defined Goals for Health Behavior  Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandyam%2C+A">Aishwarya Mandyam</a>, 
<a href="/search/cs?searchtype=author&query=Joerke%2C+M">Matthew Joerke</a>, 
<a href="/search/cs?searchtype=author&query=Engelhardt%2C+B+E">Barbara E. Engelhardt</a>, 
<a href="/search/cs?searchtype=author&query=Brunskill%2C+E">Emma Brunskill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Physical inactivity remains a major public health concern, having
associations with adverse health outcomes such as cardiovascular disease and
type-2 diabetes. Mobile health applications present a promising avenue for
low-cost, scalable physical activity promotion, yet often suffer from small
effect sizes and low adherence rates, particularly in comparison to human
coaching. Goal-setting is a critical component of health coaching that has been
underutilized in adaptive algorithms for mobile health interventions. This
paper introduces a modification to the Thompson sampling algorithm that places
emphasis on individualized goal-setting by optimizing personalized reward
functions. As a step towards supporting goal-setting, this paper offers a
balanced approach that can leverage shared structure while optimizing
individual preferences and goals. We prove that our modification incurs only a
constant penalty on the cumulative regret while preserving the sample
complexity benefits of data sharing. In a physical activity simulator, we
demonstrate that our algorithm achieves substantial improvements in cumulative
regret compared to baselines that do not share data or do not optimize for
individualized rewards.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09489" title="Abstract">arXiv:2311.09489</a> [<a href="/pdf/2311.09489" title="Download PDF">pdf</a>, <a href="/format/2311.09489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MirrorNet: A TEE-Friendly Framework for Secure On-device DNN Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yukui Luo</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+S">Shijin Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaolin Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCAD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Deep neural network (DNN) models have become prevalent in edge devices for
real-time inference. However, they are vulnerable to model extraction attacks
and require protection. Existing defense approaches either fail to fully
safeguard model confidentiality or result in significant latency issues. To
overcome these challenges, this paper presents MirrorNet, which leverages
Trusted Execution Environment (TEE) to enable secure on-device DNN inference.
It generates a TEE-friendly implementation for any given DNN model to protect
the model confidentiality, while meeting the stringent computation and storage
constraints of TEE. The framework consists of two key components: the backbone
model (BackboneNet), which is stored in the normal world but achieves lower
inference accuracy, and the Companion Partial Monitor (CPM), a lightweight
mirrored branch stored in the secure world, preserving model confidentiality.
During inference, the CPM monitors the intermediate results from the
BackboneNet and rectifies the classification output to achieve higher accuracy.
To enhance flexibility, MirrorNet incorporates two modules: the CPM Strategy
Generator, which generates various protection strategies, and the Performance
Emulator, which estimates the performance of each strategy and selects the most
optimal one. Extensive experiments demonstrate the effectiveness of MirrorNet
in providing security guarantees while maintaining low computation latency,
making MirrorNet a practical and promising solution for secure on-device DNN
inference. For the evaluation, MirrorNet can achieve a 18.6% accuracy gap
between authenticated and illegal use, while only introducing 0.99% hardware
overhead.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09490" title="Abstract">arXiv:2311.09490</a> [<a href="/pdf/2311.09490" title="Download PDF">pdf</a>, <a href="/format/2311.09490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Visibility Region and Channel Estimation for Extremely Large-scale  MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+A">Anzheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun-bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yijin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wence Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yijian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongkang Yu</a>, 
<a href="/search/cs?searchtype=author&query=de+Lamare%2C+R+C">Rodrigo C. de Lamare</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to IEEE journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this work, we investigate the channel estimation (CE) problem for
extremely large-scale multiple-input-multiple-output (XL-MIMO) systems,
considering both the spherical wavefront effect and spatial non-stationarity
(SnS). Unlike existing non-stationary CE methods that rely on the statistical
characteristics of channels in the spatial or temporal domain, our approach
seeks to leverage sparsity in both the spatial and wavenumber domains
simultaneously to achieve an accurate estimation.To this end, we introduce a
two-stage visibility region (VR) detection and CE framework. Specifically, in
the first stage, the belief regarding the visibility of antennas is obtained
through a structured message passing (MP) scheme, which fully exploits the
block sparse structure of the antenna-domain channel. In the second stage,
using the obtained VR information and wavenumber-domain sparsity, we accurately
estimate the SnS channel employing the belief-based orthogonal matching pursuit
(BB-OMP) method. Simulations demonstrate that the proposed algorithms lead to a
significant enhancement in VR detection and CE accuracy, especially in low
signal-to-noise ratio (SNR) scenarios.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09497" title="Abstract">arXiv:2311.09497</a> [<a href="/pdf/2311.09497" title="Download PDF">pdf</a>, <a href="/format/2311.09497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Peer Reviews of Peer Reviews: A Randomized Controlled Trial and Other  Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+A">Alexander Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Stelmakh%2C+I">Ivan Stelmakh</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+A">Alice Oh</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Alekh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Belgrave%2C+D">Danielle Belgrave</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N+B">Nihar B. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Is it possible to reliably evaluate the quality of peer reviews? We study
this question driven by two primary motivations -- incentivizing high-quality
reviewing using assessed quality of reviews and measuring changes to review
quality in experiments. We conduct a large scale study at the NeurIPS 2022
conference, a top-tier conference in machine learning, in which we invited
(meta)-reviewers and authors to evaluate reviews given to submitted papers.
First, we conduct a RCT to examine bias due to the length of reviews. We
generate elongated versions of reviews by adding substantial amounts of
non-informative content. Participants in the control group evaluate the
original reviews, whereas participants in the experimental group evaluate the
artificially lengthened versions. We find that lengthened reviews are scored
(statistically significantly) higher quality than the original reviews.
Additionally, in analysis of observational data we find that authors are
positively biased towards reviews recommending acceptance of their own papers,
even after controlling for confounders of review length, quality, and different
numbers of papers per author. We also measure disagreement rates between
multiple evaluations of the same review of 28%-32%, which is comparable to that
of paper reviewers at NeurIPS. Further, we assess the amount of miscalibration
of evaluators of reviews using a linear model of quality scores and find that
it is similar to estimates of miscalibration of paper reviewers at NeurIPS.
Finally, we estimate the amount of variability in subjective opinions around
how to map individual criteria to overall scores of review quality and find
that it is roughly the same as that in the review of papers. Our results
suggest that the various problems that exist in reviews of papers --
inconsistency, bias towards irrelevant factors, miscalibration, subjectivity --
also arise in reviewing of reviews.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09498" title="Abstract">arXiv:2311.09498</a> [<a href="/pdf/2311.09498" title="Download PDF">pdf</a>, <a href="/ps/2311.09498" title="Download PostScript">ps</a>, <a href="/format/2311.09498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Wide Evacuation Traffic Prediction in a Rapidly Intensifying  Hurricane from Traffic Detectors and Facebook Movement Data: A Deep Learning  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rashid%2C+M+M">Md Mobasshir Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+R">Rezaur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+S">Samiul Hasan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Traffic prediction during hurricane evacuation is essential for optimizing
the use of transportation infrastructures. It can reduce evacuation time by
providing information on future congestion in advance. However, evacuation
traffic prediction can be challenging as evacuation traffic patterns is
significantly different than regular period traffic. A data-driven traffic
prediction model is developed in this study by utilizing traffic detector and
Facebook movement data during Hurricane Ian, a rapidly intensifying hurricane.
We select 766 traffic detectors from Florida's 4 major interstates to collect
traffic features. Additionally, we use Facebook movement data collected during
Hurricane Ian's evacuation period. The deep-learning model is first trained on
regular period (May-August 2022) data to understand regular traffic patterns
and then Hurricane Ian's evacuation period data is used as test data. The model
achieves 95% accuracy (RMSE = 356) during regular period, but it underperforms
with 55% accuracy (RMSE = 1084) during the evacuation period. Then, a transfer
learning approach is adopted where a pretrained model is used with additional
evacuation related features to predict evacuation period traffic. After
transfer learning, the model achieves 89% accuracy (RMSE = 514). Adding
Facebook movement data further reduces model's RMSE value to 393 and increases
accuracy to 93%. The proposed model is capable to forecast traffic up to
6-hours in advance. Evacuation traffic management officials can use the
developed traffic prediction model to anticipate future traffic congestion in
advance and take proactive measures to reduce delays during evacuation.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09499" title="Abstract">arXiv:2311.09499</a> [<a href="/pdf/2311.09499" title="Download PDF">pdf</a>, <a href="/format/2311.09499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Center Focusing Network for Real-Time LiDAR Panoptic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yongli Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Baocai Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">LiDAR panoptic segmentation facilitates an autonomous vehicle to
comprehensively understand the surrounding objects and scenes and is required
to run in real time. The recent proposal-free methods accelerate the algorithm,
but their effectiveness and efficiency are still limited owing to the
difficulty of modeling non-existent instance centers and the costly
center-based clustering modules. To achieve accurate and real-time LiDAR
panoptic segmentation, a novel center focusing network (CFNet) is introduced.
Specifically, the center focusing feature encoding (CFFE) is proposed to
explicitly understand the relationships between the original LiDAR points and
virtual instance centers by shifting the LiDAR points and filling in the center
points. Moreover, to leverage the redundantly detected centers, a fast center
deduplication module (CDM) is proposed to select only one center for each
instance. Experiments on the SemanticKITTI and nuScenes panoptic segmentation
benchmarks demonstrate that our CFNet outperforms all existing methods by a
large margin and is 1.6 times faster than the most efficient method. The code
is available at https://github.com/GangZhang842/CFNet.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09500" title="Abstract">arXiv:2311.09500</a> [<a href="/pdf/2311.09500" title="Download PDF">pdf</a>, <a href="/format/2311.09500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pseudo-keypoints RKHS Learning for Self-supervised 6DoF Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yangzheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Greenspan%2C+M">Michael Greenspan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper addresses the simulation-to-real domain gap in 6DoF PE, and
proposes a novel self-supervised keypoint radial voting-based 6DoF PE
framework, effectively narrowing this gap using a learnable kernel in RKHS. We
formulate this domain gap as a distance in high-dimensional feature space,
distinct from previous iterative matching methods. We propose an adapter
network, which evolves the network parameters from the source domain, which has
been massively trained on synthetic data with synthetic poses, to the target
domain, which is trained on real data. Importantly, the real data training only
uses pseudo-poses estimated by pseudo-keypoints, and thereby requires no real
groundtruth data annotations. RKHSPose achieves state-of-the-art performance on
three commonly used 6DoF PE datasets including LINEMOD (+4.2%), Occlusion
LINEMOD (+2%), and YCB-Video (+3%). It also compares favorably to fully
supervised methods on all six applicable BOP core datasets, achieving within
-10.8% to -0.3% of the top fully supervised results.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09502" title="Abstract">arXiv:2311.09502</a> [<a href="/pdf/2311.09502" title="Download PDF">pdf</a>, <a href="/format/2311.09502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SQATIN: Supervised Instruction Tuning Meets Question Answering for  Improved Dialogue NLU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razumovskaia%2C+E">Evgeniia Razumovskaia</a>, 
<a href="/search/cs?searchtype=author&query=Glava%C5%A1%2C+G">Goran Glava&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Task-oriented dialogue (ToD) systems help users execute well-defined tasks
across a variety of domains (e.g., $\textit{flight booking}$ or $\textit{food
ordering}$), with their Natural Language Understanding (NLU) components being
dedicated to the analysis of user utterances, predicting users' intents
($\textit{Intent Detection}$, ID) and extracting values for informational slots
($\textit{Value Extraction}$, VE). In most domains, labelled NLU data is
scarce, making sample-efficient learning -- enabled with effective transfer
paradigms -- paramount. In this work, we introduce SQATIN, a new framework for
dialog NLU based on (i) instruction tuning and (ii) question-answering-based
formulation of ID and VE tasks. According to the evaluation on established NLU
benchmarks, SQATIN sets the new state of the art in dialogue NLU, substantially
surpassing the performance of current models based on standard fine-tuning
objectives in both in-domain training and cross-domain transfer. SQATIN yields
particularly large performance gains in cross-domain transfer, owing to the
fact that our QA-based instruction tuning leverages similarities between
natural language descriptions of classes (i.e., slots and intents) across
domains.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09505" title="Abstract">arXiv:2311.09505</a> [<a href="/pdf/2311.09505" title="Download PDF">pdf</a>, <a href="/format/2311.09505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SegMix: A Simple Structure-Aware Data Augmentation Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pei%2C+Y">Yuxin Pei</a>, 
<a href="/search/cs?searchtype=author&query=Bhuse%2C+P">Pushkar Bhuse</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengzhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Upload of a work done in 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Interpolation-based Data Augmentation (DA) methods (Mixup) linearly
interpolate the inputs and labels of two or more training examples. Mixup has
more recently been adapted to the field of Natural Language Processing (NLP),
mainly for sequence labeling tasks. However, such a simple adoption yields
mixed or unstable improvements over the baseline models. We argue that the
direct-adoption methods do not account for structures in NLP tasks. To this
end, we propose SegMix, a collection of interpolation-based DA algorithms that
can adapt to task-specific structures. SegMix poses fewer constraints on data
structures, is robust to various hyperparameter settings, applies to more task
settings, and adds little computational overhead. In the algorithm's core, we
apply interpolation methods on task-specific meaningful segments, in contrast
to applying them on sequences as in prior work. We find SegMix to be a flexible
framework that combines rule-based DA methods with interpolation-based methods,
creating interesting mixtures of DA techniques. We show that SegMix
consistently improves performance over strong baseline models in Named Entity
Recognition (NER) and Relation Extraction (RE) tasks, especially under
data-scarce settings. Furthermore, this method is easy to implement and adds
negligible training overhead.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09506" title="Abstract">arXiv:2311.09506</a> [<a href="/pdf/2311.09506" title="Download PDF">pdf</a>, <a href="/format/2311.09506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Impact of Weight Sharing Decisions on Knowledge  Transfer in Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andle%2C+J">Josh Andle</a>, 
<a href="/search/cs?searchtype=author&query=Payani%2C+A">Ali Payani</a>, 
<a href="/search/cs?searchtype=author&query=Yasaei-Sekeh%2C+S">Salimeh Yasaei-Sekeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 Figures, 4 Tables, 2 Algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Continual Learning (CL) has generated attention as a method of avoiding
Catastrophic Forgetting (CF) in the sequential training of neural networks,
improving network efficiency and adaptability to different tasks. Additionally,
CL serves as an ideal setting for studying network behavior and Forward
Knowledge Transfer (FKT) between tasks. Pruning methods for CL train
subnetworks to handle the sequential tasks which allows us to take a structured
approach to investigating FKT. Sharing prior subnetworks' weights leverages
past knowledge for the current task through FKT. Understanding which weights to
share is important as sharing all weights can yield sub-optimal accuracy. This
paper investigates how different sharing decisions affect the FKT between
tasks. Through this lens we demonstrate how task complexity and similarity
influence the optimal weight sharing decisions, giving insights into the
relationships between tasks and helping inform decision making in similar CL
methods. We implement three sequential datasets designed to emphasize variation
in task complexity and similarity, reporting results for both ResNet-18 and
VGG-16. By sharing in accordance with the decisions supported by our findings,
we show that we can improve task accuracy compared to other sharing decisions.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09510" title="Abstract">arXiv:2311.09510</a> [<a href="/pdf/2311.09510" title="Download PDF">pdf</a>, <a href="/format/2311.09510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Size Does Not Fit All: Customizing Open-Domain Procedures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lal%2C+Y+K">Yash Kumar Lal</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Brahman%2C+F">Faeze Brahman</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+B+P">Bodhisattwa Prasad Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">How-to procedures, such as how to plant a garden, are ubiquitous. But one
size does not fit all - humans often need to customize these procedural plans
according to their specific needs, e.g., planting a garden without pesticides.
While LLMs can fluently generate generic procedures, we present the first study
on how well LLMs can customize open-domain procedures. We introduce
CustomPlans, a probe dataset of customization hints that encodes diverse user
needs for open-domain How-to procedures. Using LLMs as CustomizationAgent and
ExecutionAgent in different settings, we establish their abilities to perform
open-domain procedure customization. Human evaluation shows that using these
agents in a Sequential setting is the best, but they are good enough only ~51%
of the time. Error analysis shows that LLMs do not sufficiently address user
customization needs in their generated procedures.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09511" title="Abstract">arXiv:2311.09511</a> [<a href="/pdf/2311.09511" title="Download PDF">pdf</a>, <a href="/format/2311.09511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Systems with Symmetries using Equivariant Autoregressive  Reservoir Computers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Vides%2C+F">Fredy Vides</a>, 
<a href="/search/eess?searchtype=author&query=Nogueira%2C+I+B+R">Idelfonso B. R. Nogueira</a>, 
<a href="/search/eess?searchtype=author&query=Banegas%2C+L">Lendy Banegas</a>, 
<a href="/search/eess?searchtype=author&query=Flores%2C+E">Evelyn Flores</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">The investigation reported in this document focuses on identifying systems
with symmetries using equivariant autoregressive reservoir computers. General
results in structured matrix approximation theory are presented, exploring a
two-fold approach. Firstly, a comprehensive examination of generic
symmetry-preserving nonlinear time delay embedding is conducted. This involves
analyzing time series data sampled from an equivariant system under study.
Secondly, sparse least-squares methods are applied to discern approximate
representations of the output coupling matrices. These matrices play a pivotal
role in determining the nonlinear autoregressive representation of an
equivariant system. The structural characteristics of these matrices are
dictated by the set of symmetries inherent in the system. The document outlines
prototypical algorithms derived from the described techniques, offering insight
into their practical applications. Emphasis is placed on their effectiveness in
the identification and predictive simulation of equivariant nonlinear systems,
regardless of whether such systems exhibit chaotic behavior.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09513" title="Abstract">arXiv:2311.09513</a> [<a href="/pdf/2311.09513" title="Download PDF">pdf</a>, <a href="/ps/2311.09513" title="Download PostScript">ps</a>, <a href="/format/2311.09513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequencing Matters: A Generate-Retrieve-Generate Model for Building  Conversational Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patwardhan%2C+Q">Quinn Patwardhan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G+H">Grace Hui Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted as part of the Thirty-Second Text REtrieval Conference (TREC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper contains what the Georgetown InfoSense group has done in regard to
solving the challenges presented by TREC iKAT 2023. Our submitted runs
outperform the median runs by a significant margin, exhibiting superior
performance in nDCG across various cut numbers and in overall success rate. Our
approach uses a Generate-Retrieve-Generate method, which we've found to greatly
outpace Retrieve-Then-Generate approaches for the purposes of iKAT. Our
solution involves the use of Large Language Models (LLMs) for initial answers,
answer grounding by BM25, passage quality filtering by logistic regression, and
answer generation by LLMs again. We leverage several purpose-built Language
Models, including BERT, Chat-based, and text-to-transfer-based models, for text
understanding, classification, generation, and summarization. The official
results of the TREC evaluation contradict our initial self-evaluation, which
may suggest that a decrease in the reliance on our retrieval and classification
methods is better. Nonetheless, our findings suggest that the sequence of
involving these different components matters, where we see an essentiality of
using LLMs before using search engines.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09514" title="Abstract">arXiv:2311.09514</a> [<a href="/pdf/2311.09514" title="Download PDF">pdf</a>, <a href="/format/2311.09514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Know Thy Neighbors: A Graph Based Approach for Effective Sensor-Based  Human Activity Recognition in Smart Homes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%2C+S">Srivatsa P</a>, 
<a href="/search/cs?searchtype=author&query=Pl%C3%B6tz%2C+T">Thomas Pl&#xf6;tz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">There has been a resurgence of applications focused on Human Activity
Recognition (HAR) in smart homes, especially in the field of ambient
intelligence and assisted living technologies. However, such applications
present numerous significant challenges to any automated analysis system
operating in the real world, such as variability, sparsity, and noise in sensor
measurements. Although state-of-the-art HAR systems have made considerable
strides in addressing some of these challenges, they especially suffer from a
practical limitation: they require successful pre-segmentation of continuous
sensor data streams before automated recognition, i.e., they assume that an
oracle is present during deployment, which is capable of identifying time
windows of interest across discrete sensor events. To overcome this limitation,
we propose a novel graph-guided neural network approach that performs activity
recognition by learning explicit co-firing relationships between sensors. We
accomplish this by learning a more expressive graph structure representing the
sensor network in a smart home, in a data-driven manner. Our approach maps
discrete input sensor measurements to a feature space through the application
of attention mechanisms and hierarchical pooling of node embeddings. We
demonstrate the effectiveness of our proposed approach by conducting several
experiments on CASAS datasets, showing that the resulting graph-guided neural
network outperforms the state-of-the-art method for HAR in smart homes across
multiple datasets and by large margins. These results are promising because
they push HAR for smart homes closer to real-world applications.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09516" title="Abstract">arXiv:2311.09516</a> [<a href="/pdf/2311.09516" title="Download PDF">pdf</a>, <a href="/format/2311.09516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Adaptive Neural Network on FPGA: Enhancing Adaptability  through Dynamic Classifier Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouazzaoui%2C+A+E">Achraf El Bouazzaoui</a>, 
<a href="/search/cs?searchtype=author&query=Hadjoudja%2C+A">Abdelkader Hadjoudja</a>, 
<a href="/search/cs?searchtype=author&query=Mouhib%2C+O">Omar Mouhib</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">This research studies an adaptive neural network with a Dynamic Classifier
Selection framework on Field-Programmable Gate Arrays (FPGAs). The evaluations
are conducted across three different datasets. By adjusting parameters, the
architecture surpasses all models in the ensemble set in accuracy and shows an
improvement of up to 8% compared to a singular neural network implementation.
The research also emphasizes considerable resource savings of up to 109.28%,
achieved via partial reconfiguration rather than a traditional fixed approach.
Such improved efficiency suggests that the architecture is ideal for settings
limited by computational capacity, like in edge computing scenarios. The
collected data highlights the architecture's two main benefits: high
performance and real-world application, signifying a notable input to
FPGA-based ensemble learning methods.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09517" title="Abstract">arXiv:2311.09517</a> [<a href="/pdf/2311.09517" title="Download PDF">pdf</a>, <a href="/format/2311.09517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GEE! Grammar Error Explanation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yixiao Song</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K">Kalpesh Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+R">Rajesh Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Gimpel%2C+K">Kevin Gimpel</a>, 
<a href="/search/cs?searchtype=author&query=Iyyer%2C+M">Mohit Iyyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, 24 pages, code and data available in <a href="https://github.com/Yixiao-Song/GEE-with-LLMs">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Grammatical error correction tools are effective at correcting grammatical
errors in users' input sentences but do not provide users with \textit{natural
language} explanations about their errors. Such explanations are essential for
helping users learn the language by gaining a deeper understanding of its
grammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, we
propose the task of grammar error explanation, where a system needs to provide
one-sentence explanations for each grammatical error in a pair of erroneous and
corrected sentences. We analyze the capability of GPT-4 in grammar error
explanation, and find that it only produces explanations for 60.2% of the
errors using one-shot prompting. To improve upon this performance, we develop a
two-step pipeline that leverages fine-tuned and prompted large language models
to perform structured atomic token edit extraction, followed by prompting GPT-4
to generate explanations. We evaluate our pipeline on German and Chinese
grammar error correction data sampled from language learners with a wide range
of proficiency levels. Human evaluation reveals that our pipeline produces
93.9% and 98.0% correct explanations for German and Chinese data, respectively.
To encourage further research in this area, we will open-source our data and
code.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09518" title="Abstract">arXiv:2311.09518</a> [<a href="/pdf/2311.09518" title="Download PDF">pdf</a>, <a href="/format/2311.09518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From GPT-3 to GPT-4: On the Evolving Efficacy of LLMs to Answer  Multiple-choice Questions for Programming Classes in Higher Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Savelka%2C+J">Jaromir Savelka</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Arav Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Bogart%2C+C">Christopher Bogart</a>, 
<a href="/search/cs?searchtype=author&query=Sakr%2C+M">Majd Sakr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2303.08033">arXiv:2303.08033</a>, <a href="/abs/2306.10073">arXiv:2306.10073</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">We explore the evolving efficacy of three generative pre-trained transformer
(GPT) models in generating answers for multiple-choice questions (MCQ) from
introductory and intermediate Python programming courses in higher education.
We focus on the differences in capabilities of the models prior to the release
of ChatGPT (Nov '22), at the time of the release, and today (i.e., Aug '23).
Recent studies have established that the abilities of the OpenAI's GPT models
to handle assessments originally designed for humans keep increasing as the
newer more capable models are released. However, the qualitative differences in
the capabilities and limitations of these models to reason about and/or analyze
programming MCQs have been under-explored. We evaluated three OpenAI's GPT
models on formative and summative MCQ assessments from three Python courses
(530 questions) focusing on the qualitative differences in the evolving
efficacy of the subsequent models. This study provides further evidence and
insight into the trajectory of the current developments where there already
exists a technology that can be utilized by students to collect passing scores,
with no effort whatsoever, on what today counts as viable programming knowledge
and skills assessments. This study could be leveraged by educators and
institutions to better understand the recent technological developments in
order to adapt the design of programming assessments as well as to fuel the
necessary discussions into how assessments in future programming classes should
be updated.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09519" title="Abstract">arXiv:2311.09519</a> [<a href="/pdf/2311.09519" title="Download PDF">pdf</a>, <a href="/format/2311.09519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Code to Improve In-context Learning for Semantic Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogin%2C+B">Ben Bogin</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shivanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>, 
<a href="/search/cs?searchtype=author&query=Sabharwal%2C+A">Ashish Sabharwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-context learning (ICL) is an appealing approach for semantic parsing due
to its few-shot nature and improved generalization. However, learning to parse
to rare domain-specific languages (DSLs) from just a few demonstrations is
challenging, limiting the performance of even the most capable LLMs. In this
work, we improve the effectiveness of ICL for semantic parsing by (1) using
general-purpose programming languages such as Python instead of DSLs, and (2)
augmenting prompts with a structured domain description that includes, e.g.,
the available classes and functions. We show that both these changes
significantly improve accuracy across three popular datasets. Combined, they
lead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional
split), nearly closing the performance gap between easier i.i.d.\ and harder
compositional splits when used with a strong model, and reducing the need for a
large number of demonstrations. We find that the resemblance of the target
parse language to general-purpose code is a more important factor than the
language's popularity in pre-training corpora. Our findings provide an improved
methodology for building semantic parsers in the modern context of ICL with
LLMs.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09520" title="Abstract">arXiv:2311.09520</a> [<a href="/pdf/2311.09520" title="Download PDF">pdf</a>, <a href="/format/2311.09520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MDFL: Multi-domain Diffusion-driven Feature Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daixun Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weiying Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunsong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">High-dimensional images, known for their rich semantic information, are
widely applied in remote sensing and other fields. The spatial information in
these images reflects the object's texture features, while the spectral
information reveals the potential spectral representations across different
bands. Currently, the understanding of high-dimensional images remains limited
to a single-domain perspective with performance degradation. Motivated by the
masking texture effect observed in the human visual system, we present a
multi-domain diffusion-driven feature learning network (MDFL) , a scheme to
redefine the effective information domain that the model really focuses on.
This method employs diffusion-based posterior sampling to explicitly consider
joint information interactions between the high-dimensional manifold structures
in the spectral, spatial, and frequency domains, thereby eliminating the
influence of masking texture effects in visual models. Additionally, we
introduce a feature reuse mechanism to gather deep and raw features of
high-dimensional data. We demonstrate that MDFL significantly improves the
feature extraction performance of high-dimensional data, thereby providing a
powerful aid for revealing the intrinsic patterns and structures of such data.
The experimental results on three multi-modal remote sensing datasets show that
MDFL reaches an average overall accuracy of 98.25%, outperforming various
state-of-the-art baseline schemes. The code will be released, contributing to
the computer vision community.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09521" title="Abstract">arXiv:2311.09521</a> [<a href="/pdf/2311.09521" title="Download PDF">pdf</a>, <a href="/format/2311.09521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMRFact: Enhancing Summarization Factuality Evaluation with AMR-driven  Training Data Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Haoyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kung-Hsiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Jingnong Qu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Ensuring factual consistency is crucial in various natural language
processing tasks, particularly in abstractive summarization, where preserving
the integrity of information is paramount. Prior entailment-based approaches
often generate factually inconsistent summaries and then train a classifier on
the generated data. However, summaries produced by these approaches are either
of low coherence or lack error-type coverage. To address these issues, we
propose AMRFact, a novel framework that generates factually inconsistent
summaries using Abstract Meaning Representation (AMR). Our approach parses
factually correct summaries into AMR graphs and injects controlled factual
inconsistencies to create negative examples, allowing for coherent factually
inconsistent summaries to be generated with high error-type coverage.
Additionally, we present a data selection module NegFilter based on natural
language inference and BARTScore to ensure the quality of the generated
negative samples. Experimental results demonstrate that our approach
significantly outperforms previous systems on the AggreFact-SOTA dataset,
showcasing its efficacy in assessing factuality in abstractive summarization.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09522" title="Abstract">arXiv:2311.09522</a> [<a href="/pdf/2311.09522" title="Download PDF">pdf</a>, <a href="/format/2311.09522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reversed Indexes $\approx$ Values in Wavelet Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xiangjun Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This work presents a discovery to advance the wisdom in a particular Succinct
Data Structure: Wavelet Tree (Grossi, Gupta, and Vitter 2003). The discovery is
first made by showing the feasibility of Reversed Indexes = Values: for
integers within $[0,2^{N})$, there exists a Wavelet Tree that its compressed
indexes can be equivalent to the Leibniz Binary system (Leibniz 1703), with
only the bit reversal. Then we show how to strengthen the discovery by
generalizing it into Reversed Indexes $\approx$ Values, by applying a longest
common subsequence in bits and its patterns. Finally, we conjuncture potential
implications of the above ideas by discussing its benefits, and modifications
to the RAM model.
<br />The discovery reveals that: (1) the usability of Succinct Data Structure can
be significantly expanded, by enabling Computation Directly on Compression; and
(2) near-optimal lossless compression can still yield close connections with
the Leibniz Binary System (Leibniz 1703), which breeds polymorphic
functionalities within a single piece of the information. This work also
provides an initial analysis of the benefits from the method (and potentially
other extensions), and suggests potential modifications.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09525" title="Abstract">arXiv:2311.09525</a> [<a href="/pdf/2311.09525" title="Download PDF">pdf</a>, <a href="/format/2311.09525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NGEL-SLAM: Neural Implicit Representation-based Global Consistent  Low-Latency SLAM System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yunxuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+R">Rong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yiyi Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures, 2024 ICRA under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Neural implicit representations have emerged as a promising solution for
providing dense geometry in Simultaneous Localization and Mapping (SLAM).
However, existing methods in this direction fall short in terms of global
consistency and low latency. This paper presents NGEL-SLAM to tackle the above
challenges. To ensure global consistency, our system leverages a traditional
feature-based tracking module that incorporates loop closure. Additionally, we
maintain a global consistent map by representing the scene using multiple
neural implicit fields, enabling quick adjustment to the loop closure.
Moreover, our system allows for fast convergence through the use of
octree-based implicit representations. The combination of rapid response to
loop closure and fast convergence makes our system a truly low-latency system
that achieves global consistency. Our system enables rendering high-fidelity
RGB-D images, along with extracting dense and complete surfaces. Experiments on
both synthetic and real-world datasets suggest that our system achieves
state-of-the-art tracking and mapping accuracy while maintaining low latency.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09526" title="Abstract">arXiv:2311.09526</a> [<a href="/pdf/2311.09526" title="Download PDF">pdf</a>, <a href="/format/2311.09526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Serverless Optimization with In-place Scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+V">Vincent Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Chou%2C+J">Jerry Chou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Serverless computing has gained popularity due to its cost efficiency, ease
of deployment, and enhanced scalability. However, in serverless environments,
servers are initiated only after receiving a request, leading to increased
response times. This delay is commonly known as the cold start problem. In this
study, we explore the in-place scaling feature released in Kubernetes v1.27 and
examine its impact on serverless computing. Our experimental results reveal
improvements in request latency, with reductions ranging from 1.16 to 18.15
times across various workloads when compared to traditional cold policy.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09528" title="Abstract">arXiv:2311.09528</a> [<a href="/pdf/2311.09528" title="Download PDF">pdf</a>, <a href="/format/2311.09528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhilin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Jiaqi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+V">Virginia Adams</a>, 
<a href="/search/cs?searchtype=author&query=Sreedhar%2C+M+N">Makesh Narsimhan Sreedhar</a>, 
<a href="/search/cs?searchtype=author&query=Egert%2C+D">Daniel Egert</a>, 
<a href="/search/cs?searchtype=author&query=Delalleau%2C+O">Olivier Delalleau</a>, 
<a href="/search/cs?searchtype=author&query=Scowcroft%2C+J+P">Jane Polak Scowcroft</a>, 
<a href="/search/cs?searchtype=author&query=Kant%2C+N">Neel Kant</a>, 
<a href="/search/cs?searchtype=author&query=Swope%2C+A">Aidan Swope</a>, 
<a href="/search/cs?searchtype=author&query=Kuchaiev%2C+O">Oleksii Kuchaiev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing open-source helpfulness preference datasets do not specify what
makes some responses more helpful and others less so. Models trained on these
datasets can incidentally learn to model dataset artifacts (e.g. preferring
longer but unhelpful responses only due to their length). To alleviate this
problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated
for the various aspects that make responses helpful. Specifically, our
37k-sample dataset has annotations for correctness, coherence, complexity, and
verbosity in addition to overall helpfulness of responses. Training Llama 2 70B
using the HelpSteer dataset with SteerLM technique produces a model that scores
7.54 on MT Bench, which is currently the highest score for open models that do
not require training data from more powerful models (e.g. GPT4). We release
this dataset with CC-BY-4.0 license at
https://huggingface.co/datasets/nvidia/HelpSteer
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09529" title="Abstract">arXiv:2311.09529</a> [<a href="/pdf/2311.09529" title="Download PDF">pdf</a>, <a href="/format/2311.09529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransCrimeNet: A Transformer-Based Model for Text-Based Crime Prediction  in Criminal Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chen Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">This paper presents TransCrimeNet, a novel transformer-based model for
predicting future crimes in criminal networks from textual data. Criminal
network analysis has become vital for law enforcement agencies to prevent
crimes. However, existing graph-based methods fail to effectively incorporate
crucial textual data like social media posts and interrogation transcripts that
provide valuable insights into planned criminal activities. To address this
limitation, we develop TransCrimeNet which leverages the representation
learning capabilities of transformer models like BERT to extract features from
unstructured text data. These text-derived features are fused with graph
embeddings of the criminal network for accurate prediction of future crimes.
Extensive experiments on real-world criminal network datasets demonstrate that
TransCrimeNet outperforms previous state-of-the-art models by 12.7\% in F1
score for crime prediction. The results showcase the benefits of combining
textual and graph-based features for actionable insights to disrupt criminal
enterprises.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09532" title="Abstract">arXiv:2311.09532</a> [<a href="/pdf/2311.09532" title="Download PDF">pdf</a>, <a href="/format/2311.09532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightEMU: Hardware Assisted Fuzzing of Trusted Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shan%2C+H">Haoqi Shan</a>, 
<a href="/search/cs?searchtype=author&query=Nissankararao%2C+S">Sravani Nissankararao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yujia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Moyao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yier Jin</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+D">Dean Sullivan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by IEEE International Symposium on Hardware Oriented Security and Trust (HOST'2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Trusted Execution Environments (TEEs) are deployed in many CPU designs
because of the confidentiality and integrity guarantees they provide. ARM
TrustZone is a TEE extensively deployed on smart phones, IoT devices, and
notebooks. Specifically, TrustZone is used to separate code execution and data
into two worlds, normal world and secure world. However, this separation
inherently prevents traditional fuzzing approaches which rely upon
coverage-guided feedback and existing fuzzing research is, therefore, extremely
limited. In this paper, we present a native and generic method to perform
efficient and scalable feedback-driven fuzzing on Trusted Applications (TAs)
using ARM CoreSight. We propose LightEMU, a novel fuzzing framework that allows
us to fuzz TAs by decoupling them from relied TEE. We argue that LightEMU is a
promising first-stage approach for rapidly discovering TA vulnerabilities prior
to investing effort in whole system TEE evaluation precisely because the
majority of publicly disclosed TrustZone bugs reside in the TA code itself. We
implement LightEMU and adapt it to Teegris, Trusty, OP-TEE and QSEE and
evaluate 8 real-world TAs while triggering 3 unique crashes and achieving x10
time speedup when fuzzing TAs using the state-of-the-art TrustZone fuzzing
framework.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09533" title="Abstract">arXiv:2311.09533</a> [<a href="/pdf/2311.09533" title="Download PDF">pdf</a>, <a href="/format/2311.09533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Large Language Model Adaptation for Improved Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoxi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Arik%2C+S+%C3%96">Sercan &#xd6;. Arik</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+T">Tomas Pfister</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have achieved remarkable advancements in natural
language understanding, generation, and manipulation of text-based data.
However, one major issue towards their widespread deployment in the real world
is that they can generate "hallucinated" answers that are not factual. Towards
this end, this paper focuses on improving grounding from a holistic perspective
with a novel framework, AGREE, Adaptation of LLMs for GRounding EnhancEment. We
start with the design of an iterative test-time adaptation (TTA) capability
that takes into account the support information generated in self-grounded
responses. To effectively enable this capability, we tune LLMs to ground the
claims in their responses to retrieved documents by providing citations. This
tuning on top of the pre-trained LLMs requires a small amount of data that
needs to be constructed in a particular way to learn the grounding information,
for which we introduce a data construction method. Our results show that the
tuning-based AGREE framework generates better grounded responses with more
accurate citations compared to prompting-based approaches.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09535" title="Abstract">arXiv:2311.09535</a> [<a href="/pdf/2311.09535" title="Download PDF">pdf</a>, <a href="/format/2311.09535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FunctionMarker: Watermarking Language Datasets via Knowledge Injection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kejiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Kunsheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated superior performance in
various natural language processing tasks. Meanwhile, they require extensive
training data, raising concerns related to dataset copyright protection.
Backdoor-based watermarking is a viable approach to protect the copyright of
classification datasets. However, these methods may introduce malicious
misclassification behaviors into watermarked LLMs by attackers and also affect
the semantic information of the watermarked text. To address these issues, we
propose FunctionMarker, a novel copyright protection method for language
datasets via knowledge injection. FunctionMarker enables LLMs to learn specific
knowledge through fine-tuning on watermarked datasets, and we can extract the
embedded watermark by obtaining the responses of LLMs to specific
knowledge-related queries. Considering watermark capacity and stealthness, we
select customizable functions as specific knowledge for LLMs to learn and embed
the watermark into them. Moreover, FunctionMarker can embed multi-bit
watermarks while preserving the original semantic information, thereby
increasing the difficulty of adaptive attacks. We take mathematical functions
as an instance to evaluate the effectiveness of FunctionMarker, and experiments
show that only 0.3% of watermarked text achieves a 90% watermark extraction
accuracy in most cases, validating our method's effectiveness.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09537" title="Abstract">arXiv:2311.09537</a> [<a href="/pdf/2311.09537" title="Download PDF">pdf</a>, <a href="/format/2311.09537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Future Full-Ocean Deep SSPs Prediction based on Hierarchical Long  Short-Term Memory Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiajun Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Pengfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sijia Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2310.09522">arXiv:2310.09522</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">The spatial-temporal distribution of underwater sound velocity affects the
propagation mode of underwater acoustic signals. Therefore, rapid estimation
and prediction of underwater sound velocity distribution is crucial for
providing underwater positioning, navigation and timing (PNT) services.
Currently, sound speed profile (SSP) inversion methods have a faster time
response rate compared to direct measurement methods, however, most SSP
inversion methods focus on constructing spatial dimensional sound velocity
fields and are highly dependent on sonar observation data, thus high
requirements have been placed on observation data sources. To explore the
distribution pattern of sound velocity in the time dimension and achieve future
SSP prediction without sonar observation data, we propose a hierarchical long
short-term memory (H-LSTM) neural network for SSP prediction. By our SSP
prediction method, the sound speed distribution could be estimated without any
on-site data measurement process, so that the time efficiency could be greatly
improved. Through comparing with other state-of-the-art methods, H-LSTM has
better accuracy performance on prediction of monthly average sound velocity
distribution, which is less than 1 m/s in different depth layers.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09538" title="Abstract">arXiv:2311.09538</a> [<a href="/pdf/2311.09538" title="Download PDF">pdf</a>, <a href="/format/2311.09538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Privacy Risks in Online Self-Disclosures with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dou%2C+Y">Yao Dou</a>, 
<a href="/search/cs?searchtype=author&query=Krsek%2C+I">Isadora Krsek</a>, 
<a href="/search/cs?searchtype=author&query=Naous%2C+T">Tarek Naous</a>, 
<a href="/search/cs?searchtype=author&query=Kabra%2C+A">Anubha Kabra</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sauvik Das</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LLMs, Privacy, HCI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Self-disclosure, while being common and rewarding in social media
interaction, also poses privacy risks. In this paper, we take the initiative to
protect the user-side privacy associated with online self-disclosure through
identification and abstraction. We develop a taxonomy of 19 self-disclosure
categories, and curate a large corpus consisting of 4.8K annotated disclosure
spans. We then fine-tune a language model for identification, achieving over
75% in Token F$_1$. We further conduct a HCI user study, with 82\% of
participants viewing the model positively, highlighting its real world
applicability. Motivated by the user feedback, we introduce the task of
self-disclosure abstraction. We experiment with both one-span abstraction and
three-span abstraction settings, and explore multiple fine-tuning strategies.
Our best model can generate diverse abstractions that moderately reduce privacy
risks while maintaining high utility according to human evaluation.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09540" title="Abstract">arXiv:2311.09540</a> [<a href="/pdf/2311.09540" title="Download PDF">pdf</a>, <a href="/format/2311.09540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedFusion: Manifold Driven Federated Learning for Multi-satellite and  Multi-modality Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">DaiXun Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weiying Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunsong Li</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+L">Leyuan Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-satellite, multi-modality in-orbit fusion is a challenging task as it
explores the fusion representation of complex high-dimensional data under
limited computational resources. Deep neural networks can reveal the underlying
distribution of multi-modal remote sensing data, but the in-orbit fusion of
multimodal data is more difficult because of the limitations of different
sensor imaging characteristics, especially when the multimodal data follows
non-independent identically distribution (Non-IID) distributions. To address
this problem while maintaining classification performance, this paper proposes
a manifold-driven multi-modality fusion framework, FedFusion, which randomly
samples local data on each client to jointly estimate the prominent manifold
structure of shallow features of each client and explicitly compresses the
feature matrices into a low-rank subspace through cascading and additive
approaches, which is used as the feature input of the subsequent classifier.
Considering the physical space limitations of the satellite constellation, we
developed a multimodal federated learning module designed specifically for
manifold data in a deep latent space. This module achieves iterative updating
of the sub-network parameters of each client through global weighted averaging,
constructing a framework that can represent compact representations of each
client. The proposed framework surpasses existing methods in terms of
performance on three multimodal datasets, achieving a classification average
accuracy of 94.35$\%$ while compressing communication costs by a factor of 4.
Furthermore, extensive numerical evaluations of real-world satellite images
were conducted on the orbiting edge computing architecture based on Jetson TX2
industrial modules, which demonstrated that FedFusion significantly reduced
training time by 48.4 minutes (15.18%) while optimizing accuracy.}
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09542" title="Abstract">arXiv:2311.09542</a> [<a href="/pdf/2311.09542" title="Download PDF">pdf</a>, <a href="/format/2311.09542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Pragmatic Awareness in Question Answering: A Case Study in  Maternal and Infant Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srikanth%2C+N">Neha Srikanth</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+R">Rupak Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Rudinger%2C+R">Rachel Rudinger</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J">Jordan Boyd-Graber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Questions posed by information-seeking users often contain implicit false or
potentially harmful assumptions. In a high-risk domain such as maternal and
infant health, a question-answering system must recognize these pragmatic
constraints and go beyond simply answering user questions, examining them in
context to respond helpfully. To achieve this, we study pragmatic inferences
made when mothers ask questions about pregnancy and infant care. Some of the
inferences in these questions evade detection by existing methods, risking the
possibility of QA systems failing to address them which can have dangerous
health and policy implications. We explore the viability of detecting
inferences from questions using large language models and illustrate that
informing existing QA pipelines with pragmatic inferences produces responses
that can mitigate the propagation of harmful beliefs.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09543" title="Abstract">arXiv:2311.09543</a> [<a href="/pdf/2311.09543" title="Download PDF">pdf</a>, <a href="/format/2311.09543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Ming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jian%2C+W">Weihua Jian</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+P">Pengfei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongyuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Though significant progress in human pose and shape recovery from monocular
RGB images has been made in recent years, obtaining 3D human motion with high
accuracy and temporal consistency from videos remains challenging. Existing
video-based methods tend to reconstruct human motion from global image
features, which lack detailed representation capability and limit the
reconstruction accuracy. In this paper, we propose a Temporal-Aware Refining
Network (TAR), to synchronously explore temporal-aware global and local image
features for accurate pose and shape recovery. First, a global transformer
encoder is introduced to obtain temporal global features from static feature
sequences. Second, a bidirectional ConvGRU network takes the sequence of
high-resolution feature maps as input, and outputs temporal local feature maps
that maintain high resolution and capture the local motion of the human body.
Finally, a recurrent refinement module iteratively updates estimated SMPL
parameters by leveraging both global and local temporal information to achieve
accurate and smooth results. Extensive experiments demonstrate that our TAR
obtains more accurate results than previous state-of-the-art methods on popular
benchmarks, i.e., 3DPW, MPI-INF-3DHP, and Human3.6M.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09544" title="Abstract">arXiv:2311.09544</a> [<a href="/pdf/2311.09544" title="Download PDF">pdf</a>, <a href="/format/2311.09544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling User Modeling: Large-scale Online User Representations for Ads  Personalization in Meta
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dai Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuewei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ru Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yaning Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Dong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Min Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fenggang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minghai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huayu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yunnan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Z">Zhan Shu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mindi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Sri Reddy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Effective user representations are pivotal in personalized advertising.
However, stringent constraints on training throughput, serving latency, and
memory, often limit the complexity and input feature set of online ads ranking
models. This challenge is magnified in extensive systems like Meta's, which
encompass hundreds of models with diverse specifications, rendering the
tailoring of user representation learning for each model impractical. To
address these challenges, we present Scaling User Modeling (SUM), a framework
widely deployed in Meta's ads ranking system, designed to facilitate efficient
and scalable sharing of online user representation across hundreds of ads
models. SUM leverages a few designated upstream user models to synthesize user
embeddings from massive amounts of user features with advanced modeling
techniques. These embeddings then serve as inputs to downstream online ads
ranking models, promoting efficient representation sharing. To adapt to the
dynamic nature of user features and ensure embedding freshness, we designed SUM
Online Asynchronous Platform (SOAP), a latency free online serving system
complemented with model freshness and embedding stabilization, which enables
frequent user model updates and online inference of user embeddings upon each
user request. We share our hands-on deployment experiences for the SUM
framework and validate its superiority through comprehensive experiments. To
date, SUM has been launched to hundreds of ads ranking models in Meta,
processing hundreds of billions of user requests daily, yielding significant
online metric gains and infrastructure cost savings.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09548" title="Abstract">arXiv:2311.09548</a> [<a href="/pdf/2311.09548" title="Download PDF">pdf</a>, <a href="/format/2311.09548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universally Optimal Information Dissemination and Shortest Paths in the  HYBRID Distributed Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi-Jun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hecht%2C+O">Oren Hecht</a>, 
<a href="/search/cs?searchtype=author&query=Leitersdorf%2C+D">Dean Leitersdorf</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+P">Philipp Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work combines the three preprints <a href="/abs/2304.06317">arXiv:2304.06317</a>, <a href="/abs/2304.07107">arXiv:2304.07107</a>, and <a href="/abs/2306.05977">arXiv:2306.05977</a> with some improved results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In this work we consider the HYBRID model of distributed computing,
introduced recently by Augustine, Hinnenthal, Kuhn, Scheideler, and Schneider
(SODA 2020), where nodes have access to two different communication modes:
high-bandwidth local communication along the edges of the graph and
low-bandwidth all-to-all communication, capturing the non-uniform nature of
modern communication networks.
<br />Prior work in HYBRID has focused on showing existentially optimal algorithms,
meaning there exists a pathological family of instances on which no algorithm
can do better. This neglects the fact that such worst-case instances often do
not appear or can be actively avoided in practice. In this work, we focus on
the notion of universal optimality, first raised by Garay, Kutten, and Peleg
(FOCS 1993). Roughly speaking, a universally optimal algorithm is one that,
given any input graph, runs as fast as the best algorithm designed specifically
for that graph.
<br />We show the first universally optimal algorithms in HYBRID. We present
universally optimal solutions for fundamental information dissemination tasks,
such as broadcasting and unicasting multiple messages in HYBRID. Furthermore,
we apply these tools to obtain universally optimal solutions for various
shortest paths problems in HYBRID.
<br />A main conceptual contribution of this work is the conception of a new graph
parameter called neighborhood quality that captures the inherent complexity of
many fundamental graph problems in HYBRID.
<br />We also show new existentially optimal shortest paths algorithms in HYBRID,
which are utilized as key subroutines in our universally optimal algorithms and
are of independent interest. Our new algorithms for $k$-source shortest paths
match the existing $\tilde{\Omega}(\sqrt{k})$ lower bound for all $k$.
Previously, the lower bound was only known to be tight when $k \in
\tilde{\Omega}(n^{2/3})$.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09549" title="Abstract">arXiv:2311.09549</a> [<a href="/pdf/2311.09549" title="Download PDF">pdf</a>, <a href="/format/2311.09549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SparseAuto: An Auto-Scheduler for Sparse Tensor Computations Using  Recursive Loop Nest Restructuring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dias%2C+A">Adhitha Dias</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+L">Logan Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Sundararajah%2C+K">Kirshanthan Sundararajah</a>, 
<a href="/search/cs?searchtype=author&query=Pelenitsyn%2C+A">Artem Pelenitsyn</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+M">Milind Kulkarni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Automated code generation and performance optimizations for sparse tensor
algebra are cardinal since they have become essential in many real-world
applications like quantum computing, physics, chemistry, and machine learning.
General sparse tensor algebra compilers are not always versatile enough to
generate asymptotically optimal code for sparse tensor contractions. This paper
shows how to optimize and generate asymptotically better schedules for complex
tensor expressions using kernel fission and fusion. We present a generalized
loop transformation to achieve loop nesting for minimized memory footprint and
reduced asymptotic complexity.
<br />Furthermore, we present an auto-scheduler that uses a partially ordered
set-based cost model that uses both time and auxiliary memory complexities in
its pruning stages. In addition, we highlight the use of SMT solvers in sparse
auto-schedulers to prune the Pareto frontier of schedules to the smallest
number of possible schedules with user-defined constraints available at compile
time. Finally, we show that our auto-scheduler can select asymptotically better
schedules that use our compiler transformation to generate optimized code. Our
results show that the auto-scheduler achieves orders of magnitude speedup
compared to the TACO-generated code for several real-world tensor algebra
computations on different real-world inputs.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09550" title="Abstract">arXiv:2311.09550</a> [<a href="/pdf/2311.09550" title="Download PDF">pdf</a>, <a href="/format/2311.09550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Speed Odyssey for Deployable Quantization of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+R">Ran Meng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiduo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yifan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiangxiang Chu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yerui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuchen Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The large language model era urges faster and less costly inference. Prior
model compression works on LLMs tend to undertake a software-centric approach
primarily focused on the simulated quantization performance. By neglecting the
feasibility of deployment, these approaches are typically disabled in real
practice. They used to drastically push down the quantization bit range for a
reduced computation which might not be supported by the mainstream hardware, or
involve sophisticated algorithms that introduce extra computation or memory
access overhead. We argue that pursuing a hardware-centric approach in the
construction of quantization algorithms is crucial. In this regard, we are
driven to build our compression method on top of hardware awareness,
eliminating impractical algorithm choices while maximizing the benefit of
hardware acceleration. Our method, OdysseyLLM, comes with a novel W4A8 kernel
implementation called FastGEMM and a combined recipe of quantization
strategies. Extensive experiments manifest the superiority of our W4A8 method
which brings the actual speed boosting up to \textbf{4$\times$} compared to
Hugging Face FP16 inference and \textbf{2.23$\times$} vs. the state-of-the-art
inference engine TensorRT-LLM in FP16, and \textbf{1.45$\times$} vs.
TensorRT-LLM in INT8, yet without substantially harming the performance.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09552" title="Abstract">arXiv:2311.09552</a> [<a href="/pdf/2311.09552" title="Download PDF">pdf</a>, <a href="/format/2311.09552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Few-Shot Training Example Generators: A Case  Study in Fallacy Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alhindi%2C+T">Tariq Alhindi</a>, 
<a href="/search/cs?searchtype=author&query=Muresan%2C+S">Smaranda Muresan</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recognizing fallacies is crucial for ensuring the quality and validity of
arguments across various domains. However, computational fallacy recognition
faces challenges due to the diverse genres, domains, and types of fallacies
found in datasets. This leads to a highly multiclass, and even multi-label,
setup with substantial class imbalance. In this study, we aim to enhance
existing models for fallacy recognition by incorporating additional context and
by leveraging large language models to generate synthetic data, thus increasing
the representation of the infrequent classes. We experiment with GPT3.5 to
generate synthetic examples and we examine the impact of prompt settings for
this. Moreover, we explore zero-shot and few-shot scenarios to evaluate the
effectiveness of using the generated examples for training smaller models
within a unified fallacy recognition framework. Furthermore, we analyze the
overlap between the synthetic data and existing fallacy datasets. Finally, we
investigate the usefulness of providing supplementary context for detecting
fallacy types that need such context, e.g., diversion fallacies. Our evaluation
results demonstrate consistent improvements across fallacy types, datasets, and
generators.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09553" title="Abstract">arXiv:2311.09553</a> [<a href="/pdf/2311.09553" title="Download PDF">pdf</a>, <a href="/format/2311.09553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Program-Aided Reasoners (better) Know What They Know
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kabra%2C+A">Anubha Kabra</a>, 
<a href="/search/cs?searchtype=author&query=Rangreji%2C+S">Sanketh Rangreji</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+Y">Yash Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+A">Aman Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+E">Emmy Liu</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Prior work shows that program-aided reasoning, in which large language models
(LLMs) are combined with programs written in programming languages such as
Python, can significantly improve accuracy on various reasoning tasks. However,
while accuracy is essential, it is also important for such reasoners to "know
what they know", which can be quantified through the calibration of the model.
In this paper, we compare the calibration of Program Aided Language Models
(PAL) and text-based Chain-of-thought (COT) prompting techniques over 5
datasets and 2 model types: LLaMA models and OpenAI models. Our results
indicate that PAL leads to improved calibration in 75% of the instances. Our
analysis uncovers that prompting styles that produce lesser diversity in
generations also have more calibrated results, and thus we also experiment with
inducing lower generation diversity using temperature scaling and find that for
certain temperatures, PAL is not only more accurate but is also more calibrated
than COT. Overall, we demonstrate that, in the majority of cases, program-aided
reasoners better know what they know than text-based counterparts.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09555" title="Abstract">arXiv:2311.09555</a> [<a href="/pdf/2311.09555" title="Download PDF">pdf</a>, <a href="/format/2311.09555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft and Rigid Object Grasping With Cross-Structure Hand Using Bilateral  Control-Based Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamane%2C+K">Koki Yamane</a>, 
<a href="/search/cs?searchtype=author&query=Sakaino%2C+S">Sho Sakaino</a>, 
<a href="/search/cs?searchtype=author&query=Tsuji%2C+T">Toshiaki Tsuji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 11 figures, Submitted to IEEE RA-L
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Object grasping is an important ability required for various robot tasks. In
particular, tasks that require precise force adjustments during operation, such
as grasping an unknown object or using a grasped tool, are difficult for humans
to program in advance. Recently, AI-based algorithms that can imitate human
force skills have been actively explored as a solution. In particular,
bilateral control-based imitation learning achieves human-level motion speeds
with environmental adaptability, only requiring human demonstration and without
programming. However, owing to hardware limitations, its grasping performance
remains limited, and tasks that involves grasping various objects are yet to be
achieved. Here, we developed a cross-structure hand to grasp various objects.
We experimentally demonstrated that the integration of bilateral control-based
imitation learning and the cross-structure hand is effective for grasping
various objects and harnessing tools.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09558" title="Abstract">arXiv:2311.09558</a> [<a href="/pdf/2311.09558" title="Download PDF">pdf</a>, <a href="/format/2311.09558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pachinko: Patching Interpretable QA Models through Natural Language  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malaviya%2C+C">Chaitanya Malaviya</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Subin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Dan Roth</a>, 
<a href="/search/cs?searchtype=author&query=Yatskar%2C+M">Mark Yatskar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code &amp; data available at <a href="https://github.com/chaitanyamalaviya/pachinko">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Eliciting feedback from end users of NLP models can be beneficial for
improving models. However, how should we present model responses to users so
they are most amenable to be corrected from user feedback? Further, what
properties do users value to understand and trust responses? We answer these
questions by analyzing the effect of rationales generated by QA models to
support their answers. We specifically consider decomposed question-answering
models that first extract an intermediate rationale based on a context and a
question and then use solely this rationale to answer the question. A rationale
outlines the approach followed by the model to answer the question. Our work
considers various formats of these rationales that vary according to
well-defined properties of interest. We sample these rationales from large
language models using few-shot prompting for two reading comprehension
datasets, and then perform two user studies. In the first one, we present users
with incorrect answers and corresponding rationales of various formats and ask
them to provide natural language feedback to revise the rationale. We then
measure the effectiveness of this feedback in patching these rationales through
in-context learning. The second study evaluates how well different rationale
formats enable users to understand and trust model answers, when they are
correct. We find that rationale formats significantly affect how easy it is (1)
for users to give feedback for rationales, and (2) for models to subsequently
execute this feedback. In addition to influencing critiquablity, certain
formats significantly enhance user reported understanding and trust of model
outputs.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09559" title="Abstract">arXiv:2311.09559</a> [<a href="/pdf/2311.09559" title="Download PDF">pdf</a>, <a href="/format/2311.09559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enchancing Semi-Supervised Learning for Extractive Summarization with an  LLM-based pseudolabeler
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+G">Gaurav Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Vechtomova%2C+O">Olga Vechtomova</a>, 
<a href="/search/cs?searchtype=author&query=Laradji%2C+I+H">Issam H. Laradji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This work tackles the task of extractive text summarization in a limited
labeled data scenario using a semi-supervised approach. Specifically, we
propose a prompt-based pseudolabel selection strategy using GPT-4. We evaluate
our method on three text summarization datasets: TweetSumm, WikiHow, and
ArXiv/PubMed. Our experiments show that by using an LLM to evaluate and
generate pseudolabels, we can improve the ROUGE-1 by 10-20\% on the different
datasets, which is akin to enhancing pretrained models. We also show that such
a method needs a smaller pool of unlabeled examples to perform better.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09562" title="Abstract">arXiv:2311.09562</a> [<a href="/pdf/2311.09562" title="Download PDF">pdf</a>, <a href="/format/2311.09562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Reevaluation of Event Extraction: Past, Present, and Future Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kuan-Hao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+I">I-Hung Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Parekh%2C+T">Tanmay Parekh</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhiyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Natarajan%2C+P">Premkumar Natarajan</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Event extraction has attracted much attention in recent years due to its
potential for many applications. However, recent studies observe some
evaluation challenges, suggesting that reported scores might not reflect the
true performance. In this work, we first identify and discuss these evaluation
challenges, including the unfair comparisons resulting from different
assumptions about data or different data preprocessing steps, the
incompleteness of the current evaluation framework leading to potential dataset
bias or data split bias, and low reproducibility of prior studies. To address
these challenges, we propose TextEE, a standardized, fair, and reproducible
benchmark for event extraction. TextEE contains standardized data preprocessing
scripts and splits for more than ten datasets across different domains. In
addition, we aggregate and re-implement over ten event extraction approaches
published in recent years and conduct a comprehensive reevaluation. Finally, we
explore the capability of large language models in event extraction and discuss
some future challenges. We expect TextEE will serve as a reliable benchmark for
event extraction, facilitating future research in the field.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09563" title="Abstract">arXiv:2311.09563</a> [<a href="/pdf/2311.09563" title="Download PDF">pdf</a>, <a href="/format/2311.09563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Objective Transmission Expansion: An Offshore Wind Power  Integration Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khanal%2C+S">Saroj Khanal</a>, 
<a href="/search/eess?searchtype=author&query=Graf%2C+C">Christoph Graf</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+Z">Zhirui Liang</a>, 
<a href="/search/eess?searchtype=author&query=Dvorkin%2C+Y">Yury Dvorkin</a>, 
<a href="/search/eess?searchtype=author&query=%C3%9Cnel%2C+B">Bur&#xe7;in &#xdc;nel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We describe a multi-objective, multistage generation, storage and
transmission expansion planning (GS&amp;TEP) model for the electric power sector,
emphasizing coordinated offshore and onshore grid planning. Unlike traditional
capacity expansion models that focus exclusively on investment and operational
costs, our model explicitly accounts for negative externalities such as the
social cost of greenhouse gas emissions and local damages from reduced air
quality. We use an 8-zone representation of the ISO-NE system to study the
sensitivity of grid expansion decisions with the primary focus on optimal
capacity expansion decisions due to large-scale offshore wind power
integration. Our model allows investment decisions to be made in multiple
stages and accounts for short-term uncertainty during operations through
scenarios. Our results indicate that considering externalities leads to greater
upfront investment in cleaner generation and storage, which are largely offset
by lower expected operational costs.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09564" title="Abstract">arXiv:2311.09564</a> [<a href="/pdf/2311.09564" title="Download PDF">pdf</a>, <a href="/format/2311.09564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parmar%2C+M">Mihir Parmar</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+A">Aakanksha Naik</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+H">Himanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+D">Disha Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Many large language models (LLMs) for medicine have largely been evaluated on
short texts, and their ability to handle longer sequences such as a complete
electronic health record (EHR) has not been systematically explored. Assessing
these models on long sequences is crucial since prior work in the general
domain has demonstrated performance degradation of LLMs on longer texts.
Motivated by this, we introduce LongBoX, a collection of seven medical datasets
in text-to-text format, designed to investigate model performance on long
sequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT)
and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We
further evaluate two techniques designed for long-sequence handling: (i)
local-global attention, and (ii) Fusion-in-Decoder (FiD). Our results
demonstrate mixed results with long-sequence handling - while scores on some
datasets increase, there is substantial room for improvement. We hope that
LongBoX facilitates the development of more effective long-sequence techniques
for the medical domain. Data and source code are available at
https://github.com/Mihir3009/LongBoX.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09566" title="Abstract">arXiv:2311.09566</a> [<a href="/pdf/2311.09566" title="Download PDF">pdf</a>, <a href="/format/2311.09566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Knowledge Distillation Approach for Sepsis Outcome Prediction from  Multivariate Clinical Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wong%2C+A">Anna Wong</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Shu Ge</a>, 
<a href="/search/cs?searchtype=author&query=Oufattole%2C+N">Nassim Oufattole</a>, 
<a href="/search/cs?searchtype=author&query=Dejl%2C+A">Adam Dejl</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+M">Megan Su</a>, 
<a href="/search/cs?searchtype=author&query=Saeedi%2C+A">Ardavan Saeedi</a>, 
<a href="/search/cs?searchtype=author&query=Lehman%2C+L+H">Li-wei H. Lehman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sepsis is a life-threatening condition triggered by an extreme infection
response. Our objective is to forecast sepsis patient outcomes using their
medical history and treatments, while learning interpretable state
representations to assess patients' risks in developing various adverse
outcomes. While neural networks excel in outcome prediction, their limited
interpretability remains a key issue. In this work, we use knowledge
distillation via constrained variational inference to distill the knowledge of
a powerful "teacher" neural network model with high predictive power to train a
"student" latent variable model to learn interpretable hidden state
representations to achieve high predictive performance for sepsis outcome
prediction. Using real-world data from the MIMIC-IV database, we trained an
LSTM as the "teacher" model to predict mortality for sepsis patients, given
information about their recent history of vital signs, lab values and
treatments. For our student model, we use an autoregressive hidden Markov model
(AR-HMM) to learn interpretable hidden states from patients' clinical time
series, and use the posterior distribution of the learned state representations
to predict various downstream outcomes, including hospital mortality, pulmonary
edema, need for diuretics, dialysis, and mechanical ventilation. Our results
show that our approach successfully incorporates the constraint to achieve high
predictive power similar to the teacher model, while maintaining the generative
performance.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09569" title="Abstract">arXiv:2311.09569</a> [<a href="/pdf/2311.09569" title="Download PDF">pdf</a>, <a href="/format/2311.09569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Optimisation with Random Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiayi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Riedel%2C+S">Sebastian Riedel</a>, 
<a href="/search/cs?searchtype=author&query=Stenetorp%2C+P">Pontus Stenetorp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary files are available at <a href="https://github.com/yaolu/random-prompt">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Using the generative nature of a language model to generate task-relevant
separators has shown competitive results compared to human-curated prompts like
"TL;DR". We demonstrate that even randomly chosen tokens from the vocabulary as
separators can achieve near-state-of-the-art performance. We analyse this
phenomenon in detail using three different random generation strategies,
establishing that the language space is rich with potential good separators,
regardless of the underlying language model size. These observations challenge
the common assumption that an effective prompt should be human-readable or
task-relevant. Experimental results show that using random separators leads to
an average 16% relative improvement across nine text classification tasks on
seven language models, compared to human-curated separators, and is on par with
automatic prompt searching methods.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09571" title="Abstract">arXiv:2311.09571</a> [<a href="/pdf/2311.09571" title="Download PDF">pdf</a>, <a href="/format/2311.09571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Decatur%2C+D">Dale Decatur</a>, 
<a href="/search/cs?searchtype=author&query=Lang%2C+I">Itai Lang</a>, 
<a href="/search/cs?searchtype=author&query=Aberman%2C+K">Kfir Aberman</a>, 
<a href="/search/cs?searchtype=author&query=Hanocka%2C+R">Rana Hanocka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://threedle.github.io/3d-paintbrush">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this work we develop 3D Paintbrush, a technique for automatically
texturing local semantic regions on meshes via text descriptions. Our method is
designed to operate directly on meshes, producing texture maps which seamlessly
integrate into standard graphics pipelines. We opt to simultaneously produce a
localization map (to specify the edit region) and a texture map which conforms
to it. This synergistic approach improves the quality of both the localization
and the stylization. To enhance the details and resolution of the textured
area, we leverage multiple stages of a cascaded diffusion model to supervise
our local editing technique with generative priors learned from images at
different resolutions. Our technique, referred to as Cascaded Score
Distillation (CSD), simultaneously distills scores at multiple resolutions in a
cascaded fashion, enabling control over both the granularity and global
understanding of the supervision. We demonstrate the effectiveness of 3D
Paintbrush to locally texture a variety of shapes within different semantic
regions. Project page: https://threedle.github.io/3d-paintbrush
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09574" title="Abstract">arXiv:2311.09574</a> [<a href="/pdf/2311.09574" title="Download PDF">pdf</a>, <a href="/format/2311.09574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LymphoML: An interpretable artificial intelligence-based method  identifies morphologic features that correlate with lymphoma subtype
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shankar%2C+V">Vivek Shankar</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoli Yang</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+V">Vrishab Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+B">Brent Tan</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+O">Oscar Silva</a>, 
<a href="/search/cs?searchtype=author&query=Rojansky%2C+R">Rebecca Rojansky</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+A">Andrew Ng</a>, 
<a href="/search/cs?searchtype=author&query=Valvert%2C+F">Fabiola Valvert</a>, 
<a href="/search/cs?searchtype=author&query=Briercheck%2C+E">Edward Briercheck</a>, 
<a href="/search/cs?searchtype=author&query=Weinstock%2C+D">David Weinstock</a>, 
<a href="/search/cs?searchtype=author&query=Natkunam%2C+Y">Yasodha Natkunam</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Pol%2C+S">Sebastian Fernandez-Pol</a>, 
<a href="/search/cs?searchtype=author&query=Rajpurkar%2C+P">Pranav Rajpurkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in Proceedings of the 3rd Machine Learning for Health symposium, Proceedings of Machine Learning Research (PMLR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&amp;E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&amp;E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML's interpretable models, developed on a
limited volume of H&amp;E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&amp;E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09576" title="Abstract">arXiv:2311.09576</a> [<a href="/pdf/2311.09576" title="Download PDF">pdf</a>, <a href="/ps/2311.09576" title="Download PostScript">ps</a>, <a href="/format/2311.09576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Work State-Centric AI Agents: Design, Implementation, and Management of  Cognitive Work Threads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">AI agents excel in executing predefined tasks, but the dynamic management of
work state information during task execution remains an underexplored area. We
propose a work state-centric AI agent model employing "work notes" to record
and reflect the state throughout task execution. This paper details the model's
architecture, featuring worker threads for task oversight, planner modules for
task decomposition and planning, and executor modules for performing subtasks
using a ReAct-inspired thought-action loop. We provide an exhaustive work state
record incorporating plans and outcomes, constituting a comprehensive work
journal. Our results show that this model not only improves task execution
efficiency but also lays a solid foundation for subsequent task analysis and
auditing.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09577" title="Abstract">arXiv:2311.09577</a> [<a href="/pdf/2311.09577" title="Download PDF">pdf</a>, <a href="/format/2311.09577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group-Aware Interest Disentangled Dual-Training for Personalized  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaolong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaohan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingdai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, 2023 IEEE International Conference on Big Data
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Personalized recommender systems aim to predict users' preferences for items.
It has become an indispensable part of online services. Online social platforms
enable users to form groups based on their common interests. The users' group
participation on social platforms reveals their interests and can be utilized
as side information to mitigate the data sparsity and cold-start problem in
recommender systems. Users join different groups out of different interests. In
this paper, we generate group representation from the user's interests and
propose IGRec (Interest-based Group enhanced Recommendation) to utilize the
group information accurately. It consists of four modules. (1) Interest
disentangler via self-gating that disentangles users' interests from their
initial embedding representation. (2) Interest aggregator that generates the
interest-based group representation by Gumbel-Softmax aggregation on the group
members' interests. (3) Interest-based group aggregation that fuses user's
representation with the participated group representation. (4) A dual-trained
rating prediction module to utilize both user-item and group-item interactions.
We conduct extensive experiments on three publicly available datasets. Results
show IGRec can effectively alleviate the data sparsity problem and enhance the
recommender system with interest-based group representation. Experiments on the
group recommendation task further show the informativeness of interest-based
group representation.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09578" title="Abstract">arXiv:2311.09578</a> [<a href="/pdf/2311.09578" title="Download PDF">pdf</a>, <a href="/format/2311.09578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Renduchintala%2C+A">Adithya Renduchintala</a>, 
<a href="/search/cs?searchtype=author&query=Konuk%2C+T">Tugrul Konuk</a>, 
<a href="/search/cs?searchtype=author&query=Kuchaiev%2C+O">Oleksii Kuchaiev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose Tied-LoRA, a simple paradigm utilizes weight tying and selective
training to further increase parameter efficiency of the Low-rank adaptation
(LoRA) method. Our investigations include all feasible combinations parameter
training/freezing in conjunction with weight tying to identify the optimal
balance between performance and the number of trainable parameters. Through
experiments covering a variety of tasks and two base language models, we
provide analysis revealing trade-offs between efficiency and performance. Our
experiments uncovered a particular Tied-LoRA configuration that stands out by
demonstrating comparable performance across several tasks while employing only
13~\% percent of parameters utilized by the standard LoRA method.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09579" title="Abstract">arXiv:2311.09579</a> [<a href="/pdf/2311.09579" title="Download PDF">pdf</a>, <a href="/format/2311.09579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crafting In-context Examples according to LMs&#x27; Parametric Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yoonsang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Atreya%2C+P">Pranav Atreya</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Eunsol Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-context learning has been applied to knowledge-rich tasks such as question
answering. In such scenarios, in-context examples are used to trigger a
behaviour in the language model: namely, it should surface information stored
in its parametric knowledge. We study the construction of in-context example
sets, with a focus on the parametric knowledge of the model regarding
in-context examples. We identify 'known' examples, where models can correctly
answer from its parametric knowledge, and 'unknown' ones. Our experiments show
that prompting with 'unknown' examples decreases the performance, potentially
as it encourages hallucination rather than searching its parametric knowledge.
Constructing an in-context example set that presents both known and unknown
information performs the best across diverse settings. We perform analysis on
three multi-answer question answering datasets, which allows us to further
study answer set ordering strategies based on the LM's knowledge about each
answer. Together, our study sheds lights on how to best construct in-context
example sets for knowledge-rich tasks.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09580" title="Abstract">arXiv:2311.09580</a> [<a href="/pdf/2311.09580" title="Download PDF">pdf</a>, <a href="/format/2311.09580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMOE: Mixture of Multimodal Interaction Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haofei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P+P">Paul Pu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Morency%2C+L">Louis-Philippe Morency</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multimodal machine learning, which studies the information and interactions
across various input modalities, has made significant advancements in
understanding the relationship between images and descriptive text. However,
this is just a portion of the potential multimodal interactions seen in the
real world and does not include new interactions between conflicting utterances
and gestures in predicting sarcasm, for example. Notably, the current methods
for capturing shared information often do not extend well to these more nuanced
interactions, sometimes performing as low as 50% in binary classification. In
this paper, we address this problem via a new approach called MMOE, which
stands for a mixture of multimodal interaction experts. Our method
automatically classifies data points from unlabeled multimodal datasets by
their interaction type and employs specialized models for each specific
interaction. Based on our experiments, this approach improves performance on
these challenging interactions by more than 10%, leading to an overall increase
of 2% for tasks like sarcasm prediction. As a result, interaction
quantification provides new insights for dataset analysis and yields simple
approaches that obtain state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09581" title="Abstract">arXiv:2311.09581</a> [<a href="/pdf/2311.09581" title="Download PDF">pdf</a>, <a href="/format/2311.09581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Medical Text Evaluation with GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yiqing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Gero%2C+Z">Zelalem Gero</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+C">Cliff Wong</a>, 
<a href="/search/cs?searchtype=author&query=Naumann%2C+T">Tristan Naumann</a>, 
<a href="/search/cs?searchtype=author&query=Poon%2C+H">Hoifung Poon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the evaluation of medical text generation, it is essential to scrutinize
each piece of information and ensure the utmost accuracy of the evaluation.
Existing evaluation metrics either focus on coarse-level evaluation that
assigns one score for the whole generated output or rely on evaluation models
trained on general domain, resulting in inaccuracies when adapted to the
medical domain. To address these issues, we propose a set of factuality-centric
evaluation aspects and design corresponding GPT-4-based metrics for medical
text generation. We systematically compare these metrics with existing ones on
clinical note generation and medical report summarization tasks, revealing low
inter-metric correlation. A comprehensive human evaluation confirms that the
proposed GPT-4-based metrics exhibit substantially higher agreement with human
judgments than existing evaluation metrics. Our study contributes to the
understanding of medical text generation evaluation and offers a more reliable
alternative to existing metrics.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09584" title="Abstract">arXiv:2311.09584</a> [<a href="/pdf/2311.09584" title="Download PDF">pdf</a>, <a href="/format/2311.09584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dichotomy Hierarchy Characterizing Linear Time Subgraph Counting in  Bounded Degeneracy Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paul-Pena%2C+D">Daniel Paul-Pena</a>, 
<a href="/search/cs?searchtype=author&query=Seshadhri%2C+C">C. Seshadhri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Subgraph and homomorphism counting are fundamental algorithmic problems.
Given a constant-sized pattern graph $H$ and a large input graph $G$, we wish
to count the number of $H$-homomorphisms/subgraphs in $G$. Given the massive
sizes of real-world graphs and the practical importance of counting problems,
we focus on when (near) linear time algorithms are possible. The seminal work
of Chiba-Nishizeki (SICOMP 1985) shows that for bounded degeneracy graphs $G$,
clique and $4$-cycle counting can be done linear time. Recent works (Bera et
al, SODA 2021, JACM 2022) show a dichotomy theorem characterizing the patterns
$H$ for which $H$-homomorphism counting is possible in linear time, for bounded
degeneracy inputs $G$. At the other end, Ne\v{s}et\v{r}il and Ossona de Mendez
used their deep theory of "sparsity" to define bounded expansion graphs. They
prove that, for all $H$, $H$-homomorphism counting can be done in linear time
for bounded expansion inputs. What lies between? For a specific $H$, can we
characterize input classes where $H$-homomorphism counting is possible in
linear time?
<br />We discover a hierarchy of dichotomy theorems that precisely answer the above
questions. We show the existence of an infinite sequence of graph classes
$\mathcal{G}_0$ $\supseteq$ $\mathcal{G}_1$ $\supseteq$ ... $\supseteq$
$\mathcal{G}_\infty$ where $\mathcal{G}_0$ is the class of bounded degeneracy
graphs, and $\mathcal{G}_\infty$ is the class of bounded expansion graphs. Fix
any constant sized pattern graph $H$. Let $LICL(H)$ denote the length of the
longest induced cycle in $H$. We prove the following. If $LICL(H) &lt; 3(r+2)$,
then $H$-homomorphisms can be counted in linear time for inputs in
$\mathcal{G}_r$. If $LICL(H) \geq 3(r+2)$, then $H$-homomorphism counting on
inputs from $\mathcal{G}_r$ takes $\Omega(m^{1+\gamma})$ time. We prove similar
dichotomy theorems for subgraph counting.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09585" title="Abstract">arXiv:2311.09585</a> [<a href="/pdf/2311.09585" title="Download PDF">pdf</a>, <a href="/format/2311.09585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LifeTox: Unveiling Implicit Toxicity in Life Advice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minbeom Kim</a>, 
<a href="/search/cs?searchtype=author&query=Koo%2C+J">Jahyun Koo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwanhee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Joonsuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwaran Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+K">Kyomin Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As large language models become increasingly integrated into daily life,
detecting implicit toxicity across diverse contexts is crucial. To this end, we
introduce LifeTox, a dataset designed for identifying implicit toxicity within
a broad range of advice-seeking scenarios. Unlike existing safety datasets,
LifeTox comprises diverse contexts derived from personal experiences through
open-ended questions. Experiments demonstrate that RoBERTa fine-tuned on
LifeTox matches or surpasses the zero-shot performance of large language models
in toxicity classification tasks. These results underscore the efficacy of
LifeTox in addressing the complex challenges inherent in implicit toxicity.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09591" title="Abstract">arXiv:2311.09591</a> [<a href="/pdf/2311.09591" title="Download PDF">pdf</a>, <a href="/ps/2311.09591" title="Download PostScript">ps</a>, <a href="/format/2311.09591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating material discovery with a threshold-driven hybrid  acquisition policy-based Bayesian optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raihan%2C+A+S">Ahmed Shoyeb Raihan</a>, 
<a href="/search/cs?searchtype=author&query=Khosravi%2C+H">Hamed Khosravi</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Srinjoy Das</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+I">Imtiaz Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (stat.ML)

</div>
<p class="mathjax">Advancements in materials play a crucial role in technological progress.
However, the process of discovering and developing materials with desired
properties is often impeded by substantial experimental costs, extensive
resource utilization, and lengthy development periods. To address these
challenges, modern approaches often employ machine learning (ML) techniques
such as Bayesian Optimization (BO), which streamline the search for optimal
materials by iteratively selecting experiments that are most likely to yield
beneficial results. However, traditional BO methods, while beneficial, often
struggle with balancing the trade-off between exploration and exploitation,
leading to sub-optimal performance in material discovery processes. This paper
introduces a novel Threshold-Driven UCB-EI Bayesian Optimization (TDUE-BO)
method, which dynamically integrates the strengths of Upper Confidence Bound
(UCB) and Expected Improvement (EI) acquisition functions to optimize the
material discovery process. Unlike the classical BO, our method focuses on
efficiently navigating the high-dimensional material design space (MDS).
TDUE-BO begins with an exploration-focused UCB approach, ensuring a
comprehensive initial sweep of the MDS. As the model gains confidence,
indicated by reduced uncertainty, it transitions to the more exploitative EI
method, focusing on promising areas identified earlier. The UCB-to-EI switching
policy dictated guided through continuous monitoring of the model uncertainty
during each step of sequential sampling results in navigating through the MDS
more efficiently while ensuring rapid convergence. The effectiveness of TDUE-BO
is demonstrated through its application on three different material datasets,
showing significantly better approximation and optimization performance over
the EI and UCB-based BO methods in terms of the RMSE scores and convergence
efficiency, respectively.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09592" title="Abstract">arXiv:2311.09592</a> [<a href="/pdf/2311.09592" title="Download PDF">pdf</a>, <a href="/format/2311.09592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable and Adaptively Secure Any-Trust Distributed Key Generation and  All-hands Checkpointing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Hanwen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+T">Tiancheng Mai</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Q">Qiang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The classical distributed key generation protocols (DKG) are resurging due to
their widespread applications in blockchain. While efforts have been made to
improve DKG communication, practical large scale deployments are still yet to
come, due to various challenges including broadcast channel scalability and
worst-case complaint phase. In this paper, we propose a practical DKG for
DL-based cryptosystems, with only (quasi-)linear computation/communication cost
per participant, with the help of a public ledger, and beacon; Notably, our DKG
only incurs constant-size blockchain storage cost for broadcast, even in the
face of worst-case complaints. Moreover, our protocol satisfies adaptive
security. The key to our improvements lies in delegating the most costly
operations to an Any-Trust group. This group is randomly sampled and consists
of a small number of individuals. The population only trusts that at least one
member in the group is honest, without knowing which one. Additionally, we
introduce an extended broadcast channel based on a blockchain and data
dispersal network (such as IPFS), enabling reliable broadcasting of
arbitrary-size messages at the cost of constant-size blockchain storage, which
may be of independent interest.
<br />Our DKG leads to a fully practical instantiation of Filecoin's checkpointing
mechanism, in which all validators of a Proof-of-Stake (PoS) blockcahin
periodically run DKG and threshold signing to create checkpoints on Bitcoin,
thereby enhancing the security of the PoS chain. In comparison with another
checkpointing approach of Babylon (Oakland, 2023), ours enjoys a significally
smaller monetary cost of Bitcoin transaction fees. For a PoS chain with
$2^{12}$ validators, our cost is merely 0.6\% of that incurred by Babylon's
approach.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09593" title="Abstract">arXiv:2311.09593</a> [<a href="/pdf/2311.09593" title="Download PDF">pdf</a>, <a href="/format/2311.09593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Step Dialogue Workflow Action Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+R">Ramya Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Elenberg%2C+E">Ethan Elenberg</a>, 
<a href="/search/cs?searchtype=author&query=Narangodage%2C+H">Hashan Narangodage</a>, 
<a href="/search/cs?searchtype=author&query=McDonald%2C+R">Ryan McDonald</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In task-oriented dialogue, a system often needs to follow a sequence of
actions, called a workflow, that complies with a set of guidelines in order to
complete a task. In this paper, we propose the novel problem of multi-step
workflow action prediction, in which the system predicts multiple future
workflow actions. Accurate prediction of multiple steps allows for multi-turn
automation, which can free up time to focus on more complex tasks. We propose
three modeling approaches that are simple to implement yet lead to more action
automation: 1) fine-tuning on a training dataset, 2) few-shot in-context
learning leveraging retrieval and large language model prompting, and 3)
zero-shot graph traversal, which aggregates historical action sequences into a
graph for prediction. We show that multi-step action prediction produces
features that improve accuracy on downstream dialogue tasks like predicting
task success, and can increase automation of steps by 20% without requiring as
much feedback from a human overseeing the system.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09599" title="Abstract">arXiv:2311.09599</a> [<a href="/pdf/2311.09599" title="Download PDF">pdf</a>, <a href="/format/2311.09599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradual Source Domain Expansion for Unsupervised Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Westfechtel%2C+T">Thomas Westfechtel</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+H">Hao-Wei Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dexuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Harada%2C+T">Tatsuya Harada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised domain adaptation (UDA) tries to overcome the need for a large
labeled dataset by transferring knowledge from a source dataset, with lots of
labeled data, to a target dataset, that has no labeled data. Since there are no
labels in the target domain, early misalignment might propagate into the later
stages and lead to an error build-up. In order to overcome this problem, we
propose a gradual source domain expansion (GSDE) algorithm. GSDE trains the UDA
task several times from scratch, each time reinitializing the network weights,
but each time expands the source dataset with target data. In particular, the
highest-scoring target data of the previous run are employed as pseudo-source
samples with their respective pseudo-label. Using this strategy, the
pseudo-source samples induce knowledge extracted from the previous run directly
from the start of the new training. This helps align the two domains better,
especially in the early training epochs. In this study, we first introduce a
strong baseline network and apply our GSDE strategy to it. We conduct
experiments and ablation studies on three benchmarks (Office-31, OfficeHome,
and DomainNet) and outperform state-of-the-art methods. We further show that
the proposed GSDE strategy can improve the accuracy of a variety of different
state-of-the-art UDA approaches.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09601" title="Abstract">arXiv:2311.09601</a> [<a href="/pdf/2311.09601" title="Download PDF">pdf</a>, <a href="/format/2311.09601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code Models are Zero-shot Precondition Reasoners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Logeswaran%2C+L">Lajanugen Logeswaran</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+S">Sungryull Sohn</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Y">Yiwei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A+Z">Anthony Zhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dong-Ki Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shim%2C+D">Dongsub Shim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Moontae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Honglak Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips Foundation Models for Decision Making Workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">One of the fundamental skills required for an agent acting in an environment
to complete tasks is the ability to understand what actions are plausible at
any given point. This work explores a novel use of code representations to
reason about action preconditions for sequential decision making tasks. Code
representations offer the flexibility to model procedural activities and
associated constraints as well as the ability to execute and verify constraint
satisfaction. Leveraging code representations, we extract action preconditions
from demonstration trajectories in a zero-shot manner using pre-trained code
models. Given these extracted preconditions, we propose a precondition-aware
action sampling strategy that ensures actions predicted by a policy are
consistent with preconditions. We demonstrate that the proposed approach
enhances the performance of few-shot policy learning approaches across
task-oriented dialog and embodied textworld benchmarks.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09602" title="Abstract">arXiv:2311.09602</a> [<a href="/pdf/2311.09602" title="Download PDF">pdf</a>, <a href="/format/2311.09602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models (Mostly) Do Not Consider Emotion Triggers When  Predicting Emotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Smriti Singh</a>, 
<a href="/search/cs?searchtype=author&query=Caragea%2C+C">Cornelia Caragea</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+J">Junyi Jessy Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Situations and events evoke emotions in humans, but to what extent do they
inform the prediction of emotion detection models? Prior work in emotion
trigger or cause identification focused on training models to recognize events
that trigger an emotion. Instead, this work investigates how well
human-annotated emotion triggers correlate with features that models deemed
salient in their prediction of emotions. First, we introduce a novel dataset
EmoTrigger, consisting of 900 social media posts sourced from three different
datasets; these were annotated by experts for emotion triggers with high
agreement. Using EmoTrigger, we evaluate the ability of large language models
(LLMs) to identify emotion triggers, and conduct a comparative analysis of the
features considered important for these tasks between LLMs and fine-tuned
models. Our analysis reveals that emotion triggers are largely not considered
salient features for emotion prediction models, instead there is intricate
interplay between various features and the task of emotion detection.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09603" title="Abstract">arXiv:2311.09603</a> [<a href="/pdf/2311.09603" title="Download PDF">pdf</a>, <a href="/format/2311.09603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCORE: A framework for Self-Contradictory Reasoning Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Isabelle Lee</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yongkang Du</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+S">Soumya Sanyal</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jieyu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated impressive reasoning ability
in various language-based tasks. Despite many proposed reasoning methods aimed
at enhancing performance in downstream tasks, two fundamental questions
persist: Does reasoning genuinely support predictions, and how reliable is the
quality of reasoning? In this paper, we propose a framework \textsc{SCORE} to
analyze how well LLMs can reason. Specifically, we focus on self-contradictory
reasoning, where reasoning does not support the prediction. We find that LLMs
often contradict themselves when performing reasoning tasks that involve
contextual information and commonsense. The model may miss evidence or use
shortcuts, thereby exhibiting self-contradictory behaviors. We also employ the
Point-of-View (POV) method, which probes models to generate reasoning from
multiple perspectives, as a diagnostic tool for further analysis. We find that
though LLMs may appear to perform well in one-perspective settings, they fail
to stabilize such behavior in multi-perspectives settings. Even for correct
predictions, the reasoning may be messy and incomplete, and LLMs can easily be
led astray from good reasoning. \textsc{SCORE}'s results underscore the lack of
robustness required for trustworthy reasoning and the urgency for further
research to establish best practices for a comprehensive evaluation of
reasoning beyond accuracy-based metrics.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09605" title="Abstract">arXiv:2311.09605</a> [<a href="/pdf/2311.09605" title="Download PDF">pdf</a>, <a href="/format/2311.09605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring and Improving Attentiveness to Partial Inputs with  Counterfactuals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elazar%2C+Y">Yanai Elazar</a>, 
<a href="/search/cs?searchtype=author&query=Paranjape%2C+B">Bhargavi Paranjape</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wiegreffe%2C+S">Sarah Wiegreffe</a>, 
<a href="/search/cs?searchtype=author&query=Raghavi%2C+K">Khyathi Raghavi</a>, 
<a href="/search/cs?searchtype=author&query=Srikumar%2C+V">Vivek Srikumar</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sameer Singh</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The inevitable appearance of spurious correlations in training datasets hurts
the generalization of NLP models on unseen data. Previous work has found that
datasets with paired inputs are prone to correlations between a specific part
of the input (e.g., the hypothesis in NLI) and the label; consequently, models
trained only on those outperform chance. Are these correlations picked up by
models trained on the full input data? To address this question, we propose a
new evaluation method, Counterfactual Attentiveness Test (CAT). CAT uses
counterfactuals by replacing part of the input with its counterpart from a
different example (subject to some restrictions), expecting an attentive model
to change its prediction. Using CAT, we systematically investigate established
supervised and in-context learning models on ten datasets spanning four tasks:
natural language inference, reading comprehension, paraphrase detection, and
visual &amp; language reasoning. CAT reveals that reliance on such correlations is
mainly data-dependent. Surprisingly, we find that GPT3 becomes less attentive
with an increased number of demonstrations, while its accuracy on the test data
improves. Our results demonstrate that augmenting training or demonstration
data with counterfactuals is effective in improving models' attentiveness. We
show that models' attentiveness measured by CAT reveals different conclusions
from solely measuring correlations in data.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09606" title="Abstract">arXiv:2311.09606</a> [<a href="/pdf/2311.09606" title="Download PDF">pdf</a>, <a href="/format/2311.09606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GistScore: Learning Better Representations for In-Context Example  Selection with Gist Bottlenecks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shivanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rosenbaum%2C+C">Clemens Rosenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Elenberg%2C+E+R">Ethan R. Elenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have the ability to perform in-context learning
(ICL) of new tasks by conditioning on prompts comprising a few task examples.
This work studies the problem of selecting the best examples given a candidate
pool to improve ICL performance on given a test input. Existing approaches
either require training with feedback from a much larger LLM or are
computationally expensive. We propose a novel metric, GistScore, based on
Example Gisting, a novel approach for training example retrievers for ICL using
an attention bottleneck via Gisting, a recent technique for compressing task
instructions. To tradeoff performance with ease of use, we experiment with both
fine-tuning gist models on each dataset and multi-task training a single model
on a large collection of datasets. On 21 diverse datasets spanning 9 tasks, we
show that our fine-tuned models get state-of-the-art ICL performance with 20%
absolute average gain over off-the-shelf retrievers and 7% over the best prior
methods. Our multi-task model generalizes well out-of-the-box to new task
categories, datasets, and prompt templates with retrieval speeds that are
consistently thousands of times faster than the best prior training-free
method.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09611" title="Abstract">arXiv:2311.09611</a> [<a href="/pdf/2311.09611" title="Download PDF">pdf</a>, <a href="/format/2311.09611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeltaLCA: Comparative Life-Cycle Assessment for Electronics Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4hnlein%2C+F">Felix H&#xe4;hnlein</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Y">Yuxuan Mei</a>, 
<a href="/search/cs?searchtype=author&query=Englhardt%2C+Z">Zachary Englhardt</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+S">Shwetak Patel</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+A">Adriana Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+V">Vikram Iyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Reducing the environmental footprint of electronics and computing devices
requires new tools that empower designers to make informed decisions about
sustainability during the design process itself. This is not possible with
current tools for life cycle assessment (LCA) which require substantial domain
expertise and time to evaluate the numerous chips and other components that
make up a device. We observe first that informed decision-making does not
require absolute metrics and can instead be done by comparing designs. Second,
we can use domain-specific heuristics to perform these comparisons. We combine
these insights to develop DeltaLCA, an open-source interactive design tool that
addresses the dual challenges of automating life cycle inventory generation and
data availability by performing comparative analyses of electronics designs.
Users can upload standard design files from Electronic Design Automation (EDA)
software and the tool will guide them through determining which one has greater
carbon footprint. DeltaLCA leverages electronics-specific LCA datasets and
heuristics and tries to automatically rank the two designs, prompting users to
provide additional information only when necessary. We show through case
studies DeltaLCA achieves the same result as evaluating full LCAs, and that it
accelerates LCA comparisons from eight expert-hours to a single click for
devices with ~30 components, and 15 minutes for more complex devices with ~100
components.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09612" title="Abstract">arXiv:2311.09612</a> [<a href="/pdf/2311.09612" title="Download PDF">pdf</a>, <a href="/format/2311.09612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient End-to-End Visual Document Understanding with Rationale  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Alekh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+M">Mandar Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Thomason%2C+J">Jesse Thomason</a>, 
<a href="/search/cs?searchtype=author&query=Toutanova%2C+K">Kristina Toutanova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Understanding visually situated language requires recognizing text and visual
elements, and interpreting complex layouts. State-of-the-art methods commonly
use specialized pre-processing tools, such as optical character recognition
(OCR) systems, that map document image inputs to extracted information in the
space of textual tokens, and sometimes also employ large language models (LLMs)
to reason in text token space. However, the gains from external tools and LLMs
come at the cost of increased computational and engineering complexity. In this
paper, we ask whether small pretrained image-to-text models can learn selective
text or layout recognition and reasoning as an intermediate inference step in
an end-to-end model for pixel-level visual language understanding. We
incorporate the outputs of such OCR tools, LLMs, and larger multimodal models
as intermediate ``rationales'' on training data, and train a small student
model to predict both rationales and answers for input questions based on those
training examples. A student model based on Pix2Struct (282M parameters)
achieves consistent improvements on three visual document understanding
benchmarks representing infographics, scanned documents, and figures, with
improvements of more than 4\% absolute over a comparable Pix2Struct model that
predicts answers directly.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09613" title="Abstract">arXiv:2311.09613</a> [<a href="/pdf/2311.09613" title="Download PDF">pdf</a>, <a href="/format/2311.09613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Socrates: Evaluating LLMs through explanation critiques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuling Gu</a>, 
<a href="/search/cs?searchtype=author&query=Tafjord%2C+O">Oyvind Tafjord</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While LLMs can provide reasoned explanations along with their answers, the
nature and quality of those explanations are still poorly understood. In
response, our goal is to define a detailed way of characterizing the
explanation capabilities of modern models and to create a nuanced,
interpretable explanation evaluation tool that can generate such
characterizations automatically, without relying on expensive API calls or
human annotations. Our approach is to (a) define the new task of explanation
critiquing - identifying and categorizing any main flaw in an explanation and
providing suggestions to address the flaw, (b) create a sizeable,
human-verified dataset for this task, and (c) train an open-source, automatic
critiquing model (called Digital Socrates) using this data. Through
quantitative and qualitative analysis, we demonstrate how Digital Socrates is
useful for revealing insights about student models by examining their reasoning
chains, and how it can provide high-quality, nuanced, automatic evaluation of
those model explanations for the first time. Digital Socrates thus fills an
important gap in evaluation tools for understanding and improving the
explanation behavior of models.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09614" title="Abstract">arXiv:2311.09614</a> [<a href="/pdf/2311.09614" title="Download PDF">pdf</a>, <a href="/format/2311.09614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comprehensive Evaluation and Insights into the Use of Deep Neural  Networks to Detect and Quantify Lymphoma Lesions in PET/CT Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahamed%2C+S">Shadab Ahamed</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yixi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gowdy%2C+C">Claire Gowdy</a>, 
<a href="/search/cs?searchtype=author&query=O%2C+J+H">Joo H. O</a>, 
<a href="/search/cs?searchtype=author&query=Bloise%2C+I">Ingrid Bloise</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+D">Don Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Martineau%2C+P">Patrick Martineau</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A9nard%2C+F">Fran&#xe7;ois B&#xe9;nard</a>, 
<a href="/search/cs?searchtype=author&query=Yousefirizi%2C+F">Fereshteh Yousefirizi</a>, 
<a href="/search/cs?searchtype=author&query=Dodhia%2C+R">Rahul Dodhia</a>, 
<a href="/search/cs?searchtype=author&query=Lavista%2C+J+M">Juan M. Lavista</a>, 
<a href="/search/cs?searchtype=author&query=Weeks%2C+W+B">William B. Weeks</a>, 
<a href="/search/cs?searchtype=author&query=Uribe%2C+C+F">Carlos F. Uribe</a>, 
<a href="/search/cs?searchtype=author&query=Rahmim%2C+A">Arman Rahmim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This study performs comprehensive evaluation of four neural network
architectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion
segmentation from PET/CT images. These networks were trained, validated, and
tested on a diverse, multi-institutional dataset of 611 cases. Internal testing
(88 cases; total metabolic tumor volume (TMTV) range [0.52, 2300] ml) showed
SegResNet as the top performer with a median Dice similarity coefficient (DSC)
of 0.76 and median false positive volume (FPV) of 4.55 ml; all networks had a
median false negative volume (FNV) of 0 ml. On the unseen external test set
(145 cases with TMTV range: [0.10, 2480] ml), SegResNet achieved the best
median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml.
We assessed reproducibility of six lesion measures, calculated their prediction
errors, and examined DSC performance in relation to these lesion measures,
offering insights into segmentation accuracy and clinical relevance.
Additionally, we introduced three lesion detection criteria, addressing the
clinical need for identifying lesions, counting them, and segmenting based on
metabolic characteristics. We also performed expert intra-observer variability
analysis revealing the challenges in segmenting ``easy'' vs. ``hard'' cases, to
assist in the development of more resilient segmentation algorithms. Finally,
we performed inter-observer agreement assessment underscoring the importance of
a standardized ground truth segmentation protocol involving multiple expert
annotators. Code is available at:
https://github.com/microsoft/lymphoma-segmentation-dnn
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09615" title="Abstract">arXiv:2311.09615</a> [<a href="/pdf/2311.09615" title="Download PDF">pdf</a>, <a href="/format/2311.09615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Retrieval Augmentation and the Limitations of Language Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Ting-Rui Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X+V">Xinyan Velocity Yu</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+J">Joshua Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+O">Ollie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Isabelle Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yogatama%2C+D">Dani Yogatama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Augmenting a language model (LM) with $k$-nearest neighbors (kNN) retrieval
on its training data alone can decrease its perplexity, though the underlying
reasons for this remains elusive. In this work, we first rule out one
previously posited possibility -- the "softmax bottleneck." We further identify
the MLP hurdle phenomenon, where the final MLP layer in LMs may impede LM
optimization early on. We explore memorization and generalization in language
models with two new datasets, where advanced model like GPT-3.5-turbo find
generalizing to irrelevant information in the training data challenging.
However, incorporating kNN retrieval to vanilla GPT-2 117M can consistently
improve performance in this setting.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09619" title="Abstract">arXiv:2311.09619</a> [<a href="/pdf/2311.09619" title="Download PDF">pdf</a>, <a href="/format/2311.09619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Take One Step at a Time to Know Incremental Utility of Demonstration: An  Analysis on Reranking for Few-Shot In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+K">Kazuma Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+K">Karthik Raman</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-Context Learning (ICL) is an emergent capability of Large Language Models
(LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new
tasks. Previous studies have shown that using LLMs' outputs as labels is
effective in training models to select demonstrations. Such a label is expected
to estimate utility of a demonstration in ICL; however, it has not been well
understood how different labeling strategies affect results on target tasks.
This paper presents an analysis on different utility functions by focusing on
LLMs' output probability given ground-truth output, and task-specific reward
given LLMs' prediction. Unlike the previous work, we introduce a novel labeling
method, incremental utility, which estimates how much incremental knowledge is
brought into the LLMs by a demonstration. We conduct experiments with
instruction-tuned LLMs on binary/multi-class classification, segmentation, and
translation across Arabic, English, Finnish, Japanese, and Spanish. Our results
show that (1) the probability is effective when the probability values are
distributed across the whole value range (on the classification tasks), and (2)
the downstream metric is more robust when nuanced reward values are provided
with long outputs (on the segmentation and translation tasks). We then show
that the proposed incremental utility further helps ICL by contrasting how the
LLMs perform with and without the demonstrations.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09620" title="Abstract">arXiv:2311.09620</a> [<a href="/pdf/2311.09620" title="Download PDF">pdf</a>, <a href="/format/2311.09620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GAIA: Delving into Gradient-based Attribution Abnormality for  Out-of-distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinggang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xiaoyang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianzong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Jiguang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Detecting out-of-distribution (OOD) examples is crucial to guarantee the
reliability and safety of deep neural networks in real-world settings. In this
paper, we offer an innovative perspective on quantifying the disparities
between in-distribution (ID) and OOD data -- analyzing the uncertainty that
arises when models attempt to explain their predictive decisions. This
perspective is motivated by our observation that gradient-based attribution
methods encounter challenges in assigning feature importance to OOD data,
thereby yielding divergent explanation patterns. Consequently, we investigate
how attribution gradients lead to uncertain explanation outcomes and introduce
two forms of abnormalities for OOD detection: the zero-deflation abnormality
and the channel-wise average abnormality. We then propose GAIA, a simple and
effective approach that incorporates Gradient Abnormality Inspection and
Aggregation. The effectiveness of GAIA is validated on both commonly utilized
(CIFAR) and large-scale (ImageNet-1k) benchmarks. Specifically, GAIA reduces
the average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100 compared to
advanced post-hoc methods.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09622" title="Abstract">arXiv:2311.09622</a> [<a href="/pdf/2311.09622" title="Download PDF">pdf</a>, <a href="/ps/2311.09622" title="Download PostScript">ps</a>, <a href="/format/2311.09622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homography Initialization and Dynamic Weighting Algorithm Based on a  Downward-Looking Camera and IMU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+B">Bo Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yongkang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Deng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zhigang Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In recent years, the technology in visual-inertial odometry (VIO) has matured
considerably and has been widely used in many applications. However, we still
encounter challenges when applying VIO to a micro air vehicle (MAV) equipped
with a downward-looking camera. Specifically, VIO cannot compute the correct
initialization results during take-off and the cumulative drift is large when
the MAV is flying in the air. To overcome these problems, we propose a
homographybased initialization method, which utilizes the fact that the
features detected by the downward-looking camera during take-off are
approximately on the same plane. Then we introduce the prior normal vector and
motion field to make states more accurate. In addition, to deal with the
cumulative drift, a strategy for dynamically weighting visual residuals is
proposed. Finally, we evaluate our method on the collected real-world datasets.
The results demonstrate that our system can be successfully initialized no
matter how the MAV takes off and the positioning errors are also greatly
improved.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09624" title="Abstract">arXiv:2311.09624</a> [<a href="/pdf/2311.09624" title="Download PDF">pdf</a>, <a href="/format/2311.09624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Recommendation System for Enhanced Customer Experience: A Novel  Image-to-Text Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ayedi%2C+M+F">Mohamaed Foued Ayedi</a>, 
<a href="/search/cs?searchtype=author&query=Salem%2C+H+B">Hiba Ben Salem</a>, 
<a href="/search/cs?searchtype=author&query=Hammami%2C+S">Soulaimen Hammami</a>, 
<a href="/search/cs?searchtype=author&query=Said%2C+A+B">Ahmed Ben Said</a>, 
<a href="/search/cs?searchtype=author&query=Jabbar%2C+R">Rateb Jabbar</a>, 
<a href="/search/cs?searchtype=author&query=CHabbouh%2C+A">Achraf CHabbouh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 5th Deep Learning Indaba Conference (DLI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing fashion recommendation systems encounter difficulties in using
visual data for accurate and personalized recommendations. This research
describes an innovative end-to-end pipeline that uses artificial intelligence
to provide fine-grained visual interpretation for fashion recommendations. When
customers upload images of desired products or outfits, the system
automatically generates meaningful descriptions emphasizing stylistic elements.
These captions guide retrieval from a global fashion product catalogue to offer
similar alternatives that fit the visual characteristics of the original image.
On a dataset of over 100,000 categorized fashion photos, the pipeline was
trained and evaluated. The F1-score for the object detection model was 0.97,
exhibiting exact fashion object recognition capabilities optimized for
recommendation. This visually aware system represents a key advancement in
customer engagement through personalized fashion recommendations
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09625" title="Abstract">arXiv:2311.09625</a> [<a href="/pdf/2311.09625" title="Download PDF">pdf</a>, <a href="/format/2311.09625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DECDM: Document Enhancement using Cycle-Consistent Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Rimchala%2C+J">Joy Rimchala</a>, 
<a href="/search/cs?searchtype=author&query=Mouatadid%2C+L">Lalla Mouatadid</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+K">Kamalika Das</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sricharan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The performance of optical character recognition (OCR) heavily relies on
document image quality, which is crucial for automatic document processing and
document intelligence. However, most existing document enhancement methods
require supervised data pairs, which raises concerns about data separation and
privacy protection, and makes it challenging to adapt these methods to new
domain pairs. To address these issues, we propose DECDM, an end-to-end
document-level image translation method inspired by recent advances in
diffusion models. Our method overcomes the limitations of paired training by
independently training the source (noisy input) and target (clean output)
models, making it possible to apply domain-specific diffusion models to other
pairs. DECDM trains on one dataset at a time, eliminating the need to scan both
datasets concurrently, and effectively preserving data privacy from the source
or target domain. We also introduce simple data augmentation strategies to
improve character-glyph conservation during translation. We compare DECDM with
state-of-the-art methods on multiple synthetic data and benchmark datasets,
such as document denoising and {\color{black}shadow} removal, and demonstrate
the superiority of performance quantitatively and qualitatively.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09627" title="Abstract">arXiv:2311.09627</a> [<a href="/pdf/2311.09627" title="Download PDF">pdf</a>, <a href="/format/2311.09627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRISPR: Eliminating Bias Neurons from an Instruction-following Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nakyeong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+T">Taegwan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+K">Kyomin Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) executing tasks through instruction-based
prompts often face challenges stemming from distribution differences between
user instructions and training instructions. This leads to distractions and
biases, especially when dealing with inconsistent dynamic labels. In this
paper, we introduces a novel bias mitigation method, CRISPR, designed to
alleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods
to identify bias neurons influencing biased outputs and employs pruning to
eliminate the bias neurons. Experimental results demonstrate the method's
effectiveness in mitigating biases in instruction-based prompting, enhancing
language model performance on social bias benchmarks without compromising
pre-existing knowledge. CRISPR proves highly practical, model-agnostic,
offering flexibility in adapting to evolving social biases.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09628" title="Abstract">arXiv:2311.09628</a> [<a href="/pdf/2311.09628" title="Download PDF">pdf</a>, <a href="/format/2311.09628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynDiffix: More accurate synthetic structured data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Francis%2C+P">Paul Francis</a>, 
<a href="/search/cs?searchtype=author&query=Berneanu%2C+C">Cristian Berneanu</a>, 
<a href="/search/cs?searchtype=author&query=Gashi%2C+E">Edon Gashi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This paper introduces SynDiffix, a mechanism for generating statistically
accurate, anonymous synthetic data for structured data. Recent open source and
commercial systems use Generative Adversarial Networks or Transformed Auto
Encoders to synthesize data, and achieve anonymity through
overfitting-avoidance. By contrast, SynDiffix exploits traditional mechanisms
of aggregation, noise addition, and suppression among others. Compared to
CTGAN, ML models generated from SynDiffix are twice as accurate, marginal and
column pairs data quality is one to two orders of magnitude more accurate, and
execution time is two orders of magnitude faster. Compared to the best
commercial product we measured (MostlyAI), ML model accuracy is comparable,
marginal and pairs accuracy is 5 to 10 times better, and execution time is an
order of magnitude faster. Similar to the other approaches, SynDiffix
anonymization is very strong. This paper describes SynDiffix and compares its
performance with other popular open source and commercial systems.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09630" title="Abstract">arXiv:2311.09630</a> [<a href="/pdf/2311.09630" title="Download PDF">pdf</a>, <a href="/format/2311.09630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Scroll to Misbelief: Modeling the Unobservable Susceptibility to  Misinformation on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M+D">Mingyu Derek Ma</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+W">Wenna Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Azure Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weiyan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Susceptibility to misinformation describes the extent to believe unverifiable
claims, which is hidden in people's mental process and infeasible to observe.
Existing susceptibility studies heavily rely on the self-reported beliefs,
making any downstream applications on susceptability hard to scale. To address
these limitations, in this work, we propose a computational model to infer
users' susceptibility levels given their activities. Since user's
susceptibility is a key indicator for their reposting behavior, we utilize the
supervision from the observable sharing behavior to infer the underlying
susceptibility tendency. The evaluation shows that our model yields estimations
that are highly aligned with human judgment on users' susceptibility level
comparisons. Building upon such large-scale susceptibility labeling, we further
conduct a comprehensive analysis of how different social factors relate to
susceptibility. We find that political leanings and psychological factors are
associated with susceptibility in varying degrees.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09632" title="Abstract">arXiv:2311.09632</a> [<a href="/pdf/2311.09632" title="Download PDF">pdf</a>, <a href="/format/2311.09632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Continual Knowledge Learning for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Tongjun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+K">Karthick Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Seah%2C+C+W">Chun Wei Seah</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuhao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) serve as repositories of extensive world
knowledge, enabling them to perform tasks such as question-answering and
fact-checking. However, this knowledge can become obsolete as global contexts
change. In this paper, we introduce a novel problem in the realm of continual
learning: Online Continual Knowledge Learning (OCKL). This problem formulation
aims to manage the dynamic nature of world knowledge in LMs under real-time
constraints. We propose a new benchmark and evaluation metric designed to
measure both the rate of new knowledge acquisition and the retention of
previously learned knowledge. Our empirical evaluation, conducted using a
variety of state-of-the-art methods, establishes robust base-lines for OCKL.
Our results reveal that existing continual learning approaches are
unfortunately insufficient for tackling the unique challenges posed by OCKL. We
identify key factors that influence the trade-off between knowledge acquisition
and retention, thereby advancing our understanding of how to train LMs in a
continually evolving environment.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09635" title="Abstract">arXiv:2311.09635</a> [<a href="/pdf/2311.09635" title="Download PDF">pdf</a>, <a href="/format/2311.09635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating In-Context Learning of Libraries for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Arkil Patel</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Siva Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
<a href="/search/cs?searchtype=author&query=Dasigi%2C+P">Pradeep Dasigi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Contemporary Large Language Models (LLMs) exhibit a high degree of code
generation and comprehension capability. A particularly promising area is their
ability to interpret code modules from unfamiliar libraries for solving
user-instructed tasks. Recent work has shown that large proprietary LLMs can
learn novel library usage in-context from demonstrations. These results raise
several open questions: whether demonstrations of library usage is required,
whether smaller (and more open) models also possess such capabilities, etc. In
this work, we take a broader approach by systematically evaluating a diverse
array of LLMs across three scenarios reflecting varying levels of domain
specialization to understand their abilities and limitations in generating code
based on libraries defined in-context. Our results show that even smaller
open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding
of novel code libraries based on specification presented in-context. Our
findings further reveal that LLMs exhibit a surprisingly high proficiency in
learning novel library modules even when provided with just natural language
descriptions or raw code implementations of the functions, which are often
cheaper to obtain than demonstrations. Overall, our results pave the way for
harnessing LLMs in more adaptable and dynamic coding environments.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09639" title="Abstract">arXiv:2311.09639</a> [<a href="/pdf/2311.09639" title="Download PDF">pdf</a>, <a href="/format/2311.09639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Quantification of Image Reconstruction Uncertainty without  Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+S">Sirui Bi</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+V">Victor Fung</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Computational imaging plays a pivotal role in determining hidden information
from sparse measurements. A robust inverse solver is crucial to fully
characterize the uncertainty induced by these measurements, as it allows for
the estimation of the complete posterior of unrecoverable targets. This, in
turn, facilitates a probabilistic interpretation of observational data for
decision-making. In this study, we propose a deep variational framework that
leverages a deep generative model to learn an approximate posterior
distribution to effectively quantify image reconstruction uncertainty without
the need for training data. We parameterize the target posterior using a
flow-based model and minimize their Kullback-Leibler (KL) divergence to achieve
accurate uncertainty estimation. To bolster stability, we introduce a robust
flow-based model with bi-directional regularization and enhance expressivity
through gradient boosting. Additionally, we incorporate a space-filling design
to achieve substantial variance reduction on both latent prior space and target
posterior space. We validate our method on several benchmark tasks and two
real-world applications, namely fastMRI and black hole image reconstruction.
Our results indicate that our method provides reliable and high-quality image
reconstruction with robust uncertainty estimation.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09641" title="Abstract">arXiv:2311.09641</a> [<a href="/pdf/2311.09641" title="Download PDF">pdf</a>, <a href="/format/2311.09641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Exploitability of Reinforcement Learning with Human Feedback for  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiongxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junlin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Reinforcement Learning with Human Feedback (RLHF) is a methodology designed
to align Large Language Models (LLMs) with human preferences, playing an
important role in LLMs alignment. Despite its advantages, RLHF relies on human
annotators to rank the text, which can introduce potential security
vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the
ranking score by up-ranking any malicious text to steer the LLM adversarially.
To assess the red-teaming of RLHF against human preference data poisoning, we
propose RankPoison, a poisoning attack method on candidates' selection of
preference rank flipping to reach certain malicious behaviors (e.g., generating
longer sequences, which can increase the computational cost). With poisoned
dataset generated by RankPoison, we can perform poisoning attacks on LLMs to
generate longer tokens without hurting the original safety alignment
performance. Moreover, applying RankPoison, we also successfully implement a
backdoor attack where LLMs can generate longer answers under questions with the
trigger word. Our findings highlight critical security challenges in RLHF,
underscoring the necessity for more robust alignment methods for LLMs.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09645" title="Abstract">arXiv:2311.09645</a> [<a href="/pdf/2311.09645" title="Download PDF">pdf</a>, <a href="/format/2311.09645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PELS: A Lightweight and Flexible Peripheral Event Linking System for  Ultra-Low Power IoT Processors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ottaviano%2C+A">Alessandro Ottaviano</a>, 
<a href="/search/cs?searchtype=author&query=Balas%2C+R">Robert Balas</a>, 
<a href="/search/cs?searchtype=author&query=Sauter%2C+P">Philippe Sauter</a>, 
<a href="/search/cs?searchtype=author&query=Eggimann%2C+M">Manuel Eggimann</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, accepted at DATE24 conference, pre camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">A key challenge for ultra-low-power (ULP) devices is handling peripheral
linking, where the main central processing unit (CPU) periodically mediates the
interaction among multiple peripherals following wake-up events. Current
solutions address this problem by either integrating event interconnects that
route single-wire event lines among peripherals or by general-purpose I/O
processors, with a strong trade-off between the latency, efficiency of the
former, and the flexibility of the latter. In this paper, we present an
open-source, peripheral-agnostic, lightweight, and flexible Peripheral Event
Linking System (PELS) that combines dedicated event routing with a tiny I/O
processor. With the proposed approach, the power consumption of a linking event
is reduced by 2.5 times compared to a baseline relying on the main core for the
event-linking process, at a low area of just 7 kGE in its minimal
configuration, when integrated into a ULP RISC-V IoT processor.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09646" title="Abstract">arXiv:2311.09646</a> [<a href="/pdf/2311.09646" title="Download PDF">pdf</a>, <a href="/format/2311.09646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructing Continuous Light Field From Single Coded Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+Y">Yuya Ishikawa</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+K">Keita Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Tsutake%2C+C">Chihiro Tsutake</a>, 
<a href="/search/cs?searchtype=author&query=Fujii%2C+T">Toshiaki Fujii</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, Volume 11, Pages 99387-99396, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">We propose a method for reconstructing a continuous light field of a target
scene from a single observed image. Our method takes the best of two worlds:
joint aperture-exposure coding for compressive light-field acquisition, and a
neural radiance field (NeRF) for view synthesis. Joint aperture-exposure coding
implemented in a camera enables effective embedding of 3-D scene information
into an observed image, but in previous works, it was used only for
reconstructing discretized light-field views. NeRF-based neural rendering
enables high quality view synthesis of a 3-D scene from continuous viewpoints,
but when only a single image is given as the input, it struggles to achieve
satisfactory quality. Our method integrates these two techniques into an
efficient and end-to-end trainable pipeline. Trained on a wide variety of
scenes, our method can reconstruct continuous light fields accurately and
efficiently without any test time optimization. To our knowledge, this is the
first work to bridge two worlds: camera design for efficiently acquiring 3-D
information and neural rendering.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09648" title="Abstract">arXiv:2311.09648</a> [<a href="/pdf/2311.09648" title="Download PDF">pdf</a>, <a href="/format/2311.09648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event Causality Is Key to Computational Story Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yidan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+Q">Qin Chao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Psychological research suggests the central role of event causality in human
story understanding. Further, event causality has been heavily utilized in
symbolic story generation. However, few machine learning systems for story
understanding employ event causality, partially due to the lack of reliable
methods for identifying open-world causal event relations. Leveraging recent
progress in large language models (LLMs), we present the first method for event
causality identification that leads to material improvements in computational
story understanding. We design specific prompts for extracting event causal
relations from GPT. Against human-annotated event causal relations in the
GLUCOSE dataset, our technique performs on par with supervised models, while
being easily generalizable to stories of different types and lengths. The
extracted causal relations lead to 5.7\% improvements on story quality
evaluation and 8.7\% on story video-text alignment. Our findings indicate
enormous untapped potential for event causality in computational story
understanding.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09649" title="Abstract">arXiv:2311.09649</a> [<a href="/pdf/2311.09649" title="Download PDF">pdf</a>, <a href="/format/2311.09649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICXML: An In-Context Learning Framework for Zero-Shot Extreme  Multi-Label Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yaxin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zamani%2C+H">Hamed Zamani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper focuses on the task of Extreme Multi-Label Classification (XMC)
whose goal is to predict multiple labels for each instance from an extremely
large label space. While existing research has primarily focused on fully
supervised XMC, real-world scenarios often lack complete supervision signals,
highlighting the importance of zero-shot settings. Given the large label space,
utilizing in-context learning approaches is not trivial. We address this issue
by introducing In-Context Extreme Multilabel Learning (ICXML), a two-stage
framework that cuts down the search space by generating a set of candidate
labels through incontext learning and then reranks them. Extensive experiments
suggest that ICXML advances the state of the art on two diverse public
benchmarks.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09651" title="Abstract">arXiv:2311.09651</a> [<a href="/pdf/2311.09651" title="Download PDF">pdf</a>, <a href="/format/2311.09651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;It&#x27;s not like Jarvis, but it&#x27;s pretty close!&quot; -- Examining ChatGPT&#x27;s  Usage among Undergraduate Students in Computer Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joshi%2C+I">Ishika Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Budhiraja%2C+R">Ritvik Budhiraja</a>, 
<a href="/search/cs?searchtype=author&query=Akolekar%2C+H+D">Harshal D Akolekar</a>, 
<a href="/search/cs?searchtype=author&query=Challa%2C+J+S">Jagat Sesh Challa</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dhruv Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ACE 2024: <a href="https://aceconference2024.github.io/aceconference2024/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) such as ChatGPT and Google Bard have garnered
significant attention in the academic community. Previous research has
evaluated these LLMs for various applications such as generating programming
exercises and solutions. However, these evaluations have predominantly been
conducted by instructors and researchers, not considering the actual usage of
LLMs by students. This study adopts a student-first approach to comprehensively
understand how undergraduate computer science students utilize ChatGPT, a
popular LLM, released by OpenAI. We employ a combination of student surveys and
interviews to obtain valuable insights into the benefits, challenges, and
suggested improvements related to ChatGPT. Our findings suggest that a majority
of students (over 57%) have a convincingly positive outlook towards adopting
ChatGPT as an aid in coursework-related tasks. However, our research also
highlights various challenges that must be resolved for long-term acceptance of
ChatGPT amongst students. The findings from this investigation have broader
implications and may be applicable to other LLMs and their role in computing
education.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09652" title="Abstract">arXiv:2311.09652</a> [<a href="/pdf/2311.09652" title="Download PDF">pdf</a>, <a href="/format/2311.09652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-based Motion-Robust Accurate Shape Estimation for Mixed  Reflectance Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dashpute%2C+A">Aniket Dashpute</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiazhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+J">James Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Cossairt%2C+O">Oliver Cossairt</a>, 
<a href="/search/cs?searchtype=author&query=Veeraraghavan%2C+A">Ashok Veeraraghavan</a>, 
<a href="/search/cs?searchtype=author&query=Willomitzer%2C+F">Florian Willomitzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Event-based structured light systems have recently been introduced as an
exciting alternative to conventional frame-based triangulation systems for the
3D measurements of diffuse surfaces. Important benefits include the fast
capture speed and the high dynamic range provided by the event camera - albeit
at the cost of lower data quality. So far, both low-accuracy event-based as
well as high-accuracy frame-based 3D imaging systems are tailored to a specific
surface type, such as diffuse or specular, and can not be used for a broader
class of object surfaces ("mixed reflectance scenes"). In this paper, we
present a novel event-based structured light system that enables fast 3D
imaging of mixed reflectance scenes with high accuracy. On the captured events,
we use epipolar constraints that intrinsically enable decomposing the measured
reflections into diffuse, two-bounce specular, and other multi-bounce
reflections. The diffuse objects in the scene are reconstructed using
triangulation. Eventually, the reconstructed diffuse scene parts are used as a
"display" to evaluate the specular scene parts via deflectometry. This novel
procedure allows us to use the entire scene as a virtual screen, using only a
scanning laser and an event camera. The resulting system achieves fast and
motion-robust (14Hz) reconstructions of mixed reflectance scenes with &lt; 500
$\mu$m accuracy. Moreover, we introduce a "superfast" capture mode (250Hz) for
the 3D measurement of diffuse scenes.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09653" title="Abstract">arXiv:2311.09653</a> [<a href="/pdf/2311.09653" title="Download PDF">pdf</a>, <a href="/format/2311.09653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved TokenPose with Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Anning Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Over the past few years, the vision transformer and its various forms have
gained significance in human pose estimation. By treating image patches as
tokens, transformers can capture global relationships wisely, estimate the
keypoint tokens by leveraging the visual tokens, and recognize the posture of
the human body. Nevertheless, global attention is computationally demanding,
which poses a challenge for scaling up transformer-based methods to
high-resolution features. In this paper, we introduce sparsity in both keypoint
token attention and visual token attention to improve human pose estimation.
Experimental results on the MPII dataset demonstrate that our model has a
higher level of accuracy and proved the feasibility of the method, achieving
new state-of-the-art results. The idea can also provide references for other
transformer-based models.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09655" title="Abstract">arXiv:2311.09655</a> [<a href="/pdf/2311.09655" title="Download PDF">pdf</a>, <a href="/format/2311.09655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-View Spectrogram Transformer for Respiratory Sound Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wentao He</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuchen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jianfeng Ren</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+R">Ruibin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xudong Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Deep neural networks have been applied to audio spectrograms for respiratory
sound classification. Existing models often treat the spectrogram as a
synthetic image while overlooking its physical characteristics. In this paper,
a Multi-View Spectrogram Transformer (MVST) is proposed to embed different
views of time-frequency characteristics into the vision transformer.
Specifically, the proposed MVST splits the mel-spectrogram into different sized
patches, representing the multi-view acoustic elements of a respiratory sound.
These patches and positional embeddings are then fed into transformer encoders
to extract the attentional information among patches through a self-attention
mechanism. Finally, a gated fusion scheme is designed to automatically weigh
the multi-view features to highlight the best one in a specific scenario.
Experimental results on the ICBHI dataset demonstrate that the proposed MVST
significantly outperforms state-of-the-art methods for classifying respiratory
sounds.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09656" title="Abstract">arXiv:2311.09656</a> [<a href="/pdf/2311.09656" title="Download PDF">pdf</a>, <a href="/format/2311.09656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Chemistry Reasoning with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Siru Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bing Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Lianhui Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper studies the problem of solving complex chemistry problems with
large language models (LLMs). Despite the extensive general knowledge in LLMs
(such as GPT-4), they struggle with chemistry reasoning that requires faithful
grounded reasoning with diverse chemical knowledge and an integrative
understanding of chemical interactions. We propose InstructChem, a new
structured reasoning approach that substantially boosts the LLMs' chemical
reasoning capabilities. InstructChem explicitly decomposes the reasoning into
three critical phrases, including chemical formulae generation by LLMs that
offers the basis for subsequent grounded reasoning, step-by-step reasoning that
makes multi-step derivations with the identified formulae for a preliminary
answer, and iterative review-and-refinement that steers LLMs to progressively
revise the previous phases for increasing confidence, leading to the final
high-confidence answer. We conduct extensive experiments on four different
chemistry challenges, including quantum chemistry, quantum mechanics, physical
chemistry, and chemistry kinetics. Our approach significantly enhances GPT-4 on
chemistry reasoning, yielding an 8% average absolute improvement and a 30% peak
improvement. We further use the generated reasoning by GPT-4 to fine-tune
smaller LMs (e.g., Vicuna) and observe strong improvement of the smaller LMs.
This validates our approach and enables LLMs to generate high-quality
reasoning.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09657" title="Abstract">arXiv:2311.09657</a> [<a href="/pdf/2311.09657" title="Download PDF">pdf</a>, <a href="/ps/2311.09657" title="Download PostScript">ps</a>, <a href="/format/2311.09657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Access in Ukraine: characteristics and evolution from 2012 to 2021
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaliuzhna%2C+N">Nataliia Kaliuzhna</a>, 
<a href="/search/cs?searchtype=author&query=Hauschke%2C+C">Christian Hauschke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">This study investigates development of open access (OA) to publications
produced by authors affiliated with Ukrainian universities and research
organisations in the period 2012-2021. In order to get a comprehensive overview
we assembled data from three popular databases: Dimensions, Web of Science
(WoS) and Scopus. Our final dataset consisted of 187,135 records. To determine
the OA status of each article, this study utilised Unpaywall data which was
obtained via API. It was determined that 71.5% of all considered articles
during the observed period were openly available at the time of analysis. Our
findings show that gold OA was the most prevalent type of OA through a 10 years
studied period. We also took a look at how OA varies by research fields, how
dominant large commercial publishers are in disseminating national research and
the preferences of authors regarding where to self-archive articles versions.
We concluded that Ukraine needs to be thoughtful with engagement with large
publishers and make sure academics control publishing, not for profit
companies, which would monopolise research output distribution, leaving
national publishers behind. Beyond that we put a special emphasis on the
importance of FAIRness of national scholarly communication infrastructure in
monitoring OA uptake.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09659" title="Abstract">arXiv:2311.09659</a> [<a href="/pdf/2311.09659" title="Download PDF">pdf</a>, <a href="/format/2311.09659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Deformable Object Manipulation By Using Interactive Perception  and Assistive Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Peng Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In the field of robotic manipulation, the proficiency of deformable object
manipulation lags behind human capabilities due to the inherent characteristics
of deformable objects. These objects have infinite degrees of freedom,
resulting in non-trivial perception and state estimation, and complex dynamics,
complicating the prediction of future configurations. Although recent research
has focused on deformable object manipulation, most approaches rely on static
vision and simple manipulation techniques, limiting the performance level. This
paper proposes two solutions to enhance the performance: interactive perception
and the use of assistive tools. The first solution posits that optimal
perspectives exist during deformable object manipulation, facilitating easier
state estimation. By exploring the action-perception regularity, interactive
perception facilitates better manipulation and perception. The second solution
advocates for the use of assistive tools, a hallmark of human intelligence, to
improve manipulation performance. For instance, a folding board can aid in
garment folding tasks by reducing object deformation and managing complex
dynamics. Hence, this research aims to address the deformable object
manipulation problem by incorporating interactive perception and assistive
tools to augment manipulation performance.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09661" title="Abstract">arXiv:2311.09661</a> [<a href="/pdf/2311.09661" title="Download PDF">pdf</a>, <a href="/format/2311.09661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolving Domain Adaptation of Pretrained Language Models for Text  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yun-Shiuan Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Dhruv Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Uppaal%2C+R">Rheeya Uppaal</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ananya Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Luhang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sreedhar%2C+M+N">Makesh Narsimhan Sreedhar</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sijia Yang</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+T+T">Timothy T. Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junjie Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Adapting pre-trained language models (PLMs) for time-series text
classification amidst evolving domain shifts (EDS) is critical for maintaining
accuracy in applications like stance detection. This study benchmarks the
effectiveness of evolving domain adaptation (EDA) strategies, notably
self-training, domain-adversarial training, and domain-adaptive pretraining,
with a focus on an incremental self-training method. Our analysis across
various datasets reveals that this incremental method excels at adapting PLMs
to EDS, outperforming traditional domain adaptation techniques. These findings
highlight the importance of continually updating PLMs to ensure their
effectiveness in real-world applications, paving the way for future research
into PLM robustness against the natural temporal evolution of language.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09662" title="Abstract">arXiv:2311.09662</a> [<a href="/pdf/2311.09662" title="Download PDF">pdf</a>, <a href="/format/2311.09662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AXI-REALM: A Lightweight and Modular Interconnect Extension for Traffic  Regulation and Monitoring of Heterogeneous Real-Time SoCs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benz%2C+T">Thomas Benz</a>, 
<a href="/search/cs?searchtype=author&query=Ottaviano%2C+A">Alessandro Ottaviano</a>, 
<a href="/search/cs?searchtype=author&query=Balas%2C+R">Robert Balas</a>, 
<a href="/search/cs?searchtype=author&query=Garofalo%2C+A">Angelo Garofalo</a>, 
<a href="/search/cs?searchtype=author&query=Restuccia%2C+F">Francesco Restuccia</a>, 
<a href="/search/cs?searchtype=author&query=Biondi%2C+A">Alessandro Biondi</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, accepted as a regular paper at DATE24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">The increasing demand for heterogeneous functionality in the automotive
industry and the evolution of chip manufacturing processes have led to the
transition from federated to integrated critical real-time embedded systems
(CRTESs). This leads to higher integration challenges of conventional timing
predictability techniques due to access contention on shared resources, which
can be resolved by providing system-level observability and controllability in
hardware. We focus on the interconnect as a shared resource and propose
AXI-REALM, a lightweight, modular, and technology-independent real-time
extension to industry-standard AXI4 interconnects, available open-source.
AXI-REALM uses a credit-based mechanism to distribute and control the bandwidth
in a multi-subordinate system on periodic time windows, proactively prevents
denial of service from malicious actors in the system, and tracks each
manager's access and interference statistics for optimal budget and period
selection. We provide detailed performance and implementation cost assessment
in a 12nm node and an end-to-end functional case study implementing AXI-REALM
into an open-source Linux-capable RISC-V SoC. In a system with a
general-purpose core and a hardware accelerator's DMA engine causing
interference on the interconnect, AXI-REALM achieves fair bandwidth
distribution among managers, allowing the core to recover 68.2 % of its
performance compared to the case without contention. Moreover, near-ideal
performance (above 95 %) can be achieved by distributing the available
bandwidth in favor of the core, improving the worst-case memory access latency
from 264 to below eight cycles. Our approach minimizes buffering compared to
other solutions and introduces only 2.45 % area overhead compared to the
original SoC.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09663" title="Abstract">arXiv:2311.09663</a> [<a href="/pdf/2311.09663" title="Download PDF">pdf</a>, <a href="/format/2311.09663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zenkai -- Framework For Exploring Beyond Backpropagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Short%2C+G">Greg Short</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages. Source code available at <a href="https://www.github.com/short-greg/zenkai">this https URL</a> with documentation at <a href="https://zenkai.readthedocs.io/en/latest/.">this https URL</a> Can be installed with pip install zenkai
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Zenkai is an open-source framework designed to give researchers more control
and flexibility over building and training deep learning machines. It does this
by dividing the deep learning machine into layers of semi-autonomous learning
machines with their own target and learning algorithm. This is to allow
researchers greater exploration such as the use of non-differentiable layers or
learning algorithms beyond those based on error backpropagation.
<br />Backpropagation Rumelhart et al. [1986] has powered deep learning to become
one of the most exciting fields of the 21st century. As a result, a large
number of software tools have been developed to support efficient
implementation and training of neural networks through the use of backpropa-
gation. While these have been critical to the success of deep learning,
building frameworks around backpropagation can make it challenging to implement
solutions that do not adhere to it. Zenkai aims to make it easier to get around
these limitations and help researchers more easily explore new frontiers in
deep learning that do not strictly adhere to the backpropagation framework.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09665" title="Abstract">arXiv:2311.09665</a> [<a href="/pdf/2311.09665" title="Download PDF">pdf</a>, <a href="/format/2311.09665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case  Study on Wisdom of Partisan Crowds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yun-Shiuan Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Suresh%2C+S">Siddharth Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Harlalka%2C+N">Nikunj Harlalka</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+A">Agam Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Hawkins%2C+R">Robert Hawkins</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sijia Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Dhavan Shah</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junjie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+T+T">Timothy T. Rogers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study investigates the potential of Large Language Models (LLMs) to
simulate human group dynamics, particularly within politically charged
contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to
role-play as Democrat and Republican personas, engaging in a structured
interaction akin to human group study. Our approach evaluates how agents'
responses evolve through social influence. Our key findings indicate that LLM
agents role-playing detailed personas and without Chain-of-Thought (CoT)
reasoning closely align with human behaviors, while having CoT reasoning hurts
the alignment. However, incorporating explicit biases into agent prompts does
not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning
LLMs with human data shows promise in achieving human-like behavior but poses a
risk of overfitting certain behaviors. These findings show the potential and
limitations of using LLM agents in modeling human group phenomena.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09667" title="Abstract">arXiv:2311.09667</a> [<a href="/pdf/2311.09667" title="Download PDF">pdf</a>, <a href="/format/2311.09667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repetitive nonoverlapping sequential pattern mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+M">Meng Geng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Youxi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fournier-Viger%2C+P">Philippe Fournier-Viger</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xingquan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xindong Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Sequential pattern mining (SPM) is an important branch of knowledge discovery
that aims to mine frequent sub-sequences (patterns) in a sequential database.
Various SPM methods have been investigated, and most of them are classical SPM
methods, since these methods only consider whether or not a given pattern
occurs within a sequence. Classical SPM can only find the common features of
sequences, but it ignores the number of occurrences of the pattern in each
sequence, i.e., the degree of interest of specific users. To solve this
problem, this paper addresses the issue of repetitive nonoverlapping sequential
pattern (RNP) mining and proposes the RNP-Miner algorithm. To reduce the number
of candidate patterns, RNP-Miner adopts an itemset pattern join strategy. To
improve the efficiency of support calculation, RNP-Miner utilizes the candidate
support calculation algorithm based on the position dictionary. To validate the
performance of RNP-Miner, 10 competitive algorithms and 20 sequence databases
were selected. The experimental results verify that RNP-Miner outperforms the
other algorithms, and using RNPs can achieve a better clustering performance
than raw data and classical frequent patterns. All the algorithms were
developed using the PyCharm environment and can be downloaded from
https://github.com/wuc567/Pattern-Mining/tree/master/RNP-Miner.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09668" title="Abstract">arXiv:2311.09668</a> [<a href="/pdf/2311.09668" title="Download PDF">pdf</a>, <a href="/format/2311.09668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Generation Quality of Watermarked Large Language Models  via Word Importance Scoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhouxing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The strong general capabilities of Large Language Models (LLMs) bring
potential ethical risks if they are unrestrictedly accessible to malicious
users. Token-level watermarking inserts watermarks in the generated texts by
altering the token probability distributions with a private random number
generator seeded by its prefix tokens. However, this watermarking algorithm
alters the logits during generation, which can lead to a downgraded text
quality if it chooses to promote tokens that are less relevant given the input.
In this work, we propose to improve the quality of texts generated by a
watermarked language model by Watermarking with Importance Scoring (WIS). At
each generation step, we estimate the importance of the token to generate, and
prevent it from being impacted by watermarking if it is important for the
semantic correctness of the output. We further propose three methods to predict
importance scoring, including a perturbation-based method and two model-based
methods. Empirical experiments show that our method can generate texts with
better quality with comparable level of detection rate.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09670" title="Abstract">arXiv:2311.09670</a> [<a href="/pdf/2311.09670" title="Download PDF">pdf</a>, <a href="/format/2311.09670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An inexact Matrix-Newton method for solving NEPv
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Werner%2C+T">Tom Werner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Our main contribution in this paper is to present an inexact Matrix-Newton
algorithm that uses the tools for Newton's method in Banach spaces to solve a
particular type of matrix valued problem: the nonlinear eigenvalue problem with
eigenvector dependency (NEPv). We provide the conditions for our algorithm to
be applicable to NEPv and show how to exploit the problem structure for an ef-
ficient implementation. Various numerical experiments are provided that
indicate the advantage of quadratic order of convergence over the linear order
of the well- established SCF algorithm.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09671" title="Abstract">arXiv:2311.09671</a> [<a href="/pdf/2311.09671" title="Download PDF">pdf</a>, <a href="/ps/2311.09671" title="Download PostScript">ps</a>, <a href="/format/2311.09671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Contrastive Learning With Theory Guarantee
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+N+N">Ngoc N. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+L">Lam Tran</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Hoang Phan</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+A">Anh Bui</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+T">Tung Pham</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T">Toan Tran</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Trung Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 0 figures. arXiv admin note: text overlap with <a href="/abs/2305.10252">arXiv:2305.10252</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Contrastive learning (CL) is a self-supervised training paradigm that allows
us to extract meaningful features without any label information. A typical CL
framework is divided into two phases, where it first tries to learn the
features from unlabelled data, and then uses those features to train a linear
classifier with the labeled data. While a fair amount of existing theoretical
works have analyzed how the unsupervised loss in the first phase can support
the supervised loss in the second phase, none has examined the connection
between the unsupervised loss and the robust supervised loss, which can shed
light on how to construct an effective unsupervised loss for the first phase of
CL. To fill this gap, our work develops rigorous theories to dissect and
identify which components in the unsupervised loss can help improve the robust
supervised loss and conduct proper experiments to verify our findings.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09675" title="Abstract">arXiv:2311.09675</a> [<a href="/pdf/2311.09675" title="Download PDF">pdf</a>, <a href="/format/2311.09675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where Do People Tell Stories Online? Story Detection Across Online  Communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Antoniak%2C+M">Maria Antoniak</a>, 
<a href="/search/cs?searchtype=author&query=Mire%2C+J">Joel Mire</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>, 
<a href="/search/cs?searchtype=author&query=Ash%2C+E">Elliott Ash</a>, 
<a href="/search/cs?searchtype=author&query=Piper%2C+A">Andrew Piper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">People share stories online for a myriad of purposes, whether as a means of
self-disclosure, processing difficult personal experiences, providing needed
information or entertainment, or persuading others to share their beliefs.
Better understanding of online storytelling can illuminate the dynamics of
social movements, sensemaking practices, persuasion strategies, and more.
However, unlike other media such as books and visual content where the
narrative nature of the content is often overtly signaled at the document
level, studying storytelling in online communities is challenging due to the
mixture of storytelling and non-storytelling behavior, which can be
interspersed within documents and across diverse topics and settings. We
introduce a codebook and create the Storytelling in Online Communities Corpus,
an expert-annotated dataset of 502 English-language posts and comments with
labeled story and event spans. Using our corpus, we train and evaluate an
online story detection model, which we use to investigate the role storytelling
of in different social contexts. We identify distinctive features of online
storytelling, the prevalence of storytelling among different communities, and
the conversational patterns of storytelling.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09677" title="Abstract">arXiv:2311.09677</a> [<a href="/pdf/2311.09677" title="Download PDF">pdf</a>, <a href="/format/2311.09677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R-Tuning: Teaching Large Language Models to Refuse Unknown Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+Y+R">Yi R. Fung</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Q">Qing Lian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have revolutionized numerous domains with their
impressive performance but still face their challenges. A predominant issue is
the propensity for these models to generate non-existent facts, a concern
termed hallucination. Our research is motivated by the observation that
previous instruction tuning methods force the model to complete a sentence no
matter whether the model knows the knowledge or not. When the question is out
of the parametric knowledge, it will try to make up something and fail to
indicate when it lacks knowledge. In this paper, we present a new approach
called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized
by first identifying the knowledge gap between parametric knowledge and the
instruction tuning data. Then, we construct the refusal-aware data based on the
knowledge intersection, to tune LLMs to refrain from responding to questions
beyond its parametric knowledge. Experimental results demonstrate this new
instruction tuning approach effectively improves a model's ability to answer
known questions and refrain from answering unknown questions. Furthermore, when
tested on out-of-domain datasets, the refusal ability was found to be a
meta-skill that could be generalized to other tasks. Further analysis
surprisingly finds that learning the uncertainty during training displays a
better ability to estimate uncertainty than uncertainty-based testing. Our code
will be released at https://github.com/shizhediao/R-Tuning.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09680" title="Abstract">arXiv:2311.09680</a> [<a href="/pdf/2311.09680" title="Download PDF">pdf</a>, <a href="/format/2311.09680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Large Models in Vision: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Ziyan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
in reliable methods. Despite the abundance of literature on trustworthy LMs in
language, a systematic survey specifically delving into the trustworthiness of
LMs in vision remains absent. In order to mitigate this gap, we summarize four
relevant concerns that obstruct the trustworthy usage in vision of LMs in this
survey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers'
understanding of the field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09682" title="Abstract">arXiv:2311.09682</a> [<a href="/pdf/2311.09682" title="Download PDF">pdf</a>, <a href="/format/2311.09682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MacGyver: Are Large Language Models Creative Problem Solvers?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yufei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Ravichander%2C+A">Abhilasha Ravichander</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Lianhui Qin</a>, 
<a href="/search/cs?searchtype=author&query=Bras%2C+R+L">Ronan Le Bras</a>, 
<a href="/search/cs?searchtype=author&query=Marjieh%2C+R">Raja Marjieh</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>, 
<a href="/search/cs?searchtype=author&query=Brahman%2C+F">Faeze Brahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We explore the creative problem-solving capabilities of modern large language
models (LLMs) in a constrained setting. The setting requires circumventing a
cognitive bias known in psychology as ''functional fixedness'' to use familiar
objects in innovative or unconventional ways. To this end, we create MacGyver,
an automatically generated dataset consisting of 1,600 real-world problems that
deliberately trigger functional fixedness and require thinking
'out-of-the-box'. We then present our collection of problems to both LLMs and
humans to compare and contrast their problem-solving abilities. We show that
MacGyver is challenging for both groups, but in unique and complementary ways.
For example, humans typically excel in solving problems that they are familiar
with but may struggle with tasks requiring domain-specific knowledge, leading
to a higher variance. On the other hand, LLMs, being exposed to a variety of
highly specialized knowledge, attempt broader problems but are prone to
overconfidence and propose actions that are physically infeasible or
inefficient. We also provide a detailed error analysis of LLMs, and demonstrate
the potential of enhancing their problem-solving ability with novel prompting
techniques such as iterative step-wise reflection and divergent-convergent
thinking. This work provides insight into the creative problem-solving
capabilities of humans and AI and illustrates how psychological paradigms can
be extended into large-scale tasks for comparing humans and machines.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09683" title="Abstract">arXiv:2311.09683</a> [<a href="/pdf/2311.09683" title="Download PDF">pdf</a>, <a href="/ps/2311.09683" title="Download PostScript">ps</a>, <a href="/format/2311.09683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling daily mobility using mobile data traffic at fine  spatiotemporal scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christidis%2C+P">Panayotis Christidis</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalo%2C+M+V">Maria Vega Gonzalo</a>, 
<a href="/search/cs?searchtype=author&query=Radics%2C+M">Miklos Radics</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NetMob 2023 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We applied a data-driven approach that explores the usability of the NetMob
2023 dataset in modelling mobility patterns within an urban context. We
combined the data with a highly suitable external source, the ENACT dataset,
which provides a 1 km x 1km grid with estimates of the day and night population
across Europe. We developed three sets of XGBoost models that predict the
population in each 100m x 100m grid cell used in NetMob2023 based on the mobile
data traffic of the 68 online services covered in the dataset, using the ENACT
values as ground truth. The results suggest that the NetMob 2023 data can be
useful for the estimation of the day and night population and grid cell level
and can explain part of the dynamics of urban mobility.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09684" title="Abstract">arXiv:2311.09684</a> [<a href="/pdf/2311.09684" title="Download PDF">pdf</a>, <a href="/format/2311.09684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Physicians Know How to Prompt? The Need for Automatic Prompt  Optimization Help in Clinical Note Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zonghai Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jaafar%2C+A">Ahmed Jaafar</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beining Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yue Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhichao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Equal contribution for the first two authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study examines the effect of prompt engineering on the performance of
Large Language Models (LLMs) in clinical note generation. We introduce an
Automatic Prompt Optimization (APO) framework to refine initial prompts and
compare the outputs of medical experts, non-medical experts, and APO-enhanced
GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in
standardizing prompt quality across clinical note sections. A human-in-the-loop
approach shows that experts maintain content quality post-APO, with a
preference for their own modifications, suggesting the value of expert
customization. We recommend a two-phase optimization process, leveraging
APO-GPT4 for consistency and expert input for personalization.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09687" title="Abstract">arXiv:2311.09687</a> [<a href="/pdf/2311.09687" title="Download PDF">pdf</a>, <a href="/format/2311.09687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inducing Political Bias Allows Language Models Anticipate Partisan  Reactions to Controversies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zihao He</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Siyi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Ashwin Rao</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+K">Kristina Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Social media platforms are rife with politically charged discussions.
Therefore, accurately deciphering and predicting partisan biases using Large
Language Models (LLMs) is increasingly critical. In this study, we address the
challenge of understanding political bias in digitized discourse using LLMs.
While traditional approaches often rely on finetuning separate models for each
political faction, our work innovates by employing a singular,
instruction-tuned LLM to reflect a spectrum of political ideologies. We present
a comprehensive analytical framework, consisting of Partisan Bias Divergence
Assessment and Partisan Class Tendency Prediction, to evaluate the model's
alignment with real-world political ideologies in terms of stances, emotions,
and moral foundations. Our findings reveal the model's effectiveness in
capturing emotional and moral nuances, albeit with some challenges in stance
detection, highlighting the intricacies and potential for refinement in NLP
tools for politically sensitive contexts. This research contributes
significantly to the field by demonstrating the feasibility and importance of
nuanced political understanding in LLMs, particularly for applications
requiring acute awareness of political bias.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09690" title="Abstract">arXiv:2311.09690</a> [<a href="/pdf/2311.09690" title="Download PDF">pdf</a>, <a href="/format/2311.09690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CDMPP: A Device-Model Agnostic Framework for Latency Prediction of  Tensor Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hanpeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Junwei Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Juntao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yanghua Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yibo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haibin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EuroSys 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Performance (cs.PF)

</div>
<p class="mathjax">Deep Neural Networks (DNNs) have shown excellent performance in a wide range
of machine learning applications. Knowing the latency of running a DNN model or
tensor program on a specific device is useful in various tasks, such as DNN
graph- or tensor-level optimization and device selection. Considering the large
space of DNN models and devices that impede direct profiling of all
combinations, recent efforts focus on building a predictor to model the
performance of DNN models on different devices. However, none of the existing
attempts have achieved a cost model that can accurately predict the performance
of various tensor programs while supporting both training and inference
accelerators. We propose CDMPP, an efficient tensor program latency prediction
framework for both cross-model and cross-device prediction. We design an
informative but efficient representation of tensor programs, called compact
ASTs, and a pre-order-based positional encoding method, to capture the internal
structure of tensor programs. We develop a domain-adaption-inspired method to
learn domain-invariant representations and devise a KMeans-based sampling
algorithm, for the predictor to learn from different domains (i.e., different
DNN operators and devices). Our extensive experiments on a diverse range of DNN
models and devices demonstrate that CDMPP significantly outperforms
state-of-the-art baselines with 14.03% and 10.85% prediction error for
cross-model and cross-device prediction, respectively, and one order of
magnitude higher training efficiency. The implementation and the expanded
dataset are available at https://github.com/joapolarbear/cdmpp.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09692" title="Abstract">arXiv:2311.09692</a> [<a href="/pdf/2311.09692" title="Download PDF">pdf</a>, <a href="/format/2311.09692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting Unsupervised Reinforcement Learning with Self-Reference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+A">Andrew Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+E">Erle Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Rui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Matthieu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong-Jin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Humans possess the ability to draw on past experiences explicitly when
learning new tasks and applying them accordingly. We believe this capacity for
self-referencing is especially advantageous for reinforcement learning agents
in the unsupervised pretrain-then-finetune setting. During pretraining, an
agent's past experiences can be explicitly utilized to mitigate the
nonstationarity of intrinsic rewards. In the finetuning phase, referencing
historical trajectories prevents the unlearning of valuable exploratory
behaviors. Motivated by these benefits, we propose the Self-Reference (SR)
approach, an add-on module explicitly designed to leverage historical
information and enhance agent performance within the pretrain-finetune
paradigm. Our approach achieves state-of-the-art results in terms of
Interquartile Mean (IQM) performance and Optimality Gap reduction on the
Unsupervised Reinforcement Learning Benchmark for model-free methods, recording
an 86% IQM and a 16% Optimality Gap. Additionally, it improves current
algorithms by up to 17% IQM and reduces the Optimality Gap by 31%. Beyond
performance enhancement, the Self-Reference add-on also increases sample
efficiency, a crucial attribute for real-world applications.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09693" title="Abstract">arXiv:2311.09693</a> [<a href="/pdf/2311.09693" title="Download PDF">pdf</a>, <a href="/format/2311.09693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BLT: Can Large Language Models Handle Basic Legal Text?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blair-Stanek%2C+A">Andrew Blair-Stanek</a>, 
<a href="/search/cs?searchtype=author&query=Holzenberger%2C+N">Nils Holzenberger</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We find that the best publicly available LLMs like GPT-4 and PaLM 2 currently
perform poorly at basic text handling required of lawyers or paralegals, such
as looking up the text at a line of a witness deposition or at a subsection of
a contract. We introduce a benchmark to quantify this poor performance, which
casts into doubt LLMs' current reliability as-is for legal practice. Finetuning
for these tasks brings an older LLM to near-perfect performance on our test set
and also raises performance on a related legal task. This stark result
highlights the need for more domain expertise in LLM training.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09694" title="Abstract">arXiv:2311.09694</a> [<a href="/pdf/2311.09694" title="Download PDF">pdf</a>, <a href="/format/2311.09694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Ashim Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rajendhran%2C+R">Rishanth Rajendhran</a>, 
<a href="/search/cs?searchtype=author&query=Stringham%2C+N">Nathan Stringham</a>, 
<a href="/search/cs?searchtype=author&query=Srikumar%2C+V">Vivek Srikumar</a>, 
<a href="/search/cs?searchtype=author&query=Marasovi%C4%87%2C+A">Ana Marasovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Are the longstanding robustness issues in NLP resolved by today's larger and
more performant models? To address this question, we conduct a thorough
investigation using 19 models of different sizes spanning different
architectural choices and pretraining objectives. We conduct evaluations using
(a) OOD and challenge test sets, (b) CheckLists, (c) contrast sets, and (d)
adversarial inputs. Our analysis reveals that not all OOD tests provide further
insight into robustness. Evaluating with CheckLists and contrast sets shows
significant gaps in model performance; merely scaling models does not make them
sufficiently robust. Finally, we point out that current approaches for
adversarial evaluations of models are themselves problematic: they can be
easily thwarted, and in their current forms, do not represent a sufficiently
deep probe of model robustness. We conclude that not only is the question of
robustness in NLP as yet unresolved, but even some of the approaches to measure
robustness need to be reassessed.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09696" title="Abstract">arXiv:2311.09696</a> [<a href="/pdf/2311.09696" title="Download PDF">pdf</a>, <a href="/format/2311.09696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fumbling in Babel: An Investigation into ChatGPT&#x27;s Language  Identification Ability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Rui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Adebara%2C+I">Ife Adebara</a>, 
<a href="/search/cs?searchtype=author&query=Doan%2C+K+D">Khai Duy Doan</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q">Qisheng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, ChatGPT has emerged as a powerful NLP tool that can carry out
several tasks. However, the range of languages ChatGPT can handle remains
largely a mystery. In this work, we investigate ChatGPT's language
identification abilities. For this purpose, we compile Babel-670, a benchmark
comprising $670$ languages representing $23$ language families. Languages in
Babel-670 run the gamut between the very high-resource to the very low-resource
and are spoken in five continents. We then study ChatGPT's (both GPT-3.5 and
GPT-4) ability to (i) identify both language names and language codes (ii)
under both zero- and few-shot conditions (iii) with and without provision of
label set. When compared to smaller finetuned language identification tools, we
find that ChatGPT lags behind. Our empirical analysis shows the reality that
ChatGPT still resides in a state of potential enhancement before it can
sufficiently serve diverse communities.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09702" title="Abstract">arXiv:2311.09702</a> [<a href="/pdf/2311.09702" title="Download PDF">pdf</a>, <a href="/format/2311.09702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go  without Hallucination?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bangzheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Ben Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xingyu Fu</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Dan Roth</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the recent advancement in large language models (LLMs) and their high
performances across numerous benchmarks, recent research has unveiled that LLMs
suffer from hallucinations and unfaithful reasoning. This work studies a
specific type of hallucination induced by semantic associations. Specifically,
we investigate to what extent LLMs take shortcuts from certain keyword/entity
biases in the prompt instead of following the correct reasoning path. To
quantify this phenomenon, we propose a novel probing method and benchmark
called EureQA. We start from questions that LLMs will answer correctly with
utmost certainty, and mask the important entity with evidence sentence
recursively, asking models to find masked entities according to a chain of
evidence before answering the question.
<br />During the construction of the evidence, we purposefully replace semantic
clues (entities) that may lead to the correct answer with distractor clues
(evidence) that will not directly lead to the correct answer but require a
chain-like reasoning process. We evaluate if models can follow the correct
reasoning chain instead of short-cutting through distractor clues. We find that
existing LLMs lack the necessary capabilities to follow correct reasoning paths
and resist the attempt of greedy shortcuts. We show that the distractor
semantic associations often lead to model hallucination, which is strong
evidence that questions the validity of current LLM reasoning.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09704" title="Abstract">arXiv:2311.09704</a> [<a href="/pdf/2311.09704" title="Download PDF">pdf</a>, <a href="/format/2311.09704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> International System of Quantities library in VDM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freitas%2C+L">Leo Freitas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages,, 1 figure, 21st Overture Workshop, Lubeck 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The International Systems of Quantities (ISQ) standard was published in 1960
to tame the wide diversity of measurement systems being developed across the
world, such as the centimetre-gram-second versus the meter-kilogram-second for
example. Such a standard is highly motivated by the potential of ``trivial''
(rather error-prone) mistakes in converting between incompatible units. There
have been such accidents in space missions, medical devices, etc. Thus,
rendering modelling or simulation experiments unusable or unsafe. We address
this problem by providing a \textbf{SAFE}-ISQ VDM-library that is: Simple,
Accurate, Fast, and Effective. It extends an ecosystem of other VDM
mathematical toolkit extensions, which include a translation and proof
environment for VDM in Isabelle at https://github.com/leouk/VDM_Toolkit.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09706" title="Abstract">arXiv:2311.09706</a> [<a href="/pdf/2311.09706" title="Download PDF">pdf</a>, <a href="/format/2311.09706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Autonomous Hypothesis Verification via Language Models with  Minimal Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takagi%2C+S">Shiro Takagi</a>, 
<a href="/search/cs?searchtype=author&query=Yamauchi%2C+R">Ryutaro Yamauchi</a>, 
<a href="/search/cs?searchtype=author&query=Kumagai%2C+W">Wataru Kumagai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Research automation efforts usually employ AI as a tool to automate specific
tasks within the research process. To create an AI that truly conduct research
themselves, it must independently generate hypotheses, design verification
plans, and execute verification. Therefore, we investigated if an AI itself
could autonomously generate and verify hypothesis for a toy machine learning
research problem. We prompted GPT-4 to generate hypotheses and Python code for
hypothesis verification with limited methodological guidance. Our findings
suggest that, in some instances, GPT-4 can autonomously generate and validate
hypotheses without detailed guidance. While this is a promising result, we also
found that none of the verifications were flawless, and there remain
significant challenges in achieving autonomous, human-level research using only
generic instructions. These findings underscore the need for continued
exploration to develop a general and autonomous AI researcher.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09707" title="Abstract">arXiv:2311.09707</a> [<a href="/pdf/2311.09707" title="Download PDF">pdf</a>, <a href="/format/2311.09707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization  in Programming Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diera%2C+A">Andor Diera</a>, 
<a href="/search/cs?searchtype=author&query=Dahou%2C+A">Abdelhalim Dahou</a>, 
<a href="/search/cs?searchtype=author&query=Galke%2C+L">Lukas Galke</a>, 
<a href="/search/cs?searchtype=author&query=Karl%2C+F">Fabian Karl</a>, 
<a href="/search/cs?searchtype=author&query=Sihler%2C+F">Florian Sihler</a>, 
<a href="/search/cs?searchtype=author&query=Scherp%2C+A">Ansgar Scherp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at GenBench workshop, EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Language models can serve as a valuable tool for software developers to
increase productivity. Large generative models can be used for code generation
and code completion, while smaller encoder-only models are capable of
performing code search tasks using natural language queries.These capabilities
are heavily influenced by the quality and diversity of the available training
data. Source code datasets used for training usually focus on the most popular
languages and testing is mostly conducted on the same distributions, often
overlooking low-resource programming languages. Motivated by the NLP
generalization taxonomy proposed by Hupkes et.\,al., we propose a new benchmark
dataset called GenCodeSearchNet (GeCS) which builds upon existing natural
language code search datasets to systemically evaluate the programming language
understanding generalization capabilities of language models. As part of the
full dataset, we introduce a new, manually curated subset StatCodeSearch that
focuses on R, a popular but so far underrepresented programming language that
is often used by researchers outside the field of computer science. For
evaluation and comparison, we collect several baseline results using fine-tuned
BERT-style models and GPT-style large language models in a zero-shot setting.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09708" title="Abstract">arXiv:2311.09708</a> [<a href="/pdf/2311.09708" title="Download PDF">pdf</a>, <a href="/format/2311.09708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Self-enhancement Multitask Framework for Unsupervised Aspect Category  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thi-Nhung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+H">Hoang Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Kiem-Hieu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+T">Tuan-Dung Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Our work addresses the problem of unsupervised Aspect Category Detection
using a small set of seed words. Recent works have focused on learning
embedding spaces for seed words and sentences to establish similarities between
sentences and aspects. However, aspect representations are limited by the
quality of initial seed words, and model performances are compromised by noise.
To mitigate this limitation, we propose a simple framework that automatically
enhances the quality of initial seed words and selects high-quality sentences
for training instead of using the entire dataset. Our main concepts are to add
a number of seed words to the initial set and to treat the task of noise
resolution as a task of augmenting data for a low-resource task. In addition,
we jointly train Aspect Category Detection with Aspect Term Extraction and
Aspect Term Polarity to further enhance performance. This approach facilitates
shared representation learning, allowing Aspect Category Detection to benefit
from the additional guidance offered by other tasks. Extensive experiments
demonstrate that our framework surpasses strong baselines on standard datasets.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09709" title="Abstract">arXiv:2311.09709</a> [<a href="/pdf/2311.09709" title="Download PDF">pdf</a>, <a href="/format/2311.09709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Inference with Lexical Shortlisting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogoychev%2C+N">Nikolay Bogoychev</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pinzhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Haddow%2C+B">Barry Haddow</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+A">Alexandra Birch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language model (LLM) inference is computation and memory intensive, so
we adapt lexical shortlisting to it hoping to improve both. While lexical
shortlisting is well-explored in tasks like machine translation, it requires
modifications before being suitable for LLMs as the intended applications vary
significantly. Our work studies two heuristics to shortlist sub-vocabulary at
LLM inference time: Unicode-based script filtering and corpus-based selection.
We explore different LLM families and sizes, and we find that lexical
shortlisting can reduce the memory usage of some models by nearly 50\% and has
an upper bound of 25\% improvement in generation speed. In this pilot study, we
also identify the drawbacks of such vocabulary selection methods and propose
avenues for future research.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09711" title="Abstract">arXiv:2311.09711</a> [<a href="/pdf/2311.09711" title="Download PDF">pdf</a>, <a href="/format/2311.09711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Second-order Rate Analysis of a Two-user Gaussian Interference Channel  with Heterogeneous Blocklength Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kailun Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+P">Pin-Hsun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Mross%2C+M">Marcel Mross</a>, 
<a href="/search/cs?searchtype=author&query=Jorswieck%2C+E+A">Eduard A. Jorswieck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We consider a two-user Gaussian interference channel with heterogeneous
blocklength constraints (HB-GIC), strong interference, and two private
messages. We propose to apply the successive interference cancellation with
early decoding, i.e., decoding a message with a number of received symbols less
than the blocklength at the receiver. We determine the necessary number of
received symbols to achieve successful decoding of the longer codeword that
satisfies the input power constraints and target average error probability
constraints.
<br />To attain the results, we investigate the dependence testing bound analysis
over an independent and identically distributed (i.i.d.) Gaussian input.
<br />Besides, we derive the second-order achievable rate region of the considered
HB-GIC. By numerical results based on the rate-profile approach, we compare the
derived second-order rate region to the first-order one, which shows the rate
back-off of the considered model due to the impact of finite blocklength.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09712" title="Abstract">arXiv:2311.09712</a> [<a href="/pdf/2311.09712" title="Download PDF">pdf</a>, <a href="/format/2311.09712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularized Conventions: Equilibrium Computation as a Model of Pragmatic  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacob%2C+A+P">Athul Paul Jacob</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Andreas%2C+J">Jacob Andreas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present a model of pragmatic language understanding, where utterances are
produced and understood by searching for regularized equilibria of signaling
games. In this model (which we call ReCo, for Regularized Conventions),
speakers and listeners search for contextually appropriate utterance--meaning
mappings that are both close to game-theoretically optimal conventions and
close to a shared, ''default'' semantics. By characterizing pragmatic
communication as equilibrium search, we obtain principled sampling algorithms
and formal guarantees about the trade-off between communicative success and
naturalness. Across several datasets capturing real and idealized human
judgments about pragmatic implicatures, ReCo matches or improves upon
predictions made by best response and rational speech act models of language
understanding.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09714" title="Abstract">arXiv:2311.09714</a> [<a href="/pdf/2311.09714" title="Download PDF">pdf</a>, <a href="/format/2311.09714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of the Metrics, Uses, and Subjects of Diversity-Based  Techniques in Software Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elgendy%2C+I+T">Islam T. Elgendy</a>, 
<a href="/search/cs?searchtype=author&query=Hierons%2C+R+M">Robert M. Hierons</a>, 
<a href="/search/cs?searchtype=author&query=McMinn%2C+P">Phil McMinn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages, 7 figures, 9 tables, and uses PRIMEarxiv.sty
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">There has been a significant amount of interest regarding the use of
diversity-based testing techniques in software testing over the past two
decades. Diversity-based testing (DBT) technique uses similarity metrics to
leverage the dissimilarity between software artefacts - such as requirements,
abstract models, program structures, or inputs - in order to address a software
testing problem. DBT techniques have been used to assist in finding solutions
to several different types of problems including generating test cases,
prioritising them, and reducing very large test suites. This paper is a
systematic survey of DBT techniques that summarises the key aspects and trends
of 144 papers that report the use of 70 different similarity metrics with 24
different types of software artefacts, which have been used by researchers to
tackle 11 different types of software testing problems. We further present an
analysis of the recent trends in DBT techniques and review the different
application domains to which the techniques have been applied, giving an
overview of the tools developed by researchers to do so. Finally, the paper
identifies some DBT challenges that are potential topics for future work.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09718" title="Abstract">arXiv:2311.09718</a> [<a href="/pdf/2311.09718" title="Download PDF">pdf</a>, <a href="/format/2311.09718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You don&#x27;t need a personality test to know these models are unreliable:  Assessing the Reliability of Large Language Models on Psychometric  Instruments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+B">Bangzhao Shu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lechen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+M">Minje Choi</a>, 
<a href="/search/cs?searchtype=author&query=Dunagan%2C+L">Lavinia Dunagan</a>, 
<a href="/search/cs?searchtype=author&query=Card%2C+D">Dallas Card</a>, 
<a href="/search/cs?searchtype=author&query=Jurgens%2C+D">David Jurgens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, 5 tables. First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The versatility of Large Language Models (LLMs) on natural language
understanding tasks has made them popular for research in social sciences. In
particular, to properly understand the properties and innate personas of LLMs,
researchers have performed studies that involve using prompts in the form of
questions that ask LLMs of particular opinions. In this study, we take a
cautionary step back and examine whether the current format of prompting
enables LLMs to provide responses in a consistent and robust manner. We first
construct a dataset that contains 693 questions encompassing 39 different
instruments of persona measurement on 115 persona axes. Additionally, we design
a set of prompts containing minor variations and examine LLM's capabilities to
generate accurate answers, as well as consistency variations to examine their
consistency towards simple perturbations such as switching the option order.
Our experiments on 15 different open-source LLMs reveal that even simple
perturbations are sufficient to significantly downgrade a model's
question-answering ability, and that most LLMs have low negation consistency.
Our results suggest that the currently widespread practice of prompting is
insufficient to accurately capture model perceptions, and we discuss potential
alternatives to improve such issues.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09721" title="Abstract">arXiv:2311.09721</a> [<a href="/pdf/2311.09721" title="Download PDF">pdf</a>, <a href="/format/2311.09721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Evaluating the Integration of Reasoning and Action in LLM Agents with  Database Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nan%2C+L">Linyong Nan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Ellen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+W">Weijin Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenfei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study introduces a new long-form database question answering dataset
designed to evaluate how Large Language Models (LLMs) interact with a SQL
interpreter. The task necessitates LLMs to strategically generate multiple SQL
queries to retrieve sufficient data from a database, to reason with the
acquired context, and to synthesize them into a comprehensive analytical
narrative. Our findings highlight that this task poses great challenges even
for the state-of-the-art GPT-4 model. We propose and evaluate two interaction
strategies, and provide a fine-grained analysis of the individual stages within
the interaction. A key discovery is the identification of two primary
bottlenecks hindering effective interaction: the capacity for planning and the
ability to generate multiple SQL queries. To address the challenge of
accurately assessing answer quality, we introduce a multi-agent evaluation
framework that simulates the academic peer-review process, enhancing the
precision and reliability of our evaluations. This framework allows for a more
nuanced understanding of the strengths and limitations of current LLMs in
complex retrieval and reasoning tasks.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09724" title="Abstract">arXiv:2311.09724</a> [<a href="/pdf/2311.09724" title="Download PDF">pdf</a>, <a href="/format/2311.09724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outcome-supervised Verifiers for Planning in Mathematical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+A">Anningzhe Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/FreedomIntelligence/OVM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) often struggle with maintaining accuracy across
a sequence of intermediate reasoning steps in mathematical reasoning, leading
to error propagation that undermines the final result. The current methodology
to mitigate this issue primarily involves using a verifier model to assess the
correctness of generated solution candidates, focusing either on the overall
reasoning path or on an incomplete reasoning path. By rethinking this approach,
we argue that assessing potentials of incomplete reasoning paths could be more
advantageous as it guides towards correct final answers, transforming the task
into a \textit{planning} problem. Our proposed verifier, the
Outcome-supervision Value Model (OVM), employs outcome supervision for
training, offering an efficient and intuitive method for \textit{planning} by
prioritizing steps that lead to accurate conclusions over mere per-step
correctness. Furthermore, the OVM eschews the need for labor-intensive
annotations on step-level correctness, enhancing its scalability. Our
experiments on two multi-step mathematical reasoning datasets, GSM8K and Game
of 24, demonstrate the superior performance of the OVM model. Notably, in
GSM8K, our \textbf{OVM-7B model achieves state-of-the-art results among LLMs up
to 13B parameters}; especially it does not utilize GPT-4 or code execution.
These findings offer a novel perspective on the role of outcome supervision in
training verifiers for multi-step reasoning tasks and provide theoretical
justification for its advantage in value estimation for planning.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09726" title="Abstract">arXiv:2311.09726</a> [<a href="/pdf/2311.09726" title="Download PDF">pdf</a>, <a href="/format/2311.09726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MS-Former: Memory-Supported Transformer for Weakly Supervised Change  Detection with Patch-Level Annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenglai Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changdong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianju Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fully supervised change detection methods have achieved significant
advancements in performance, yet they depend severely on acquiring costly
pixel-level labels. Considering that the patch-level annotations also contain
abundant information corresponding to both changed and unchanged objects in
bi-temporal images, an intuitive solution is to segment the changes with
patch-level annotations. How to capture the semantic variations associated with
the changed and unchanged regions from the patch-level annotations to obtain
promising change results is the critical challenge for the weakly supervised
change detection task. In this paper, we propose a memory-supported transformer
(MS-Former), a novel framework consisting of a bi-directional attention block
(BAB) and a patch-level supervision scheme (PSS) tailored for weakly supervised
change detection with patch-level annotations. More specifically, the BAM
captures contexts associated with the changed and unchanged regions from the
temporal difference features to construct informative prototypes stored in the
memory bank. On the other hand, the BAM extracts useful information from the
prototypes as supplementary contexts to enhance the temporal difference
features, thereby better distinguishing changed and unchanged regions. After
that, the PSS guides the network learning valuable knowledge from the
patch-level annotations, thus further elevating the performance. Experimental
results on three benchmark datasets demonstrate the effectiveness of our
proposed method in the change detection task. The demo code for our work will
be publicly available at \url{https://github.com/guanyuezhen/MS-Former}.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09727" title="Abstract">arXiv:2311.09727</a> [<a href="/pdf/2311.09727" title="Download PDF">pdf</a>, <a href="/format/2311.09727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Comments Given in Documents Inspection in Software  Development PBL and Investigation of the Impact on Students
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sato%2C+O">Oh Sato</a>, 
<a href="/search/cs?searchtype=author&query=Hazeyama%2C+A">Atsuo Hazeyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">This study considers inspection conducted in software development PBL as
learning feedback and investigates the impact of each inspection comment on
students. The authors have already collected most inspection comments for not
only requirements specification but also UML diagrams on GitHub. The authors
develop a tool that collects comments given in Figma to GitHub. We examine the
impact on students of each classification of inspection comments based on the
post-lesson questionnaire submitted by the students. Finally, we present the
benefits that classification of inspection comments can bring to PBL and
discuss automatic comment classification by machine learning enabled by
text-based comments and the concept of software development PBL support
application enabled by automatic classification of inspection comments.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09730" title="Abstract">arXiv:2311.09730</a> [<a href="/pdf/2311.09730" title="Download PDF">pdf</a>, <a href="/format/2311.09730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning with Whom? Large Language Models Have Gender and Racial Biases  in Subjective NLP Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huaman Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+J">Jiaxin Pei</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+M">Minje Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jurgens%2C+D">David Jurgens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Human perception of language depends on personal backgrounds like gender and
ethnicity. While existing studies have shown that large language models (LLMs)
hold values that are closer to certain societal groups, it is unclear whether
their prediction behaviors on subjective NLP tasks also exhibit a similar bias.
In this study, leveraging the POPQUORN dataset which contains annotations of
diverse demographic backgrounds, we conduct a series of experiments on four
popular LLMs to investigate their capability to understand group differences
and potential biases in their predictions for politeness and offensiveness. We
find that for both tasks, model predictions are closer to the labels from White
and female participants. We further explore prompting with the target
demographic labels and show that including the target demographic in the prompt
actually worsens the model's performance. More specifically, when being
prompted to respond from the perspective of "Black" and "Asian" individuals,
models show lower performance in predicting both overall scores as well as the
scores from corresponding groups. Our results suggest that LLMs hold gender and
racial biases for subjective NLP tasks and that demographic-infused prompts
alone may be insufficient to mitigate such effects. Code and data are available
at https://github.com/Jiaxin-Pei/LLM-Group-Bias.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09731" title="Abstract">arXiv:2311.09731</a> [<a href="/pdf/2311.09731" title="Download PDF">pdf</a>, <a href="/format/2311.09731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prudent Silence or Foolish Babble? Examining Large Language Models&#x27;  Responses to the Unknown
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Genglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lifan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) often struggle when faced with situations where
they lack the prerequisite knowledge to generate a sensical response. In these
cases, models tend to fabricate and hallucinate, rather than appropriately
signaling uncertainty as humans would. This behavior misaligns with human
conversational norms and presents challenges surrounding responsible and
ethical AI development. This work aims to systematically investigate LLMs'
behaviors in such situations. We curate an adversarial question-answering
benchmark containing unanswerable questions targeting information absent from
the LLM's training data. Concretely, these unanswerable questions contain
non-existent concepts or false premises. When presented with such unanswerable
questions, an LLM should appropriately convey uncertainty, and be able to
challenge the premise and refuse to generate a response. While facing
answerable valid questions, a model should demonstrate a positive correlation
between accuracy and confidence. Using a model-agnostic unified confidence
elicitation approach, we observe that LLMs that have gone through instruction
finetuning and reinforcement learning from human feedback (RLHF) perform
significantly better than their counterparts that do not. Moreover, uncertainty
expression 1 through our elicitation method does not always stay consistent
with the perceived confidence of the direct response of an LLM. Our findings
call for further research into teaching LLMs to proactively and reliably
express uncertainty.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09732" title="Abstract">arXiv:2311.09732</a> [<a href="/pdf/2311.09732" title="Download PDF">pdf</a>, <a href="/format/2311.09732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Source Prompt: Coordinated Pre-training of Language Models on Diverse  Corpora from Multiple Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yipei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+D">Dakuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiaqing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yipeng Geng</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+Y">Yingsi Xin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hengkui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Ken Chen</a>, 
<a href="/search/cs?searchtype=author&query=zhang%2C+r">ruiji zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yanghua Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pre-trained language models (PLMs) have established the new paradigm in the
field of NLP. For more powerful PLMs, one of the most popular and successful
way is to continuously scale up sizes of the models and the pre-training
corpora. These large corpora are generally obtained by converging smaller ones
from multiple sources, they are thus growing increasingly diverse. However, the
side-effects of these colossal converged corpora remain understudied. In this
paper, we identify the disadvantage of heterogeneous corpora from multiple
sources for pre-training PLMs. Towards coordinated pre-training on diverse
corpora, we further propose source prompts (SP), which explicitly prompt the
model of the data source at the pre-training and fine-tuning stages. Results of
extensive experiments demonstrate that PLMs pre-trained with SP on diverse
corpora gain significant improvement in various downstream tasks.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09733" title="Abstract">arXiv:2311.09733</a> [<a href="/pdf/2311.09733" title="Download PDF">pdf</a>, <a href="/format/2311.09733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOKA: Moral Knowledge Augmentation for Moral Event Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X+F">Xinliang Frederick Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Winston Wu</a>, 
<a href="/search/cs?searchtype=author&query=Beauchamp%2C+N">Nick Beauchamp</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">News media employ moral language to create memorable stories, and readers
often engage with the content that align with their values. Moral theories have
been applied to news analysis studying moral values in isolation, while the
intricate dynamics among participating entities in shaping moral events have
been overlooked. This is mainly due to the use of obscure language to conceal
evident ideology and values, coupled with the insufficient moral reasoning
capability in most existing NLP systems, where LLMs are no exception. To study
this phenomenon, we first annotate a new dataset, MORAL EVENTS, consisting of
5,494 structured annotations on 474 news articles by diverse US media across
the political spectrum. We further propose MOKA, a moral event extraction
framework with MOral Knowledge Augmentation, that leverages knowledge derived
from moral words and moral scenarios. Experimental results show that MOKA
outperforms competitive baselines across three moral event understanding tasks.
Further analyses illuminate the selective reporting of moral events by media
outlets of different ideological leanings, suggesting the significance of
event-level morality analysis in news. Our datasets and codebase are available
at https://github.com/launchnlp/MOKA.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09734" title="Abstract">arXiv:2311.09734</a> [<a href="/pdf/2311.09734" title="Download PDF">pdf</a>, <a href="/format/2311.09734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking the Newsworthiness of Public Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spangher%2C+A">Alexander Spangher</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>, 
<a href="/search/cs?searchtype=author&query=Welsh%2C+B">Ben Welsh</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tumgoren%2C+S">Serdar Tumgoren</a>, 
<a href="/search/cs?searchtype=author&query=May%2C+J">Jonathan May</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Journalists must find stories in huge amounts of textual data (e.g. leaks,
bills, press releases) as part of their jobs: determining when and why text
becomes news can help us understand coverage patterns and help us build
assistive tools. Yet, this is challenging because very few labelled links
exist, language use between corpora is very different, and text may be covered
for a variety of reasons. In this work we focus on news coverage of local
public policy in the San Francisco Bay Area by the San Francisco Chronicle.
First, we gather news articles, public policy documents and meeting recordings
and link them using probabilistic relational modeling, which we show is a
low-annotation linking methodology that outperforms other retrieval-based
baselines. Second, we define a new task: newsworthiness prediction, to predict
if a policy item will get covered. We show that different aspects of public
policy discussion yield different newsworthiness signals. Finally we perform
human evaluation with expert journalists and show our systems identify policies
they consider newsworthy with 68% F1 and our coverage recommendations are
helpful with an 84% win-rate.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09735" title="Abstract">arXiv:2311.09735</a> [<a href="/pdf/2311.09735" title="Download PDF">pdf</a>, <a href="/format/2311.09735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GEO: Generative Engine Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+P">Pranjal Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Murahari%2C+V">Vishvak Murahari</a>, 
<a href="/search/cs?searchtype=author&query=Rajpurohit%2C+T">Tanmay Rajpurohit</a>, 
<a href="/search/cs?searchtype=author&query=Kalyan%2C+A">Ashwin Kalyan</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+K+R">Karthik R Narasimhan</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A">Ameet Deshpande</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">The advent of large language models (LLMs) has ushered in a new paradigm of
search engines that use generative models to gather and summarize information
to answer user queries. This emerging technology, which we formalize under the
unified framework of Generative Engines (GEs), has the potential to generate
accurate and personalized responses, and is rapidly replacing traditional
search engines like Google and Bing. Generative Engines typically satisfy
queries by synthesizing information from multiple sources and summarizing them
with the help of LLMs. While this shift significantly improves \textit{user}
utility and \textit{generative search engine} traffic, it results in a huge
challenge for the third stakeholder -- website and content creators. Given the
black-box and fast-moving nature of Generative Engines, content creators have
little to no control over when and how their content is displayed. With
generative engines here to stay, the right tools should be provided to ensure
that creator economy is not severely disadvantaged. To address this, we
introduce Generative Engine Optimization (GEO), a novel paradigm to aid content
creators in improving the visibility of their content in Generative Engine
responses through a black-box optimization framework for optimizing and
defining visibility metrics. We facilitate systematic evaluation in this new
paradigm by introducing GEO-bench, a benchmark of diverse user queries across
multiple domains, coupled with sources required to answer these queries.
Through rigorous evaluation, we show that GEO can boost visibility by up to
40\% in generative engine responses. Moreover, we show the efficacy of these
strategies varies across domains, underscoring the need for domain-specific
methods. Our work opens a new frontier in the field of information discovery
systems, with profound implications for generative engines and content
creators.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09736" title="Abstract">arXiv:2311.09736</a> [<a href="/pdf/2311.09736" title="Download PDF">pdf</a>, <a href="/format/2311.09736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARE: Extracting Experimental Findings From Clinical Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naik%2C+A">Aakanksha Naik</a>, 
<a href="/search/cs?searchtype=author&query=Kuehl%2C+B">Bailey Kuehl</a>, 
<a href="/search/cs?searchtype=author&query=Bransom%2C+E">Erin Bransom</a>, 
<a href="/search/cs?searchtype=author&query=Downey%2C+D">Doug Downey</a>, 
<a href="/search/cs?searchtype=author&query=Hope%2C+T">Tom Hope</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Extracting fine-grained experimental findings from literature can provide
massive utility for scientific applications. Prior work has focused on
developing annotation schemas and datasets for limited aspects of this problem,
leading to simpler information extraction datasets which do not capture the
real-world complexity and nuance required for this task. Focusing on
biomedicine, this work presents CARE (Clinical Aggregation-oriented Result
Extraction) -- a new IE dataset for the task of extracting clinical findings.
We develop a new annotation schema capturing fine-grained findings as n-ary
relations between entities and attributes, which includes phenomena challenging
for current IE systems such as discontinuous entity spans, nested relations,
and variable arity n-ary relations. Using this schema, we collect extensive
annotations for 700 abstracts from two sources: clinical trials and case
reports. We also benchmark the performance of various state-of-the-art IE
systems on our dataset, including extractive models and generative LLMs in
fully supervised and limited data settings. Our results demonstrate the
difficulty of our dataset -- even SOTA models such as GPT4 struggle,
particularly on relation extraction. We release our annotation schema and CARE
to encourage further research on extracting and aggregating scientific findings
from literature.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09737" title="Abstract">arXiv:2311.09737</a> [<a href="/pdf/2311.09737" title="Download PDF">pdf</a>, <a href="/format/2311.09737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality  MRI Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingnan Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhitong Gao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuming He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, Machine Learning for Health (ML4H) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Cross-modal MRI segmentation is of great value for computer-aided medical
diagnosis, enabling flexible data acquisition and model generalization.
However, most existing methods have difficulty in handling local variations in
domain shift and typically require a significant amount of data for training,
which hinders their usage in practice. To address these problems, we propose a
novel adaptive domain generalization framework, which integrates a
learning-free cross-domain representation based on image gradient maps and a
class prior-informed test-time adaptation strategy for mitigating local domain
shift. We validate our approach on two multi-modal MRI datasets with six
cross-modal segmentation tasks. Across all the task settings, our method
consistently outperforms competing approaches and shows a stable performance
even with limited training data.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09741" title="Abstract">arXiv:2311.09741</a> [<a href="/pdf/2311.09741" title="Download PDF">pdf</a>, <a href="/format/2311.09741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Constitutes a Faithful Summary? Preserving Author Perspectives in  News Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaochuang Han</a>, 
<a href="/search/cs?searchtype=author&query=Balachandran%2C+V">Vidhisha Balachandran</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C+Y">Chan Young Park</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sachin Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we take a first step towards designing summarization systems
that are faithful to the author's opinions and perspectives. Focusing on a case
study of preserving political perspectives in news summarization, we find that
existing approaches alter the political opinions and stances of news articles
in more than 50% of summaries, misrepresenting the intent and perspectives of
the news authors. We thus propose P^3Sum, a diffusion model-based summarization
approach controlled by political perspective classifiers. In P^3Sum, the
political leaning of a generated summary is iteratively evaluated at each
decoding step, and any drift from the article's original stance incurs a loss
back-propagated to the embedding layers, steering the political stance of the
summary at inference time. Extensive experiments on three news summarization
datasets demonstrate that P^3Sum outperforms state-of-the-art summarization
systems and large language models by up to 11.4% in terms of the success rate
of stance preservation, with on-par performance on standard summarization
utility metrics. These findings highlight the lacunae that even for
state-of-the-art models it is still challenging to preserve author perspectives
in news summarization, while P^3Sum presents an important first step towards
evaluating and developing summarization systems that are faithful to author
intent and perspectives.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09743" title="Abstract">arXiv:2311.09743</a> [<a href="/pdf/2311.09743" title="Download PDF">pdf</a>, <a href="/format/2311.09743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capturing Perspectives of Crowdsourced Annotators in Subjective Learning  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mokhberian%2C+N">Negar Mokhberian</a>, 
<a href="/search/cs?searchtype=author&query=Marmarelis%2C+M+G">Myrl G. Marmarelis</a>, 
<a href="/search/cs?searchtype=author&query=Hopp%2C+F+R">Frederic R. Hopp</a>, 
<a href="/search/cs?searchtype=author&query=Basile%2C+V">Valerio Basile</a>, 
<a href="/search/cs?searchtype=author&query=Morstatter%2C+F">Fred Morstatter</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+K">Kristina Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In most classification models, it has been assumed to have a single ground
truth label for each data point. However, subjective tasks like toxicity
classification can lead to genuine disagreement among annotators. In these
cases aggregating labels will result in biased labeling and, consequently,
biased models that can overlook minority opinions. Previous studies have shed
light on the pitfalls of label aggregation and have introduced a handful of
practical approaches to tackle this issue. Recently proposed multi-annotator
models, which predict labels individually per annotator, are vulnerable to
under-determination for annotators with small samples. This problem is
especially the case in crowd-sourced datasets. In this work, we propose
Annotator Aware Representations for Texts (AART) for subjective classification
tasks. We will show the improvement of our method on metrics that assess the
performance on capturing annotators' perspectives. Additionally, our approach
involves learning representations for annotators, allowing for an exploration
of the captured annotation behaviors.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09744" title="Abstract">arXiv:2311.09744</a> [<a href="/pdf/2311.09744" title="Download PDF">pdf</a>, <a href="/format/2311.09744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redefining the Laparoscopic Spatial Sense: AI-based Intra- and  Postoperative Measurement from Stereoimages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+L">Leopold M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Hemmer%2C+P">Patrick Hemmer</a>, 
<a href="/search/cs?searchtype=author&query=Queisner%2C+M">Moritz Queisner</a>, 
<a href="/search/cs?searchtype=author&query=Sauer%2C+I">Igor Sauer</a>, 
<a href="/search/cs?searchtype=author&query=Allmendinger%2C+S">Simeon Allmendinger</a>, 
<a href="/search/cs?searchtype=author&query=Jakubik%2C+J">Johannes Jakubik</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%B6ssing%2C+M">Michael V&#xf6;ssing</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChl%2C+N">Niklas K&#xfc;hl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38th AAAI Conference on Artificial Intelligence (AAAI-24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A significant challenge in image-guided surgery is the accurate measurement
task of relevant structures such as vessel segments, resection margins, or
bowel lengths. While this task is an essential component of many surgeries, it
involves substantial human effort and is prone to inaccuracies. In this paper,
we develop a novel human-AI-based method for laparoscopic measurements
utilizing stereo vision that has been guided by practicing surgeons. Based on a
holistic qualitative requirements analysis, this work proposes a comprehensive
measurement method, which comprises state-of-the-art machine learning
architectures, such as RAFT-Stereo and YOLOv8. The developed method is assessed
in various realistic experimental evaluation environments. Our results outline
the potential of our method achieving high accuracies in distance measurements
with errors below 1 mm. Furthermore, on-surface measurements demonstrate
robustness when applied in challenging environments with textureless regions.
Overall, by addressing the inherent challenges of image-guided surgery, we lay
the foundation for a more robust and accurate solution for intra- and
postoperative measurements, enabling more precise, safe, and efficient surgical
procedures.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09745" title="Abstract">arXiv:2311.09745</a> [<a href="/pdf/2311.09745" title="Download PDF">pdf</a>, <a href="/format/2311.09745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application-Centric Benchmarking of Distributed FaaS Platforms using  BeFaaS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grambow%2C+M">Martin Grambow</a>, 
<a href="/search/cs?searchtype=author&query=Pfandzelter%2C+T">Tobias Pfandzelter</a>, 
<a href="/search/cs?searchtype=author&query=Bermbach%2C+D">David Bermbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2102.12770">arXiv:2102.12770</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Due to the popularity of the FaaS programming model, there is now a wide
variety of commercial and open-source FaaS systems. Hence, for comparison of
different FaaS systems and their configuration options, FaaS application
developers rely on FaaS benchmarking frameworks. Existing frameworks, however,
tend to evaluate only single isolated aspects, a more holistic
application-centric benchmarking framework is still missing. In previous work,
we proposed BeFaaS, an extensible application-centric benchmarking framework
for FaaS environments that focuses on the evaluation of FaaS platforms through
realistic and typical examples of FaaS applications. In this extended paper, we
(i) enhance our benchmarking framework with additional features for distributed
FaaS setups, (ii) design application benchmarks reflecting typical FaaS use
cases, and (iii) use them to run extensive experiments with commercial cloud
FaaS platforms (AWS Lambda, Azure Functions, Google Cloud Functions) and the
tinyFaaS edge serverless platform. BeFaaS now includes four FaaS
application-centric benchmarks, is extensible for additional workload profiles
and platforms, and supports federated benchmark runs in which the benchmark
application is distributed over multiple FaaS systems while collecting
fine-grained measurement results for drill-down analysis. Our experiment
results show that (i) network transmission is a major contributor to response
latency for function chains, (ii) this effect is exacerbated in hybrid
edge-cloud deployments, (iii) the trigger delay between a published event and
the start of the triggered function ranges from about 100ms for AWS Lambda to
800ms for Google Cloud Functions, and (iv) Azure Functions shows the best cold
start behavior for our workloads.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09748" title="Abstract">arXiv:2311.09748</a> [<a href="/pdf/2311.09748" title="Download PDF">pdf</a>, <a href="/format/2311.09748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Translation Aligned Sentence Embeddings for Turkish Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Unlu%2C+E">Eren Unlu</a>, 
<a href="/search/cs?searchtype=author&query=Ciftci%2C+U">Unver Ciftci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Due to the limited availability of high quality datasets for training
sentence embeddings in Turkish, we propose a training methodology and a regimen
to develop a sentence embedding model. The central idea is simple but effective
: is to fine-tune a pretrained encoder-decoder model in two consecutive stages,
where the first stage involves aligning the embedding space with translation
pairs. Thanks to this alignment, the prowess of the main model can be better
projected onto the target language in a sentence embedding setting where it can
be fine-tuned with high accuracy in short duration with limited target language
dataset.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09750" title="Abstract">arXiv:2311.09750</a> [<a href="/pdf/2311.09750" title="Download PDF">pdf</a>, <a href="/format/2311.09750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensembles of Quantum Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tolotti%2C+E">Emiliano Tolotti</a>, 
<a href="/search/cs?searchtype=author&query=Zardini%2C+E">Enrico Zardini</a>, 
<a href="/search/cs?searchtype=author&query=Blanzieri%2C+E">Enrico Blanzieri</a>, 
<a href="/search/cs?searchtype=author&query=Pastorello%2C+D">Davide Pastorello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">In the current era, known as Noisy Intermediate-Scale Quantum (NISQ),
encoding large amounts of data in the quantum devices is challenging and the
impact of noise significantly affects the quality of the obtained results. A
viable approach for the execution of quantum classification algorithms is the
introduction of a well-known machine learning paradigm, namely, the ensemble
methods. Indeed, the ensembles combine multiple internal classifiers, which are
characterized by compact sizes due to the smaller data subsets used for
training, to achieve more accurate and robust prediction performance. In this
way, it is possible to reduce the qubits requirements with respect to a single
larger classifier while achieving comparable or improved performance. In this
work, we present an implementation and an extensive empirical evaluation of
ensembles of quantum classifiers for binary classification, with the purpose of
providing insights into their effectiveness, limitations, and potential for
enhancing the performance of basic quantum models. In particular, three
classical ensemble methods and three quantum classifiers have been taken into
account here. Hence, the scheme that has been implemented (in Python) has a
hybrid nature. The results (obtained on real-world datasets) have shown an
accuracy advantage for the ensemble techniques with respect to the single
quantum classifiers, and also an improvement in robustness. In fact, the
ensembles have turned out to be able to mitigate both unsuitable data
normalizations and repeated measurement inaccuracies, making quantum
classifiers more stable.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09753" title="Abstract">arXiv:2311.09753</a> [<a href="/pdf/2311.09753" title="Download PDF">pdf</a>, <a href="/format/2311.09753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIFFNAT: Improving Diffusion Image Quality Using Natural Image  Statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+A">Aniket Roy</a>, 
<a href="/search/cs?searchtype=author&query=Suin%2C+M">Maiterya Suin</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Anshul Shah</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+K">Ketul Shah</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chellappa%2C+R">Rama Chellappa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models have advanced generative AI significantly in terms of
editing and creating naturalistic images. However, efficiently improving
generated image quality is still of paramount interest. In this context, we
propose a generic "naturalness" preserving loss function, viz., kurtosis
concentration (KC) loss, which can be readily applied to any standard diffusion
model pipeline to elevate the image quality. Our motivation stems from the
projected kurtosis concentration property of natural images, which states that
natural images have nearly constant kurtosis values across different band-pass
versions of the image. To retain the "naturalness" of the generated images, we
enforce reducing the gap between the highest and lowest kurtosis values across
the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note
that our approach does not require any additional guidance like classifier or
classifier-free guidance to improve the image quality. We validate the proposed
approach for three diverse tasks, viz., (1) personalized few-shot finetuning
using text guidance, (2) unconditional image generation, and (3) image
super-resolution. Integrating the proposed KC loss has improved the perceptual
quality across all these tasks in terms of both FID, MUSIQ score, and user
evaluation.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09755" title="Abstract">arXiv:2311.09755</a> [<a href="/pdf/2311.09755" title="Download PDF">pdf</a>, <a href="/format/2311.09755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Does Calibration Data Affect the Post-training Pruning and  Quantization of Large Language Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Williams%2C+M">Miles Williams</a>, 
<a href="/search/cs?searchtype=author&query=Aletras%2C+N">Nikolaos Aletras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pruning and quantization form the foundation of model compression for neural
networks, enabling efficient inference for large language models (LLMs).
Recently, various quantization and pruning techniques have demonstrated
state-of-the-art performance in a post-training setting. They rely upon
calibration data, a small set of unlabeled examples, to generate layer
activations. However, no prior work has systematically investigated how the
calibration data impacts the effectiveness of model compression methods. In
this paper, we present the first extensive empirical study on the effect of
calibration data upon LLM performance. We trial a variety of pruning and
quantization methods, tasks, models, and datasets. Surprisingly, we find
substantial variations in downstream task performance, contrasting existing
work that suggests a greater level of robustness to the calibration data.
Finally, we make a series of recommendations for the effective use of
calibration data in LLM quantization and pruning.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09756" title="Abstract">arXiv:2311.09756</a> [<a href="/pdf/2311.09756" title="Download PDF">pdf</a>, <a href="/format/2311.09756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children&#x27;s  Storybook Narratives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaju Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuxuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuanzhe Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Ying Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuling Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">AI models (including LLM) often rely on narrative question-answering (QA)
datasets to provide customized QA functionalities to support downstream
children education applications; however, existing datasets only include QA
pairs that are grounded within the given storybook content, but children can
learn more when teachers refer the storybook content to real-world knowledge
(e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which is
annotated by children education experts, to supplement 278 storybook narratives
with educationally appropriate commonsense knowledge. The dataset has 5,868 QA
pairs that not only originate from the storybook narrative but also contain the
commonsense knowledge grounded by an external knowledge graph (i.e.,
ConceptNet). A follow-up experiment shows that a smaller model (T5-large)
fine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineered
LLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This result
suggests that: 1) our dataset brings novel challenges to existing LLMs, and 2)
human experts' data annotation are still critical as they have much nuanced
knowledge that LLMs do not know in the children educational domain.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09757" title="Abstract">arXiv:2311.09757</a> [<a href="/pdf/2311.09757" title="Download PDF">pdf</a>, <a href="/format/2311.09757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UFPS: A unified framework for partially-annotated federated segmentation  in heterogeneous data distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Le Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L+Y">Li Yan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+T+Y">Tie Yong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+S+H">Shi Hui Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Partially supervised segmentation is a label-saving method based on datasets
with fractional classes labeled and intersectant. However, it is still far from
landing on real-world medical applications due to privacy concerns and data
heterogeneity. As a remedy without privacy leakage, federated partially
supervised segmentation (FPSS) is formulated in this work. The main challenges
for FPSS are class heterogeneity and client drift. We propose a Unified
Federated Partially-labeled Segmentation (UFPS) framework to segment pixels
within all classes for partially-annotated datasets by training a totipotential
global model without class collision. Our framework includes Unified Label
Learning and sparsed Unified Sharpness Aware Minimization for unification of
class and feature space, respectively. We find that vanilla combinations for
traditional methods in partially supervised segmentation and federated learning
are mainly hampered by class collision through empirical study. Our
comprehensive experiments on real medical datasets demonstrate better
deconflicting and generalization ability of UFPS compared with modified
methods.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09758" title="Abstract">arXiv:2311.09758</a> [<a href="/pdf/2311.09758" title="Download PDF">pdf</a>, <a href="/format/2311.09758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OrchestraLLM: Efficient Orchestration of Language Models for Dialogue  State Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chia-Hsuan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ostendorf%2C+M">Mari Ostendorf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have revolutionized the landscape of Natural
Language Processing systems, but are computationally expensive. To reduce the
cost without sacrificing performance, previous studies have explored various
approaches to harness the potential of Small Language Models (SLMs) as
cost-effective alternatives to their larger counterparts. Driven by findings
that SLMs and LLMs exhibit complementary strengths in a structured knowledge
extraction task, this work presents a novel SLM/LLM routing framework designed
to improve computational efficiency and enhance task performance. First,
exemplar pools are created to represent the types of contexts where each LM
provides a more reliable answer, leveraging a sentence embedding fine-tuned so
that context similarity is close to dialogue state similarity. Then, during
inference, the k-nearest exemplars to the testing instance are retrieved, and
the instance is routed according to majority vote. In dialogue state tracking
tasks, the proposed routing framework enhances performance substantially
compared to relying solely on LLMs, while reducing the computational costs by
over 50%.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09759" title="Abstract">arXiv:2311.09759</a> [<a href="/pdf/2311.09759" title="Download PDF">pdf</a>, <a href="/format/2311.09759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scene Text Image Super-resolution based on Text-conditional Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noguchi%2C+C">Chihiro Noguchi</a>, 
<a href="/search/cs?searchtype=author&query=Fukuda%2C+S">Shun Fukuda</a>, 
<a href="/search/cs?searchtype=author&query=Yamanaka%2C+M">Masao Yamanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scene Text Image Super-resolution (STISR) has recently achieved great success
as a preprocessing method for scene text recognition. STISR aims to transform
blurred and noisy low-resolution (LR) text images in real-world settings into
clear high-resolution (HR) text images suitable for scene text recognition. In
this study, we leverage text-conditional diffusion models (DMs), known for
their impressive text-to-image synthesis capabilities, for STISR tasks. Our
experimental results revealed that text-conditional DMs notably surpass
existing STISR methods. Especially when texts from LR text images are given as
input, the text-conditional DMs are able to produce superior quality
super-resolution text images. Utilizing this capability, we propose a novel
framework for synthesizing LR-HR paired text image datasets. This framework
consists of three specialized text-conditional DMs, each dedicated to text
image synthesis, super-resolution, and image degradation. These three modules
are vital for synthesizing distinct LR and HR paired images, which are more
suitable for training STISR methods. Our experiments confirmed that these
synthesized image pairs significantly enhance the performance of STISR methods
in the TextZoom evaluation.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09760" title="Abstract">arXiv:2311.09760</a> [<a href="/pdf/2311.09760" title="Download PDF">pdf</a>, <a href="/format/2311.09760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eventually Lattice-Linear Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A+T">Arya Tanmay Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S+S">Sandeep S Kulkarni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2109.13216">arXiv:2109.13216</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Lattice-linear systems allow nodes to execute asynchronously. We introduce
eventually lattice-linear algorithms, where lattices are induced only among the
states in a subset of the state space. The algorithm guarantees that the system
transitions to a state in one of the lattices. Then, the algorithm behaves
lattice linearly while traversing to an optimal state through that lattice.
<br />We present a lattice-linear self-stabilizing algorithm for service demand
based minimal dominating set (SDMDS) problem. Using this as an example, we
elaborate the working of, and define, eventually lattice-linear algorithms.
Then, we present eventually lattice-linear self-stabilizing algorithms for
minimal vertex cover (\mvc), maximal independent set (\mis), graph colouring
(\gc) and 2-dominating set problems (\tds).
<br />Algorithms for SDMDS, \mvc and \mis converge in 1 round plus $n$ moves
(within $2n$ moves), \gc in $n+4m$ moves, and \tds in 1 round plus $2n$ moves
(within $3n$ moves). These results are an improvement over the existing
literature. We also present experimental results to show performance gain
demonstrating the benefit of lattice-linearity.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09761" title="Abstract">arXiv:2311.09761</a> [<a href="/pdf/2311.09761" title="Download PDF">pdf</a>, <a href="/format/2311.09761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Helwe%2C+C">Chadi Helwe</a>, 
<a href="/search/cs?searchtype=author&query=Calamai%2C+T">Tom Calamai</a>, 
<a href="/search/cs?searchtype=author&query=Paris%2C+P">Pierre-Henri Paris</a>, 
<a href="/search/cs?searchtype=author&query=Clavel%2C+C">Chlo&#xe9; Clavel</a>, 
<a href="/search/cs?searchtype=author&query=Suchanek%2C+F">Fabian Suchanek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Fallacies can be used to spread disinformation, fake news, and propaganda,
underlining the importance of their detection. Automated detection and
classification of fallacies, however, remain challenging, mainly because of the
innate subjectivity of the task and the need for a comprehensive, unified
approach in existing research. Addressing these limitations, our study
introduces a novel taxonomy of fallacies that aligns and refines previous
classifications, a new annotation scheme tailored for subjective NLP tasks, and
a new evaluation method designed to handle subjectivity, adapted to precision,
recall, and F1-Score metrics. Using our annotation scheme, the paper introduces
MAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset.
MAFALDA is based on examples from various previously existing fallacy datasets
under our unified taxonomy across three levels of granularity. We then evaluate
several language models under a zero-shot learning setting using MAFALDA to
assess their fallacy detection and classification capability. Our comprehensive
evaluation not only benchmarks the performance of these models but also
provides valuable insights into their strengths and limitations in addressing
fallacious reasoning.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09762" title="Abstract">arXiv:2311.09762</a> [<a href="/pdf/2311.09762" title="Download PDF">pdf</a>, <a href="/format/2311.09762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-Guided Reasoning for Multi-Hop Question Answering in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinyoung Park</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Ameen Patel</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+O+Z">Omar Zia Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">Hyunwoo J. Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joo-Kyung Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning
capabilities of Large Language Models (LLMs) by generating a series of
rationales before the final answer. We analyze the reasoning paths generated by
CoT and find two issues in multi-step reasoning: (i) Generating rationales
irrelevant to the question, (ii) Unable to compose subquestions or queries for
generating/retrieving all the relevant information. To address them, we propose
a graph-guided CoT prompting method, which guides the LLMs to reach the correct
answer with graph representation/verification steps. Specifically, we first
leverage LLMs to construct a "question/rationale graph" by using knowledge
extraction prompting given the initial question and the rationales generated in
the previous steps. Then, the graph verification step diagnoses the current
rationale triplet by comparing it with the existing question/rationale graph to
filter out irrelevant rationales and generate follow-up questions to obtain
relevant information. Additionally, we generate CoT paths that exclude the
extracted graph information to represent the context information missed from
the graph extraction. Our graph-guided reasoning method shows superior
performance compared to previous CoT prompting and the variants on multi-hop
question answering benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09763" title="Abstract">arXiv:2311.09763</a> [<a href="/pdf/2311.09763" title="Download PDF">pdf</a>, <a href="/format/2311.09763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-time Backdoor Mitigation for Black-Box Large Language Models with  Defensive Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+W">Wenjie Mo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiashu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiongxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing studies in backdoor defense have predominantly focused on the
training phase, overlooking the critical aspect of testing time defense. This
gap becomes particularly pronounced in the context of Large Language Models
(LLMs) deployed as Web Services, which typically offer only black-box access,
rendering training-time defenses impractical. To bridge this gap, our work
introduces defensive demonstrations, an innovative backdoor defense strategy
for blackbox large language models. Our method involves identifying the task
and retrieving task-relevant demonstrations from an uncontaminated pool. These
demonstrations are then combined with user queries and presented to the model
during testing, without requiring any modifications/tuning to the black-box
model or insights into its internal mechanisms. Defensive demonstrations are
designed to counteract the adverse effects of triggers, aiming to recalibrate
and correct the behavior of poisoned models during test-time evaluations.
Extensive experiments show that defensive demonstrations are effective in
defending both instance-level and instruction-level backdoor attacks, not only
rectifying the behavior of poisoned models but also surpassing existing
baselines in most scenarios.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09765" title="Abstract">arXiv:2311.09765</a> [<a href="/pdf/2311.09765" title="Download PDF">pdf</a>, <a href="/format/2311.09765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in  Dense Encoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunji Lee</a>, 
<a href="/search/cs?searchtype=author&query=Soldaini%2C+L">Luca Soldaini</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+K">Kyle Lo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prevailing research practice today often relies on training dense retrievers
on existing large datasets such as MSMARCO and then experimenting with ways to
improve zero-shot generalization capabilities to unseen domains. While prior
work has tackled this challenge through resource-intensive steps such as data
augmentation, architectural modifications, increasing model size, or even
further base model pretraining, comparatively little investigation has examined
whether the training procedures themselves can be improved to yield better
generalization capabilities in the resulting models. In this work, we recommend
a simple recipe for training dense encoders: Train on MSMARCO with
parameter-efficient methods, such as LoRA, and opt for using in-batch negatives
unless given well-constructed hard negatives. We validate these recommendations
using the BEIR benchmark and find results are persistent across choice of dense
encoder and base model size and are complementary to other resource-intensive
strategies for out-of-domain generalization such as architectural modifications
or additional pretraining. We hope that this thorough and impartial study
around various training techniques, which augments other resource-intensive
methods, offers practical insights for developing a dense retrieval model that
effectively generalizes, even when trained on a single dataset.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09766" title="Abstract">arXiv:2311.09766</a> [<a href="/pdf/2311.09766" title="Download PDF">pdf</a>, <a href="/format/2311.09766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Moosavi%2C+N+S">Nafise Sadat Moosavi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic evaluation of generated textual content presents an ongoing
challenge within the field of NLP. Given the impressive capabilities of modern
language models (LMs) across diverse NLP tasks, there is a growing trend to
employ these models in creating innovative evaluation metrics for automated
assessment of generation tasks. This paper investigates a pivotal question: Do
language model-driven evaluation metrics inherently exhibit bias favoring texts
generated by the same underlying language model? Specifically, we assess
whether prominent LM-based evaluation metrics--namely, BARTScore, T5Score, and
GPTScore--demonstrate a favorable bias toward their respective underlying LMs
in the context of summarization tasks. Our findings unveil a latent bias,
particularly pronounced when such evaluation metrics are used in an
reference-free manner without leveraging gold summaries. These results
underscore that assessments provided by generative evaluation models can be
influenced by factors beyond the inherent text quality, highlighting the
necessity of developing more dependable evaluation protocols in the future.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09768" title="Abstract">arXiv:2311.09768</a> [<a href="/pdf/2311.09768" title="Download PDF">pdf</a>, <a href="/format/2311.09768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing dataset affinity prediction in object detection to assess  training data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Becker%2C+S">Stefan Becker</a>, 
<a href="/search/cs?searchtype=author&query=Bayer%2C+J">Jens Bayer</a>, 
<a href="/search/cs?searchtype=author&query=Hug%2C+R">Ronny Hug</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCbner%2C+W">Wolfgang H&#xfc;bner</a>, 
<a href="/search/cs?searchtype=author&query=Arens%2C+M">Michael Arens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Data pooling offers various advantages, such as increasing the sample size,
improving generalization, reducing sampling bias, and addressing data sparsity
and quality, but it is not straightforward and may even be counterproductive.
Assessing the effectiveness of pooling datasets in a principled manner is
challenging due to the difficulty in estimating the overall information content
of individual datasets. Towards this end, we propose incorporating a data
source prediction module into standard object detection pipelines. The module
runs with minimal overhead during inference time, providing additional
information about the data source assigned to individual detections. We show
the benefits of the so-called dataset affinity score by automatically selecting
samples from a heterogeneous pool of vehicle datasets. The results show that
object detectors can be trained on a significantly sparser set of training
samples without losing detection accuracy.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09770" title="Abstract">arXiv:2311.09770</a> [<a href="/pdf/2311.09770" title="Download PDF">pdf</a>, <a href="/format/2311.09770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DINO-VITS: Data-Efficient Noise-Robust Zero-Shot Voice Cloning via  Multi-Tasking with Self-Supervised Speaker Verification Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pankov%2C+V">Vikentii Pankov</a>, 
<a href="/search/cs?searchtype=author&query=Pronina%2C+V">Valeria Pronina</a>, 
<a href="/search/cs?searchtype=author&query=Kuzmin%2C+A">Alexander Kuzmin</a>, 
<a href="/search/cs?searchtype=author&query=Borisov%2C+M">Maksim Borisov</a>, 
<a href="/search/cs?searchtype=author&query=Usoltsev%2C+N">Nikita Usoltsev</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Golubkov%2C+A">Alexander Golubkov</a>, 
<a href="/search/cs?searchtype=author&query=Ermolenko%2C+N">Nikolai Ermolenko</a>, 
<a href="/search/cs?searchtype=author&query=Shirshova%2C+A">Aleksandra Shirshova</a>, 
<a href="/search/cs?searchtype=author&query=Matveeva%2C+Y">Yulia Matveeva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recent progress in self-supervised representation learning has opened up new
opportunities for training from unlabeled data and has been a growing trend in
voice conversion. However, unsupervised training of voice cloning seems to
remain a challenging task. In this paper we propose a semi-supervised zero-shot
voice cloning approach that works by adapting a HuBERT-based voice conversion
system to the voice cloning task and shows the robustness of such a system to
noises both in training data (we add noises resulting in up to 0db
signal-to-noise-ratio to 35% of training data with no significant degradation
of evaluation metrics) and in the target speaker reference audio at inference.
Moreover, such a method does not require any type of denoising or
noise-labeling of training data. Finally, we introduce a novel multi-tasking
approach by incorporating self-supervised DINO loss into joint training of a
CAM++ based speaker verification system and a unit-based VITS cloning system.
We show that it significantly improves the quality of generated audio over
baselines, especially for noisy target speaker references.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09773" title="Abstract">arXiv:2311.09773</a> [<a href="/pdf/2311.09773" title="Download PDF">pdf</a>, <a href="/format/2311.09773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To be or not to be? an exploration of continuously controllable prompt  engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuhan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mukai Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingyu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As the use of large language models becomes more widespread, techniques like
parameter-efficient fine-tuning and other methods for controlled generation are
gaining traction for customizing models and managing their outputs. However,
the challenge of precisely controlling how prompts influence these models is an
area ripe for further investigation. In response, we introduce ControlPE
(Continuously Controllable Prompt Engineering). ControlPE enables finer
adjustments to prompt effects, complementing existing prompt engineering, and
effectively controls continuous targets. This approach harnesses the power of
LoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting,
enabling fine-tuned adjustments to the impact of prompts. Our methodology
involves generating specialized datasets for prompt distillation, incorporating
these prompts into the LoRA model, and carefully adjusting LoRA merging weight
to regulate the influence of prompts. This provides a dynamic and adaptable
tool for prompt control. Through our experiments, we have validated the
practicality and efficacy of ControlPE. It proves to be a promising solution
for control a variety of prompts, ranging from generating short responses
prompts, refusal prompts to chain-of-thought prompts.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09774" title="Abstract">arXiv:2311.09774</a> [<a href="/pdf/2311.09774" title="Download PDF">pdf</a>, <a href="/format/2311.09774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+A">Anningzhe Gao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Feng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shunian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dingjie Song</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Wenya Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Chuyi Kong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianquan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Adapting a language model into a specific domain, a.k.a `domain adaption', is
a common practice when specialized knowledge, e.g. medicine, is not
encapsulated in a general language model like Llama2. The challenge lies in the
heterogeneity of data across the two training stages, as it varies in
languages, genres, or formats. To tackle this and simplify the learning
protocol, we propose to transform heterogeneous data, from the both
pre-training and supervised stages, into a unified, simple input-output pair
format. We validate the new protocol in the domains where proprietary LLMs like
ChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The
developed model, HuatuoGPT-II, has shown state-of-the-art performance in
Chinese medicine domain on a number of benchmarks, e.g. medical licensing
exams. It even outperforms proprietary models like ChatGPT and GPT-4 in some
aspects, especially in Traditional Chinese Medicine. Expert manual evaluations
further validate HuatuoGPT-II's advantages over existing LLMs. Notably,
HuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing
Examination where it achieved the best performance, showcasing not only its
effectiveness but also its generalization capabilities.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09775" title="Abstract">arXiv:2311.09775</a> [<a href="/pdf/2311.09775" title="Download PDF">pdf</a>, <a href="/format/2311.09775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEGA: A Memory-Efficient GNN Accelerator Exploiting Degree-Aware  Mixed-Precision Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zeyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fanrong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zejian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+Z">Zitao Mo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qinghao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaoyao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jian Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15pages, 22 figures. Accepted at HPCA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) are becoming a promising technique in various
domains due to their excellent capabilities in modeling non-Euclidean data.
Although a spectrum of accelerators has been proposed to accelerate the
inference of GNNs, our analysis demonstrates that the latency and energy
consumption induced by DRAM access still significantly impedes the improvement
of performance and energy efficiency. To address this issue, we propose a
Memory-Efficient GNN Accelerator (MEGA) through algorithm and hardware
co-design in this work. Specifically, at the algorithm level, through an
in-depth analysis of the node property, we observe that the data-independent
quantization in previous works is not optimal in terms of accuracy and memory
efficiency. This motivates us to propose the Degree-Aware mixed-precision
quantization method, in which a proper bitwidth is learned and allocated to a
node according to its in-degree to compress GNNs as much as possible while
maintaining accuracy. At the hardware level, we employ a heterogeneous
architecture design in which the aggregation and combination phases are
implemented separately with different dataflows. In order to boost the
performance and energy efficiency, we also present an Adaptive-Package format
to alleviate the storage overhead caused by the fine-grained bitwidth and
diverse sparsity, and a Condense-Edge scheduling method to enhance the data
locality and further alleviate the access irregularity induced by the extremely
sparse adjacency matrix in the graph. We implement our MEGA accelerator in a
28nm technology node. Extensive experiments demonstrate that MEGA can achieve
an average speedup of 38.3x, 7.1x, 4.0x, 3.6x and 47.6x, 7.2x, 5.4x, 4.5x
energy savings over four state-of-the-art GNN accelerators, HyGCN, GCNAX, GROW,
and SGCN, respectively, while retaining task accuracy.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09776" title="Abstract">arXiv:2311.09776</a> [<a href="/pdf/2311.09776" title="Download PDF">pdf</a>, <a href="/format/2311.09776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What to tell when? -- Information Provision as a Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rakow%2C+A">Astrid Rakow</a> (German Aerospace Center (DLR) e.V.), 
<a href="/search/cs?searchtype=author&query=Hajnorouzi%2C+M">Mehrnoush Hajnorouzi</a> (German Aerospace Center (DLR) e.V.), 
<a href="/search/cs?searchtype=author&query=Bairy%2C+A">Akhila Bairy</a> (Carl von Ossietzky University of Oldenburg)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 1-9
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Constantly informing systems (CIS), that is technical systems that provide us
with information over a long period of time, face the challenge of providing us
with helpful information. The information base of a human model changes over
time but also his mood and his ability to accept information. An information
provision strategy should hence take such aspects into account. In this paper,
we describe our vision of an approach to aid the design of CIS. We envision
using psychological models of the human mind and emotions in an information
provision game. Its analysis gives comparative insights into design variants.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09777" title="Abstract">arXiv:2311.09777</a> [<a href="/pdf/2311.09777" title="Download PDF">pdf</a>, <a href="/format/2311.09777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust Modelling and Verification Using Event-B
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fathabadi%2C+A+S">Asieh Salehi Fathabadi</a> (University of Southampton), 
<a href="/search/cs?searchtype=author&query=Yazdanpanah%2C+V">Vahid Yazdanpanah</a> (University of Southampton)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 10-16
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Trust is a crucial component in collaborative multiagent systems (MAS)
involving humans and autonomous AI agents. Rather than assuming trust based on
past system behaviours, it is important to formally verify trust by modelling
the current state and capabilities of agents. We argue for verifying actual
trust relations based on agents abilities to deliver intended outcomes in
specific contexts. To enable reasoning about different notions of trust, we
propose using the refinement-based formal method Event-B. Refinement allows
progressively introducing new aspects of trust from abstract to concrete models
incorporating knowledge and runtime states. We demonstrate modelling three
trust concepts and verifying associated trust properties in MAS. The formal,
correctness-by-construction approach allows to deduce guarantees about
trustworthy autonomy in human-AI partnerships. Overall, our contribution
facilitates rigorous verification of trust in multiagent systems.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09778" title="Abstract">arXiv:2311.09778</a> [<a href="/pdf/2311.09778" title="Download PDF">pdf</a>, <a href="/format/2311.09778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certified Control for Train Sign Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ro%C3%9Fbach%2C+J">Jan Ro&#xdf;bach</a> (Heinrich-Heine-Universit&#xe4;t D&#xfc;sseldorf), 
<a href="/search/cs?searchtype=author&query=Leuschel%2C+M">Michael Leuschel</a> (Heinrich-Heine-Universit&#xe4;t D&#xfc;sseldorf)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 69-76
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">There is considerable industrial interest in integrating AI techniques into
railway systems, notably for fully autonomous train systems. The KI-LOK
research project is involved in developing new methods for certifying such
AI-based systems. Here we explore the utility of a certified control
architecture for a runtime monitor that prevents false positive detection of
traffic signs in an AI-based perception system. The monitor uses classical
computer vision algorithms to check if the signs -- detected by an AI object
detection model -- fit predefined specifications. We provide such
specifications for some critical signs and integrate a Python prototype of the
monitor with a popular object detection model to measure relevant performance
metrics on generated data. Our initial results are promising, achieving
considerable precision gains with only minor recall reduction; however, further
investigation into generalization possibilities will be necessary.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09780" title="Abstract">arXiv:2311.09780</a> [<a href="/pdf/2311.09780" title="Download PDF">pdf</a>, <a href="/format/2311.09780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Checking for Closed-Loop Robot Reactive Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chandler%2C+C">Christopher Chandler</a> (School of Computing Science, University of Glasgow), 
<a href="/search/cs?searchtype=author&query=Porr%2C+B">Bernd Porr</a> (School of Biomedical Engineering, University of Glasgow), 
<a href="/search/cs?searchtype=author&query=Miller%2C+A">Alice Miller</a> (School of Computing Science, University of Glasgow), 
<a href="/search/cs?searchtype=author&query=Lafratta%2C+G">Giulia Lafratta</a> (School of Engineering, University of Glasgow)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 77-94
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">In this paper, we show how model checking can be used to create multi-step
plans for a differential drive wheeled robot so that it can avoid immediate
danger. Using a small, purpose built model checking algorithm in situ we
generate plans in real-time in a way that reflects the egocentric reactive
response of simple biological agents. Our approach is based on chaining
temporary control systems which are spawned to eliminate disturbances in the
local environment that disrupt an autonomous agent from its preferred action
(or resting state). The method involves a novel discretization of 2D LiDAR data
which is sensitive to bounded stochastic variations in the immediate
environment. We operationalise multi-step planning using invariant checking by
forward depth-first search, using a cul-de-sac scenario as a first test case.
Our results demonstrate that model checking can be used to plan efficient
trajectories for local obstacle avoidance, improving on the performance of a
reactive agent which can only plan one step. We achieve this in near real-time
using no pre-computed data. While our method has limitations, we believe our
approach shows promise as an avenue for the development of safe, reliable and
transparent trajectory planning in the context of autonomous vehicles.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09781" title="Abstract">arXiv:2311.09781</a> [<a href="/pdf/2311.09781" title="Download PDF">pdf</a>, <a href="/format/2311.09781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Reachability Analysis and Space Convexification for Autonomous  Racing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogomolov%2C+S">Sergiy Bogomolov</a> (Newcastle University), 
<a href="/search/cs?searchtype=author&query=Johnson%2C+T+T">Taylor T. Johnson</a> (Vanderbilt University), 
<a href="/search/cs?searchtype=author&query=Lopez%2C+D+M">Diego Manzanas Lopez</a> (Vanderbilt University), 
<a href="/search/cs?searchtype=author&query=Musau%2C+P">Patrick Musau</a> (Vanderbilt University), 
<a href="/search/cs?searchtype=author&query=Stankaitis%2C+P">Paulius Stankaitis</a> (Newcastle University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 95-112
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">This paper presents an optimisation-based approach for an obstacle avoidance
problem within an autonomous vehicle racing context. Our control regime
leverages online reachability analysis and sensor data to compute the maximal
safe traversable region that an agent can traverse within the environment. The
idea is to first compute a non-convex safe region, which then can be
convexified via a novel coupled separating hyperplane algorithm. This derived
safe area is then used to formulate a nonlinear model-predictive control
problem that seeks to find an optimal and safe driving trajectory. We evaluate
the proposed approach through a series of diverse experiments and assess the
runtime requirements of our proposed approach through an analysis of the
effects of a set of varying optimisation objectives for generating these
coupled hyperplanes.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09782" title="Abstract">arXiv:2311.09782</a> [<a href="/pdf/2311.09782" title="Download PDF">pdf</a>, <a href="/format/2311.09782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More Samples or More Prompt Inputs? Exploring Effective In-Context  Sampling for LLM Few-Shot Prompt Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guiming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+R">Ruishi Zou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuxuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiachen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hendler%2C+J">James Hendler</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While most existing works on LLM prompt-engineering focus only on how to
select a better set of data samples inside one single prompt input (In-Context
Learning or ICL), why can't we design and leverage multiple prompt inputs
together to further improve the LLM performance? In this work, we propose
In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to
produce the most confident prediction results by optimizing the construction of
multiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL
and Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate
that ICS can consistently enhance LLM's prediction performance and confidence.
An ablation study suggests that a diversity-based ICS strategy may further
improve LLM's performance, which sheds light on a new yet promising future
research direction.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09783" title="Abstract">arXiv:2311.09783</a> [<a href="/pdf/2311.09783" title="Download PDF">pdf</a>, <a href="/format/2311.09783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Data Contamination in Modern Benchmarks for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chunyuan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent observations have underscored a disparity between the inflated
benchmark scores and the actual performance of LLMs, raising concerns about
potential contamination of evaluation benchmarks. This issue is especially
critical for closed-source models and certain open-source models where training
data transparency is lacking. In this paper we study data contamination by
proposing two methods tailored for both open-source and proprietary LLMs. We
first introduce a retrieval-based system to explore potential overlaps between
evaluation benchmarks and pretraining corpora. We further present a novel
investigation protocol named \textbf{T}estset \textbf{S}lot Guessing
(\textit{TS-Guessing}), applicable to both open and proprietary models. This
approach entails masking a wrong answer in a multiple-choice question and
prompting the model to fill in the gap. Additionally, it involves obscuring an
unlikely word in an evaluation example and asking the model to produce it. We
find that certain commercial LLMs could surprisingly guess the missing option
in various test sets. Specifically, in the TruthfulQA benchmark, we find that
LLMs exhibit notable performance improvement when provided with additional
metadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4
demonstrated an exact match rate of 52\% and 57\%, respectively, in guessing
the missing options in benchmark test data. We hope these results underscore
the need for more robust evaluation methodologies and benchmarks in the field.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09784" title="Abstract">arXiv:2311.09784</a> [<a href="/pdf/2311.09784" title="Download PDF">pdf</a>, <a href="/format/2311.09784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Generation of Scenarios for System-level Simulation-based  Verification of Autonomous Driving Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goyal%2C+S">Srajan Goyal</a> (Fondazione Bruno Kessler and University of Trento), 
<a href="/search/cs?searchtype=author&query=Griggio%2C+A">Alberto Griggio</a> (Fondazione Bruno Kessler), 
<a href="/search/cs?searchtype=author&query=Kimblad%2C+J">Jacob Kimblad</a> (Fondazione Bruno Kessler), 
<a href="/search/cs?searchtype=author&query=Tonetta%2C+S">Stefano Tonetta</a> (Fondazione Bruno Kessler)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 113-129
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">With increasing complexity of Automated Driving Systems (ADS), ensuring their
safety and reliability has become a critical challenge. The Verification and
Validation (V&amp;V) of these systems are particularly demanding when AI components
are employed to implement perception and/or control functions. In ESA-funded
project VIVAS, we developed a generic framework for system-level
simulation-based V&amp;V of autonomous systems. The approach is based on a
simulation model of the system, an abstract model that describes symbolically
the system behavior, and formal methods to generate scenarios and verify the
simulation executions. Various coverage criteria can be defined to guide the
automated generation of the scenarios.
<br />In this paper, we describe the instantiation of the VIVAS framework for an
ADS case study. This is based on the integration of CARLA, a widely-used
driving simulator, and its ScenarioRunner tool, which enables the creation of
diverse and complex driving scenarios. This is also used in the CARLA
Autonomous Driving Challenge to validate different ADS agents for perception
and control based on AI, shared by the CARLA community. We describe the
development of an abstract ADS model and the formulation of a coverage
criterion that focuses on the behaviors of vehicles relative to the vehicle
with ADS under verification. Leveraging the VIVAS framework, we generate and
execute various driving scenarios, thus testing the capabilities of the AI
components. The results show the effectiveness of VIVAS in automatically
generating scenarios for system-level simulation-based V&amp;V of an automated
driving system using CARLA and ScenarioRunner. Therefore, they highlight the
potential of the approach as a powerful tool in the future of ADS V&amp;V
methodologies.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09785" title="Abstract">arXiv:2311.09785</a> [<a href="/pdf/2311.09785" title="Download PDF">pdf</a>, <a href="/format/2311.09785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enforcing Timing Properties in Motorway Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bischopink%2C+C">Christopher Bischopink</a> (University Oldenburg)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 130-143
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In previous work, we proposed a Runtime Enforcement Approach to deal with
timing properties in motorway traffic, which are present in form of Timed
Multi-Lane Spatial Logic (TMLSL) formulae, a logic tailored to express both
spatial and timing properties. Employing communication between the cars, we
utilised a nondeterministic controller guessing which actions to execute next
for each car, before asking the local monitors of the cars for permission to
execute the announced actions. In this contribution, we consider a more
reasonable controller that only considers sequences that satisfy its own
properties. This is done utilising region automata that one can generate from
the cars' specifications. In the approach, we also came along a minor
decidability result for TMLSL.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09786" title="Abstract">arXiv:2311.09786</a> [<a href="/pdf/2311.09786" title="Download PDF">pdf</a>, <a href="/format/2311.09786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correct-by-Construction Control for Stochastic and Uncertain Dynamical  Models via Formal Abstractions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Badings%2C+T">Thom Badings</a> (Radboud University), 
<a href="/search/eess?searchtype=author&query=Jansen%2C+N">Nils Jansen</a> (Radboud University), 
<a href="/search/eess?searchtype=author&query=Romao%2C+L">Licio Romao</a> (University of Oxford), 
<a href="/search/eess?searchtype=author&query=Abate%2C+A">Alessandro Abate</a> (University of Oxford)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>. arXiv admin note: text overlap with <a href="/abs/2301.01526">arXiv:2301.01526</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 144-152
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Automated synthesis of correct-by-construction controllers for autonomous
systems is crucial for their deployment in safety-critical scenarios. Such
autonomous systems are naturally modeled as stochastic dynamical models. The
general problem is to compute a controller that provably satisfies a given
task, represented as a probabilistic temporal logic specification. However,
factors such as stochastic uncertainty, imprecisely known parameters, and
hybrid features make this problem challenging. We have developed an abstraction
framework that can be used to solve this problem under various modeling
assumptions. Our approach is based on a robust finite-state abstraction of the
stochastic dynamical model in the form of a Markov decision process with
intervals of probabilities (iMDP). We use state-of-the-art verification
techniques to compute an optimal policy on the iMDP with guarantees for
satisfying the given specification. We then show that, by construction, we can
refine this policy into a feedback controller for which these guarantees carry
over to the dynamical model. In this short paper, we survey our recent research
in this area and highlight two challenges (related to scalability and dealing
with nonlinear dynamics) that we aim to address with our ongoing research.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09787" title="Abstract">arXiv:2311.09787</a> [<a href="/pdf/2311.09787" title="Download PDF">pdf</a>, <a href="/format/2311.09787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3vLTL: A Tool to Generate Automata for Three-valued LTL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belardinelli%2C+F">Francesco Belardinelli</a> (Imperial College London), 
<a href="/search/cs?searchtype=author&query=Ferrando%2C+A">Angelo Ferrando</a> (University of Genoa), 
<a href="/search/cs?searchtype=author&query=Malvone%2C+V">Vadim Malvone</a> (Telecom Paris)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 180-187
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-valued logics have a long tradition in the literature on system
verification, including run-time verification. However, comparatively fewer
model-checking tools have been developed for multi-valued specification
languages. We present 3vLTL, a tool to generate Buchi automata from formulas in
Linear-time Temporal Logic (LTL) interpreted on a three-valued semantics. Given
an LTL formula, a set of atomic propositions as the alphabet for the automaton,
and a truth value, our procedure generates a Buchi automaton that accepts all
the words that assign the chosen truth value to the LTL formula. Given the
particular type of the output of the tool, it can also be seamlessly processed
by third-party libraries in a natural way. That is, the Buchi automaton can
then be used in the context of formal verification to check whether an LTL
formula is true, false, or undefined on a given model.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09788" title="Abstract">arXiv:2311.09788</a> [<a href="/pdf/2311.09788" title="Download PDF">pdf</a>, <a href="/format/2311.09788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Proved Formal Specification and Verification of STL Operators as  Synchronous Observers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bellanger%2C+C">C&#xe9;line Bellanger</a> (ENAC, Universit&#xe9; de Toulouse), 
<a href="/search/cs?searchtype=author&query=Garoche%2C+P">Pierre-Lo&#xef;c Garoche</a> (ENAC, Universit&#xe9; de Toulouse), 
<a href="/search/cs?searchtype=author&query=Martel%2C+M">Matthieu Martel</a> (Universit&#xe9; de Perpignan Via Domitia), 
<a href="/search/cs?searchtype=author&query=Picard%2C+C">C&#xe9;lia Picard</a> (ENAC, Universit&#xe9; de Toulouse)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 188-204
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Signal Temporal Logic (STL) is a convenient formalism to express bounded
horizon properties of autonomous critical systems. STL extends LTL to
real-valued signals and associates a non-singleton bound interval to each
temporal operators. In this work we provide a rigorous encoding of non-nested
discrete-time STL formulas into Lustre synchronous observers.
<br />Our encoding provides a three-valued online semantics for the observers and
therefore enables both the verification of the property and the search of
counter-examples. A key contribution of this work is an instrumented proof of
the validity of the implementation. Each node is proved correct with respect to
the original STL semantics. All the experiments are automated with the Kind2
model-checker and the Z3 SMT solver.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09790" title="Abstract">arXiv:2311.09790</a> [<a href="/pdf/2311.09790" title="Download PDF">pdf</a>, <a href="/format/2311.09790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking Boundaries: Balancing Performance and Robustness in Deep  Wireless Traffic Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Romain%2C+I">Ilbert Romain</a>, 
<a href="/search/cs?searchtype=author&query=Thai%2C+V+H">V. Hoang Thai</a>, 
<a href="/search/cs?searchtype=author&query=Zonghua%2C+Z">Zhang Zonghua</a>, 
<a href="/search/cs?searchtype=author&query=Themis%2C+P">Palpanas Themis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Balancing the trade-off between accuracy and robustness is a long-standing
challenge in time series forecasting. While most of existing robust algorithms
have achieved certain suboptimal performance on clean data, sustaining the same
performance level in the presence of data perturbations remains extremely hard.
% In this paper, we study a wide array of perturbation scenarios and propose
novel defense mechanisms against adversarial attacks using real-world telecom
data. We compare our strategy against two existing adversarial training
algorithms under a range of maximal allowed perturbations, defined using
$\ell_{\infty}$-norm, $\in [0.1,0.4]$. % Our findings reveal that our hybrid
strategy, which is composed of a classifier to detect adversarial examples, a
denoiser to eliminate noise from the perturbed data samples, and a standard
forecaster, achieves the best performance on both clean and perturbed data. %
Our optimal model can retain up to $92.02\%$ the performance of the original
forecasting model in terms of Mean Squared Error (MSE) on clean data, while
being more robust than the standard adversarially trained models on perturbed
data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing
methods on normal and perturbed data, respectively. In addition, the components
of our models can be trained in parallel, resulting in better computational
efficiency. % Our results indicate that we can optimally balance the trade-off
between the performance and robustness of forecasting models by improving the
classifier and denoiser, even in the presence of sophisticated and destructive
poisoning attacks.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09791" title="Abstract">arXiv:2311.09791</a> [<a href="/pdf/2311.09791" title="Download PDF">pdf</a>, <a href="/format/2311.09791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-cost singular value decomposition with optimal sensor placement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hetherington%2C+A">Ashton Hetherington</a>, 
<a href="/search/cs?searchtype=author&query=Clainche%2C+S+L">Soledad Le Clainche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">This paper presents a new method capable of reconstructing datasets with
great precision and very low computational cost using a novel variant of the
singular value decomposition (SVD) algorithm that has been named low-cost SVD
(lcSVD). This algorithm allows to reconstruct a dataset from a minimum amount
of points, that can be selected randomly, equidistantly or can be calculated
using the optimal sensor placement functionality that is also presented in this
paper, which finds minimizing the reconstruction error to validate the
calculated sensor positions. This method also allows to find the optimal number
of sensors, aiding users in optimizing experimental data recollection. The
method is tested in a series of datasets, which vary between experimental and
numerical simulations, two- and three-dimensional data and laminar and
turbulent flow, have been used to demonstrate the capacity of this method based
on its high reconstruction accuracy, robustness, and computational resource
optimization. Maximum speed-up factors of 630 and memory reduction of 37\% are
found when compared to the application of standard SVD to the dataset.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09793" title="Abstract">arXiv:2311.09793</a> [<a href="/pdf/2311.09793" title="Download PDF">pdf</a>, <a href="/format/2311.09793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fossil 2.0: Formal Certificate Synthesis for the Verification and  Control of Dynamical Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Edwards%2C+A">Alec Edwards</a>, 
<a href="/search/eess?searchtype=author&query=Peruffo%2C+A">Andrea Peruffo</a>, 
<a href="/search/eess?searchtype=author&query=Abate%2C+A">Alessandro Abate</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">This paper presents Fossil 2.0, a new major release of a software tool for
the synthesis of certificates (e.g., Lyapunov and barrier functions) for
dynamical systems modelled as ordinary differential and difference equations.
Fossil 2.0 is much improved from its original release, including new
interfaces, a significantly expanded certificate portfolio, controller
synthesis and enhanced extensibility. We present these new features as part of
this tool paper. Fossil implements a counterexample-guided inductive synthesis
(CEGIS) loop ensuring the soundness of the method. Our tool uses neural
networks as templates to generate candidate functions, which are then formally
proven by an SMT solver acting as an assertion verifier. Improvements with
respect to the first release include a wider range of certificates, synthesis
of control laws, and support for discrete-time models.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09794" title="Abstract">arXiv:2311.09794</a> [<a href="/pdf/2311.09794" title="Download PDF">pdf</a>, <a href="/format/2311.09794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Non-Locality of Edge Insertions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brunck%2C+F">Florestan Brunck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">We challenge the idea that edge insertions are local improvement operations
and show that the edge-insertion algorithm must sometimes insert an edge
between vertices that are at the farthest combinatorial distance apart, and
that this edge must also cross linearly many edges of the triangulation for the
algorithm to escape a local optimum and return the optimal triangulation.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09796" title="Abstract">arXiv:2311.09796</a> [<a href="/pdf/2311.09796" title="Download PDF">pdf</a>, <a href="/format/2311.09796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting User Requests in the Context of Natural Language Standing  Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moghe%2C+N">Nikita Moghe</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+P">Patrick Xia</a>, 
<a href="/search/cs?searchtype=author&query=Andreas%2C+J">Jacob Andreas</a>, 
<a href="/search/cs?searchtype=author&query=Eisner%2C+J">Jason Eisner</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>, 
<a href="/search/cs?searchtype=author&query=Jhamtani%2C+H">Harsh Jhamtani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Users of natural language interfaces, generally powered by Large Language
Models (LLMs),often must repeat their preferences each time they make a similar
request. To alleviate this, we propose including some of a user's preferences
and instructions in natural language -- collectively termed standing
instructions -- as additional context for such interfaces. For example, when a
user states I'm hungry, their previously expressed preference for Persian food
will be automatically added to the LLM prompt, so as to influence the search
for relevant restaurants. We develop NLSI, a language-to-program dataset
consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is
paired with a user profile (a set of users specific standing instructions) and
corresponding structured representations (API calls). A key challenge in NLSI
is to identify which subset of the standing instructions is applicable to a
given dialogue. NLSI contains diverse phenomena, from simple preferences to
interdependent instructions such as triggering a hotel search whenever the user
is booking tickets to an event. We conduct experiments on NLSI using prompting
with large language models and various retrieval approaches, achieving a
maximum of 44.7% exact match on API prediction. Our results demonstrate the
challenges in identifying the relevant standing instructions and their
interpretation into API calls.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09797" title="Abstract">arXiv:2311.09797</a> [<a href="/pdf/2311.09797" title="Download PDF">pdf</a>, <a href="/format/2311.09797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance  Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongjun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yitao Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs'
capabilities in applying financial knowledge to solve complex math word
problems. Compared to prior works, this study features three core advancements.
First, KnowledgeMath includes 1,259 problems with a hybrid of textual and
tabular content and require college-level knowledge in the finance domain for
effective resolution. Second, we provide expert-annotated, detailed solution
references in Python program format, ensuring a high-quality benchmark for LLM
assessment. Finally, we evaluate a wide spectrum of 14 LLMs with different
prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. The
current best-performing system (i.e., GPT-4 with Program-of-Thoughts) achieves
only 45.4% accuracy, leaving substantial room for improvement. While
knowledge-augmented LLMs can improve the performance (e.g., from 23.9% to 32.0%
for GPT-3.5), it is still significantly lower the estimated human expert
performance of 94%. We believe that KnowledgeMath can facilitate future
research on domain-specific knowledge retrieval and augmentation into the math
word problem-solving process. We will release the benchmark and code at
https://github.com/yale-nlp/KnowledgeMath.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09799" title="Abstract">arXiv:2311.09799</a> [<a href="/pdf/2311.09799" title="Download PDF">pdf</a>, <a href="/format/2311.09799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Far Can We Extract Diverse Perspectives from Large Language Models?  Criteria-Based Diversity Prompting!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayati%2C+S+A">Shirley Anugrah Hayati</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minhwa Lee</a>, 
<a href="/search/cs?searchtype=author&query=Rajagopal%2C+D">Dheeraj Rajagopal</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NLP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Collecting diverse human data on subjective NLP topics is costly and
challenging. As Large Language Models (LLMs) have developed human-like
capabilities, there is a recent trend in collaborative efforts between humans
and LLMs for generating diverse data, offering potential scalable and efficient
solutions. However, the extent of LLMs' capability to generate diverse
perspectives on subjective topics remains an unexplored question. In this
study, we investigate LLMs' capacity for generating diverse perspectives and
rationales on subjective topics, such as social norms and argumentative texts.
We formulate this problem as diversity extraction in LLMs and propose a
criteria-based prompting technique to ground diverse opinions and measure
perspective diversity from the generated criteria words. Our results show that
measuring semantic diversity through sentence embeddings and distance metrics
is not enough to measure perspective diversity. To see how far we can extract
diverse perspectives from LLMs, or called diversity coverage, we employ a
step-by-step recall prompting for generating more outputs from the model in an
iterative manner. As we apply our prompting method to other tasks (hate speech
labeling and story continuation), indeed we find that LLMs are able to generate
diverse opinions according to the degree of task subjectivity.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09800" title="Abstract">arXiv:2311.09800</a> [<a href="/pdf/2311.09800" title="Download PDF">pdf</a>, <a href="/format/2311.09800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of  Information-Seeking Dialogue via Behavioural Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razumovskaia%2C+E">Evgeniia Razumovskaia</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Markovi%C4%87%2C+P">Pavle Markovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Cichy%2C+T">Tomasz Cichy</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+T">Tsung-Hsien Wen</a>, 
<a href="/search/cs?searchtype=author&query=Budzianowski%2C+P">Pawe&#x142; Budzianowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Factuality is a crucial requirement in information seeking dialogue: the
system should respond to the user's queries so that the responses are
meaningful and aligned with the knowledge provided to the system. However, most
modern large language models suffer from hallucinations, that is, they generate
responses not supported by or contradicting the knowledge source. To mitigate
the issue and increase faithfulness of information-seeking dialogue systems, we
introduce BeInfo, a simple yet effective method that applies behavioural tuning
to aid information-seeking dialogue. Relying on three standard datasets, we
show that models tuned with BeInfo} become considerably more faithful to the
knowledge source both for datasets and domains seen during BeInfo-tuning, as
well as on unseen domains, when applied in a zero-shot manner. In addition, we
show that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo
demonstrate strong performance on data from real `production' conversations and
outperform GPT4 when tuned on a limited amount of such realistic in-domain
dialogues.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09802" title="Abstract">arXiv:2311.09802</a> [<a href="/pdf/2311.09802" title="Download PDF">pdf</a>, <a href="/format/2311.09802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Leyang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Though prompting LLMs with various reasoning structures produces reasoning
proofs along with answers, these proofs are not ensured to be causal and
reliable due to the inherent defects of LLMs. Tracking such deficiencies, we
present a neuro-symbolic integration method, in which a neural LLM is used to
represent the knowledge of the problem while an LLM-free symbolic solver is
adopted to do deliberative reasoning using the knowledge. Specifically, our
customized meta-interpreters allow the production of reasoning proofs and
support flexible search strategies. These reasoning proofs are ensured to be
causal and reliable because of the deterministic executing nature of the
symbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT
baseline by nearly double in accuracy and more than triple in proof similarity.
On GSM8K, our method also shows accuracy improvements and nearly doubled proof
similarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09803" title="Abstract">arXiv:2311.09803</a> [<a href="/pdf/2311.09803" title="Download PDF">pdf</a>, <a href="/format/2311.09803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning effects in variable autonomy human-robot systems: how much  training is enough?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiou%2C+M">Manolis Chiou</a>, 
<a href="/search/cs?searchtype=author&query=Talha%2C+M">Mohammed Talha</a>, 
<a href="/search/cs?searchtype=author&query=Stolkin%2C+R">Rustam Stolkin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is a preprint of the paper published on the IEEE International Conference on Systems, Man and Cybernetics (SMC) 2019
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2019 IEEE International Conference on Systems, Man and Cybernetics
  (SMC),pp. 720-727
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This paper investigates learning effects and human operator training
practices in variable autonomy robotic systems. These factors are known to
affect performance of a human-robot system and are frequently overlooked. We
present the results from an experiment inspired by a search and rescue scenario
in which operators remotely controlled a mobile robot with either
Human-Initiative (HI) or Mixed-Initiative (MI) control. Evidence suggests
learning in terms of primary navigation task and secondary (distractor) task
performance. Further evidence is provided that MI and HI performance in a pure
navigation task is equal. Lastly, guidelines are proposed for experimental
design and operator training practices.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09805" title="Abstract">arXiv:2311.09805</a> [<a href="/pdf/2311.09805" title="Download PDF">pdf</a>, <a href="/format/2311.09805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in  Understanding Long Documents with Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yitao Long</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongjun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+L">Linyong Nan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lyuhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kamoi%2C+R">Ryo Kamoi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent LLMs have demonstrated remarkable performance in solving exam-like
math word problems. However, the degree to which these numerical reasoning
skills are effective in real-world scenarios, particularly in expert domains,
is still largely unexplored. This paper introduces DocMath-Eval, a
comprehensive benchmark specifically designed to evaluate the numerical
reasoning and problem-solving capabilities of LLMs in the context of
understanding and analyzing financial documents containing both text and
tables. We evaluate a wide spectrum of 19 LLMs, including those specialized in
coding and finance. We also incorporate different prompting strategies (i.e.,
Chain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the
capabilities and limitations of existing LLMs in DocMath-Eval. We found that,
although the current best-performing system (i.e., GPT-4), can perform well on
simple problems such as calculating the rate of increase in a financial metric
within a short document context, it significantly lags behind human experts in
more complex problems grounded in longer contexts. We believe DocMath-Eval can
be used as a valuable benchmark to evaluate LLMs' capabilities to solve
challenging numerical reasoning problems in expert domains. We will release the
benchmark and code at https://github.com/yale-nlp/DocMath-Eval.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09806" title="Abstract">arXiv:2311.09806</a> [<a href="/pdf/2311.09806" title="Download PDF">pdf</a>, <a href="/format/2311.09806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction  on Mobile Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jingnan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yichao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bowen Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+J">Jiangjing Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="http://g-1nonly.github.io/EvaSurf-Website/">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Reconstructing real-world 3D objects has numerous applications in computer
vision, such as virtual reality, video games, and animations. Ideally, 3D
reconstruction methods should generate high-fidelity results with 3D
consistency in real-time. Traditional methods match pixels between images using
photo-consistency constraints or learned features, while differentiable
rendering methods like Neural Radiance Fields (NeRF) use surface-based
representations or differentiable volume rendering to generate high-fidelity
scenes. However, these methods require excessive runtime for rendering, making
them impractical for daily applications. To address these challenges, we
present $\textbf{EvaSurf}$, an $\textbf{E}$fficient
$\textbf{V}$iew-$\textbf{A}$ware Implicit Textured $\textbf{Surf}$ace
Reconstruction method on Mobile Devices. In our method, we first employ an
efficient surface-based model with a multi-view supervision module to ensure
accurate mesh creation. To enable high-fidelity rendering, we learn an implicit
texture embedded with a set of Gaussian lobes to capture view-dependent
information. Furthermore, With the explicit geometry and the implicit texture,
we can employ a lightweight neural shader to reduce the expense of computation
and further support real-time rendering on common mobile devices. Extensive
experiments demonstrate that our method can reconstruct high-quality appearance
and accurate mesh on both synthetic and real-world datasets. Moreover, our
method can be trained in just 1-2 hours using a single GPU and run on mobile
devices at over 40FPS (Frames Per Second), with a final package required for
rendering taking up only 40-50 MB.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09807" title="Abstract">arXiv:2311.09807</a> [<a href="/pdf/2311.09807" title="Download PDF">pdf</a>, <a href="/format/2311.09807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Curious Decline of Linguistic Diversity: Training Language Models on  Synthetic Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yanzhu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+G">Guokan Shang</a>, 
<a href="/search/cs?searchtype=author&query=Vazirgiannis%2C+M">Michalis Vazirgiannis</a>, 
<a href="/search/cs?searchtype=author&query=Clavel%2C+C">Chlo&#xe9; Clavel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study investigates the consequences of training large language models
(LLMs) on synthetic data generated by their predecessors, an increasingly
prevalent practice aimed at addressing the limited supply of human-generated
training data. Diverging from the usual emphasis on performance metrics, we
focus on the impact of this training methodology on linguistic diversity,
especially when conducted recursively over time. To assess this, we developed a
set of novel metrics targeting lexical, syntactic, and semantic diversity,
applying them in recursive fine-tuning experiments across various natural
language generation tasks. Our findings reveal a marked decrease in the
diversity of the models' outputs through successive iterations. This trend
underscores the potential risks of training LLMs on predecessor-generated text,
particularly concerning the preservation of linguistic richness. Our study
highlights the need for careful consideration of the long-term effects of such
training approaches on the linguistic capabilities of LLMs.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09808" title="Abstract">arXiv:2311.09808</a> [<a href="/pdf/2311.09808" title="Download PDF">pdf</a>, <a href="/format/2311.09808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PixT3: Pixel-based Table To Text generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alonso%2C+I">I&#xf1;igo Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Agirre%2C+E">Eneko Agirre</a>, 
<a href="/search/cs?searchtype=author&query=Lapata%2C+M">Mirella Lapata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Table-to-Text has been traditionally approached as a linear language to text
problem. However, visually represented tables are rich in visual information
and serve as a concise, effective form of representing data and its
relationships. When using text-based approaches, after the linearization
process, this information is either lost or represented in a space inefficient
manner. This inefficiency has remained a constant challenge for text-based
approaches making them struggle with large tables. In this paper, we
demonstrate that image representation of tables are more space-efficient than
the typical textual linearizations, and multi-modal approaches are competitive
in Table-to-Text tasks. We present PixT3, a multimodal table-to-text model that
outperforms the state-of-the-art (SotA) in the ToTTo benchmark in a pure
Table-to-Text setting while remaining competitive in controlled Table-to-Text
scenarios. It also generalizes better in unseen datasets, outperforming ToTTo
SotA in all generation settings. Additionally, we introduce a new intermediate
training curriculum to reinforce table structural awareness, leading to
improved generation and overall faithfulness of the models.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09809" title="Abstract">arXiv:2311.09809</a> [<a href="/pdf/2311.09809" title="Download PDF">pdf</a>, <a href="/format/2311.09809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Differentiable Logics for Learning Systems: A Research Preview
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Flinkow%2C+T">Thomas Flinkow</a> (Maynooth University), 
<a href="/search/cs?searchtype=author&query=Pearlmutter%2C+B+A">Barak A. Pearlmutter</a> (Maynooth University), 
<a href="/search/cs?searchtype=author&query=Monahan%2C+R">Rosemary Monahan</a> (Maynooth University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 17-29
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Extensive research on formal verification of machine learning (ML) systems
indicates that learning from data alone often fails to capture underlying
background knowledge. A variety of verifiers have been developed to ensure that
a machine-learnt model satisfies correctness and safety properties, however,
these verifiers typically assume a trained network with fixed weights.
ML-enabled autonomous systems are required to not only detect incorrect
predictions, but should also possess the ability to self-correct, continuously
improving and adapting. A promising approach for creating ML models that
inherently satisfy constraints is to encode background knowledge as logical
constraints that guide the learning process via so-called differentiable
logics. In this research preview, we compare and evaluate various logics from
the literature in weakly-supervised contexts, presenting our findings and
highlighting open problems for future work. Our experimental results are
broadly consistent with results reported previously in literature; however,
learning with differentiable logics introduces a new hyperparameter that is
difficult to tune and has significant influence on the effectiveness of the
logics.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09810" title="Abstract">arXiv:2311.09810</a> [<a href="/pdf/2311.09810" title="Download PDF">pdf</a>, <a href="/format/2311.09810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Formal Fault Injection for Safety Assessment of Automated  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farooqui%2C+A">Ashfaq Farooqui</a> (Dependable Transport Systems, RISE Research Institutes of Sweden, Bor&#xe5;s, Sweden), 
<a href="/search/cs?searchtype=author&query=Sangchoolie%2C+B">Behrooz Sangchoolie</a> (Dependable Transport Systems, RISE Research Institutes of Sweden, Bor&#xe5;s, Sweden)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 153-161
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Reasoning about safety, security, and other dependability attributes of
autonomous systems is a challenge that needs to be addressed before the
adoption of such systems in day-to-day life. Formal methods is a class of
methods that mathematically reason about a system's behavior. Thus, a
correctness proof is sufficient to conclude the system's dependability.
However, these methods are usually applied to abstract models of the system,
which might not fully represent the actual system. Fault injection, on the
other hand, is a testing method to evaluate the dependability of systems.
However, the amount of testing required to evaluate the system is rather large
and often a problem. This vision paper introduces formal fault injection, a
fusion of these two techniques throughout the development lifecycle to enhance
the dependability of autonomous systems. We advocate for a more cohesive
approach by identifying five areas of mutual support between formal methods and
fault injection. By forging stronger ties between the two fields, we pave the
way for developing safe and dependable autonomous systems. This paper delves
into the integration's potential and outlines future research avenues,
addressing open challenges along the way.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09811" title="Abstract">arXiv:2311.09811</a> [<a href="/pdf/2311.09811" title="Download PDF">pdf</a>, <a href="/ps/2311.09811" title="Download PostScript">ps</a>, <a href="/format/2311.09811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Runtime Verification of Learning Properties for Reinforcement Learning  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mannucci%2C+T">Tommaso Mannucci</a> (TNO -- Netherlands Organisation for Applied Scientific Research), 
<a href="/search/cs?searchtype=author&query=de+Oliveira+Filho%2C+J">Julio de Oliveira Filho</a> (TNO -- Netherlands Organisation for Applied Scientific Research)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings FMAS 2023, <a href="/abs/2311.08987">arXiv:2311.08987</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 395, 2023, pp. 205-219
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reinforcement learning (RL) algorithms interact with their environment in a
trial-and-error fashion. Such interactions can be expensive, inefficient, and
timely when learning on a physical system rather than in a simulation. This
work develops new runtime verification techniques to predict when the learning
phase has not met or will not meet qualitative and timely expectations. This
paper presents three verification properties concerning the quality and
timeliness of learning in RL algorithms. With each property, we propose design
steps for monitoring and assessing the properties during the system's
operation.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09812" title="Abstract">arXiv:2311.09812</a> [<a href="/pdf/2311.09812" title="Download PDF">pdf</a>, <a href="/format/2311.09812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Propaganda Span Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasanain%2C+M">Maram Hasanain</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Fatema Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+F">Firoj Alam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> propaganda, span detection, disinformation, misinformation, fake news, LLMs, GPT-4
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The use of propagandistic techniques in online communication has increased in
recent years, aiming to manipulate online audiences. Efforts to automatically
detect and debunk such content have been made, addressing various modeling
scenarios. These include determining whether the content (text, image, or
multimodal) (i) is propagandistic, (ii) employs one or more techniques, and
(iii) includes techniques with identifiable spans. Significant research efforts
have been devoted to the first two scenarios compared to the latter. Therefore,
in this study, we focus on the task of detecting propagandistic textual spans.
We investigate whether large language models such as GPT-4 can be utilized to
perform the task of an annotator. For the experiments, we used an in-house
developed dataset consisting of annotations from multiple annotators. Our
results suggest that providing more information to the model as prompts
improves the annotation agreement and performance compared to human
annotations. We plan to make the annotated labels from multiple annotators,
including GPT-4, available for the community.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09814" title="Abstract">arXiv:2311.09814</a> [<a href="/pdf/2311.09814" title="Download PDF">pdf</a>, <a href="/ps/2311.09814" title="Download PostScript">ps</a>, <a href="/format/2311.09814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stacked Intelligent Metasurface-Aided MIMO Transceiver Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jiancheng An</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+D+W+K">Derrick Wing Kwan Ng</a>, 
<a href="/search/cs?searchtype=author&query=Di+Renzo%2C+M">Marco Di Renzo</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">M&#xe9;rouane Debbah</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Next-generation wireless networks are expected to utilize the limited radio
frequency (RF) resources more efficiently with the aid of intelligent
transceivers. To this end, we propose a promising transceiver architecture
relying on stacked intelligent metasurfaces (SIM). An SIM is constructed by
stacking an array of programmable metasurface layers, where each layer consists
of a massive number of low-cost passive meta-atoms that individually manipulate
the electromagnetic (EM) waves. By appropriately configuring the passive
meta-atoms, an SIM is capable of accomplishing advanced computation and signal
processing tasks, such as multiple-input multiple-output (MIMO)
precoding/combining, multi-user interference mitigation, and radar sensing, as
the EM wave propagates through the multiple layers of the metasurface, which
effectively reduces both the RF-related energy consumption and processing
delay. Inspired by this, we provide an overview of the SIM-aided MIMO
transceiver design, which encompasses its hardware architecture and its
potential benefits over state-of-the-art solutions. Furthermore, we discuss
promising application scenarios and identify the open research challenges
associated with the design of advanced SIM architectures for next-generation
wireless networks. Finally, numerical results are provided for quantifying the
benefits of wave-based signal processing in wireless systems.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09815" title="Abstract">arXiv:2311.09815</a> [<a href="/pdf/2311.09815" title="Download PDF">pdf</a>, <a href="/format/2311.09815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Indoor Localization for Smart Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=%C3%81lvarez-Merino%2C+C+S">Carlos S. &#xc1;lvarez-Merino</a>, 
<a href="/search/eess?searchtype=author&query=Khatib%2C+E+J">Emil J. Khatib</a>, 
<a href="/search/eess?searchtype=author&query=Mu%C3%B1oz%2C+A+T">Antonio Tarrias Mu&#xf1;oz</a>, 
<a href="/search/eess?searchtype=author&query=Barco%2C+R">Raquel Barco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This comprehensive study delves into the realm of indoor positioning
technologies within the domain of Smart Education (SE). Focusing on typical
techniques and technologies in educational settings, the research emphasizes
the importance and potential services of localization in SE. Moreover, this
work explores the feasibility and limitations of these technologies, providing
a detailed account of their role in educational settings. The paper also
contains in an innovative Proof of Concept (PoC), demonstrating an automatic
attendance control (AAC) system that integrates 5G and WiFi technologies. This
PoC effectively showcases the possibilities and effectiveness of location-based
services in educational surroundings even with a limited budget, setting the
stage for optimizing teaching time, enhancing the quality of education.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09816" title="Abstract">arXiv:2311.09816</a> [<a href="/pdf/2311.09816" title="Download PDF">pdf</a>, <a href="/format/2311.09816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Trade-offs of Watermarking Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ajith%2C+A">Anirudh Ajith</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sameer Singh</a>, 
<a href="/search/cs?searchtype=author&query=Pruthi%2C+D">Danish Pruthi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Amidst growing concerns of large language models (LLMs) being misused for
generating misinformation or completing homework assignments, watermarking has
emerged as an effective solution for distinguishing human-written and
LLM-generated text. A prominent watermarking strategy is to embed a signal into
generated text by upsampling a (pseudorandomly-chosen) subset of tokens at
every generation step. Although this signal is imperceptible to a human reader,
it is detectable through statistical testing. However, implanting such signals
alters the model's output distribution and can have unintended effects when
watermarked LLMs are used for downstream applications. In this work, we
evaluate the performance of watermarked LLMs on a diverse suite of tasks,
including text classification, textual entailment, reasoning, question
answering, translation, summarization, and language modeling. We find that
watermarking has negligible impact on the performance of tasks posed as k-class
classification problems in the average case. However, the accuracy can plummet
to that of a random classifier for some scenarios (that occur with
non-negligible probability). Tasks that are cast as multiple-choice questions
and short-form generation are surprisingly unaffected by watermarking. For
long-form generation tasks, including summarization and translation, we see a
drop of 15-20% in the performance due to watermarking. Our findings highlight
the trade-offs that users should be cognizant of when using watermarked models,
and point to cases where future research could improve existing trade-offs.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09817" title="Abstract">arXiv:2311.09817</a> [<a href="/pdf/2311.09817" title="Download PDF">pdf</a>, <a href="/format/2311.09817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural-Logic Human-Object Interaction Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liulei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jianan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenguan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023; Code: <a href="https://github.com/weijianan1/LogicHOI">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The interaction decoder utilized in prevalent Transformer-based HOI detectors
typically accepts pre-composed human-object pairs as inputs. Though achieving
remarkable performance, such paradigm lacks feasibility and cannot explore
novel combinations over entities during decoding. We present L OGIC HOI, a new
HOI detector that leverages neural-logic reasoning and Transformer to infer
feasible interactions between entities. Specifically, we modify the
self-attention mechanism in vanilla Transformer, enabling it to reason over the
&lt;human, action, object&gt; triplet and constitute novel interactions. Meanwhile,
such reasoning process is guided by two crucial properties for understanding
HOI: affordances (the potential actions an object can facilitate) and proxemics
(the spatial relations between humans and objects). We formulate these two
properties in first-order logic and ground them into continuous space to
constrain the learning process of our approach, leading to improved performance
and zero-shot generalization capabilities. We evaluate L OGIC HOI on V-COCO and
HICO-DET under both normal and zero-shot setups, achieving significant
improvements over existing methods.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09818" title="Abstract">arXiv:2311.09818</a> [<a href="/pdf/2311.09818" title="Download PDF">pdf</a>, <a href="/format/2311.09818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SUQL: Conversational Search over Structured and Unstructured Data with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jialiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tjangnaka%2C+W">Wesley Tjangnaka</a>, 
<a href="/search/cs?searchtype=author&query=Semnani%2C+S+J">Sina J. Semnani</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C+J">Chen Jie Yu</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%A1vid%2C+G">Gui D&#xe1;vid</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+M+S">Monica S. Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Many knowledge sources consist of both structured information such as
relational databases as well as unstructured free text. Building a
conversational interface to such data sources is challenging.
<br />This paper introduces SUQL, Structured and Unstructured Query Language, the
first formal executable representation that naturally covers compositions of
structured and unstructured data queries. Specifically, it augments SQL with
several free-text primitives to form a precise, succinct, and expressive
representation. This paper also presents a conversational search agent based on
large language models, including a few-shot contextual semantic parser for
SUQL.
<br />To validate our approach, we introduce a dataset consisting of crowdsourced
questions and conversations about real restaurants. Over 51% of the questions
in the dataset require both structured and unstructured data, suggesting that
it is a common phenomenon. We show that our few-shot conversational agent based
on SUQL finds an entity satisfying all user requirements 89.3% of the time,
compared to just 65.0% for a strong and commonly used baseline.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09819" title="Abstract">arXiv:2311.09819</a> [<a href="/pdf/2311.09819" title="Download PDF">pdf</a>, <a href="/format/2311.09819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical  Instruments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinlin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Z">Zhen Lei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to IEEE International Symposium on Biomedical Imaging (ISBI) 2024 for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In surgical procedures, correct instrument counting is essential. Instance
segmentation is a location method that locates not only an object's bounding
box but also each pixel's specific details. However, obtaining mask-level
annotations is labor-intensive in instance segmentation. To address this issue,
we propose a novel yet effective weakly-supervised surgical instrument instance
segmentation approach, named Point-based Weakly-supervised Instance
Segmentation (PWISeg). PWISeg adopts an FCN-based architecture with
point-to-box and point-to-mask branches to model the relationships between
feature points and bounding boxes, as well as feature points and segmentation
masks on FPN, accomplishing instrument detection and segmentation jointly in a
single model. Since mask level annotations are hard to available in the real
world, for point-to-mask training, we introduce an unsupervised projection
loss, utilizing the projected relation between predicted masks and bboxes as
supervision signal. On the other hand, we annotate a few pixels as the key
pixel for each instrument. Based on this, we further propose a key pixel
association loss and a key pixel distribution loss, driving the point-to-mask
branch to generate more accurate segmentation predictions. To comprehensively
evaluate this task, we unveil a novel surgical instrument dataset with manual
annotations, setting up a benchmark for further research. Our comprehensive
research trial validated the superior performance of our PWISeg. The results
show that the accuracy of surgical instrument segmentation is improved,
surpassing most methods of instance segmentation via weakly supervised bounding
boxes. This improvement is consistently observed in our proposed dataset and
when applied to the public HOSPI-Tools dataset.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09820" title="Abstract">arXiv:2311.09820</a> [<a href="/pdf/2311.09820" title="Download PDF">pdf</a>, <a href="/format/2311.09820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IterCQR: Iterative Conversational Query Reformulation without Human  Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+Y">Yunah Jang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kang-il Lee</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+H">Hyunkyung Bae</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+S">Seungpil Won</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwanhee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+K">Kyomin Jung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">In conversational search, which aims to retrieve passages containing
essential information, queries suffer from high dependency on the preceding
dialogue context. Therefore, reformulating conversational queries into
standalone forms is essential for the effective utilization of off-the-shelf
retrievers. Previous methodologies for conversational query search frequently
depend on human-annotated gold labels. However, these manually crafted queries
often result in sub-optimal retrieval performance and require high collection
costs. In response to these challenges, we propose Iterative Conversational
Query Reformulation (IterCQR), a methodology that conducts query reformulation
without relying on human oracles. IterCQR iteratively trains the QR model by
directly leveraging signal from information retrieval (IR) as a reward. Our
proposed IterCQR method shows state-of-the-art performance on two datasets,
demonstrating its effectiveness on both sparse and dense retrievers. Notably,
IterCQR exhibits robustness in domain-shift, low-resource, and topic-shift
scenarios.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09821" title="Abstract">arXiv:2311.09821</a> [<a href="/pdf/2311.09821" title="Download PDF">pdf</a>, <a href="/format/2311.09821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Temporal Reasoning of Large Language Models via a  Multi-Hop QA Dataset and Pseudo-Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Q">Qingyu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+H+T">Hwee Tou Ng</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Knowledge in the real world is being updated constantly. However, it is
costly to frequently update large language models (LLMs). Therefore, it is
crucial for LLMs to understand the concept of temporal knowledge. However,
prior works on temporal question answering did not emphasize multi-answer and
multi-hop types of temporal reasoning. In this paper, we propose a complex
temporal question-answering (QA) dataset Complex-TR that focuses on
multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel
data augmentation strategy to improve the complex temporal reasoning capability
and robustness of LLMs. We conducted experiments on multiple temporal QA
datasets. Experimental results show that our method is able to improve LLMs'
performance on temporal QA benchmarks by significant margins.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09825" title="Abstract">arXiv:2311.09825</a> [<a href="/pdf/2311.09825" title="Download PDF">pdf</a>, <a href="/format/2311.09825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Still Wins over LLM: An Empirical Study of Active Learning on  Domain-Specific Annotation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuxuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tun Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+J">Toby Jia-Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated considerable advances, and
several claims have been made about their exceeding human performance. However,
in real-world tasks, domain knowledge is often required. Low-resource learning
methods like Active Learning (AL) have been proposed to tackle the cost of
domain expert annotation, raising this question: Can LLMs surpass compact
models trained with expert annotations in domain-specific tasks? In this work,
we conduct an empirical experiment on four datasets from three different
domains comparing SOTA LLMs with small models trained on expert annotations
with AL. We found that small models can outperform GPT-3.5 with a few hundreds
of labeled data, and they achieve higher or similar performance with GPT-4
despite that they are hundreds time smaller. Based on these findings, we posit
that LLM predictions can be used as a warmup method in real-world applications
and human experts remain indispensable in tasks involving data annotation
driven by domain-specific knowledge.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09827" title="Abstract">arXiv:2311.09827</a> [<a href="/pdf/2311.09827" title="Download PDF">pdf</a>, <a href="/format/2311.09827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cognitive Overload: Jailbreaking Large Language Models with Overloaded  Logical Thinking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Ben Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B+Z">Bang Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large language models (LLMs) have demonstrated increasing power, they
have also given rise to a wide range of harmful behaviors. As representatives,
jailbreak attacks can provoke harmful or unethical responses from LLMs, even
after safety alignment. In this paper, we investigate a novel category of
jailbreak attacks specifically designed to target the cognitive structure and
processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in
the face of (1) multilingual cognitive overload, (2) veiled expression, and (3)
effect-to-cause reasoning. Different from previous jailbreak attacks, our
proposed cognitive overload is a black-box attack with no need for knowledge of
model architecture or access to model weights. Experiments conducted on
AdvBench and MasterKey reveal that various LLMs, including both popular
open-source model Llama 2 and the proprietary model ChatGPT, can be compromised
through cognitive overload. Motivated by cognitive psychology work on managing
cognitive load, we further investigate defending cognitive overload attack from
two perspectives. Empirical studies show that our cognitive overload from three
perspectives can jailbreak all studied LLMs successfully, while existing
defense strategies can hardly mitigate the caused malicious uses effectively.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09828" title="Abstract">arXiv:2311.09828</a> [<a href="/pdf/2311.09828" title="Download PDF">pdf</a>, <a href="/format/2311.09828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced  African Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiayi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Adelani%2C+D+I">David Ifeoluwa Adelani</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Sweta Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Rei%2C+R">Ricardo Rei</a>, 
<a href="/search/cs?searchtype=author&query=Briakou%2C+E">Eleftheria Briakou</a>, 
<a href="/search/cs?searchtype=author&query=Carpuat%2C+M">Marine Carpuat</a>, 
<a href="/search/cs?searchtype=author&query=Masiak%2C+M">Marek Masiak</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuanli He</a>, 
<a href="/search/cs?searchtype=author&query=Bourhim%2C+S">Sofia Bourhim</a>, 
<a href="/search/cs?searchtype=author&query=Bukula%2C+A">Andiswa Bukula</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+M">Muhidin Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Olatoye%2C+T">Temitayo Olatoye</a>, 
<a href="/search/cs?searchtype=author&query=Mokayede%2C+H">Hamam Mokayede</a>, 
<a href="/search/cs?searchtype=author&query=Mwase%2C+C">Christine Mwase</a>, 
<a href="/search/cs?searchtype=author&query=Kimotho%2C+W">Wangui Kimotho</a>, 
<a href="/search/cs?searchtype=author&query=Yuehgoh%2C+F">Foutse Yuehgoh</a>, 
<a href="/search/cs?searchtype=author&query=Aremu%2C+A">Anuoluwapo Aremu</a>, 
<a href="/search/cs?searchtype=author&query=Ojo%2C+J">Jessica Ojo</a>, 
<a href="/search/cs?searchtype=author&query=Muhammad%2C+S+H">Shamsuddeen Hassan Muhammad</a>, 
<a href="/search/cs?searchtype=author&query=Osei%2C+S">Salomey Osei</a>, 
<a href="/search/cs?searchtype=author&query=Omotayo%2C+A">Abdul-Hakeem Omotayo</a>, 
<a href="/search/cs?searchtype=author&query=Chukwuneke%2C+C">Chiamaka Chukwuneke</a>, 
<a href="/search/cs?searchtype=author&query=Ogayo%2C+P">Perez Ogayo</a>, 
<a href="/search/cs?searchtype=author&query=Hourrane%2C+O">Oumaima Hourrane</a>, 
<a href="/search/cs?searchtype=author&query=Anigri%2C+S+E">Salma El Anigri</a>, 
<a href="/search/cs?searchtype=author&query=Ndolela%2C+L">Lolwethu Ndolela</a>, 
<a href="/search/cs?searchtype=author&query=Mangwana%2C+T">Thabiso Mangwana</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+S+A">Shafie Abdi Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+A">Ayinde Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Awoyomi%2C+O+O">Oluwabusayo Olufunke Awoyomi</a>, 
<a href="/search/cs?searchtype=author&query=Alkhaled%2C+L">Lama Alkhaled</a>, 
<a href="/search/cs?searchtype=author&query=Al-Azzawi%2C+S">Sana Al-Azzawi</a>, 
<a href="/search/cs?searchtype=author&query=Etori%2C+N+A">Naome A. Etori</a>, 
<a href="/search/cs?searchtype=author&query=Ochieng%2C+M">Millicent Ochieng</a>, 
<a href="/search/cs?searchtype=author&query=Siro%2C+C">Clemencia Siro</a>, 
<a href="/search/cs?searchtype=author&query=Njoroge%2C+S">Samuel Njoroge</a>, 
<a href="/search/cs?searchtype=author&query=Muchiri%2C+E">Eric Muchiri</a>, 
<a href="/search/cs?searchtype=author&query=Kimotho%2C+W">Wangari Kimotho</a>, 
<a href="/search/cs?searchtype=author&query=Momo%2C+L+N+W">Lyse Naomi Wamba Momo</a>, 
<a href="/search/cs?searchtype=author&query=Abolade%2C+D">Daud Abolade</a>, 
<a href="/search/cs?searchtype=author&query=Ajao%2C+S">Simbiat Ajao</a>, 
<a href="/search/cs?searchtype=author&query=Adewumi%2C+T">Tosin Adewumi</a>, 
<a href="/search/cs?searchtype=author&query=Shode%2C+I">Iyanuoluwa Shode</a>, 
<a href="/search/cs?searchtype=author&query=Macharm%2C+R">Ricky Macharm</a>, 
<a href="/search/cs?searchtype=author&query=Iro%2C+R+N">Ruqayya Nasir Iro</a>, 
<a href="/search/cs?searchtype=author&query=Abdullahi%2C+S+S">Saheed S. Abdullahi</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+S+E">Stephen E. Moore</a>,  et al. (10 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the progress we have recorded in scaling multilingual machine
translation (MT) models and evaluation data to several under-resourced African
languages, it is difficult to measure accurately the progress we have made on
these languages because evaluation is often performed on n-gram matching
metrics like BLEU that often have worse correlation with human judgments.
Embedding-based metrics such as COMET correlate better; however, lack of
evaluation data with human ratings for under-resourced languages, complexity of
annotation guidelines like Multidimensional Quality Metrics (MQM), and limited
language coverage of multilingual encoders have hampered their applicability to
African languages. In this paper, we address these challenges by creating
high-quality human evaluation data with a simplified MQM guideline for
error-span annotation and direct assessment (DA) scoring for 13 typologically
diverse African languages. Furthermore, we develop AfriCOMET, a COMET
evaluation metric for African languages by leveraging DA training data from
high-resource languages and African-centric multilingual encoder
(AfroXLM-Roberta) to create the state-of-the-art evaluation metric for African
languages MT with respect to Spearman-rank correlation with human judgments
(+0.406).
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09829" title="Abstract">arXiv:2311.09829</a> [<a href="/pdf/2311.09829" title="Download PDF">pdf</a>, <a href="/format/2311.09829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FollowEval: A Multi-Dimensional Benchmark for Assessing the  Instruction-Following Capability of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jing%2C+Y">Yimin Jing</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Renren Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiahao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Huishi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaohua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Deyi Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The effective assessment of the instruction-following ability of large
language models (LLMs) is of paramount importance. A model that cannot adhere
to human instructions might be not able to provide reliable and helpful
responses. In pursuit of this goal, various benchmarks have been constructed to
evaluate the instruction-following capacity of these models. However, these
benchmarks are limited to a single language and are constructed using automated
approaches, which restricts their applicability and the quality of the test
examples they contain. To bridge this gap, we introduce the FollowEval
benchmark in this paper. This benchmark is composed of instances in both
English and Chinese, and all test examples are crafted by human experts.
Furthermore, the FollowEval benchmark is designed to assess LLMs across five
critical dimensions of instruction following: string manipulation, commonsense
reasoning, logical reasoning, spatial reasoning, and response constraints. To
enhance the complexity and present a sufficient challenge, each test example is
designed to evaluate more than one dimension. We have evaluated various LLMs
using the FollowEval benchmark and found that their performance significantly
lags behind that of humans. This highlights the considerable room for
improvement in the instruction-following ability of these models.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09830" title="Abstract">arXiv:2311.09830</a> [<a href="/pdf/2311.09830" title="Download PDF">pdf</a>, <a href="/format/2311.09830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoPlanBench: : Automatically generating benchmarks for LLM planners  from PDDL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stein%2C+K">Katharina Stein</a>, 
<a href="/search/cs?searchtype=author&query=Koller%2C+A">Alexander Koller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">LLMs are being increasingly used for planning-style tasks, but their
capabilities for planning and reasoning are poorly understood. We present a
novel method for automatically converting planning benchmarks written in PDDL
into textual descriptions and offer a benchmark dataset created with our
method. We show that while the best LLM planners do well on many planning
tasks, others remain out of reach of current methods.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09832" title="Abstract">arXiv:2311.09832</a> [<a href="/pdf/2311.09832" title="Download PDF">pdf</a>, <a href="/format/2311.09832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-Mark: Towards Lossless Watermarking Through Lexical Redundancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuaiyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bingzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-fai Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text watermarking has emerged as an important technique for detecting
machine-generated text. However, existing methods can severely degrade text
quality due to arbitrary vocabulary partitioning, which disrupts the language
model's expressiveness and impedes textual coherence. To mitigate this, we
introduce XMark, a novel approach that capitalizes on text redundancy within
the lexical space. Specifically, XMark incorporates a mutually exclusive rule
for synonyms during the language model decoding process, thereby integrating
prior knowledge into vocabulary partitioning and preserving the capabilities of
language generation. We present theoretical analyses and empirical evidence
demonstrating that XMark substantially enhances text generation fluency while
maintaining watermark detectability. Furthermore, we investigate watermarking's
impact on the emergent abilities of large language models, including zero-shot
and few-shot knowledge recall, logical reasoning, and instruction following.
Our comprehensive experiments confirm that XMark consistently outperforms
existing methods in retaining these crucial capabilities of LLMs.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09834" title="Abstract">arXiv:2311.09834</a> [<a href="/pdf/2311.09834" title="Download PDF">pdf</a>, <a href="/format/2311.09834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of the HASOC Subtrack at FIRE 2023: Identification of Tokens  Contributing to Explicit Hate in English by Span Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masud%2C+S">Sarah Masud</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+A">Mohammad Aflah Khan</a>, 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M+S">Md. Shad Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure, 4 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As hate speech continues to proliferate on the web, it is becoming
increasingly important to develop computational methods to mitigate it.
Reactively, using black-box models to identify hateful content can perplex
users as to why their posts were automatically flagged as hateful. On the other
hand, proactive mitigation can be achieved by suggesting rephrasing before a
post is made public. However, both mitigation techniques require information
about which part of a post contains the hateful aspect, i.e., what spans within
a text are responsible for conveying hate. Better detection of such spans can
significantly reduce explicitly hateful content on the web. To further
contribute to this research area, we organized HateNorm at HASOC-FIRE 2023,
focusing on explicit span detection in English Tweets. A total of 12 teams
participated in the competition, with the highest macro-F1 observed at 0.58.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09835" title="Abstract">arXiv:2311.09835</a> [<a href="/pdf/2311.09835" title="Download PDF">pdf</a>, <a href="/format/2311.09835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ML-Bench: Large Language Models Leverage Open-source Libraries for  Machine Learning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zefan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Junjie Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yanjun Shao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zexuan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Helan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zengxian Yang</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+K">Kaikai An</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruijun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Shuzheng Si</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haozhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhengliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+Y">Yiming Zong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhiwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+B">Baobao Chang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models have shown promising performance in code generation
benchmarks. However, a considerable divide exists between these benchmark
achievements and their practical applicability, primarily attributed to
real-world programming's reliance on pre-existing libraries. Instead of
evaluating LLMs to code from scratch, this work aims to propose a new
evaluation setup where LLMs use open-source libraries to finish machine
learning tasks. Therefore, we propose ML-Bench, an expansive benchmark
developed to assess the effectiveness of LLMs in leveraging existing functions
in open-source libraries. Consisting of 10044 samples spanning 130 tasks over
14 notable machine learning GitHub repositories. In this setting, given a
specific machine learning task instruction and the accompanying README in a
codebase, an LLM is tasked to generate code to accomplish the task. This
necessitates the comprehension of long and language-code interleaved documents,
as well as the understanding of complex cross-file code structures, introducing
new challenges. Notably, while GPT-4 exhibits remarkable improvement over other
LLMs, it manages to accomplish only 39.73\% of the tasks, leaving a huge space
for improvement. We address these challenges by proposing ML-Agent, designed to
effectively navigate the codebase, locate documentation, retrieve code, and
generate executable code. Empirical results demonstrate that ML-Agent, built
upon GPT-4, results in further improvements. Code, data, and models are
available at \url{https://ml-bench.github.io/}.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09836" title="Abstract">arXiv:2311.09836</a> [<a href="/pdf/2311.09836" title="Download PDF">pdf</a>, <a href="/format/2311.09836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peper%2C+J+J">Joseph J. Peper</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+W">Wenzhao Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We investigate pre-training techniques for abstractive multi-document
summarization (MDS), which is much less studied than summarizing single
documents. Though recent work has demonstrated the effectiveness of
highlighting information salience for pre-training strategy design, it
struggles to generate abstractive and reflective summaries, which are critical
properties for MDS. To this end, we present PELMS, a pre-trained model that
uses objectives based on semantic coherence heuristics and faithfulness
constraints with un-labeled multi-document inputs, to promote the generation of
concise, fluent, and faithful summaries. To support the training of PELMS, we
compile MultiPT, a multi-document pre-training corpus containing over 93
million documents to form more than 3 million unlabeled topic-centric document
clusters, covering diverse genres such as product reviews, news, and general
knowledge. We perform extensive evaluation of PELMS in low-shot settings on a
wide range of MDS datasets. Our approach consistently outperforms competitive
comparisons with respect to overall informativeness, abstractiveness,
coherence, and faithfulness.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09839" title="Abstract">arXiv:2311.09839</a> [<a href="/pdf/2311.09839" title="Download PDF">pdf</a>, <a href="/format/2311.09839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Load Data Valuation in Multi-Energy Systems: An End-to-End Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yangze Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+J">Jie Song</a>, 
<a href="/search/eess?searchtype=author&query=Cui%2C+X">Xueyuan Cui</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Accurate load forecasting serves as the foundation for the flexible operation
of multi-energy systems (MES). Multi-energy loads are tightly coupled and
exhibit significant uncertainties. Many works focus on enhancing forecasting
accuracy by leveraging cross-sector information. However, data owners may not
be motivated to share their data unless it leads to substantial benefits.
Ensuring a reasonable data valuation can encourage them to share their data
willingly. This paper presents an end-to-end framework to quantify multi-energy
load data value by integrating forecasting and decision processes. To address
optimization problems with integer variables, a two-stage end-to-end model
solution is proposed. Moreover, a profit allocation strategy based on
contribution to cost savings is investigated to encourage data sharing in MES.
The experimental results demonstrate a significant decrease in operation costs,
suggesting that the proposed valuation approach more effectively extracts the
inherent data value than traditional methods. According to the proposed
incentive mechanism, all sectors can benefit from data sharing by improving
forecasting accuracy or receiving economic compensation.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09841" title="Abstract">arXiv:2311.09841</a> [<a href="/pdf/2311.09841" title="Download PDF">pdf</a>, <a href="/format/2311.09841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging LLMs in Scholarly Knowledge Graph Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taffa%2C+T+A">Tilahun Abedissa Taffa</a>, 
<a href="/search/cs?searchtype=author&query=Usbeck%2C+R">Ricardo Usbeck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a scholarly Knowledge Graph Question Answering (KGQA)
that answers bibliographic natural language questions by leveraging a large
language model (LLM) in a few-shot manner. The model initially identifies the
top-n similar training questions related to a given test question via a
BERT-based sentence encoder and retrieves their corresponding SPARQL. Using the
top-n similar question-SPARQL pairs as an example and the test question creates
a prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs
the SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and
returns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of
the Scholarly-QALD-23 challenge benchmarks.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09847" title="Abstract">arXiv:2311.09847</a> [<a href="/pdf/2311.09847" title="Download PDF">pdf</a>, <a href="/format/2311.09847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overcoming Data Scarcity in Biomedical Imaging with a Foundational  Multi-Task Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4fer%2C+R">Raphael Sch&#xe4;fer</a>, 
<a href="/search/cs?searchtype=author&query=Nicke%2C+T">Till Nicke</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6fener%2C+H">Henning H&#xf6;fener</a>, 
<a href="/search/cs?searchtype=author&query=Lange%2C+A">Annkristin Lange</a>, 
<a href="/search/cs?searchtype=author&query=Merhof%2C+D">Dorit Merhof</a>, 
<a href="/search/cs?searchtype=author&query=Feuerhake%2C+F">Friedrich Feuerhake</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+V">Volkmar Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Lotz%2C+J">Johannes Lotz</a>, 
<a href="/search/cs?searchtype=author&query=Kiessling%2C+F">Fabian Kiessling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Foundational models, pretrained on a large scale, have demonstrated
substantial success across non-medical domains. However, training these models
typically requires large, comprehensive datasets, which contrasts with the
smaller and more heterogeneous datasets common in biomedical imaging. Here, we
propose a multi-task learning strategy that decouples the number of training
tasks from memory requirements. We trained a Universal bioMedical PreTrained
model (UMedPT) on a multi-task database including tomographic, microscopic, and
X-ray images, with various labelling strategies such as classification,
segmentation, and object detection. The UMedPT foundational model outperformed
ImageNet pretraining and the previous state-of-the-art models. For tasks
related to the pretraining database, it maintained its performance with only 1%
of the original training data and without fine-tuning. For out-of-domain tasks
it required not more than 50% of the original training data. In an external
independent validation imaging features extracted using UMedPT proved to be a
new standard for cross-center transferability.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09848" title="Abstract">arXiv:2311.09848</a> [<a href="/pdf/2311.09848" title="Download PDF">pdf</a>, <a href="/format/2311.09848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion-Augmented Neural Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonito%2C+L">Lorenzo Bonito</a>, 
<a href="/search/cs?searchtype=author&query=Requeima%2C+J">James Requeima</a>, 
<a href="/search/cs?searchtype=author&query=Shysheya%2C+A">Aliaksandra Shysheya</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the NeurIPS 2023 Workshop on Diffusion Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Over the last few years, Neural Processes have become a useful modelling tool
in many application areas, such as healthcare and climate sciences, in which
data are scarce and prediction uncertainty estimates are indispensable.
However, the current state of the art in the field (AR CNPs; Bruinsma et al.,
2023) presents a few issues that prevent its widespread deployment. This work
proposes an alternative, diffusion-based approach to NPs which, through
conditioning on noised datasets, addresses many of these limitations, whilst
also exceeding SOTA performance.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09849" title="Abstract">arXiv:2311.09849</a> [<a href="/pdf/2311.09849" title="Download PDF">pdf</a>, <a href="/format/2311.09849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rusty Detection Using Image Processing For Maintenance Of Stations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tung%2C+D+D">Dao Duy Tung</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+H+X">Ho Xuan Hung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This study addresses the challenge of accurately seg-menting rusted areas on
painted construction surfaces. A method leveraging digital image processing is
explored to calculate the percentage of rust present on painted coatings. The
proposed segmentation approach is based on the HSV color model. To equalize
luminosity and mitigate the influence of illumination, a fundamental model of
single-scale Retinex is applied specifically to the saturation component.
<br />Subsequently, the image undergoes further processing, involv-ing manual color
filtering. This step is crucial for refining the identification of rusted
regions. To enhance precision and filter out noise, the pixel areas selected
through color filtering are subjected to the DBScan algorithm. This multi-step
process aims to achieve a robust segmentation of rusted areas on painted
construction surfaces, providing a valuable contribution to the field of
corrosion detection and analysis.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09850" title="Abstract">arXiv:2311.09850</a> [<a href="/pdf/2311.09850" title="Download PDF">pdf</a>, <a href="/format/2311.09850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-Relay-Aided Text Transmission: Placement Optimization and  Bandwidth Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Changsheng You</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zeyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaibin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, accepted for IEEE Global Communication Conference (GLOBECOM) 2023 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Semantic communication has emerged as a promising technology to break the
Shannon limit by extracting the meaning of source data and sending relevant
semantic information only. However, some mobile devices may have limited
computation and storage resources, which renders it difficult to deploy and
implement the resource-demanding deep learning based semantic encoder/decoder.
To tackle this challenge, we propose in this paper a new semantic relay
(SemRelay), which is equipped with a semantic receiver for assisting text
transmission from a resource-abundant base station (BS) to a
resource-constrained mobile device. Specifically, the SemRelay first decodes
the semantic information sent by the BS (with a semantic transmitter) and then
forwards it to the user by adopting conventional bit transmission, hence
effectively improving the text transmission efficiency. We formulate an
optimization problem to maximize the achievable (effective) bit rate by jointly
designing the SemRelay placement and bandwidth allocation. Although this
problem is non-convex and generally difficult to solve, we propose an efficient
penalty-based algorithm to obtain a high-quality suboptimal solution. Numerical
results show the close-to-optimal performance of the proposed algorithm as well
as significant rate performance gain of the proposed SemRelay over conventional
decode-and-forward relay.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09851" title="Abstract">arXiv:2311.09851</a> [<a href="/pdf/2311.09851" title="Download PDF">pdf</a>, <a href="/format/2311.09851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Urban traffic congestion control: a DeePC change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rimoldi%2C+A">Alessio Rimoldi</a>, 
<a href="/search/eess?searchtype=author&query=Cenedese%2C+C">Carlo Cenedese</a>, 
<a href="/search/eess?searchtype=author&query=Padoan%2C+A">Alberto Padoan</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>, 
<a href="/search/eess?searchtype=author&query=Lygeros%2C+J">John Lygeros</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to IEEE ECC24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Urban traffic congestion remains a pressing challenge in our rapidly
expanding cities, despite the abundance of available data and the efforts of
policymakers. By leveraging behavioral system theory and data-driven control,
this paper exploits the DeePC algorithm in the context of urban traffic control
performed via dynamic traffic lights. To validate our approach, we consider a
high-fidelity case study using the state-of-the-art simulation software package
Simulation of Urban MObility (SUMO). Preliminary results indicate that DeePC
outperforms existing approaches across various key metrics, including travel
time and CO$_2$ emissions, demonstrating its potential for effective traffic
management
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09852" title="Abstract">arXiv:2311.09852</a> [<a href="/pdf/2311.09852" title="Download PDF">pdf</a>, <a href="/format/2311.09852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Short vs. Long-term Coordination of Drones: When Distributed  Optimization Meets Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Chuhao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Pournaras%2C+E">Evangelos Pournaras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Swarms of smart drones, with the support of charging technology, can provide
completing sensing capabilities in Smart Cities, such as traffic monitoring and
disaster response. Existing approaches, including distributed optimization and
deep reinforcement learning (DRL), aim to coordinate drones to achieve
cost-effective, high-quality navigation, sensing, and recharging. However, they
have distinct challenges: short-term optimization struggles to provide
sustained benefits, while long-term DRL lacks scalability, resilience, and
flexibility. To bridge this gap, this paper introduces a new progressive
approach that encompasses the planning and selection based on distributed
optimization, as well as DRL-based flying direction scheduling. Extensive
experiment with datasets generated from realisitic urban mobility demonstrate
the outstanding performance of the proposed solution in traffic monitoring
compared to three baseline methods.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09854" title="Abstract">arXiv:2311.09854</a> [<a href="/pdf/2311.09854" title="Download PDF">pdf</a>, <a href="/format/2311.09854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SurvTimeSurvival: Survival Analysis On The Patient With Multiple  Visits/Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hung Le</a>, 
<a href="/search/cs?searchtype=author&query=Eng-Jon%2C+O">Ong Eng-Jon</a>, 
<a href="/search/cs?searchtype=author&query=Miroslaw%2C+B">Bober Miroslaw</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Findings Track in Machine Learning For Health (ML4H) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
<p class="mathjax">The accurate prediction of survival times for patients with severe diseases
remains a critical challenge despite recent advances in artificial
intelligence. This study introduces "SurvTimeSurvival: Survival Analysis On
Patients With Multiple Visits/Records", utilizing the Transformer model to not
only handle the complexities of time-varying covariates but also covariates
data. We also tackle the data sparsity issue common to survival analysis
datasets by integrating synthetic data generation into the learning process of
our model. We show that our method outperforms state-of-the-art deep learning
approaches on both covariates and time-varying covariates datasets. Our
approach aims not only to enhance the understanding of individual patient
survival trajectories across various medical conditions, thereby improving
prediction accuracy, but also to play a pivotal role in designing clinical
trials and creating new treatments.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09856" title="Abstract">arXiv:2311.09856</a> [<a href="/pdf/2311.09856" title="Download PDF">pdf</a>, <a href="/format/2311.09856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contribution Evaluation in Federated Learning: Examining Current  Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siomos%2C+V">Vasilis Siomos</a>, 
<a href="/search/cs?searchtype=author&query=Passerat-Palmbach%2C+J">Jonathan Passerat-Palmbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at New Frontiers in Federated Learning: Privacy, Fairness, Robustness, Personalization and Data Ownership workshop @NeurIPS 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Federated Learning (FL) has seen increasing interest in cases where entities
want to collaboratively train models while maintaining privacy and governance
over their data. In FL, clients with private and potentially heterogeneous data
and compute resources come together to train a common model without raw data
ever leaving their locale. Instead, the participants contribute by sharing
local model updates, which, naturally, differ in quality. Quantitatively
evaluating the worth of these contributions is termed the Contribution
Evaluation (CE) problem. We review current CE approaches from the underlying
mathematical framework to efficiently calculate a fair value for each client.
Furthermore, we benchmark some of the most promising state-of-the-art
approaches, along with a new one we introduce, on MNIST and CIFAR-10, to
showcase their differences. Designing a fair and efficient CE method, while a
small part of the overall FL system design, is tantamount to the mainstream
adoption of FL.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09858" title="Abstract">arXiv:2311.09858</a> [<a href="/pdf/2311.09858" title="Download PDF">pdf</a>, <a href="/ps/2311.09858" title="Download PostScript">ps</a>, <a href="/format/2311.09858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomially Over-Parameterized Convolutional Neural Networks Contain  Structured Strong Winning Lottery Tickets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Cunha%2C+A">Arthur da Cunha</a>, 
<a href="/search/cs?searchtype=author&query=d%27Amore%2C+F">Francesco d&#x27;Amore</a>, 
<a href="/search/cs?searchtype=author&query=Natale%2C+E">Emanuele Natale</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR)

</div>
<p class="mathjax">The Strong Lottery Ticket Hypothesis (SLTH) states that randomly-initialised
neural networks likely contain subnetworks that perform well without any
training. Although unstructured pruning has been extensively studied in this
context, its structured counterpart, which can deliver significant
computational and memory efficiency gains, has been largely unexplored. One of
the main reasons for this gap is the limitations of the underlying mathematical
tools used in formal analyses of the SLTH. In this paper, we overcome these
limitations: we leverage recent advances in the multidimensional generalisation
of the Random Subset-Sum Problem and obtain a variant that admits the
stochastic dependencies that arise when addressing structured pruning in the
SLTH. We apply this result to prove, for a wide class of random Convolutional
Neural Networks, the existence of structured subnetworks that can approximate
any sufficiently smaller network.
<br />This result provides the first sub-exponential bound around the SLTH for
structured pruning, opening up new avenues for further research on the
hypothesis and contributing to the understanding of the role of
over-parameterization in deep learning.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09860" title="Abstract">arXiv:2311.09860</a> [<a href="/pdf/2311.09860" title="Download PDF">pdf</a>, <a href="/format/2311.09860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity  Extraction Focused on Machine Learning Models and Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Otto%2C+W">Wolfgang Otto</a>, 
<a href="/search/cs?searchtype=author&query=Zloch%2C+M">Matth&#xe4;us Zloch</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">Lu Gan</a>, 
<a href="/search/cs?searchtype=author&query=Karmakar%2C+S">Saurav Karmakar</a>, 
<a href="/search/cs?searchtype=author&query=Dietze%2C+S">Stefan Dietze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure, Accepted at EMNLP2023-Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Named Entity Recognition (NER) models play a crucial role in various NLP
tasks, including information extraction (IE) and text understanding. In
academic writing, references to machine learning models and datasets are
fundamental components of various computer science publications and necessitate
accurate models for identification. Despite the advancements in NER, existing
ground truth datasets do not treat fine-grained types like ML model and model
architecture as separate entity types, and consequently, baseline models cannot
recognize them as such. In this paper, we release a corpus of 100 manually
annotated full-text scientific publications and a first baseline model for 10
entity types centered around ML models and datasets. In order to provide a
nuanced understanding of how ML models and datasets are mentioned and utilized,
our dataset also contains annotations for informal mentions like "our
BERT-based model" or "an image CNN". You can find the ground truth dataset and
code to replicate model training at https://data.gesis.org/gsap/gsap-ner.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09861" title="Abstract">arXiv:2311.09861</a> [<a href="/pdf/2311.09861" title="Download PDF">pdf</a>, <a href="/format/2311.09861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PsyBench: a balanced and in-depth Psychological Chinese Evaluation  Benchmark for Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junlei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hongliang He</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+N">Nirui Song</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shuyuan He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+%5C">\\Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Huachuan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Anqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lizhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhenzhong Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As Large Language Models (LLMs) are becoming prevalent in various fields,
there is an urgent need for improved NLP benchmarks that encompass all the
necessary knowledge of individual discipline. Many contemporary benchmarks for
foundational models emphasize a broad range of subjects but often fall short in
presenting all the critical subjects and encompassing necessary professional
knowledge of them. This shortfall has led to skewed results, given that LLMs
exhibit varying performance across different subjects and knowledge areas. To
address this issue, we present psybench, the first comprehensive Chinese
evaluation suite that covers all the necessary knowledge required for graduate
entrance exams. psybench offers a deep evaluation of a model's strengths and
weaknesses in psychology through multiple-choice questions. Our findings show
significant differences in performance across different sections of a subject,
highlighting the risk of skewed results when the knowledge in test sets is not
balanced. Notably, only the ChatGPT model reaches an average accuracy above
$70\%$, indicating that there is still plenty of room for improvement. We
expect that psybench will help to conduct thorough evaluations of base models'
strengths and weaknesses and assist in practical application in the field of
psychology.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09862" title="Abstract">arXiv:2311.09862</a> [<a href="/pdf/2311.09862" title="Download PDF">pdf</a>, <a href="/format/2311.09862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Which Modality should I use -- Text, Motif, or Image? : Understanding  Graphs with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+D">Debarati Das</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+I">Ishaan Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+J">Jaideep Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Large language models (LLMs) are revolutionizing various fields by leveraging
large text corpora for context-aware intelligence. Due to the context size,
however, encoding an entire graph with LLMs is fundamentally limited. This
paper explores how to better integrate graph data with LLMs and presents a
novel approach using various encoding modalities (e.g., text, image, and motif)
and approximation of global connectivity of a graph using different prompting
methods to enhance LLMs' effectiveness in handling complex graph structures.
The study also introduces GraphTMI, a new benchmark for evaluating LLMs in
graph structure analysis, focusing on factors such as homophily, motif
presence, and graph difficulty. Key findings reveal that image modality,
supported by advanced vision-language models like GPT-4V, is more effective
than text in managing token limits while retaining critical information. The
research also examines the influence of different factors on each encoding
modality's performance. This study highlights the current limitations and
charts future directions for LLMs in graph understanding and reasoning tasks.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09865" title="Abstract">arXiv:2311.09865</a> [<a href="/pdf/2311.09865" title="Download PDF">pdf</a>, <a href="/format/2311.09865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuel Saving Effect and Performance of Velocity Control for Modern  Combustion-Powered Scooters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kre%C3%9F%2C+J">Jannis Kre&#xdf;</a>, 
<a href="/search/eess?searchtype=author&query=Rau%2C+J">Jens Rau</a>, 
<a href="/search/eess?searchtype=author&query=Hebert%2C+H">Hektor Hebert</a>, 
<a href="/search/eess?searchtype=author&query=Perez-Pe%C3%B1a%2C+F">Fernando Perez-Pe&#xf1;a</a>, 
<a href="/search/eess?searchtype=author&query=Schmidt%2C+K">Karsten Schmidt</a>, 
<a href="/search/eess?searchtype=author&query=Morgado-Est%C3%A9vez%2C+A">Arturo Morgado-Est&#xe9;vez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper investigates the performance and fuel-saving effect of a velocity
control algorithm on modern 50 cc scooters (Euro 5). The European Parliament
has adopted major CO$_2$ emission reductions by 2030. But modern combustion-
powered scooters are inefficiently restricted and emit unnecessary amounts of
CO$_2$. Replacing the original restriction method with the system presented in
this paper, the engine's operating point is being improved significantly.
Therefore, a Throttle-by-Wire-System senses the rider's throttle command and
manipulates the throttle valve. A redundant wheel speed sensor measures the
precise vehicle velocity using the magneto-resistive principle. The entire
system is managed by a central ECU, executing the actual velocity control,
fail-safe functions, power supply and handling inputs/outputs. For velocity
control, an adaptive PI-controller has been simulated, virtually tuned and
implemented, limiting the max. velocity regulated by legal constraints (45
km/h). In this way, the environmentally harmful restrictors used today can be
bypassed. By implementing a human-machine interface, including a virtual
dashboard, the system is capable of interfacing with the rider. For evaluation
purposes a measurement box has been developed, logging vehicle orientation,
system/control variables and engine parameters. A Peugeot Kisbee 50 4T (Euro 5)
is serving as test vehicle. Finally, the system has been evaluated regarding
performance and fuel efficiency both through simulation and road testing. Fuel
savings of 13.6 % in real-world test scenarios were achieved while maintaining
vehicle performance.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09866" title="Abstract">arXiv:2311.09866</a> [<a href="/pdf/2311.09866" title="Download PDF">pdf</a>, <a href="/format/2311.09866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A numerical method for solving elliptic equations on real closed  algebraic curves and surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hao%2C+W">Wenrui Hao</a>, 
<a href="/search/math?searchtype=author&query=Hauenstein%2C+J+D">Jonathan D. Hauenstein</a>, 
<a href="/search/math?searchtype=author&query=Regan%2C+M+H">Margaret H. Regan</a>, 
<a href="/search/math?searchtype=author&query=Tang%2C+T">Tingting Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">There are many numerical methods for solving partial different equations
(PDEs) on manifolds such as classical implicit, finite difference, finite
element, and isogeometric analysis methods which aim at improving the
interoperability between finite element method and computer aided design (CAD)
software. However, these approaches have difficulty when the domain has
singularities since the solution at the singularity may be multivalued. This
paper develops a novel numerical approach to solve elliptic PDEs on real,
closed, connected, orientable, and almost smooth algebraic curves and surfaces.
Our method integrates numerical algebraic geometry, differential geometry, and
a finite difference scheme which is demonstrated on several examples.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09867" title="Abstract">arXiv:2311.09867</a> [<a href="/pdf/2311.09867" title="Download PDF">pdf</a>, <a href="/format/2311.09867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel and Sequential Resources Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benatti%2C+A">Alexandre Benatti</a>, 
<a href="/search/cs?searchtype=author&query=da+F.+Costa%2C+L">Luciano da F. Costa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">A large number of real and abstract systems involve the transformation of
some basic resource into respective products under the action of multiple
processing agents, which can be understood as multiple-agent production systems
(MAP). At each discrete time instant, for each agent, a fraction of the
resources is assumed to be kept, forwarded to other agents, or converted into
work with some efficiency. The present work describes a systematic study of
nine basic MAP architectures subdivided into two main groups, namely parallel
and sequential distribution of resources from a single respective source.
Several types of interconnections among the involved processing agents are also
considered. The resulting MAP architectures are studied in terms of the total
amount of work, the dispersion of the resources (states) among the agents, and
the transition times from the start of operation until the respective steady
state. Several interesting results are obtained and discussed, including the
observation that some of the parallel designs were able to yield maximum work
and minimum state dispersion, achieved at the expense of the transition time
and use of several interconnections between the source and the agents. The
results obtained for the sequential designs indicate that relatively high
performance can be obtained for some specific cases.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09868" title="Abstract">arXiv:2311.09868</a> [<a href="/pdf/2311.09868" title="Download PDF">pdf</a>, <a href="/format/2311.09868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INTERVENOR: Prompt the Coding Ability of Large Language Models with the  Interactive Chain of Repairing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanbin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+G">Ganqu Cui</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Ning Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Ge Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 15 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics
human code repairing behavior (iteratively judging, rethinking, and repairing)
and prompts the coding ability of regard Large Language Models (LLMs).
Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code
Teacher, to play different roles in code repairing and work interactively to
repair the generated codes. The Code Learner is asked to generate and repair
code according to the instructions from the Code Teacher. The Code Teacher
rethinks the code errors according to the corresponding feedback from compilers
and iteratively generates the chain-of-repairing (CoR) to guide the code
repairing process for Code Learner. Our experiments show that INTERVENOR
outperforms the state-of-the-art methods and achieves about 13% and 4.5%
improvements over the GPT-3.5 model in code generation and code translation
tasks, respectively. Our further analyses show that CoR can illuminate the bug
reasons and solution plans via natural language. Thanks to the feedback of code
compilers, INTERVENOR can accurately identify the syntax errors and assertion
errors in the code and provide precise instructions to repair codes, making
LLMs achieve the plateau performance with only three repairing turns. All data
and codes are available at https://github.com/NEUIR/INTERVENOR
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09878" title="Abstract">arXiv:2311.09878</a> [<a href="/pdf/2311.09878" title="Download PDF">pdf</a>, <a href="/format/2311.09878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety Aware Autonomous Path Planning Using Model Predictive  Reinforcement Learning for Inland Waterways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vanneste%2C+A">Astrid Vanneste</a>, 
<a href="/search/cs?searchtype=author&query=Vanneste%2C+S">Simon Vanneste</a>, 
<a href="/search/cs?searchtype=author&query=Vasseur%2C+O">Olivier Vasseur</a>, 
<a href="/search/cs?searchtype=author&query=Janssens%2C+R">Robin Janssens</a>, 
<a href="/search/cs?searchtype=author&query=Billast%2C+M">Mattias Billast</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+A">Ali Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Mets%2C+K">Kevin Mets</a>, 
<a href="/search/cs?searchtype=author&query=De+Schepper%2C+T">Tom De Schepper</a>, 
<a href="/search/cs?searchtype=author&query=Mercelis%2C+S">Siegfried Mercelis</a>, 
<a href="/search/cs?searchtype=author&query=Hellinckx%2C+P">Peter Hellinckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> \c{opyright} 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In recent years, interest in autonomous shipping in urban waterways has
increased significantly due to the trend of keeping cars and trucks out of city
centers. Classical approaches such as Frenet frame based planning and potential
field navigation often require tuning of many configuration parameters and
sometimes even require a different configuration depending on the situation. In
this paper, we propose a novel path planning approach based on reinforcement
learning called Model Predictive Reinforcement Learning (MPRL). MPRL calculates
a series of waypoints for the vessel to follow. The environment is represented
as an occupancy grid map, allowing us to deal with any shape of waterway and
any number and shape of obstacles. We demonstrate our approach on two scenarios
and compare the resulting path with path planning using a Frenet frame and path
planning based on a proximal policy optimization (PPO) agent. Our results show
that MPRL outperforms both baselines in both test scenarios. The PPO based
approach was not able to reach the goal in either scenario while the Frenet
frame approach failed in the scenario consisting of a corner with obstacles.
MPRL was able to safely (collision free) navigate to the goal in both of the
test scenarios.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09879" title="Abstract">arXiv:2311.09879</a> [<a href="/pdf/2311.09879" title="Download PDF">pdf</a>, <a href="/ps/2311.09879" title="Download PostScript">ps</a>, <a href="/format/2311.09879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Layer Optimization for Statistical QoS Provision in C-RAN with  Finite-Length Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hancheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Langtian Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The cloud radio access network (C-RAN) has become the foundational structure
for various emerging communication paradigms, leveraging the flexible
deployment of distributed access points (APs) and centralized task processing.
In this paper, we propose a cross-layer optimization framework based on a
practical finite-length coding communication system in C-RAN, aiming at
maximizing bandwidth efficiency while providing statistical quality of service
(QoS) for individual services. Based on the theoretical results from effective
capacity and finite-length coding, we formulate a joint optimization problem
involving modulation and coding schemes (MCS), retransmission count, initial
bandwidth allocation and AP selection, which reflects the coordinated decision
of parameters across the physical layer, data link layer and transport layer.
To tackle such a mixed-integer nonlinear programming (MINLP) problem, we
firstly decompose it into a transmission parameter decision (TPD) sub-problem
and a user association (UA) sub-problem, which can be solved by a binary
search-based algorithm and an auction-based algorithm respectively. Simulation
results demonstrate that the proposed model can accurately capture the impact
of QoS requirements and channel quality on the optimal transmission parameters.
Furthermore, compared with fixed transmission parameter setting, the proposed
algorithms achieve the bandwidth efficiency gain up to 27.87% under various
traffic and channel scenarios.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09881" title="Abstract">arXiv:2311.09881</a> [<a href="/pdf/2311.09881" title="Download PDF">pdf</a>, <a href="/format/2311.09881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Software Genome Project: Venture to the Genomic Pathways of Open  Source Software and Its Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yueming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">With the boom in modern software development, open-source software has become
an integral part of various industries, driving progress in computer science.
However, the immense complexity and diversity of the open-source ecosystem also
pose a series of challenges, including issues of quality, security, management,
maintenance, compliance, and sustainability. Existing open-source governance
approaches, while excelling in community building and collaboration, still face
shortcomings in decentralized management, security, and maintenance. To address
these challenges, inspired by the Human Genome Project, we treat the software
source code as software DNA and propose the \textbf{Software Genome Project},
which is geared towards the secure monitoring and exploitation of open-source
software. By identifying and labeling integrated and classified code features
at a fine-grained level, and effectively identifying safeguards for functional
implementations and non-functional requirements at different levels of
granularity, Software Genome Project builds a complete set of software genome
maps to help developers and managers gain a deeper understanding of software
complexity and diversity. By dissecting and summarizing functional and
undesirable genes, Software Genome Project helps facilitate targeted software
remediation and optimization, provides valuable insight and understanding of
the entire software ecosystem, and supports critical development tasks such as
technology selection and open source governance. This project is expected to
drive the evolution of software development towards more efficient, reliable,
and sustainable software solutions.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09882" title="Abstract">arXiv:2311.09882</a> [<a href="/pdf/2311.09882" title="Download PDF">pdf</a>, <a href="/format/2311.09882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic modeling of an alkaline electrolyzer plant for process  simulation and optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cantisani%2C+N">Nicola Cantisani</a>, 
<a href="/search/eess?searchtype=author&query=Dovits%2C+J">Josefine Dovits</a>, 
<a href="/search/eess?searchtype=author&query=J%C3%B8rgensen%2C+J+B">John Bagterp J&#xf8;rgensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to European Control Conference (ECC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We develop a mathematical model for dynamical simulation of an alkaline
electrolyzer plant. We model each component of the system with mass and energy
balances. Our modeling strategy consists of a rigorous and systematic
formulation using differential algebraic equations (DAE), along with a
thermodynamic library that evaluates thermophysical properties. We show steady
state diagrams for the electrolyzer stack, and perform dynamic simulations.
Dynamic modelling of an electrolyzer enables simulation and model-based
optimization and control for optimal hydrogen production under varying
operating conditions.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09887" title="Abstract">arXiv:2311.09887</a> [<a href="/pdf/2311.09887" title="Download PDF">pdf</a>, <a href="/format/2311.09887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LIO-EKF: High Frequency LiDAR-Inertial Odometry using Extended Kalman  Filters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yibin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guadagnino%2C+T">Tiziano Guadagnino</a>, 
<a href="/search/cs?searchtype=author&query=Wiesmann%2C+L">Louis Wiesmann</a>, 
<a href="/search/cs?searchtype=author&query=Klingbeil%2C+L">Lasse Klingbeil</a>, 
<a href="/search/cs?searchtype=author&query=Stachniss%2C+C">Cyrill Stachniss</a>, 
<a href="/search/cs?searchtype=author&query=Kuhlmann%2C+H">Heiner Kuhlmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Odometry estimation is a key element for every autonomous system requiring
navigation in an unknown environment. In modern mobile robots, 3D
LiDAR-inertial systems are often used for this task. By fusing LiDAR scans and
IMU measurements, these systems can reduce the accumulated drift caused by
sequentially registering individual LiDAR scans and provide a robust pose
estimate. Although effective, LiDAR-inertial odometry systems require proper
parameter tuning to be deployed. In this paper, we propose LIO-EKF, a
tightly-coupled LiDAR-inertial odometry system based on point-to-point
registration and the classical extended Kalman filter scheme. We propose an
adaptive data association that considers the relative pose uncertainty, the map
discretization errors, and the LiDAR noise. In this way, we can substantially
reduce the parameters to tune for a given type of environment. The experimental
evaluation suggests that the proposed system performs on par with the
state-of-the-art LiDAR-inertial odometry pipelines, but is significantly faster
in computing the odometry.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09888" title="Abstract">arXiv:2311.09888</a> [<a href="/pdf/2311.09888" title="Download PDF">pdf</a>, <a href="/format/2311.09888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Field Velocity Sensing and Predictive Beamforming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaolin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+X">Xidong Mu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The novel concept of near-field velocity sensing is proposed. In contrast to
far-field velocity sensing, near-field velocity sensing enables the
simultaneous estimation of both radial and transverse velocities of a moving
target. A maximum-likelihood-based method is proposed for jointly estimating
the radial and transverse velocities from the echo signals. Assisted by
near-field velocity sensing, a predictive beamforming framework is proposed for
a moving communication user, which requires no channel estimation but achieves
seamless data transmission. Finally, numerical examples validate the proposed
approaches.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09889" title="Abstract">arXiv:2311.09889</a> [<a href="/pdf/2311.09889" title="Download PDF">pdf</a>, <a href="/format/2311.09889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Generation from Human Brain Activities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Ziyi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+Q">Qingyao Ai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lioma%2C+C">Christina Lioma</a>, 
<a href="/search/cs?searchtype=author&query=Ruotsalo%2C+T">Tuukka Ruotsalo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under Submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generating human language through non-invasive brain-computer interfaces
(BCIs) has the potential to unlock many applications, such as serving disabled
patients and improving communication. Currently, however, generating language
via BCIs has been previously successful only within a classification setup for
selecting pre-generated sentence continuation candidates with the most likely
cortical semantic representation. Inspired by recent research that revealed
associations between the brain and the large computational language models, we
propose a generative language BCI that utilizes the capacity of a large
language model (LLM) jointly with a semantic brain decoder to directly generate
language from functional magnetic resonance imaging (fMRI) input. The proposed
model can generate coherent language sequences aligned with the semantic
content of visual or auditory language stimuli perceived, without prior
knowledge of any pre-generated candidates. We compare the language generated
from the presented model with a random control, pre-generated language
selection approach, and a standard LLM, which generates common coherent text
solely based on the next word likelihood according to statistical language
training data. The proposed model is found to generate language that is more
aligned with semantic stimulus in response to which brain input is sampled. Our
findings demonstrate the potential and feasibility of employing BCIs in direct
language generation.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09902" title="Abstract">arXiv:2311.09902</a> [<a href="/pdf/2311.09902" title="Download PDF">pdf</a>, <a href="/format/2311.09902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selection of Distinct Morphologies to Divide &amp; Conquer Gigapixel  Pathology Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shafique%2C+A">Abubakr Shafique</a>, 
<a href="/search/cs?searchtype=author&query=Alfasly%2C+S">Saghir Alfasly</a>, 
<a href="/search/cs?searchtype=author&query=Alsaafin%2C+A">Areej Alsaafin</a>, 
<a href="/search/cs?searchtype=author&query=Nejat%2C+P">Peyman Nejat</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+J+A">Jibran A. Khan</a>, 
<a href="/search/cs?searchtype=author&query=Tizhoosh%2C+H+R">H.R.Tizhoosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Whole slide images (WSIs) are massive digital pathology files illustrating
intricate tissue structures. Selecting a small, representative subset of
patches from each WSI is essential yet challenging. Therefore, following the
"Divide &amp; Conquer" approach becomes essential to facilitate WSI analysis
including the classification and the WSI matching in computational pathology.
To this end, we propose a novel method termed "Selection of Distinct
Morphologies" (SDM) to choose a subset of WSI patches. The aim is to encompass
all inherent morphological variations within a given WSI while simultaneously
minimizing the number of selected patches to represent these variations,
ensuring a compact yet comprehensive set of patches. This systematically
curated patch set forms what we term a "montage". We assess the
representativeness of the SDM montage across various public and private
histopathology datasets. This is conducted by using the leave-one-out WSI
search and matching evaluation method, comparing it with the state-of-the-art
Yottixel's mosaic. SDM demonstrates remarkable efficacy across all datasets
during its evaluation. Furthermore, SDM eliminates the necessity for empirical
parameterization, a crucial aspect of Yottixel's mosaic, by inherently
optimizing the selection process to capture the distinct morphological features
within the WSI.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09904" title="Abstract">arXiv:2311.09904</a> [<a href="/pdf/2311.09904" title="Download PDF">pdf</a>, <a href="/ps/2311.09904" title="Download PostScript">ps</a>, <a href="/format/2311.09904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capacitated Network Bargaining Games: Stability and Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanit%C3%A0%2C+L">Laura Sanit&#xe0;</a>, 
<a href="/search/cs?searchtype=author&query=Verberk%2C+L">Lucy Verberk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Capacitated network bargaining games are popular combinatorial games that
involve the structure of matchings in graphs. We show that it is always
possible to stabilize unweighted instances of this problem (that is, ensure
that they admit a stable outcome) via capacity-reduction and edge-removal
operations, without decreasing the total value that the players can get.
<br />Furthermore, for general weighted instances, we show that computing a minimum
amount of vertex-capacity to reduce to make an instance stable is a
polynomial-time solvable problem. We then exploit this to give approximation
results for the NP-hard problem of stabilizing a graph via edge-removal
operations.
<br />Our work extends and generalizes previous results in the literature that
dealt with an uncapacitated version of the problem, using several new
arguments. In particular, while previous results mainly used combinatorial
techniques, we here rely on polyhedral arguments and, more specifically, on the
notion of circuits of a polytope.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09910" title="Abstract">arXiv:2311.09910</a> [<a href="/pdf/2311.09910" title="Download PDF">pdf</a>, <a href="/ps/2311.09910" title="Download PostScript">ps</a>, <a href="/format/2311.09910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DNA-Correcting Codes in DNA Storage Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huawei Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In [1], the authors proposed a new model of DNA storage system that
integrates all three steps of retrieval and introduced the concept of
DNA-correcting codes, which guarantees that the output of the storage system
can be decoded to the original data. They also gave necessary and sufficient
conditions for DNA-correcting codes when the data part is free of errors. In
this paper, we generalize their results to the general case.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09918" title="Abstract">arXiv:2311.09918</a> [<a href="/pdf/2311.09918" title="Download PDF">pdf</a>, <a href="/ps/2311.09918" title="Download PostScript">ps</a>, <a href="/format/2311.09918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proceedings of the 18th International Workshop on Logical Frameworks and  Meta-Languages: Theory and Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ciaffaglione%2C+A">Alberto Ciaffaglione</a> (Universit&#xe0; degli Studi di Udine), 
<a href="/search/cs?searchtype=author&query=Olarte%2C+C">Carlos Olarte</a> (LIPN, CNRS UMR 7030, Universit&#xe9; Sorbonne Paris Nord)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This volume contains the proceedings of the workshop Logical Frameworks and Meta-Languages: Theory and Practice (LFMTP'23)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 396, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Logical frameworks and meta-languages form a common substrate for
representing, implementing and reasoning about a wide variety of deductive
systems of interest in logic and computer science. Their design, implementation
and their use in reasoning tasks, ranging from the correctness of software to
the properties of formal systems, have been the focus of considerable research
over the last two decades. This workshop brings together designers,
implementors and practitioners to discuss various aspects impinging on the
structure and utility of logical frameworks, including the treatment of
variable binding, inductive and co-inductive reasoning techniques and the
expressiveness and lucidity of the reasoning process.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09919" title="Abstract">arXiv:2311.09919</a> [<a href="/pdf/2311.09919" title="Download PDF">pdf</a>, <a href="/format/2311.09919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSR-Diff: Depth Map Super-Resolution with Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+B">Bin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Rui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q">Qingmin Liao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenming Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Color-guided depth map super-resolution (CDSR) improve the spatial resolution
of a low-quality depth map with the corresponding high-quality color map,
benefiting various applications such as 3D reconstruction, virtual reality, and
augmented reality. While conventional CDSR methods typically rely on
convolutional neural networks or transformers, diffusion models (DMs) have
demonstrated notable effectiveness in high-level vision tasks. In this work, we
present a novel CDSR paradigm that utilizes a diffusion model within the latent
space to generate guidance for depth map super-resolution. The proposed method
comprises a guidance generation network (GGN), a depth map super-resolution
network (DSRN), and a guidance recovery network (GRN). The GGN is specifically
designed to generate the guidance while managing its compactness. Additionally,
we integrate a simple but effective feature fusion module and a
transformer-style feature extraction module into the DSRN, enabling it to
leverage guided priors in the extraction, fusion, and reconstruction of
multi-model images. Taking into account both accuracy and efficiency, our
proposed method has shown superior performance in extensive experiments when
compared to state-of-the-art methods. Our codes will be made available at
https://github.com/shiyuan7/DSR-Diff.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09922" title="Abstract">arXiv:2311.09922</a> [<a href="/pdf/2311.09922" title="Download PDF">pdf</a>, <a href="/ps/2311.09922" title="Download PostScript">ps</a>, <a href="/format/2311.09922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast multiplication by two&#x27;s complement addition of numbers represented  as a set of polynomial radix 2 indexes, stored as an integer list for  massively parallel computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stocks%2C+M">Mark Stocks</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
<p class="mathjax">We demonstrate a multiplication method based on numbers represented as set of
polynomial radix 2 indices stored as an integer list. The 'polynomial integer
index multiplication' method is a set of algorithms implemented in python code.
We demonstrate the method to be faster than both the Number Theoretic Transform
(NTT) and Karatsuba for multiplication within a certain bit range. Also
implemented in python code for comparison purposes with the polynomial radix 2
integer method. We demonstrate that it is possible to express any integer or
real number as a list of integer indices, representing a finite series in base
two. The finite series of integer index representation of a number can then be
stored and distributed across multiple CPUs / GPUs. We show that operations of
addition and multiplication can be applied as two's complement additions
operating on the index integer representations and can be fully distributed
across a given CPU / GPU architecture. We demonstrate fully distributed
arithmetic operations such that the 'polynomial integer index multiplication'
method overcomes the current limitation of parallel multiplication methods. Ie,
the need to share common core memory and common disk for the calculation of
results and intermediate results.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09929" title="Abstract">arXiv:2311.09929</a> [<a href="/pdf/2311.09929" title="Download PDF">pdf</a>, <a href="/format/2311.09929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mutating etcd Towards Edge Suitability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeffery%2C+A">Andrew Jeffery</a>, 
<a href="/search/cs?searchtype=author&query=Howard%2C+H">Heidi Howard</a>, 
<a href="/search/cs?searchtype=author&query=Mortier%2C+R">Richard Mortier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In the edge environment servers are no longer being co-located away from
clients, instead they are being co-located with clients away from other
servers, focusing on reliable and performant operation. Orchestration
platforms, such as Kubernetes, are a key system being transitioned to the edge
but they remain unsuited to the environment, stemming primarily from their
critical key-value stores. In this work we derive requirements from the edge
environment showing that, fundamentally, the design of distributed key-value
datastores, such as etcd, is unsuited to meet them. Using these requirements,
we explore the design space for distributed key-value datastores and implement
two successive mutations of etcd for different points: mergeable-etcd and
dismerge, trading linearizability for causal consistency based on CRDTs.
mergeable-etcd retains the linear revision history but encounters inherent
shortcomings, whilst dismerge embraces the causal model. Both stores are
local-first, maintaining reliable performance under network partitions and
variability, drastically surpassing etcd's performance, whilst maintaining
competitive performance in reliable settings.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09930" title="Abstract">arXiv:2311.09930</a> [<a href="/pdf/2311.09930" title="Download PDF">pdf</a>, <a href="/format/2311.09930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Monitoring and Retraining Language Models in Real-World  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasundra%2C+J">Jaykumar Kasundra</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+C">Claudia Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Mirsafian%2C+M">Melicaalsadat Mirsafian</a>, 
<a href="/search/cs?searchtype=author&query=Skylaki%2C+S">Stavroula Skylaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the Machine Learning (ML) model development lifecycle, training candidate
models using an offline holdout dataset and identifying the best model for the
given task is only the first step. After the deployment of the selected model,
continuous model monitoring and model retraining is required in many real-world
applications. There are multiple reasons for retraining, including data or
concept drift, which may be reflected on the model performance as monitored by
an appropriate metric. Another motivation for retraining is the acquisition of
increasing amounts of data over time, which may be used to retrain and improve
the model performance even in the absence of drifts. We examine the impact of
various retraining decision points on crucial factors, such as model
performance and resource utilization, in the context of Multilabel
Classification models. We explain our key decision points and propose a
reference framework for designing an effective model retraining strategy.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09932" title="Abstract">arXiv:2311.09932</a> [<a href="/pdf/2311.09932" title="Download PDF">pdf</a>, <a href="/format/2311.09932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Communication GSC System with Energy Harvesting Nodes aided by  Opportunistic Routing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+L">Lei Teng</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+W">Wannian An</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+X">Xiaoqi Qin</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaodong Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In this paper, a cooperative communication network based on energy-harvesting
(EH) decode-and-forward (DF) relays is proposed. For relay nodes, there is
harvest-storage-use (HSU) structure in this system. And energy can be obtained
from the surrounding environment through energy buffering. In order to improve
the performance of the communication system, the opportunistic routing
algorithm and the generalized selection combining (GSC) algorithm are adopted
in this communication system. In addition, from discrete-time continuous-state
space Markov chain model (DCSMC), a theoretical expression of the energy
limiting distribution stored in infinite buffers is derived. Through using the
probability distribution and state transition matrix, the theoretical
expressions of system outage probability, throughput and time cost of per
packet are obtained. Through the simulation verification, the theoretical
results are in good agreement with the simulation results.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09934" title="Abstract">arXiv:2311.09934</a> [<a href="/pdf/2311.09934" title="Download PDF">pdf</a>, <a href="/format/2311.09934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Echo Chambers within the Russo-Ukrainian War: The Role of Bipartisan  Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peixian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Haq%2C+E">Ehsan-Ul Haq</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yiming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+P">Pan Hui</a>, 
<a href="/search/cs?searchtype=author&query=Tyson%2C+G">Gareth Tyson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The ongoing Russia-Ukraine war has been extensively discussed on social
media. One commonly observed problem in such discussions is the emergence of
echo chambers, where users are rarely exposed to opinions outside their
worldview. Prior literature on this topic has assumed that such users hold a
single consistent view. However, recent work has revealed that complex topics
(such as the war) often trigger bipartisanship among certain people. With this
in mind, we study the presence of echo chambers on Twitter related to the
Russo-Ukrainian war. We measure their presence and identify an important subset
of bipartisan users who vary their opinions during the invasion. We explore the
role they play in the communications graph and identify features that
distinguish them from remaining users. We conclude by discussing their
importance and how they can improve the quality of discourse surrounding the
war.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09939" title="Abstract">arXiv:2311.09939</a> [<a href="/pdf/2311.09939" title="Download PDF">pdf</a>, <a href="/format/2311.09939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+S">Stefanos-Iordanis Papadopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Koutlis%2C+C">Christos Koutlis</a>, 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+S">Symeon Papadopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Petrantonakis%2C+P+C">Panagiotis C. Petrantonakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Online misinformation is often multimodal in nature, i.e., it is caused by
misleading associations between texts and accompanying images. To support the
fact-checking process, researchers have been recently developing automatic
multimodal methods that gather and analyze external information, evidence,
related to the image-text pairs under examination. However, prior works assumed
all collected evidence to be relevant. In this study, we introduce a "Relevant
Evidence Detection" (RED) module to discern whether each piece of evidence is
relevant, to support or refute the claim. Specifically, we develop the
"Relevant Evidence Detection Directed Transformer" (RED-DOT) and explore
multiple architectural variants (e.g., single or dual-stage) and mechanisms
(e.g., "guided attention"). Extensive ablation and comparative experiments
demonstrate that RED-DOT achieves significant improvements over the
state-of-the-art on the VERITE benchmark by up to 28.5%. Furthermore, our
evidence re-ranking and element-wise modality fusion led to RED-DOT achieving
competitive and even improved performance on NewsCLIPings+, without the need
for numerous evidence or multiple backbone encoders. Finally, our qualitative
analysis demonstrates that the proposed "guided attention" module has the
potential to enhance the architecture's interpretability. We release our code
at: https://github.com/stevejpapad/relevant-evidence-detection
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09941" title="Abstract">arXiv:2311.09941</a> [<a href="/pdf/2311.09941" title="Download PDF">pdf</a>, <a href="/format/2311.09941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ghost Value Augmentation for $k$-ECSS and $k$-ECSM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hershkowitz%2C+D+E">D Ellis Hershkowitz</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+N">Nathan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Zenklusen%2C+R">Rico Zenklusen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">We give a poly-time algorithm for the $k$-edge-connected spanning subgraph
($k$-ECSS) problem that returns a solution of cost no greater than the cheapest
$(k+10)$-ECSS on the same graph. Our approach enhances the iterative relaxation
framework with a new ingredient, which we call ghost values, that allows for
high sparsity in intermediate problems.
<br />Our guarantees improve upon the best-known approximation factor of $2$ for
$k$-ECSS whenever the optimal value of $(k+10)$-ECSS is close to that of
$k$-ECSS. This is a property that holds for the closely related problem
$k$-edge-connected spanning multi-subgraph ($k$-ECSM), which is identical to
$k$-ECSS except edges can be selected multiple times at the same cost. As a
consequence, we obtain a
$\left(1+O\left(\frac{1}{k}\right)\right)$-approximation for $k$-ECSM, which
resolves a conjecture of Pritchard and improves upon a recent
$1+O\left(\frac{1}{k}\right)$ approximation of Karlin, Klein, Oveis Gharan, and
Zhang. Moreover, we present a matching lower bound for $k$-ECSM, showing that
our approximation ratio is tight up to the constant factor in
$O\left(\frac{1}{k}\right)$, unless $P=NP$.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09944" title="Abstract">arXiv:2311.09944</a> [<a href="/pdf/2311.09944" title="Download PDF">pdf</a>, <a href="/format/2311.09944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Physics-Informed Neural Network approach for compartmental  epidemiological models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Millevoi%2C+C">Caterina Millevoi</a>, 
<a href="/search/math?searchtype=author&query=Pasetto%2C+D">Damiano Pasetto</a>, 
<a href="/search/math?searchtype=author&query=Ferronato%2C+M">Massimiliano Ferronato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Compartmental models provide simple and efficient tools to analyze the
relevant transmission processes during an outbreak, to produce short-term
forecasts or transmission scenarios, and to assess the impact of vaccination
campaigns. However, their calibration is not straightforward, since many
factors contribute to the rapid change of the transmission dynamics during an
epidemic. For example, there might be changes in the individual awareness, the
imposition of non-pharmacological interventions and the emergence of new
variants. As a consequence, model parameters such as the transmission rate are
doomed to change in time, making their assessment more challenging. Here, we
propose to use Physics-Informed Neural Networks (PINNs) to track the temporal
changes in the model parameters and provide an estimate of the model state
variables. PINNs recently gained attention in many engineering applications
thanks to their ability to consider both the information from data (typically
uncertain) and the governing equations of the system. The ability of PINNs to
identify unknown model parameters makes them particularly suitable to solve
ill-posed inverse problems, such as those arising in the application of
epidemiological models. Here, we develop a reduced-split approach for the
implementation of PINNs to estimate the temporal changes in the state variables
and transmission rate of an epidemic based on the SIR model equation and
infectious data. The main idea is to split the training first on the
epidemiological data, and then on the residual of the system equations. The
proposed method is applied to five synthetic test cases and two real scenarios
reproducing the first months of the COVID-19 Italian pandemic. Our results show
that the split implementation of PINNs outperforms the standard approach in
terms of accuracy (up to one order of magnitude) and computational times (speed
up of 20%).
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09945" title="Abstract">arXiv:2311.09945</a> [<a href="/pdf/2311.09945" title="Download PDF">pdf</a>, <a href="/format/2311.09945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Attention-Based Denoising Framework for Personality Detection in  Social Media Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Q">Qirui Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenkang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yihua Du</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lei Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In social media networks, users produce a large amount of text content
anytime, providing researchers with a valuable approach to digging for
personality-related information. Personality detection based on user-generated
texts is a universal method that can be used to build user portraits. The
presence of noise in social media texts hinders personality detection. However,
previous studies have not fully addressed this challenge. Inspired by the
scanning reading technique, we propose an attention-based information
extraction mechanism (AIEM) for long texts, which is applied to quickly locate
valuable pieces of information, and focus more attention on the deep semantics
of key pieces. Then, we provide a novel attention-based denoising framework
(ADF) for personality detection tasks and achieve state-of-the-art performance
on two commonly used datasets. Notably, we obtain an average accuracy
improvement of 10.2% on the gold standard Twitter-Myers-Briggs Type Indicator
(Twitter-MBTI) dataset. We made our code publicly available on GitHub. We shed
light on how AIEM works to magnify personality-related signals.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09947" title="Abstract">arXiv:2311.09947</a> [<a href="/pdf/2311.09947" title="Download PDF">pdf</a>, <a href="/ps/2311.09947" title="Download PostScript">ps</a>, <a href="/format/2311.09947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Disaster Analysis using Satellite Imagery and Social-Media Data  for Emergency Response Situations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandyam%2C+S">Sukeerthi Mandyam</a>, 
<a href="/search/cs?searchtype=author&query=MG%2C+S+P">Shanmuga Priya MG</a>, 
<a href="/search/cs?searchtype=author&query=Suresh%2C+S">Shalini Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+K">Kavitha Srinivasan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Disaster Management is one of the most promising research areas because of
its significant economic, environmental and social repercussions. This research
focuses on analyzing different types of data (pre and post satellite images and
twitter data) related to disaster management for in-depth analysis of
location-wise emergency requirements. This research has been divided into two
stages, namely, satellite image analysis and twitter data analysis followed by
integration using location. The first stage involves pre and post disaster
satellite image analysis of the location using multi-class land cover
segmentation technique based on U-Net architecture. The second stage focuses on
mapping the region with essential information about the disaster situation and
immediate requirements for relief operations. The severely affected regions are
demarcated and twitter data is extracted using keywords respective to that
location. The extraction of situational information from a large corpus of raw
tweets adopts Content Word based Tweet Summarization (COWTS) technique. An
integration of these modules using real-time location-based mapping and
frequency analysis technique gathers multi-dimensional information in the
advent of disaster occurrence such as the Kerala and Mississippi floods that
were analyzed and validated as test cases. The novelty of this research lies in
the application of segmented satellite images for disaster relief using
highlighted land cover changes and integration of twitter data by mapping these
region-specific filters for obtaining a complete overview of the disaster.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09948" title="Abstract">arXiv:2311.09948</a> [<a href="/pdf/2311.09948" title="Download PDF">pdf</a>, <a href="/format/2311.09948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hijacking Large Language Models via Adversarial In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiang%2C+Y">Yao Qiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiangyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dongxiao Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs
for specific tasks by utilizing labeled examples as demonstrations in the
precondition prompts. Despite its promising performance, ICL suffers from
instability with the choice and arrangement of examples. Additionally, crafted
adversarial attacks pose a notable threat to the robustness of ICL. However,
existing attacks are either easy to detect, rely on external models, or lack
specificity towards ICL. To address these issues, this work introduces a novel
transferable attack for ICL, aiming to hijack LLMs to generate the targeted
response. The proposed LLM hijacking attack leverages a gradient-based prompt
search method to learn and append imperceptible adversarial suffixes to the
in-context demonstrations. Extensive experimental results on various tasks and
datasets demonstrate the effectiveness of our LLM hijacking attack, resulting
in a distracted attention towards adversarial tokens, consequently leading to
the targeted unwanted outputs.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09962" title="Abstract">arXiv:2311.09962</a> [<a href="/pdf/2311.09962" title="Download PDF">pdf</a>, <a href="/format/2311.09962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised learning of multi-omics embeddings in the low-label,  high-data regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hurry%2C+C+J">Christian John Hurry</a>, 
<a href="/search/cs?searchtype=author&query=Slade%2C+E">Emma Slade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Contrastive, self-supervised learning (SSL) is used to train a model that
predicts cancer type from miRNA, mRNA or RPPA expression data. This model, a
pretrained FT-Transformer, is shown to outperform XGBoost and CatBoost,
standard benchmarks for tabular data, when labelled samples are scarce but the
number of unlabelled samples is high. This is despite the fact that the
datasets we use have $\mathcal{O}(10^{1})$ classes and
$\mathcal{O}(10^{2})-\mathcal{O}(10^{4})$ features. After demonstrating the
efficacy of our chosen method of self-supervised pretraining, we investigate
SSL for multi-modal models. A late-fusion model is proposed, where each omics
is passed through its own sub-network, the outputs of which are averaged and
passed to the pretraining or downstream objective function. Multi-modal
pretraining is shown to improve predictions from a single omics, and we argue
that this is useful for datasets with many unlabelled multi-modal samples, but
few labelled unimodal samples. Additionally, we show that pretraining each
omics-specific module individually is highly effective. This enables the
application of the proposed model in a variety of contexts where a large amount
of unlabelled data is available from each omics, but only a few labelled
samples.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09963" title="Abstract">arXiv:2311.09963</a> [<a href="/pdf/2311.09963" title="Download PDF">pdf</a>, <a href="/format/2311.09963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic modeling of wing-assisted inclined running with a morphing  multi-modal robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sihite%2C+E">Eric Sihite</a>, 
<a href="/search/cs?searchtype=author&query=Ramezani%2C+A">Alireza Ramezani</a>, 
<a href="/search/cs?searchtype=author&query=Gharib%2C+M">Morteza Gharib</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Robot designs can take many inspirations from nature, where there are many
examples of highly resilient and fault-tolerant locomotion strategies to
navigate complex terrains by using multi-functional appendages. For example,
Chukar and Hoatzin birds can repurpose their wings for quadrupedal walking and
wing-assisted incline running (WAIR) to climb steep surfaces. We took
inspiration from nature and designed a morphing robot with multi-functional
thruster-wheel appendages that allows the robot to change its mode of
locomotion by transforming into a rover, quad-rotor, mobile inverted pendulum
(MIP), and other modes. In this work, we derive a dynamic model and formulate a
nonlinear model predictive controller to perform WAIR to showcase the unique
capabilities of our robot. We implemented the model and controller in a
numerical simulation and experiments to show their feasibility and the
capabilities of our transforming multi-modal robot.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09965" title="Abstract">arXiv:2311.09965</a> [<a href="/pdf/2311.09965" title="Download PDF">pdf</a>, <a href="/format/2311.09965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SurgPLAN: Surgical Phase Localization Network for Phase Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xingjian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">You Pang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinlin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zongmin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Z">Zhen Lei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Surgical phase recognition is crucial to providing surgery understanding in
smart operating rooms. Despite great progress in automatic surgical phase
recognition, most existing methods are still restricted by two problems. First,
these methods cannot capture discriminative visual features for each frame and
motion information with simple 2D networks. Second, the frame-by-frame
recognition paradigm degrades the performance due to unstable predictions
within each phase, termed as phase shaking. To address these two challenges, we
propose a Surgical Phase LocAlization Network, named SurgPLAN, to facilitate a
more accurate and stable surgical phase recognition with the principle of
temporal detection. Specifically, we first devise a Pyramid SlowFast (PSF)
architecture to serve as the visual backbone to capture multi-scale spatial and
temporal features by two branches with different frame sampling rates.
Moreover, we propose a Temporal Phase Localization (TPL) module to generate the
phase prediction based on temporal region proposals, which ensures accurate and
consistent predictions within each surgical phase. Extensive experiments
confirm the significant advantages of our SurgPLAN over frame-by-frame
approaches in terms of both accuracy and stability.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09967" title="Abstract">arXiv:2311.09967</a> [<a href="/pdf/2311.09967" title="Download PDF">pdf</a>, <a href="/ps/2311.09967" title="Download PostScript">ps</a>, <a href="/format/2311.09967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Sequential Optimization Under Observability Don&#x27;t Cares
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marakkalage%2C+D+S">Dewmini Sudara Marakkalage</a>, 
<a href="/search/cs?searchtype=author&query=Testa%2C+E">Eleonora Testa</a>, 
<a href="/search/cs?searchtype=author&query=Neto%2C+W+L">Walter Lau Neto</a>, 
<a href="/search/cs?searchtype=author&query=Mishchenko%2C+A">Alan Mishchenko</a>, 
<a href="/search/cs?searchtype=author&query=De+Micheli%2C+G">Giovanni De Micheli</a>, 
<a href="/search/cs?searchtype=author&query=Amar%C3%B9%2C+L">Luca Amar&#xf9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the conference Design, Automation and Test in Europe (DATE) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Sequential logic synthesis can provide better Power-Performance-Area (PPA)
than combinational logic synthesis since it explores a larger solution space.
As the gate cost in advanced technologies keeps rising, sequential logic
synthesis provides a powerful alternative that is gaining momentum in the EDA
community. In this work, we present a new scalable algorithm for
don't-care-based sequential logic synthesis. Our new approach is based on
sequential k-step induction and can apply both redundancy removal and
resubstitution transformations under Sequential Observability Don't Cares
(SODCs). Using SODC-based optimizations with induction is a challenging problem
due to dependencies and alignment of don't cares among the base case and the
inductive case. We propose a new approach utilizing the full power of SODCs
without limiting the solution space. Our algorithm is implemented as part of an
industrial tool and achieves 6.9% average area improvement after technology
mapping when compared to state-of-the-art sequential synthesis methods.
Moreover, all the new sequential optimizations can be verified using
state-of-the-art sequential verification tools.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09969" title="Abstract">arXiv:2311.09969</a> [<a href="/pdf/2311.09969" title="Download PDF">pdf</a>, <a href="/ps/2311.09969" title="Download PostScript">ps</a>, <a href="/format/2311.09969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining bias perpetuation in academic search engines: an algorithm  audit of Google and Semantic Scholar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kacperski%2C+C">Celina Kacperski</a>, 
<a href="/search/cs?searchtype=author&query=Bielig%2C+M">Mona Bielig</a>, 
<a href="/search/cs?searchtype=author&query=Makorthyk%2C+M">Mykola Makorthyk</a>, 
<a href="/search/cs?searchtype=author&query=Sydorova%2C+M">Maryna Sydorova</a>, 
<a href="/search/cs?searchtype=author&query=Ulloa%2C+R">Roberto Ulloa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Researchers rely on academic web search engines to find scientific sources,
but search engine mechanisms may selectively present content that aligns with
biases embedded in the queries. This study examines whether confirmation-biased
queries prompted into Google Scholar and Semantic Scholar will yield skewed
results. Six queries (topics across health and technology domains such as
"vaccines" or "internet use") were analyzed for disparities in search results.
We confirm that biased queries (targeting "benefits" or "risks") affect search
results in line with the bias, with technology-related queries displaying more
significant disparities. Overall, Semantic Scholar exhibited fewer disparities
than Google Scholar. Topics rated as more polarizing did not consistently show
more skewed results. Academic search results that perpetuate confirmation bias
have strong implications for both researchers and citizens searching for
evidence. More research is needed to explore how scientific inquiry and
academic search engines interact.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09974" title="Abstract">arXiv:2311.09974</a> [<a href="/pdf/2311.09974" title="Download PDF">pdf</a>, <a href="/ps/2311.09974" title="Download PostScript">ps</a>, <a href="/format/2311.09974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiansong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peizhong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, the code of this paper is releasing soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, self-supervised contrastive learning has emerged as a
distinguished paradigm in the artificial intelligence landscape. It facilitates
unsupervised feature learning through contrastive delineations at the instance
level. However, crafting an effective self-supervised paradigm remains a
pivotal challenge within this field. This paper delves into two crucial factors
impacting self-supervised contrastive learning-bach size and pretext tasks, and
from a data processing standpoint, proposes an adaptive technique of batch
fusion. The proposed method, via dimensionality reduction and reconstruction of
batch data, enables formerly isolated individual data to partake in intra-batch
communication through the Embedding Layer. Moreover, it adaptively amplifies
the self-supervised feature encoding capability as the training progresses. We
conducted a linear classification test of this method based on the classic
contrastive learning framework on ImageNet-1k. The empirical findings
illustrate that our approach achieves state-of-the-art performance under
equitable comparisons. Benefiting from its "plug-and-play" characteristics, we
further explored other contrastive learning methods. On the ImageNet-100,
compared to the original performance, the top1 has seen a maximum increase of
1.25%. We suggest that the proposed method may contribute to the advancement of
data-driven self-supervised learning research, bringing a fresh perspective to
this community.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09975" title="Abstract">arXiv:2311.09975</a> [<a href="/pdf/2311.09975" title="Download PDF">pdf</a>, <a href="/ps/2311.09975" title="Download PostScript">ps</a>, <a href="/format/2311.09975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Version Age of Information Minimization over Fading Broadcast Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karevvanavar%2C+G">Gangadhar Karevvanavar</a>, 
<a href="/search/cs?searchtype=author&query=Pable%2C+H">Hrishikesh Pable</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+O">Om Patil</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+R+V">Rajshekhar V Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We consider a base station (BS) that receives version update packets from
multiple exogenous streams and broadcasts them to corresponding destination
users over a fading broadcast channel using a non-orthogonal multiple access
(NOMA) scheme. In each stream, packets arrive randomly, with each new packet's
index being one higher than that of the immediate previous packet. Moreover,
the arrival of a new version update renders previous versions obsolete. In this
case, we consider the version age of information (VAoI) at a user, defined as
the difference in the version index of the latest available packet at the BS
and that at the user, as a metric of freshness of information. Our objective is
to minimize a weighted sum of the long-term expected average VAoI across users
and average power by optimally scheduling the update packets from different
streams for transmission and transmitting them with appropriate powers to
guarantee their successful delivery. We obtain the optimal policy within the
class of channel-only stationary randomized policies (CO-SRP), which make
transmission decisions based only on the channel power gain realizations at
each decision time, and the optimal Markov decision process (MDP) based
solution, along with its structural properties. Via numerical simulations, we
show that the proposed optimal CO-SRP performs close to the MDP-based solution.
Additionally, a time division multiple access (TDMA) scheme that allows
transmission to at most one user at any given time instant exhibits comparable
performance to NOMA when the average power consumed is low. However, as the
average power consumed increases, NOMA outperforms TDMA.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09976" title="Abstract">arXiv:2311.09976</a> [<a href="/pdf/2311.09976" title="Download PDF">pdf</a>, <a href="/format/2311.09976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revolutionizing Customer Interactions: Insights and Challenges in  Deploying ChatGPT and Generative Chatbots for FAQs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khennouche%2C+F">Feriel Khennouche</a>, 
<a href="/search/cs?searchtype=author&query=Elmir%2C+Y">Youssef Elmir</a>, 
<a href="/search/cs?searchtype=author&query=Djebari%2C+N">Nabil Djebari</a>, 
<a href="/search/cs?searchtype=author&query=Himeur%2C+Y">Yassine Himeur</a>, 
<a href="/search/cs?searchtype=author&query=Amira%2C+A">Abbes Amira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In the rapidly evolving domain of artificial intelligence, chatbots have
emerged as a potent tool for various applications ranging from e-commerce to
healthcare. This research delves into the intricacies of chatbot technology,
from its foundational concepts to advanced generative models like ChatGPT. We
present a comprehensive taxonomy of existing chatbot approaches, distinguishing
between rule-based, retrieval-based, generative, and hybrid models. A specific
emphasis is placed on ChatGPT, elucidating its merits for frequently asked
questions (FAQs)-based chatbots, coupled with an exploration of associated
Natural Language Processing (NLP) techniques such as named entity recognition,
intent classification, and sentiment analysis. The paper further delves into
the customization and fine-tuning of ChatGPT, its integration with knowledge
bases, and the consequent challenges and ethical considerations that arise.
Through real-world applications in domains such as online shopping, healthcare,
and education, we underscore the transformative potential of chatbots. However,
we also spotlight open challenges and suggest future research directions,
emphasizing the need for optimizing conversational flow, advancing dialogue
mechanics, improving domain adaptability, and enhancing ethical considerations.
The research culminates in a call for further exploration in ensuring
transparent, ethical, and user-centric chatbot systems.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09979" title="Abstract">arXiv:2311.09979</a> [<a href="/pdf/2311.09979" title="Download PDF">pdf</a>, <a href="/format/2311.09979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unambiguity and Fewness for Nonuniform Families of Polynomial-Size  Nondeterministic Finite Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamakami%2C+T">Tomoyuki Yamakami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> (A4, 10pt, 17 pages) This work corrects and also significantly alters the preliminary report that appeared in the Proceedings of the 16th International Conference on Reachability Problems (RP 2022), Kaiserslautern, Germany, October 17--21, 2022, Lecture Notes in Computer Science, vol. 13608, pp. 77--92, Springer Cham, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Computational Complexity (cs.CC); Computation and Language (cs.CL)

</div>
<p class="mathjax">Nonuniform families of polynomial-size finite automata, which are series of
indexed finite automata having polynomially many inner states, are used in the
past literature to solve nonuniform families of promise decision problems.
Among such nonuniform families of finite automata, we focus our attention, in
particular, on the variants of nondeterministic finite automata, which have at
most "one" (unambiguous), "polynomially many" (few) accepting computation
paths, or unambiguous/few computation paths leading to each fixed
configuration. When such machines are limited to make only one-way head moves,
we can prove with no unproven hardness assumptions that some of these variants
are different in computational power from each other. As for two-way machines
restricted to instances of polynomially-bounded length, families of two-way
polynomial-size nondeterministic finite automata are equivalent in power to
families of polynomial-size unambiguous finite automata.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09984" title="Abstract">arXiv:2311.09984</a> [<a href="/pdf/2311.09984" title="Download PDF">pdf</a>, <a href="/ps/2311.09984" title="Download PostScript">ps</a>, <a href="/format/2311.09984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Modeling, Analyzing, and Decision-Making in Disease  Spread Dynamics and Medicine/Vaccine Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panthakkalakath%2C+Z+E">Zenin Easa Panthakkalakath</a>, 
<a href="/search/cs?searchtype=author&query=Neeraj">Neeraj</a>, 
<a href="/search/cs?searchtype=author&query=Mathew%2C+J">Jimson Mathew</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">The challenges posed by epidemics and pandemics are immense, especially if
the causes are novel. This article introduces a versatile open-source
simulation framework designed to model intricate dynamics of infectious
diseases across diverse population centres. Taking inspiration from historical
precedents such as the Spanish flu and COVID-19, and geographical economic
theories such as Central place theory, the simulation integrates agent-based
modelling to depict the movement and interactions of individuals within
different settlement hierarchies. Additionally, the framework provides a tool
for decision-makers to assess and strategize optimal distribution plans for
limited resources like vaccines or cures as well as to impose mobility
restrictions.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09989" title="Abstract">arXiv:2311.09989</a> [<a href="/pdf/2311.09989" title="Download PDF">pdf</a>, <a href="/ps/2311.09989" title="Download PostScript">ps</a>, <a href="/format/2311.09989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Xputer: Bridging Data Gaps with NMF, XGBoost, and a Streamlined GUI  Experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Younus%2C+S">Saleena Younus</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6nnstrand%2C+L">Lars R&#xf6;nnstrand</a>, 
<a href="/search/cs?searchtype=author&query=Kazi%2C+J+U">Julhash U. Kazi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM); Methodology (stat.ME)

</div>
<p class="mathjax">The rapid proliferation of data across diverse fields has accentuated the
importance of accurate imputation for missing values. This task is crucial for
ensuring data integrity and deriving meaningful insights. In response to this
challenge, we present Xputer, a novel imputation tool that adeptly integrates
Non-negative Matrix Factorization (NMF) with the predictive strengths of
XGBoost. One of Xputer's standout features is its versatility: it supports zero
imputation, enables hyperparameter optimization through Optuna, and allows
users to define the number of iterations. For enhanced user experience and
accessibility, we have equipped Xputer with an intuitive Graphical User
Interface (GUI) ensuring ease of handling, even for those less familiar with
computational tools. In performance benchmarks, Xputer not only rivals the
computational speed of established tools such as IterativeImputer but also
often outperforms them in terms of imputation accuracy. Furthermore, Xputer
autonomously handles a diverse spectrum of data types, including categorical,
continuous, and Boolean, eliminating the need for prior preprocessing. Given
its blend of performance, flexibility, and user-friendly design, Xputer emerges
as a state-of-the-art solution in the realm of data imputation.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09991" title="Abstract">arXiv:2311.09991</a> [<a href="/pdf/2311.09991" title="Download PDF">pdf</a>, <a href="/format/2311.09991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Market Research on IIoT Standard Compliance Monitoring Providers and  deriving Attributes for IIoT Compliance Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oberhofer%2C+D">Daniel Oberhofer</a>, 
<a href="/search/cs?searchtype=author&query=Hornsteiner%2C+M">Markus Hornsteiner</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6nig%2C+S">Stefan Sch&#xf6;nig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Adapting security architectures to common standards like IEC 62443 or ISO
27000 in the Industrial Internet of Things (IIoT) involves complex processes
and compliance reports. Automatic monitoring of compliance status would enhance
this process. Despite limited research, practical applications exist. This
paper conducts a market study on providers implementing IEC 62443 in IIoT,
aiming to formulate a catalog of monitorable attributes aligned with the
standard. The study reveals challenges, such as a lack of formal separation in
security architectures, limiting visibility. Despite these challenges,
practical implementations share commonalities, providing insights into viable
monitoring properties. The research serves as a crucial entry point into
developing a comprehensive catalog of monitorable attributes for IEC 62443
standards in IIoT.
<br />Aligned with the IEC 62443 SR catalog of document 3-3, monitorable attributes
are derived based on current research about IIoT security and Expert Knowledge.
The provided tables serve as an exemplary extract, not exhaustive, defining
three types of attributes based on their origin of creation.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09993" title="Abstract">arXiv:2311.09993</a> [<a href="/pdf/2311.09993" title="Download PDF">pdf</a>, <a href="/format/2311.09993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI for Hate Speech Detection: Evaluation and Findings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pendzel%2C+S">Sagi Pendzel</a>, 
<a href="/search/cs?searchtype=author&query=Wullach%2C+T">Tomer Wullach</a>, 
<a href="/search/cs?searchtype=author&query=Adler%2C+A">Amir Adler</a>, 
<a href="/search/cs?searchtype=author&query=Minkov%2C+E">Einat Minkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automatic hate speech detection using deep neural models is hampered by the
scarcity of labeled datasets, leading to poor generalization. To mitigate this
problem, generative AI has been utilized to generate large amounts of synthetic
hate speech sequences from available labeled examples, leveraging the generated
data in finetuning large pre-trained language models (LLMs). In this chapter,
we provide a review of relevant methods, experimental setups and evaluation of
this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT,
we apply and evaluate the impact of train set augmentation with generated data
using LLMs that have been already adapted for hate detection, including
RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical
study corroborates our previous findings, showing that this approach improves
hate speech generalization, boosting recall performance across data
distributions. In addition, we explore and compare the performance of the
finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results
demonstrate that while better generalization is achieved using the GPT-3.5
model, it achieves mediocre recall and low precision on most datasets. It is an
open question whether the sensitivity of models such as GPT-3.5, and onward,
can be improved using similar techniques of text generation.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09994" title="Abstract">arXiv:2311.09994</a> [<a href="/pdf/2311.09994" title="Download PDF">pdf</a>, <a href="/format/2311.09994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards more Practical Threat Models in Artificial Intelligence Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grosse%2C+K">Kathrin Grosse</a>, 
<a href="/search/cs?searchtype=author&query=Bieringer%2C+L">Lukas Bieringer</a>, 
<a href="/search/cs?searchtype=author&query=Besold%2C+T+R">Tarek Richard Besold</a>, 
<a href="/search/cs?searchtype=author&query=Alahi%2C+A">Alexandre Alahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures, 7 tables, under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent works have identified a gap between research and practice in
artificial intelligence security: threats studied in academia do not always
reflect the practical use and security risks of AI. For example, while models
are often studied in isolation, they form part of larger ML pipelines in
practice. Recent works also brought forward that adversarial manipulations
introduced by academic attacks are impractical. We take a first step towards
describing the full extent of this disparity. To this end, we revisit the
threat models of the six most studied attacks in AI security research and match
them to AI usage in practice via a survey with \textbf{271} industrial
practitioners. On the one hand, we find that all existing threat models are
indeed applicable. On the other hand, there are significant mismatches:
research is often too generous with the attacker, assuming access to
information not frequently available in real-world settings. Our paper is thus
a call for action to study more practical threat models in artificial
intelligence security.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09998" title="Abstract">arXiv:2311.09998</a> [<a href="/pdf/2311.09998" title="Download PDF">pdf</a>, <a href="/format/2311.09998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepEMD: A Transformer-based Fast Estimation of the Earth Mover&#x27;s  Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A+K">Atul Kumar Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Fleuret%2C+F">Francois Fleuret</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The Earth Mover's Distance (EMD) is the measure of choice between point
clouds. However the computational cost to compute it makes it prohibitive as a
training loss, and the standard approach is to use a surrogate such as the
Chamfer distance. We propose an attention-based model to compute an accurate
approximation of the EMD that can be used as a training loss for generative
models. To get the necessary accurate estimation of the gradients we train our
model to explicitly compute the matching between point clouds instead of EMD
itself. We cast this new objective as the estimation of an attention matrix
that approximates the ground truth matching matrix. Experiments show that this
model provides an accurate estimate of the EMD and its gradient with a wall
clock speed-up of more than two orders of magnitude with respect to the exact
Hungarian matching algorithm and one order of magnitude with respect to the
standard approximate Sinkhorn algorithm, allowing in particular to train a
point cloud VAE with the EMD itself. Extensive evaluation show the remarkable
behaviour of this model when operating out-of-distribution, a key requirement
for a distance surrogate. Finally, the model generalizes very well to point
clouds during inference several times larger than during training.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09999" title="Abstract">arXiv:2311.09999</a> [<a href="/pdf/2311.09999" title="Download PDF">pdf</a>, <a href="/format/2311.09999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransFusion -- A Transparency-Based Diffusion Model for Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%C4%8Dka%2C+M">Matic Fu&#x10d;ka</a>, 
<a href="/search/cs?searchtype=author&query=Zavrtanik%2C+V">Vitjan Zavrtanik</a>, 
<a href="/search/cs?searchtype=author&query=Sko%C4%8Daj%2C+D">Danijel Sko&#x10d;aj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Surface anomaly detection is a vital component in manufacturing inspection.
Reconstructive anomaly detection methods restore the normal appearance of an
object, ideally modifying only the anomalous regions. Due to the limitations of
commonly used reconstruction architectures, the produced reconstructions are
often poor and either still contain anomalies or lack details in anomaly-free
regions. Recent reconstructive methods adopt diffusion models, however with the
standard diffusion process the problems are not adequately addressed. We
propose a novel transparency-based diffusion process, where the transparency of
anomalous regions is progressively increased, restoring their normal appearance
accurately and maintaining the appearance of anomaly-free regions without loss
of detail. We propose TRANSparency DifFUSION (TransFusion), a discriminative
anomaly detection method that implements the proposed diffusion process,
enabling accurate downstream anomaly detection. TransFusion achieves
state-of-the-art performance on both the VisA and the MVTec AD datasets, with
an image-level AUROC of 98.5% and 99.2%, respectively.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10002" title="Abstract">arXiv:2311.10002</a> [<a href="/pdf/2311.10002" title="Download PDF">pdf</a>, <a href="/format/2311.10002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Straggler-resilient Federated Learning: Tackling Computation  Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongda Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Ping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Narayana%2C+C+V+A">C V Aswartha Narayana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) enables many resource-limited devices to train a
model collaboratively without data sharing. However, many existing works focus
on model-homogeneous FL, where the global and local models are the same size,
ignoring the inherently heterogeneous computational capabilities of different
devices and restricting resource-constrained devices from contributing to FL.
In this paper, we consider model-heterogeneous FL and propose Federated Partial
Model Training (FedPMT), where devices with smaller computational capabilities
work on partial models (subsets of the global model) and contribute to the
global model. Different from Dropout-based partial model generation, which
removes neurons in hidden layers at random, model training in FedPMT is
achieved from the back-propagation perspective. As such, all devices in FedPMT
prioritize the most crucial parts of the global model. Theoretical analysis
shows that the proposed partial model training design has a similar convergence
rate to the widely adopted Federated Averaging (FedAvg) algorithm,
$\mathcal{O}(1/T)$, with the sub-optimality gap enlarged by a constant factor
related to the model splitting design in FedPMT. Empirical results show that
FedPMT significantly outperforms the existing benchmark FedDrop. Meanwhile,
compared to the popular model-homogeneous benchmark, FedAvg, FedPMT reaches the
learning target in a shorter completion time, thus achieving a better trade-off
between learning accuracy and completion time.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10005" title="Abstract">arXiv:2311.10005</a> [<a href="/pdf/2311.10005" title="Download PDF">pdf</a>, <a href="/format/2311.10005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Flexibility and Robustness of LSM Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huynh%2C+A">Andy Huynh</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+H+A">Harshal A. Chaudhari</a>, 
<a href="/search/cs?searchtype=author&query=Terzi%2C+E">Evimaria Terzi</a>, 
<a href="/search/cs?searchtype=author&query=Athanassoulis%2C+M">Manos Athanassoulis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 19 figures, VLDB-J. arXiv admin note: substantial text overlap with <a href="/abs/2110.13801">arXiv:2110.13801</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Log-Structured Merge trees (LSM trees) are increasingly used as part of the
storage engine behind several data systems, and are frequently deployed in the
cloud. As the number of applications relying on LSM-based storage backends
increases, the problem of performance tuning of LSM trees receives increasing
attention. We consider both nominal tunings - where workload and execution
environment are accurately known a priori - and robust tunings - which consider
uncertainty in the workload knowledge. This type of workload uncertainty is
common in modern applications, notably in shared infrastructure environments
like the public cloud.
<br />To address this problem, we introduce ENDURE, a new paradigm for tuning LSM
trees in the presence of workload uncertainty. Specifically, we focus on the
impact of the choice of compaction policy, size ratio, and memory allocation on
the overall performance. ENDURE considers a robust formulation of the
throughput maximization problem and recommends a tuning that offers
near-optimal throughput when the executed workload is not the same, instead in
a neighborhood of the expected workload. Additionally, we explore the
robustness of flexible LSM designs by proposing a new unified design called
K-LSM that encompasses existing designs. We deploy our robust tuning system,
ENDURE, on a state-of-the-art key-value store, RocksDB, and demonstrate
throughput improvements of up to 5x in the presence of uncertainty. Our results
indicate that the tunings obtained by ENDURE are more robust than tunings
obtained under our expanded LSM design space. This indicates that robustness
may not be inherent to a design, instead, it is an outcome of a tuning process
that explicitly accounts for uncertainty.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10011" title="Abstract">arXiv:2311.10011</a> [<a href="/pdf/2311.10011" title="Download PDF">pdf</a>, <a href="/format/2311.10011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SQLNet: Scale-Modulated Query and Localization Network for Few-Shot  Class-Agnostic Counting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hefeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yandong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianshui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Keze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Liang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The class-agnostic counting (CAC) task has recently been proposed to solve
the problem of counting all objects of an arbitrary class with several
exemplars given in the input image. To address this challenging task, existing
leading methods all resort to density map regression, which renders them
impractical for downstream tasks that require object locations and restricts
their ability to well explore the scale information of exemplars for
supervision. To address the limitations, we propose a novel localization-based
CAC approach, termed Scale-modulated Query and Localization Network (SQLNet).
It fully explores the scales of exemplars in both the query and localization
stages and achieves effective counting by accurately locating each object and
predicting its approximate size. Specifically, during the query stage, rich
discriminative representations of the target class are acquired by the
Hierarchical Exemplars Collaborative Enhancement (HECE) module from the few
exemplars through multi-scale exemplar cooperation with equifrequent size
prompt embedding. These representations are then fed into the Exemplars-Unified
Query Correlation (EUQC) module to interact with the query features in a
unified manner and produce the correlated query tensor. In the localization
stage, the Scale-aware Multi-head Localization (SAML) module utilizes the query
tensor to predict the confidence, location, and size of each potential object.
Moreover, a scale-aware localization loss is introduced, which exploits
flexible location associations and exemplar scales for supervision to optimize
the model performance. Extensive experiments demonstrate that SQLNet
outperforms state-of-the-art methods on popular CAC benchmarks, achieving
excellent performance not only in counting accuracy but also in localization
and bounding box generation. Our codes will be available at
https://github.com/HCPLab-SYSU/SQLNet
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10012" title="Abstract">arXiv:2311.10012</a> [<a href="/pdf/2311.10012" title="Download PDF">pdf</a>, <a href="/format/2311.10012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Real-World Orbital Motion Laws from Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Funenga%2C+J">Jo&#xe3;o Funenga</a>, 
<a href="/search/cs?searchtype=author&query=Guimar%C3%A3es%2C+M">Marta Guimar&#xe3;es</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+H">Henrique Costa</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+C">Cl&#xe1;udia Soares</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Earth and Planetary Astrophysics (astro-ph.EP); Instrumentation and Methods for Astrophysics (astro-ph.IM)

</div>
<p class="mathjax">A novel approach is presented for discovering PDEs that govern the motion of
satellites in space. The method is based on SINDy, a data-driven technique
capable of identifying the underlying dynamics of complex physical systems from
time series data. SINDy is utilized to uncover PDEs that describe the laws of
physics in space, which are non-deterministic and influenced by various factors
such as drag or the reference area (related to the attitude of the satellite).
In contrast to prior works, the physically interpretable coordinate system is
maintained, and no dimensionality reduction technique is applied to the data.
By training the model with multiple representative trajectories of LEO -
encompassing various inclinations, eccentricities, and altitudes - and testing
it with unseen orbital motion patterns, a mean error of around 140 km for the
positions and 0.12 km/s for the velocities is achieved. The method offers the
advantage of delivering interpretable, accurate, and complex models of orbital
motion that can be employed for propagation or as inputs to predictive models
for other variables of interest, such as atmospheric drag or the probability of
collision in an encounter with a spacecraft or space objects. In conclusion,
the work demonstrates the promising potential of using SINDy to discover the
equations governing the behaviour of satellites in space. The technique has
been successfully applied to uncover PDEs describing the motion of satellites
in LEO with high accuracy. The method possesses several advantages over
traditional models, including the ability to provide physically interpretable,
accurate, and complex models of orbital motion derived from high-entropy
datasets. These models can be utilised for propagation or as inputs to
predictive models for other variables of interest.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10018" title="Abstract">arXiv:2311.10018</a> [<a href="/pdf/2311.10018" title="Download PDF">pdf</a>, <a href="/format/2311.10018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Overconfidence Problem in Semantic 3D Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marques%2C+J+M+C">Joao Marcos Correia Marques</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+A">Albert Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenlong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hauser%2C+K">Kris Hauser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a preprint for the work submitted to the ICRA 2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Semantic 3D mapping, the process of fusing depth and image segmentation
information between multiple views to build 3D maps annotated with object
classes in real-time, is a recent topic of interest. This paper highlights the
fusion overconfidence problem, in which conventional mapping methods assign
high confidence to the entire map even when they are incorrect, leading to
miscalibrated outputs. Several methods to improve uncertainty calibration at
different stages in the fusion pipeline are presented and compared on the
ScanNet dataset. We show that the most widely used Bayesian fusion strategy is
among the worst calibrated, and propose a learned pipeline that combines fusion
and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map
calibration while retaining real-time capability. We further illustrate the
importance of map calibration on a downstream task by showing that
incorporating proper semantic fusion on a modular ObjectNav agent improves its
success rates. Our code will be provided on Github for reproducibility upon
acceptance.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10025" title="Abstract">arXiv:2311.10025</a> [<a href="/pdf/2311.10025" title="Download PDF">pdf</a>, <a href="/format/2311.10025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Neural Network-Based Federated Learning System for Imbalanced  and Non-IID Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+M+R">Mahfuzur Rahman Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+M">Muhammad Ibrahim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">With the growth of machine learning techniques, privacy of data of users has
become a major concern. Most of the machine learning algorithms rely heavily on
large amount of data which may be collected from various sources. Collecting
these data yet maintaining privacy policies has become one of the most
challenging tasks for the researchers. To combat this issue, researchers have
introduced federated learning, where a prediction model is learnt by ensuring
the privacy of data of clients data. However, the prevalent federated learning
algorithms possess an accuracy and efficiency trade-off, especially for non-IID
data. In this research, we propose a centralized, neural network-based
federated learning system. The centralized algorithm incorporates micro-level
parallel processing inspired by the traditional mini-batch algorithm where the
client devices and the server handle the forward and backward propagation
respectively. We also devise a semi-centralized version of our proposed
algorithm. This algorithm takes advantage of edge computing for minimizing the
load from the central server, where clients handle both the forward and
backward propagation while sacrificing the overall train time to some extent.
We evaluate our proposed systems on five well-known benchmark datasets and
achieve satisfactory performance in a reasonable time across various data
distribution settings as compared to some existing benchmark algorithms.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10026" title="Abstract">arXiv:2311.10026</a> [<a href="/pdf/2311.10026" title="Download PDF">pdf</a>, <a href="/format/2311.10026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guaranteeing Control Requirements via Reward Shaping in Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=De+Lellis%2C+F">Francesco De Lellis</a>, 
<a href="/search/eess?searchtype=author&query=Coraggio%2C+M">Marco Coraggio</a>, 
<a href="/search/eess?searchtype=author&query=Russo%2C+G">Giovanni Russo</a>, 
<a href="/search/eess?searchtype=author&query=Musolesi%2C+M">Mirco Musolesi</a>, 
<a href="/search/eess?searchtype=author&query=di+Bernardo%2C+M">Mario di Bernardo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In addressing control problems such as regulation and tracking through
reinforcement learning, it is often required to guarantee that the acquired
policy meets essential performance and stability criteria such as a desired
settling time and steady-state error prior to deployment. Motivated by this
necessity, we present a set of results and a systematic reward shaping
procedure that (i) ensures the optimal policy generates trajectories that align
with specified control requirements and (ii) allows to assess whether any given
policy satisfies them. We validate our approach through comprehensive numerical
experiments conducted in two representative environments from OpenAI Gym: the
Inverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular
and deep reinforcement learning methods, our experiments consistently affirm
the efficacy of our proposed framework, highlighting its effectiveness in
ensuring policy adherence to the prescribed control requirements.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10034" title="Abstract">arXiv:2311.10034</a> [<a href="/pdf/2311.10034" title="Download PDF">pdf</a>, <a href="/format/2311.10034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Match and Locate: low-frequency monocular odometry based on deep feature  matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konev%2C+S">Stepan Konev</a>, 
<a href="/search/cs?searchtype=author&query=Biktairov%2C+Y">Yuriy Biktairov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Accurate and robust pose estimation plays a crucial role in many robotic
systems. Popular algorithms for pose estimation typically rely on high-fidelity
and high-frequency signals from various sensors. Inclusion of these sensors
makes the system less affordable and much more complicated. In this work we
introduce a novel approach for the robotic odometry which only requires a
single camera and, importantly, can produce reliable estimates given even
extremely low-frequency signal of around one frame per second. The approach is
based on matching image features between the consecutive frames of the video
stream using deep feature matching models. The resulting coarse estimate is
then adjusted by a convolutional neural network, which is also responsible for
estimating the scale of the transition, otherwise irretrievable using only the
feature matching information. We evaluate the performance of the approach in
the AISG-SLA Visual Localisation Challenge and find that while being
computationally efficient and easy to implement our method shows competitive
results with only around $3^{\circ}$ of orientation estimation error and $2m$
of translation estimation error taking the third place in the challenge.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10039" title="Abstract">arXiv:2311.10039</a> [<a href="/pdf/2311.10039" title="Download PDF">pdf</a>, <a href="/ps/2311.10039" title="Download PostScript">ps</a>, <a href="/format/2311.10039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Software Dependability Measurement at the Age Of 36
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Binder%2C+R+V">Robert V. Binder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures. Accepted for publication in IEEE Computer, April 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computational Engineering, Finance, and Science (cs.CE); Cryptography and Security (cs.CR); Performance (cs.PF)

</div>
<p class="mathjax">Thirty-six years after the first edition of IEEE standard 982.1, Measures of
the Software Aspects of Dependability, the third edition focuses on the
measurement of in-service software dependability. This article explains how
this new point of view evolved and shaped the third edition's guidance for
software dependability measurement.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10040" title="Abstract">arXiv:2311.10040</a> [<a href="/pdf/2311.10040" title="Download PDF">pdf</a>, <a href="/format/2311.10040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A characterization of efficiently compilable constraint languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berkholz%2C+C">Christoph Berkholz</a>, 
<a href="/search/cs?searchtype=author&query=Mengel%2C+S">Stefan Mengel</a>, 
<a href="/search/cs?searchtype=author&query=Wilhelm%2C+H">Hermann Wilhelm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">A central task in knowledge compilation is to compile a CNF-SAT instance into
a succinct representation format that allows efficient operations such as
testing satisfiability, counting, or enumerating all solutions. Useful
representation formats studied in this area range from ordered binary decision
diagrams (OBDDs) to circuits in decomposable negation normal form (DNNFs).
<br />While it is known that there exist CNF formulas that require exponential size
representations, the situation is less well studied for other types of
constraints than Boolean disjunctive clauses. The constraint satisfaction
problem (CSP) is a powerful framework that generalizes CNF-SAT by allowing
arbitrary sets of constraints over any finite domain. The main goal of our work
is to understand for which type of constraints (also called the constraint
language) it is possible to efficiently compute representations of polynomial
size. We answer this question completely and prove two tight characterizations
of efficiently compilable constraint languages, depending on whether target
format is structured.
<br />We first identify the combinatorial property of ``strong blockwise
decomposability'' and show that if a constraint language has this property, we
can compute DNNF representations of linear size. For all other constraint
languages we construct families of CSP-instances that provably require DNNFs of
exponential size. For a subclass of ``strong uniformly blockwise decomposable''
constraint languages we obtain a similar dichotomy for structured DNNFs. In
fact, strong (uniform) blockwise decomposability even allows efficient
compilation into multi-valued analogs of OBDDs and FBDDs, respectively. Thus,
we get complete characterizations for all knowledge compilation classes between
O(B)DDs and DNNFs.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10041" title="Abstract">arXiv:2311.10041</a> [<a href="/pdf/2311.10041" title="Download PDF">pdf</a>, <a href="/format/2311.10041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Reinforcement Learning for Robotics and Continuous Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paleja%2C+R">Rohan Paleja</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Letian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yaru Niu</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+A">Andrew Silva</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaoxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ritchie%2C+C">Chace Ritchie</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sugju Choi</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kimberlee Chestnut Chang</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+H+E">Hongtei Eric Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nageshrao%2C+S">Subramanya Nageshrao</a>, 
<a href="/search/cs?searchtype=author&query=Gombolay%2C+M">Matthew Gombolay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2202.02352">arXiv:2202.02352</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Interpretability in machine learning is critical for the safe deployment of
learned policies across legally-regulated and safety-critical domains. While
gradient-based approaches in reinforcement learning have achieved tremendous
success in learning policies for continuous control problems such as robotics
and autonomous driving, the lack of interpretability is a fundamental barrier
to adoption. We propose Interpretable Continuous Control Trees (ICCTs), a
tree-based model that can be optimized via modern, gradient-based,
reinforcement learning approaches to produce high-performing, interpretable
policies. The key to our approach is a procedure for allowing direct
optimization in a sparse decision-tree-like representation. We validate ICCTs
against baselines across six domains, showing that ICCTs are capable of
learning policies that parity or outperform baselines by up to 33% in
autonomous driving scenarios while achieving a 300x-600x reduction in the
number of parameters against deep learning baselines. We prove that ICCTs can
serve as universal function approximators and display analytically that ICCTs
can be verified in linear time. Furthermore, we deploy ICCTs in two realistic
driving domains, based on interstate Highway-94 and 280 in the US. Finally, we
verify ICCT's utility with end-users and find that ICCTs are rated easier to
simulate, quicker to validate, and more interpretable than neural networks.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10042" title="Abstract">arXiv:2311.10042</a> [<a href="/pdf/2311.10042" title="Download PDF">pdf</a>, <a href="/format/2311.10042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Depth Insight -- Contribution of Different Features to Indoor  Single-image Depth Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+Y">Yuwen Heng</a>, 
<a href="/search/cs?searchtype=author&query=Niranjan%2C+M">Mahesan Niranjan</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hansung Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Depth estimation from a single image is a challenging problem in computer
vision because binocular disparity or motion information is absent. Whereas
impressive performances have been reported in this area recently using
end-to-end trained deep neural architectures, as to what cues in the images
that are being exploited by these black box systems is hard to know. To this
end, in this work, we quantify the relative contributions of the known cues of
depth in a monocular depth estimation setting using an indoor scene data set.
Our work uses feature extraction techniques to relate the single features of
shape, texture, colour and saturation, taken in isolation, to predict depth. We
find that the shape of objects extracted by edge detection substantially
contributes more than others in the indoor setting considered, while the other
features also have contributions in varying degrees. These insights will help
optimise depth estimation models, boosting their accuracy and robustness. They
promise to broaden the practical applications of vision-based depth estimation.
The project code is attached to the supplementary material and will be
published on GitHub.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10047" title="Abstract">arXiv:2311.10047</a> [<a href="/pdf/2311.10047" title="Download PDF">pdf</a>, <a href="/ps/2311.10047" title="Download PostScript">ps</a>, <a href="/format/2311.10047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frozen Set Design for Precoded Polar Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miloslavskaya%2C+V">Vera Miloslavskaya</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yonghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Vucetic%2C+B">Branka Vucetic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, submitted to IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper focuses on the frozen set design for precoded polar codes decoded
by the successive cancellation list (SCL) algorithm. We propose a novel frozen
set design method, whose computational complexity is low due to the use of
analytical bounds and constrained frozen set structure. We derive new bounds
based on the recently published complexity analysis of SCL with near
maximum-likelihood (ML) performance. To predict the ML performance, we employ
the state-of-the-art bounds relying on the code weight distribution. The bounds
and constrained frozen set structure are incorporated into the genetic
algorithm to generate optimized frozen sets with low complexity. Our simulation
results show that the constructed precoded polar codes of length 512 have a
superior frame error rate (FER) performance compared to the state-of-the-art
codes under SCL decoding with various list sizes.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10049" title="Abstract">arXiv:2311.10049</a> [<a href="/pdf/2311.10049" title="Download PDF">pdf</a>, <a href="/format/2311.10049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inherently Interpretable Time Series Classification via Multiple  Instance Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Early%2C+J">Joseph Early</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+G+K">Gavin KC Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Cutajar%2C+K">Kurt Cutajar</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hanting Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kandola%2C+J">Jas Kandola</a>, 
<a href="/search/cs?searchtype=author&query=Twomey%2C+N">Niall Twomey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under submission at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Conventional Time Series Classification (TSC) methods are often black boxes
that obscure inherent interpretation of their decision-making processes. In
this work, we leverage Multiple Instance Learning (MIL) to overcome this issue,
and propose a new framework called MILLET: Multiple Instance Learning for
Locally Explainable Time series classification. We apply MILLET to existing
deep learning TSC models and show how they become inherently interpretable
without compromising (and in some cases, even improving) predictive
performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel
synthetic dataset that is specially designed to facilitate interpretability
evaluation. On these datasets, we show MILLET produces sparse explanations
quickly that are of higher quality than other well-known interpretability
methods. To the best of our knowledge, our work with MILLET, which is available
on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the
first to develop general MIL methods for TSC and apply them to an extensive
variety of domains
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10050" title="Abstract">arXiv:2311.10050</a> [<a href="/pdf/2311.10050" title="Download PDF">pdf</a>, <a href="/format/2311.10050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph models for Cybersecurity -- A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wachter%2C+J">Jasmin Wachter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Graph models are helpful means of analyzing computer networks as well as
complex system architectures for security. In this paper we evaluate the
current state of research for representing and analysing cyber-attack using
graph models, i.e. attack graph (AG) formalisms. We propose a taxonomy on
attack graph formalisms, based on 70 models, which we analysed with respect to
their \textit{graph semantic}, involved agents and analysis features.
Additionally, we adress which formalisms allow for automatic attack graph
generation from raw or processes data inputs. Our taxonomy is especially
designed to help users and applied researchers identify a suitable AG model for
their needs. A summary of the individual AG formalisms is provided as
supplementary material.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10051" title="Abstract">arXiv:2311.10051</a> [<a href="/pdf/2311.10051" title="Download PDF">pdf</a>, <a href="/format/2311.10051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tabular Few-Shot Generalization Across Heterogeneous Feature Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Max Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Kobalczyk%2C+K">Katarzyna Kobalczyk</a>, 
<a href="/search/cs?searchtype=author&query=Petrovic%2C+A">Andrija Petrovic</a>, 
<a href="/search/cs?searchtype=author&query=Nikolic%2C+M">Mladen Nikolic</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>, 
<a href="/search/cs?searchtype=author&query=Delibasic%2C+B">Boris Delibasic</a>, 
<a href="/search/cs?searchtype=author&query=Lio%2C+P">Petro Lio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tabular learning, Deep learning, Few shot learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Despite the prevalence of tabular datasets, few-shot learning remains
under-explored within this domain. Existing few-shot methods are not directly
applicable to tabular datasets due to varying column relationships, meanings,
and permutational invariance. To address these challenges, we propose FLAT-a
novel approach to tabular few-shot learning, encompassing knowledge sharing
between datasets with heterogeneous feature spaces. Utilizing an encoder
inspired by Dataset2Vec, FLAT learns low-dimensional embeddings of datasets and
their individual columns, which facilitate knowledge transfer and
generalization to previously unseen datasets. A decoder network parametrizes
the predictive target network, implemented as a Graph Attention Network, to
accommodate the heterogeneous nature of tabular datasets. Experiments on a
diverse collection of 118 UCI datasets demonstrate FLAT's successful
generalization to new tabular datasets and a considerable improvement over the
baselines.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10054" title="Abstract">arXiv:2311.10054</a> [<a href="/pdf/2311.10054" title="Download PDF">pdf</a>, <a href="/format/2311.10054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is &quot;A Helpful Assistant&quot; the Best Role for Large Language Models? A  Systematic Evaluation of Social Roles in System Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Mingqian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+J">Jiaxin Pei</a>, 
<a href="/search/cs?searchtype=author&query=Jurgens%2C+D">David Jurgens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Prompting serves as the major way humans interact with Large Language Models
(LLM). Commercial AI systems commonly define the role of the LLM in system
prompts. For example, ChatGPT uses "You are a helpful assistant" as part of the
default system prompt. But is "a helpful assistant" the best role for LLMs? In
this study, we present a systematic evaluation of how social roles in system
prompts affect model performance. We curate a list of 162 roles covering 6
types of interpersonal relationships and 8 types of occupations. Through
extensive analysis of 3 popular LLMs and 2457 questions, we show that adding
interpersonal roles in prompts consistently improves the models' performance
over a range of questions. Moreover, while we find that using gender-neutral
roles and specifying the role as the audience leads to better performances,
predicting which role leads to the best performance remains a challenging task,
and that frequency, similarity, and perplexity do not fully explain the effect
of social roles on model performances. Our results can help inform the design
of system prompts for AI systems. Code and data are available at
https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10055" title="Abstract">arXiv:2311.10055</a> [<a href="/pdf/2311.10055" title="Download PDF">pdf</a>, <a href="/ps/2311.10055" title="Download PostScript">ps</a>, <a href="/format/2311.10055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Minimum Clique Routing Problem on Cycles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Escalante%2C+M">Mariana Escalante</a>, 
<a href="/search/cs?searchtype=author&query=Matamala%2C+M">Mart&#xed;n Matamala</a>, 
<a href="/search/cs?searchtype=author&query=Rapaport%2C+I">Iv&#xe1;n Rapaport</a>, 
<a href="/search/cs?searchtype=author&query=Tolomei%2C+P">Paola Tolomei</a>, 
<a href="/search/cs?searchtype=author&query=Torres%2C+L+M">Luis Miguel Torres</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">In the Minimum Clique Routing Problem on Cycles \textsc{MCRPC} we are given a
cycle together with a set of demands (weighted origin-destination pairs) and
the goal is to route all the pairs minimizing the maximum weighted clique of
the intersection graph induced by the routing. The vertices of this graph are
the demands with their corresponding weights and two demands are adjacent when
their routes share at least one arc. In this work we are not only interested in
the \textsc{MCRPC} but also in two natural subproblems. First, we consider the
situation where the demands are disjoint, in the sense that every two demands
do not share any of their corresponding ends. Second, we analyze the subproblem
where the weights of the routes are all equal. We first show that the problem
is NP-complete even in the subproblem of disjoint demands. For the case of
arbitrary weights, we exhibit a simple combinatorial 2-approximation algorithm
and a $\frac{3}{2}$-approximation algorithm based on rounding a solution of a
relaxation of an integer linear programming formulation of our problem.
Finally, we give a Fixed Parameter Tractable algorithm for the case of uniform
weights, whose parameter is related to the maximum degree of the intersection
graph induced by any routing.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10057" title="Abstract">arXiv:2311.10057</a> [<a href="/pdf/2311.10057" title="Download PDF">pdf</a>, <a href="/format/2311.10057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Song Describer Dataset: a Corpus of Audio Captions for  Music-and-Language Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manco%2C+I">Ilaria Manco</a>, 
<a href="/search/cs?searchtype=author&query=Weck%2C+B">Benno Weck</a>, 
<a href="/search/cs?searchtype=author&query=Doh%2C+S">SeungHeon Doh</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+M">Minz Won</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bodganov%2C+D">Dmitry Bodganov</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yusong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Ke Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tovstogan%2C+P">Philip Tovstogan</a>, 
<a href="/search/cs?searchtype=author&query=Benetos%2C+E">Emmanouil Benetos</a>, 
<a href="/search/cs?searchtype=author&query=Quinton%2C+E">Elio Quinton</a>, 
<a href="/search/cs?searchtype=author&query=Fazekas%2C+G">Gy&#xf6;rgy Fazekas</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+J">Juhan Nam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 Workshop on Machine Learning for Audio
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of
high-quality audio-caption pairs, designed for the evaluation of
music-and-language models. The dataset consists of 1.1k human-written natural
language descriptions of 706 music recordings, all publicly accessible and
released under Creative Common licenses. To showcase the use of our dataset, we
benchmark popular models on three key music-and-language tasks (music
captioning, text-to-music generation and music-language retrieval). Our
experiments highlight the importance of cross-dataset evaluation and offer
insights into how researchers can use SDD to gain a broader understanding of
model performance.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10064" title="Abstract">arXiv:2311.10064</a> [<a href="/pdf/2311.10064" title="Download PDF">pdf</a>, <a href="/ps/2311.10064" title="Download PostScript">ps</a>, <a href="/format/2311.10064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Deviations of Dyadic Lines in Fast Hough Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smirnov%2C+G">Gleb Smirnov</a>, 
<a href="/search/cs?searchtype=author&query=Karpenko%2C+S">Simon Karpenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Probability (math.PR)

</div>
<p class="mathjax">Fast Hough transform is a widely used algorithm in pattern recognition. The
algorithm relies on approximating lines using a specific discrete line model
called dyadic lines. The worst-case deviation of a dyadic line from the ideal
line it used to construct grows as $O(log(n))$, where $n$ is the linear size of
the image. But few lines actually reach the worst-case bound. The present paper
addresses a statistical analysis of the deviation of a dyadic line from its
ideal counterpart. Specifically, our findings show that the mean deviation is
zero, and the variance grows as $O(log(n))$. As $n$ increases, the distribution
of these (suitably normalized) deviations converges towards a normal
distribution with zero mean and a small variance. This limiting result makes an
essential use of ergodic theory.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10065" title="Abstract">arXiv:2311.10065</a> [<a href="/pdf/2311.10065" title="Download PDF">pdf</a>, <a href="/format/2311.10065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Environment Assessment for Safe Autonomous Quadrotor Landing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Secchiero%2C+M">Mattia Secchiero</a>, 
<a href="/search/cs?searchtype=author&query=Bobbili%2C+N">Nishanth Bobbili</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Loianno%2C+G">Giuseppe Loianno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 1 table, submitted to IEEE International Conference on Robotics and Automation (ICRA), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Autonomous identification and evaluation of safe landing zones are of
paramount importance for ensuring the safety and effectiveness of aerial robots
in the event of system failures, low battery, or the successful completion of
specific tasks. In this paper, we present a novel approach for detection and
assessment of potential landing sites for safe quadrotor landing. Our solution
efficiently integrates 2D and 3D environmental information, eliminating the
need for external aids such as GPS and computationally intensive elevation
maps. The proposed pipeline combines semantic data derived from a Neural
Network (NN), to extract environmental features, with geometric data obtained
from a disparity map, to extract critical geometric attributes such as slope,
flatness, and roughness. We define several cost metrics based on these
attributes to evaluate safety, stability, and suitability of regions in the
environments and identify the most suitable landing area. Our approach runs in
real-time on quadrotors equipped with limited computational capabilities.
Experimental results conducted in diverse environments demonstrate that the
proposed method can effectively assess and identify suitable landing areas,
enabling the safe and autonomous landing of a quadrotor.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10075" title="Abstract">arXiv:2311.10075</a> [<a href="/pdf/2311.10075" title="Download PDF">pdf</a>, <a href="/ps/2311.10075" title="Download PostScript">ps</a>, <a href="/format/2311.10075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve  Health Literacy and Communication in Pediatric Populations and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amin%2C+K+S">Kanhai S. Amin</a>, 
<a href="/search/cs?searchtype=author&query=Mayes%2C+L">Linda Mayes</a>, 
<a href="/search/cs?searchtype=author&query=Khosla%2C+P">Pavan Khosla</a>, 
<a href="/search/cs?searchtype=author&query=Doshi%2C+R">Rushabh Doshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 Table, 3 Figures, and 3 Supplemental Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Purpose: Enhanced health literacy has been linked to better health outcomes;
however, few interventions have been studied. We investigate whether large
language models (LLMs) can serve as a medium to improve health literacy in
children and other populations.
<br />Methods: We ran 288 conditions using 26 different prompts through
ChatGPT-3.5, Microsoft Bing, and Google Bard. Given constraints imposed by rate
limits, we tested a subset of 150 conditions through ChatGPT-4. The primary
outcome measurements were the reading grade level (RGL) and word counts of
output.
<br />Results: Across all models, output for basic prompts such as "Explain" and
"What is (are)" were at, or exceeded, a 10th-grade RGL. When prompts were
specified to explain conditions from the 1st to 12th RGL, we found that LLMs
had varying abilities to tailor responses based on RGL. ChatGPT-3.5 provided
responses that ranged from the 7th-grade to college freshmen RGL while
ChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL.
Microsoft Bing provided responses from the 9th to 11th RGL while Google Bard
provided responses from the 7th to 10th RGL.
<br />Discussion: ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade
level outputs. Meanwhile Bard and Bing tended to consistently produce an RGL
that is at the high school level regardless of prompt. Additionally, Bard's
hesitancy in providing certain outputs indicates a cautious approach towards
health information. LLMs demonstrate promise in enhancing health communication,
but future research should verify the accuracy and effectiveness of such tools
in this context.
<br />Implications: LLMs face challenges in crafting outputs below a sixth-grade
reading level. However, their capability to modify outputs above this threshold
provides a potential mechanism to improve health literacy and communication in
a pediatric population and beyond.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10081" title="Abstract">arXiv:2311.10081</a> [<a href="/pdf/2311.10081" title="Download PDF">pdf</a>, <a href="/format/2311.10081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DRESS: Instructing Large Vision-Language Models to Align and Interact  with Humans via Natural Language Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sikka%2C+K">Karan Sikka</a>, 
<a href="/search/cs?searchtype=author&query=Cogswell%2C+M">Michael Cogswell</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Divakaran%2C+A">Ajay Divakaran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The feedback datasets will be released at: <a href="https://huggingface.co/datasets/YangyiYY/LVLM_NLF">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present DRESS, a large vision language model (LVLM) that innovatively
exploits Natural Language feedback (NLF) from Large Language Models to enhance
its alignment and interactions by addressing two key limitations in the
state-of-the-art LVLMs. First, prior LVLMs generally rely only on the
instruction finetuning stage to enhance alignment with human preferences.
Without incorporating extra feedback, they are still prone to generate
unhelpful, hallucinated, or harmful responses. Second, while the visual
instruction tuning data is generally structured in a multi-turn dialogue
format, the connections and dependencies among consecutive conversational turns
are weak. This reduces the capacity for effective multi-turn interactions. To
tackle these, we propose a novel categorization of the NLF into two key types:
critique and refinement. The critique NLF identifies the strengths and
weaknesses of the responses and is used to align the LVLMs with human
preferences. The refinement NLF offers concrete suggestions for improvement and
is adopted to improve the interaction ability of the LVLMs-- which focuses on
LVLMs' ability to refine responses by incorporating feedback in multi-turn
interactions. To address the non-differentiable nature of NLF, we generalize
conditional reinforcement learning for training. Our experimental results
demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and
harmless (21.03%) responses, and more effectively learn from feedback during
multi-turn interactions compared to SOTA LVMLs.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10083" title="Abstract">arXiv:2311.10083</a> [<a href="/pdf/2311.10083" title="Download PDF">pdf</a>, <a href="/ps/2311.10083" title="Download PostScript">ps</a>, <a href="/format/2311.10083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Tradeoffs in Language Model Decoding with Informational  Interpretations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Chung-Ching Chang</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+W+W">William W. Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Yun-Hsuan Sung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose a theoretical framework for formulating language model decoder
algorithms with dynamic programming and information theory. With dynamic
programming, we lift the design of decoder algorithms from the logit space to
the action-state value function space, and show that the decoding algorithms
are consequences of optimizing the action-state value functions. Each component
in the action-state value function space has an information theoretical
interpretation. With the lifting and interpretation, it becomes evident what
the decoder algorithm is optimized for, and hence facilitating the arbitration
of the tradeoffs in sensibleness, diversity, and attribution.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10085" title="Abstract">arXiv:2311.10085</a> [<a href="/pdf/2311.10085" title="Download PDF">pdf</a>, <a href="/format/2311.10085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computationally Efficient Sparsified Online Newton Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Devvrit%2C+F">Fnu Devvrit</a>, 
<a href="/search/cs?searchtype=author&query=Duvvuri%2C+S+S">Sai Surya Duvvuri</a>, 
<a href="/search/cs?searchtype=author&query=Anil%2C+R">Rohan Anil</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vineet Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Dhillon%2C+I">Inderjit Dhillon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages. First two authors contributed equally. Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Optimization and Control (math.OC)

</div>
<p class="mathjax">Second-order methods hold significant promise for enhancing the convergence
of deep neural network training; however, their large memory and computational
demands have limited their practicality. Thus there is a need for scalable
second-order methods that can efficiently train large models. In this paper, we
introduce the Sparsified Online Newton (SONew) method, a memory-efficient
second-order algorithm that yields a sparsified yet effective preconditioner.
The algorithm emerges from a novel use of the LogDet matrix divergence measure;
we combine it with sparsity constraints to minimize regret in the online convex
optimization framework. Empirically, we test our method on large scale
benchmarks of up to 1B parameters. We achieve up to 30% faster convergence,
3.4% relative improvement in validation performance, and 80% relative
improvement in training loss, in comparison to memory efficient optimizers
including first order methods. Powering the method is a surprising fact --
imposing structured sparsity patterns, like tridiagonal and banded structure,
requires little to no overhead, making it as efficient and parallelizable as
first-order methods. In wall-clock time, tridiagonal SONew is only about 3%
slower per step than first-order methods but gives overall gains due to much
faster convergence. In contrast, one of the state-of-the-art (SOTA)
memory-intensive second-order methods, Shampoo, is unable to scale to large
benchmarks. Additionally, while Shampoo necessitates significant engineering
efforts to scale to large benchmarks, SONew offers a more straightforward
implementation, increasing its practical appeal. SONew code is available at:
https://github.com/devvrit/SONew
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10089" title="Abstract">arXiv:2311.10089</a> [<a href="/pdf/2311.10089" title="Download PDF">pdf</a>, <a href="/format/2311.10089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emu Edit: Precise Image Editing via Recognition and Generation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sheynin%2C+S">Shelly Sheynin</a>, 
<a href="/search/cs?searchtype=author&query=Polyak%2C+A">Adam Polyak</a>, 
<a href="/search/cs?searchtype=author&query=Singer%2C+U">Uriel Singer</a>, 
<a href="/search/cs?searchtype=author&query=Kirstain%2C+Y">Yuval Kirstain</a>, 
<a href="/search/cs?searchtype=author&query=Zohar%2C+A">Amit Zohar</a>, 
<a href="/search/cs?searchtype=author&query=Ashual%2C+O">Oron Ashual</a>, 
<a href="/search/cs?searchtype=author&query=Parikh%2C+D">Devi Parikh</a>, 
<a href="/search/cs?searchtype=author&query=Taigman%2C+Y">Yaniv Taigman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Instruction-based image editing holds immense potential for a variety of
applications, as it enables users to perform any editing operation using a
natural language instruction. However, current models in this domain often
struggle with accurately executing user instructions. We present Emu Edit, a
multi-task image editing model which sets state-of-the-art results in
instruction-based image editing. To develop Emu Edit we train it to multi-task
across an unprecedented range of tasks, such as region-based editing, free-form
editing, and Computer Vision tasks, all of which are formulated as generative
tasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we
provide it with learned task embeddings which guide the generation process
towards the correct edit type. Both these elements are essential for Emu Edit's
outstanding performance. Furthermore, we show that Emu Edit can generalize to
new tasks, such as image inpainting, super-resolution, and compositions of
editing tasks, with just a few labeled examples. This capability offers a
significant advantage in scenarios where high-quality samples are scarce.
Lastly, to facilitate a more rigorous and informed assessment of instructable
image editing models, we release a new challenging and versatile benchmark that
includes seven different image editing tasks.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10090" title="Abstract">arXiv:2311.10090</a> [<a href="/pdf/2311.10090" title="Download PDF">pdf</a>, <a href="/format/2311.10090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JaxMARL: Multi-Agent RL Environments in JAX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rutherford%2C+A">Alexander Rutherford</a>, 
<a href="/search/cs?searchtype=author&query=Ellis%2C+B">Benjamin Ellis</a>, 
<a href="/search/cs?searchtype=author&query=Gallici%2C+M">Matteo Gallici</a>, 
<a href="/search/cs?searchtype=author&query=Cook%2C+J">Jonathan Cook</a>, 
<a href="/search/cs?searchtype=author&query=Lupu%2C+A">Andrei Lupu</a>, 
<a href="/search/cs?searchtype=author&query=Ingvarsson%2C+G">Gardar Ingvarsson</a>, 
<a href="/search/cs?searchtype=author&query=Willi%2C+T">Timon Willi</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Akbir Khan</a>, 
<a href="/search/cs?searchtype=author&query=de+Witt%2C+C+S">Christian Schroeder de Witt</a>, 
<a href="/search/cs?searchtype=author&query=Souly%2C+A">Alexandra Souly</a>, 
<a href="/search/cs?searchtype=author&query=Bandyopadhyay%2C+S">Saptarashmi Bandyopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Samvelyan%2C+M">Mikayel Samvelyan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Minqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lange%2C+R+T">Robert Tjarko Lange</a>, 
<a href="/search/cs?searchtype=author&query=Whiteson%2C+S">Shimon Whiteson</a>, 
<a href="/search/cs?searchtype=author&query=Lacerda%2C+B">Bruno Lacerda</a>, 
<a href="/search/cs?searchtype=author&query=Hawes%2C+N">Nick Hawes</a>, 
<a href="/search/cs?searchtype=author&query=Rocktaschel%2C+T">Tim Rocktaschel</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chris Lu</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J+N">Jakob Nicolaus Foerster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Benchmarks play an important role in the development of machine learning
algorithms. For example, research in reinforcement learning (RL) has been
heavily influenced by available environments and benchmarks. However, RL
environments are traditionally run on the CPU, limiting their scalability with
typical academic compute. Recent advancements in JAX have enabled the wider use
of hardware acceleration to overcome these computational hurdles, enabling
massively parallel RL training pipelines and environments. This is particularly
useful for multi-agent reinforcement learning (MARL) research. First of all,
multiple agents must be considered at each environment step, adding
computational burden, and secondly, the sample complexity is increased due to
non-stationarity, decentralised partial observability, or other MARL
challenges. In this paper, we present JaxMARL, the first open-source code base
that combines ease-of-use with GPU enabled efficiency, and supports a large
number of commonly used MARL environments as well as popular baseline
algorithms. When considering wall clock time, our experiments show that per-run
our JAX-based training pipeline is up to 12500x faster than existing
approaches. This enables efficient and thorough evaluations, with the potential
to alleviate the evaluation crisis of the field. We also introduce and
benchmark SMAX, a vectorised, simplified version of the popular StarCraft
Multi-Agent Challenge, which removes the need to run the StarCraft II game
engine. This not only enables GPU acceleration, but also provides a more
flexible MARL environment, unlocking the potential for self-play,
meta-learning, and other future applications in MARL. We provide code at
https://github.com/flairox/jaxmarl.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10091" title="Abstract">arXiv:2311.10091</a> [<a href="/pdf/2311.10091" title="Download PDF">pdf</a>, <a href="/format/2311.10091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Shells for Efficient Neural Radiance Field Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tianchang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Nimier-David%2C+M">Merlin Nimier-David</a>, 
<a href="/search/cs?searchtype=author&query=Sharp%2C+N">Nicholas Sharp</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+A">Alexander Keller</a>, 
<a href="/search/cs?searchtype=author&query=Fidler%2C+S">Sanja Fidler</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+T">Thomas M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Gojcic%2C+Z">Zan Gojcic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGGRAPH Asia 2023. Project page: research.nvidia.com/labs/toronto-ai/adaptive-shells/
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Neural radiance fields achieve unprecedented quality for novel view
synthesis, but their volumetric formulation remains expensive, requiring a huge
number of samples to render high-resolution images. Volumetric encodings are
essential to represent fuzzy geometry such as foliage and hair, and they are
well-suited for stochastic optimization. Yet, many scenes ultimately consist
largely of solid surfaces which can be accurately rendered by a single sample
per pixel. Based on this insight, we propose a neural radiance formulation that
smoothly transitions between volumetric- and surface-based rendering, greatly
accelerating rendering speed and even improving visual fidelity. Our method
constructs an explicit mesh envelope which spatially bounds a neural volumetric
representation. In solid regions, the envelope nearly converges to a surface
and can often be rendered with a single sample. To this end, we generalize the
NeuS formulation with a learned spatially-varying kernel size which encodes the
spread of the density, fitting a wide kernel to volume-like regions and a tight
kernel to surface-like regions. We then extract an explicit mesh of a narrow
band around the surface, with width determined by the kernel size, and
fine-tune the radiance field within this band. At inference time, we cast rays
against the mesh and evaluate the radiance field only within the enclosed
region, greatly reducing the number of samples required. Experiments show that
our approach enables efficient rendering at very high fidelity. We also
demonstrate that the extracted envelope enables downstream applications such as
animation and simulation.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10092" title="Abstract">arXiv:2311.10092</a> [<a href="/pdf/2311.10092" title="Download PDF">pdf</a>, <a href="/format/2311.10092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Traffic Video Object Detection using Motion Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yanqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jing He</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6nlieb%2C+C">Carola-Bibiane Sch&#xf6;nlieb</a>, 
<a href="/search/cs?searchtype=author&query=Aviles-Rivero%2C+A+I">Angelica I Aviles-Rivero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Traffic videos inherently differ from generic videos in their stationary
camera setup, thus providing a strong motion prior where objects often move in
a specific direction over a short time interval. Existing works predominantly
employ generic video object detection framework for traffic video object
detection, which yield certain advantages such as broad applicability and
robustness to diverse scenarios. However, they fail to harness the strength of
motion prior to enhance detection accuracy. In this work, we propose two
innovative methods to exploit the motion prior and boost the performance of
both fully-supervised and semi-supervised traffic video object detection.
Firstly, we introduce a new self-attention module that leverages the motion
prior to guide temporal information integration in the fully-supervised
setting. Secondly, we utilise the motion prior to develop a pseudo-labelling
mechanism to eliminate noisy pseudo labels for the semi-supervised setting.
Both of our motion-prior-centred methods consistently demonstrates superior
performance, outperforming existing state-of-the-art approaches by a margin of
2% in terms of mAP.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10093" title="Abstract">arXiv:2311.10093</a> [<a href="/pdf/2311.10093" title="Download PDF">pdf</a>, <a href="/format/2311.10093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Chosen One: Consistent Characters in Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Avrahami%2C+O">Omri Avrahami</a>, 
<a href="/search/cs?searchtype=author&query=Hertz%2C+A">Amir Hertz</a>, 
<a href="/search/cs?searchtype=author&query=Vinker%2C+Y">Yael Vinker</a>, 
<a href="/search/cs?searchtype=author&query=Arar%2C+M">Moab Arar</a>, 
<a href="/search/cs?searchtype=author&query=Fruchter%2C+S">Shlomi Fruchter</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+O">Ohad Fried</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Or%2C+D">Daniel Cohen-Or</a>, 
<a href="/search/cs?searchtype=author&query=Lischinski%2C+D">Dani Lischinski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page is available at <a href="https://omriavrahami.com/the-chosen-one">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in text-to-image generation models have unlocked vast
potential for visual creativity. However, these models struggle with generation
of consistent characters, a crucial aspect for numerous real-world applications
such as story visualization, game development asset design, advertising, and
more. Current methods typically rely on multiple pre-existing images of the
target character or involve labor-intensive manual processes. In this work, we
propose a fully automated solution for consistent character generation, with
the sole input being a text prompt. We introduce an iterative procedure that,
at each stage, identifies a coherent set of images sharing a similar identity
and extracts a more consistent identity from this set. Our quantitative
analysis demonstrates that our method strikes a better balance between prompt
alignment and identity consistency compared to the baseline methods, and these
findings are reinforced by a user study. To conclude, we showcase several
practical applications of our approach. Project page is available at
https://omriavrahami.com/the-chosen-one
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Fri, 17 Nov 23</h3>
<dl>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.09376" title="Abstract">arXiv:2206.09376</a> (cross-list from quant-ph) [<a href="/pdf/2206.09376" title="Download PDF">pdf</a>, <a href="/format/2206.09376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Encoding High-level Quantum Programs as SZX-diagrams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Borgna%2C+A">Augustin Borgna</a> (Universit&#xe9; de Lorraine, Universit&#xe9; Paris-Saclay), 
<a href="/search/quant-ph?searchtype=author&query=Romero%2C+R">Rafael Romero</a> (Universidad de Buenos Aires, Universidad de la Rep&#xfa;blica-MEC)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 141-169
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO); Programming Languages (cs.PL)

</div>
<p class="mathjax">The Scalable ZX-calculus is a compact graphical language used to reason about
linear maps between quantum states. These diagrams have multiple applications,
but they frequently have to be constructed in a case-by-case basis. In this
work we present a method to encode quantum programs implemented in a fragment
of the linear dependently typed Proto-Quipper-D language as families of
SZX-diagrams. We define a subset of translatable Proto-Quipper-D programs and
show that our procedure is able to encode non-trivial algorithms as diagrams
that grow linearly on the size of the program.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09223" title="Abstract">arXiv:2311.09223</a> (cross-list from eess.IV) [<a href="/pdf/2311.09223" title="Download PDF">pdf</a>, <a href="/format/2311.09223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-line-of-sight imaging in the presence of scattering media using  phasor fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Luesia%2C+P">Pablo Luesia</a>, 
<a href="/search/eess?searchtype=author&query=Crespo%2C+M">Miguel Crespo</a>, 
<a href="/search/eess?searchtype=author&query=Jarabo%2C+A">Adrian Jarabo</a>, 
<a href="/search/eess?searchtype=author&query=Redo-Sanchez%2C+A">Albert Redo-Sanchez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Optics Letters, 2022, vol. 47, no 15, p. 3796-3799
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Non-line-of-sight (NLOS) imaging aims to reconstruct partially or completely
occluded scenes. Recent approaches have demonstrated high-quality
reconstructions of complex scenes with arbitrary reflectance, occlusions, and
significant multi-path effects. However, previous works focused on surface
scattering only, which reduces the generality in more challenging scenarios
such as scenes submerged in scattering media. In this work, we investigate
current state-of-the-art NLOS imaging methods based on phasor fields to
reconstruct scenes submerged in scattering media. We empirically analyze the
capability of phasor fields in reconstructing complex synthetic scenes
submerged in thick scattering media. We also apply the method to real scenes,
showing that it performs similarly to recent diffuse optical tomography
methods.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09229" title="Abstract">arXiv:2311.09229</a> (cross-list from q-bio.NC) [<a href="/pdf/2311.09229" title="Download PDF">pdf</a>, <a href="/format/2311.09229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAPCODRE: A Computational Systems Biology and Machine Learning-Based  Approach to Predict Cognitive Disorder Risk in the Elderly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Mamidala%2C+S">Srilekha Mamidala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">As global life expectancy improves, the population of the elderly, persons
that are aged 65 years and older, is steadily increasing as well. However, with
aging populations a greater prevalence of cognitive impairment has emerged,
ranging from mild dementia to severe dementias such as Alzheimer's disease due
to genetic and environmental influences, among others. The purpose of this
research was to develop a computational algorithm to predict the risk of
developing cognitive disorders using a dual machine learning and systems
biology approach. The proposed method CAPCODRE (Computational Approach to
Predict COgnitive Disorder Risk for the Elderly) utilized air, water, and noise
environmental pollution data coupled with a gene-protein interaction network,
in addition to cognitive impairment hospitalizations in the United States to
create a tailorable, interactive network able to predict risk of dementia and
Alzheimer's disease. This network was inputted into a random selection
optimization algorithm to select optimal training parameters for training via
k-nearest neighbors, random forest regression, and decision trees. CAPCODRE was
successfully able to predict and model risk of cognitive health issues through
measures of specificity, sensitivity, and accuracy of &gt;85%. The algorithm was
integrated into an app for users to receive personalized predictions based on
their medical history and geographic location. CAPCODRE can point to the extent
of environmental pollution on human health and reveal steps to mitigate risk of
severe cognitive impairment. This research also has the potential to address
racial disparities in cognitive disorder diagnoses and treatment, promoting
more equitable and accessible care.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09253" title="Abstract">arXiv:2311.09253</a> (cross-list from eess.IV) [<a href="/pdf/2311.09253" title="Download PDF">pdf</a>, <a href="/format/2311.09253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Perception-Robustness Tradeoff in Deterministic Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ohayon%2C+G">Guy Ohayon</a>, 
<a href="/search/eess?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>, 
<a href="/search/eess?searchtype=author&query=Elad%2C+M">Michael Elad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">We study the behavior of deterministic methods for solving inverse problems
in imaging. These methods are commonly designed to achieve two goals: (1)
attaining high perceptual quality, and (2) generating reconstructions that are
consistent with the measurements. We provide a rigorous proof that the better a
predictor satisfies these two requirements, the larger its Lipschitz constant
must be, regardless of the nature of the degradation involved. In particular,
to approach perfect perceptual quality and perfect consistency, the Lipschitz
constant of the model must grow to infinity. This implies that such methods are
necessarily more susceptible to adversarial attacks. We demonstrate our theory
on single image super-resolution algorithms, addressing both noisy and
noiseless settings. We also show how this undesired behavior can be leveraged
to explore the posterior distribution, thereby allowing the deterministic model
to imitate stochastic methods.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09255" title="Abstract">arXiv:2311.09255</a> (cross-list from econ.TH) [<a href="/pdf/2311.09255" title="Download PDF">pdf</a>, <a href="/format/2311.09255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial intelligence and the skill premium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Bloom%2C+D+E">David E. Bloom</a>, 
<a href="/search/econ?searchtype=author&query=Prettner%2C+K">Klaus Prettner</a>, 
<a href="/search/econ?searchtype=author&query=Saadaoui%2C+J">Jamel Saadaoui</a>, 
<a href="/search/econ?searchtype=author&query=Veruete%2C+M">Mario Veruete</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">What will likely be the effect of the emergence of ChatGPT and other forms of
artificial intelligence (AI) on the skill premium? To address this question, we
develop a nested constant elasticity of substitution production function that
distinguishes between industrial robots and AI. Industrial robots predominantly
substitute for low-skill workers, whereas AI mainly helps to perform the tasks
of high-skill workers. We show that AI reduces the skill premium as long as it
is more substitutable for high-skill workers than low-skill workers are for
high-skill workers.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09261" title="Abstract">arXiv:2311.09261</a> (cross-list from q-bio.QM) [<a href="/pdf/2311.09261" title="Download PDF">pdf</a>, <a href="/format/2311.09261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural  Network with Biomedical Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Y">Yongqi Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Yao%2C+Q">Quanming Yao</a>, 
<a href="/search/q-bio?searchtype=author&query=Yue%2C+L">Ling Yue</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+X">Xian Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Z">Ziheng Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Lin%2C+Z">Zhenxi Lin</a>, 
<a href="/search/q-bio?searchtype=author&query=Zheng%2C+Y">Yefeng Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Nature Computational Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurately predicting drug-drug interactions (DDI) for emerging drugs, which
offer possibilities for treating and alleviating diseases, with computational
methods can improve patient care and contribute to efficient drug development.
However, many existing computational methods require large amounts of known DDI
information, which is scarce for emerging drugs. In this paper, we propose
EmerGNN, a graph neural network (GNN) that can effectively predict interactions
for emerging drugs by leveraging the rich information in biomedical networks.
EmerGNN learns pairwise representations of drugs by extracting the paths
between drug pairs, propagating information from one drug to the other, and
incorporating the relevant biomedical concepts on the paths. The different
edges on the biomedical network are weighted to indicate the relevance for the
target DDI prediction. Overall, EmerGNN has higher accuracy than existing
approaches in predicting interactions for emerging drugs and can identify the
most relevant information on the biomedical network.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09312" title="Abstract">arXiv:2311.09312</a> (cross-list from q-bio.BM) [<a href="/pdf/2311.09312" title="Download PDF">pdf</a>, <a href="/format/2311.09312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> H-Packer: Holographic Rotationally Equivariant Convolutional Neural  Network for Protein Side-Chain Packing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Visani%2C+G+M">Gian Marco Visani</a>, 
<a href="/search/q-bio?searchtype=author&query=Galvin%2C+W">William Galvin</a>, 
<a href="/search/q-bio?searchtype=author&query=Pun%2C+M+N">Michael Neal Pun</a>, 
<a href="/search/q-bio?searchtype=author&query=Nourmohammad%2C+A">Armita Nourmohammad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a conference paper at MLCB 2023. 8 pages main body, 20 pages with appendix. 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurately modeling protein 3D structure is essential for the design of
functional proteins. An important sub-task of structure modeling is protein
side-chain packing: predicting the conformation of side-chains (rotamers) given
the protein's backbone structure and amino-acid sequence. Conventional
approaches for this task rely on expensive sampling procedures over
hand-crafted energy functions and rotamer libraries. Recently, several deep
learning methods have been developed to tackle the problem in a data-driven
way, albeit with vastly different formulations (from image-to-image translation
to directly predicting atomic coordinates). Here, we frame the problem as a
joint regression over the side-chains' true degrees of freedom: the dihedral
$\chi$ angles. We carefully study possible objective functions for this task,
while accounting for the underlying symmetries of the task. We propose
Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain
packing built on top of two light-weight rotationally equivariant neural
networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is
computationally efficient and shows favorable performance against conventional
physics-based algorithms and is competitive against alternative deep learning
solutions.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09354" title="Abstract">arXiv:2311.09354</a> (cross-list from q-bio.QM) [<a href="/pdf/2311.09354" title="Download PDF">pdf</a>, <a href="/ps/2311.09354" title="Download PostScript">ps</a>, <a href="/format/2311.09354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nondestructive, quantitative viability analysis of 3D tissue cultures  using machine learning image segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Trettner%2C+K+J">Kylie J. Trettner</a>, 
<a href="/search/q-bio?searchtype=author&query=Hsieh%2C+J">Jeremy Hsieh</a>, 
<a href="/search/q-bio?searchtype=author&query=Xiao%2C+W">Weikun Xiao</a>, 
<a href="/search/q-bio?searchtype=author&query=Lee%2C+J+S+H">Jerry S.H. Lee</a>, 
<a href="/search/q-bio?searchtype=author&query=Armani%2C+A+M">Andrea M. Armani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 total pages, Main text and SI included, 11 figures, 6 tables, 5 datasets (provided on linked GitHub), linked image files on Zenodo
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Ascertaining the collective viability of cells in different cell culture
conditions has typically relied on averaging colorimetric indicators and is
often reported out in simple binary readouts. Recent research has combined
viability assessment techniques with image-based deep-learning models to
automate the characterization of cellular properties. However, further
development of viability measurements to assess the continuity of possible
cellular states and responses to perturbation across cell culture conditions is
needed. In this work, we demonstrate an image processing algorithm for
quantifying cellular viability in 3D cultures without the need for assay-based
indicators. We show that our algorithm performs similarly to a pair of human
experts in whole-well images over a range of days and culture matrix
compositions. To demonstrate potential utility, we perform a longitudinal study
investigating the impact of a known therapeutic on pancreatic cancer spheroids.
Using images taken with a high content imaging system, the algorithm
successfully tracks viability at the individual spheroid and whole-well level.
The method we propose reduces analysis time by 97% in comparison to the
experts. Because the method is independent of the microscope or imaging system
used, this approach lays the foundation for accelerating progress in and for
improving the robustness and reproducibility of 3D culture analysis across
biological and clinical research.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09365" title="Abstract">arXiv:2311.09365</a> (cross-list from math.OC) [<a href="/pdf/2311.09365" title="Download PDF">pdf</a>, <a href="/format/2311.09365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semidefinite Programming by Projective Cutting Planes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Porumbel%2C+D">Daniel Porumbel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Mathematical Software (cs.MS); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Seeking tighter relaxations of combinatorial optimization problems,
semidefinite programming is a generalization of linear programming that offers
better bounds and is still polynomially solvable. Yet, in practice, a
semidefinite program is still significantly harder to solve than a similar-size
Linear Program (LP). It is well-known that a semidefinite program can be
written as an LP with infinitely-many cuts that could be solved by repeated
separation in a Cutting-Planes scheme; this approach is likely to end up in
failure. We proposed in [Projective Cutting-Planes, Daniel Porumbel, Siam
Journal on Optimization, 2020] the Projective Cutting-Planes method that
upgrades t he well-known separation sub-problem to the projection sub-problem:
given a feasible $y$ inside a polytope $P$ and a direction $d$, find the
maximum $t^*$ so that $y+t^*d\in P$. Using this new sub-problem, one can
generate a sequence of both inner and outer solutions that converge to the
optimum over $P$. This paper shows that the projection sub-problem can be
solved very efficiently in a semidefinite programming context, enabling the
resulting method to compete very well with state-of-the-art semidefinite
optimization software (refined over decades). Results suggest it may the
fastest method for matrix sizes larger than $2000\times 2000$.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09369" title="Abstract">arXiv:2311.09369</a> (cross-list from stat.ML) [<a href="/pdf/2311.09369" title="Download PDF">pdf</a>, <a href="/format/2311.09369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-dependent Probabilistic Generative Models for Disease Progression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zaballa%2C+O">Onintze Zaballa</a>, 
<a href="/search/stat?searchtype=author&query=P%C3%A9rez%2C+A">Aritz P&#xe9;rez</a>, 
<a href="/search/stat?searchtype=author&query=G%C3%B3mez-Inhiesto%2C+E">Elisa G&#xf3;mez-Inhiesto</a>, 
<a href="/search/stat?searchtype=author&query=Acaiturri-Ayesta%2C+T">Teresa Acaiturri-Ayesta</a>, 
<a href="/search/stat?searchtype=author&query=Lozano%2C+J+A">Jose A. Lozano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Electronic health records contain valuable information for monitoring
patients' health trajectories over time. Disease progression models have been
developed to understand the underlying patterns and dynamics of diseases using
these data as sequences. However, analyzing temporal data from EHRs is
challenging due to the variability and irregularities present in medical
records. We propose a Markovian generative model of treatments developed to (i)
model the irregular time intervals between medical events; (ii) classify
treatments into subtypes based on the patient sequence of medical events and
the time intervals between them; and (iii) segment treatments into subsequences
of disease progression patterns. We assume that sequences have an associated
structure of latent variables: a latent class representing the different
subtypes of treatments; and a set of latent stages indicating the phase of
progression of the treatments. We use the Expectation-Maximization algorithm to
learn the model, which is efficiently solved with a dynamic programming-based
method. Various parametric models have been employed to model the time
intervals between medical events during the learning process, including the
geometric, exponential, and Weibull distributions. The results demonstrate the
effectiveness of our model in recovering the underlying model from data and
accurately modeling the irregular time intervals between medical actions.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09375" title="Abstract">arXiv:2311.09375</a> (cross-list from math.OC) [<a href="/pdf/2311.09375" title="Download PDF">pdf</a>, <a href="/format/2311.09375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HypOp: Distributed Constrained Combinatorial Optimization leveraging  Hypergraph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Heydaribeni%2C+N">Nasimeh Heydaribeni</a>, 
<a href="/search/math?searchtype=author&query=Zhan%2C+X">Xinrui Zhan</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+R">Ruisi Zhang</a>, 
<a href="/search/math?searchtype=author&query=Eliassi-Rad%2C+T">Tina Eliassi-Rad</a>, 
<a href="/search/math?searchtype=author&query=Koushanfar%2C+F">Farinaz Koushanfar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Scalable addressing of high dimensional constrained combinatorial
optimization problems is a challenge that arises in several science and
engineering disciplines. Recent work introduced novel application of graph
neural networks for solving polynomial-cost unconstrained combinatorial
optimization problems. This paper proposes a new framework, called HypOp, which
greatly advances the state of the art for solving combinatorial optimization
problems in several aspects: (i) it generalizes the prior results to
constrained optimization problems with an arbitrary cost function; (ii) it
broadens the application to higher dimensional problems by leveraging a
hypergraph neural network structure; (iii) it enables scalability to much
larger problems by introducing a new distributed and parallel architecture for
hypergraph neural network training; (iv) it demonstrates generalizability to
other problem formulations by knowledge transfer from the learned experience of
addressing one set of cost/constraints to another set for the same hypergraph;
(v) it significantly boosts the solution accuracy compared with the prior art
by suggesting a fine-tuning step using simulated annealing; (vi) HypOp shows a
remarkable progress on benchmark examples, with run times improved by up to
fivefold using a combination of fine-tuning and distributed training
techniques. The framework allows addressing a novel set of scientific problems
including hypergraph MaxCut problem, satisfiability problems (3SAT), and
resource allocation. We showcase the application of HypOp in scientific
discovery by solving a hypergraph MaxCut problem on the NDC drug-substance
hypergraph. Through extensive experimentation on a variety of combinatorial
optimization problems, HypOp demonstrates superiority over existing
unsupervised learning-based solvers and generic optimization methods.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09408" title="Abstract">arXiv:2311.09408</a> (cross-list from math.OC) [<a href="/pdf/2311.09408" title="Download PDF">pdf</a>, <a href="/format/2311.09408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Feedback Optimization via Sensitivity Decoupling:  Stability and Sub-optimality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+W">Wenbin Wang</a>, 
<a href="/search/math?searchtype=author&query=He%2C+Z">Zhiyu He</a>, 
<a href="/search/math?searchtype=author&query=Belgioioso%2C+G">Giuseppe Belgioioso</a>, 
<a href="/search/math?searchtype=author&query=Bolognani%2C+S">Saverio Bolognani</a>, 
<a href="/search/math?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Online feedback optimization is a controller design paradigm for optimizing
the steady-state behavior of a dynamical system. It employs an optimization
algorithm as a feedback controller and utilizes real-time measurements to
bypass knowing exact plant dynamics and disturbances. Different from existing
centralized settings, we present a fully decentralized feedback optimization
controller for networked systems to lift the communication burden and improve
scalability. We approximate the overall input-output sensitivity matrix through
its diagonal elements, which capture local model information. For the
closed-loop behavior, we characterize the stability and bound the
sub-optimality due to decentralization. We prove that the proposed
decentralized controller yields solutions that correspond to the Nash
equilibria of a non-cooperative game.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09472" title="Abstract">arXiv:2311.09472</a> (cross-list from physics.soc-ph) [<a href="/pdf/2311.09472" title="Download PDF">pdf</a>, <a href="/ps/2311.09472" title="Download PostScript">ps</a>, <a href="/format/2311.09472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phases of the net-zero energy transition and strategies to achieve it
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Markard%2C+J">Joche Markard</a>, 
<a href="/search/physics?searchtype=author&query=Rosenbloom%2C+D">Daniel Rosenbloom</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in: Araujo, K. (Ed.), Routledge Handbook of Energy Transitions.
  Routledge, New York, pp. 102-123 (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The net-zero energy transition is an extraordinary societal challenge. It
requires a swift, radical and economy wide transformation. With the aim of
informing research and policy, we identify general phases of this transition
and the overarching strategies that may be brought to bear in tackling this
challenge. Drawing from the literature on sustainability transition studies, we
depict the net-zero energy transition as a non-linear, cumulative process that
involves multiple, interdependent transitions in different sectors. Future
emission targets can only be reached if policymaking will play a strong role in
guiding these transitions. To understand the increasing complexity of the
policy challenge, we distinguish four overlapping phases of development:
emergence of low-carbon innovations, transition of a single sector
(electricity), transitions of multiple sectors based on low-carbon electricity,
and transitions in difficult-to-decarbonize sectors. We argue that each phase
comes with new policy challenges on top of the already existing ones. Finally,
we discuss the merits and limitations of five general strategies for
decarbonization: efficiency improvement, low-carbon electrification, low-carbon
fuels, negative emissions and "untapped demand-side approaches." While
electrification has emerged as the dominant strategy, new low-carbon fuels
(e.g., based on hydrogen) but also more radical changes (e.g., substitution of
carbon-intensive products or lifestyle changes) merit further attention.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09491" title="Abstract">arXiv:2311.09491</a> (cross-list from stat.ML) [<a href="/pdf/2311.09491" title="Download PDF">pdf</a>, <a href="/format/2311.09491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial Bayesian Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zammit-Mangion%2C+A">Andrew Zammit-Mangion</a>, 
<a href="/search/stat?searchtype=author&query=Kaminski%2C+M+D">Michael D. Kaminski</a>, 
<a href="/search/stat?searchtype=author&query=Tran%2C+B">Ba-Hien Tran</a>, 
<a href="/search/stat?searchtype=author&query=Filippone%2C+M">Maurizio Filippone</a>, 
<a href="/search/stat?searchtype=author&query=Cressie%2C+N">Noel Cressie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Statistical models for spatial processes play a central role in statistical
analyses of spatial data. Yet, it is the simple, interpretable, and well
understood models that are routinely employed even though, as is revealed
through prior and posterior predictive checks, these can poorly characterise
the spatial heterogeneity in the underlying process of interest. Here, we
propose a new, flexible class of spatial-process models, which we refer to as
spatial Bayesian neural networks (SBNNs). An SBNN leverages the
representational capacity of a Bayesian neural network; it is tailored to a
spatial setting by incorporating a spatial "embedding layer" into the network
and, possibly, spatially-varying network parameters. An SBNN is calibrated by
matching its finite-dimensional distribution at locations on a fine gridding of
space to that of a target process of interest. That process could be easy to
simulate from or we have many realisations from it. We propose several variants
of SBNNs, most of which are able to match the finite-dimensional distribution
of the target process at the selected grid better than conventional BNNs of
similar complexity. We also show that a single SBNN can be used to represent a
variety of spatial processes often used in practice, such as Gaussian processes
and lognormal processes. We briefly discuss the tools that could be used to
make inference with SBNNs, and we conclude with a discussion of their
advantages and limitations.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09503" title="Abstract">arXiv:2311.09503</a> (cross-list from quant-ph) [<a href="/pdf/2311.09503" title="Download PDF">pdf</a>, <a href="/ps/2311.09503" title="Download PostScript">ps</a>, <a href="/format/2311.09503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLTS Hamiltonians and Strongly-Explicit SoS Lower Bounds from Low-Rate  Quantum LDPC Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Golowich%2C+L">Louis Golowich</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kaufman%2C+T">Tali Kaufman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Recent constructions of the first asymptotically good quantum LDPC (qLDPC)
codes led to two breakthroughs in complexity theory: the NLTS (No Low-Energy
Trivial States) theorem (Anshu, Breuckmann, and Nirkhe, STOC'23), and explicit
lower bounds against a linear number of levels of the Sum-of-Squares (SoS)
hierarchy (Hopkins and Lin, FOCS'22).
<br />In this work, we obtain improvements to both of these results using qLDPC
codes of low rate:
<br />- Whereas Anshu et al. only obtained NLTS Hamiltonians from qLDPC codes of
linear dimension, we show the stronger result that qLDPC codes of arbitrarily
small positive dimension yield NLTS Hamiltonians.
<br />- The SoS lower bounds of Hopkins and Lin are only weakly explicit because
they require running Gaussian elimination to find a nontrivial codeword, which
takes polynomial time. We resolve this shortcoming by introducing a new method
of planting a strongly explicit nontrivial codeword in linear-distance qLDPC
codes, which in turn yields strongly explicit SoS lower bounds.
<br />Our "planted" qLDPC codes may be of independent interest, as they provide a
new way of ensuring a qLDPC code has positive dimension without resorting to
parity check counting, and therefore provide more flexibility in the code
construction.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09536" title="Abstract">arXiv:2311.09536</a> (cross-list from physics.soc-ph) [<a href="/pdf/2311.09536" title="Download PDF">pdf</a>, <a href="/format/2311.09536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlation networks: Interdisciplinary approaches beyond thresholding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Masuda%2C+N">Naoki Masuda</a>, 
<a href="/search/physics?searchtype=author&query=Boyd%2C+Z+M">Zachary M. Boyd</a>, 
<a href="/search/physics?searchtype=author&query=Garlaschelli%2C+D">Diego Garlaschelli</a>, 
<a href="/search/physics?searchtype=author&query=Mucha%2C+P+J">Peter J. Mucha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Many empirical networks originate from correlational data, arising in domains
as diverse as psychology, neuroscience, genomics, microbiology, finance, and
climate science. Specialized algorithms and theory have been developed in
different application domains for working with such networks, as well as in
statistics, network science, and computer science, often with limited
communication between practitioners in different fields. This leaves
significant room for cross-pollination across disciplines. A central challenge
is that it is not always clear how to best transform correlation matrix data
into networks for the application at hand, and probably the most widespread
method, i.e., thresholding on the correlation value to create either unweighted
or weighted networks, suffers from multiple problems. In this article, we
review various methods of constructing and analyzing correlation networks,
ranging from thresholding and its improvements to weighted networks,
regularization, dynamic correlation networks, threshold-free approaches, and
more. Finally, we propose and discuss a variety of key open questions currently
confronting this field.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09590" title="Abstract">arXiv:2311.09590</a> (cross-list from eess.IV) [<a href="/pdf/2311.09590" title="Download PDF">pdf</a>, <a href="/format/2311.09590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARformer: An Efficient Metal Artifact Reduction Transformer for Dental  CBCT Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shi%2C+Y">Yuxuan Shi</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jun Xu</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+D">Dinggang Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Cone Beam Computed Tomography (CBCT) plays a key role in dental diagnosis and
surgery. However, the metal teeth implants could bring annoying metal artifacts
during the CBCT imaging process, interfering diagnosis and downstream
processing such as tooth segmentation. In this paper, we develop an efficient
Transformer to perform metal artifacts reduction (MAR) from dental CBCT images.
The proposed MAR Transformer (MARformer) reduces computation complexity in the
multihead self-attention by a new Dimension-Reduced Self-Attention (DRSA)
module, based on that the CBCT images have globally similar structure. A
Patch-wise Perceptive Feed Forward Network (P2FFN) is also proposed to perceive
local image information for fine-grained restoration. Experimental results on
CBCT images with synthetic and real-world metal artifacts show that our
MARformer is efficient and outperforms previous MAR methods and two restoration
Transformers.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09596" title="Abstract">arXiv:2311.09596</a> (cross-list from q-bio.BM) [<a href="/pdf/2311.09596" title="Download PDF">pdf</a>, <a href="/format/2311.09596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Drug Repurposing Hypotheses through the Combination of  Disease-Specific Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Jain%2C+A">Ayush Jain</a>, 
<a href="/search/q-bio?searchtype=author&query=Laure-Charpignon%2C+M">Marie Laure-Charpignon</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+I+Y">Irene Y. Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Philippakis%2C+A">Anthony Philippakis</a>, 
<a href="/search/q-bio?searchtype=author&query=Alaa%2C+A">Ahmed Alaa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The drug development pipeline for a new compound can last 10-20 years and
cost over 10 billion. Drug repurposing offers a more time- and cost-effective
alternative. Computational approaches based on biomedical knowledge graph
representations have recently yielded new drug repurposing hypotheses. In this
study, we present a novel, disease-specific hypergraph representation learning
technique to derive contextual embeddings of biological pathways of various
lengths but that all start at any given drug and all end at the disease of
interest. Further, we extend this method to multi-disease hypergraphs. To
determine the repurposing potential of each of the 1,522 drugs, we derive
drug-specific distributions of cosine similarity values and ultimately consider
the median for ranking. Cosine similarity values are computed between (1) all
biological pathways starting at the considered drug and ending at the disease
of interest and (2) all biological pathways starting at drugs currently
prescribed against that disease and ending at the disease of interest. We
illustrate our approach with Alzheimer's disease (AD) and two of its risk
factors: hypertension (HTN) and type 2 diabetes (T2D). We compare each drug's
rank across four hypergraph settings (single- or multi-disease): AD only, AD +
HTN, AD + T2D, and AD + HTN + T2D. Notably, our framework led to the
identification of two promising drugs whose repurposing potential was
significantly higher in hypergraphs combining two diseases: dapagliflozin
(antidiabetic; moved up, from top 32$\%$ to top 7$\%$, across all considered
drugs) and debrisoquine (antihypertensive; moved up, from top 76$\%$ to top
23$\%$). Our approach serves as a hypothesis generation tool, to be paired with
a validation pipeline relying on laboratory experiments and semi-automated
parsing of the biomedical literature.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09607" title="Abstract">arXiv:2311.09607</a> (cross-list from eess.IV) [<a href="/pdf/2311.09607" title="Download PDF">pdf</a>, <a href="/format/2311.09607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Task Learning Approach for Unified Biometric Estimation from Fetal  Ultrasound Anomaly Scans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qazi%2C+M+A">Mohammad Areeb Qazi</a>, 
<a href="/search/eess?searchtype=author&query=Alam%2C+M+T">Mohammed Talha Alam</a>, 
<a href="/search/eess?searchtype=author&query=Almakky%2C+I">Ibrahim Almakky</a>, 
<a href="/search/eess?searchtype=author&query=Diehl%2C+W+G">Werner Gerhard Diehl</a>, 
<a href="/search/eess?searchtype=author&query=Bricker%2C+L">Leanne Bricker</a>, 
<a href="/search/eess?searchtype=author&query=Yaqub%2C+M">Mohammad Yaqub</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 Pages, 4 Figures, The 4th International Conference on Medical Imaging and Computer-Aided Diagnosis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Precise estimation of fetal biometry parameters from ultrasound images is
vital for evaluating fetal growth, monitoring health, and identifying potential
complications reliably. However, the automated computerized segmentation of the
fetal head, abdomen, and femur from ultrasound images, along with the
subsequent measurement of fetal biometrics, remains challenging. In this work,
we propose a multi-task learning approach to classify the region into head,
abdomen and femur as well as estimate the associated parameters. We were able
to achieve a mean absolute error (MAE) of 1.08 mm on head circumference, 1.44
mm on abdomen circumference and 1.10 mm on femur length with a classification
accuracy of 99.91\% on a dataset of fetal Ultrasound images. To achieve this,
we leverage a weighted joint classification and segmentation loss function to
train a U-Net architecture with an added classification head. The code can be
accessed through
\href{https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans.git}{\texttt{Github}
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09618" title="Abstract">arXiv:2311.09618</a> (cross-list from physics.soc-ph) [<a href="/pdf/2311.09618" title="Download PDF">pdf</a>, <a href="/format/2311.09618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulating Opinion Dynamics with Networks of LLM-based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chuang%2C+Y">Yun-Shiuan Chuang</a>, 
<a href="/search/physics?searchtype=author&query=Goyal%2C+A">Agam Goyal</a>, 
<a href="/search/physics?searchtype=author&query=Harlalka%2C+N">Nikunj Harlalka</a>, 
<a href="/search/physics?searchtype=author&query=Suresh%2C+S">Siddharth Suresh</a>, 
<a href="/search/physics?searchtype=author&query=Hawkins%2C+R">Robert Hawkins</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+S">Sijia Yang</a>, 
<a href="/search/physics?searchtype=author&query=Shah%2C+D">Dhavan Shah</a>, 
<a href="/search/physics?searchtype=author&query=Hu%2C+J">Junjie Hu</a>, 
<a href="/search/physics?searchtype=author&query=Rogers%2C+T+T">Timothy T. Rogers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Accurately simulating human opinion dynamics is crucial for understanding a
variety of societal phenomena, including polarization and the spread of
misinformation. However, the agent-based models (ABMs) commonly used for such
simulations lack fidelity to human behavior. We propose a new approach to
simulating opinion dynamics based on populations of Large Language Models
(LLMs). Our findings reveal a strong inherent bias in LLM agents towards
accurate information, leading to consensus in line with scientific reality.
However, this bias limits the simulation of individuals with resistant views on
issues like climate change. After inducing confirmation bias through prompt
engineering, we observed opinion fragmentation in line with existing
agent-based research. These insights highlight the promise and limitations of
LLM agents in this domain and suggest a path forward: refining LLMs with
real-world discourse to better simulate the evolution of human beliefs.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09623" title="Abstract">arXiv:2311.09623</a> (cross-list from eess.IV) [<a href="/pdf/2311.09623" title="Download PDF">pdf</a>, <a href="/ps/2311.09623" title="Download PostScript">ps</a>, <a href="/format/2311.09623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Apoptosis classification using attention based spatio temporal graph  convolution neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Awasthi%2C+A">Akash Awasthi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Accurate classification of apoptosis plays an important role in cell biology
research. There are many state-of-the-art approaches which use deep CNNs to
perform the apoptosis classification but these approaches do not account for
the cell interaction. Our paper proposes the Attention Graph spatio-temporal
graph convolutional network to classify the cell death based on the target
cells in the video. This method considers the interaction of multiple target
cells at each time stamp. We model the whole video sequence as a set of graphs
and classify the target cell in the video as dead or alive. Our method
encounters both spatial and temporal relationships.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09631" title="Abstract">arXiv:2311.09631</a> (cross-list from quant-ph) [<a href="/pdf/2311.09631" title="Download PDF">pdf</a>, <a href="/format/2311.09631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Pauli Spectrum of QAC0
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Nadimpalli%2C+S">Shivam Nadimpalli</a>, 
<a href="/search/quant-ph?searchtype=author&query=Parham%2C+N">Natalie Parham</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vasconcelos%2C+F">Francisca Vasconcelos</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yuen%2C+H">Henry Yuen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">The circuit class $\mathsf{QAC}^0$ was introduced by Moore (1999) as a model
for constant depth quantum circuits where the gate set includes many-qubit
Toffoli gates. Proving lower bounds against such circuits is a longstanding
challenge in quantum circuit complexity; in particular, showing that
polynomial-size $\mathsf{QAC}^0$ cannot compute the parity function has
remained an open question for over 20 years.
<br />In this work, we identify a notion of the \emph{Pauli spectrum} of
$\mathsf{QAC}^0$ circuits, which can be viewed as the quantum analogue of the
Fourier spectrum of classical $\mathsf{AC}^0$ circuits. We conjecture that the
Pauli spectrum of $\mathsf{QAC}^0$ circuits satisfies \emph{low-degree
concentration}, in analogy to the famous Linial, Nisan, Mansour theorem on the
low-degree Fourier concentration of $\mathsf{AC}^0$ circuits. If true, this
conjecture immediately implies that polynomial-size $\mathsf{QAC}^0$ circuits
cannot compute parity.
<br />We prove this conjecture for the class of depth-$d$, polynomial-size
$\mathsf{QAC}^0$ circuits with at most $n^{O(1/d)}$ auxiliary qubits. We obtain
new circuit lower bounds and learning results as applications: this class of
circuits cannot correctly compute
<br />-- the $n$-bit parity function on more than $(\frac{1}{2} +
2^{-\Omega(n^{1/d})})$-fraction of inputs, and
<br />-- the $n$-bit majority function on more than $(1 -
1/\mathrm{poly}(n))$-fraction of inputs. \end{itemize} Additionally we show
that this class of $\mathsf{QAC}^0$ circuits with limited auxiliary qubits can
be learned with quasipolynomial sample complexity, giving the first learning
result for $\mathsf{QAC}^0$ circuits.
<br />More broadly, our results add evidence that ``Pauli-analytic'' techniques can
be a powerful tool in studying quantum circuits.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09642" title="Abstract">arXiv:2311.09642</a> (cross-list from eess.IV) [<a href="/pdf/2311.09642" title="Download PDF">pdf</a>, <a href="/format/2311.09642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Anomaly Detection for Chest X-Ray Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ni%2C+H">Haoqi Ni</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Ximiao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+M">Min Xu</a>, 
<a href="/search/eess?searchtype=author&query=Lang%2C+N">Ning Lang</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+X">Xiuzhuang Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Chest X-Ray (CXR) examination is a common method for assessing thoracic
diseases in clinical applications. While recent advances in deep learning have
enhanced the significance of visual analysis for CXR anomaly detection, current
methods often miss key cues in anomaly images crucial for identifying disease
regions, as they predominantly rely on unsupervised training with normal
images. This letter focuses on a more practical setup in which few-shot anomaly
images with only image-level labels are available during training. For this
purpose, we propose WSCXR, a weakly supervised anomaly detection framework for
CXR. WSCXR firstly constructs sets of normal and anomaly image features
respectively. It then refines the anomaly image features by eliminating normal
region features through anomaly feature mining, thus fully leveraging the
scarce yet crucial features of diseased areas. Additionally, WSCXR employs a
linear mixing strategy to augment the anomaly features, facilitating the
training of anomaly detector with few-shot anomaly images. Experiments on two
CXR datasets demonstrate the effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09716" title="Abstract">arXiv:2311.09716</a> (cross-list from q-bio.NC) [<a href="/pdf/2311.09716" title="Download PDF">pdf</a>, <a href="/ps/2311.09716" title="Download PostScript">ps</a>, <a href="/format/2311.09716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using the Built-in iPhone Body Tracking System for Neurological Tests:  The Example of Assessing Arm Weakness in Stroke Patients. A Preliminary  Evaluation of Accuracy and Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Lippi%2C+V">Vittorio Lippi</a>, 
<a href="/search/q-bio?searchtype=author&query=Walz%2C+I+D">Isabelle Daniela Walz</a>, 
<a href="/search/q-bio?searchtype=author&query=Heimbach%2C+T">Tobias Heimbach</a>, 
<a href="/search/q-bio?searchtype=author&query=Meier%2C+S">Simone Meier</a>, 
<a href="/search/q-bio?searchtype=author&query=Brich%2C+J">Jochen Brich</a>, 
<a href="/search/q-bio?searchtype=author&query=Haverkamp%2C+C">Christian Haverkamp</a>, 
<a href="/search/q-bio?searchtype=author&query=Maurer%2C+C">Christoph Maurer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages. Presented at ICINCO 2023, november 2023 Rome (Italy)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of ICINCO 2023 - Volume 2, pages 181-188 ISBN:
  978-989-758-670-5; ISSN: 2184-2809
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Timely treatment of stroke is critical to minimize brain damage. Therefore,
efforts are being made to educate the public on detecting stroke symptoms,
e.g., face, arms, and speech test (FAST). In this position paper, we propose to
perform the arm weakness test using the integrated video tracking from an
iPhone - some general tests to assess the tracking quality and discuss
potential critical points. The test has been performed on 4 stroke patients.
The result is compared with the report of the clinician. Although presenting
some limitations, the system proved to be able to detect arm weakness as a
symptom of stroke. We envisage that introducing a portable body tracking system
in such clinical tests will provide advantages in terms of objectivity,
repeatability, and the possibility to record and compare groups of patients.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09740" title="Abstract">arXiv:2311.09740</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2311.09740" title="Download PDF">pdf</a>, <a href="/format/2311.09740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redefining Super-Resolution: Fine-mesh PDE predictions without classical  simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sarkar%2C+R+K">Rajat Kumar Sarkar</a>, 
<a href="/search/physics?searchtype=author&query=Majumdar%2C+R">Ritam Majumdar</a>, 
<a href="/search/physics?searchtype=author&query=Jadhav%2C+V">Vishal Jadhav</a>, 
<a href="/search/physics?searchtype=author&query=Sakhinana%2C+S+S">Sagar Srinivas Sakhinana</a>, 
<a href="/search/physics?searchtype=author&query=Runkana%2C+V">Venkataramana Runkana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Machine Learning and the Physical Sciences Workshop, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">In Computational Fluid Dynamics (CFD), coarse mesh simulations offer
computational efficiency but often lack precision. Applying conventional
super-resolution to these simulations poses a significant challenge due to the
fundamental contrast between downsampling high-resolution images and
authentically emulating low-resolution physics. The former method conserves
more of the underlying physics, surpassing the usual constraints of real-world
scenarios. We propose a novel definition of super-resolution tailored for
PDE-based problems. Instead of simply downsampling from a high-resolution
dataset, we use coarse-grid simulated data as our input and predict fine-grid
simulated outcomes. Employing a physics-infused UNet upscaling method, we
demonstrate its efficacy across various 2D-CFD problems such as discontinuity
detection in Burger's equation, Methane combustion, and fouling in Industrial
heat exchangers. Our method enables the generation of fine-mesh solutions
bypassing traditional simulation, ensuring considerable computational saving
and fidelity to the original ground truth outcomes. Through diverse boundary
conditions during training, we further establish the robustness of our method,
paving the way for its broad applications in engineering and scientific CFD
solvers.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09767" title="Abstract">arXiv:2311.09767</a> (cross-list from physics.optics) [<a href="/pdf/2311.09767" title="Download PDF">pdf</a>, <a href="/format/2311.09767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New advancements, challenges and opportunities of nanophotonics for  neuromorphic computing: A state-of-the-art review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Li%2C+R">Renjie Li</a>, 
<a href="/search/physics?searchtype=author&query=Gong%2C+Y">Yuanhao Gong</a>, 
<a href="/search/physics?searchtype=author&query=Huang%2C+H">Hai Huang</a>, 
<a href="/search/physics?searchtype=author&query=Zhou%2C+Y">Yuze Zhou</a>, 
<a href="/search/physics?searchtype=author&query=Mao%2C+S">Sixuan Mao</a>, 
<a href="/search/physics?searchtype=author&query=Chang-Hasnain%2C+C">Connie Chang-Hasnain</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+Z">Zhaoyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages,17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The expansion of optoelectronic devices on photonic integration platforms has
led to significant growth in the field of photonic computing. Photonic
integrated circuits have facilitated the creation of ultrafast artificial
neural networks, forming the basis for a novel category of information
processing devices. Their application extends to diverse domains such as
medical diagnosis, language models, telecommunications, quantum computing, and
the metaverse, addressing the escalating demands of machine learning and
artificial intelligence (AI). In contrast, conventional electronics faces
challenges in latency, crosstalk, and energy consumption. Neuromorphic
photonics emerges as a compelling solution, featuring sub-nanosecond latencies,
minimal heat dissipation, and high parallelism, expanding the scope of AI and
Optical Neural Networks. This review explores recent advances in integrated
photonic neuromorphic systems, focusing on materials and device engineering
breakthroughs needed to overcome existing challenges. Examining various
technologies in AI accelerators, from traditional optics to PICs, we assess
energy efficiency through operations per joule and compute density in
operations per squared millimeter per second. A comparative analysis highlights
crucial technical aspects, emphasizing nanophotonic components like VCSEL
lasers, optical interconnects, nanocavity resonators, and frequency microcombs.
These components showcase recent breakthroughs in photonic engineering and
materials science, enabling the creation of customized neuromorphic systems for
AI tasks. Despite progress, current technologies face obstacles in achieving
photonic AI accelerators with computing speed and energy efficiencies reaching
the petaOPS range. The review explores potential future approaches in new
devices, fabrication, materials, scalability, and integration to enhance
critical performance metrics.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09822" title="Abstract">arXiv:2311.09822</a> (cross-list from eess.IV) [<a href="/pdf/2311.09822" title="Download PDF">pdf</a>, <a href="/format/2311.09822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAM-E: Mammographic synthetic image generation with diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Montoya-del-Angel%2C+R">Ricardo Montoya-del-Angel</a>, 
<a href="/search/eess?searchtype=author&query=Sam-Millan%2C+K">Karla Sam-Millan</a>, 
<a href="/search/eess?searchtype=author&query=Vilanova%2C+J+C">Joan C Vilanova</a>, 
<a href="/search/eess?searchtype=author&query=Mart%C3%AD%2C+R">Robert Mart&#xed;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages + 2 pages of references, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Generative models are used as an alternative data augmentation technique to
alleviate the data scarcity problem faced in the medical imaging field.
Diffusion models have gathered special attention due to their innovative
generation approach, the high quality of the generated images and their
relatively less complex training process compared with Generative Adversarial
Networks. Still, the implementation of such models in the medical domain
remains at early stages. In this work, we propose exploring the use of
diffusion models for the generation of high quality full-field digital
mammograms using state-of-the-art conditional diffusion pipelines.
Additionally, we propose using stable diffusion models for the inpainting of
synthetic lesions on healthy mammograms. We introduce MAM-E, a pipeline of
generative models for high quality mammography synthesis controlled by a text
prompt and capable of generating synthetic lesions on specific regions of the
breast. Finally, we provide quantitative and qualitative assessment of the
generated images and easy-to-use graphical user interfaces for mammography
synthesis.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09846" title="Abstract">arXiv:2311.09846</a> (cross-list from eess.IV) [<a href="/pdf/2311.09846" title="Download PDF">pdf</a>, <a href="/format/2311.09846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GroupMixer: Patch-based Group Convolutional Neural Network for Breast  Cancer Detection from Histopathological Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Modarres%2C+A">Ardavan Modarres</a>, 
<a href="/search/eess?searchtype=author&query=Esfahani%2C+E+E">Erfan Ebrahim Esfahani</a>, 
<a href="/search/eess?searchtype=author&query=Bahrami%2C+M">Mahsa Bahrami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diagnosis of breast cancer malignancy at the early stages is a crucial step
for controlling its side effects. Histopathological analysis provides a unique
opportunity for malignant breast cancer detection. However, such a task would
be tedious and time-consuming for the histopathologists. Deep Neural Networks
enable us to learn informative features directly from raw histopathological
images without manual feature extraction. Although Convolutional Neural
Networks (CNNs) have been the dominant architectures in the computer vision
realm, Transformer-based architectures have shown promising results in
different computer vision tasks. Although harnessing the capability of
Transformer-based architectures for medical image analysis seems interesting,
these architectures are large, have a significant number of trainable
parameters, and require large datasets to be trained on, which are usually rare
in the medical domain. It has been claimed and empirically proved that at least
part of the superior performance of Transformer-based architectures in Computer
Vision domain originates from patch embedding operation. In this paper, we
borrowed the previously introduced idea of integrating a fully Convolutional
Neural Network architecture with Patch Embedding operation and presented an
efficient CNN architecture for breast cancer malignancy detection from
histopathological images. Despite the number of parameters that is
significantly smaller than other methods, the accuracy performance metrics
achieved 97.65%, 98.92%, 99.21%, and 98.01% for 40x, 100x, 200x, and 400x
magnifications respectively. We took a step forward and modified the
architecture using Group Convolution and Channel Shuffling ideas and reduced
the number of trainable parameters even more with a negligible decline in
performance and achieved 95.42%, 98.16%, 96.05%, and 97.92% accuracy for the
mentioned magnifications respectively.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09875" title="Abstract">arXiv:2311.09875</a> (cross-list from stat.CO) [<a href="/pdf/2311.09875" title="Download PDF">pdf</a>, <a href="/format/2311.09875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbiased and Multilevel Methods for a Class of Diffusions Partially  Observed via Marked Point Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Alvarez%2C+M">Miguel Alvarez</a>, 
<a href="/search/stat?searchtype=author&query=Jasra%2C+A">Ajay Jasra</a>, 
<a href="/search/stat?searchtype=author&query=Ruzayqat%2C+H">Hamza Ruzayqat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Numerical Analysis (math.NA); Methodology (stat.ME)

</div>
<p class="mathjax">In this article we consider the filtering problem associated to partially
observed diffusions, with observations following a marked point process. In the
model, the data form a point process with observation times that have its
intensity driven by a diffusion, with the associated marks also depending upon
the diffusion process. We assume that one must resort to time-discretizing the
diffusion process and develop particle and multilevel particle filters to
recursively approximate the filter. In particular, we prove that our multilevel
particle filter can achieve a mean square error (MSE) of
$\mathcal{O}(\epsilon^2)$ ($\epsilon&gt;0$ and arbitrary) with a cost of
$\mathcal{O}(\epsilon^{-2.5})$ versus using a particle filter which has a cost
of $\mathcal{O}(\epsilon^{-3})$ to achieve the same MSE. We then show how this
methodology can be extended to give unbiased (that is with no
time-discretization error) estimators of the filter, which are proved to have
finite variance and with high-probability have finite cost. Finally, we extend
our methodology to the problem of online static-parameter estimation.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09891" title="Abstract">arXiv:2311.09891</a> (cross-list from cond-mat.other) [<a href="/pdf/2311.09891" title="Download PDF">pdf</a>, <a href="/format/2311.09891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On some elusive aspects of databases hindering AI based discovery: A  case study on superconducting materials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Trezza%2C+G">Giovanni Trezza</a>, 
<a href="/search/cond-mat?searchtype=author&query=Chiavazzo%2C+E">Eliodoro Chiavazzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 3 figures (main), 3 figures (supp info)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Condensed Matter (cond-mat.other)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">It stands to reason that the amount and the quality of big data is of key
importance for setting up accurate AI-driven models. Nonetheless, we believe
there are still critical roadblocks in the inherent generation of databases,
that are often underestimated and poorly discussed in the literature. In our
view, such issues can seriously hinder the AI-based discovery process, even
when high quality, sufficiently large and highly reputable data sources are
available. Here, considering superconducting and thermoelectric materials as
two representative case studies, we specifically discuss three aspects, namely
intrinsically biased sample selection, possible hidden variables, disparate
data age. Importantly, to our knowledge, we suggest and test a first strategy
capable of detecting and quantifying the presence of the intrinsic data bias.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09942" title="Abstract">arXiv:2311.09942</a> (cross-list from eess.IV) [<a href="/pdf/2311.09942" title="Download PDF">pdf</a>, <a href="/format/2311.09942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Transformers: A Leap Forward in Lung Cancer Image Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bechar%2C+A">Amine Bechar</a>, 
<a href="/search/eess?searchtype=author&query=Elmir%2C+Y">Youssef Elmir</a>, 
<a href="/search/eess?searchtype=author&query=Medjoudj%2C+R">Rafik Medjoudj</a>, 
<a href="/search/eess?searchtype=author&query=Himeur%2C+Y">Yassine Himeur</a>, 
<a href="/search/eess?searchtype=author&query=Amira%2C+A">Abbes Amira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, and 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper discusses the role of Transfer Learning (TL) and transformers in
cancer detection based on image analysis. With the enormous evolution of cancer
patients, the identification of cancer cells in a patient's body has emerged as
a trend in the field of Artificial Intelligence (AI). This process involves
analyzing medical images, such as Computed Tomography (CT) scans and Magnetic
Resonance Imaging (MRIs), to identify abnormal growths that may help in cancer
detection. Many techniques and methods have been realized to improve the
quality and performance of cancer classification and detection, such as TL,
which allows the transfer of knowledge from one task to another with the same
task or domain. TL englobes many methods, particularly those used in image
analysis, such as transformers and Convolutional Neural Network (CNN) models
trained on the ImageNet dataset. This paper analyzes and criticizes each method
of TL based on image analysis and compares the results of each method, showing
that transformers have achieved the best results with an accuracy of 97.41% for
colon cancer detection and 94.71% for Histopathological Lung cancer. Future
directions for cancer detection based on image analysis are also discussed.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09952" title="Abstract">arXiv:2311.09952</a> (cross-list from stat.ML) [<a href="/pdf/2311.09952" title="Download PDF">pdf</a>, <a href="/format/2311.09952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-based generative models learn manifold-like structures with  constrained mixing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wenliang%2C+L+K">Li Kevin Wenliang</a>, 
<a href="/search/stat?searchtype=author&query=Moran%2C+B">Ben Moran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2022 Workshop on Score-Based Methods
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">How do score-based generative models (SBMs) learn the data distribution
supported on a low-dimensional manifold? We investigate the score model of a
trained SBM through its linear approximations and subspaces spanned by local
feature vectors. During diffusion as the noise decreases, the local
dimensionality increases and becomes more varied between different sample
sequences. Importantly, we find that the learned vector field mixes samples by
a non-conservative field within the manifold, although it denoises with normal
projections as if there is an energy function in off-manifold directions. At
each noise level, the subspace spanned by the local features overlap with an
effective density function. These observations suggest that SBMs can flexibly
mix samples with the learned score field while carefully maintaining a
manifold-like structure of the data distribution.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09958" title="Abstract">arXiv:2311.09958</a> (cross-list from eess.IV) [<a href="/pdf/2311.09958" title="Download PDF">pdf</a>, <a href="/format/2311.09958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VertDetect: Fully End-to-End 3D Vertebral Instance Segmentation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Klein%2C+G">Geoff Klein</a>, 
<a href="/search/eess?searchtype=author&query=Hardisty%2C+M">Michael Hardisty</a>, 
<a href="/search/eess?searchtype=author&query=Whyne%2C+C">Cari Whyne</a>, 
<a href="/search/eess?searchtype=author&query=Martel%2C+A+L">Anne L. Martel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Vertebral detection and segmentation are critical steps for treatment
planning in spine surgery and radiation therapy. Accurate identification and
segmentation are complicated in imaging that does not include the full spine,
in cases with variations in anatomy (T13 and/or L6 vertebrae), and in the
presence of fracture or hardware. This paper proposes VertDetect, a fully
automated end-to-end 3D vertebral instance segmentation Convolutional Neural
Network (CNN) model to predict vertebral level labels and segmentations for all
vertebrae present in a CT scan. The utilization of a shared CNN backbone
provides the detection and segmentation branches of the network with feature
maps containing both spinal and vertebral level information. A Graph
Convolutional Network (GCN) layer is used to improve vertebral labelling by
using the known structure of the spine. This model achieved a Dice Similarity
Coefficient (DSC) of 0.883 (95% CI, 0.843-0.906) and 0.882 (95% CI,
0.835-0.909) in the VerSe 2019 and 0.868 (95\% CI, 0.834-0.890) and 0.869 (95\%
CI, 0.832-0.891) in the VerSe 2020 public and hidden test sets, respectively.
This model achieved state-of-the-art performance for an end-to-end
architecture, whose design facilitates the extraction of features that can be
subsequently used for downstream tasks.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09966" title="Abstract">arXiv:2311.09966</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2311.09966" title="Download PDF">pdf</a>, <a href="/format/2311.09966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The divergence-free velocity formulation of the consistent Navier-Stokes  Cahn-Hilliard model with non-matching densities, divergence-conforming  discretization, and benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Eikelder%2C+M+t">M. ten Eikelder</a>, 
<a href="/search/physics?searchtype=author&query=Schillinger%2C+D">D. Schillinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint, 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The prototypical diffuse-interface model that describes multi-component flows
is the Navier-Stokes Cahn-Hilliard model (NSCH). Over the last decades many
NSCH models have appeared that claim to describe the same physical phenomena,
yet are distinct from one another. In a recent article [M.F.P. ten Eikelder,
K.G. van der Zee, I. Akkerman, and D. Schillinger, Math. Mod. Meth. Appl. S.
33, pp 175-221, 2023.] we have established a unified framework of virtually all
NSCH models. The framework reveals that there is only a single consistent NSCH
model that naturally emanates from the underlying mixture theory. In the
current article we present, verify and validate this novel consistent NSCH
model by means of numerical simulation. To this purpose we discretize a
divergence-free velocity formulation of the NSCH model using
divergence-conforming isogeometric spaces. We compare computations of our
consistent model to results of existing models from literature. The predictive
capability of the numerical methodology is demonstrated via three-dimensional
computations of a rising bubble and the contraction of a liquid filament that
compare well with experimental data.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09995" title="Abstract">arXiv:2311.09995</a> (cross-list from quant-ph) [<a href="/pdf/2311.09995" title="Download PDF">pdf</a>, <a href="/format/2311.09995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Realistic Runtime Analysis for Quantum Simplex Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ammann%2C+S">Sabrina Ammann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hess%2C+M">Maximilian Hess</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ramacciotti%2C+D">Debora Ramacciotti</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fekete%2C+S+P">S&#xe1;ndor P. Fekete</a>, 
<a href="/search/quant-ph?searchtype=author&query=Goedicke%2C+P+L+A">Paulina L. A. Goedicke</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gross%2C+D">David Gross</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lefterovici%2C+A">Andreea Lefterovici</a>, 
<a href="/search/quant-ph?searchtype=author&query=Osborne%2C+T+J">Tobias J. Osborne</a>, 
<a href="/search/quant-ph?searchtype=author&query=Perk%2C+M">Michael Perk</a>, 
<a href="/search/quant-ph?searchtype=author&query=Rotundo%2C+A">Antonio Rotundo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Skelton%2C+S+E">S. E. Skelton</a>, 
<a href="/search/quant-ph?searchtype=author&query=Stiller%2C+S">Sebastian Stiller</a>, 
<a href="/search/quant-ph?searchtype=author&query=de+Wolff%2C+T">Timo de Wolff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">In recent years, strong expectations have been raised for the possible power
of quantum computing for solving difficult optimization problems, based on
theoretical, asymptotic worst-case bounds. Can we expect this to have
consequences for Linear and Integer Programming when solving instances of
practically relevant size, a fundamental goal of Mathematical Programming,
Operations Research and Algorithm Engineering? Answering this question faces a
crucial impediment: The lack of sufficiently large quantum platforms prevents
performing real-world tests for comparison with classical methods.
<br />In this paper, we present a quantum analog for classical runtime analysis
when solving real-world instances of important optimization problems. To this
end, we measure the expected practical performance of quantum computers by
analyzing the expected gate complexity of a quantum algorithm. The lack of
practical quantum platforms for experimental comparison is addressed by hybrid
benchmarking, in which the algorithm is performed on a classical system,
logging the expected cost of the various subroutines that are employed by the
quantum versions. In particular, we provide an analysis of quantum methods for
Linear Programming, for which recent work has provided asymptotic speedup
through quantum subroutines for the Simplex method. We show that a practical
quantum advantage for realistic problem sizes would require quantum gate
operation times that are considerably below current physical limitations.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09997" title="Abstract">arXiv:2311.09997</a> (cross-list from stat.ML) [<a href="/pdf/2311.09997" title="Download PDF">pdf</a>, <a href="/format/2311.09997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-data Learning for Bayesian Additive Regression Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Goedhart%2C+J+M">Jeroen M. Goedhart</a>, 
<a href="/search/stat?searchtype=author&query=Klausch%2C+T">Thomas Klausch</a>, 
<a href="/search/stat?searchtype=author&query=Janssen%2C+J">Jurriaan Janssen</a>, 
<a href="/search/stat?searchtype=author&query=van+de+Wiel%2C+M+A">Mark A. van de Wiel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 3 Figures, 2 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Medical prediction applications often need to deal with small sample sizes
compared to the number of covariates. Such data pose problems for prediction
and variable selection, especially when the covariate-response relationship is
complicated. To address these challenges, we propose to incorporate co-data,
i.e. external information on the covariates, into Bayesian additive regression
trees (BART), a sum-of-trees prediction model that utilizes priors on the tree
parameters to prevent overfitting. To incorporate co-data, an empirical Bayes
(EB) framework is developed that estimates, assisted by a co-data model, prior
covariate weights in the BART model. The proposed method can handle multiple
types of co-data simultaneously. Furthermore, the proposed EB framework enables
the estimation of the other hyperparameters of BART as well, rendering an
appealing alternative to cross-validation. We show that the method finds
relevant covariates and that it improves prediction compared to default BART in
simulations. If the covariate-response relationship is nonlinear, the method
benefits from the flexibility of BART to outperform regression-based co-data
learners. Finally, the use of co-data enhances prediction in an application to
diffuse large B-cell lymphoma prognosis based on clinical covariates, gene
mutations, DNA translocations, and DNA copy number data.
<br />Keywords: Bayesian additive regression trees; Empirical Bayes; Co-data;
High-dimensional data; Omics; Prediction
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10013" title="Abstract">arXiv:2311.10013</a> (cross-list from math.FA) [<a href="/pdf/2311.10013" title="Download PDF">pdf</a>, <a href="/ps/2311.10013" title="Download PostScript">ps</a>, <a href="/format/2311.10013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Change-of-Measure Method, Block Lewis Weights, and Approximating  Matrix Block Norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Manoj%2C+N+S">Naren Sarayu Manoj</a>, 
<a href="/search/math?searchtype=author&query=Ovsiankin%2C+M">Max Ovsiankin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages. comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Data Structures and Algorithms (cs.DS); Probability (math.PR)

</div>
<p class="mathjax">Given a matrix $\mathbf{A} \in \mathbb{R}^{k \times n}$, a partitioning of
$[k]$ into groups $S_1,\dots,S_m$, an outer norm $p$, and a collection of inner
norms such that either $p \ge 1$ and $p_1,\dots,p_m \ge 2$ or $p_1=\dots=p_m=p
\ge 1/\log n$, we prove that there is a sparse weight vector $\mathbf{\beta}
\in \mathbb{R}^{m}$ such that $\sum_{i=1}^m \beta_i \cdot
\|\mathbf{A}_{S_i}\mathbf{x}\|_{p_i}^p \approx_{1\pm\varepsilon} \sum_{i=1}^m
\|\mathbf{A}_{S_i}\mathbf{x}\|_{p_i}^p$, where the number of nonzero entries of
$\mathbf{\beta}$ is at most $O_{p,p_i}(\varepsilon^{-2}n^{\max(1,p/2)}(\log
n)^2(\log(n/\varepsilon)))$. When $p_1\dots,p_m \ge 2$, this weight vector
arises from an importance sampling procedure based on the block Lewis weights,
a recently proposed generalization of Lewis weights. Additionally, we prove
that there exist efficient algorithms to find the sparse weight vector
$\mathbf{\beta}$ in several important regimes of $p$ and $p_1,\dots,p_m$.
<br />Our main technical contribution is a substantial generalization of the
change-of-measure method that Bourgain, Lindenstrauss, and Milman used to
obtain the analogous result when every group has size $1$. Our generalization
allows one to analyze change of measures beyond those implied by D. Lewis's
original construction, including the measure implied by the block Lewis weights
and natural approximations of this measure.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10023" title="Abstract">arXiv:2311.10023</a> (cross-list from stat.ML) [<a href="/pdf/2311.10023" title="Download PDF">pdf</a>, <a href="/format/2311.10023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Optimization for Network Resource Allocation and Comparison with  Reinforcement Learning Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sid-Ali%2C+A">Ahmed Sid-Ali</a>, 
<a href="/search/stat?searchtype=author&query=Lambadaris%2C+I">Ioannis Lambadaris</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+Y+Q">Yiqiang Q. Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Shaikhet%2C+G">Gennady Shaikhet</a>, 
<a href="/search/stat?searchtype=author&query=Asgharnia%2C+A">Amirhossein Asgharnia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We tackle in this paper an online network resource allocation problem with
job transfers. The network is composed of many servers connected by
communication links. The system operates in discrete time; at each time slot,
the administrator reserves resources at servers for future job requests, and a
cost is incurred for the reservations made. Then, after receptions, the jobs
may be transferred between the servers to best accommodate the demands. This
incurs an additional transport cost. Finally, if a job request cannot be
satisfied, there is a violation that engenders a cost to pay for the blocked
job. We propose a randomized online algorithm based on the exponentially
weighted method. We prove that our algorithm enjoys a sub-linear in time
regret, which indicates that the algorithm is adapting and learning from its
experiences and is becoming more efficient in its decision-making as it
accumulates more data. Moreover, we test the performance of our algorithm on
artificial data and compare it against a reinforcement learning method where we
show that our proposed method outperforms the latter.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10052" title="Abstract">arXiv:2311.10052</a> (cross-list from quant-ph) [<a href="/pdf/2311.10052" title="Download PDF">pdf</a>, <a href="/format/2311.10052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entanglement buffering with two quantum memories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Davies%2C+B">Bethany Davies</a>, 
<a href="/search/quant-ph?searchtype=author&query=I%C3%B1esta%2C+%C3%81+G">&#xc1;lvaro G. I&#xf1;esta</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wehner%2C+S">Stephanie Wehner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Performance (cs.PF)

</div>
<p class="mathjax">Quantum networks crucially rely on the availability of high-quality entangled
pairs of qubits, known as entangled links, distributed across distant nodes.
Maintaining the quality of these links is a challenging task due to the
presence of time-dependent noise, also known as decoherence. Entanglement
purification protocols offer a solution by converting multiple low-quality
entangled states into a smaller number of higher-quality ones. In this work, we
introduce a framework to analyse the performance of entanglement buffering
setups that combine entanglement consumption, decoherence, and entanglement
purification. We propose two key metrics: the availability, which is the
steady-state probability that an entangled link is present, and the average
consumed fidelity, which quantifies the steady-state quality of consumed links.
We then investigate a two-node system, where each node possesses two quantum
memories: one for long-term entanglement storage, and another for entanglement
generation. We model this setup as a continuous-time stochastic process and
derive analytical expressions for the performance metrics. Our findings unveil
a trade-off between the availability and the average consumed fidelity. We also
bound these performance metrics for a buffering system that employs the
well-known bilocal Clifford purification protocols. Importantly, our analysis
demonstrates that, in the presence of noise, consistently purifying the
buffered entanglement increases the average consumed fidelity, even when some
buffered entanglement is discarded due to purification failures.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10053" title="Abstract">arXiv:2311.10053</a> (cross-list from math.OC) [<a href="/pdf/2311.10053" title="Download PDF">pdf</a>, <a href="/format/2311.10053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-optimal Closed-loop Method via Lyapunov Damping for Convex  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Maier%2C+S">Severin Maier</a>, 
<a href="/search/math?searchtype=author&query=Castera%2C+C">Camille Castera</a>, 
<a href="/search/math?searchtype=author&query=Ochs%2C+P">Peter Ochs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS)

</div>
<p class="mathjax">We introduce an autonomous system with closed-loop damping for first-order
convex optimization. While, to this day, optimal rates of convergence are only
achieved by non-autonomous methods via open-loop damping (e.g., Nesterov's
algorithm), we show that our system is the first one featuring a closed-loop
damping while exhibiting a rate arbitrarily close to the optimal one. We do so
by coupling the damping and the speed of convergence of the system via a
well-chosen Lyapunov function. We then derive a practical first-order algorithm
called LYDIA by discretizing our system, and present numerical experiments
supporting our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10063" title="Abstract">arXiv:2311.10063</a> (cross-list from nucl-th) [<a href="/pdf/2311.10063" title="Download PDF">pdf</a>, <a href="/format/2311.10063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FENDL: A library for fusion research and applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nucl-th?searchtype=author&query=Schnabel%2C+G">G. Schnabel</a>, 
<a href="/search/nucl-th?searchtype=author&query=Aldama%2C+D+L">D.L. Aldama</a>, 
<a href="/search/nucl-th?searchtype=author&query=Bohm%2C+T">T. Bohm</a>, 
<a href="/search/nucl-th?searchtype=author&query=Fischer%2C+U">U. Fischer</a>, 
<a href="/search/nucl-th?searchtype=author&query=Kunieda%2C+S">S. Kunieda</a>, 
<a href="/search/nucl-th?searchtype=author&query=Trkov%2C+A">A. Trkov</a>, 
<a href="/search/nucl-th?searchtype=author&query=Konno%2C+C">C. Konno</a>, 
<a href="/search/nucl-th?searchtype=author&query=Capote%2C+R">R. Capote</a>, 
<a href="/search/nucl-th?searchtype=author&query=Koning%2C+A+J">A.J. Koning</a>, 
<a href="/search/nucl-th?searchtype=author&query=Breidokaite%2C+S">S. Breidokaite</a>, 
<a href="/search/nucl-th?searchtype=author&query=Eade%2C+T">T. Eade</a>, 
<a href="/search/nucl-th?searchtype=author&query=Fabbri%2C+M">M. Fabbri</a>, 
<a href="/search/nucl-th?searchtype=author&query=Flammini%2C+D">D. Flammini</a>, 
<a href="/search/nucl-th?searchtype=author&query=Isolan%2C+L">L. Isolan</a>, 
<a href="/search/nucl-th?searchtype=author&query=Kodeli%2C+I">I. Kodeli</a>, 
<a href="/search/nucl-th?searchtype=author&query=Ko%C5%A1%C5%A5%C3%A1l%2C+M">M. Ko&#x161;&#x165;&#xe1;l</a>, 
<a href="/search/nucl-th?searchtype=author&query=Kwon%2C+S">S. Kwon</a>, 
<a href="/search/nucl-th?searchtype=author&query=Laghi%2C+D">D. Laghi</a>, 
<a href="/search/nucl-th?searchtype=author&query=Leichtle%2C+D">D. Leichtle</a>, 
<a href="/search/nucl-th?searchtype=author&query=Nakayama%2C+S">S. Nakayama</a>, 
<a href="/search/nucl-th?searchtype=author&query=Ohta%2C+M">M. Ohta</a>, 
<a href="/search/nucl-th?searchtype=author&query=Packer%2C+L+W">L.W. Packer</a>, 
<a href="/search/nucl-th?searchtype=author&query=Qiu%2C+Y">Y. Qiu</a>, 
<a href="/search/nucl-th?searchtype=author&query=Sato%2C+S">S. Sato</a>, 
<a href="/search/nucl-th?searchtype=author&query=Sawan%2C+M">M. Sawan</a>, 
<a href="/search/nucl-th?searchtype=author&query=Schulc%2C+M">M. Schulc</a>, 
<a href="/search/nucl-th?searchtype=author&query=Stankunas%2C+G">G. Stankunas</a>, 
<a href="/search/nucl-th?searchtype=author&query=Sumini%2C+M">M. Sumini</a>, 
<a href="/search/nucl-th?searchtype=author&query=Valentine%2C+A">A. Valentine</a>, 
<a href="/search/nucl-th?searchtype=author&query=Villari%2C+R">R. Villari</a>, 
<a href="/search/nucl-th?searchtype=author&query=%C5%BDohar%2C+A">A. &#x17d;ohar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 81 pages, 114 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Nuclear Theory (nucl-th)</span>; Digital Libraries (cs.DL); Nuclear Experiment (nucl-ex)

</div>
<p class="mathjax">The Fusion Evaluated Nuclear Data Library (FENDL) is a comprehensive and
validated collection of nuclear cross section data coordinated by the
International Atomic Energy Agency (IAEA) Nuclear Data Section (NDS). FENDL
assembles the best nuclear data for fusion applications selected from available
nuclear data libraries and has been under development for decades. FENDL
contains sub-libraries for incident neutron, proton, and deuteron cross
sections including general purpose and activation files used for particle
transport and nuclide inventory calculations.
<br />We describe the history, selection of evaluations for the various
sub-libraries (neutron, proton, deuteron) with the focus on transport and
reactor dosimetry applications, the processing of the nuclear data for
application codes, and the development of the TENDL-2017 library which is the
currently recommended activation library for FENDL. We briefly describe the
IAEA IRDFF library as the recommended library for dosimetry fusion
applications. We also present work on validation of the neutron sub-library
using a variety of fusion relevant computational and experimental benchmarks. A
variety of cross section libraries are used for the validation work including
FENDL-2.1, FENDL-3.1d, FENDL-3.2, ENDF/B-VIII.0, and JEFF-3.2 with the emphasis
on the FENDL libraries. The results of the experimental validation showed that
the performance of FENDL-3.2b is at least as good and in most cases better than
FENDL-2.1.
<br />Future work will consider improved evaluations developed by the International
Nuclear Data Evaluation Network (INDEN). Additional work will be needed to
investigate differences in gas production in structural materials. Covariance
matrices need to be updated to support the development of fusion technology.
Additional validation work for high-energy neutrons, protons and deuterons, and
the activation library will be needed.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Fri, 17 Nov 23</h3>
<dl>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.10814" title="Abstract">arXiv:2101.10814</a> (replaced) [<a href="/e-print/2101.10814" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spread and defend infection in graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gupta%2C+A+T">Arya Tanmay Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> incomplete work. major revision required
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Discrete Mathematics (cs.DM); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.02710" title="Abstract">arXiv:2102.02710</a> (replaced) [<a href="/pdf/2102.02710" title="Download PDF">pdf</a>, <a href="/format/2102.02710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matching Impatient and Heterogeneous Demand and Supply
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aveklouris%2C+A">Angelos Aveklouris</a>, 
<a href="/search/math?searchtype=author&query=DeValve%2C+L">Levi DeValve</a>, 
<a href="/search/math?searchtype=author&query=Stock%2C+M">Maximiliano Stock</a>, 
<a href="/search/math?searchtype=author&query=Ward%2C+A+R">Amy R. Ward</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Performance (cs.PF); Systems and Control (eess.SY); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.11572" title="Abstract">arXiv:2103.11572</a> (replaced) [<a href="/pdf/2103.11572" title="Download PDF">pdf</a>, <a href="/format/2103.11572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Structured Policy Iteration for Homogeneous Distributed  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alemzadeh%2C+S">Siavash Alemzadeh</a>, 
<a href="/search/eess?searchtype=author&query=Talebi%2C+S">Shahriar Talebi</a>, 
<a href="/search/eess?searchtype=author&query=Mesbahi%2C+M">Mehran Mesbahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> S. Alemzadeh and S. Talebi contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Multiagent Systems (cs.MA); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.12679" title="Abstract">arXiv:2104.12679</a> (replaced) [<a href="/pdf/2104.12679" title="Download PDF">pdf</a>, <a href="/format/2104.12679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Spatial Frequency Based Constraints on Adversarial Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernhard%2C+R">R&#xe9;mi Bernhard</a>, 
<a href="/search/cs?searchtype=author&query=Moellic%2C+P">Pierre-Alain Moellic</a>, 
<a href="/search/cs?searchtype=author&query=Mermillod%2C+M">Martial Mermillod</a>, 
<a href="/search/cs?searchtype=author&query=Bourrier%2C+Y">Yannick Bourrier</a>, 
<a href="/search/cs?searchtype=author&query=Cohendet%2C+R">Romain Cohendet</a>, 
<a href="/search/cs?searchtype=author&query=Solinas%2C+M">Miguel Solinas</a>, 
<a href="/search/cs?searchtype=author&query=Reyboz%2C+M">Marina Reyboz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.03178" title="Abstract">arXiv:2112.03178</a> (replaced) [<a href="/pdf/2112.03178" title="Download PDF">pdf</a>, <a href="/format/2112.03178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Student of Games: A unified learning algorithm for both perfect and  imperfect information games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmid%2C+M">Martin Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Moravcik%2C+M">Matej Moravcik</a>, 
<a href="/search/cs?searchtype=author&query=Burch%2C+N">Neil Burch</a>, 
<a href="/search/cs?searchtype=author&query=Kadlec%2C+R">Rudolf Kadlec</a>, 
<a href="/search/cs?searchtype=author&query=Davidson%2C+J">Josh Davidson</a>, 
<a href="/search/cs?searchtype=author&query=Waugh%2C+K">Kevin Waugh</a>, 
<a href="/search/cs?searchtype=author&query=Bard%2C+N">Nolan Bard</a>, 
<a href="/search/cs?searchtype=author&query=Timbers%2C+F">Finbarr Timbers</a>, 
<a href="/search/cs?searchtype=author&query=Lanctot%2C+M">Marc Lanctot</a>, 
<a href="/search/cs?searchtype=author&query=Holland%2C+G+Z">G. Zacharias Holland</a>, 
<a href="/search/cs?searchtype=author&query=Davoodi%2C+E">Elnaz Davoodi</a>, 
<a href="/search/cs?searchtype=author&query=Christianson%2C+A">Alden Christianson</a>, 
<a href="/search/cs?searchtype=author&query=Bowling%2C+M">Michael Bowling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Science Advances
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Science Advances 9, eadg3256 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.00292" title="Abstract">arXiv:2201.00292</a> (replaced) [<a href="/pdf/2201.00292" title="Download PDF">pdf</a>, <a href="/format/2201.00292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Data Representation for Machine Learning at the Pareto Frontier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+S">Shizhou Xu</a>, 
<a href="/search/stat?searchtype=author&query=Strohmer%2C+T">Thomas Strohmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 62 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.03397" title="Abstract">arXiv:2202.03397</a> (replaced) [<a href="/pdf/2202.03397" title="Download PDF">pdf</a>, <a href="/format/2202.03397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bilevel Optimization with a Lower-level Contraction: Optimal Sample  Complexity without Warm-start
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Grazzi%2C+R">Riccardo Grazzi</a>, 
<a href="/search/stat?searchtype=author&query=Pontil%2C+M">Massimiliano Pontil</a>, 
<a href="/search/stat?searchtype=author&query=Salzo%2C+S">Saverio Salzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Corrected Remark 18 + other small edits. Code at <a href="https://github.com/CSML-IIT-UCL/bioptexps">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Machine Learning Research, volume 24, number 167, pages
  1-37, year 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.05907" title="Abstract">arXiv:2202.05907</a> (replaced) [<a href="/pdf/2202.05907" title="Download PDF">pdf</a>, <a href="/ps/2202.05907" title="Download PostScript">ps</a>, <a href="/format/2202.05907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and perfect sampling of subgraphs and polymer systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blanca%2C+A">Antonio Blanca</a>, 
<a href="/search/cs?searchtype=author&query=Cannon%2C+S">Sarah Cannon</a>, 
<a href="/search/cs?searchtype=author&query=Perkins%2C+W">Will Perkins</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.10115" title="Abstract">arXiv:2202.10115</a> (replaced) [<a href="/pdf/2202.10115" title="Download PDF">pdf</a>, <a href="/format/2202.10115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Smoothing and Thresholding Image Segmentation Framework  with Weighted Anisotropic-Isotropic Total Variation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bui%2C+K">Kevin Bui</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Y">Yifei Lou</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+F">Fredrick Park</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Jack Xin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> final version sent to Springer CAMC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.10367" title="Abstract">arXiv:2202.10367</a> (replaced) [<a href="/pdf/2202.10367" title="Download PDF">pdf</a>, <a href="/ps/2202.10367" title="Download PostScript">ps</a>, <a href="/format/2202.10367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilities of the third type: Statistical Relational Learning and  Reasoning with Relative Frequencies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weitk%C3%A4mper%2C+F">Felix Weitk&#xe4;mper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.13235" title="Abstract">arXiv:2202.13235</a> (replaced) [<a href="/pdf/2202.13235" title="Download PDF">pdf</a>, <a href="/format/2202.13235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A survey of BWT variants for string collections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cenzato%2C+D">Davide Cenzato</a>, 
<a href="/search/cs?searchtype=author&query=Lipt%C3%A1k%2C+Z">Zsuzsanna Lipt&#xe1;k</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.00256" title="Abstract">arXiv:2205.00256</a> (replaced) [<a href="/pdf/2205.00256" title="Download PDF">pdf</a>, <a href="/format/2205.00256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneous Graph Neural Networks using Self-supervised Reciprocally  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huo%2C+C">Cuiying Huo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Dongxiao He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yawen Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Di Jin</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+J">Jianwu Dang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weixiong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pedrycz%2C+W">Witold Pedrycz</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lingfei Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.05357" title="Abstract">arXiv:2205.05357</a> (replaced) [<a href="/pdf/2205.05357" title="Download PDF">pdf</a>, <a href="/format/2205.05357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Status Quo: A Contemporary Survey of Advances and Challenges  in Audio Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuenan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zeyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mengyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.07394" title="Abstract">arXiv:2205.07394</a> (replaced) [<a href="/pdf/2205.07394" title="Download PDF">pdf</a>, <a href="/format/2205.07394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sibyl: Adaptive and Extensible Data Placement in Hybrid Storage Systems  Using Online Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gagandeep Singh</a>, 
<a href="/search/cs?searchtype=author&query=Nadig%2C+R">Rakesh Nadig</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jisung Park</a>, 
<a href="/search/cs?searchtype=author&query=Bera%2C+R">Rahul Bera</a>, 
<a href="/search/cs?searchtype=author&query=Hajinazar%2C+N">Nastaran Hajinazar</a>, 
<a href="/search/cs?searchtype=author&query=Novo%2C+D">David Novo</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Luna%2C+J">Juan G&#xf3;mez-Luna</a>, 
<a href="/search/cs?searchtype=author&query=Stuijk%2C+S">Sander Stuijk</a>, 
<a href="/search/cs?searchtype=author&query=Corporaal%2C+H">Henk Corporaal</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.09048" title="Abstract">arXiv:2205.09048</a> (replaced) [<a href="/pdf/2205.09048" title="Download PDF">pdf</a>, <a href="/format/2205.09048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Contrast Masked Autoencoders Are Powerful Pathological  Representation Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Quan%2C+H">Hao Quan</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xingyu Li</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+W">Weixing Chen</a>, 
<a href="/search/eess?searchtype=author&query=Bai%2C+Q">Qun Bai</a>, 
<a href="/search/eess?searchtype=author&query=Zou%2C+M">Mingchen Zou</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+R">Ruijie Yang</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+T">Tingting Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Qi%2C+R">Ruiqun Qi</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+X">Xinghua Gao</a>, 
<a href="/search/eess?searchtype=author&query=Cui%2C+X">Xiaoyu Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.12683" title="Abstract">arXiv:2205.12683</a> (replaced) [<a href="/pdf/2205.12683" title="Download PDF">pdf</a>, <a href="/format/2205.12683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Fano&#x27;s Inequality in Ensemble Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morishita%2C+T">Terufumi Morishita</a>, 
<a href="/search/cs?searchtype=author&query=Morio%2C+G">Gaku Morio</a>, 
<a href="/search/cs?searchtype=author&query=Horiguchi%2C+S">Shota Horiguchi</a>, 
<a href="/search/cs?searchtype=author&query=Ozaki%2C+H">Hiroaki Ozaki</a>, 
<a href="/search/cs?searchtype=author&query=Nukaga%2C+N">Nobuo Nukaga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.13842" title="Abstract">arXiv:2205.13842</a> (replaced) [<a href="/pdf/2205.13842" title="Download PDF">pdf</a>, <a href="/format/2205.13842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Krylov subspace restarting for matrix Laplace transforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Frommer%2C+A">Andreas Frommer</a>, 
<a href="/search/math?searchtype=author&query=Kahl%2C+K">Karsten Kahl</a>, 
<a href="/search/math?searchtype=author&query=Schweitzer%2C+M">Marcel Schweitzer</a>, 
<a href="/search/math?searchtype=author&query=Tsolakis%2C+M">Manuel Tsolakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.08955" title="Abstract">arXiv:2206.08955</a> (replaced) [<a href="/pdf/2206.08955" title="Download PDF">pdf</a>, <a href="/format/2206.08955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making first order linear logic a generating grammar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Slavnov%2C+S">Sergey Slavnov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Logic in Computer Science (cs.LO); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.09376" title="Abstract">arXiv:2206.09376</a> (replaced) [<a href="/pdf/2206.09376" title="Download PDF">pdf</a>, <a href="/format/2206.09376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Encoding High-level Quantum Programs as SZX-diagrams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Borgna%2C+A">Augustin Borgna</a> (Universit&#xe9; de Lorraine, Universit&#xe9; Paris-Saclay), 
<a href="/search/quant-ph?searchtype=author&query=Romero%2C+R">Rafael Romero</a> (Universidad de Buenos Aires, Universidad de la Rep&#xfa;blica-MEC)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings QPL 2022, <a href="/abs/2311.08375">arXiv:2311.08375</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 394, 2023, pp. 141-169
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.03403" title="Abstract">arXiv:2207.03403</a> (replaced) [<a href="/pdf/2207.03403" title="Download PDF">pdf</a>, <a href="/format/2207.03403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the design and analysis of near-term quantum network protocols using  Markov decision processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Khatri%2C+S">Sumeet Khatri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: 19+37 pages, 17 figures; updated references; minor changes to the structure and presentation; similar to the published version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AVS Quantum Sci. 4, 030501 (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04876" title="Abstract">arXiv:2207.04876</a> (replaced) [<a href="/pdf/2207.04876" title="Download PDF">pdf</a>, <a href="/format/2207.04876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Intrinsic Structures of Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shao-Qun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jia-Yi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jin-Hui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Huan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Bin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhi-Hua Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.10265" title="Abstract">arXiv:2207.10265</a> (replaced) [<a href="/pdf/2207.10265" title="Download PDF">pdf</a>, <a href="/format/2207.10265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FOCUS: Fairness via Agent-Awareness for Federated Learning on  Heterogeneous Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+W">Wenda Chu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chulin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Nourian%2C+A">Arash Nourian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.12560" title="Abstract">arXiv:2207.12560</a> (replaced) [<a href="/pdf/2207.12560" title="Download PDF">pdf</a>, <a href="/format/2207.12560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMLB: an AutoML Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gijsbers%2C+P">Pieter Gijsbers</a>, 
<a href="/search/cs?searchtype=author&query=Bueno%2C+M+L+P">Marcos L. P. Bueno</a>, 
<a href="/search/cs?searchtype=author&query=Coors%2C+S">Stefan Coors</a>, 
<a href="/search/cs?searchtype=author&query=LeDell%2C+E">Erin LeDell</a>, 
<a href="/search/cs?searchtype=author&query=Poirier%2C+S">S&#xe9;bastien Poirier</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+J">Janek Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Bischl%2C+B">Bernd Bischl</a>, 
<a href="/search/cs?searchtype=author&query=Vanschoren%2C+J">Joaquin Vanschoren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> UNDER REVIEW: Revised submission to JMLR, with updated results from June 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09106" title="Abstract">arXiv:2208.09106</a> (replaced) [<a href="/pdf/2208.09106" title="Download PDF">pdf</a>, <a href="/format/2208.09106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Risk-Sensitive Approach to Policy Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Markowitz%2C+J">Jared Markowitz</a>, 
<a href="/search/cs?searchtype=author&query=Gardner%2C+R+W">Ryan W. Gardner</a>, 
<a href="/search/cs?searchtype=author&query=Llorens%2C+A">Ashley Llorens</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+R">Raman Arora</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+I">I-Jeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 13 figures. AAAI 2023 (Special Track on Safe and Robust AI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10161" title="Abstract">arXiv:2208.10161</a> (replaced) [<a href="/pdf/2208.10161" title="Download PDF">pdf</a>, <a href="/format/2208.10161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUDGUARD: Taming Malicious Majorities in Federated Learning using  Privacy-Preserving Byzantine-Robust Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingkai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huanhuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Decouchant%2C+J">J&#xe9;r&#xe9;mie Decouchant</a>, 
<a href="/search/cs?searchtype=author&query=Picek%2C+S">Stjepan Picek</a>, 
<a href="/search/cs?searchtype=author&query=Laoutaris%2C+N">Nikolaos Laoutaris</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kaitai Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.11459" title="Abstract">arXiv:2208.11459</a> (replaced) [<a href="/pdf/2208.11459" title="Download PDF">pdf</a>, <a href="/format/2208.11459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Fault-Tolerant Connectivity Labeling Scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Izumi%2C+T">Taisuke Izumi</a>, 
<a href="/search/cs?searchtype=author&query=Emek%2C+Y">Yuval Emek</a>, 
<a href="/search/cs?searchtype=author&query=Wadayama%2C+T">Tadashi Wadayama</a>, 
<a href="/search/cs?searchtype=author&query=Masuzawa%2C+T">Toshimitsu Masuzawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.01473" title="Abstract">arXiv:2209.01473</a> (replaced) [<a href="/pdf/2209.01473" title="Download PDF">pdf</a>, <a href="/format/2209.01473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-based Analysis and Specification of Functional Requirements and  Tests for Complex Automotive Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wiecher%2C+C">Carsten Wiecher</a>, 
<a href="/search/cs?searchtype=author&query=Mandel%2C+C">Constantin Mandel</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnther%2C+M">Matthias G&#xfc;nther</a>, 
<a href="/search/cs?searchtype=author&query=Fischbach%2C+J">Jannik Fischbach</a>, 
<a href="/search/cs?searchtype=author&query=Greenyer%2C+J">Joel Greenyer</a>, 
<a href="/search/cs?searchtype=author&query=Greinert%2C+M">Matthias Greinert</a>, 
<a href="/search/cs?searchtype=author&query=Wolff%2C+C">Carsten Wolff</a>, 
<a href="/search/cs?searchtype=author&query=Dumitrescu%2C+R">Roman Dumitrescu</a>, 
<a href="/search/cs?searchtype=author&query=Mendez%2C+D">Daniel Mendez</a>, 
<a href="/search/cs?searchtype=author&query=Albers%2C+A">Albert Albers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.04849" title="Abstract">arXiv:2209.04849</a> (replaced) [<a href="/pdf/2209.04849" title="Download PDF">pdf</a>, <a href="/ps/2209.04849" title="Download PostScript">ps</a>, <a href="/format/2209.04849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information content in formal languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burgstaller%2C+B">Bernhard Burgstaller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Content is completely unchanged, but explanatory text is inserted between lemmas, theorems and proofs for better understandability of the paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Mathematical Physics (math-ph); Commutative Algebra (math.AC)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01125" title="Abstract">arXiv:2210.01125</a> (replaced) [<a href="/pdf/2210.01125" title="Download PDF">pdf</a>, <a href="/ps/2210.01125" title="Download PostScript">ps</a>, <a href="/format/2210.01125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep  Reconstruction without Reference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guo%2C+X">Xiaodong Guo</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Longhui Li</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+D">Dingyue Chang</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+P">Peng He</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+P">Peng Feng</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+H">Hengyong Yu</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+W">Weiwen Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TCI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05673" title="Abstract">arXiv:2210.05673</a> (replaced) [<a href="/pdf/2210.05673" title="Download PDF">pdf</a>, <a href="/ps/2210.05673" title="Download PostScript">ps</a>, <a href="/format/2210.05673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Deterioration of Deep Learning Models after Clinical  Deployment: A Case Study with Auto-segmentation for Definitive Prostate  Cancer Radiotherapy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+B">Biling Wang</a>, 
<a href="/search/eess?searchtype=author&query=Dohopolski%2C+M">Michael Dohopolski</a>, 
<a href="/search/eess?searchtype=author&query=Bai%2C+T">Ti Bai</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Junjie Wu</a>, 
<a href="/search/eess?searchtype=author&query=Hannan%2C+R">Raquibul Hannan</a>, 
<a href="/search/eess?searchtype=author&query=Desai%2C+N">Neil Desai</a>, 
<a href="/search/eess?searchtype=author&query=Garant%2C+A">Aurelie Garant</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+D">Daniel Yang</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+D">Dan Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+M">Mu-Han Lin</a>, 
<a href="/search/eess?searchtype=author&query=Timmerman%2C+R">Robert Timmerman</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xinlei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+S">Steve Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13722" title="Abstract">arXiv:2210.13722</a> (replaced) [<a href="/pdf/2210.13722" title="Download PDF">pdf</a>, <a href="/format/2210.13722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Selection of Informative Alternative Relational Query Plans  for Database Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Bhowmick%2C+S+S">Sourav S Bhowmick</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zihao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiangtao Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update the Example on page three; Clarify that non-human-centered characterization can be obtained by changing Algorithm 1 in Section 6.4; Add a link to access our ARENA system on page ten; Add the investigation of usage which can demonstrates educational value of ARENA system in Section 8.2:
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15642" title="Abstract">arXiv:2210.15642</a> (replaced) [<a href="/pdf/2210.15642" title="Download PDF">pdf</a>, <a href="/format/2210.15642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Existential Definability over the Subword Ordering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baumann%2C+P">Pascal Baumann</a>, 
<a href="/search/cs?searchtype=author&query=Ganardi%2C+M">Moses Ganardi</a>, 
<a href="/search/cs?searchtype=author&query=Thinniyam%2C+R+S">Ramanathan S. Thinniyam</a>, 
<a href="/search/cs?searchtype=author&query=Zetzsche%2C+G">Georg Zetzsche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11846" title="Abstract">arXiv:2211.11846</a> (replaced) [<a href="/pdf/2211.11846" title="Download PDF">pdf</a>, <a href="/format/2211.11846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Labeled Nearest Neighbor Search and Metric Spanners via Locality  Sensitive Orderings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Filtser%2C+A">Arnold Filtser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.01679" title="Abstract">arXiv:2212.01679</a> (replaced) [<a href="/pdf/2212.01679" title="Download PDF">pdf</a>, <a href="/format/2212.01679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Tree-Width and Path-Width of Conjunctive Regular Path Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Figueira%2C+D">Diego Figueira</a>, 
<a href="/search/cs?searchtype=author&query=Morvan%2C+R">R&#xe9;mi Morvan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal version submitted to LMCS special issue (v3) of an ICDT'23 paper "Approximation and Semantic Tree-width of Conjunctive Regular Path Queries" (v2). 55 pages and 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Databases (cs.DB); Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03169" title="Abstract">arXiv:2212.03169</a> (replaced) [<a href="/pdf/2212.03169" title="Download PDF">pdf</a>, <a href="/format/2212.03169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Brain-Computer Interfaces Meet the Metaverse: Landscape,  Demonstrator, Trends, Challenges, and Concerns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernal%2C+S+L">Sergio L&#xf3;pez Bernal</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+M+Q">Mario Quiles P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Beltr%C3%A1n%2C+E+T+M">Enrique Tom&#xe1;s Mart&#xed;nez Beltr&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+G+M">Gregorio Mart&#xed;nez P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Celdr%C3%A1n%2C+A+H">Alberto Huertas Celdr&#xe1;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05197" title="Abstract">arXiv:2212.05197</a> (replaced) [<a href="/pdf/2212.05197" title="Download PDF">pdf</a>, <a href="/ps/2212.05197" title="Download PostScript">ps</a>, <a href="/format/2212.05197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Model-Driven Analysis of Resilience of GossipSub to Attacks from  Misbehaving Peers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ankit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=von+Hippel%2C+M">Max von Hippel</a>, 
<a href="/search/cs?searchtype=author&query=Manolios%2C+P">Pete Manolios</a>, 
<a href="/search/cs?searchtype=author&query=Nita-Rotaru%2C+C">Cristina Nita-Rotaru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Security and Privacy 2024 (Oakland)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02386" title="Abstract">arXiv:2301.02386</a> (replaced) [<a href="/pdf/2301.02386" title="Download PDF">pdf</a>, <a href="/format/2301.02386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stochastic ADMM Algorithm for Large-Scale Ptychography with Weighted  Difference of Anisotropic and Isotropic Total Variation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bui%2C+K">Kevin Bui</a>, 
<a href="/search/math?searchtype=author&query=Di%2C+Z">Zichao Di</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> revised and extended proofs in Appendices. Fixed typos, too
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04822" title="Abstract">arXiv:2301.04822</a> (replaced) [<a href="/pdf/2301.04822" title="Download PDF">pdf</a>, <a href="/ps/2301.04822" title="Download PostScript">ps</a>, <a href="/format/2301.04822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private estimation algorithms for stochastic block models and mixture  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Addad%2C+V">Vincent Cohen-Addad</a>, 
<a href="/search/cs?searchtype=author&query=d%27Orsi%2C+T">Tommaso d&#x27;Orsi</a>, 
<a href="/search/cs?searchtype=author&query=Epasto%2C+A">Alessandro Epasto</a>, 
<a href="/search/cs?searchtype=author&query=Imola%2C+J">Jacob Imola</a>, 
<a href="/search/cs?searchtype=author&query=Steurer%2C+D">David Steurer</a>, 
<a href="/search/cs?searchtype=author&query=Tiegel%2C+S">Stefan Tiegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10365" title="Abstract">arXiv:2301.10365</a> (replaced) [<a href="/pdf/2301.10365" title="Download PDF">pdf</a>, <a href="/format/2301.10365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Consistent Deep Rigid MRI Motion Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Singh%2C+N+M">Nalini M. Singh</a>, 
<a href="/search/eess?searchtype=author&query=Dey%2C+N">Neel Dey</a>, 
<a href="/search/eess?searchtype=author&query=Hoffmann%2C+M">Malte Hoffmann</a>, 
<a href="/search/eess?searchtype=author&query=Fischl%2C+B">Bruce Fischl</a>, 
<a href="/search/eess?searchtype=author&query=Adalsteinsson%2C+E">Elfar Adalsteinsson</a>, 
<a href="/search/eess?searchtype=author&query=Frost%2C+R">Robert Frost</a>, 
<a href="/search/eess?searchtype=author&query=Dalca%2C+A+V">Adrian V. Dalca</a>, 
<a href="/search/eess?searchtype=author&query=Golland%2C+P">Polina Golland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at MIDL 2023. 14 pages, 6 figures. Keywords: motion correction, magnetic resonance imaging, deep learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12618" title="Abstract">arXiv:2301.12618</a> (replaced) [<a href="/pdf/2301.12618" title="Download PDF">pdf</a>, <a href="/format/2301.12618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junguang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baixu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Junwei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Ximei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dapeng%2C+L">Liu Dapeng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jie Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12954" title="Abstract">arXiv:2301.12954</a> (replaced) [<a href="/pdf/2301.12954" title="Download PDF">pdf</a>, <a href="/format/2301.12954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Double-Edged Sword of Diversity: How Diversity, Conflict, and  Psychological Safety Impact Software Teams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verwijs%2C+C">Christiaan Verwijs</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+D">Daniel Russo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00903" title="Abstract">arXiv:2302.00903</a> (replaced) [<a href="/pdf/2302.00903" title="Download PDF">pdf</a>, <a href="/format/2302.00903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No One Left Behind: Real-World Federated Class-Incremental Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiahua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongliu Li</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+Y">Yang Cong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Gan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence 2023 (TPAMI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03667" title="Abstract">arXiv:2302.03667</a> (replaced) [<a href="/pdf/2302.03667" title="Download PDF">pdf</a>, <a href="/ps/2302.03667" title="Download PostScript">ps</a>, <a href="/format/2302.03667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Random Dictator Is All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arieli%2C+I">Itai Arieli</a>, 
<a href="/search/cs?searchtype=author&query=Babichenko%2C+Y">Yakov Babichenko</a>, 
<a href="/search/cs?searchtype=author&query=Talgam-Cohen%2C+I">Inbal Talgam-Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Zabarnyi%2C+K">Konstantin Zabarnyi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05425" title="Abstract">arXiv:2302.05425</a> (replaced) [<a href="/pdf/2302.05425" title="Download PDF">pdf</a>, <a href="/format/2302.05425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Based Object Tracking in Walking Droplet and Granular  Intruder Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kara%2C+E">Erdi Kara</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">George Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J+J">Joseph J. Williams</a>, 
<a href="/search/cs?searchtype=author&query=Ferrandez-Quinto%2C+G">Gonzalo Ferrandez-Quinto</a>, 
<a href="/search/cs?searchtype=author&query=Rhoden%2C+L+J">Leviticus J. Rhoden</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Maximilian Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kutz%2C+J+N">J. Nathan Kutz</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+A">Aminur Rahman</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Real-Time Image Processing, Vol. 20, Art. No. 86, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07661" title="Abstract">arXiv:2302.07661</a> (replaced) [<a href="/pdf/2302.07661" title="Download PDF">pdf</a>, <a href="/format/2302.07661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D  Panoramic Color Images from LiDAR Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cortinhal%2C+T">Tiago Cortinhal</a>, 
<a href="/search/cs?searchtype=author&query=Aksoy%2C+E+E">Eren Erdal Aksoy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14838" title="Abstract">arXiv:2302.14838</a> (replaced) [<a href="/pdf/2302.14838" title="Download PDF">pdf</a>, <a href="/format/2302.14838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvoPrompting: Language Models for Code-Level Neural Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dohan%2C+D+M">David M. Dohan</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+D+R">David R. So</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00783" title="Abstract">arXiv:2303.00783</a> (replaced) [<a href="/pdf/2303.00783" title="Download PDF">pdf</a>, <a href="/format/2303.00783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Examples Exist in Two-Layer ReLU Networks for Low  Dimensional Linear Subspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Melamed%2C+O">Odelia Melamed</a>, 
<a href="/search/cs?searchtype=author&query=Yehudai%2C+G">Gilad Yehudai</a>, 
<a href="/search/cs?searchtype=author&query=Vardi%2C+G">Gal Vardi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready version for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02575" title="Abstract">arXiv:2303.02575</a> (replaced) [<a href="/pdf/2303.02575" title="Download PDF">pdf</a>, <a href="/format/2303.02575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MITFAS: Mutual Information based Temporal Feature Alignment and Sampling  for Aerial Video Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xian%2C+R">Ruiqi Xian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03151" title="Abstract">arXiv:2303.03151</a> (replaced) [<a href="/pdf/2303.03151" title="Download PDF">pdf</a>, <a href="/format/2303.03151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource-aware Cyber Deception in Cloud-Native Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zambianco%2C+M">Marco Zambianco</a>, 
<a href="/search/cs?searchtype=author&query=Facchinetti%2C+C">Claudio Facchinetti</a>, 
<a href="/search/cs?searchtype=author&query=Doriguzzi-Corin%2C+R">Roberto Doriguzzi-Corin</a>, 
<a href="/search/cs?searchtype=author&query=Siracusa%2C+D">Domenico Siracusa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03290" title="Abstract">arXiv:2303.03290</a> (replaced) [<a href="/pdf/2303.03290" title="Download PDF">pdf</a>, <a href="/ps/2303.03290" title="Download PostScript">ps</a>, <a href="/format/2303.03290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AmQA: Amharic Question Answering Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abedissa%2C+T">Tilahun Abedissa</a>, 
<a href="/search/cs?searchtype=author&query=Usbeck%2C+R">Ricardo Usbeck</a>, 
<a href="/search/cs?searchtype=author&query=Assabie%2C+Y">Yaregal Assabie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03608" title="Abstract">arXiv:2303.03608</a> (replaced) [<a href="/pdf/2303.03608" title="Download PDF">pdf</a>, <a href="/format/2303.03608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Interpretable and Efficient Automatic Reference-Based  Summarization Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fabbri%2C+A+R">Alexander R. Fabbri</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chien-Sheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+D">Dragomir Radev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Camera Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04749" title="Abstract">arXiv:2303.04749</a> (replaced) [<a href="/pdf/2303.04749" title="Download PDF">pdf</a>, <a href="/ps/2303.04749" title="Download PostScript">ps</a>, <a href="/format/2303.04749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Robust Backward Reachable Sets for Set-Theoretic Model  Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Attar%2C+M">Mehran Attar</a>, 
<a href="/search/eess?searchtype=author&query=Lucia%2C+W">Walter Lucia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint jointly submitted to IEEE Control Systems Letters (L-CSS) and IEEE Conference on Decision and Control (CDC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06817" title="Abstract">arXiv:2303.06817</a> (replaced) [<a href="/pdf/2303.06817" title="Download PDF">pdf</a>, <a href="/format/2303.06817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformation-Invariant Network for Few-Shot Object Detection in Remote  Sensing Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nanqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Celik%2C+T">Turgay Celik</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Z">Zongxin Gan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Heng-Chao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TGRS. Modified some errors from the previous version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08691" title="Abstract">arXiv:2303.08691</a> (replaced) [<a href="/pdf/2303.08691" title="Download PDF">pdf</a>, <a href="/format/2303.08691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Reconstruct Signals From Binary Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tachella%2C+J">Juli&#xe1;n Tachella</a>, 
<a href="/search/eess?searchtype=author&query=Jacques%2C+L">Laurent Jacques</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://openreview.net/forum?id=ioFIAQOBOS">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> TMLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09986" title="Abstract">arXiv:2303.09986</a> (replaced) [<a href="/pdf/2303.09986" title="Download PDF">pdf</a>, <a href="/format/2303.09986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards AI-controlled FES-restoration of movements: Learning cycling  stimulation pattern with reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wannawas%2C+N">Nat Wannawas</a>, 
<a href="/search/cs?searchtype=author&query=Faisal%2C+A+A">A. Aldo Faisal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07185" title="Abstract">arXiv:2304.07185</a> (replaced) [<a href="/pdf/2304.07185" title="Download PDF">pdf</a>, <a href="/ps/2304.07185" title="Download PostScript">ps</a>, <a href="/format/2304.07185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounded Poincar&#xe9; operators for twisted and BGG complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=%C4%8Cap%2C+A">Andreas &#x10c;ap</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+K">Kaibo Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published in J. Math. Pures Appl. DOI: 10.1016/j.matpur.2023.09.008
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Functional Analysis (math.FA)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07438" title="Abstract">arXiv:2304.07438</a> (replaced) [<a href="/pdf/2304.07438" title="Download PDF">pdf</a>, <a href="/format/2304.07438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tractable Control for Autoregressive Language Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Honghua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+M">Meihua Dang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Broeck%2C+G">Guy Van den Broeck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12470" title="Abstract">arXiv:2304.12470</a> (replaced) [<a href="/pdf/2304.12470" title="Download PDF">pdf</a>, <a href="/format/2304.12470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-based Estimation of Fatigue and Engagement in Cognitive Training  Sessions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Turnbull%2C+A">Adam Turnbull</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yunlong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Heffner%2C+K">Kathi Heffner</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F+V">Feng Vankee Lin</a>, 
<a href="/search/cs?searchtype=author&query=Adeli%2C+E">Ehsan Adeli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02547" title="Abstract">arXiv:2305.02547</a> (replaced) [<a href="/pdf/2305.02547" title="Download PDF">pdf</a>, <a href="/format/2305.02547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PersonaLLM: Investigating the Ability of Large Language Models to  Express Big Five Personality Traits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiajie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xubo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kabbara%2C+J">Jad Kabbara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First version uploaded at IC2S2 in May 2023. Full paper submitted in November 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02816" title="Abstract">arXiv:2305.02816</a> (replaced) [<a href="/pdf/2305.02816" title="Download PDF">pdf</a>, <a href="/ps/2305.02816" title="Download PostScript">ps</a>, <a href="/format/2305.02816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shannon meets Gray: Noise-robust, Low-sensitivity Codes with  Applications in Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lolck%2C+D+R">David Rasmussen Lolck</a>, 
<a href="/search/cs?searchtype=author&query=Pagh%2C+R">Rasmus Pagh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05400" title="Abstract">arXiv:2305.05400</a> (replaced) [<a href="/pdf/2305.05400" title="Download PDF">pdf</a>, <a href="/format/2305.05400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Corruption Robustness of Image Classifiers with Random  Lp-norm Corruptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siedel%2C+G">Georg Siedel</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Weijia Shao</a>, 
<a href="/search/cs?searchtype=author&query=Vock%2C+S">Silvia Vock</a>, 
<a href="/search/cs?searchtype=author&query=Morozov%2C+A">Andrey Morozov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to VISAPP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05411" title="Abstract">arXiv:2305.05411</a> (replaced) [<a href="/pdf/2305.05411" title="Download PDF">pdf</a>, <a href="/ps/2305.05411" title="Download PostScript">ps</a>, <a href="/format/2305.05411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4/3-Approximation of Graphic TSP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%87ivril%2C+A">Ali &#xc7;ivril</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, further simplification of the algorithm, now proving an upper bound on the integrality gap of the LP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06123" title="Abstract">arXiv:2305.06123</a> (replaced) [<a href="/pdf/2305.06123" title="Download PDF">pdf</a>, <a href="/format/2305.06123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let It TEE: Asynchronous Byzantine Atomic Broadcast with $n \geq 2f+1$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leinweber%2C+M">Marc Leinweber</a>, 
<a href="/search/cs?searchtype=author&query=Hartenstein%2C+H">Hannes Hartenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06626" title="Abstract">arXiv:2305.06626</a> (replaced) [<a href="/pdf/2305.06626" title="Download PDF">pdf</a>, <a href="/format/2305.06626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When the Majority is Wrong: Modeling Annotator Disagreement for  Subjective Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fleisig%2C+E">Eve Fleisig</a>, 
<a href="/search/cs?searchtype=author&query=Abebe%2C+R">Rediet Abebe</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11860" title="Abstract">arXiv:2305.11860</a> (replaced) [<a href="/pdf/2305.11860" title="Download PDF">pdf</a>, <a href="/format/2305.11860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let&#x27;s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning  and Coding with LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+P">Pranjal Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+A">Aman Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12025" title="Abstract">arXiv:2305.12025</a> (replaced) [<a href="/pdf/2305.12025" title="Download PDF">pdf</a>, <a href="/format/2305.12025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Biomembrane-based Memcapacitive Reservoir Computing System for Energy  Efficient Temporal Data Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+M+R">Md Razuan Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+A+S">Ahmed Salah Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Armendarez%2C+N+X">Nicholas Xavier Armendarez</a>, 
<a href="/search/cs?searchtype=author&query=Najem%2C+J+S">Joseph S. Najem</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+S">Md Sakib Hasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplementary information is attached under the main text
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14239" title="Abstract">arXiv:2305.14239</a> (replaced) [<a href="/pdf/2305.14239" title="Download PDF">pdf</a>, <a href="/format/2305.14239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Learning to Summarize with Large Language Models as References
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+K">Kejian Shi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K+S">Katherine S He</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+L">Longtian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Fabbri%2C+A+R">Alexander R. Fabbri</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+D">Dragomir Radev</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GitHub Repo: <a href="https://github.com/yixinL7/SumLLM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14456" title="Abstract">arXiv:2305.14456</a> (replaced) [<a href="/pdf/2305.14456" title="Download PDF">pdf</a>, <a href="/format/2305.14456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Having Beer after Prayer? Measuring Cultural Bias in Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naous%2C+T">Tarek Naous</a>, 
<a href="/search/cs?searchtype=author&query=Ryan%2C+M+J">Michael J. Ryan</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14806" title="Abstract">arXiv:2305.14806</a> (replaced) [<a href="/pdf/2305.14806" title="Download PDF">pdf</a>, <a href="/format/2305.14806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AWESOME: GPU Memory-constrained Long Document Summarization using Memory  Mechanism and Global Salient Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Shuyang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14824" title="Abstract">arXiv:2305.14824</a> (replaced) [<a href="/pdf/2305.14824" title="Download PDF">pdf</a>, <a href="/format/2305.14824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Temporal Misalignment by Discarding Outdated Facts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M+J+Q">Michael J.Q. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Eunsol Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted into EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15270" title="Abstract">arXiv:2305.15270</a> (replaced) [<a href="/pdf/2305.15270" title="Download PDF">pdf</a>, <a href="/format/2305.15270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reversible Graph Neural Network-based Reaction Distribution Learning for  Multiple Appropriate Facial Reactions Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Spitale%2C+M">Micol Spitale</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gunes%2C+H">Hatice Gunes</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Siyang Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15798" title="Abstract">arXiv:2305.15798</a> (replaced) [<a href="/pdf/2305.15798" title="Download PDF">pdf</a>, <a href="/format/2305.15798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Bo-Kyeong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Hyoung-Kyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Castells%2C+T">Thibault Castells</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Shinkook Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated results; Preliminary version at ICML Workshop on ES-FoMo (2023): <a href="https://openreview.net/forum?id=bOVydU0XKC">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16694" title="Abstract">arXiv:2305.16694</a> (replaced) [<a href="/pdf/2305.16694" title="Download PDF">pdf</a>, <a href="/ps/2305.16694" title="Download PostScript">ps</a>, <a href="/format/2305.16694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reputation-based Persuasion Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arieli%2C+I">Itai Arieli</a>, 
<a href="/search/cs?searchtype=author&query=Madmon%2C+O">Omer Madmon</a>, 
<a href="/search/cs?searchtype=author&query=Tennenholtz%2C+M">Moshe Tennenholtz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17718" title="Abstract">arXiv:2305.17718</a> (replaced) [<a href="/pdf/2305.17718" title="Download PDF">pdf</a>, <a href="/format/2305.17718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuseCap: Leveraging Large Language Models for Enriched Fused Image  Captions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rotstein%2C+N">Noam Rotstein</a>, 
<a href="/search/cs?searchtype=author&query=Bensaid%2C+D">David Bensaid</a>, 
<a href="/search/cs?searchtype=author&query=Brody%2C+S">Shaked Brody</a>, 
<a href="/search/cs?searchtype=author&query=Ganz%2C+R">Roy Ganz</a>, 
<a href="/search/cs?searchtype=author&query=Kimmel%2C+R">Ron Kimmel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18651" title="Abstract">arXiv:2305.18651</a> (replaced) [<a href="/pdf/2305.18651" title="Download PDF">pdf</a>, <a href="/format/2305.18651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UMD: Unsupervised Model Detection for X2X Backdoor Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Zhen Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zidi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 40th International Conference on Machine Learning
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 40th International Conference on Machine
  Learning, PMLR 202:38013-38038, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18787" title="Abstract">arXiv:2305.18787</a> (replaced) [<a href="/pdf/2305.18787" title="Download PDF">pdf</a>, <a href="/format/2305.18787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universality and Limitations of Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+J">Jatin Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18952" title="Abstract">arXiv:2305.18952</a> (replaced) [<a href="/pdf/2305.18952" title="Download PDF">pdf</a>, <a href="/format/2305.18952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Practicality of Generative Retrieval on Dynamic Corpora
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Soyoung Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Chaeeun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunji Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Joel Jang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sohee Yang</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19730" title="Abstract">arXiv:2305.19730</a> (replaced) [<a href="/pdf/2305.19730" title="Download PDF">pdf</a>, <a href="/format/2305.19730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Representations&#x27; Study of Latent Image Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaufman%2C+I">Ilya Kaufman</a>, 
<a href="/search/cs?searchtype=author&query=Azencot%2C+O">Omri Azencot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01765" title="Abstract">arXiv:2306.01765</a> (replaced) [<a href="/pdf/2306.01765" title="Download PDF">pdf</a>, <a href="/ps/2306.01765" title="Download PostScript">ps</a>, <a href="/format/2306.01765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Message in a Bottle -- An Update to the Golden Record
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J+H">Jonathan H. Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Berea%2C+A">Anamaria Berea</a>, 
<a href="/search/cs?searchtype=author&query=Bowden%2C+H">Heather Bowden</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+P">Prithwis Das</a>, 
<a href="/search/cs?searchtype=author&query=Fahy%2C+K+A">Kristen A. Fahy</a>, 
<a href="/search/cs?searchtype=author&query=Ginsberg%2C+J">Joseph Ginsberg</a>, 
<a href="/search/cs?searchtype=author&query=Jew%2C+R">Robert Jew</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kershenbaum%2C+A">Arik Kershenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Kipping%2C+D">David Kipping</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+G">Graham Lau</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+K">Karen Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Lendo%2C+C+I+N">C. Isabel Nunez Lendo</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+P+E">Philip E. Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Searra%2C+N">Nick Searra</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+S+F">Stuart F. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Traphagan%2C+J">John Traphagan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Physics Education (physics.ed-ph); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02582" title="Abstract">arXiv:2306.02582</a> (replaced) [<a href="/pdf/2306.02582" title="Download PDF">pdf</a>, <a href="/format/2306.02582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Point Annotations with Superpixel and Confidence Learning  Guided for Improving Semi-Supervised OCT Fluid Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+T">Tengjin Weng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+K">Kai Jin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhiming Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gewen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submission to MIA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02896" title="Abstract">arXiv:2306.02896</a> (replaced) [<a href="/pdf/2306.02896" title="Download PDF">pdf</a>, <a href="/format/2306.02896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representational Strengths and Limitations of Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanford%2C+C">Clayton Sanford</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+D">Daniel Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Telgarsky%2C+M">Matus Telgarsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06422" title="Abstract">arXiv:2306.06422</a> (replaced) [<a href="/pdf/2306.06422" title="Download PDF">pdf</a>, <a href="/ps/2306.06422" title="Download PostScript">ps</a>, <a href="/format/2306.06422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Weight Distributions of Two Classes of Linear Codes From Perfect  Nonlinear Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huawei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+K">Keqin Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07255" title="Abstract">arXiv:2306.07255</a> (replaced) [<a href="/pdf/2306.07255" title="Download PDF">pdf</a>, <a href="/format/2306.07255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Matrix Flows for Gaussian Graphical Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Negri%2C+M+M">Marcello Massimo Negri</a>, 
<a href="/search/cs?searchtype=author&query=Torres%2C+F+A">F. Arend Torres</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+V">Volker Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS23 version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10532" title="Abstract">arXiv:2306.10532</a> (replaced) [<a href="/pdf/2306.10532" title="Download PDF">pdf</a>, <a href="/format/2306.10532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Elastic Embedding Learning for On-Device Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruiqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Liang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11928" title="Abstract">arXiv:2306.11928</a> (replaced) [<a href="/pdf/2306.11928" title="Download PDF">pdf</a>, <a href="/ps/2306.11928" title="Download PostScript">ps</a>, <a href="/format/2306.11928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Problem: Learning with Variational Objectives on Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cabannes%2C+V">Vivien Cabannes</a>, 
<a href="/search/stat?searchtype=author&query=Domingo-Enrich%2C+C">Carles Domingo-Enrich</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Big Data, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12047" title="Abstract">arXiv:2306.12047</a> (replaced) [<a href="/pdf/2306.12047" title="Download PDF">pdf</a>, <a href="/format/2306.12047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Residual-Based Error Corrector Operator to Enhance Accuracy and  Reliability of Neural Operator Surrogates of Nonlinear Variational  Boundary-Value Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jha%2C+P+K">Prashant K. Jha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 14 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12251" title="Abstract">arXiv:2306.12251</a> (replaced) [<a href="/pdf/2306.12251" title="Download PDF">pdf</a>, <a href="/format/2306.12251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jianheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+F">Fengrui Hua</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Ziqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Datasets and Benchmarks Track camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12983" title="Abstract">arXiv:2306.12983</a> (replaced) [<a href="/pdf/2306.12983" title="Download PDF">pdf</a>, <a href="/format/2306.12983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards More Realistic Membership Inference Attacks on Large Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dubi%C5%84ski%2C+J">Jan Dubi&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Kowalczuk%2C+A">Antoni Kowalczuk</a>, 
<a href="/search/cs?searchtype=author&query=Pawlak%2C+S">Stanis&#x142;aw Pawlak</a>, 
<a href="/search/cs?searchtype=author&query=Rokita%2C+P">Przemys&#x142;aw Rokita</a>, 
<a href="/search/cs?searchtype=author&query=Trzci%C5%84ski%2C+T">Tomasz Trzci&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Morawiecki%2C+P">Pawe&#x142; Morawiecki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14670" title="Abstract">arXiv:2306.14670</a> (replaced) [<a href="/pdf/2306.14670" title="Download PDF">pdf</a>, <a href="/format/2306.14670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jagadeesan%2C+M">Meena Jagadeesan</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Steinhardt%2C+J">Jacob Steinhardt</a>, 
<a href="/search/cs?searchtype=author&query=Haghtalab%2C+N">Nika Haghtalab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS 2023; this is the full version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14752" title="Abstract">arXiv:2306.14752</a> (replaced) [<a href="/pdf/2306.14752" title="Download PDF">pdf</a>, <a href="/format/2306.14752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedLSAM: Localize and Segment Anything Model for 3D CT Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+W">Wenhui Lei</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaofan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaoting Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to MIA. Code is public at <a href="https://github.com/openmedlab/MedLSAM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15518" title="Abstract">arXiv:2306.15518</a> (replaced) [<a href="/e-print/2306.15518" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Paradigm Shift in Sustainability Disclosure Analysis: Empowering  Stakeholders with CHATREPORT, a Language Model-Based Tool
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jingwei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Bingler%2C+J">Julia Bingler</a>, 
<a href="/search/cs?searchtype=author&query=Colesanti-Senni%2C+C">Chiara Colesanti-Senni</a>, 
<a href="/search/cs?searchtype=author&query=Kraus%2C+M">Mathias Kraus</a>, 
<a href="/search/cs?searchtype=author&query=Gostlow%2C+G">Glen Gostlow</a>, 
<a href="/search/cs?searchtype=author&query=Schimanski%2C+T">Tobias Schimanski</a>, 
<a href="/search/cs?searchtype=author&query=Stammbach%2C+D">Dominik Stammbach</a>, 
<a href="/search/cs?searchtype=author&query=Vaghefi%2C+S+A">Saeid Ashraf Vaghefi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Webersinke%2C+N">Nicolas Webersinke</a>, 
<a href="/search/cs?searchtype=author&query=Wekhof%2C+T">Tobias Wekhof</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tingyu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Leippold%2C+M">Markus Leippold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A new version of the ChatReport paper: <a href="/abs/2307.15770">arXiv:2307.15770</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15566" title="Abstract">arXiv:2306.15566</a> (replaced) [<a href="/pdf/2306.15566" title="Download PDF">pdf</a>, <a href="/format/2306.15566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MTFS: a Moving Target Defense-Enabled File System for Malware Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+der+Assen%2C+J">Jan von der Assen</a>, 
<a href="/search/cs?searchtype=author&query=Celdr%C3%A1n%2C+A+H">Alberto Huertas Celdr&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Sefa%2C+R">Rinor Sefa</a>, 
<a href="/search/cs?searchtype=author&query=Bovet%2C+G">G&#xe9;r&#xf4;me Bovet</a>, 
<a href="/search/cs?searchtype=author&query=Stiller%2C+B">Burkhard Stiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00310" title="Abstract">arXiv:2307.00310</a> (replaced) [<a href="/pdf/2307.00310" title="Download PDF">pdf</a>, <a href="/format/2307.00310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thudi%2C+A">Anvith Thudi</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+H">Hengrui Jia</a>, 
<a href="/search/cs?searchtype=author&query=Meehan%2C+C">Casey Meehan</a>, 
<a href="/search/cs?searchtype=author&query=Shumailov%2C+I">Ilia Shumailov</a>, 
<a href="/search/cs?searchtype=author&query=Papernot%2C+N">Nicolas Papernot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00534" title="Abstract">arXiv:2307.00534</a> (replaced) [<a href="/pdf/2307.00534" title="Download PDF">pdf</a>, <a href="/format/2307.00534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shared Growth of Graph Neural Networks via Prompted Free-direction  Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+K">Kaituo Feng</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+Y">Yikun Miao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoren Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2206.06561">arXiv:2206.06561</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01484" title="Abstract">arXiv:2307.01484</a> (replaced) [<a href="/pdf/2307.01484" title="Download PDF">pdf</a>, <a href="/format/2307.01484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust finite element methods and solvers for the Biot--Brinkman  equations in vorticity form
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Caraballo%2C+R">Ruben Caraballo</a>, 
<a href="/search/math?searchtype=author&query=In%2C+C+W">Chansophea Wathanak In</a>, 
<a href="/search/math?searchtype=author&query=Mart%C3%ADn%2C+A+F">Alberto F. Mart&#xed;n</a>, 
<a href="/search/math?searchtype=author&query=Ruiz-Baier%2C+R">Ricardo Ruiz-Baier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01822" title="Abstract">arXiv:2307.01822</a> (replaced) [<a href="/pdf/2307.01822" title="Download PDF">pdf</a>, <a href="/format/2307.01822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional equivariance and modified vector fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Stern%2C+A">Ari Stern</a>, 
<a href="/search/math?searchtype=author&query=Suri%2C+S">Sanah Suri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages; v2: minor revisions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02074" title="Abstract">arXiv:2307.02074</a> (replaced) [<a href="/pdf/2307.02074" title="Download PDF">pdf</a>, <a href="/format/2307.02074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arbitrageurs&#x27; profits, LVR, and sandwich attacks: batch trading as an  AMM design response
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Canidio%2C+A">Andrea Canidio</a>, 
<a href="/search/cs?searchtype=author&query=Fritsch%2C+R">Robin Fritsch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Arbitrage profits, Loss-vs-Rebalancing (LVR), MEV, Sandwich attacks, AMM, Mechanism design, Batch trading
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06276" title="Abstract">arXiv:2307.06276</a> (replaced) [<a href="/pdf/2307.06276" title="Download PDF">pdf</a>, <a href="/format/2307.06276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connectivity Labeling and Routing with Multiple Vertex Failures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parter%2C+M">Merav Parter</a>, 
<a href="/search/cs?searchtype=author&query=Petruschka%2C+A">Asaf Petruschka</a>, 
<a href="/search/cs?searchtype=author&query=Pettie%2C+S">Seth Pettie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06448" title="Abstract">arXiv:2307.06448</a> (replaced) [<a href="/pdf/2307.06448" title="Download PDF">pdf</a>, <a href="/ps/2307.06448" title="Download PostScript">ps</a>, <a href="/format/2307.06448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Internal parametricity, without an interval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altenkirch%2C+T">Thorsten Altenkirch</a>, 
<a href="/search/cs?searchtype=author&query=Chamoun%2C+Y">Yorgo Chamoun</a>, 
<a href="/search/cs?searchtype=author&query=Kaposi%2C+A">Ambrus Kaposi</a>, 
<a href="/search/cs?searchtype=author&query=Shulman%2C+M">Michael Shulman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07711" title="Abstract">arXiv:2307.07711</a> (replaced) [<a href="/pdf/2307.07711" title="Download PDF">pdf</a>, <a href="/ps/2307.07711" title="Download PostScript">ps</a>, <a href="/format/2307.07711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sandpile Prediction on Structured Undirected Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+R">Ruinian Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingbang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Munro%2C+I">Ian Munro</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Richard Peng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Q">Qingyu Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 62 pages, submitted to STOC24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08390" title="Abstract">arXiv:2307.08390</a> (replaced) [<a href="/pdf/2307.08390" title="Download PDF">pdf</a>, <a href="/format/2307.08390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlation-aware Spatial-Temporal Graph Learning for Multivariate  Time-series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+H+Y">Huan Yee Koh</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Ming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+L">Lianhua Chi</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+K+T">Khoa T. Phan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y+P">Yi-Ping Phoebe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+W">Wei Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, double columns, 10 tables, 3 figures. Accepted to IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08442" title="Abstract">arXiv:2307.08442</a> (replaced) [<a href="/pdf/2307.08442" title="Download PDF">pdf</a>, <a href="/format/2307.08442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Algorithms for Energy Games in Special Cases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Forster%2C+S">Sebastian Forster</a> (University of Salzburg), 
<a href="/search/cs?searchtype=author&query=Skarlatos%2C+A">Antonis Skarlatos</a> (University of Salzburg), 
<a href="/search/cs?searchtype=author&query=de+Vos%2C+T">Tijn de Vos</a> (University of Salzburg)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings GandALF 2023, <a href="/abs/2309.17318">arXiv:2309.17318</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EPTCS 390, 2023, pp. 236-252
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08555" title="Abstract">arXiv:2307.08555</a> (replaced) [<a href="/pdf/2307.08555" title="Download PDF">pdf</a>, <a href="/format/2307.08555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Splitting-off in Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%A9rczi%2C+K">Krist&#xf3;f B&#xe9;rczi</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+K">Karthekeyan Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Kir%C3%A1ly%2C+T">Tam&#xe1;s Kir&#xe1;ly</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Shubhang Kulkarni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version of the manuscript only includes results related to hypergraph splitting-off. Two new applications of hypergraph splitting-off are included in this version. In the interest of brevity, the current version omits results on constructing weak covers of skew-supermodular functions from the previous version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10915" title="Abstract">arXiv:2307.10915</a> (replaced) [<a href="/pdf/2307.10915" title="Download PDF">pdf</a>, <a href="/format/2307.10915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Fine-Tuning Strategies for Self-supervised Medical Imaging  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+O">Muhammad Osama Khan</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yi Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12327" title="Abstract">arXiv:2307.12327</a> (replaced) [<a href="/pdf/2307.12327" title="Download PDF">pdf</a>, <a href="/format/2307.12327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Hyperspectral Image Change Detection Network Based on Band  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yao%2C+Q">Qingren Yao</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yuan Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+C">Chang Tang</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+W">Wei Xiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16179" title="Abstract">arXiv:2307.16179</a> (replaced) [<a href="/e-print/2307.16179" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Touch Revolution: How Haptic Feedback Elevates Wearable  Device Experiences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+L">Li He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The content is not particularly relevant to the research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00002" title="Abstract">arXiv:2308.00002</a> (replaced) [<a href="/pdf/2308.00002" title="Download PDF">pdf</a>, <a href="/ps/2308.00002" title="Download PostScript">ps</a>, <a href="/format/2308.00002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Overview Of Temporal Commonsense Reasoning and Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wenzel%2C+G">Georg Wenzel</a>, 
<a href="/search/cs?searchtype=author&query=Jatowt%2C+A">Adam Jatowt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 7 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00245" title="Abstract">arXiv:2308.00245</a> (replaced) [<a href="/pdf/2308.00245" title="Download PDF">pdf</a>, <a href="/format/2308.00245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hitchhiker&#x27;s Guide to Program Analysis: A Journey with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yu Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yizhuo Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhiyun Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02463" title="Abstract">arXiv:2308.02463</a> (replaced) [<a href="/pdf/2308.02463" title="Download PDF">pdf</a>, <a href="/format/2308.02463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Generalist Foundation Model for Radiology by Leveraging  Web-scale 2D&amp;3D Medical Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chaoyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoman Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03656" title="Abstract">arXiv:2308.03656</a> (replaced) [<a href="/pdf/2308.03656" title="Download PDF">pdf</a>, <a href="/format/2308.03656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using  EmotionBench
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jen-tse Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+M+H">Man Ho Lam</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+E+J">Eric John Li</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shujie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenxiang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages. Added demographic distribution of the user study. Added ethics statements. Added more details. Fixed some typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06927" title="Abstract">arXiv:2308.06927</a> (replaced) [<a href="/pdf/2308.06927" title="Download PDF">pdf</a>, <a href="/ps/2308.06927" title="Download PostScript">ps</a>, <a href="/format/2308.06927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The algorithmic second law of thermodynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Ebtekar%2C+A">Aram Ebtekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, LaTeX; major revision, added Jarzynski and Landauer inequalities
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Mechanics (cond-mat.stat-mech)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09543" title="Abstract">arXiv:2308.09543</a> (replaced) [<a href="/pdf/2308.09543" title="Download PDF">pdf</a>, <a href="/format/2308.09543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent State Models of Training Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+M+Y">Michael Y. Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Saphra%2C+N">Naomi Saphra</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11181" title="Abstract">arXiv:2308.11181</a> (replaced) [<a href="/e-print/2308.11181" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Interaction -- Bridging Time and Experience in Human-Computer  Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+L">Li He</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+B">Baixi Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxi Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The content is not particularly relevant to the research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12770" title="Abstract">arXiv:2308.12770</a> (replaced) [<a href="/pdf/2308.12770" title="Download PDF">pdf</a>, <a href="/format/2308.12770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WavMark: Watermarking for Audio Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shujie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xiaoyong Du</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16137" title="Abstract">arXiv:2308.16137</a> (replaced) [<a href="/pdf/2308.16137" title="Download PDF">pdf</a>, <a href="/format/2308.16137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LM-Infinite: Simple On-the-Fly Length Generalization for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chi Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wenhan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sinong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16328" title="Abstract">arXiv:2308.16328</a> (replaced) [<a href="/e-print/2308.16328" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debunking Disinformation: Revolutionizing Truth with NLP in Fake News  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+L">Li He</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Siyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+A">Ailun Pei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The content is not particularly relevant to the research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16619" title="Abstract">arXiv:2308.16619</a> (replaced) [<a href="/pdf/2308.16619" title="Download PDF">pdf</a>, <a href="/format/2308.16619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Compressed Segmentation Volumes for Scientific Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piochowiak%2C+M">Max Piochowiak</a>, 
<a href="/search/cs?searchtype=author&query=Dachsbacher%2C+C">Carsten Dachsbacher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE VIS 2023 - The supplemental materials can be found at <a href="https://osf.io/8nbdk/">this https URL</a> | ACM 2012 CCS - Human-centered computing, Visualization, Visualization application domains, Scientific visualization; Information systems, Data management systems, Data structures, Data layout, Data compression
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02340" title="Abstract">arXiv:2309.02340</a> (replaced) [<a href="/pdf/2309.02340" title="Download PDF">pdf</a>, <a href="/format/2309.02340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Infinite-Resolution Texture using GANs with Patch-by-Patch  Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdellatif%2C+A">Alhasan Abdellatif</a>, 
<a href="/search/cs?searchtype=author&query=Elsheikh%2C+A+H">Ahmed H. Elsheikh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05405" title="Abstract">arXiv:2309.05405</a> (replaced) [<a href="/pdf/2309.05405" title="Download PDF">pdf</a>, <a href="/format/2309.05405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-Stage Hybrid Supervision Framework for Fast, Low-resource, and  Accurate Organ and Pan-cancer Segmentation in Abdomen CT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+W">Wentao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Tian%2C+T">Tong Tian</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+W">Weijin Xu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Lemeng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haoyuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+H">Huihua Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06978" title="Abstract">arXiv:2309.06978</a> (replaced) [<a href="/pdf/2309.06978" title="Download PDF">pdf</a>, <a href="/format/2309.06978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable JPEG: The Devil is in the Details
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reich%2C+C">Christoph Reich</a>, 
<a href="/search/cs?searchtype=author&query=Debnath%2C+B">Biplob Debnath</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+D">Deep Patel</a>, 
<a href="/search/cs?searchtype=author&query=Chakradhar%2C+S">Srimat Chakradhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024. Project page: <a href="https://christophreich1996.github.io/differentiable_jpeg/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07383" title="Abstract">arXiv:2309.07383</a> (replaced) [<a href="/pdf/2309.07383" title="Download PDF">pdf</a>, <a href="/ps/2309.07383" title="Download PostScript">ps</a>, <a href="/format/2309.07383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rates of Convergence in Certain Native Spaces of Approximations used in  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bouland%2C+A">Ali Bouland</a>, 
<a href="/search/eess?searchtype=author&query=Niu%2C+S">Shengyuan Niu</a>, 
<a href="/search/eess?searchtype=author&query=Paruchuri%2C+S+T">Sai Tej Paruchuri</a>, 
<a href="/search/eess?searchtype=author&query=Kurdila%2C+A">Andrew Kurdila</a>, 
<a href="/search/eess?searchtype=author&query=Burns%2C+J">John Burns</a>, 
<a href="/search/eess?searchtype=author&query=Schuster%2C+E">Eugenio Schuster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09801" title="Abstract">arXiv:2309.09801</a> (replaced) [<a href="/pdf/2309.09801" title="Download PDF">pdf</a>, <a href="/ps/2309.09801" title="Download PostScript">ps</a>, <a href="/format/2309.09801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Optimal Contracts: How to Exploit Small Action Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bacchiocchi%2C+F">Francesco Bacchiocchi</a>, 
<a href="/search/cs?searchtype=author&query=Castiglioni%2C+M">Matteo Castiglioni</a>, 
<a href="/search/cs?searchtype=author&query=Marchesi%2C+A">Alberto Marchesi</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10923" title="Abstract">arXiv:2309.10923</a> (replaced) [<a href="/pdf/2309.10923" title="Download PDF">pdf</a>, <a href="/format/2309.10923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-automatic staging area for high-quality structured data extraction  from scientific literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foppiano%2C+L">Luca Foppiano</a>, 
<a href="/search/cs?searchtype=author&query=Mato%2C+T">Tomoya Mato</a>, 
<a href="/search/cs?searchtype=author&query=Terashima%2C+K">Kensei Terashima</a>, 
<a href="/search/cs?searchtype=author&query=Suarez%2C+P+O">Pedro Ortiz Suarez</a>, 
<a href="/search/cs?searchtype=author&query=Tou%2C+T">Taku Tou</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+C">Chikako Sakai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei-Sheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Amagasa%2C+T">Toshiyuki Amagasa</a>, 
<a href="/search/cs?searchtype=author&query=Takano%2C+Y">Yoshihiko Takano</a>, 
<a href="/search/cs?searchtype=author&query=Ishii%2C+M">Masashi Ishii</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 tables, 6 figures, 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Superconductivity (cond-mat.supr-con); Databases (cs.DB); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12620" title="Abstract">arXiv:2309.12620</a> (replaced) [<a href="/pdf/2309.12620" title="Download PDF">pdf</a>, <a href="/format/2309.12620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Preference Learning Methods for Sorting Problems with  Multiple Temporal Criteria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Mengzhuo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Kadzi%C5%84ski%2C+M">Mi&#x142;osz Kadzi&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingpeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16490" title="Abstract">arXiv:2309.16490</a> (replaced) [<a href="/pdf/2309.16490" title="Download PDF">pdf</a>, <a href="/format/2309.16490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active SLAM Utility Function Exploiting Path Entropy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+M+F">Muhammad Farhan Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Fremont%2C+V">Vincent Fremont</a>, 
<a href="/search/cs?searchtype=author&query=Fantoni%2C+I">Isabelle Fantoni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures, Submitted to IEEE SOLI Conference. arXiv admin note: text overlap with <a href="/abs/2212.11654">arXiv:2212.11654</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04955" title="Abstract">arXiv:2310.04955</a> (replaced) [<a href="/pdf/2310.04955" title="Download PDF">pdf</a>, <a href="/format/2310.04955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-Theoretic Bounds on The Removal of Attribute-Specific Bias  From Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiazhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Khayatkhoei%2C+M">Mahyar Khayatkhoei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiageng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hanchen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hussein%2C+M+E">Mohamed E. Hussein</a>, 
<a href="/search/cs?searchtype=author&query=AbdAlmageed%2C+W">Wael AbdAlmageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures, 3 tables. To appear in Algorithmic Fairness through the Lens of Time Workshop at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05213" title="Abstract">arXiv:2310.05213</a> (replaced) [<a href="/pdf/2310.05213" title="Download PDF">pdf</a>, <a href="/ps/2310.05213" title="Download PostScript">ps</a>, <a href="/format/2310.05213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quantum Approach for Reducing Communications in Classical  Cryptographic Primitives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+J">Jiayu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08176" title="Abstract">arXiv:2310.08176</a> (replaced) [<a href="/pdf/2310.08176" title="Download PDF">pdf</a>, <a href="/format/2310.08176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infinite Width Graph Neural Networks for Node Regression/ Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cobanoglu%2C+Y">Yunus Cobanoglu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 Pages, 2 Figures (with subfigures), multiple tables, v2: made table of contents fit to one page and added derivatives on GAT*NTK and GAT*GP in A.4, v3: shorten parts of introduction and fixed typos, added numberings to equations and discussion section
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09444" title="Abstract">arXiv:2310.09444</a> (replaced) [<a href="/pdf/2310.09444" title="Download PDF">pdf</a>, <a href="/format/2310.09444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Heterogeneity in Medical Federated learning via Vision  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darzi%2C+E">Erfan Darzi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yiqing Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yangming Ou</a>, 
<a href="/search/cs?searchtype=author&query=Sijtsema%2C+N+M">Nanna M. Sijtsema</a>, 
<a href="/search/cs?searchtype=author&query=van+Ooijen%2C+P+M+A">P.M.A van Ooijen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09917" title="Abstract">arXiv:2310.09917</a> (replaced) [<a href="/pdf/2310.09917" title="Download PDF">pdf</a>, <a href="/format/2310.09917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical study of pretrained multilingual language models for zero-shot  cross-lingual generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chirkova%2C+N">Nadezhda Chirkova</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Sheng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Nikoulina%2C+V">Vassilina Nikoulina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10865" title="Abstract">arXiv:2310.10865</a> (replaced) [<a href="/pdf/2310.10865" title="Download PDF">pdf</a>, <a href="/format/2310.10865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Will the Prince Get True Love&#x27;s Kiss? On the Model Sensitivity to Gender  Perturbation over Fairytale Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chance%2C+C">Christina Chance</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Da Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11722" title="Abstract">arXiv:2310.11722</a> (replaced) [<a href="/pdf/2310.11722" title="Download PDF">pdf</a>, <a href="/format/2310.11722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Self-diagnostic Atomic Knowledge in Chinese Medical  Foundation Model: A Computational Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yaxin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Feng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12059" title="Abstract">arXiv:2310.12059</a> (replaced) [<a href="/pdf/2310.12059" title="Download PDF">pdf</a>, <a href="/format/2310.12059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Symbol Binding Ability of Large Language Models for  Multiple-Choice Questions in Vietnamese General Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duc-Vu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quoc-Nam Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SoICT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13007" title="Abstract">arXiv:2310.13007</a> (replaced) [<a href="/pdf/2310.13007" title="Download PDF">pdf</a>, <a href="/format/2310.13007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Critical Survey on Fairness Benefits of XAI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deck%2C+L">Luca Deck</a>, 
<a href="/search/cs?searchtype=author&query=Schoeffer%2C+J">Jakob Schoeffer</a>, 
<a href="/search/cs?searchtype=author&query=De-Arteaga%2C+M">Maria De-Arteaga</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChl%2C+N">Niklas K&#xfc;hl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15015" title="Abstract">arXiv:2310.15015</a> (replaced) [<a href="/pdf/2310.15015" title="Download PDF">pdf</a>, <a href="/ps/2310.15015" title="Download PostScript">ps</a>, <a href="/format/2310.15015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Deep Learning for Abstractive Code Summarization of  Unofficial Documentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naghshzan%2C+A">AmirHossein Naghshzan</a>, 
<a href="/search/cs?searchtype=author&query=Guerrouj%2C+L">Latifa Guerrouj</a>, 
<a href="/search/cs?searchtype=author&query=Baysal%2C+O">Olga Baysal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15756" title="Abstract">arXiv:2310.15756</a> (replaced) [<a href="/pdf/2310.15756" title="Download PDF">pdf</a>, <a href="/ps/2310.15756" title="Download PostScript">ps</a>, <a href="/format/2310.15756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Low-SNR Asymptotic Capacity of Optical Wireless Channels with  Average-Intensity Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longguang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15808" title="Abstract">arXiv:2310.15808</a> (replaced) [<a href="/pdf/2310.15808" title="Download PDF">pdf</a>, <a href="/format/2310.15808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissecting the Performance of Satellite Network Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raman%2C+A">Aravindh Raman</a>, 
<a href="/search/cs?searchtype=author&query=Varvello%2C+M">Matteo Varvello</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Hyunseok Chang</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+N">Nishanth Sastry</a>, 
<a href="/search/cs?searchtype=author&query=Zaki%2C+Y">Yasir Zaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at International Conference on emerging Networking EXperiments and Technologies (CoNEXT 2023). Please cite the CoNEXT version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16776" title="Abstract">arXiv:2310.16776</a> (replaced) [<a href="/pdf/2310.16776" title="Download PDF">pdf</a>, <a href="/format/2310.16776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEFT: Data Efficient Fine-Tuning for Large Language Models via  Unsupervised Core-Set Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+D">Devleena Das</a>, 
<a href="/search/cs?searchtype=author&query=Khetan%2C+V">Vivek Khetan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16872" title="Abstract">arXiv:2310.16872</a> (replaced) [<a href="/pdf/2310.16872" title="Download PDF">pdf</a>, <a href="/format/2310.16872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SonoSAMTrack -- Segment and Track Anything on Ultrasound Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ravishankar%2C+H">Hariharan Ravishankar</a>, 
<a href="/search/eess?searchtype=author&query=Patil%2C+R">Rohan Patil</a>, 
<a href="/search/eess?searchtype=author&query=Melapudi%2C+V">Vikram Melapudi</a>, 
<a href="/search/eess?searchtype=author&query=Suthar%2C+H">Harsh Suthar</a>, 
<a href="/search/eess?searchtype=author&query=Anzengruber%2C+S">Stephan Anzengruber</a>, 
<a href="/search/eess?searchtype=author&query=Bhatia%2C+P">Parminder Bhatia</a>, 
<a href="/search/eess?searchtype=author&query=Taha%2C+K">Kass-Hout Taha</a>, 
<a href="/search/eess?searchtype=author&query=Annangi%2C+P">Pavan Annangi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18583" title="Abstract">arXiv:2310.18583</a> (replaced) [<a href="/pdf/2310.18583" title="Download PDF">pdf</a>, <a href="/ps/2310.18583" title="Download PostScript">ps</a>, <a href="/format/2310.18583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Multi-Modality Learning for Multi-Label Skin Lesion  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+E">Euijoon Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+L">Lei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinman Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19537" title="Abstract">arXiv:2310.19537</a> (replaced) [<a href="/pdf/2310.19537" title="Download PDF">pdf</a>, <a href="/format/2310.19537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On consequences of finetuning on data with highly discriminative  features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masarczyk%2C+W">Wojciech Masarczyk</a>, 
<a href="/search/cs?searchtype=author&query=Trzci%C5%84ski%2C+T">Tomasz Trzci&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Ostaszewski%2C+M">Mateusz Ostaszewski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 -- UniReps Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19548" title="Abstract">arXiv:2310.19548</a> (replaced) [<a href="/pdf/2310.19548" title="Download PDF">pdf</a>, <a href="/format/2310.19548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation Theory, Computing, and Deep Learning on the Wasserstein  Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fornasier%2C+M">Massimo Fornasier</a>, 
<a href="/search/math?searchtype=author&query=Heid%2C+P">Pascal Heid</a>, 
<a href="/search/math?searchtype=author&query=Sodini%2C+G+E">Giacomo Enrico Sodini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added link to GitHub repository
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Functional Analysis (math.FA)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20522" title="Abstract">arXiv:2310.20522</a> (replaced) [<a href="/pdf/2310.20522" title="Download PDF">pdf</a>, <a href="/ps/2310.20522" title="Download PostScript">ps</a>, <a href="/format/2310.20522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight bounds on adjacency labels for monotone graph classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bonnet%2C+%C3%89">&#xc9;douard Bonnet</a>, 
<a href="/search/math?searchtype=author&query=Duron%2C+J">Julien Duron</a>, 
<a href="/search/math?searchtype=author&query=Sylvester%2C+J">John Sylvester</a>, 
<a href="/search/math?searchtype=author&query=Zamaraev%2C+V">Viktor Zamaraev</a>, 
<a href="/search/math?searchtype=author&query=Zhukovskii%2C+M">Maksim Zhukovskii</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 1 figure. arXiv admin note: text overlap with <a href="/abs/2307.11225">arXiv:2307.11225</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00440" title="Abstract">arXiv:2311.00440</a> (replaced) [<a href="/pdf/2311.00440" title="Download PDF">pdf</a>, <a href="/ps/2311.00440" title="Download PostScript">ps</a>, <a href="/format/2311.00440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum $k$- vs. $\ell$-colourings of graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakajima%2C+T">Tamio-Vesa Nakajima</a>, 
<a href="/search/cs?searchtype=author&query=%C5%BDivn%C3%BD%2C+S">Stanislav &#x17d;ivn&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> an extra hardness result
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00619" title="Abstract">arXiv:2311.00619</a> (replaced) [<a href="/pdf/2311.00619" title="Download PDF">pdf</a>, <a href="/format/2311.00619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loss Modeling for Multi-Annotator Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jinadu%2C+U">Uthman Jinadu</a>, 
<a href="/search/cs?searchtype=author&query=Annan%2C+J">Jesse Annan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+S">Shanshan Wen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yi Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00790" title="Abstract">arXiv:2311.00790</a> (replaced) [<a href="/pdf/2311.00790" title="Download PDF">pdf</a>, <a href="/format/2311.00790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction Artifacts in Metaphor Identification Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boisson%2C+J">Joanne Boisson</a>, 
<a href="/search/cs?searchtype=author&query=Espinosa-Anke%2C+L">Luis Espinosa-Anke</a>, 
<a href="/search/cs?searchtype=author&query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Short paper accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00912" title="Abstract">arXiv:2311.00912</a> (replaced) [<a href="/pdf/2311.00912" title="Download PDF">pdf</a>, <a href="/ps/2311.00912" title="Download PostScript">ps</a>, <a href="/format/2311.00912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whitney-type estimates for convex functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Prymak%2C+A">Andriy Prymak</a>, 
<a href="/search/math?searchtype=author&query=Singh%2C+J">Jaskaran Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proof of Prop. 8 was simplified
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01052" title="Abstract">arXiv:2311.01052</a> (replaced) [<a href="/pdf/2311.01052" title="Download PDF">pdf</a>, <a href="/format/2311.01052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilient Multiple Choice Learning: A learned scoring scheme with  application to audio scene analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Letzelter%2C+V">Victor Letzelter</a>, 
<a href="/search/stat?searchtype=author&query=Fontaine%2C+M">Mathieu Fontaine</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+M">Micka&#xeb;l Chen</a>, 
<a href="/search/stat?searchtype=author&query=P%C3%A9rez%2C+P">Patrick P&#xe9;rez</a>, 
<a href="/search/stat?searchtype=author&query=Essid%2C+S">Slim Essid</a>, 
<a href="/search/stat?searchtype=author&query=Richard%2C+G">Ga&#xeb;l Richard</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in neural information processing systems, Dec 2023, New
  Orleans, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01655" title="Abstract">arXiv:2311.01655</a> (replaced) [<a href="/pdf/2311.01655" title="Download PDF">pdf</a>, <a href="/format/2311.01655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Spurious Correlations via Robust Visual Concepts in Real and  AI-Generated Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dammu%2C+P+P+S">Preetam Prabhu Srikar Dammu</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+C">Chirag Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted at 37th Conference on Neural Information Processing Systems (NeurIPS 2023), XAIA Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01766" title="Abstract">arXiv:2311.01766</a> (replaced) [<a href="/pdf/2311.01766" title="Download PDF">pdf</a>, <a href="/format/2311.01766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Support or Refute: Analyzing the Stance of Evidence to Detect  Out-of-Context Mis- and Disinformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jie Guo</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+W">Weidong Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shujun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02782" title="Abstract">arXiv:2311.02782</a> (replaced) [<a href="/pdf/2311.02782" title="Download PDF">pdf</a>, <a href="/format/2311.02782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Generic Anomaly Detection and Understanding: Large-scale  Visual-linguistic Model (GPT-4V) Takes the Lead
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yunkang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaohao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaonan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Weiming Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. Evaluated GPT-4V on 4 modalities, 9 tasks, and 15 datasets. The first three authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02802" title="Abstract">arXiv:2311.02802</a> (replaced) [<a href="/pdf/2311.02802" title="Download PDF">pdf</a>, <a href="/format/2311.02802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating Worker Perspectives into MTurk Annotation Practices for  NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+O">Olivia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fleisig%2C+E">Eve Fleisig</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02869" title="Abstract">arXiv:2311.02869</a> (replaced) [<a href="/pdf/2311.02869" title="Download PDF">pdf</a>, <a href="/format/2311.02869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight equivariant interaction graph neural network for accurate  and efficient interatomic potential and force predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziduo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Q">Qiujie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+Y">Calvin Yu-Chian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lei Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03220" title="Abstract">arXiv:2311.03220</a> (replaced) [<a href="/pdf/2311.03220" title="Download PDF">pdf</a>, <a href="/format/2311.03220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALYMPICS: Language Agents Meet Game Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+S">Shaoguang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yuzhe Cai</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenshan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fengyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+T">Tao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04950" title="Abstract">arXiv:2311.04950</a> (replaced) [<a href="/pdf/2311.04950" title="Download PDF">pdf</a>, <a href="/format/2311.04950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight Diffusion Models with Distillation-Based Block Neural  Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+C">Chaoyu Guan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yansong Tang</a>, 
<a href="/search/cs?searchtype=author&query=zhu%2C+W">Wenwu zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05116" title="Abstract">arXiv:2311.05116</a> (replaced) [<a href="/pdf/2311.05116" title="Download PDF">pdf</a>, <a href="/format/2311.05116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covering Number of Real Algebraic Varieties and Beyond: Improved Bounds  and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/math?searchtype=author&query=Kileel%2C+J">Joe Kileel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05237" title="Abstract">arXiv:2311.05237</a> (replaced) [<a href="/pdf/2311.05237" title="Download PDF">pdf</a>, <a href="/format/2311.05237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Widely Applicable Strong Baseline for Sports Ball Detection and Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarashima%2C+S">Shuhei Tarashima</a>, 
<a href="/search/cs?searchtype=author&query=Haq%2C+M+A">Muhammad Abdul Haq</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yushan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tagawa%2C+N">Norio Tagawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BMVC2023. Code &amp; dataset : <a href="https://github.com/nttcom/WASB-SBDT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05371" title="Abstract">arXiv:2311.05371</a> (replaced) [<a href="/pdf/2311.05371" title="Download PDF">pdf</a>, <a href="/format/2311.05371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Robust Deep Physiological Measurement Models with Synthetic  Video-based Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yuxuan Ou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuzhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuntang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+S">Shwetak Patel</a>, 
<a href="/search/cs?searchtype=author&query=McDuf%2C+D">Daniel McDuf</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuzhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05417" title="Abstract">arXiv:2311.05417</a> (replaced) [<a href="/pdf/2311.05417" title="Download PDF">pdf</a>, <a href="/format/2311.05417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting the Position Uncertainty at the Time of Closest Approach with  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guimar%C3%A3es%2C+M">Marta Guimar&#xe3;es</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+C">Cl&#xe1;udia Soares</a>, 
<a href="/search/cs?searchtype=author&query=Manfletti%2C+C">Chiara Manfletti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05426" title="Abstract">arXiv:2311.05426</a> (replaced) [<a href="/pdf/2311.05426" title="Download PDF">pdf</a>, <a href="/format/2311.05426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Learning of Conjunction Data Messages Through a Bayesian  Non-Homogeneous Poisson Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guimar%C3%A3es%2C+M">Marta Guimar&#xe3;es</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+C">Cl&#xe1;udia Soares</a>, 
<a href="/search/cs?searchtype=author&query=Manfletti%2C+C">Chiara Manfletti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05430" title="Abstract">arXiv:2311.05430</a> (replaced) [<a href="/pdf/2311.05430" title="Download PDF">pdf</a>, <a href="/format/2311.05430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taxonomy for Resident Space Objects in LEO: A Deep Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guimar%C3%A3es%2C+M">Marta Guimar&#xe3;es</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+C">Cl&#xe1;udia Soares</a>, 
<a href="/search/cs?searchtype=author&query=Manfletti%2C+C">Chiara Manfletti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06115" title="Abstract">arXiv:2311.06115</a> (replaced) [<a href="/pdf/2311.06115" title="Download PDF">pdf</a>, <a href="/format/2311.06115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Scalable Kernel Matrix Approximations using Hierarchical  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gaddameedi%2C+K">Keerthi Gaddameedi</a>, 
<a href="/search/math?searchtype=author&query=Reiz%2C+S">Severin Reiz</a>, 
<a href="/search/math?searchtype=author&query=Neckel%2C+T">Tobias Neckel</a>, 
<a href="/search/math?searchtype=author&query=Bungartz%2C+H">Hans-Joachim Bungartz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> * Author one and author two - Equal contribution. Accepted for IC 2023 held in conjunction with the BenchCouncil <a href="https://www.benchcouncil.org/ic2023/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06330" title="Abstract">arXiv:2311.06330</a> (replaced) [<a href="/pdf/2311.06330" title="Download PDF">pdf</a>, <a href="/format/2311.06330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Agent-Based Modeling: On the Use of Large Language Models in  Computer Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zengqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Run Peng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuyuan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chuan Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source codes are available at <a href="https://github.com/Roihn/SABM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE); Multiagent Systems (cs.MA); General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06399" title="Abstract">arXiv:2311.06399</a> (replaced) [<a href="/pdf/2311.06399" title="Download PDF">pdf</a>, <a href="/format/2311.06399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conservation properties of the augmented basis update &amp; Galerkin  integrator for kinetic problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Einkemmer%2C+L">Lukas Einkemmer</a>, 
<a href="/search/math?searchtype=author&query=Kusch%2C+J">Jonas Kusch</a>, 
<a href="/search/math?searchtype=author&query=Schotth%C3%B6fer%2C+S">Steffen Schotth&#xf6;fer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06694" title="Abstract">arXiv:2311.06694</a> (replaced) [<a href="/pdf/2311.06694" title="Download PDF">pdf</a>, <a href="/format/2311.06694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Multi-View Language Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitra%2C+C">Chancharik Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+A">Abrar Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Corona%2C+R">Rodolfo Corona</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Thomason%2C+J">Jesse Thomason</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07141" title="Abstract">arXiv:2311.07141</a> (replaced) [<a href="/pdf/2311.07141" title="Download PDF">pdf</a>, <a href="/format/2311.07141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SABAF: Removing Strong Attribute Bias from Neural Networks with  Adversarial Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiazhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Khayatkhoei%2C+M">Mahyar Khayatkhoei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiageng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hanchen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hussein%2C+M+E">Mohamed E. Hussein</a>, 
<a href="/search/cs?searchtype=author&query=AbdAlmageed%2C+W">Wael AbdAlmageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 18 figures, 32 tables. This work is an extended version of our paper (<a href="/abs/2310.04955">arXiv:2310.04955</a>). Code will be released at <a href="https://github.com/jiazhi412/strong_attribute_bias">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07194" title="Abstract">arXiv:2311.07194</a> (replaced) [<a href="/pdf/2311.07194" title="Download PDF">pdf</a>, <a href="/format/2311.07194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Dialogue Comprehension Ability of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=She%2C+S">Shuaijie She</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingyun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yanke Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07370" title="Abstract">arXiv:2311.07370</a> (replaced) [<a href="/pdf/2311.07370" title="Download PDF">pdf</a>, <a href="/format/2311.07370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification of developmental and brain disorders via graph  convolutional aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salim%2C+I">Ibrahim Salim</a>, 
<a href="/search/cs?searchtype=author&query=Hamza%2C+A+B">A. Ben Hamza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07468" title="Abstract">arXiv:2311.07468</a> (replaced) [<a href="/pdf/2311.07468" title="Download PDF">pdf</a>, <a href="/format/2311.07468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation  of the Reversal Curse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+A">Ang Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shufang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Q">Quan Tu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07469" title="Abstract">arXiv:2311.07469</a> (replaced) [<a href="/pdf/2311.07469" title="Download PDF">pdf</a>, <a href="/format/2311.07469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InCA: Rethinking In-Car Conversational System Assessment Leveraging  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Friedl%2C+K+E">Ken E. Friedl</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A+G">Abbas Goher Khan</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+S+R">Soumya Ranjan Sahoo</a>, 
<a href="/search/cs?searchtype=author&query=Rony%2C+M+R+A+H">Md Rashad Al Hasan Rony</a>, 
<a href="/search/cs?searchtype=author&query=Germies%2C+J">Jana Germies</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%BC%C3%9F%2C+C">Christian S&#xfc;&#xdf;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07587" title="Abstract">arXiv:2311.07587</a> (replaced) [<a href="/pdf/2311.07587" title="Download PDF">pdf</a>, <a href="/format/2311.07587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frontier Language Models are not Robust to Adversarial Arithmetic, or  &quot;What do I need to say so you agree 2+2=5?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freeman%2C+C+D">C. Daniel Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Culp%2C+L">Laura Culp</a>, 
<a href="/search/cs?searchtype=author&query=Parisi%2C+A">Aaron Parisi</a>, 
<a href="/search/cs?searchtype=author&query=Bileschi%2C+M+L">Maxwell L Bileschi</a>, 
<a href="/search/cs?searchtype=author&query=Elsayed%2C+G+F">Gamaleldin F Elsayed</a>, 
<a href="/search/cs?searchtype=author&query=Rizkowsky%2C+A">Alex Rizkowsky</a>, 
<a href="/search/cs?searchtype=author&query=Simpson%2C+I">Isabelle Simpson</a>, 
<a href="/search/cs?searchtype=author&query=Alemi%2C+A">Alex Alemi</a>, 
<a href="/search/cs?searchtype=author&query=Nova%2C+A">Azade Nova</a>, 
<a href="/search/cs?searchtype=author&query=Adlam%2C+B">Ben Adlam</a>, 
<a href="/search/cs?searchtype=author&query=Bohnet%2C+B">Bernd Bohnet</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+G">Gaurav Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Sedghi%2C+H">Hanie Sedghi</a>, 
<a href="/search/cs?searchtype=author&query=Mordatch%2C+I">Igor Mordatch</a>, 
<a href="/search/cs?searchtype=author&query=Gur%2C+I">Izzeddin Gur</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaehoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Co-Reyes%2C+J">JD Co-Reyes</a>, 
<a href="/search/cs?searchtype=author&query=Pennington%2C+J">Jeffrey Pennington</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kelvin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Swersky%2C+K">Kevin Swersky</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+K">Kshiteej Mahajan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Lechao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rosanne Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kornblith%2C+S">Simon Kornblith</a>, 
<a href="/search/cs?searchtype=author&query=Constant%2C+N">Noah Constant</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P+J">Peter J. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Novak%2C+R">Roman Novak</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yundi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Fiedel%2C+N">Noah Fiedel</a>, 
<a href="/search/cs?searchtype=author&query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07622" title="Abstract">arXiv:2311.07622</a> (replaced) [<a href="/pdf/2311.07622" title="Download PDF">pdf</a>, <a href="/format/2311.07622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretrain like Your Inference: Masked Tuning Improves Zero-Shot Composed  Image Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+H">Hanjiang Lai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07634" title="Abstract">arXiv:2311.07634</a> (replaced) [<a href="/pdf/2311.07634" title="Download PDF">pdf</a>, <a href="/format/2311.07634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ActiveDC: Distribution Calibration for Active Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenshuai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhenhui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+J">Jinzhou Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07870" title="Abstract">arXiv:2311.07870</a> (replaced) [<a href="/pdf/2311.07870" title="Download PDF">pdf</a>, <a href="/format/2311.07870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoML for Large Capacity Modeling of Meta&#x27;s Ranking Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kuang-Hung Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mengying Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Buyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sehgal%2C+V">Vivek Sehgal</a>, 
<a href="/search/cs?searchtype=author&query=Panchal%2C+R+R">Rudresh Rajnikant Panchal</a>, 
<a href="/search/cs?searchtype=author&query=Hotaj%2C+E">Eugen Hotaj</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Daifeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jamey Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shali Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huayu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen-Yen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiyan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wei Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Hang Yin and Kuang-Hung Liu contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08167" title="Abstract">arXiv:2311.08167</a> (replaced) [<a href="/pdf/2311.08167" title="Download PDF">pdf</a>, <a href="/format/2311.08167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeDe: Balancing Blockchain Privacy and Regulatory Compliance by  Selective De-Anonymization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+N">Naveen Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Gajera%2C+M">Mitul Gajera</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+A">Amit Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Ivey-Law%2C+H">Hamish Ivey-Law</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08258" title="Abstract">arXiv:2311.08258</a> (replaced) [<a href="/pdf/2311.08258" title="Download PDF">pdf</a>, <a href="/ps/2311.08258" title="Download PostScript">ps</a>, <a href="/format/2311.08258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unprecedented reach and rich online journeys drive hate and extremism  globally
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sear%2C+R">Richard Sear</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+N+F">Neil F. Johnson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Human-Computer Interaction (cs.HC); Adaptation and Self-Organizing Systems (nlin.AO); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08329" title="Abstract">arXiv:2311.08329</a> (replaced) [<a href="/pdf/2311.08329" title="Download PDF">pdf</a>, <a href="/format/2311.08329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KTRL+F: Knowledge-Augmented In-Document Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+H">Hanseok Oh</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Haebin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+M">Miyoung Ko</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunji Lee</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08398" title="Abstract">arXiv:2311.08398</a> (replaced) [<a href="/pdf/2311.08398" title="Download PDF">pdf</a>, <a href="/format/2311.08398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Temporally Grounded?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yifu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ziser%2C+Y">Yftah Ziser</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Ponti%2C+E+M">Edoardo M. Ponti</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08433" title="Abstract">arXiv:2311.08433</a> (replaced) [<a href="/e-print/2311.08433" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clinical Characteristics and Laboratory Biomarkers in ICU-admitted  Septic Patients with and without Bacteremia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Baek%2C+S">Sangwon Baek</a>, 
<a href="/search/q-bio?searchtype=author&query=Lee%2C+S+J">Seung Jun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article is not the right fit to be published as preprint in arXiv
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08562" title="Abstract">arXiv:2311.08562</a> (replaced) [<a href="/pdf/2311.08562" title="Download PDF">pdf</a>, <a href="/format/2311.08562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAgIC: Investigation of Large Language Model Powered Multi-Agent in  Cognition, Adaptability, Rationality and Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Daquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hongyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S+K">See Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiashi Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08728" title="Abstract">arXiv:2311.08728</a> (replaced) [<a href="/pdf/2311.08728" title="Download PDF">pdf</a>, <a href="/ps/2311.08728" title="Download PostScript">ps</a>, <a href="/format/2311.08728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Placement of Capacitor in Distribution System using Particle  Swarm Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haq%2C+I+U">Izhar Ul Haq</a> (School of Automation, Central South University, China)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 3 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08870" title="Abstract">arXiv:2311.08870</a> (replaced) [<a href="/pdf/2311.08870" title="Download PDF">pdf</a>, <a href="/format/2311.08870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Shot Federated Learning with Classifier-Guided Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingzhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+S">Shangchao Su</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+X">Xiangyang Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08996" title="Abstract">arXiv:2311.08996</a> (replaced) [<a href="/pdf/2311.08996" title="Download PDF">pdf</a>, <a href="/format/2311.08996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel Estimation for mmWave MIMO using sub-6 GHz Out-of-Band  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasic%2C+F">Faruk Pasic</a>, 
<a href="/search/cs?searchtype=author&query=Hofer%2C+M">Markus Hofer</a>, 
<a href="/search/cs?searchtype=author&query=Mussbah%2C+M">Mariam Mussbah</a>, 
<a href="/search/cs?searchtype=author&query=Caban%2C+S">Sebastian Caban</a>, 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+S">Stefan Schwarz</a>, 
<a href="/search/cs?searchtype=author&query=Zemen%2C+T">Thomas Zemen</a>, 
<a href="/search/cs?searchtype=author&query=Mecklenbr%C3%A4uker%2C+C+F">Christoph F. Mecklenbr&#xe4;uker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE International Conference on Communications (ICC), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09000" title="Abstract">arXiv:2311.09000</a> (replaced) [<a href="/pdf/2311.09000" title="Download PDF">pdf</a>, <a href="/format/2311.09000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and  Correction of LLM Output
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+R+G">Revanth Gangi Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Mujahid%2C+Z+M">Zain Muhammad Mujahid</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Arnav Arora</a>, 
<a href="/search/cs?searchtype=author&query=Rubashevskii%2C+A">Aleksandr Rubashevskii</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+J">Jiahui Geng</a>, 
<a href="/search/cs?searchtype=author&query=Afzal%2C+O+M">Osama Mohammed Afzal</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Borenstein%2C+N">Nadav Borenstein</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+A">Aditya Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09051" title="Abstract">arXiv:2311.09051</a> (replaced) [<a href="/pdf/2311.09051" title="Download PDF">pdf</a>, <a href="/format/2311.09051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional Finite Element curl div Complexes and Application to Quad  Curl Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+X">Xuehai Huang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09093" title="Abstract">arXiv:2311.09093</a> (replaced) [<a href="/pdf/2311.09093" title="Download PDF">pdf</a>, <a href="/format/2311.09093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of Computer Vision in Autonomous Vehicles: Methods,  Challenges and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xingshuai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cappuccio%2C+M+L">Massimiliano L. Cappuccio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09174" title="Abstract">arXiv:2311.09174</a> (replaced) [<a href="/pdf/2311.09174" title="Download PDF">pdf</a>, <a href="/format/2311.09174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AbsPyramid: Benchmarking the Abstraction Ability of Language Models with  a Unified Entailment Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haochen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tianqing Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sehyun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item410">Cross-lists</a></li>
<li><a href="#item451">Replacements</a></li>
</ul>
<small>[ total of 637 entries:  <b>1-637</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2311">2311</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
