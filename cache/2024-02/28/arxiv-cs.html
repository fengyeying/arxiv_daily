<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Mon 26 Feb 24  to  Tue 27 Feb 24, announced Wed, 28 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item439">Cross-lists</a></li>
<li><a href="#item494">Replacements</a></li>
</ul>
<small>[ total of 754 entries:  <b>1-754</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Wed, 28 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16853" title="Abstract">arXiv:2402.16853</a> [<a href="/pdf/2402.16853" title="Download PDF">pdf</a>, <a href="/format/2402.16853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PyRQA - Conducting Recurrence Quantification Analysis on Very Long Time  Series Efficiently
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rawald%2C+T">Tobias Rawald</a>, 
<a href="/search/cs?searchtype=author&query=Sips%2C+M">Mike Sips</a>, 
<a href="/search/cs?searchtype=author&query=Marwan%2C+N">Norbert Marwan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computers &amp; Geosciences, 104, 101-108 (2017)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">PyRQA is a software package that efficiently conducts recurrence
quantification analysis (RQA) on time series consisting of more than one
million data points. RQA is a method from non-linear time series analysis that
quantifies the recurrent behaviour of systems. Existing implementations to RQA
are not capable of analysing such very long time series at all or require large
amounts of time to calculate the quantitative measures. PyRQA overcomes their
limitations by conducting the RQA computations in a highly parallel manner.
Building on the OpenCL framework, PyRQA leverages the computing capabilities of
a variety of parallel hardware architectures, such as GPUs. The underlying
computing approach partitions the RQA computations and enables to employ
multiple compute devices at the same time. The goal of this publication is to
demonstrate the features and the runtime efficiency of PyRQA. For this purpose
we employ a real-world example, comparing the dynamics of two climatological
time series, and a synthetic example, reducing the runtime regarding the
analysis of a series consisting of over one million data points from almost
eight hours using state-of-the-art RQA software to roughly 69 seconds using
PyRQA.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16855" title="Abstract">arXiv:2402.16855</a> [<a href="/pdf/2402.16855" title="Download PDF">pdf</a>, <a href="/format/2402.16855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MB-RACS: Measurement-Bounds-based Rate-Adaptive Image Compressed Sensing  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yujun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Naiqi Li</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Baoyi An</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shu-Tao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaowei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Conventional compressed sensing (CS) algorithms typically apply a uniform
sampling rate to different image blocks. A more strategic approach could be to
allocate the number of measurements adaptively, based on each image block's
complexity. In this paper, we propose a Measurement-Bounds-based Rate-Adaptive
Image Compressed Sensing Network (MB-RACS) framework, which aims to adaptively
determine the sampling rate for each image block in accordance with traditional
measurement bounds theory. Moreover, since in real-world scenarios statistical
information about the original image cannot be directly obtained, we suggest a
multi-stage rate-adaptive sampling strategy. This strategy sequentially adjusts
the sampling ratio allocation based on the information gathered from previous
samplings. We formulate the multi-stage rate-adaptive sampling as a convex
optimization problem and address it using a combination of Newton's method and
binary search techniques. Additionally, we enhance our decoding process by
incorporating skip connections between successive iterations to facilitate a
richer transmission of feature information across iterations. Our experiments
demonstrate that the proposed MB-RACS method surpasses current leading methods,
with experimental evidence also underscoring the effectiveness of each module
within our proposed framework.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16857" title="Abstract">arXiv:2402.16857</a> [<a href="/pdf/2402.16857" title="Download PDF">pdf</a>, <a href="/format/2402.16857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel method to compute the contact surface area between an organ and  cancer tissue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bulanti%2C+A">Alessandra Bulanti</a>, 
<a href="/search/cs?searchtype=author&query=Carf%C3%AC%2C+A">Alessandro Carf&#xec;</a>, 
<a href="/search/cs?searchtype=author&query=Traverso%2C+P">Paolo Traverso</a>, 
<a href="/search/cs?searchtype=author&query=Terrone%2C+C">Carlo Terrone</a>, 
<a href="/search/cs?searchtype=author&query=Mastrogiovanni%2C+F">Fulvio Mastrogiovanni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>

</div>
<p class="mathjax">With "contact surface area" (CSA) we refers to the area of contact between a
tumor and an organ. This indicator has been identified as a predictive factor
for surgical peri-operative parameters, particularly in the context of kidney
cancer. However, state-of-the-art algorithms for computing the CSA rely on
assumptions about the tumor shape and require manual human annotation. In this
study, we introduce an innovative method that relies on 3D reconstructions of
tumors and organs to provide an accurate and objective estimate of the CSA. Our
approach consists of a segmentation protocol for reconstructing organs and
tumors from Computed Tomography (CT) images and an algorithm leveraging the
reconstructed meshes to compute the CSA. With the aim to contributing to the
literature with replicable results, we provide an open-source implementation of
our algorithm, along with an easy-to-use graphical user interface to support
its adoption and widespread use. We evaluated the accuracy of our method using
both a synthetic dataset and reconstructions of 87 real tumor-organ pairs.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16858" title="Abstract">arXiv:2402.16858</a> [<a href="/pdf/2402.16858" title="Download PDF">pdf</a>, <a href="/format/2402.16858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pragmatic Goal-Oriented Communications under Semantic-Effectiveness  Channel Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%BCttebr%C3%A4ucker%2C+T">Tom&#xe1;s H&#xfc;ttebr&#xe4;ucker</a>, 
<a href="/search/cs?searchtype=author&query=Sana%2C+M">Mohamed Sana</a>, 
<a href="/search/cs?searchtype=author&query=Strinati%2C+E+C">Emilio Calvanese Strinati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in 2024 IEEE Consumer Communications and Networking Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In forthcoming AI-assisted 6G networks, integrating semantic, pragmatic, and
goal-oriented communication strategies becomes imperative. This integration
will enable sensing, transmission, and processing of exclusively pertinent task
data, ensuring conveyed information possesses understandable, pragmatic
semantic significance, aligning with destination needs and goals. Without
doubt, no communication is error free. Within this context, besides errors
stemming from typical wireless communication dynamics, potential distortions
between transmitter-intended and receiver-interpreted meanings can emerge due
to limitations in semantic processing capabilities, as well as language and
knowledge representation disparities between transmitters and receivers. The
main contribution of this paper is two-fold. First, it proposes and details a
novel mathematical modeling of errors stemming from language mismatches at both
semantic and effectiveness levels. Second, it provides a novel algorithmic
solution to counteract these types of errors which leverages optimal transport
theory. Our numerical results show the potential of the proposed mechanism to
compensate for language mismatches, thereby enhancing the attainability of
reliable communication under noisy communication environments.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16860" title="Abstract">arXiv:2402.16860</a> [<a href="/pdf/2402.16860" title="Download PDF">pdf</a>, <a href="/format/2402.16860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Mars Image Content-Based Search with Interpretable Machine  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vasu%2C+B">Bhavan Vasu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Steven Lu</a>, 
<a href="/search/cs?searchtype=author&query=Dunkel%2C+E">Emily Dunkel</a>, 
<a href="/search/cs?searchtype=author&query=Wagstaff%2C+K+L">Kiri L. Wagstaff</a>, 
<a href="/search/cs?searchtype=author&query=Grimes%2C+K">Kevin Grimes</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+M">Michael McAuley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">The NASA Planetary Data System (PDS) hosts millions of images of planets,
moons, and other bodies collected throughout many missions. The ever-expanding
nature of data and user engagement demands an interpretable content
classification system to support scientific discovery and individual curiosity.
In this paper, we leverage a prototype-based architecture to enable users to
understand and validate the evidence used by a classifier trained on images
from the Mars Science Laboratory (MSL) Curiosity rover mission. In addition to
providing explanations, we investigate the diversity and correctness of
evidence used by the content-based classifier. The work presented in this paper
will be deployed on the PDS Image Atlas, replacing its non- interpretable
counterpart.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16861" title="Abstract">arXiv:2402.16861</a> [<a href="/pdf/2402.16861" title="Download PDF">pdf</a>, <a href="/format/2402.16861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Tuning Network Control Architectures with Joint Sensor and Actuator  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ganapathy%2C+K">Karthik Ganapathy</a>, 
<a href="/search/eess?searchtype=author&query=Shames%2C+I">Iman Shames</a>, 
<a href="/search/eess?searchtype=author&query=de+Badyn%2C+M+H">Mathias Hudoba de Badyn</a>, 
<a href="/search/eess?searchtype=author&query=Summers%2C+T">Tyler Summers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, submitted to IEEE-TCNS. arXiv admin note: text overlap with <a href="/abs/2301.06699">arXiv:2301.06699</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We formulate a mathematical framework for designing a self-tuning network
control architecture, and propose a computationally-feasible greedy algorithm
for online architecture optimization. In this setting, the locations of active
sensors and actuators in the network, as well as the feedback control policy
are jointly adapted using all available information about the network states
and dynamics to optimize a performance criterion. We show that the case with
full-state feedback can be solved with dynamic programming, and in the
linear-quadratic setting, the optimal cost functions and policies are piecewise
quadratic and piecewise linear, respectively. Our framework is extended for
joint sensor and actuator selection for dynamic output feedback control with
both control performance and architecture costs. For large networks where
exhaustive architecture search is prohibitive, we describe a greedy heuristic
for actuator selection and propose a greedy swapping algorithm for joint sensor
and actuator selection. Via numerical experiments, we demonstrate a dramatic
performance improvement of greedy self-tuning architectures over fixed
architectures. Our general formulation provides an extremely rich and
challenging problem space with opportunities to apply a wide variety of
approximation methods from stochastic control, system identification,
reinforcement learning, and static architecture design for practical
model-based control.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16862" title="Abstract">arXiv:2402.16862</a> [<a href="/pdf/2402.16862" title="Download PDF">pdf</a>, <a href="/format/2402.16862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Common Randomness, No-signaling and Information Structure in  Decentralized Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhingra%2C+A">Apurva Dhingra</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A+A">Ankur A. Kulkarni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This work revisits the no-signaling condition for decentralized information
structures. We produce examples to show that within the no-signaling polytope
exist strategies that cannot be achieved by passive common randomness but
instead require agents to either share their observations with a mediator or
communicate directly with each other. This poses a question mark on whether the
no-signaling condition truly captures the decentralized information structure
in the strictest sense.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16863" title="Abstract">arXiv:2402.16863</a> [<a href="/pdf/2402.16863" title="Download PDF">pdf</a>, <a href="/ps/2402.16863" title="Download PostScript">ps</a>, <a href="/format/2402.16863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Inspired Chaotic Salp Swarm Optimization for Dynamic  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pathak%2C+S">Sanjai Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Mani%2C+A">Ashish Mani</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+M">Mayank Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+A">Amlan Chatterjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 2 figures, 1 algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Many real-world problems are dynamic optimization problems that are unknown
beforehand. In practice, unpredictable events such as the arrival of new jobs,
due date changes, and reservation cancellations, changes in parameters or
constraints make the search environment dynamic. Many algorithms are designed
to deal with stationary optimization problems, but these algorithms do not face
dynamic optimization problems or manage them correctly. Although some
optimization algorithms are proposed to deal with the changes in dynamic
environments differently, there are still areas of improvement in existing
algorithms due to limitations or drawbacks, especially in terms of locating and
following the previously identified optima. With this in mind, we studied a
variant of SSA known as QSSO, which integrates the principles of quantum
computing. An attempt is made to improve the overall performance of standard
SSA to deal with the dynamic environment effectively by locating and tracking
the global optima for DOPs. This work is an extension of the proposed new
algorithm QSSO, known as the Quantum-inspired Chaotic Salp Swarm Optimization
(QCSSO) Algorithm, which details the various approaches considered while
solving DOPs. A chaotic operator is employed with quantum computing to respond
to change and guarantee to increase individual searchability by improving
population diversity and the speed at which the algorithm converges. We
experimented by evaluating QCSSO on a well-known generalized dynamic benchmark
problem (GDBG) provided for CEC 2009, followed by a comparative numerical study
with well-regarded algorithms. As promised, the introduced QCSSO is discovered
as the rival algorithm for DOPs.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16864" title="Abstract">arXiv:2402.16864</a> [<a href="/pdf/2402.16864" title="Download PDF">pdf</a>, <a href="/format/2402.16864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Resource Allocation and Trajectory Design for Resilient Multi-UAV  Communication Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+L">Linghui Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+P">Peihao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jianxin Liao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyu Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Wireless Communications Letters, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In contrast to terrestrial wireless networks, dynamic Unmanned Aerial Vehicle
(UAV) networks are susceptible to unexpected link failures arising from UAV
breakdowns or the depletion of its batteries. Drastic user rate fluctuations
and sum rate drops can occur due to the unexpected UAV link failures. Previous
research has focused primarily on re-establishing these links to maintain
service continuity, while neglecting overall system performance, including sum
rate and user rate fluctuations. This letter proposes a resilient UAV network
design utilizing the modern portfolio theory (MPT), which jointly optimizes the
bandwidth allocation, UAV-user association, and UAV trajectories to enhance the
overall service stability. Specifically, the design incorporates a novel
utility function based on MPT to achieve a better balance between the sum rate
and user rate fluctuations. To solve the joint optimization problem, we propose
an iterative algorithm based on alternating optimization (AO) and successive
convex approximation (SCA). Simulation results show that our scheme outperforms
the other two baselines in terms of sum rate and user rate fluctuations.
Furthermore, the resilience requirement in terms of sum rate, user rate
fluctuations and user fairness can be achieved by flexibly tuning weight factor
in our proposed algorithm.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16866" title="Abstract">arXiv:2402.16866</a> [<a href="/pdf/2402.16866" title="Download PDF">pdf</a>, <a href="/format/2402.16866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computation Rate Maximization for Wireless Powered Edge Computing With  Multi-User Cooperation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+B">Bo Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qianying Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+M">Min Wei</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Z">Zheyan Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenbo Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Open Journal of the Communications Society
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The combination of mobile edge computing (MEC) and radio frequency-based
wireless power transfer (WPT) presents a promising technique for providing
sustainable energy supply and computing services at the network edge. This
study considers a wireless-powered mobile edge computing system that includes a
hybrid access point (HAP) equipped with a computing unit and multiple Internet
of Things (IoT) devices. In particular, we propose a novel muti-user
cooperation scheme to improve computation performance, where collaborative
clusters are dynamically formed. Each collaborative cluster comprises a source
device (SD) and an auxiliary device (AD), where the SD can partition the
computation task into various segments for local processing, offloading to the
HAP, and remote execution by the AD with the assistance of the HAP.
Specifically, we aims to maximize the weighted sum computation rate (WSCR) of
all the IoT devices in the network. This involves jointly optimizing
collaboration, time and data allocation among multiple IoT devices and the HAP,
while considering the energy causality property and the minimum data processing
requirement of each device. Initially, an optimization algorithm based on the
interior-point method is designed for time and data allocation. Subsequently, a
priority-based iterative algorithm is developed to search for a near-optimal
solution to the multi-user collaboration scheme. Finally, a deep learning-based
approach is devised to further accelerate the algorithm's operation, building
upon the initial two algorithms. Simulation results show that the performance
of the proposed algorithms is comparable to that of the exhaustive search
method, and the deep learning-based algorithm significantly reduces the
execution time of the algorithm.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16868" title="Abstract">arXiv:2402.16868</a> [<a href="/pdf/2402.16868" title="Download PDF">pdf</a>, <a href="/format/2402.16868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Codebook-enabled Generative End-to-end Semantic Communication Powered by  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+P">Peigen Ye</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yaping Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shumin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaodong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Shuguang Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Codebook-based generative semantic communication attracts increasing
attention, since only indices are required to be transmitted when the codebook
is shared between transmitter and receiver. However, due to the fact that the
semantic relations among code vectors are not necessarily related to the
distance of the corresponding code indices, the performance of the
codebook-enabled semantic communication system is susceptible to the channel
noise. Thus, how to improve the system robustness against the noise requires
careful design. This paper proposes a robust codebook-assisted image semantic
communication system, where semantic codec and codebook are first jointly
constructed, and then vector-to-index transformer is designed guided by the
codebook to eliminate the effects of channel noise, and achieve image
generation. Thanks to the assistance of the high-quality codebook to the
Transformer, the generated images at the receiver outperform those of the
compared methods in terms of visual perception. In the end, numerical results
and generated images demonstrate the advantages of the generative semantic
communication method over JPEG+LDPC and traditional joint source channel coding
(JSCC) methods.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16869" title="Abstract">arXiv:2402.16869</a> [<a href="/pdf/2402.16869" title="Download PDF">pdf</a>, <a href="/ps/2402.16869" title="Download PostScript">ps</a>, <a href="/format/2402.16869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Considering Fundamental Rights in the European Standardisation of  Artificial Intelligence: Nonsense or Strategic Alliance?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ho-Dac%2C+M">Marion Ho-Dac</a> (UA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the European context, both the EU AI Act proposal and the draft
Standardisation Request on safe and trustworthy AI link standardisation to
fundamental rights. However, these texts do not provide any guidelines that
specify and detail the relationship between AI standards and fundamental
rights, its meaning or implication. This chapter aims to clarify this critical
regulatory blind spot. The main issue tackled is whether the adoption of AI
harmonised standards, based on the future AI Act, should take into account
fundamental rights. In our view, the response is yes. The high risks posed by
certain AI systems relate in particular to infringements of fundamental rights.
Therefore, mitigating such risks involves fundamental rights considerations and
this is what future harmonised standards should reflect. At the same time,
valid criticisms of the European standardisation process have to be addressed.
Finally, the practical incorporation of fundamental rights considerations in
the ongoing European standardisation of AI systems is discussed.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16870" title="Abstract">arXiv:2402.16870</a> [<a href="/pdf/2402.16870" title="Download PDF">pdf</a>, <a href="/format/2402.16870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pioneering Deterministic Scheduling and Network Structure Optimization  for Time-Critical Computing Tasks in Industrial IoT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yujiao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yining Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huayu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qingmin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Renchao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Gang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F+R">F. Richard Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Industrial Internet of Things (IIoT) has become a critical technology to
accelerate the process of digital and intelligent transformation of industries.
As the cooperative relationship between smart devices in IIoT becomes more
complex, getting deterministic responses of IIoT periodic time-critical
computing tasks becomes a crucial and nontrivial problem. However, few current
works in cloud/edge/fog computing focus on this problem. This paper is a
pioneer to explore the deterministic scheduling and network structural
optimization problems for IIoT periodic time-critical computing tasks. We first
formulate the two problems and derive theorems to help quickly identify
computation and network resource sharing conflicts. Based on this, we propose a
deterministic scheduling algorithm, \textit{IIoTBroker}, which realizes
deterministic response for each IIoT task by optimizing the fine-grained
computation and network resources allocations, and a network optimization
algorithm, \textit{IIoTDeployer}, providing a cost-effective structural upgrade
solution for existing IIoT networks. Our methods are illustrated to be
cost-friendly, scalable, and deterministic response guaranteed with low
computation cost from our simulation results.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16871" title="Abstract">arXiv:2402.16871</a> [<a href="/pdf/2402.16871" title="Download PDF">pdf</a>, <a href="/ps/2402.16871" title="Download PostScript">ps</a>, <a href="/format/2402.16871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bike3S: A Tool for Bike Sharing Systems Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+A">Alberto Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Billhardt%2C+H">Holger Billhardt</a>, 
<a href="/search/cs?searchtype=author&query=Ossowski%2C+S">Sascha Ossowski</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez%2C+%C3%93">&#xd3;scar S&#xe1;nchez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Simulation 14(4), 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vehicle sharing systems are becoming increasingly popular. The effectiveness
of such systems depends, among other factors, on different strategic and
operational management decisions and policies, like the dimension of the fleet
or the distribution of vehicles. It is of foremost importance to be able to
anticipate and evaluate the potential effects of such strategies before they
can be successfully deployed. In this paper we present Bike3S, a simulator for
a station-based bike sharing system. The simulator performs semi-realistic
simulations of the operation of a bike sharing system and allows for evaluating
and testing different management decisions and strategies. In particular, the
simulator has been designed to test different station capacities, station
distributions, and balancing strategies. The simulator carries out microscopic
agent-based simulations, where users of different types can be defined that act
according to their individual goals and objectives which influences the overall
dynamics of the whole system.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16872" title="Abstract">arXiv:2402.16872</a> [<a href="/pdf/2402.16872" title="Download PDF">pdf</a>, <a href="/format/2402.16872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NFT1000: A Visual Text Dataset For Non-Fungible Token Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuxun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yunfei Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haowei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Li Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bing Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Weiming Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages,7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">With the rise of 'Metaverse' and 'Web3.0', NFT ( Non-Fungible Token ) has
emerged as a kind of pivotal digital asset, garnering significant attention. By
the end of November 2023, more than 1.4 billion NFT tokens have been minted
across various blockchain platforms. To effectively locate a satisfactory NFT
token, conducting searches within the extensive array of NFT data is essential.
The challenge in NFT retrieval is heightened due to the high degree of
similarity among different NFT tokens, in terms of regional and semantic
aspects. Achieving accurate and efficient retrieval within the large-scale,
highly similar NFT data presents a formidable challenge for both the academic
and industrial communities. In this paper, we will introduce a dataset named
'NFT Top1000 Visual Text Dataset'(henceforth, NFT1000), containing 7.56 million
image-text pairs, and being collected from 1000 most famous PFP NFT collections
by sales volume on the Ethereum blockchain. Based on the dataset, we test the
CLIP (Contrastive Language-Image Pretraining) models as a baseline.
Additionally, we also propose a concept of Comprehensive Variance Index (CVI in
short), which is a robust metric designed to assess the similarity and
retrieval difficulty of visual-text pairs data.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16873" title="Abstract">arXiv:2402.16873</a> [<a href="/pdf/2402.16873" title="Download PDF">pdf</a>, <a href="/format/2402.16873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handover Management through Reconfigurable Intelligent Surfaces for VLC  under Blockage Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palitharathna%2C+K+W+S">Kapila W. S. Palitharathna</a>, 
<a href="/search/cs?searchtype=author&query=Vegni%2C+A+M">Anna Maria Vegni</a>, 
<a href="/search/cs?searchtype=author&query=Diamantoulakis%2C+P+D">Panagiotis D. Diamantoulakis</a>, 
<a href="/search/cs?searchtype=author&query=Suraweera%2C+H+A">Himal A. Suraweera</a>, 
<a href="/search/cs?searchtype=author&query=Krikidis%2C+I">Ioannis Krikidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted to be presented as an invited paper at the 2024 IEEE International Symposium on Circuits and Systems (ISCAS 2024) conference. ISCAS 2024 will be held in Singapore from 19-22 May 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">In this paper, we consider an indoor visible light communication (VLC) system
with multiple "white" light emitting diodes serving to form overlapping
wireless communication cells. In order to maintain seamless connectivity to
mobile users, a handover procedure should be implemented. In particular,
practical conditions such as blockages due to obstacles inside the room
environment and the mobility of users can affect direct VLC connectivity. The
use of reconfigurable intelligent surfaces (RISs) in optical wireless systems
allows to exploit non-direct connectivity links, thus providing efficient
communication links. In this paper, we present a proactive handover mechanism
that exploits the presence of a RIS, in order to redirect the communication
links in case of blockages. The proposed approach has been implemented both in
hard and soft modes and assessed in terms of achievable data rate and handover
latency for a user walking in a given reference room at different user speeds
and blockage conditions. Our presented results and comparisons with
conventional handover methods (i.e., without RIS) are helpful in showing the
superiority of the presented algorithm.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16874" title="Abstract">arXiv:2402.16874</a> [<a href="/pdf/2402.16874" title="Download PDF">pdf</a>, <a href="/ps/2402.16874" title="Download PostScript">ps</a>, <a href="/format/2402.16874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Retrieval Processes for Language Generation with Augmented  Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghali%2C+J+P+E">Julien Pierre Edmond Ghali</a>, 
<a href="/search/cs?searchtype=author&query=Shima%2C+K">Kosuke Shima</a>, 
<a href="/search/cs?searchtype=author&query=Moriyama%2C+K">Koichi Moriyama</a>, 
<a href="/search/cs?searchtype=author&query=Mutoh%2C+A">Atsuko Mutoh</a>, 
<a href="/search/cs?searchtype=author&query=Inuzuka%2C+N">Nobuhiro Inuzuka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 10 annexes, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In the rapidly changing world of smart technology, searching for documents
has become more challenging due to the rise of advanced language models. These
models sometimes face difficulties, like providing inaccurate information,
commonly known as "hallucination." This research focuses on addressing this
issue through Retrieval-Augmented Generation (RAG), a technique that guides
models to give accurate responses based on real facts. To overcome scalability
issues, the study explores connecting user queries with sophisticated language
models such as BERT and Orca2, using an innovative query optimization process.
The study unfolds in three scenarios: first, without RAG, second, without
additional assistance, and finally, with extra help. Choosing the compact yet
efficient Orca2 7B model demonstrates a smart use of computing resources. The
empirical results indicate a significant improvement in the initial language
model's performance under RAG, particularly when assisted with prompts
augmenters. Consistency in document retrieval across different encodings
highlights the effectiveness of using language model-generated queries. The
introduction of UMAP for BERT further simplifies document retrieval while
maintaining strong results.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16875" title="Abstract">arXiv:2402.16875</a> [<a href="/pdf/2402.16875" title="Download PDF">pdf</a>, <a href="/format/2402.16875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can we predict QPP? An approach based on multivariate outliers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chifu%2C+A">Adrian-Gabriel Chifu</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%A9jean%2C+S">S&#xe9;bastien D&#xe9;jean</a>, 
<a href="/search/cs?searchtype=author&query=Garouani%2C+M">Moncef Garouani</a>, 
<a href="/search/cs?searchtype=author&query=Mothe%2C+J">Josiane Mothe</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz%2C+D">Di&#xe9;go Ortiz</a>, 
<a href="/search/cs?searchtype=author&query=Ullah%2C+M+Z">Md Zia Ullah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Query performance prediction (QPP) aims to forecast the effectiveness of a
search engine across a range of queries and documents. While state-of-the-art
predictors offer a certain level of precision, their accuracy is not flawless.
Prior research has recognized the challenges inherent in QPP but often lacks a
thorough qualitative analysis. In this paper, we delve into QPP by examining
the factors that influence the predictability of query performance accuracy. We
propose the working hypothesis that while some queries are readily predictable,
others present significant challenges. By focusing on outliers, we aim to
identify the queries that are particularly challenging to predict. To this end,
we employ multivariate outlier detection method. Our results demonstrate the
effectiveness of this approach in identifying queries on which QPP do not
perform well, yielding less reliable predictions. Moreover, we provide evidence
that excluding these hard-to-predict queries from the analysis significantly
enhances the overall accuracy of QPP.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16876" title="Abstract">arXiv:2402.16876</a> [<a href="/pdf/2402.16876" title="Download PDF">pdf</a>, <a href="/ps/2402.16876" title="Download PostScript">ps</a>, <a href="/format/2402.16876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advanced Academic Team Worker Recommendation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Collaborator recommendation is an important task in academic domain. Most of
the existing approaches have the assumption that the recommendation system only
need to recommend a specific researcher for the task. However, academic
successes can be owed to productive collaboration of a whole academic team. In
this work, we propose a new task: academic team worker recommendation: with a
given status: student, assistant professor or prime professor, research
interests and specific task, we can recommend an academic team formed as (prime
professor, assistant professor, student). For this task, we propose a model
CQBG-R(Citation-Query Blended Graph-Ranking). The key ideas is to combine the
context of the query and the papers with the graph topology to form a new
graph(CQBG), which can target at the research interests and the specific
research task for this time. The experiment results show the effectiveness of
the proposed method.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16877" title="Abstract">arXiv:2402.16877</a> [<a href="/pdf/2402.16877" title="Download PDF">pdf</a>, <a href="/format/2402.16877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Augmented Exercise Retrieval for Personalized  Language Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+A">Austin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Monroe%2C+W">Will Monroe</a>, 
<a href="/search/cs?searchtype=author&query=Bicknell%2C+K">Klinton Bicknell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at Learning Analytics and Knowledge 2024. 11 pages, 4 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the problem of zero-shot exercise retrieval in the context of online
language learning, to give learners the ability to explicitly request
personalized exercises via natural language. Using real-world data collected
from language learners, we observe that vector similarity approaches poorly
capture the relationship between exercise content and the language that
learners use to express what they want to learn. This semantic gap between
queries and content dramatically reduces the effectiveness of general-purpose
retrieval models pretrained on large scale information retrieval datasets like
MS MARCO. We leverage the generative capabilities of large language models to
bridge the gap by synthesizing hypothetical exercises based on the learner's
input, which are then used to search for relevant exercises. Our approach,
which we call mHyER, overcomes three challenges: (1) lack of relevance labels
for training, (2) unrestricted learner input content, and (3) low semantic
similarity between input and retrieval candidates. mHyER outperforms several
strong baselines on two novel benchmarks created from crowdsourced data and
publicly available data.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16878" title="Abstract">arXiv:2402.16878</a> [<a href="/pdf/2402.16878" title="Download PDF">pdf</a>, <a href="/format/2402.16878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mercer%2C+J">Johnathan Mercer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Formal mathematics is the discipline of translating mathematics into a
programming language in which any statement can be unequivocally checked by a
computer. Mathematicians and computer scientists have spent decades of
painstaking formalization efforts developing languages such as Coq, HOL, and
Lean. Machine learning research has converged on these formal math corpora and
given rise to an assortment of methodologies to aid in interactive and
automated theorem proving. However, these papers have primarily focused on one
method, for one proof task, in one language. This paper introduces EvoGPT-f: a
novel evolutionary framework for the first systematic quantitative analysis of
the differential machine learnability of five formal math corpora (Lean 3, Lean
4, Coq, HOL 4, HOL Light) using four tokenization methods (character,
word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not
put to rest the question of the "best" or "easiest" language to learn. Rather,
this framework and preliminary findings begin to illuminate the differential
machine learnability of these languages, offering a foundation to forge more
systematic quantitative and qualitative comparative research across
communities.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16880" title="Abstract">arXiv:2402.16880</a> [<a href="/pdf/2402.16880" title="Download PDF">pdf</a>, <a href="/format/2402.16880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BESA: Pruning Large Language Models with Blockwise Parameter-Efficient  Sparsity Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mengzhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shitao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+F">Fengwei An</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated outstanding performance in
various tasks, such as text summarization, text question-answering, and etc.
While their performance is impressive, the computational footprint due to their
vast number of parameters can be prohibitive. Existing solutions such as
SparseGPT and Wanda attempt to alleviate this issue through weight pruning.
However, their layer-wise approach results in significant perturbation to the
model's output and requires meticulous hyperparameter tuning, such as the
pruning rate, which can adversely affect overall model performance. To address
this, this paper introduces a novel LLM pruning technique dubbed blockwise
parameter-efficient sparsity allocation (BESA) by applying a blockwise
reconstruction loss. In contrast to the typical layer-wise pruning techniques,
BESA is characterized by two distinctive attributes: i) it targets the overall
pruning error with respect to individual transformer blocks, and ii) it
allocates layer-specific sparsity in a differentiable manner, both of which
ensure reduced performance degradation after pruning. Our experiments show that
BESA achieves state-of-the-art performance, efficiently pruning LLMs like
LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five
hours. Code is available at
\href{https://github.com/OpenGVLab/LLMPrune-BESA}{here}.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16886" title="Abstract">arXiv:2402.16886</a> [<a href="/pdf/2402.16886" title="Download PDF">pdf</a>, <a href="/ps/2402.16886" title="Download PostScript">ps</a>, <a href="/format/2402.16886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using text embedding models and vector databases as text classifiers  with the example of medical data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goel%2C+R">Rishabh Goel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, All robustness tests are in a linked pdf
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The advent of Large Language Models (LLMs) is promising and has found
application in numerous fields, but as it often is with the medical field, the
bar is typically quite high [5]. In tandem with LLMs, vector embedding models
and vector databases provide a robust way of expressing numerous modes of data
that are easily digestible by typical machine learning models. Along with the
ease of adding information, knowledge, and data to these vector databases, they
provide a compelling reason to apply them in numerous fields where the task of
retrieving information is typically done by humans. Researchers at Google have
developed a clear alternative model, Med-PaLM [6] specifically designed to
match a clinician's level of accuracy when it comes to medical knowledge. When
training classifiers, and developing models, it is imperative to maintain
factuality and reduce bias [4]. Here, we explore the use of vector databases
and embedding models as a means of encoding, and classifying text with the
example and application in the field of medicine. We show the robustness of
these tools depends heavily on the sparsity of the data presented, and even
with low amounts of data in the vector database itself, the vector database
does a good job at classifying data [9]. Using various LLMs to generate the
medical data, we also understand the limitations of the medical knowledge of
these models and encourage further expert medical review of our testing data.
By using vector databases to classify a clinician's notes on a patient
presented with a certain ailment, we understand the limitations of such
methods, but also the promise of their prospective use and with continued
testing and experimentation, hope to explore a unique use case of vector
databases and embedding models.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16887" title="Abstract">arXiv:2402.16887</a> [<a href="/pdf/2402.16887" title="Download PDF">pdf</a>, <a href="/format/2402.16887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Intelligence for Complex Network: Potential, Methodology and  Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jingtao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zihan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruikun Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Piao%2C+J">Jinghua Piao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huandong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiazhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages, 4 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Complex networks pervade various real-world systems, from the natural
environment to human societies. The essence of these networks is in their
ability to transition and evolve from microscopic disorder-where network
topology and node dynamics intertwine-to a macroscopic order characterized by
certain collective behaviors. Over the past two decades, complex network
science has significantly enhanced our understanding of the statistical
mechanics, structures, and dynamics underlying real-world networks. Despite
these advancements, there remain considerable challenges in exploring more
realistic systems and enhancing practical applications. The emergence of
artificial intelligence (AI) technologies, coupled with the abundance of
diverse real-world network data, has heralded a new era in complex network
science research. This survey aims to systematically address the potential
advantages of AI in overcoming the lingering challenges of complex network
research. It endeavors to summarize the pivotal research problems and provide
an exhaustive review of the corresponding methodologies and applications.
Through this comprehensive survey-the first of its kind on AI for complex
networks-we expect to provide valuable insights that will drive further
research and advancement in this interdisciplinary field.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16888" title="Abstract">arXiv:2402.16888</a> [<a href="/pdf/2402.16888" title="Download PDF">pdf</a>, <a href="/format/2402.16888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chaotic attractor reconstruction using small reservoirs - the influence  of topology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaurigue%2C+L">Lina Jaurigue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Emerging Technologies (cs.ET); Mathematical Physics (math-ph); Chaotic Dynamics (nlin.CD); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Forecasting timeseries based upon measured data is needed in a wide range of
applications and has been the subject of extensive research. A particularly
challenging task is the forecasting of timeseries generated by chaotic
dynamics. In recent years reservoir computing has been shown to be an effective
method of forecasting chaotic dynamics and reconstructing chaotic attractors
from data. In this work strides are made toward smaller and lower complexity
reservoirs with the goal of improved hardware implementability and more
reliable production of adequate surrogate models. We show that a reservoir of
uncoupled nodes more reliably produces long term timeseries predictions than
complex reservoir topologies. We then link the improved attractor
reconstruction of the uncoupled reservoir with smaller spectral radii of the
resulting surrogate systems. These results indicate that, the node degree plays
an important role in determining whether the desired dynamics will be stable in
the autonomous surrogate system which is attained via closed-loop operation of
the trained reservoir. In terms of hardware implementability, uncoupled nodes
would allow for greater freedom in the hardware architecture because no complex
coupling setups are needed and because, for uncoupled nodes, the system
response is equivalent for space and time multiplexing.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16889" title="Abstract">arXiv:2402.16889</a> [<a href="/pdf/2402.16889" title="Download PDF">pdf</a>, <a href="/format/2402.16889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Models are Self-Watermarked: Declaring Model Authentication  through Re-Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Desu%2C+A">Aditya Desu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuanli He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiongkai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">As machine- and AI-generated content proliferates, protecting the
intellectual property of generative models has become imperative, yet verifying
data ownership poses formidable challenges, particularly in cases of
unauthorized reuse of generated data. The challenge of verifying data ownership
is further amplified by using Machine Learning as a Service (MLaaS), which
often functions as a black-box system.
<br />Our work is dedicated to detecting data reuse from even an individual sample.
Traditionally, watermarking has been leveraged to detect AI-generated content.
However, unlike watermarking techniques that embed additional information as
triggers into models or generated content, potentially compromising output
quality, our approach identifies latent fingerprints inherently present within
the outputs through re-generation. We propose an explainable verification
procedure that attributes data ownership through re-generation, and further
amplifies these fingerprints in the generative models through iterative data
re-generation. This methodology is theoretically grounded and demonstrates
viability and robustness using recent advanced text and image generative
models. Our methodology is significant as it goes beyond protecting the
intellectual property of APIs and addresses important issues such as the spread
of misinformation and academic misconduct. It provides a useful tool to ensure
the integrity of sources and authorship, expanding its application in different
scenarios where authenticity and ownership verification are essential.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16891" title="Abstract">arXiv:2402.16891</a> [<a href="/pdf/2402.16891" title="Download PDF">pdf</a>, <a href="/format/2402.16891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+X">Xialiang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mingxuan Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vehicle routing problems (VRPs), which can be found in numerous real-world
applications, have been an important research topic for several decades.
Recently, the neural combinatorial optimization (NCO) approach that leverages a
learning-based model to solve VRPs without manual algorithm design has gained
substantial attention. However, current NCO methods typically require building
one model for each routing problem, which significantly hinders their practical
application for real-world industry problems with diverse attributes. In this
work, we make the first attempt to tackle the crucial challenge of
cross-problem generalization. In particular, we formulate VRPs as different
combinations of a set of shared underlying attributes and solve them
simultaneously via a single model through attribute composition. In this way,
our proposed model can successfully solve VRPs with unseen attribute
combinations in a zero-shot generalization manner. Extensive experiments are
conducted on eleven VRP variants, benchmark datasets, and industry logistic
scenarios. The results show that the unified model demonstrates superior
performance in the eleven VRPs, reducing the average gap to around 5% from over
20% in the existing approach and achieving a significant performance boost on
benchmark datasets as well as a real-world logistics application.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16893" title="Abstract">arXiv:2402.16893</a> [<a href="/pdf/2402.16893" title="Download PDF">pdf</a>, <a href="/format/2402.16893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented  Generation (RAG)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shenglai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiankun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yue Xing</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Retrieval-augmented generation (RAG) is a powerful technique to facilitate
language model with proprietary and private data, where data privacy is a
pivotal concern. Whereas extensive research has demonstrated the privacy risks
of large language models (LLMs), the RAG technique could potentially reshape
the inherent behaviors of LLM generation, posing new privacy issues that are
currently under-explored. In this work, we conduct extensive empirical studies
with novel attack methods, which demonstrate the vulnerability of RAG systems
on leaking the private retrieval database. Despite the new risk brought by RAG
on the retrieval data, we further reveal that RAG can mitigate the leakage of
the LLMs' training data. Overall, we provide new insights in this paper for
privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG
systems builders. Our code is available at
https://github.com/phycholosogy/RAG-privacy.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16896" title="Abstract">arXiv:2402.16896</a> [<a href="/pdf/2402.16896" title="Download PDF">pdf</a>, <a href="/format/2402.16896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Trojan Signatures in Large Language Models of Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hussain%2C+A">Aftab Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Rabin%2C+M+R+I">Md Rafiqul Islam Rabin</a>, 
<a href="/search/cs?searchtype=author&query=Alipour%2C+M+A">Mohammad Amin Alipour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Trojan signatures, as described by Fields et al. (2021), are noticeable
differences in the distribution of the trojaned class parameters (weights) and
the non-trojaned class parameters of the trojaned model, that can be used to
detect the trojaned model. Fields et al. (2021) found trojan signatures in
computer vision classification tasks with image models, such as, Resnet,
WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in
the classifier layer parameters of large language models of source code.
<br />Our results suggest that trojan signatures could not generalize to LLMs of
code. We found that trojaned code models are stubborn, even when the models
were poisoned under more explicit settings (finetuned with pre-trained weights
frozen). We analyzed nine trojaned models for two binary classification tasks:
clone and defect detection. To the best of our knowledge, this is the first
work to examine weight-based trojan signature revelation techniques for
large-language models of code and furthermore to demonstrate that detecting
trojans only from the weights in such models is a hard problem.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16897" title="Abstract">arXiv:2402.16897</a> [<a href="/pdf/2402.16897" title="Download PDF">pdf</a>, <a href="/format/2402.16897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliable Conflictive Multi-View Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Cai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+J">Jiajun Si</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Z">Ziyu Guan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiyue Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages and to be appeared in AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-view learning aims to combine multiple features to achieve more
comprehensive descriptions of data. Most previous works assume that multiple
views are strictly aligned. However, real-world multi-view data may contain
low-quality conflictive instances, which show conflictive information in
different views. Previous methods for this problem mainly focus on eliminating
the conflictive data instances by removing them or replacing conflictive views.
Nevertheless, real-world applications usually require making decisions for
conflictive instances rather than only eliminating them. To solve this, we
point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which
requires the model to provide decision results and attached reliabilities for
conflictive multi-view data. We develop an Evidential Conflictive Multi-view
Learning (ECML) method for this problem. ECML first learns view-specific
evidence, which could be termed as the amount of support to each category
collected from data. Then, we can construct view-specific opinions consisting
of decision results and reliability. In the multi-view fusion stage, we propose
a conflictive opinion aggregation strategy and theoretically prove this
strategy can exactly model the relation of multi-view common and view-specific
reliabilities. Experiments performed on 6 datasets verify the effectiveness of
ECML.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16898" title="Abstract">arXiv:2402.16898</a> [<a href="/pdf/2402.16898" title="Download PDF">pdf</a>, <a href="/format/2402.16898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex  Influence Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+N">Nguyen Do</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+T">Tanmoy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+C">Chen Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Thai%2C+M+T">My T. Thai</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Artificial Intelligence and Statistics
  (AISTATS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">Multiplex influence maximization (MIM) asks us to identify a set of seed
users such as to maximize the expected number of influenced users in a
multiplex network. MIM has been one of central research topics, especially in
nowadays social networking landscape where users participate in multiple online
social networks (OSNs) and their influences can propagate among several OSNs
simultaneously. Although there exist a couple combinatorial algorithms to MIM,
learning-based solutions have been desired due to its generalization ability to
heterogeneous networks and their diversified propagation characteristics. In
this paper, we introduce MIM-Reasoner, coupling reinforcement learning with
probabilistic graphical model, which effectively captures the complex
propagation process within and between layers of a given multiplex network,
thereby tackling the most challenging problem in MIM. We establish a
theoretical guarantee for MIM-Reasoner as well as conduct extensive analyses on
both synthetic and real-world datasets to validate our MIM-Reasoner's
performance.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16899" title="Abstract">arXiv:2402.16899</a> [<a href="/pdf/2402.16899" title="Download PDF">pdf</a>, <a href="/format/2402.16899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A prior Estimates for Deep Residual Network in Continuous-time  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Shuyu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qixuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+F">Fei Wen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tao Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep reinforcement learning excels in numerous large-scale practical
applications. However, existing performance analyses ignores the unique
characteristics of continuous-time control problems, is unable to directly
estimate the generalization error of the Bellman optimal loss and require a
boundedness assumption. Our work focuses on continuous-time control problems
and proposes a method that is applicable to all such problems where the
transition function satisfies semi-group and Lipschitz properties. Under this
method, we can directly analyze the \emph{a priori} generalization error of the
Bellman optimal loss. The core of this method lies in two transformations of
the loss function. To complete the transformation, we propose a decomposition
method for the maximum operator. Additionally, this analysis method does not
require a boundedness assumption. Finally, we obtain an \emph{a priori}
generalization error without the curse of dimensionality.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16902" title="Abstract">arXiv:2402.16902</a> [<a href="/pdf/2402.16902" title="Download PDF">pdf</a>, <a href="/format/2402.16902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Boyang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jiacheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiyue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">With the rapid scaling of large language models (LLMs), serving numerous
LoRAs concurrently has become increasingly impractical, leading to unaffordable
costs and necessitating more parameter-efficient finetuning methods. In this
work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA),
an intra-layer sharing mechanism comprising four essential components:
broadcast reduction, rotation enhancement, partially-sharing refinement, and
rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its
advantages, and effectively circumvent the drawbacks of peer parameter-sharing
methods with superior model capacity, practical feasibility, and broad
applicability. Empirical experiments demonstrate the remarkably higher
parameter efficiency of PRoLoRA in both specific parameter budget and
performance target scenarios, and its scalability to larger LLMs. Notably, with
one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple
instruction tuning datasets. Subsequently, an ablation study is conducted to
validate the necessity of individual components and highlight the superiority
of PRoLoRA over three potential variants. Hopefully, the conspicuously higher
parameter efficiency can establish PRoLoRA as a resource-friendly alternative
to LoRA.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16903" title="Abstract">arXiv:2402.16903</a> [<a href="/pdf/2402.16903" title="Download PDF">pdf</a>, <a href="/format/2402.16903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel data generation scheme for surrogate modelling with deep  operator networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choubey%2C+S">Shivam Choubey</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+B">Birupaksha Pal</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+M">Manish Agrawal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Operator-based neural network architectures such as DeepONets have emerged as
a promising tool for the surrogate modeling of physical systems. In general,
towards operator surrogate modeling, the training data is generated by solving
the PDEs using techniques such as Finite Element Method (FEM). The
computationally intensive nature of data generation is one of the biggest
bottleneck in deploying these surrogate models for practical applications. In
this study, we propose a novel methodology to alleviate the computational
burden associated with training data generation for DeepONets. Unlike existing
literature, the proposed framework for data generation does not use any partial
differential equation integration strategy, thereby significantly reducing the
computational cost associated with generating training dataset for DeepONet. In
the proposed strategy, first, the output field is generated randomly,
satisfying the boundary conditions using Gaussian Process Regression (GPR).
From the output field, the input source field can be calculated easily using
finite difference techniques. The proposed methodology can be extended to other
operator learning methods, making the approach widely applicable. To validate
the proposed approach, we employ the heat equations as the model problem and
develop the surrogate model for numerous boundary value problems.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16904" title="Abstract">arXiv:2402.16904</a> [<a href="/pdf/2402.16904" title="Download PDF">pdf</a>, <a href="/format/2402.16904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective Task offloading for Maximum Inference Accuracy and Energy  efficient Real-Time IoT Sensing Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sada%2C+A+B">Abdelkarim Ben Sada</a>, 
<a href="/search/cs?searchtype=author&query=Khelloufi%2C+A">Amar Khelloufi</a>, 
<a href="/search/cs?searchtype=author&query=Naouri%2C+A">Abdenacer Naouri</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+H">Huansheng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Dhelim%2C+S">Sahraoui Dhelim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The recent advancements in small-size inference models facilitated AI
deployment on the edge. However, the limited resource nature of edge devices
poses new challenges especially for real-time applications. Deploying multiple
inference models (or a single tunable model) varying in size and therefore
accuracy and power consumption, in addition to an edge server inference model,
can offer a dynamic system in which the allocation of inference models to
inference jobs is performed according to the current resource conditions.
Therefore, in this work, we tackle the problem of selectively allocating
inference models to jobs or offloading them to the edge server to maximize
inference accuracy under time and energy constraints. This problem is shown to
be an instance of the unbounded multidimensional knapsack problem which is
considered a strongly NP-hard problem. We propose a lightweight hybrid genetic
algorithm (LGSTO) to solve this problem. We introduce a termination condition
and neighborhood exploration techniques for faster evolution of populations. We
compare LGSTO with the Naive and Dynamic programming solutions. In addition to
classic genetic algorithms using different reproduction methods including
NSGA-II, and finally we compare to other evolutionary methods such as Particle
swarm optimization (PSO) and Ant colony optimization (ACO). Experiment results
show that LGSTO performed 3 times faster than the fastest comparable schemes
while producing schedules with higher average accuracy.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16905" title="Abstract">arXiv:2402.16905</a> [<a href="/pdf/2402.16905" title="Download PDF">pdf</a>, <a href="/ps/2402.16905" title="Download PostScript">ps</a>, <a href="/format/2402.16905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enforcing Temporal Constraints on Generative Agent Behavior with  Reactive Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rothkopf%2C+R">Raven Rothkopf</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H+T">Hannah Tongxin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Santolucito%2C+M">Mark Santolucito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">The surge in popularity of Large Language Models (LLMs) has opened doors for
new approaches to the creation of interactive agents. However, managing the
temporal behavior of such agents over the course of an interaction remains
challenging. The stateful, long-term horizon and quantitative reasoning
required for coherent agent behavior does not fit well into the LLM paradigm.
We propose a combination of formal logic-based program synthesis and LLM
content generation to create generative agents that adhere to temporal
constraints. Our approach uses Temporal Stream Logic (TSL) to generate an
automaton that enforces a temporal structure on an agent and leaves the details
of each action for a moment in time to an LLM. By using TSL, we are able to
augment the generative agent where users have a higher level of guarantees on
behavior, better interpretability of the system, and more ability to build
agents in a modular way. We evaluate our approach on different tasks involved
in creating a coherent interactive agent specialized for various application
domains. We found that over all of the tasks, our approach using TSL achieves
at least 96% adherence, whereas the pure LLM-based approach demonstrates as low
as 14.67% adherence.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16906" title="Abstract">arXiv:2402.16906</a> [<a href="/pdf/2402.16906" title="Download PDF">pdf</a>, <a href="/format/2402.16906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LDB: A Large Language Model Debugger via Verifying Runtime Execution  Step-by-step
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+L">Li Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) are leading significant progress in code
generation. Beyond one-pass code generation, recent works further integrate
unit tests and program verifiers into LLMs to iteratively refine the generated
programs. However, these works consider the generated programs as an
indivisible entity, which falls short for LLMs in debugging the programs,
especially when the programs contain complex logic flows and data operations.
In contrast, when human developers debug programs, they typically set
breakpoints and selectively examine runtime execution information. The
execution flow and the intermediate variables play a crucial role in the
debugging process, yet they are underutilized in the existing literature on
code generation. In this study, we introduce Large Language Model Debugger
(LDB), a novel debugging framework that enables LLMs to refine their generated
programs with the runtime execution information. Specifically, LDB segments the
programs into basic blocks and tracks the values of intermediate variables
after each block throughout the runtime execution. This allows LLMs to
concentrate on simpler code units within the overall execution flow, verify
their correctness against the task description block by block, and efficiently
pinpoint any potential errors. Experiments demonstrate that LDB consistently
enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and
TransCoder benchmarks, archiving new state-of-the-art performance in code
debugging for various LLM selections.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16908" title="Abstract">arXiv:2402.16908</a> [<a href="/pdf/2402.16908" title="Download PDF">pdf</a>, <a href="/ps/2402.16908" title="Download PostScript">ps</a>, <a href="/format/2402.16908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local stochastic computing using memristor-enabled stochastic logics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Lekai Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+J">Jingfang Pei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+L+W+T">Leonard W. T. Ng</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+T">Tawfique Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Pun%2C+K">Kong-Pang Pun</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shuo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+G">Guohua Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Stochastic computing offers a probabilistic approach to address challenges
posed by problems with uncertainty and noise in various fields, particularly
machine learning. The realization of stochastic computing, however, faces the
limitation of developing reliable stochastic logics. Here, we present
stochastic logics development using memristors. Specifically, we integrate
memristors into logic circuits to design the stochastic logics, wherein the
inherent stochasticity in memristor switching is harnessed to enable stochastic
number encoding and processing with well-regulated probabilities and
correlations. As a practical application of the stochastic logics, we design a
compact stochastic Roberts cross operator for edge detection. Remarkably, the
operator demonstrates exceptional contour and texture extractions, even in the
presence of 50% noise, and owning to the probabilistic nature and compact
design, the operator can consume 95% less computational costs required by
conventional binary computing. The results underscore the great potential of
our stochastic computing approach as a lightweight local solution to machine
learning challenges in autonomous driving, virtual reality, medical diagnosis,
industrial automation, and beyond.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16909" title="Abstract">arXiv:2402.16909</a> [<a href="/pdf/2402.16909" title="Download PDF">pdf</a>, <a href="/format/2402.16909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Physical Activity on Quality of Life During Pregnancy: A  Causal ML Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kazemi%2C+K">Kianoosh Kazemi</a>, 
<a href="/search/cs?searchtype=author&query=Ryht%C3%A4%2C+I">Iina Ryht&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Azimi%2C+I">Iman Azimi</a>, 
<a href="/search/cs?searchtype=author&query=Niela-Vilen%2C+H">Hannakaisa Niela-Vilen</a>, 
<a href="/search/cs?searchtype=author&query=Axelin%2C+A">Anna Axelin</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Liljeberg%2C+P">Pasi Liljeberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">The concept of Quality of Life (QoL) refers to a holistic measurement of an
individual's well-being, incorporating psychological and social aspects.
Pregnant women, especially those with obesity and stress, often experience
lower QoL. Physical activity (PA) has shown the potential to enhance the QoL.
However, pregnant women who are overweight and obese rarely meet the
recommended level of PA. Studies have investigated the relationship between PA
and QoL during pregnancy using correlation-based approaches. These methods aim
to discover spurious correlations between variables rather than causal
relationships. Besides, the existing methods mainly rely on physical activity
parameters and neglect the use of different factors such as maternal (medical)
history and context data, leading to biased estimates. Furthermore, the
estimations lack an understanding of mediators and counterfactual scenarios
that might affect them. In this paper, we investigate the causal relationship
between being physically active (treatment variable) and the QoL (outcome)
during pregnancy and postpartum. To estimate the causal effect, we develop a
Causal Machine Learning method, integrating causal discovery and causal
inference components. The data for our investigation is derived from a
long-term wearable-based health monitoring study focusing on overweight and
obese pregnant women. The machine learning (meta-learner) estimation technique
is used to estimate the causal effect. Our result shows that performing
adequate physical activity during pregnancy and postpartum improves the QoL by
units of 7.3 and 3.4 on average in physical health and psychological domains,
respectively. In the final step, four refutation analysis techniques are
employed to validate our estimation.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16910" title="Abstract">arXiv:2402.16910</a> [<a href="/pdf/2402.16910" title="Download PDF">pdf</a>, <a href="/format/2402.16910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeSy is alive and well: A LLM-driven symbolic approach for better code  comment data generation and classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akl%2C+H+A">Hanna Abi Akl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures, forthcoming chapter in the book Generative Artificial Intelligence for Code - Impact of Large Language Models on Code Generation and Summarization in the book series Transactions on Computer Systems and Networks, Springer
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present a neuro-symbolic (NeSy) workflow combining a symbolic-based
learning technique with a large language model (LLM) agent to generate
synthetic data for code comment classification in the C programming language.
We also show how generating controlled synthetic data using this workflow fixes
some of the notable weaknesses of LLM-based generation and increases the
performance of classical machine learning models on the code comment
classification task. Our best model, a Neural Network, achieves a Macro-F1
score of 91.412% with an increase of 1.033% after data augmentation.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16911" title="Abstract">arXiv:2402.16911</a> [<a href="/pdf/2402.16911" title="Download PDF">pdf</a>, <a href="/format/2402.16911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Personalized Bayesian Federated Learning via Posterior  Fine-Tune
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Mengen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kuruoglu%2C+E+E">Ercan Engin Kuruoglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Performance degradation owing to data heterogeneity and low output
interpretability are the most significant challenges faced by federated
learning in practical applications. Personalized federated learning diverges
from traditional approaches, as it no longer seeks to train a single model, but
instead tailors a unique personalized model for each client. However, previous
work focused only on personalization from the perspective of neural network
parameters and lack of robustness and interpretability. In this work, we
establish a novel framework for personalized federated learning, incorporating
Bayesian methodology which enhances the algorithm's ability to quantify
uncertainty. Furthermore, we introduce normalizing flow to achieve
personalization from the parameter posterior perspective and theoretically
analyze the impact of normalizing flow on out-of-distribution (OOD) detection
for Bayesian neural networks. Finally, we evaluated our approach on
heterogeneous datasets, and the experimental results indicate that the new
algorithm not only improves accuracy but also outperforms the baseline
significantly in OOD detection due to the reliable output of the Bayesian
approach.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16912" title="Abstract">arXiv:2402.16912</a> [<a href="/pdf/2402.16912" title="Download PDF">pdf</a>, <a href="/ps/2402.16912" title="Download PostScript">ps</a>, <a href="/format/2402.16912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adversarial Robustness Benchmark for Enterprise Network Intrusion  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vitorino%2C+J">Jo&#xe3;o Vitorino</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+M">Miguel Silva</a>, 
<a href="/search/cs?searchtype=author&query=Maia%2C+E">Eva Maia</a>, 
<a href="/search/cs?searchtype=author&query=Pra%C3%A7a%2C+I">Isabel Pra&#xe7;a</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 tables, 2 figures, FPS 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As cyber-attacks become more sophisticated, improving the robustness of
Machine Learning (ML) models must be a priority for enterprises of all sizes.
To reliably compare the robustness of different ML models for cyber-attack
detection in enterprise computer networks, they must be evaluated in
standardized conditions. This work presents a methodical adversarial robustness
benchmark of multiple decision tree ensembles with constrained adversarial
examples generated from standard datasets. The robustness of regularly and
adversarially trained RF, XGB, LGBM, and EBM models was evaluated on the
original CICIDS2017 dataset, a corrected version of it designated as NewCICIDS,
and the HIKARI dataset, which contains more recent network traffic. NewCICIDS
led to models with a better performance, especially XGB and EBM, but RF and
LGBM were less robust against the more recent cyber-attacks of HIKARI. Overall,
the robustness of the models to adversarial cyber-attack examples was improved
without their generalization to regular traffic being affected, enabling a
reliable detection of suspicious activity without costly increases of false
alarms.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16913" title="Abstract">arXiv:2402.16913</a> [<a href="/pdf/2402.16913" title="Download PDF">pdf</a>, <a href="/format/2402.16913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from  the perspective of partial differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+S">Shiyi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiduo Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Liangjian Wen</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yuan Qi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent advancements in deep learning have led to the development of various
models for long-term multivariate time-series forecasting (LMTF), many of which
have shown promising results. Generally, the focus has been on
historical-value-based models, which rely on past observations to predict
future series. Notably, a new trend has emerged with time-index-based models,
offering a more nuanced understanding of the continuous dynamics underlying
time series. Unlike these two types of models that aggregate the information of
spatial domains or temporal domains, in this paper, we consider multivariate
time series as spatiotemporal data regularly sampled from a continuous
dynamical system, which can be represented by partial differential equations
(PDEs), with the spatial domain being fixed. Building on this perspective, we
present PDETime, a novel LMTF model inspired by the principles of Neural PDE
solvers, following the encoding-integration-decoding operations. Our extensive
experimentation across seven diverse real-world LMTF datasets reveals that
PDETime not only adapts effectively to the intrinsic spatiotemporal nature of
the data but also sets new benchmarks, achieving state-of-the-art results
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16914" title="Abstract">arXiv:2402.16914</a> [<a href="/pdf/2402.16914" title="Download PDF">pdf</a>, <a href="/format/2402.16914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM  Jailbreakers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xirui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minhao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The safety alignment of Large Language Models (LLMs) is vulnerable to both
manual and automated jailbreak attacks, which adversarially trigger LLMs to
output harmful content. However, current methods for jailbreaking LLMs, which
nest entire harmful prompts, are not effective at concealing malicious intent
and can be easily identified and rejected by well-aligned LLMs. This paper
discovers that decomposing a malicious prompt into separated sub-prompts can
effectively obscure its underlying malicious intent by presenting it in a
fragmented, less detectable form, thereby addressing these limitations. We
introduce an automatic prompt \textbf{D}ecomposition and
\textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack).
DrAttack includes three key components: (a) `Decomposition' of the original
prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly
by in-context learning with semantically similar but harmless reassembling
demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts'
synonyms that maintain the original intent while jailbreaking LLMs. An
extensive empirical study across multiple open-source and closed-source LLMs
demonstrates that, with a significantly reduced number of queries, DrAttack
obtains a substantial gain of success rate over prior SOTA prompt-only
attackers. Notably, the success rate of 78.0\% on GPT-4 with merely 15 queries
surpassed previous art by 33.1\%.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16915" title="Abstract">arXiv:2402.16915</a> [<a href="/pdf/2402.16915" title="Download PDF">pdf</a>, <a href="/format/2402.16915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More Than Routing: Joint GPS and Route Modeling for Refine Trajectory  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhipeng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zheyan Tu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinhai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+D">Deguo Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guyue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yilun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jiangtao Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Trajectory representation learning plays a pivotal role in supporting various
downstream tasks. Traditional methods in order to filter the noise in GPS
trajectories tend to focus on routing-based methods used to simplify the
trajectories. However, this approach ignores the motion details contained in
the GPS data, limiting the representation capability of trajectory
representation learning. To fill this gap, we propose a novel representation
learning framework that Joint GPS and Route Modelling based on self-supervised
technology, namely JGRM. We consider GPS trajectory and route as the two modes
of a single movement observation and fuse information through inter-modal
information interaction. Specifically, we develop two encoders, each tailored
to capture representations of route and GPS trajectories respectively. The
representations from the two modalities are fed into a shared transformer for
inter-modal information interaction. Eventually, we design three
self-supervised tasks to train the model. We validate the effectiveness of the
proposed method on two real datasets based on extensive experiments. The
experimental results demonstrate that JGRM outperforms existing methods in both
road segment representation and trajectory representation tasks. Our source
code is available at Anonymous Github.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16918" title="Abstract">arXiv:2402.16918</a> [<a href="/pdf/2402.16918" title="Download PDF">pdf</a>, <a href="/format/2402.16918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+K+M">Ka Man Lo</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yiming Liang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Wenyu Du</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yuantao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zili Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Modular neural architectures are gaining increasing attention due to their
powerful capability for generalization and sample-efficient adaptation to new
domains. However, training modular models, particularly in the early stages,
poses challenges due to the optimization difficulties arising from their
intrinsic sparse connectivity. Leveraging the knowledge from monolithic models,
using techniques such as knowledge distillation, is likely to facilitate the
training of modular models and enable them to integrate knowledge from multiple
models pretrained on diverse sources. Nevertheless, conventional knowledge
distillation approaches are not tailored to modular models and can fail when
directly applied due to the unique architectures and the enormous number of
parameters involved. Motivated by these challenges, we propose a general
module-to-module knowledge distillation (m2mKD) method for transferring
knowledge between modules. Our approach involves teacher modules split from a
pretrained monolithic model, and student modules of a modular model. m2mKD
separately combines these modules with a shared meta model and encourages the
student module to mimic the behaviour of the teacher module. We evaluate the
effectiveness of m2mKD on two distinct modular neural architectures: Neural
Attentive Circuits (NACs) and Vision Mixture-of-Experts (V-MoE). By applying
m2mKD to NACs, we achieve significant improvements in IID accuracy on
Tiny-ImageNet (up to 5.6%) and OOD robustness on Tiny-ImageNet-R (up to 4.2%).
On average, we observe a 1% gain in both ImageNet and ImageNet-R. The
V-MoE-Base model trained using m2mKD also achieves 3.5% higher accuracy than
end-to-end training on ImageNet. The experimental results demonstrate that our
method offers a promising solution for connecting modular networks with
pretrained monolithic models. Code is available at
https://github.com/kamanphoebe/m2mKD.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16919" title="Abstract">arXiv:2402.16919</a> [<a href="/pdf/2402.16919" title="Download PDF">pdf</a>, <a href="/format/2402.16919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Federated Instruction Tuning via Neural Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Ming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Junxian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+J">Jiawen Weng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingsong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated Instruction Tuning (FIT) has shown the ability to achieve
collaborative model instruction tuning among massive data owners without
sharing private data. However, it still faces two key challenges, i.e., data
and resource heterogeneity. Due to the varying data distribution and
preferences among data owners, FIT cannot adapt to the personalized data of
individual owners. Moreover, clients with superior computational abilities are
constrained since they need to maintain the same fine-tuning architecture as
the weaker clients. To address these issues, we propose a novel Personalized
Federated Instruction Tuning (PerFIT) framework based on architecture search.
Specifically, PerFIT allows each client to search for a personalized
architecture by expanding the trainable parameter space of the global model
followed by pruning the parameters to the original state. This procedure allows
personalized instruction fine-tuning within expanded parameter spaces,
concurrently preserving the same number of trainable parameters. Furthermore,
to release the abilities of heterogeneous computational resources and enhance
the performance of personalization on local data, we exploit personalized
parameter-wise aggregation. The evaluation with multiple LLMs non-IID scenarios
demonstrates that compared to the state-of-the-art FIT methods, our approach
can achieve up to a 23% decrease in perplexity.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16923" title="Abstract">arXiv:2402.16923</a> [<a href="/pdf/2402.16923" title="Download PDF">pdf</a>, <a href="/ps/2402.16923" title="Download PostScript">ps</a>, <a href="/format/2402.16923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on solving basic equations over the semiring of functional  digraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dennunzio%2C+A">Alberto Dennunzio</a>, 
<a href="/search/cs?searchtype=author&query=Formenti%2C+E">Enrico Formenti</a>, 
<a href="/search/cs?searchtype=author&query=Margara%2C+L">Luciano Margara</a>, 
<a href="/search/cs?searchtype=author&query=Riva%2C+S">Sara Riva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Endowing the set of functional graphs (FGs) with the sum (disjoint union of
graphs) and product (standard direct product on graphs) operations induces on
FGs a structure of a commutative semiring $\ring$. The operations on $\ring$
can be naturally extended to the set of univariate polynomials $\ring[X]$ over
$\ring$. This paper provides a polynomial time algorithm for deciding if
equations of the type $AX=B$ have solutions when $A$ is just a single cycle and
$B$ a set of cycles of identical size. We also prove a similar complexity
result for some variants of the previous equation.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16924" title="Abstract">arXiv:2402.16924</a> [<a href="/pdf/2402.16924" title="Download PDF">pdf</a>, <a href="/ps/2402.16924" title="Download PostScript">ps</a>, <a href="/format/2402.16924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical Unification of the Fractured Aspects of Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schroeder%2C+M+J">Marcin J. Schroeder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The article has as its main objective the identification of fundamental
epistemological obstacles in the study of information related to unnecessary
methodological assumptions and the demystification of popular beliefs in the
fundamental divisions of the aspects of information that can be understood as
Bachelardian rupture of epistemological obstacles. These general considerations
are preceded by an overview of the motivations for the study of information and
the role of the concept of information in the conceptualization of
intelligence, complexity, and consciousness justifying the need for a
sufficiently general perspective in the study of information, and are followed
at the end of the article by a brief exposition of an example of a possible
application in the development of the unified theory of information free from
unnecessary divisions and claims of superiority of the existing preferences in
methodology. The reference to Gaston Bachelard and his ideas of epistemological
obstacles and epistemological ruptures seems highly appropriate for the
reflection on the development of information study, in particular in the
context of obstacles such as the absence of semantics of information,
negligence of its structural analysis, separation of its digital and analog
forms, and misguided use of mathematics.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16925" title="Abstract">arXiv:2402.16925</a> [<a href="/pdf/2402.16925" title="Download PDF">pdf</a>, <a href="/format/2402.16925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimize Control Inputs for Strong Structural Controllability Using  Reinforcement Learning with Graph Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+M">Mengbang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Weisi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bailu Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Strong structural controllability (SSC) guarantees networked system with
linear-invariant dynamics controllable for all numerical realizations of
parameters. Current research has established algebraic and graph-theoretic
conditions of SSC for zero/nonzero or zero/nonzero/arbitrary structure. One
relevant practical problem is how to fully control the system with the minimal
number of input signals and identify which nodes must be imposed signals.
Previous work shows that this optimization problem is NP-hard and it is
difficult to find the solution. To solve this problem, we formulate the graph
coloring process as a Markov decision process (MDP) according to the
graph-theoretical condition of SSC for both zero/nonzero and
zero/nonzero/arbitrary structure. We use Actor-critic method with Directed
graph neural network which represents the color information of graph to
optimize MDP. Our method is validated in a social influence network with real
data and different complex network models. We find that the number of input
nodes is determined by the average degree of the network and the input nodes
tend to select nodes with low in-degree and avoid high-degree nodes.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16926" title="Abstract">arXiv:2402.16926</a> [<a href="/pdf/2402.16926" title="Download PDF">pdf</a>, <a href="/format/2402.16926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pichler%2C+G">Georg Pichler</a>, 
<a href="/search/cs?searchtype=author&query=Romanelli%2C+M">Marco Romanelli</a>, 
<a href="/search/cs?searchtype=author&query=Manivannan%2C+D+P">Divya Prakash Manivannan</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+P">Prashanth Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Khorrami%2C+F">Farshad Khorrami</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Siddharth Garg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce a formal statistical definition for the problem of backdoor
detection in machine learning systems and use it to analyze the feasibility of
such problems, providing evidence for the utility and applicability of our
definition. The main contributions of this work are an impossibility result and
an achievability result for backdoor detection. We show a no-free-lunch
theorem, proving that universal (adversary-unaware) backdoor detection is
impossible, except for very small alphabet sizes. Thus, we argue, that backdoor
detection methods need to be either explicitly, or implicitly adversary-aware.
However, our work does not imply that backdoor detection cannot work in
specific scenarios, as evidenced by successful backdoor detection methods in
the scientific literature. Furthermore, we connect our definition to the
probably approximately correct (PAC) learnability of the out-of-distribution
detection problem.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16927" title="Abstract">arXiv:2402.16927</a> [<a href="/pdf/2402.16927" title="Download PDF">pdf</a>, <a href="/ps/2402.16927" title="Download PostScript">ps</a>, <a href="/format/2402.16927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The ICASSP 2024 Audio Deep Packet Loss Concealment Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diener%2C+L">Lorenz Diener</a>, 
<a href="/search/cs?searchtype=author&query=Branets%2C+S">Solomiya Branets</a>, 
<a href="/search/cs?searchtype=author&query=Saabas%2C+A">Ando Saabas</a>, 
<a href="/search/cs?searchtype=author&query=Cutler%2C+R">Ross Cutler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio packet loss concealment is the hiding of gaps in VoIP audio streams
caused by network packet loss. With the ICASSP 2024 Audio Deep Packet Loss
Concealment Grand Challenge, we build on the success of the previous Audio PLC
Challenge held at INTERSPEECH 2022. We evaluate models on an overall harder
dataset, and use the new ITU-T P.804 evaluation procedure to more closely
evaluate the performance of systems specifically on the PLC task. We evaluate a
total of 9 systems, 8 of which satisfy the strict real-time performance
requirements of the challenge, using both P.804 and Word Accuracy evaluations.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16928" title="Abstract">arXiv:2402.16928</a> [<a href="/pdf/2402.16928" title="Download PDF">pdf</a>, <a href="/format/2402.16928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLAP: Learning Transferable Binary Code Representations with Natural  Language Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zeyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+Z">Zihan Sha</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuchen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wenju Sun</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Han Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xi Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Binary code representation learning has shown significant performance in
binary analysis tasks. But existing solutions often have poor transferability,
particularly in few-shot and zero-shot scenarios where few or no training
samples are available for the tasks. To address this problem, we present CLAP
(Contrastive Language-Assembly Pre-training), which employs natural language
supervision to learn better representations of binary code (i.e., assembly
code) and get better transferability. At the core, our approach boosts superior
transfer learning capabilities by effectively aligning binary code with their
semantics explanations (in natural language), resulting a model able to
generate better embeddings for binary code. To enable this alignment training,
we then propose an efficient dataset engine that could automatically generate a
large and diverse dataset comprising of binary code and corresponding natural
language explanations. We have generated 195 million pairs of binary code and
explanations and trained a prototype of CLAP. The evaluations of CLAP across
various downstream tasks in binary analysis all demonstrate exceptional
performance. Notably, without any task-specific training, CLAP is often
competitive with a fully supervised baseline, showing excellent
transferability. We release our pre-trained model and code at
https://github.com/Hustcw/CLAP.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16929" title="Abstract">arXiv:2402.16929</a> [<a href="/pdf/2402.16929" title="Download PDF">pdf</a>, <a href="/format/2402.16929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs  from the Programming Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Ming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanzhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Songlian Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yijie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Daling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jigang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
<p class="mathjax">LLMs have demonstrated commendable performance across diverse domains.
Nevertheless, formulating high-quality prompts to effectively instruct LLMs
poses a challenge for non-AI experts. Existing research in prompt engineering
suggests somewhat fragmented optimization principles and designs empirically
dependent prompt optimizers. Unfortunately, these endeavors lack a structured
design template, incurring high learning costs and resulting in low
reusability. Inspired by structured reusable programming languages, we propose
LangGPT, a dual-layer prompt design framework as the programming language for
LLMs. LangGPT has an easy-to-learn normative structure and provides an extended
structure for migration and reuse. Experiments illustrate that LangGPT
significantly enhances the capacity of LLMs to produce responses of superior
quality compared to baselines. Moreover, LangGPT has proven effective in
guiding LLMs to generate high-quality prompts. We have built a community on
LangGPT to facilitate the tuition and sharing of prompt design. We also
analyzed the ease of use and reusability of LangGPT through a community user
survey.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16932" title="Abstract">arXiv:2402.16932</a> [<a href="/pdf/2402.16932" title="Download PDF">pdf</a>, <a href="/format/2402.16932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptSet: A Programmer&#x27;s Prompting Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pister%2C+K">Kaiser Pister</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+D+J">Dhruba Jyoti Paul</a>, 
<a href="/search/cs?searchtype=author&query=Brophy%2C+P">Patrick Brophy</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+I">Ishan Joshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, ICSE '24 LLM4Code Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The rise of capabilities expressed by large language models has been quickly
followed by the integration of the same complex systems into application level
logic. Algorithms, programs, systems, and companies are built around structured
prompting to black box models where the majority of the design and
implementation lies in capturing and quantifying the `agent mode'. The standard
way to shape a closed language model is to prime it for a specific task with a
tailored prompt, often initially handwritten by a human. The textual prompts
co-evolve with the codebase, taking shape over the course of project life as
artifacts which must be reviewed and maintained, just as the traditional code
files might be. Unlike traditional code, we find that prompts do not receive
effective static testing and linting to prevent runtime issues. In this work,
we present a novel dataset called PromptSet, with more than 61,000 unique
developer prompts used in open source Python programs. We perform analysis on
this dataset and introduce the notion of a static linter for prompts. Released
with this publication is a HuggingFace dataset and a Github repository to
recreate collection and processing efforts, both under the name
\texttt{pisterlabs/promptset}.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16933" title="Abstract">arXiv:2402.16933</a> [<a href="/pdf/2402.16933" title="Download PDF">pdf</a>, <a href="/format/2402.16933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avoiding Catastrophic Forgetting in Visual Classification Using Human  Concept Formation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barari%2C+N">Nicki Barari</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+X">Xin Lian</a>, 
<a href="/search/cs?searchtype=author&query=MacLellan%2C+C+J">Christopher J. MacLellan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Deep neural networks have excelled in machine learning, particularly in
vision tasks, however, they often suffer from catastrophic forgetting when
learning new tasks sequentially. In this work, we propose Cobweb4V, a novel
visual classification approach that builds on Cobweb, a human like learning
system that is inspired by the way humans incrementally learn new concepts over
time. In this research, we conduct a comprehensive evaluation, showcasing the
proficiency of Cobweb4V in learning visual concepts, requiring less data to
achieve effective learning outcomes compared to traditional methods,
maintaining stable performance over time, and achieving commendable asymptotic
behavior, without catastrophic forgetting effects. These characteristics align
with learning strategies in human cognition, positioning Cobweb4V as a
promising alternative to neural network approaches.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16934" title="Abstract">arXiv:2402.16934</a> [<a href="/pdf/2402.16934" title="Download PDF">pdf</a>, <a href="/format/2402.16934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedReview: A Review Mechanism for Rejecting Poisoned Updates in  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianhang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baochun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Federated learning has recently emerged as a decentralized approach to learn
a high-performance model without access to user data. Despite its
effectiveness, federated learning gives malicious users opportunities to
manipulate the model by uploading poisoned model updates to the server. In this
paper, we propose a review mechanism called FedReview to identify and decline
the potential poisoned updates in federated learning. Under our mechanism, the
server randomly assigns a subset of clients as reviewers to evaluate the model
updates on their training datasets in each round. The reviewers rank the model
updates based on the evaluation results and count the number of the updates
with relatively low quality as the estimated number of poisoned updates. Based
on review reports, the server employs a majority voting mechanism to integrate
the rankings and remove the potential poisoned updates in the model aggregation
process. Extensive evaluation on multiple datasets demonstrate that FedReview
can assist the server to learn a well-performed global model in an adversarial
environment.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16936" title="Abstract">arXiv:2402.16936</a> [<a href="/pdf/2402.16936" title="Download PDF">pdf</a>, <a href="/format/2402.16936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangled 3D Scene Generation with Layout Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Epstein%2C+D">Dave Epstein</a>, 
<a href="/search/cs?searchtype=author&query=Poole%2C+B">Ben Poole</a>, 
<a href="/search/cs?searchtype=author&query=Mildenhall%2C+B">Ben Mildenhall</a>, 
<a href="/search/cs?searchtype=author&query=Efros%2C+A+A">Alexei A. Efros</a>, 
<a href="/search/cs?searchtype=author&query=Holynski%2C+A">Aleksander Holynski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a method to generate 3D scenes that are disentangled into their
component objects. This disentanglement is unsupervised, relying only on the
knowledge of a large pretrained text-to-image model. Our key insight is that
objects can be discovered by finding parts of a 3D scene that, when rearranged
spatially, still produce valid configurations of the same scene. Concretely,
our method jointly optimizes multiple NeRFs from scratch - each representing
its own object - along with a set of layouts that composite these objects into
scenes. We then encourage these composited scenes to be in-distribution
according to the image generator. We show that despite its simplicity, our
approach successfully generates 3D scenes decomposed into individual objects,
enabling new capabilities in text-to-3D content creation. For results and an
interactive demo, see our project page at https://dave.ml/layoutlearning/
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16965" title="Abstract">arXiv:2402.16965</a> [<a href="/pdf/2402.16965" title="Download PDF">pdf</a>, <a href="/format/2402.16965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WIPI: A New Web Threat for LLM-Driven Web Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fangzhou Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shutong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yulong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the fast development of large language models (LLMs), LLM-driven Web
Agents (Web Agents for short) have obtained tons of attention due to their
superior capability where LLMs serve as the core part of making decisions like
the human brain equipped with multiple web tools to actively interact with
external deployed websites. As uncountable Web Agents have been released and
such LLM systems are experiencing rapid development and drawing closer to
widespread deployment in our daily lives, an essential and pressing question
arises: "Are these Web Agents secure?". In this paper, we introduce a novel
threat, WIPI, that indirectly controls Web Agent to execute malicious
instructions embedded in publicly accessible webpages. To launch a successful
WIPI works in a black-box environment. This methodology focuses on the form and
content of indirect instructions within external webpages, enhancing the
efficiency and stealthiness of the attack. To evaluate the effectiveness of the
proposed methodology, we conducted extensive experiments using 7 plugin-based
ChatGPT Web Agents, 8 Web GPTs, and 3 different open-source Web Agents. The
results reveal that our methodology achieves an average attack success rate
(ASR) exceeding 90% even in pure black-box scenarios. Moreover, through an
ablation study examining various user prefix instructions, we demonstrated that
the WIPI exhibits strong robustness, maintaining high performance across
diverse prefix instructions.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16968" title="Abstract">arXiv:2402.16968</a> [<a href="/pdf/2402.16968" title="Download PDF">pdf</a>, <a href="/ps/2402.16968" title="Download PostScript">ps</a>, <a href="/format/2402.16968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Large Language Models in Cybersecurity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Jesus+Coelho+da+Silva%2C+G">Gabriel de Jesus Coelho da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Westphall%2C+C+B">Carlos Becker Westphall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have quickly risen to prominence due to their
ability to perform at or close to the state-of-the-art in a variety of fields
while handling natural language. An important field of research is the
application of such models at the cybersecurity context. This survey aims to
identify where in the field of cybersecurity LLMs have already been applied,
the ways in which they are being used and their limitations in the field.
Finally, suggestions are made on how to improve such limitations and what can
be expected from these systems once these limitations are overcome.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16972" title="Abstract">arXiv:2402.16972</a> [<a href="/pdf/2402.16972" title="Download PDF">pdf</a>, <a href="/ps/2402.16972" title="Download PostScript">ps</a>, <a href="/format/2402.16972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Mechanisms for Consumer Surplus Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ezra%2C+T">Tomer Ezra</a>, 
<a href="/search/cs?searchtype=author&query=Schoepflin%2C+D">Daniel Schoepflin</a>, 
<a href="/search/cs?searchtype=author&query=Shaulker%2C+A">Ariel Shaulker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We consider the problem of designing auctions which maximize consumer surplus
(i.e., the social welfare minus the payments charged to the buyers). In the
consumer surplus maximization problem, a seller with a set of goods faces a set
of strategic buyers with private values, each of whom aims to maximize their
own individual utility. The seller, in contrast, aims to allocate the goods in
a way which maximizes the total buyer utility. The seller must then elicit the
values of the buyers in order to decide what goods to award each buyer. The
canonical approach in mechanism design to ensure truthful reporting of the
private information is to find appropriate prices to charge each buyer in order
to align their objective with the objective of the seller. Indeed, there are
many celebrated results to this end when the seller's objective is welfare
maximization [Clarke, 1971, Groves, 1973, Vickrey, 1961] or revenue
maximization [Myerson, 1981]. However, in the case of consumer surplus
maximization the picture is less clear -- using high payments to ensure the
highest value bidders are served necessarily decreases their surplus utility,
but using low payments may lead the seller into serving lower value bidders.
<br />Our main result in this paper is a framework for designing mechanisms which
maximize consumer surplus. We instantiate our framework in a variety of
canonical multi-parameter auction settings (i.e., unit-demand bidders with
heterogeneous items, multi-unit auctions, and auctions with divisible goods)
and use it to design auctions achieving consumer surplus with optimal
approximation guarantees against the total social welfare. Along the way, we
answer an open question posed by Hartline and Roughgarden [2008], who, to our
knowledge, were the first to study the question of consumer surplus
approximation guarantees in single-parameter settings, regarding optimal
mechanisms for two bidders.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16973" title="Abstract">arXiv:2402.16973</a> [<a href="/pdf/2402.16973" title="Download PDF">pdf</a>, <a href="/format/2402.16973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Successfully Guiding Humans with Imperfect Instructions by Highlighting  Potential Errors and Suggesting Corrections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lingjun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khanh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This paper addresses the challenge of leveraging imperfect language models to
guide human decision-making in the context of a grounded navigation task. We
show that an imperfect instruction generation model can be complemented with an
effective communication mechanism to become more successful at guiding humans.
The communication mechanism we build comprises models that can detect potential
hallucinations in instructions and suggest practical alternatives, and an
intuitive interface to present that information to users. We show that this
approach reduces the human navigation error by up to 29% with no additional
cognitive burden. This result underscores the potential of integrating diverse
communication channels into AI systems to compensate for their imperfections
and enhance their utility for humans.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16977" title="Abstract">arXiv:2402.16977</a> [<a href="/pdf/2402.16977" title="Download PDF">pdf</a>, <a href="/format/2402.16977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dealing with Data for RE: Mitigating Challenges using NLP and Generative  AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghaisas%2C+S">Smita Ghaisas</a>, 
<a href="/search/cs?searchtype=author&query=Singhal%2C+A">Anmol Singhal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 2 figures, to be published in NLP for Requirements Engineering Book
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Across the dynamic business landscape today, enterprises face an
ever-increasing range of challenges. These include the constantly evolving
regulatory environment, the growing demand for personalization within software
applications, and the heightened emphasis on governance. In response to these
multifaceted demands, large enterprises have been adopting automation that
spans from the optimization of core business processes to the enhancement of
customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a
pivotal element of modern software systems. In this context, data plays an
indispensable role. AI-centric software systems based on supervised learning
and operating at an industrial scale require large volumes of training data to
perform effectively. Moreover, the incorporation of generative AI has led to a
growing demand for adequate evaluation benchmarks. Our experience in this field
has revealed that the requirement for large datasets for training and
evaluation introduces a host of intricate challenges. This book chapter
explores the evolving landscape of Software Engineering (SE) in general, and
Requirements Engineering (RE) in particular, in this era marked by AI
integration. We discuss challenges that arise while integrating Natural
Language Processing (NLP) and generative AI into enterprise-critical software
systems. The chapter provides practical insights, solutions, and examples to
equip readers with the knowledge and tools necessary for effectively building
solutions with NLP at their cores. We also reflect on how these text
data-centric tasks sit together with the traditional RE process. We also
highlight new RE tasks that may be necessary for handling the increasingly
important text data-centricity involved in developing software systems.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16979" title="Abstract">arXiv:2402.16979</a> [<a href="/pdf/2402.16979" title="Download PDF">pdf</a>, <a href="/format/2402.16979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmic Arbitrariness in Content Moderation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gomez%2C+J+F">Juan Felipe Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Machado%2C+C+V">Caio Vieira Machado</a>, 
<a href="/search/cs?searchtype=author&query=Paes%2C+L+M">Lucas Monteiro Paes</a>, 
<a href="/search/cs?searchtype=author&query=Calmon%2C+F+P">Flavio P. Calmon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Machine learning (ML) is widely used to moderate online content. Despite its
scalability relative to human moderation, the use of ML introduces unique
challenges to content moderation. One such challenge is predictive
multiplicity: multiple competing models for content classification may perform
equally well on average, yet assign conflicting predictions to the same
content. This multiplicity can result from seemingly innocuous choices during
model development, such as random seed selection for parameter initialization.
We experimentally demonstrate how content moderation tools can arbitrarily
classify samples as toxic, leading to arbitrary restrictions on speech. We
discuss these findings in terms of human rights set out by the International
Covenant on Civil and Political Rights (ICCPR), namely freedom of expression,
non-discrimination, and procedural justice. We analyze (i) the extent of
predictive multiplicity among state-of-the-art LLMs used for detecting toxic
content; (ii) the disparate impact of this arbitrariness across social groups;
and (iii) how model multiplicity compares to unambiguous human classifications.
Our findings indicate that the up-scaled algorithmic moderation risks
legitimizing an algorithmic leviathan, where an algorithm disproportionately
manages human rights. To mitigate such risks, our study underscores the need to
identify and increase the transparency of arbitrariness in content moderation
applications. Since algorithmic content moderation is being fueled by pressing
social concerns, such as disinformation and hate speech, our discussion on
harms raises concerns relevant to policy debates. Our findings also contribute
to content moderation and intermediary liability laws being discussed and
passed in many countries, such as the Digital Services Act in the European
Union, the Online Safety Act in the United Kingdom, and the Fake News Bill in
Brazil.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16980" title="Abstract">arXiv:2402.16980</a> [<a href="/pdf/2402.16980" title="Download PDF">pdf</a>, <a href="/format/2402.16980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Saliency-Aware Automatic Buddhas Statue Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yong Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fanghan Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Buddha statues, as a symbol of many religions, have significant cultural
implications that are crucial for understanding the culture and history of
different regions, and the recognition of Buddha statues is therefore the
pivotal link in the field of Buddha study. However, the Buddha statue
recognition requires extensive time and effort from knowledgeable
professionals, making it a costly task to perform. Convolution neural networks
(CNNs) are inherently efficient at processing visual information, but CNNs
alone are likely to make inaccurate classification decisions when subjected to
the class imbalance problem. Therefore, this paper proposes an end-to-end
automatic Buddha statue recognition model based on saliency map sampling. The
proposed Grid-Wise Local Self-Attention Module (GLSA) provides extra salient
features which can serve to enrich the dataset and allow CNNs to observe in a
much more comprehensive way. Eventually, our model is evaluated on a Buddha
dataset collected with the aid of Buddha experts and outperforms
state-of-the-art networks in terms of Top-1 accuracy by 4.63\% on average,
while only marginally increasing MUL-ADD.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16981" title="Abstract">arXiv:2402.16981</a> [<a href="/pdf/2402.16981" title="Download PDF">pdf</a>, <a href="/format/2402.16981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Euclidean Sliced Optimal Transport Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Genest%2C+B">Baptiste Genest</a>, 
<a href="/search/cs?searchtype=author&query=Courty%2C+N">Nicolas Courty</a>, 
<a href="/search/cs?searchtype=author&query=Coeurjolly%2C+D">David Coeurjolly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> COMPUTER GRAPHICS forum, proc. of Eurographics 2024, volume 43,
  number 2, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">In machine learning and computer graphics, a fundamental task is the
approximation of a probability density function through a well-dispersed
collection of samples. Providing a formal metric for measuring the distance
between probability measures on general spaces, Optimal Transport (OT) emerges
as a pivotal theoretical framework within this context. However, the associated
computational burden is prohibitive in most real-world scenarios. Leveraging
the simple structure of OT in 1D, Sliced Optimal Transport (SOT) has appeared
as an efficient alternative to generate samples in Euclidean spaces. This paper
pushes the boundaries of SOT utilization in computational geometry problems by
extending its application to sample densities residing on more diverse
mathematical domains, including the spherical space Sd , the hyperbolic plane
Hd , and the real projective plane Pd . Moreover, it ensures the quality of
these samples by achieving a blue noise characteristic, regardless of the
dimensionality involved. The robustness of our approach is highlighted through
its application to various geometry processing tasks, such as the intrinsic
blue noise sampling of meshes, as well as the sampling of directions and
rotations. These applications collectively underscore the efficacy of our
methodology.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16982" title="Abstract">arXiv:2402.16982</a> [<a href="/pdf/2402.16982" title="Download PDF">pdf</a>, <a href="/format/2402.16982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model  Counting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oakley%2C+L">Lisa Oakley</a>, 
<a href="/search/cs?searchtype=author&query=Holtzen%2C+S">Steven Holtzen</a>, 
<a href="/search/cs?searchtype=author&query=Oprea%2C+A">Alina Oprea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Programmatically generating tight differential privacy (DP) bounds is a hard
problem. Two core challenges are (1) finding expressive, compact, and efficient
encodings of the distributions of DP algorithms, and (2) state space explosion
stemming from the multiple quantifiers and relational properties of the DP
definition.
<br />We address the first challenge by developing a method for tight privacy and
accuracy bound synthesis using weighted model counting on binary decision
diagrams, a state of the art technique from the artificial intelligence and
automated reasoning communities for exactly computing probability
distributions. We address the second challenge by developing a framework for
leveraging inherent symmetries in DP algorithms. Our solution benefits from
ongoing research in probabilistic programming languages, allowing us to
succinctly and expressively represent different DP algorithms with approachable
language syntax that can be used by non-experts.
<br />We provide a detailed case study of our solution on the binary randomized
response algorithm. We also evaluate an implementation of our solution using
the Dice probabilistic programming language for the randomized response and
truncated geometric above threshold algorithms. We compare to prior work on
exact DP verification using Markov chain probabilistic model checking. Very few
existing works consider mechanized analysis of accuracy guarantees for DP
algorithms. We additionally provide a detailed analysis using our technique for
finding tight accuracy bounds for DP algorithms.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16985" title="Abstract">arXiv:2402.16985</a> [<a href="/pdf/2402.16985" title="Download PDF">pdf</a>, <a href="/format/2402.16985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visualizing 2x2 Normal-Form Games: twoxtwogame LaTeX Package
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marris%2C+L">Luke Marris</a>, 
<a href="/search/cs?searchtype=author&query=Gemp%2C+I">Ian Gemp</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Leibo%2C+J+Z">Joel Z. Leibo</a>, 
<a href="/search/cs?searchtype=author&query=Piliouras%2C+G">Georgios Piliouras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Normal-form games with two players, each with two strategies, are the most
studied class of games. These so-called 2x2 games are used to model a variety
of strategic interactions. They appear in game theory, economics, and
artificial intelligence research. However, there lacks tools for describing and
visualizing such games. This work introduces a LaTeX package for visualizing
2x2 games. This work has two goals: first, to provide high-quality tools and
vector graphic visualizations, suitable for scientific publications. And
second, to help promote standardization of names and representations of 2x2
games. The LaTeX package, twoxtwogame, is maintained on GitHub and mirrored on
CTAN, and is available under a permissive Apache 2 license.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16986" title="Abstract">arXiv:2402.16986</a> [<a href="/pdf/2402.16986" title="Download PDF">pdf</a>, <a href="/format/2402.16986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long Dialog Summarization: An Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mullick%2C+A">Ankan Mullick</a>, 
<a href="/search/cs?searchtype=author&query=Bhowmick%2C+A+K">Ayan Kumar Bhowmick</a>, 
<a href="/search/cs?searchtype=author&query=R%2C+R">Raghav R</a>, 
<a href="/search/cs?searchtype=author&query=Kokku%2C+R">Ravi Kokku</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+P">Prasenjit Dey</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+P">Pawan Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+N">Niloy Ganguly</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Dialog summarization has become increasingly important in managing and
comprehending large-scale conversations across various domains. This task
presents unique challenges in capturing the key points, context, and nuances of
multi-turn long conversations for summarization. It is worth noting that the
summarization techniques may vary based on specific requirements such as in a
shopping-chatbot scenario, the dialog summary helps to learn user preferences,
whereas in the case of a customer call center, the summary may involve the
problem attributes that a user specified, and the final resolution provided.
This work emphasizes the significance of creating coherent and contextually
rich summaries for effective communication in various applications. We explore
current state-of-the-art approaches for long dialog summarization in different
domains and benchmark metrics based evaluations show that one single model does
not perform well across various areas for distinct summarization tasks.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16990" title="Abstract">arXiv:2402.16990</a> [<a href="/pdf/2402.16990" title="Download PDF">pdf</a>, <a href="/format/2402.16990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> inGRASS: Incremental Graph Spectral Sparsification via  Low-Resistance-Diameter Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aghdaei%2C+A">Ali Aghdaei</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhuo Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted on DAC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work presents inGRASS, a novel algorithm designed for incremental
spectral sparsification of large undirected graphs. The proposed inGRASS
algorithm is highly scalable and parallel-friendly, having a nearly-linear time
complexity for the setup phase and the ability to update the spectral
sparsifier in $O(\log N)$ time for each incremental change made to the original
graph with $N$ nodes. A key component in the setup phase of inGRASS is a
multilevel resistance embedding framework introduced for efficiently
identifying spectrally-critical edges and effectively detecting redundant ones,
which is achieved by decomposing the initial sparsifier into many node clusters
with bounded effective-resistance diameters leveraging a
low-resistance-diameter decomposition (LRD) scheme. The update phase of inGRASS
exploits low-dimensional node embedding vectors for efficiently estimating the
importance and uniqueness of each newly added edge. As demonstrated through
extensive experiments, inGRASS achieves up to over $200 \times$ speedups while
retaining comparable solution quality in incremental spectral sparsification of
graphs obtained from various datasets, such as circuit simulations, finite
element analysis, and social networks.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16993" title="Abstract">arXiv:2402.16993</a> [<a href="/pdf/2402.16993" title="Download PDF">pdf</a>, <a href="/format/2402.16993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Speed Planner for Automated Vehicles: A Framework for  Lagrangian Variable Speed Limit in Mixed Autonomy Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+Z">Zhe Fu</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J">Jonathan Lee</a>, 
<a href="/search/eess?searchtype=author&query=Matin%2C+H+N+Z">Hossein Nick Zinat Matin</a>, 
<a href="/search/eess?searchtype=author&query=Alanqary%2C+A">Arwa Alanqary</a>, 
<a href="/search/eess?searchtype=author&query=Urieli%2C+D">Daniel Urieli</a>, 
<a href="/search/eess?searchtype=author&query=Hornstein%2C+S">Sharon Hornstein</a>, 
<a href="/search/eess?searchtype=author&query=Kreidieh%2C+A+R">Abdul Rahman Kreidieh</a>, 
<a href="/search/eess?searchtype=author&query=Chekroun%2C+R">Raphael Chekroun</a>, 
<a href="/search/eess?searchtype=author&query=Barbour%2C+W">William Barbour</a>, 
<a href="/search/eess?searchtype=author&query=Richardson%2C+W+A">William A. Richardson</a>, 
<a href="/search/eess?searchtype=author&query=Work%2C+D">Dan Work</a>, 
<a href="/search/eess?searchtype=author&query=Piccoli%2C+B">Benedetto Piccoli</a>, 
<a href="/search/eess?searchtype=author&query=Seibold%2C+B">Benjamin Seibold</a>, 
<a href="/search/eess?searchtype=author&query=Sprinkle%2C+J">Jonathan Sprinkle</a>, 
<a href="/search/eess?searchtype=author&query=Bayen%2C+A+M">Alexandre M. Bayen</a>, 
<a href="/search/eess?searchtype=author&query=Monache%2C+M+L+D">Maria Laura Delle Monache</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces a novel control framework for Lagrangian variable speed
limits in hybrid traffic flow environments utilizing automated vehicles (AVs).
The framework was validated using a fleet of 100 connected automated vehicles
as part of the largest coordinated open-road test designed to smooth traffic
flow. The framework includes two main components: a high-level controller
deployed on the server side, named Speed Planner, and low-level controllers
called vehicle controllers deployed on the vehicle side. The Speed Planner
designs and updates target speeds for the vehicle controllers based on
real-time Traffic State Estimation (TSE) [1]. The Speed Planner comprises two
modules: a TSE enhancement module and a target speed design module. The TSE
enhancement module is designed to minimize the effects of inherent latency in
the received traffic information and to improve the spatial and temporal
resolution of the input traffic data. The target speed design module generates
target speed profiles with the goal of improving traffic flow. The vehicle
controllers are designed to track the target speed meanwhile responding to the
surrounding situation. The numerical simulation indicates the performance of
the proposed method: the bottleneck throughput has increased by 5.01%, and the
speed standard deviation has been reduced by a significant 34.36%. We further
showcase an operational study with a description of how the controller was
implemented on a field-test with 100 AVs and its comprehensive effects on the
traffic flow.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16994" title="Abstract">arXiv:2402.16994</a> [<a href="/pdf/2402.16994" title="Download PDF">pdf</a>, <a href="/format/2402.16994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrov%2C+D">Dmitry Petrov</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+P">Pradyumn Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Thamizharasan%2C+V">Vikas Thamizharasan</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+V+G">Vladimir G. Kim</a>, 
<a href="/search/cs?searchtype=author&query=Gadelha%2C+M">Matheus Gadelha</a>, 
<a href="/search/cs?searchtype=author&query=Averkiou%2C+M">Melinos Averkiou</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+S">Siddhartha Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Kalogerakis%2C+E">Evangelos Kalogerakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project webpage: <a href="https://lodurality.github.io/GEM3D/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce GEM3D -- a new deep, topology-aware generative model of 3D
shapes. The key ingredient of our method is a neural skeleton-based
representation encoding information on both shape topology and geometry.
Through a denoising diffusion probabilistic model, our method first generates
skeleton-based representations following the Medial Axis Transform (MAT), then
generates surfaces through a skeleton-driven neural implicit formulation. The
neural implicit takes into account the topological and geometric information
stored in the generated skeleton representations to yield surfaces that are
more topologically and geometrically accurate compared to previous neural field
formulations. We discuss applications of our method in shape synthesis and
point cloud reconstruction tasks, and evaluate our method both qualitatively
and quantitatively. We demonstrate significantly more faithful surface
reconstruction and diverse shape generation results compared to the
state-of-the-art, also involving challenging scenarios of reconstructing and
synthesizing structurally complex, high-genus shape surfaces from Thingi10K and
ShapeNet.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16996" title="Abstract">arXiv:2402.16996</a> [<a href="/pdf/2402.16996" title="Download PDF">pdf</a>, <a href="/format/2402.16996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Decoding Brain Activity During Passive Listening of Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fodor%2C+M+A">Mil&#xe1;n Andr&#xe1;s Fodor</a>, 
<a href="/search/cs?searchtype=author&query=Csap%C3%B3%2C+T+G">Tam&#xe1;s G&#xe1;bor Csap&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Arthur%2C+F+V">Frigyes Viktor Arthur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">The aim of the study is to investigate the complex mechanisms of speech
perception and ultimately decode the electrical changes in the brain accruing
while listening to speech. We attempt to decode heard speech from intracranial
electroencephalographic (iEEG) data using deep learning methods. The goal is to
aid the advancement of brain-computer interface (BCI) technology for speech
synthesis, and, hopefully, to provide an additional perspective on the
cognitive processes of speech perception. This approach diverges from the
conventional focus on speech production and instead chooses to investigate
neural representations of perceived speech. This angle opened up a complex
perspective, potentially allowing us to study more sophisticated neural
patterns. Leveraging the power of deep learning models, the research aimed to
establish a connection between these intricate neural activities and the
corresponding speech sounds. Despite the approach not having achieved a
breakthrough yet, the research sheds light on the potential of decoding neural
activity during speech perception. Our current efforts can serve as a
foundation, and we are optimistic about the potential of expanding and
improving upon this work to move closer towards more advanced BCIs, better
understanding of processes underlying perceived speech and its relation to
spoken speech.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16998" title="Abstract">arXiv:2402.16998</a> [<a href="/pdf/2402.16998" title="Download PDF">pdf</a>, <a href="/format/2402.16998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Do Language Models Hear? Probing for Auditory Representations in  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ngo%2C+J">Jerry Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This work explores whether language models encode meaningfully grounded
representations of sounds of objects. We learn a linear probe that retrieves
the correct text representation of an object given a snippet of audio related
to that object, where the sound representation is given by a pretrained audio
model. This probe is trained via a contrastive loss that pushes the language
representations and sound representations of an object to be close to one
another. After training, the probe is tested on its ability to generalize to
objects that were not seen during training. Across different language models
and audio models, we find that the probe generalization is above chance in many
cases, indicating that despite being trained only on raw text, language models
encode grounded knowledge of sounds for some objects.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17000" title="Abstract">arXiv:2402.17000</a> [<a href="/pdf/2402.17000" title="Download PDF">pdf</a>, <a href="/format/2402.17000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Complexity of Initial-and-Final-State Opacity for Discrete Event  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masopust%2C+T">Tom&#xe1;&#x161; Masopust</a>, 
<a href="/search/cs?searchtype=author&query=Osi%C4%8Dka%2C+P">Petr Osi&#x10d;ka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Opacity is a general framework modeling security properties of systems
interacting with a passive attacker by asserting that a part of the systems
behaviour remains secret. In initial-and-final-state opacity (IFO, for short)
the secret is whether the system evolved from a given initial state to a given
final state or not. Two algorithms for IFO verification are discussed in the
literature. One algorithm arises from a trellis-based state estimator, which
builds a semigroup of binary relations generated by the events of the
automaton, and the other is based on the reduction to language inclusion. The
worst-case time complexity of both algorithms is bounded by a super-exponential
function. We show that the super-exponential time complexity is tight for both
algorithms; however, we leave open whether there is an algorithm with a lower
time complexity. Finally, we use extensive benchmarks based on real data to
experimentally compare the existing algorithms.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17002" title="Abstract">arXiv:2402.17002</a> [<a href="/pdf/2402.17002" title="Download PDF">pdf</a>, <a href="/format/2402.17002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Symmetry Group Structures via Implicit Orthogonality Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huh%2C+D">Dongsung Huh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Group Theory (math.GR); Representation Theory (math.RT)

</div>
<p class="mathjax">We introduce the HyperCube network, a novel approach for autonomously
discovering symmetry group structures within data. The key innovation is a
unique factorization architecture coupled with a novel regularizer that
instills a powerful inductive bias towards learning orthogonal representations.
This leverages a fundamental theorem of representation theory that all
compact/finite groups can be represented by orthogonal matrices. HyperCube
efficiently learns general group operations from partially observed data,
successfully recovering complete operation tables. Remarkably, the learned
factors correspond directly to exact matrix representations of the underlying
group. Moreover, these factors capture the group's complete set of irreducible
representations, forming the generalized Fourier basis for performing group
convolutions. In extensive experiments with both group and non-group symbolic
operations, HyperCube demonstrates a dramatic 100-1000x improvement in training
speed and 2-10x greater sample efficiency compared to the Transformer baseline.
These results suggest that our approach unlocks a new class of deep learning
models capable of harnessing inherent symmetries within data, leading to
significant improvements in performance and broader applicability.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17003" title="Abstract">arXiv:2402.17003</a> [<a href="/pdf/2402.17003" title="Download PDF">pdf</a>, <a href="/format/2402.17003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monitoring Fidelity of Online Reinforcement Learning Algorithms in  Clinical Trials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trella%2C+A+L">Anna L. Trella</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K+W">Kelly W. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nahum-Shani%2C+I">Inbal Nahum-Shani</a>, 
<a href="/search/cs?searchtype=author&query=Shetty%2C+V">Vivek Shetty</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+I">Iris Yan</a>, 
<a href="/search/cs?searchtype=author&query=Doshi-Velez%2C+F">Finale Doshi-Velez</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+S+A">Susan A. Murphy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Online reinforcement learning (RL) algorithms offer great potential for
personalizing treatment for participants in clinical trials. However, deploying
an online, autonomous algorithm in the high-stakes healthcare setting makes
quality control and data quality especially difficult to achieve. This paper
proposes algorithm fidelity as a critical requirement for deploying online RL
algorithms in clinical trials. It emphasizes the responsibility of the
algorithm to (1) safeguard participants and (2) preserve the scientific utility
of the data for post-trial analyses. We also present a framework for
pre-deployment planning and real-time monitoring to help algorithm developers
and clinical researchers ensure algorithm fidelity. To illustrate our
framework's practical application, we present real-world examples from the
Oralytics clinical trial. Since Spring 2023, this trial successfully deployed
an autonomous, online RL algorithm to personalize behavioral interventions for
participants at risk for dental disease.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17005" title="Abstract">arXiv:2402.17005</a> [<a href="/pdf/2402.17005" title="Download PDF">pdf</a>, <a href="/format/2402.17005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A visualization tool to explore alphabet orderings for the  Burrows-Wheeler Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Major%2C+L">Lily Major</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+D">Dave Davies</a>, 
<a href="/search/cs?searchtype=author&query=Clare%2C+A">Amanda Clare</a>, 
<a href="/search/cs?searchtype=author&query=Daykin%2C+J+W">Jacqueline W. Daykin</a>, 
<a href="/search/cs?searchtype=author&query=Mora%2C+B">Benjamin Mora</a>, 
<a href="/search/cs?searchtype=author&query=Zarges%2C+C">Christine Zarges</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The Burrows-Wheeler Transform (BWT) is an efficient invertible text
transformation algorithm with the properties of tending to group identical
characters together in a run, and enabling search of the text. This
transformation has extensive uses particularly in lossless compression
algorithms, indexing, and within bioinformatics for sequence alignment tasks.
There has been recent interest in minimizing the number of identical character
runs ($r$) for a transform and in finding useful alphabet orderings for the
sorting step of the matrix associated with the BWT construction. This motivates
the inspection of many transforms while developing algorithms. However, the
full Burrows-Wheeler matrix is $O(n^2)$ space and therefore very difficult to
display and inspect for large input sizes. In this paper we present a graphical
user interface (GUI) for working with BWTs, which includes features for
searching for matrix row prefixes, skipping over sections in the right-most
column (the transform), and displaying BWTs while exploring alphabet orderings
with the goal of minimizing the number of runs.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17008" title="Abstract">arXiv:2402.17008</a> [<a href="/pdf/2402.17008" title="Download PDF">pdf</a>, <a href="/format/2402.17008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking LLMs on the Semantic Overlap Summarization Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salvador%2C+J">John Salvador</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+N">Naman Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Akter%2C+M">Mousumi Akter</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Souvika Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Anupam Das</a>, 
<a href="/search/cs?searchtype=author&query=Karmaker%2C+S+K">Shubhra Kanti Karmaker</a> (&quot;Santu&quot;)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Semantic Overlap Summarization (SOS) is a constrained multi-document
summarization task, where the constraint is to capture the common/overlapping
information between two alternative narratives. While recent advancements in
Large Language Models (LLMs) have achieved superior performance in numerous
summarization tasks, a benchmarking study of the SOS task using LLMs is yet to
be performed. As LLMs' responses are sensitive to slight variations in prompt
design, a major challenge in conducting such a benchmarking study is to
systematically explore a variety of prompts before drawing a reliable
conclusion. Fortunately, very recently, the TELeR taxonomy has been proposed
which can be used to design and explore various prompts for LLMs. Using this
TELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs
on the SOS Task, assessing their ability to summarize overlapping information
from multiple alternative narratives. For evaluation, we report
well-established metrics like ROUGE, BERTscore, and SEM-F1$ on two different
datasets of alternative narratives. We conclude the paper by analyzing the
strengths and limitations of various LLMs in terms of their capabilities in
capturing overlapping information The code and datasets used to conduct this
study are available at https://anonymous.4open.science/r/llm_eval-E16D.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17010" title="Abstract">arXiv:2402.17010</a> [<a href="/pdf/2402.17010" title="Download PDF">pdf</a>, <a href="/format/2402.17010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Recall Reference Location Like Humans?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xinrun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Rui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenxin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">When completing knowledge-intensive tasks, humans sometimes need not just an
answer but also a corresponding reference passage for auxiliary reading.
Previous methods required obtaining pre-segmented article chunks through
additional retrieval models. This paper explores leveraging the parameterized
knowledge stored during the pre-training phase of large language models (LLMs)
to independently recall reference passage from any starting position. We
propose a two-stage framework that simulates the scenario of humans recalling
easily forgotten references. Initially, the LLM is prompted to recall document
title identifiers to obtain a coarse-grained document set. Then, based on the
acquired coarse-grained document set, it recalls fine-grained passage. In the
two-stage recall process, we use constrained decoding to ensure that content
outside of the stored documents is not generated. To increase speed, we only
recall a short prefix in the second stage, then locate its position to retrieve
a complete passage. Experiments on KILT knowledge-sensitive tasks have verified
that LLMs can independently recall reference passage location in various task
forms, and the obtained reference significantly assist downstream tasks.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17011" title="Abstract">arXiv:2402.17011</a> [<a href="/pdf/2402.17011" title="Download PDF">pdf</a>, <a href="/format/2402.17011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffuCOMET: Contextual Commonsense Knowledge Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Silin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ismayilzada%2C+M">Mete Ismayilzada</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Mengjie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wakaki%2C+H">Hiromi Wakaki</a>, 
<a href="/search/cs?searchtype=author&query=Mitsufuji%2C+Y">Yuki Mitsufuji</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Inferring contextually-relevant and diverse commonsense to understand
narratives remains challenging for knowledge models. In this work, we develop a
series of knowledge models, DiffuCOMET, that leverage diffusion to learn to
reconstruct the implicit semantic connections between narrative contexts and
relevant commonsense knowledge. Across multiple diffusion steps, our method
progressively refines a representation of commonsense facts that is anchored to
a narrative, producing contextually-relevant and diverse commonsense inferences
for an input context. To evaluate DiffuCOMET, we introduce new metrics for
commonsense inference that more closely measure knowledge diversity and
contextual relevance. Our results on two different benchmarks, ComFact and
WebNLG+, show that knowledge generated by DiffuCOMET achieves a better
trade-off between commonsense diversity, contextual relevance and alignment to
known gold references, compared to baseline knowledge models.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17012" title="Abstract">arXiv:2402.17012</a> [<a href="/pdf/2402.17012" title="Download PDF">pdf</a>, <a href="/format/2402.17012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pandora&#x27;s White-Box: Increased Training Data Leakage in Open LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J+G">Jeffrey G. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jason Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Marvin Li</a>, 
<a href="/search/cs?searchtype=author&query=Neel%2C+S">Seth Neel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper we undertake a systematic study of privacy attacks against open
source Large Language Models (LLMs), where an adversary has access to either
the model weights, gradients, or losses, and tries to exploit them to learn
something about the underlying training data. Our headline results are the
first membership inference attacks (MIAs) against pre-trained LLMs that are
able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing
that over $50\%$ (!) of the fine-tuning dataset can be extracted from a
fine-tuned LLM in natural settings. We consider varying degrees of access to
the underlying model, customization of the language model, and resources
available to the attacker. In the pre-trained setting, we propose three new
white-box MIAs: an attack based on the gradient norm, a supervised neural
network classifier, and a single step loss ratio attack. All outperform
existing black-box baselines, and our supervised attack closes the gap between
MIA attack success against LLMs and other types of models. In fine-tuning, we
find that given access to the loss of the fine-tuned and base models, a
fine-tuned loss ratio attack FLoRA is able to achieve near perfect MIA
peformance. We then leverage these MIAs to extract fine-tuning data from
fine-tuned language models. We find that the pipeline of generating from
fine-tuned models prompted with a small snippet of the prefix of each training
example, followed by using FLoRa to select the most likely training sample,
succeeds the majority of the fine-tuning dataset after only $3$ epochs of
fine-tuning. Taken together, these findings show that highly effective MIAs are
available in almost all LLM training settings, and highlight that great care
must be taken before LLMs are fine-tuned on highly sensitive data and then
deployed.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17013" title="Abstract">arXiv:2402.17013</a> [<a href="/pdf/2402.17013" title="Download PDF">pdf</a>, <a href="/format/2402.17013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Explainability and Fairness in Swiss Judgement Prediction:  Benchmarking on a Multilingual Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%2C+S+T+Y+S">Santosh T.Y.S.S</a>, 
<a href="/search/cs?searchtype=author&query=Baumgartner%2C+N">Nina Baumgartner</a>, 
<a href="/search/cs?searchtype=author&query=St%C3%BCrmer%2C+M">Matthias St&#xfc;rmer</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>, 
<a href="/search/cs?searchtype=author&query=Niklaus%2C+J">Joel Niklaus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The assessment of explainability in Legal Judgement Prediction (LJP) systems
is of paramount importance in building trustworthy and transparent systems,
particularly considering the reliance of these systems on factors that may lack
legal relevance or involve sensitive attributes. This study delves into the
realm of explainability and fairness in LJP models, utilizing Swiss Judgement
Prediction (SJP), the only available multilingual LJP dataset. We curate a
comprehensive collection of rationales that `support' and `oppose' judgement
from legal experts for 108 cases in German, French, and Italian. By employing
an occlusion-based explainability approach, we evaluate the explainability
performance of state-of-the-art monolingual and multilingual BERT-based LJP
models, as well as models developed with techniques such as data augmentation
and cross-lingual transfer, which demonstrated prediction performance
improvement. Notably, our findings reveal that improved prediction performance
does not necessarily correspond to enhanced explainability performance,
underscoring the significance of evaluating models from an explainability
perspective. Additionally, we introduce a novel evaluation framework, Lower
Court Insertion (LCI), which allows us to quantify the influence of lower court
information on model predictions, exposing current models' biases.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17014" title="Abstract">arXiv:2402.17014</a> [<a href="/pdf/2402.17014" title="Download PDF">pdf</a>, <a href="/format/2402.17014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on  Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Narayan%2C+N">Nikhil Narayan</a>, 
<a href="/search/cs?searchtype=author&query=Biswal%2C+M">Mrutyunjay Biswal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Working Notes in CASE-EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the digital realm, rich data serves as a crucial source of insights into
the complexities of social, political, and economic landscapes. Addressing the
growing need for high-quality information on events and the imperative to
combat hate speech, this research led to the establishment of the Shared Task
on Climate Activism Stance and Hate Event Detection at CASE 2024. Focused on
climate activists contending with hate speech on social media, our study
contributes to hate speech identification from tweets. Analyzing three
sub-tasks - Hate Speech Detection (Sub-task A), Targets of Hate Speech
Identification (Sub-task B), and Stance Detection (Sub-task C) - Team Z-AGI
Labs evaluated various models, including LSTM, Xgboost, and LGBM based on
Tf-Idf. Results unveiled intriguing variations, with Catboost excelling in
Subtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the
top-performing model for Subtask-A (F1: 0.8684). This research provides
valuable insights into the suitability of classical machine learning models for
climate hate speech and stance detection, aiding informed model selection for
robust mechanisms.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17015" title="Abstract">arXiv:2402.17015</a> [<a href="/pdf/2402.17015" title="Download PDF">pdf</a>, <a href="/format/2402.17015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree-Verifiable Graph Grammars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chimes%2C+M">Mark Chimes</a>, 
<a href="/search/cs?searchtype=author&query=Iosif%2C+R">Radu Iosif</a>, 
<a href="/search/cs?searchtype=author&query=Zuleger%2C+F">Florian Zuleger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Hyperedge-Replacement grammars (HR) have been introduced by Courcelle in
order to extend the notion of context-free sets from words and trees to graphs
of bounded tree-width. While for words and trees the syntactic restrictions
that guarantee that the associated languages of words resp. trees are regular -
and hence, MSO-definable - are known, the situation is far more complicated for
graphs. Here, Courcelle proposed the notion of regular graph grammars, a
syntactic restriction of HR grammars that guarantees the definability of the
associated languages of graphs in Counting Monadic Second Order Logic (CMSO).
However, these grammars are not complete in the sense that not every
CMSO-definable set of graphs of bounded tree-width can be generated by a
regular graph grammar. In this paper, we introduce a new syntactic restriction
of HR grammars, called tree-verifiable graph grammars, and a new notion of
bounded tree-width, called embeddable bounded tree-width, where the later
restricts the trees of a tree-decomposition to be a subgraph of the analyzed
graph. The main property of tree-verifiable graph grammars is that their
associated languages are CMSO-definable and that the have bounded embeddable
tree-width. We show further that they strictly generalize the regular graph
grammars of Courcelle. Finally, we establish a completeness result, showing
that every language of graphs that is CMSO-definable and of bounded embeddable
tree-width can be generated by a tree-verifiable graph grammar.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17016" title="Abstract">arXiv:2402.17016</a> [<a href="/pdf/2402.17016" title="Download PDF">pdf</a>, <a href="/format/2402.17016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohr%2C+I">Isabelle Mohr</a>, 
<a href="/search/cs?searchtype=author&query=Krimmel%2C+M">Markus Krimmel</a>, 
<a href="/search/cs?searchtype=author&query=Sturua%2C+S">Saba Sturua</a>, 
<a href="/search/cs?searchtype=author&query=Akram%2C+M+K">Mohammad Kalim Akram</a>, 
<a href="/search/cs?searchtype=author&query=Koukounas%2C+A">Andreas Koukounas</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnther%2C+M">Michael G&#xfc;nther</a>, 
<a href="/search/cs?searchtype=author&query=Mastrapas%2C+G">Georgios Mastrapas</a>, 
<a href="/search/cs?searchtype=author&query=Ravishankar%2C+V">Vinit Ravishankar</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+J+F">Joan Fontanals Mart&#xed;nez</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Ziniu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ognawala%2C+S">Saahil Ognawala</a>, 
<a href="/search/cs?searchtype=author&query=Guzman%2C+S">Susana Guzman</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Werk%2C+M">Maximilian Werk</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Han Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">We introduce a novel suite of state-of-the-art bilingual text embedding
models that are designed to support English and another target language. These
models are capable of processing lengthy text inputs with up to 8192 tokens,
making them highly versatile for a range of natural language processing tasks
such as text retrieval, clustering, and semantic textual similarity (STS)
calculations.
<br />By focusing on bilingual models and introducing a unique multi-task learning
objective, we have significantly improved the model performance on STS tasks,
which outperforms the capabilities of existing multilingual models in both
target language understanding and cross-lingual evaluation tasks. Moreover, our
bilingual models are more efficient, requiring fewer parameters and less memory
due to their smaller vocabulary needs. Furthermore, we have expanded the
Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and
Spanish embedding models. This integration aims to stimulate further research
and advancement in text embedding technologies for these languages.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17018" title="Abstract">arXiv:2402.17018</a> [<a href="/pdf/2402.17018" title="Download PDF">pdf</a>, <a href="/format/2402.17018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Curious Case of Remarkable Resilience to Gradient Attacks via Fully  Convolutional and Differentiable Front End with a Skip Connection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boytsov%2C+L">Leonid Boytsov</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+A">Ameya Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Condessa%2C+F">Filipe Condessa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We tested front-end enhanced neural models where a frozen classifier was
prepended by a differentiable and fully convolutional model with a skip
connection. By training them using a small learning rate for about one epoch,
we obtained models that retained the accuracy of the backbone classifier while
being unusually resistant to gradient attacks including APGD and FAB-T attacks
from the AutoAttack package, which we attributed to gradient masking. The
gradient masking phenomenon is not new, but the degree of masking was quite
remarkable for fully differentiable models that did not have
gradient-shattering components such as JPEG compression or components that are
expected to cause diminishing gradients.
<br />Though black box attacks can be partially effective against gradient masking,
they are easily defeated by combining models into randomized ensembles. We
estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10,
CIFAR100, and ImageNet despite having virtually zero accuracy under adaptive
attacks. Adversarial training of the backbone classifier can further increase
resistance of the front-end enhanced model to gradient attacks. On CIFAR10, the
respective randomized ensemble achieved 90.8$\pm 2.5$% (99% CI) accuracy under
AutoAttack while having only 18.2$\pm 3.6$% accuracy under the adaptive attack.
<br />We do not establish SOTA in adversarial robustness. Instead, we make
methodological contributions and further supports the thesis that adaptive
attacks designed with the complete knowledge of model architecture are crucial
in demonstrating model robustness and that even the so-called white-box
gradient attacks can have limited applicability. Although gradient attacks can
be complemented with black-box attack such as the SQUARE attack or the
zero-order PGD, black-box attacks can be weak against randomized ensembles,
e.g., when ensemble models mask gradients.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17019" title="Abstract">arXiv:2402.17019</a> [<a href="/pdf/2402.17019" title="Download PDF">pdf</a>, <a href="/format/2402.17019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Large Language Models for Learning Complex Legal Concepts  through Storytelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiajie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mahari%2C+R">Robert Mahari</a>, 
<a href="/search/cs?searchtype=author&query=Kessler%2C+D">Daniel Kessler</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+E">Eric Ma</a>, 
<a href="/search/cs?searchtype=author&query=August%2C+T">Tal August</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+I">Irene Li</a>, 
<a href="/search/cs?searchtype=author&query=Pentland%2C+A+%27">Alex &#x27;Sandy&#x27; Pentland</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kabbara%2C+J">Jad Kabbara</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Deb Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Making legal knowledge accessible to non-experts is crucial for enhancing
general legal literacy and encouraging civic participation in democracy.
However, legal documents are often challenging to understand for people without
legal backgrounds. In this paper, we present a novel application of large
language models (LLMs) in legal education to help non-experts learn intricate
legal concepts through storytelling, an effective pedagogical tool in conveying
complex and abstract concepts. We also introduce a new dataset LegalStories,
which consists of 295 complex legal doctrines, each accompanied by a story and
a set of multiple-choice questions generated by LLMs. To construct the dataset,
we experiment with various LLMs to generate legal stories explaining these
concepts. Furthermore, we use an expert-in-the-loop method to iteratively
design multiple-choice questions. Then, we evaluate the effectiveness of
storytelling with LLMs through an RCT experiment with legal novices on 10
samples from the dataset. We find that LLM-generated stories enhance
comprehension of legal concepts and interest in law among non-native speakers
compared to only definitions. Moreover, stories consistently help participants
relate legal concepts to their lives. Finally, we find that learning with
stories shows a higher retention rate for non-native speakers in the follow-up
assessment. Our work has strong implications for using LLMs in promoting
teaching and learning in the legal field and beyond.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17020" title="Abstract">arXiv:2402.17020</a> [<a href="/pdf/2402.17020" title="Download PDF">pdf</a>, <a href="/ps/2402.17020" title="Download PostScript">ps</a>, <a href="/format/2402.17020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Algorithms Used in Intrusion Detection Systems -- A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kimanzi%2C+R">Richard Kimanzi</a>, 
<a href="/search/cs?searchtype=author&query=Kimanga%2C+P">Peter Kimanga</a>, 
<a href="/search/cs?searchtype=author&query=Cherori%2C+D">Dedan Cherori</a>, 
<a href="/search/cs?searchtype=author&query=Gikunda%2C+P+K">Patrick K. Gikunda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The increase in network attacks has necessitated the development of robust
and efficient intrusion detection systems (IDS) capable of identifying
malicious activities in real-time. In the last five years, deep learning
algorithms have emerged as powerful tools in this domain, offering enhanced
detection capabilities compared to traditional methods. This review paper
studies recent advancements in the application of deep learning techniques,
including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),
Deep Belief Networks (DBN), Deep Neural Networks (DNN), Long Short-Term Memory
(LSTM), autoencoders (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing
Networks (SNN) and hybrid models, within network intrusion detection systems.
we delve into the unique architectures, training models, and classification
methodologies tailored for network traffic analysis and anomaly detection.
Furthermore, we analyze the strengths and limitations of each deep learning
approach in terms of detection accuracy, computational efficiency, scalability,
and adaptability to evolving threats. Additionally, this paper highlights
prominent datasets and benchmarking frameworks commonly utilized for evaluating
the performance of deep learning-based IDS. This review will provide
researchers and industry practitioners with valuable insights into the
state-of-the-art deep learning algorithms for enhancing the security framework
of network environments through intrusion detection.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17029" title="Abstract">arXiv:2402.17029</a> [<a href="/pdf/2402.17029" title="Download PDF">pdf</a>, <a href="/format/2402.17029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Writer Identification Using Convolutional Neural Network  Activation Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christlein%2C+V">Vincent Christlein</a>, 
<a href="/search/cs?searchtype=author&query=Bernecker%2C+D">David Bernecker</a>, 
<a href="/search/cs?searchtype=author&query=Maier%2C+A">Andreas Maier</a>, 
<a href="/search/cs?searchtype=author&query=Angelopoulou%2C+E">Elli Angelopoulou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> fixed tab 1b
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Pattern Recognition. DAGM 2015. Lecture Notes in Computer
  Science(), vol 9358. Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Convolutional neural networks (CNNs) have recently become the
state-of-the-art tool for large-scale image classification. In this work we
propose the use of activation features from CNNs as local descriptors for
writer identification. A global descriptor is then formed by means of GMM
supervector encoding, which is further improved by normalization with the
KL-Kernel. We evaluate our method on two publicly available datasets: the ICDAR
2013 benchmark database and the CVL dataset. While we perform comparably to the
state of the art on CVL, our proposed method yields about 0.21 absolute
improvement in terms of mAP on the challenging bilingual ICDAR dataset.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17030" title="Abstract">arXiv:2402.17030</a> [<a href="/pdf/2402.17030" title="Download PDF">pdf</a>, <a href="/ps/2402.17030" title="Download PostScript">ps</a>, <a href="/format/2402.17030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transforming Stiffness and Chaos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Scheffel%2C+J">Jan Scheffel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Chaotic Dynamics (nlin.CD)

</div>
<p class="mathjax">Stiff and chaotic differential equations are challenging for time-stepping
numerical methods. For explicit methods, the required time step resolution
significantly exceeds the resolution associated with the smoothness of the
exact solution for specified accuracy. In order to improve efficiency, the
question arises whether transformation to asymptotically stable solutions can
be performed, for which neighbouring solutions converge towards each other at a
controlled rate. Employing the concept of local Lyapunov exponents, it is
demonstrated that chaotic differential equations can be successfully
transformed to obtain high accuracy, whereas stiff equations cannot. For
instance, the accuracy of explicit fourth order Runge-Kutta solution of the
Lorenz chaotic equations can be increased by two orders of magnitude.
Alternatively, the time step can be significantly extended with retained
accuracy.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17032" title="Abstract">arXiv:2402.17032</a> [<a href="/pdf/2402.17032" title="Download PDF">pdf</a>, <a href="/format/2402.17032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REFACTOR: Learning to Extract Theorems from Proofs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+P">Jin Peng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuhuai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Grosse%2C+R">Roger Grosse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Human mathematicians are often good at recognizing modular and reusable
theorems that make complex mathematical results within reach. In this paper, we
propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for
training neural networks to mimic this ability in formal mathematical theorem
proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6%
of the theorems that humans would use to write the proofs. When applying the
model to the existing Metamath library, REFACTOR extracted 16 new theorems.
With newly extracted theorems, we show that the existing proofs in the MetaMath
database can be refactored. The new theorems are used very frequently after
refactoring, with an average usage of 733.5 times, and help shorten the proof
lengths. Lastly, we demonstrate that the prover trained on the new-theorem
refactored dataset proves more test theorems and outperforms state-of-the-art
baselines by frequently leveraging a diverse set of newly extracted theorems.
Code can be found at https://github.com/jinpz/refactor.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17038" title="Abstract">arXiv:2402.17038</a> [<a href="/pdf/2402.17038" title="Download PDF">pdf</a>, <a href="/format/2402.17038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Feedback Control for Global and Optimal Safe Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cheniouni%2C+I">Ishak Cheniouni</a>, 
<a href="/search/eess?searchtype=author&query=Berkane%2C+S">Soulaimane Berkane</a>, 
<a href="/search/eess?searchtype=author&query=Tayebi%2C+A">Abdelhamid Tayebi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We propose a hybrid feedback control strategy that safely steers a point-mass
robot to a target location optimally from all initial conditions in the
n-dimensional Euclidean space with a single spherical obstacle. The robot moves
straight to the target when it has a clear line-of-sight to the target
location. Otherwise, it engages in an optimal obstacle avoidance maneuver via
the shortest path inside the cone enclosing the obstacle and having the robot's
position as a vertex. The switching strategy that avoids the undesired
equilibria, leading to global asymptotic stability (GAS) of the target
location, relies on using two appropriately designed virtual destinations,
ensuring control continuity and shortest path generation. Simulation results
illustrating the effectiveness of the proposed approach are presented.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17043" title="Abstract">arXiv:2402.17043</a> [<a href="/pdf/2402.17043" title="Download PDF">pdf</a>, <a href="/format/2402.17043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Traffic Control via Connected and Automated Vehicles: An Open-Road Field  Experiment with 100 CAVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lee%2C+J+W">Jonathan W. Lee</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/eess?searchtype=author&query=Jang%2C+K">Kathy Jang</a>, 
<a href="/search/eess?searchtype=author&query=Hayat%2C+A">Amaury Hayat</a>, 
<a href="/search/eess?searchtype=author&query=Bunting%2C+M">Matthew Bunting</a>, 
<a href="/search/eess?searchtype=author&query=Alanqary%2C+A">Arwa Alanqary</a>, 
<a href="/search/eess?searchtype=author&query=Barbour%2C+W">William Barbour</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+Z">Zhe Fu</a>, 
<a href="/search/eess?searchtype=author&query=Gong%2C+X">Xiaoqian Gong</a>, 
<a href="/search/eess?searchtype=author&query=Gunter%2C+G">George Gunter</a>, 
<a href="/search/eess?searchtype=author&query=Hornstein%2C+S">Sharon Hornstein</a>, 
<a href="/search/eess?searchtype=author&query=Kreidieh%2C+A+R">Abdul Rahman Kreidieh</a>, 
<a href="/search/eess?searchtype=author&query=Lichtl%C3%A9%2C+N">Nathan Lichtl&#xe9;</a>, 
<a href="/search/eess?searchtype=author&query=Nice%2C+M+W">Matthew W. Nice</a>, 
<a href="/search/eess?searchtype=author&query=Richardson%2C+W+A">William A. Richardson</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+A">Adit Shah</a>, 
<a href="/search/eess?searchtype=author&query=Vinitsky%2C+E">Eugene Vinitsky</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+F">Fangyu Wu</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+S">Shengquan Xiang</a>, 
<a href="/search/eess?searchtype=author&query=Almatrudi%2C+S">Sulaiman Almatrudi</a>, 
<a href="/search/eess?searchtype=author&query=Althukair%2C+F">Fahd Althukair</a>, 
<a href="/search/eess?searchtype=author&query=Bhadani%2C+R">Rahul Bhadani</a>, 
<a href="/search/eess?searchtype=author&query=Carpio%2C+J">Joy Carpio</a>, 
<a href="/search/eess?searchtype=author&query=Chekroun%2C+R">Raphael Chekroun</a>, 
<a href="/search/eess?searchtype=author&query=Cheng%2C+E">Eric Cheng</a>, 
<a href="/search/eess?searchtype=author&query=Chiri%2C+M+T">Maria Teresa Chiri</a>, 
<a href="/search/eess?searchtype=author&query=Chou%2C+F">Fang-Chieh Chou</a>, 
<a href="/search/eess?searchtype=author&query=Delorenzo%2C+R">Ryan Delorenzo</a>, 
<a href="/search/eess?searchtype=author&query=Gibson%2C+M">Marsalis Gibson</a>, 
<a href="/search/eess?searchtype=author&query=Gloudemans%2C+D">Derek Gloudemans</a>, 
<a href="/search/eess?searchtype=author&query=Gollakota%2C+A">Anish Gollakota</a>, 
<a href="/search/eess?searchtype=author&query=Ji%2C+J">Junyi Ji</a>, 
<a href="/search/eess?searchtype=author&query=Keimer%2C+A">Alexander Keimer</a>, 
<a href="/search/eess?searchtype=author&query=Khoudari%2C+N">Nour Khoudari</a>, 
<a href="/search/eess?searchtype=author&query=Mahmood%2C+M">Malaika Mahmood</a>, 
<a href="/search/eess?searchtype=author&query=Mahmood%2C+M">Mikail Mahmood</a>, 
<a href="/search/eess?searchtype=author&query=Matin%2C+H+N+Z">Hossein Nick Zinat Matin</a>, 
<a href="/search/eess?searchtype=author&query=Mcquade%2C+S">Sean Mcquade</a>, 
<a href="/search/eess?searchtype=author&query=Ramadan%2C+R">Rabie Ramadan</a>, 
<a href="/search/eess?searchtype=author&query=Urieli%2C+D">Daniel Urieli</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xia Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yanbing Wang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+R">Rita Xu</a>, 
<a href="/search/eess?searchtype=author&query=Yao%2C+M">Mengsha Yao</a>, 
<a href="/search/eess?searchtype=author&query=You%2C+Y">Yiling You</a>, 
<a href="/search/eess?searchtype=author&query=Zach%C3%A1r%2C+G">Gergely Zach&#xe1;r</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Y">Yibo Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Ameli%2C+M">Mostafa Ameli</a>, 
<a href="/search/eess?searchtype=author&query=Baig%2C+M+N">Mirza Najamuddin Baig</a>, 
<a href="/search/eess?searchtype=author&query=Bhaskaran%2C+S">Sarah Bhaskaran</a>, 
<a href="/search/eess?searchtype=author&query=Butts%2C+K">Kenneth Butts</a>,  et al. (13 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The CIRCLES project aims to reduce instabilities in traffic flow, which are
naturally occurring phenomena due to human driving behavior. These "phantom
jams" or "stop-and-go waves,"are a significant source of wasted energy. Toward
this goal, the CIRCLES project designed a control system referred to as the
MegaController by the CIRCLES team, that could be deployed in real traffic. Our
field experiment leveraged a heterogeneous fleet of 100
longitudinally-controlled vehicles as Lagrangian traffic actuators, each of
which ran a controller with the architecture described in this paper. The
MegaController is a hierarchical control architecture, which consists of two
main layers. The upper layer is called Speed Planner, and is a centralized
optimal control algorithm. It assigns speed targets to the vehicles, conveyed
through the LTE cellular network. The lower layer is a control layer, running
on each vehicle. It performs local actuation by overriding the stock adaptive
cruise controller, using the stock on-board sensors. The Speed Planner ingests
live data feeds provided by third parties, as well as data from our own control
vehicles, and uses both to perform the speed assignment. The architecture of
the speed planner allows for modular use of standard control techniques, such
as optimal control, model predictive control, kernel methods and others,
including Deep RL, model predictive control and explicit controllers. Depending
on the vehicle architecture, all onboard sensing data can be accessed by the
local controllers, or only some. Control inputs vary across different
automakers, with inputs ranging from torque or acceleration requests for some
cars, and electronic selection of ACC set points in others. The proposed
architecture allows for the combination of all possible settings proposed
above. Most configurations were tested throughout the ramp up to the
MegaVandertest.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17045" title="Abstract">arXiv:2402.17045</a> [<a href="/pdf/2402.17045" title="Download PDF">pdf</a>, <a href="/format/2402.17045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Investigation into the Performances of the State-of-the-art Machine  Learning Approaches for Various Cyber-attack Detection: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ige%2C+T">Tosin Ige</a>, 
<a href="/search/cs?searchtype=author&query=Kiekintveld%2C+C">Christopher Kiekintveld</a>, 
<a href="/search/cs?searchtype=author&query=Piplai%2C+A">Aritran Piplai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">To secure computers and information systems from attackers taking advantage
of vulnerabilities in the system to commit cybercrime, several methods have
been proposed for real-time detection of vulnerabilities to improve security
around information systems. Of all the proposed methods, machine learning had
been the most effective method in securing a system with capabilities ranging
from early detection of software vulnerabilities to real-time detection of
ongoing compromise in a system. As there are different types of cyberattacks,
each of the existing state-of-the-art machine learning models depends on
different algorithms for training which also impact their suitability for
detection of a particular type of cyberattack. In this research, we analyzed
each of the current state-of-theart machine learning models for different types
of cyberattack detection from the past 10 years with a major emphasis on the
most recent works for comparative study to identify the knowledge gap where
work is still needed to be done with regard to detection of each category of
cyberattack
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17050" title="Abstract">arXiv:2402.17050</a> [<a href="/pdf/2402.17050" title="Download PDF">pdf</a>, <a href="/format/2402.17050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning Based Oscillation Dampening: Scaling up  Single-Agent RL algorithms to a 100 AV highway field operational test
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jang%2C+K">Kathy Jang</a>, 
<a href="/search/eess?searchtype=author&query=Lichtl%C3%A9%2C+N">Nathan Lichtl&#xe9;</a>, 
<a href="/search/eess?searchtype=author&query=Vinitsky%2C+E">Eugene Vinitsky</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+A">Adit Shah</a>, 
<a href="/search/eess?searchtype=author&query=Bunting%2C+M">Matthew Bunting</a>, 
<a href="/search/eess?searchtype=author&query=Nice%2C+M">Matthew Nice</a>, 
<a href="/search/eess?searchtype=author&query=Piccoli%2C+B">Benedetto Piccoli</a>, 
<a href="/search/eess?searchtype=author&query=Seibold%2C+B">Benjamin Seibold</a>, 
<a href="/search/eess?searchtype=author&query=Work%2C+D+B">Daniel B. Work</a>, 
<a href="/search/eess?searchtype=author&query=Monache%2C+M+L+D">Maria Laura Delle Monache</a>, 
<a href="/search/eess?searchtype=author&query=Sprinkle%2C+J">Jonathan Sprinkle</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J+W">Jonathan W. Lee</a>, 
<a href="/search/eess?searchtype=author&query=Bayen%2C+A+M">Alexandre M. Bayen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In this article, we explore the technical details of the reinforcement
learning (RL) algorithms that were deployed in the largest field test of
automated vehicles designed to smooth traffic flow in history as of 2023,
uncovering the challenges and breakthroughs that come with developing RL
controllers for automated vehicles. We delve into the fundamental concepts
behind RL algorithms and their application in the context of self-driving cars,
discussing the developmental process from simulation to deployment in detail,
from designing simulators to reward function shaping. We present the results in
both simulation and deployment, discussing the flow-smoothing benefits of the
RL controller. From understanding the basics of Markov decision processes to
exploring advanced techniques such as deep RL, our article offers a
comprehensive overview and deep dive of the theoretical foundations and
practical implementations driving this rapidly evolving field. We also showcase
real-world case studies and alternative research projects that highlight the
impact of RL controllers in revolutionizing autonomous driving. From tackling
complex urban environments to dealing with unpredictable traffic scenarios,
these intelligent controllers are pushing the boundaries of what automated
vehicles can achieve. Furthermore, we examine the safety considerations and
hardware-focused technical details surrounding deployment of RL controllers
into automated vehicles. As these algorithms learn and evolve through
interactions with the environment, ensuring their behavior aligns with safety
standards becomes crucial. We explore the methodologies and frameworks being
developed to address these challenges, emphasizing the importance of building
reliable control systems for automated vehicles.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17056" title="Abstract">arXiv:2402.17056</a> [<a href="/pdf/2402.17056" title="Download PDF">pdf</a>, <a href="/format/2402.17056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Model of Back-to-Back Converter for System-Level Phasor  Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mahmood%2C+H">Hisham Mahmood</a>, 
<a href="/search/eess?searchtype=author&query=Acharya%2C+S">Samrat Acharya</a>, 
<a href="/search/eess?searchtype=author&query=Tuffner%2C+F">Francis Tuffner</a>, 
<a href="/search/eess?searchtype=author&query=Mana%2C+P">Priya Mana</a>, 
<a href="/search/eess?searchtype=author&query=Bharati%2C+A+K">Alok Kumar Bharati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The power system is expected to evolve rapidly with the increasing deployment
of power electronic interface and conditioning systems, microgrids, and hybrid
AC/DC grids. Among power electronic systems, back-to-back (BTB) converters can
be a powerful interface to integrate microgrids and networked microgrids. To
study the integration of such devices into large power systems, a balance
between power electronics model fidelity and system-level computational
efficiency is critical. In system-level simulations of bulk power systems
dominated by synchronous generators, detailed electromagnetic models of
back-to-back converters may be unnecessary and also computationally
inefficient. This paper focuses on developing a simple phasor model for
back-to-back converters that can be easily integrated into powerflow solvers to
facilitate large-scale power system simulations. The model is implemented using
C$^{++}$ language and integrated into GridLAB-D, an open source software for
distribution systems studies, as a potential new capability. The GridLAB-D
phasor domain model is validated against the electromagnetic transient (EMT)
simulation of the detailed switching model. Simulation results show that the
phasor model successfully captures the dominant dynamics of the converter with
significantly shorter simulation elapsed time.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17058" title="Abstract">arXiv:2402.17058</a> [<a href="/pdf/2402.17058" title="Download PDF">pdf</a>, <a href="/format/2402.17058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Capacity-Distortion Trade-Offs in Memoryless ISAC Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Andrei%2C+V+C">Vlad C. Andrei</a>, 
<a href="/search/cs?searchtype=author&query=Djuhera%2C+A">Aladin Djuhera</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6nich%2C+U+J">Ullrich J. M&#xf6;nich</a>, 
<a href="/search/cs?searchtype=author&query=Boche%2C+H">Holger Boche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This manuscript investigates the information-theoretic limits of integrated
sensing and communications (ISAC), aiming for simultaneous reliable
communication and precise channel state estimation. We model such a system with
a state-dependent discrete memoryless channel (SD-DMC) with present or absent
channel feedback and generalized side information at the transmitter and the
receiver, where the joint task of message decoding and state estimation is
performed at the receiver. The relationship between the achievable
communication rate and estimation error, the capacity-distortion (C-D)
trade-off, is characterized across different causality levels of the side
information. This framework is shown to be capable of modeling various
practical scenarios by assigning the side information with different meanings,
including monostatic and bistatic radar systems. The analysis is then extended
to the two-user degraded broadcast channel, and we derive an achievable C-D
region that is tight under certain conditions. To solve the optimization
problem arising in the computation of C-D functions/regions, we propose a
proximal block coordinate descent (BCD) method, prove its convergence to a
stationary point, and derive a stopping criterion. Finally, several
representative examples are studied to demonstrate the versatility of our
framework and the effectiveness of the proposed algorithm.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17061" title="Abstract">arXiv:2402.17061</a> [<a href="/pdf/2402.17061" title="Download PDF">pdf</a>, <a href="/format/2402.17061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Fidelity Methodology for Reduced Order Models with  High-Dimensional Inputs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mufti%2C+B">Bilal Mufti</a>, 
<a href="/search/cs?searchtype=author&query=Perron%2C+C">Christian Perron</a>, 
<a href="/search/cs?searchtype=author&query=Mavris%2C+D+N">Dimitri N. Mavris</a> (ASDL, Daniel Guggenheim School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, Georgia)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the early stages of aerospace design, reduced order models (ROMs) are
crucial for minimizing computational costs associated with using physics-rich
field information in many-query scenarios requiring multiple evaluations. The
intricacy of aerospace design demands the use of high-dimensional design spaces
to capture detailed features and design variability accurately. However, these
spaces introduce significant challenges, including the curse of dimensionality,
which stems from both high-dimensional inputs and outputs necessitating
substantial training data and computational effort. To address these
complexities, this study introduces a novel multi-fidelity, parametric, and
non-intrusive ROM framework designed for high-dimensional contexts. It
integrates machine learning techniques for manifold alignment and dimension
reduction employing Proper Orthogonal Decomposition (POD) and Model-based
Active Subspace with multi-fidelity regression for ROM construction. Our
approach is validated through two test cases: the 2D RAE~2822 airfoil and the
3D NASA CRM wing, assessing combinations of various fidelity levels, training
data ratios, and sample sizes. Compared to the single-fidelity PCAS method, our
multi-fidelity solution offers improved cost-accuracy benefits and achieves
better predictive accuracy with reduced computational demands. Moreover, our
methodology outperforms the manifold-aligned ROM (MA-ROM) method by 50% in
handling scenarios with large input dimensions, underscoring its efficacy in
addressing the complex challenges of aerospace design.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17062" title="Abstract">arXiv:2402.17062</a> [<a href="/pdf/2402.17062" title="Download PDF">pdf</a>, <a href="/format/2402.17062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed  Distance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+H">Haozhe Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+A">Alexander Mathis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CVPR 2024. 9 figures, many tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Human hands are highly articulated and versatile at handling objects. Jointly
estimating the 3D poses of a hand and the object it manipulates from a
monocular camera is challenging due to frequent occlusions. Thus, existing
methods often rely on intermediate 3D shape representations to increase
performance. These representations are typically explicit, such as 3D point
clouds or meshes, and thus provide information in the direct surroundings of
the intermediate hand pose estimate. To address this, we introduce HOISDF, a
Signed Distance Field (SDF) guided hand-object pose estimation network, which
jointly exploits hand and object SDFs to provide a global, implicit
representation over the complete reconstruction volume. Specifically, the role
of the SDFs is threefold: equip the visual encoder with implicit shape
information, help to encode hand-object interactions, and guide the hand and
object pose regression via SDF-based sampling and by augmenting the feature
representations. We show that HOISDF achieves state-of-the-art results on
hand-object pose estimation benchmarks (DexYCB and HO3Dv2). Code is available
at https://github.com/amathislab/HOISDF
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17065" title="Abstract">arXiv:2402.17065</a> [<a href="/pdf/2402.17065" title="Download PDF">pdf</a>, <a href="/format/2402.17065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming the Tail in Class-Conditional GANs: Knowledge Sharing via  Unconditional Training at Lower Resolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khorram%2C+S">Saeed Khorram</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Mingqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shahbazi%2C+M">Mohamad Shahbazi</a>, 
<a href="/search/cs?searchtype=author&query=Danesh%2C+M+H">Mohamad H. Danesh</a>, 
<a href="/search/cs?searchtype=author&query=Fuxin%2C+L">Li Fuxin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the extensive research on training generative adversarial networks
(GANs) with limited training data, learning to generate images from long-tailed
training distributions remains fairly unexplored. In the presence of imbalanced
multi-class training data, GANs tend to favor classes with more samples,
leading to the generation of low-quality and less diverse samples in tail
classes. In this study, we aim to improve the training of class-conditional
GANs with long-tailed data. We propose a straightforward yet effective method
for knowledge sharing, allowing tail classes to borrow from the rich
information from classes with more abundant training data. More concretely, we
propose modifications to existing class-conditional GAN architectures to ensure
that the lower-resolution layers of the generator are trained entirely
unconditionally while reserving class-conditional generation for the
higher-resolution layers. Experiments on several long-tail benchmarks and GAN
architectures demonstrate a significant improvement over existing methods in
both the diversity and fidelity of the generated images. The code is available
at https://github.com/khorrams/utlo.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17071" title="Abstract">arXiv:2402.17071</a> [<a href="/pdf/2402.17071" title="Download PDF">pdf</a>, <a href="/format/2402.17071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Path Planning for a Cooperative Navigation Aid Vehicle to Assist  Multiple Agents Intermittently
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wolek%2C+A">Artur Wolek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This paper considers the problem of planning a path for a single underwater
cooperative navigation aid (CNA) vehicle to intermittently aid a set of N
agents to minimize average navigation uncertainty. Both the CNA and agents are
modeled as constant-velocity vehicles. The agents traverse along known nominal
trajectories and the CNA plans a path to sequentially intercept them.
Navigation aiding is modeled by a scalar discrete time Kalman filter. During
path planning, the CNA considers surfacing to reduce its own navigation
uncertainty. A greedy planning algorithm is proposed that uses a heuristic
based on an optimal time-to-aid, overall navigation uncertainty reduction, and
transit time, to assign agents to the CNA. The approach is compared to an
optimal (exhaustive enumeration) algorithm through a Monte Carlo experiment
with randomized agent nominal trajectories and initial navigation uncertainty.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17073" title="Abstract">arXiv:2402.17073</a> [<a href="/pdf/2402.17073" title="Download PDF">pdf</a>, <a href="/format/2402.17073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Shot Graph Representation Learning Using Hyperdimensional Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dalvi%2C+A">Abhishek Dalvi</a>, 
<a href="/search/cs?searchtype=author&query=Honavar%2C+V">Vasant Honavar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We present a novel, simple, fast, and efficient approach for semi-supervised
learning on graphs. The proposed approach takes advantage of hyper-dimensional
computing which encodes data samples using random projections into a high
dimensional space (HD space for short). Specifically, we propose a
Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the
injectivity property of the node representations of a family of graph neural
networks. HDGL maps node features to the HD space and then uses HD operators
such as bundling and binding to aggregate information from the local
neighborhood of each node. Results of experiments with widely used benchmark
data sets show that HDGL achieves predictive performance that is competitive
with the state-of-the-art deep learning methods, without the need for
computationally expensive training.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17074" title="Abstract">arXiv:2402.17074</a> [<a href="/pdf/2402.17074" title="Download PDF">pdf</a>, <a href="/format/2402.17074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asphalt Concrete Characterization Using Digital Image Correlation: A  Systematic Review of Best Practices, Applications, and Future Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zehui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jianwei Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal of Testing and Evaluation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Digital Image Correlation (DIC) is an optical technique that measures
displacement and strain by tracking pattern movement in a sequence of captured
images during testing. DIC has gained recognition in asphalt pavement
engineering since the early 2000s. However, users often perceive the DIC
technique as an out-of-box tool and lack a thorough understanding of its
operational and measurement principles. This article presents a state-of-art
review of DIC as a crucial tool for laboratory testing of asphalt concrete
(AC), primarily focusing on the widely utilized 2D-DIC and 3D-DIC techniques.
To address frequently asked questions from users, the review thoroughly
examines the optimal methods for preparing speckle patterns, configuring
single-camera or dual-camera imaging systems, conducting DIC analyses, and
exploring various applications. Furthermore, emerging DIC methodologies such as
Digital Volume Correlation and deep-learning-based DIC are introduced,
highlighting their potential for future applications in pavement engineering.
The article also provides a comprehensive and reliable flowchart for
implementing DIC in AC characterization. Finally, critical directions for
future research are presented.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17077" title="Abstract">arXiv:2402.17077</a> [<a href="/pdf/2402.17077" title="Download PDF">pdf</a>, <a href="/format/2402.17077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallelized Spatiotemporal Binding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gautam Singh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiawei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ivanovic%2C+B">Boris Ivanovic</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungjin Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+T">Tong Che</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> See project page at <a href="https://parallel-st-binder.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">While modern best practices advocate for scalable architectures that support
long-range interactions, object-centric models are yet to fully embrace these
architectures. In particular, existing object-centric models for handling
sequential inputs, due to their reliance on RNN-based implementation, show poor
stability and capacity and are slow to train on long sequences. We introduce
Parallelizable Spatiotemporal Binder or PSB, the first
temporally-parallelizable slot learning architecture for sequential inputs.
Unlike conventional RNN-based approaches, PSB produces object-centric
representations, known as slots, for all time-steps in parallel. This is
achieved by refining the initial slots across all time-steps through a fixed
number of layers equipped with causal attention. By capitalizing on the
parallelism induced by our architecture, the proposed model exhibits a
significant boost in efficiency. In experiments, we test PSB extensively as an
encoder within an auto-encoding framework paired with a wide variety of decoder
options. Compared to the state-of-the-art, our architecture demonstrates stable
training on longer sequences, achieves parallelization that results in a 60%
increase in training speed, and yields performance that is on par with or
better on unsupervised 2D and 3D object-centric scene decomposition and
understanding.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17078" title="Abstract">arXiv:2402.17078</a> [<a href="/pdf/2402.17078" title="Download PDF">pdf</a>, <a href="/format/2402.17078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Batch Estimation of a Steady, Uniform, Flow-Field from Ground Velocity  and Heading Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wolek%2C+A">Artur Wolek</a>, 
<a href="/search/eess?searchtype=author&query=McMahon%2C+J">James McMahon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This paper presents three batch estimation methods that use noisy ground
velocity and heading measurements from a vehicle executing a circular orbit (or
similar large heading change maneuver) to estimate the speed and direction of a
steady, uniform, flow-field. The methods are based on a simple kinematic model
of the vehicle's motion and use curve-fitting or nonlinear least-square
optimization. A Monte Carlo simulation with randomized flow conditions is used
to evaluate the batch estimation methods while varying the measurement noise of
the data and the interval of unique heading traversed during the maneuver. The
methods are also compared using experimental data obtained with a Bluefin-21
unmanned underwater vehicle performing a series of circular orbit maneuvers
over a five hour period in a tide-driven flow.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17081" title="Abstract">arXiv:2402.17081</a> [<a href="/pdf/2402.17081" title="Download PDF">pdf</a>, <a href="/format/2402.17081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI  Judge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rangan%2C+K">Keshav Rangan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yiqiao Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">This study presents an innovative enhancement to retrieval-augmented
generation (RAG) systems by seamlessly integrating fine-tuned large language
models (LLMs) with vector databases. This integration capitalizes on the
combined strengths of structured data retrieval and the nuanced comprehension
provided by advanced LLMs. Central to our approach are the LoRA and QLoRA
methodologies, which stand at the forefront of model refinement through
parameter-efficient fine-tuning and memory optimization. A novel feature of our
research is the incorporation of user feedback directly into the training
process, ensuring the model's continuous adaptation to user expectations and
thus, improving its performance and applicability. Additionally, we introduce a
Quantized Influence Measure (QIM) as an innovative "AI Judge" mechanism to
enhance the precision of result selection, further refining the system's
accuracy. Accompanied by an executive diagram and a detailed algorithm for
fine-tuning QLoRA, our work provides a comprehensive framework for implementing
these advancements within chatbot technologies. This research contributes
significant insights into LLM optimization for specific uses and heralds new
directions for further development in retrieval-augmented models. Through
extensive experimentation and analysis, our findings lay a robust foundation
for future advancements in chatbot technology and retrieval systems, marking a
significant step forward in the creation of more sophisticated, precise, and
user-centric conversational AI systems.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17082" title="Abstract">arXiv:2402.17082</a> [<a href="/pdf/2402.17082" title="Download PDF">pdf</a>, <a href="/format/2402.17082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deconstructing the Veneer of Simplicity: Co-Designing Introductory  Generative AI Workshops with Local Entrepreneurs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kotturi%2C+Y">Yasmine Kotturi</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+A">Angel Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Ford%2C+G">Glenn Ford</a>, 
<a href="/search/cs?searchtype=author&query=Skirpan%2C+M">Michael Skirpan</a>, 
<a href="/search/cs?searchtype=author&query=Bigham%2C+J+P">Jeffrey P. Bigham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative AI platforms and features are permeating many aspects of work.
Entrepreneurs from lean economies in particular are well positioned to
outsource tasks to generative AI given limited resources. In this paper, we
work to address a growing disparity in use of these technologies by building on
a four-year partnership with a local entrepreneurial hub dedicated to equity in
tech and entrepreneurship. Together, we co-designed an interactive workshops
series aimed to onboard local entrepreneurs to generative AI platforms.
Alongside four community-driven and iterative workshops with entrepreneurs
across five months, we conducted interviews with 15 local entrepreneurs and
community providers. We detail the importance of communal and supportive
exposure to generative AI tools for local entrepreneurs, scaffolding actionable
use (and supporting non-use), demystifying generative AI technologies by
emphasizing entrepreneurial power, while simultaneously deconstructing the
veneer of simplicity to address the many operational skills needed for
successful application.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17088" title="Abstract">arXiv:2402.17088</a> [<a href="/pdf/2402.17088" title="Download PDF">pdf</a>, <a href="/format/2402.17088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cell-Constrained Particles for Incompressible Fluids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levi%2C+Z">Zohar Levi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Incompressibility is a fundamental condition in most fluid models.
Accumulation of simulation errors violates it and causes volume loss. Past work
suggested correction methods to battle it. These methods, however, are
imperfect and in some cases inadequate. We present a method for fluid
simulation that strictly enforces incompressibility based on a grid-related
definition of discrete incompressibility.
<br />We formulate a linear programming (LP) problem that bounds the number of
particles that end up in each grid cell. A variant of the band method is
offered for acceleration, which requires special constraints to ensure volume
preservation. Further acceleration is achieved by simplifying the problem and
adding a special band correction step that is formulated as a minimum-cost flow
problem (MCFP). We also address coupling with solids in our framework and
demonstrate advantages over prior work.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17091" title="Abstract">arXiv:2402.17091</a> [<a href="/pdf/2402.17091" title="Download PDF">pdf</a>, <a href="/format/2402.17091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural Teacher-Student Normality Learning for Multi-Class Anomaly  Detection and Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Hanqiu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingyu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual anomaly detection is a challenging open-set task aimed at identifying
unknown anomalous patterns while modeling normal data. The knowledge
distillation paradigm has shown remarkable performance in one-class anomaly
detection by leveraging teacher-student network feature comparisons. However,
extending this paradigm to multi-class anomaly detection introduces novel
scalability challenges. In this study, we address the significant performance
degradation observed in previous teacher-student models when applied to
multi-class anomaly detection, which we identify as resulting from cross-class
interference. To tackle this issue, we introduce a novel approach known as
Structural Teacher-Student Normality Learning (SNL): (1) We propose
spatial-channel distillation and intra-&amp;inter-affinity distillation techniques
to measure structural distance between the teacher and student networks. (2) We
introduce a central residual aggregation module (CRAM) to encapsulate the
normal representation space of the student network. We evaluate our proposed
approach on two anomaly detection datasets, MVTecAD and VisA. Our method
surpasses the state-of-the-art distillation-based algorithms by a significant
margin of 3.9% and 1.5% on MVTecAD and 1.2% and 2.5% on VisA in the multi-class
anomaly detection and localization tasks, respectively. Furthermore, our
algorithm outperforms the current state-of-the-art unified models on both
MVTecAD and VisA.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17092" title="Abstract">arXiv:2402.17092</a> [<a href="/pdf/2402.17092" title="Download PDF">pdf</a>, <a href="/format/2402.17092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Pioneering Study and An Innovative Information Theory-based Approach  to Enhance The Transparency in Phishing Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V">Van Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tingmin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xingliang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Grobler%2C+M">Marthie Grobler</a>, 
<a href="/search/cs?searchtype=author&query=Nepal%2C+S">Surya Nepal</a>, 
<a href="/search/cs?searchtype=author&query=Rudolph%2C+C">Carsten Rudolph</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Phishing attacks have become a serious and challenging issue for detection,
explanation, and defense. Despite more than a decade of research on phishing,
encompassing both technical and non-technical remedies, phishing continues to
be a serious problem. Nowadays, AI-based phishing detection stands out as one
of the most effective solutions for defending against phishing attacks by
providing vulnerability (i.e., phishing or benign) predictions for the data.
However, it lacks explainability in terms of providing comprehensive
interpretations for the predictions, such as identifying the specific
information that causes the data to be classified as phishing. To this end, we
propose an innovative deep learning-based approach for email (the most common
phishing way) phishing attack localization. Our method can not only predict the
vulnerability of the email data but also automatically figure out and highlight
the most important and phishing-relevant information (i.e., sentences) in each
phishing email. The selected information indicates useful explanations for the
vulnerability of the phishing email data. The rigorous experiments on seven
real-world email datasets show the effectiveness and advancement of our
proposed method in providing comprehensive explanations (by successfully
figuring out the most important and phishing-relevant information in phishing
emails) for the vulnerability of corresponding phishing data with higher
performances from nearly (1% to 3%) and (1% to 4%) in two main Label-Accuracy
and Cognitive-True-Positive measures, respectively, compared to the
state-of-the-art potential baselines.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17097" title="Abstract">arXiv:2402.17097</a> [<a href="/pdf/2402.17097" title="Download PDF">pdf</a>, <a href="/format/2402.17097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM  Responses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jeongeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yoonho Chang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+C">Chanyeol Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junseong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+J">Jy-yong Sohn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mitigating hallucination issues is one of the main challenges of LLMs we need
to overcome, in order to reliably use them in real-world scenarios. Recently,
various methods are proposed to check the factual errors in the LLM-generated
texts and revise them accordingly, to reduce the hallucination issue. In this
paper, we propose Re-Ex, a method of revising LLM-generated texts, which
introduces a novel step dubbed as the factual error explanation step. Re-Ex
revises the initial response of LLMs using 3-steps: first, external tools are
used to get the evidences on the factual errors in the response; second, LLMs
are instructed to explain the problematic parts of the response based on the
evidences gathered in the first step; finally, LLMs revise the response using
the explanation obtained in the second step. In addition to the explanation
step, we propose new prompting techniques to reduce the amount of tokens and
wall-clock time required for the response revision process. Compared with
existing methods including Factool, CoVE, and RARR, Re-Ex provides better
revision performance with less time and fewer tokens in multiple benchmarks.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17098" title="Abstract">arXiv:2402.17098</a> [<a href="/pdf/2402.17098" title="Download PDF">pdf</a>, <a href="/format/2402.17098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In Defense and Revival of Bayesian Filtering for Thermal Infrared Object  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shi-Min Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Feng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ru-Yue Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Fujita%2C+H">Hamido Fujita</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep learning-based methods monopolize the latest research in the field of
thermal infrared (TIR) object tracking. However, relying solely on deep
learning models to obtain better tracking results requires carefully selecting
feature information that is beneficial to representing the target object and
designing a reasonable template update strategy, which undoubtedly increases
the difficulty of model design. Thus, recent TIR tracking methods face many
challenges in complex scenarios. This paper introduces a novel Deep Bayesian
Filtering (DBF) method to enhance TIR tracking in these challenging situations.
DBF is distinctive in its dual-model structure: the system and observation
models. The system model leverages motion data to estimate the potential
positions of the target object based on two-dimensional Brownian motion, thus
generating a prior probability. Following this, the observation model comes
into play upon capturing the TIR image. It serves as a classifier and employs
infrared information to ascertain the likelihood of these estimated positions,
creating a likelihood probability. According to the guidance of the two models,
the position of the target object can be determined, and the template can be
dynamically updated. Experimental analysis across several benchmark datasets
reveals that DBF achieves competitive performance, surpassing most existing TIR
tracking methods in complex scenarios.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17101" title="Abstract">arXiv:2402.17101</a> [<a href="/pdf/2402.17101" title="Download PDF">pdf</a>, <a href="/format/2402.17101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T-HITL Effectively Addresses Problematic Associations in Image  Generation and Maintains Overall Visual Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Epstein%2C+S">Susan Epstein</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Vecchiato%2C+A">Alessandro Vecchiato</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Ankit Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative AI image models may inadvertently generate problematic
representations of people. Past research has noted that millions of users
engage daily across the world with these models and that the models, including
through problematic representations of people, have the potential to compound
and accelerate real-world discrimination and other harms (Bianchi et al, 2023).
In this paper, we focus on addressing the generation of problematic
associations between demographic groups and semantic concepts that may reflect
and reinforce negative narratives embedded in social data. Building on
sociological literature (Blumer, 1958) and mapping representations to model
behaviors, we have developed a taxonomy to study problematic associations in
image generation models. We explore the effectiveness of fine tuning at the
model level as a method to address these associations, identifying a potential
reduction in visual quality as a limitation of traditional fine tuning. We also
propose a new methodology with twice-human-in-the-loop (T-HITL) that promises
improvements in both reducing problematic associations and also maintaining
visual quality. We demonstrate the effectiveness of T-HITL by providing
evidence of three problematic associations addressed by T-HITL at the model
level. Our contributions to scholarship are two-fold. By defining problematic
associations in the context of machine learning models and generative AI, we
introduce a conceptual and technical taxonomy for addressing some of these
associations. Finally, we provide a method, T-HITL, that addresses these
associations and simultaneously maintains visual quality of image model
generations. This mitigation need not be a tradeoff, but rather an enhancement.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17104" title="Abstract">arXiv:2402.17104</a> [<a href="/pdf/2402.17104" title="Download PDF">pdf</a>, <a href="/format/2402.17104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Perturbations of Physical Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bassett%2C+R+L">Robert L. Bassett</a>, 
<a href="/search/cs?searchtype=author&query=Van+Dellen%2C+A">Austin Van Dellen</a>, 
<a href="/search/cs?searchtype=author&query=Austin%2C+A+P">Anthony P. Austin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Signal Processing (eess.SP); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We investigate the vulnerability of computer-vision-based signal classifiers
to adversarial perturbations of their inputs, where the signals and
perturbations are subject to physical constraints. We consider a scenario in
which a source and interferer emit signals that propagate as waves to a
detector, which attempts to classify the source by analyzing the spectrogram of
the signal it receives using a pre-trained neural network. By solving
PDE-constrained optimization problems, we construct interfering signals that
cause the detector to misclassify the source even though the perturbations to
the spectrogram of the received signal are nearly imperceptible. Though such
problems can have millions of decision variables, we introduce methods to solve
them efficiently. Our experiments demonstrate that one can compute effective
and physically realizable adversarial perturbations for a variety of machine
learning models under various physical conditions.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17108" title="Abstract">arXiv:2402.17108</a> [<a href="/pdf/2402.17108" title="Download PDF">pdf</a>, <a href="/ps/2402.17108" title="Download PostScript">ps</a>, <a href="/format/2402.17108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and  Limited Liability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Collina%2C+N">Natalie Collina</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Varun Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Aaron Roth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
<p class="mathjax">We study a repeated contracting setting in which a Principal adaptively
chooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic,
and so a mechanism for the Principal induces a $T$-round extensive form game
amongst the Agents. We give several results aimed at understanding an
under-explored aspect of contract theory -- the game induced when choosing an
Agent to contract with. First, we show that this game admits a pure-strategy
\emph{non-responsive} equilibrium amongst the Agents -- informally an
equilibrium in which the Agent's actions depend on the history of realized
states of nature, but not on the history of each other's actions, and so avoids
the complexities of collusion and threats. Next, we show that if the Principal
selects Agents using a \emph{monotone} bandit algorithm, then for any concave
contract, in any such equilibrium, the Principal obtains no regret to
contracting with the best Agent in hindsight -- not just given their realized
actions, but also to the counterfactual world in which they had offered a
guaranteed $T$-round contract to the best Agent in hindsight, which would have
induced a different sequence of actions. Finally, we show that if the Principal
selects Agents using a monotone bandit algorithm which guarantees no
swap-regret, then the Principal can additionally offer only limited liability
contracts (in which the Agent never needs to pay the Principal) while getting
no-regret to the counterfactual world in which she offered a linear contract to
the best Agent in hindsight -- despite the fact that linear contracts are not
limited liability. We instantiate this theorem by demonstrating the existence
of a monotone no swap-regret bandit algorithm, which to our knowledge has not
previously appeared in the literature.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17109" title="Abstract">arXiv:2402.17109</a> [<a href="/pdf/2402.17109" title="Download PDF">pdf</a>, <a href="/format/2402.17109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replicating Electoral Success
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tomlinson%2C+K">Kiran Tomlinson</a>, 
<a href="/search/cs?searchtype=author&query=Namjoshi%2C+T">Tanvi Namjoshi</a>, 
<a href="/search/cs?searchtype=author&query=Ugander%2C+J">Johan Ugander</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">A core tension in the study of plurality elections is the clash between the
classic Hotelling-Downs model, which predicts that two office-seeking
candidates should position themselves at the median voter's policy, and the
empirical observation that real-world democracies often have two major parties
with divergent policies. Motivated by this tension and drawing from bounded
rationality, we introduce a dynamic model of candidate positioning based on a
simple behavioral heuristic: candidates imitate the policy of previous winners.
The resulting model is closely connected to evolutionary replicator dynamics
and exhibits complex behavior, despite its simplicity. For
uniformly-distributed voters, we prove that when there are $k = 2$, $3$, or $4$
candidates per election, any symmetric candidate distribution converges over
time to a concentration of candidates at the center. With $k \ge 5$, however,
we prove that the candidate distribution does not converge to the center. For
initial distributions without any extreme candidates, we prove a stronger
statement than non-convergence, showing that the density in an interval around
the center goes to zero when $k \ge 5$. As a matter of robustness, our
conclusions are qualitatively unchanged if a small fraction of candidates are
not winner-copiers and are instead positioned uniformly at random. Beyond our
theoretical analysis, we illustrate our results in simulation; for five or more
candidates, we find a tendency towards the emergence of two clusters, a
mechanism suggestive of Duverger's Law, the empirical finding that plurality
leads to two-party systems. Our simulations also explore several variations of
the model, including non-uniform voter distributions and other forms of noise,
which exhibit similar convergence patterns. Finally, we discuss the
relationship between our model and prior work on strategic equilibria of
candidate positioning games.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17110" title="Abstract">arXiv:2402.17110</a> [<a href="/pdf/2402.17110" title="Download PDF">pdf</a>, <a href="/format/2402.17110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sinkhorn Distance Minimization for Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+X">Xiao Cui</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yulei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuting Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Enwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zihan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wengang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Houqiang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Knowledge distillation (KD) has been widely adopted to compress large
language models (LLMs). Existing KD methods investigate various divergence
measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL),
and Jensen-Shannon (JS) divergences. However, due to limitations inherent in
their assumptions and definitions, these measures fail to deliver effective
supervision when few distribution overlap exists between the teacher and the
student. In this paper, we show that the aforementioned KL, RKL, and JS
divergences respectively suffer from issues of mode-averaging, mode-collapsing,
and mode-underestimation, which deteriorates logits-based KD for diverse NLP
tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the
Sinkhorn distance to ensure a nuanced and precise assessment of the disparity
between teacher and student distributions. Besides, profit by properties of the
Sinkhorn metric, we can get rid of sample-wise KD that restricts the perception
of divergence in each teacher-student sample pair. Instead, we propose a
batch-wise reformulation to capture geometric intricacies of distributions
across samples in the high-dimensional space. Comprehensive evaluation on GLUE
and SuperGLUE, in terms of comparability, validity, and generalizability,
highlights our superiority over state-of-the-art methods on all kinds of LLMs
with encoder-only, encoder-decoder, and decoder-only architectures.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17113" title="Abstract">arXiv:2402.17113</a> [<a href="/pdf/2402.17113" title="Download PDF">pdf</a>, <a href="/format/2402.17113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transparent Image Layer Diffusion using Latent Transparency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lvmin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Agrawala%2C+M">Maneesh Agrawala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 37 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We present LayerDiffusion, an approach enabling large-scale pretrained latent
diffusion models to generate transparent images. The method allows generation
of single transparent images or of multiple transparent layers. The method
learns a "latent transparency" that encodes alpha channel transparency into the
latent manifold of a pretrained latent diffusion model. It preserves the
production-ready quality of the large diffusion model by regulating the added
transparency as a latent offset with minimal changes to the original latent
distribution of the pretrained model. In this way, any latent diffusion model
can be converted into a transparent image generator by finetuning it with the
adjusted latent space. We train the model with 1M transparent image layer pairs
collected using a human-in-the-loop collection scheme. We show that latent
transparency can be applied to different open source image generators, or be
adapted to various conditional control systems to achieve applications like
foreground/background-conditioned layer generation, joint layer generation,
structural control of layer contents, etc. A user study finds that in most
cases (97%) users prefer our natively generated transparent content over
previous ad-hoc solutions such as generating and then matting. Users also
report the quality of our generated transparent images is comparable to real
commercial transparent assets like Adobe Stock.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17115" title="Abstract">arXiv:2402.17115</a> [<a href="/pdf/2402.17115" title="Download PDF">pdf</a>, <a href="/format/2402.17115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CharNeRF: 3D Character Generation from Concept Art
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+E">Eddy Chu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Raissi%2C+C">Chedy Raissi</a>, 
<a href="/search/cs?searchtype=author&query=Bhojan%2C+A">Anand Bhojan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">3D modeling holds significant importance in the realms of AR/VR and gaming,
allowing for both artistic creativity and practical applications. However, the
process is often time-consuming and demands a high level of skill. In this
paper, we present a novel approach to create volumetric representations of 3D
characters from consistent turnaround concept art, which serves as the standard
input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been
a game-changer in image-based 3D reconstruction, to the best of our knowledge,
there is no known research that optimizes the pipeline for concept art. To
harness the potential of concept art, with its defined body poses and specific
view angles, we propose encoding it as priors for our model. We train the
network to make use of these priors for various 3D points through a learnable
view-direction-attended multi-head self-attention layer. Additionally, we
demonstrate that a combination of ray sampling and surface sampling enhances
the inference capabilities of our network. Our model is able to generate
high-quality 360-degree views of characters. Subsequently, we provide a simple
guideline to better leverage our model to extract the 3D mesh. It is important
to note that our model's inferencing capabilities are influenced by the
training data's characteristics, primarily focusing on characters with a single
head, two arms, and two legs. Nevertheless, our methodology remains versatile
and adaptable to concept art from diverse subject matters, without imposing any
specific assumptions on the data.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17117" title="Abstract">arXiv:2402.17117</a> [<a href="/pdf/2402.17117" title="Download PDF">pdf</a>, <a href="/ps/2402.17117" title="Download PostScript">ps</a>, <a href="/format/2402.17117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning (DRL)-based Methods for Serverless Stream  Processing Engines: A Vision, Architectural Elements, and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Read%2C+M+R">Maria R. Read</a>, 
<a href="/search/cs?searchtype=author&query=Dehury%2C+C">Chinmaya Dehury</a>, 
<a href="/search/cs?searchtype=author&query=Srirama%2C+S+N">Satish Narayana Srirama</a>, 
<a href="/search/cs?searchtype=author&query=Buyya%2C+R">Rajkumar Buyya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Streaming applications are becoming widespread across an extensive range of
business domains as an increasing number of sources continuously produce data
that need to be processed and analysed in real time. Modern businesses are
aggressively using streaming data to generate valuable knowledge that can be
used to automate processes, help decision-making, optimize resource usage, and
ultimately generate revenue for the organization. Despite their increased
adoption and tangible benefits, support for the automated deployment and
management of streaming applications is yet to emerge. Although a plethora of
stream management systems have flooded the open source community in recent
years, all of the existing frameworks demand a considerably challenging and
lengthy effort from human operators to manually and continuously tune their
configuration and deployment environment in order to reach and maintain the
desired performance goals. To address these challenges, this article proposes a
vision for creating Deep Reinforcement Learning (DRL)-based methods for
transforming stream processing engines into self-managed serverless solutions.
This will lead to an increase in productivity as engineers can focus on the
actual development process, an increase in application performance potentially
leading to reduced response times and more accurate and meaningful results, and
a considerable decrease in operational costs for organizations.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17119" title="Abstract">arXiv:2402.17119</a> [<a href="/pdf/2402.17119" title="Download PDF">pdf</a>, <a href="/format/2402.17119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Creating Suspenseful Stories: Iterative Planning with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+K">Kaige Xie</a>, 
<a href="/search/cs?searchtype=author&query=Riedl%2C+M">Mark Riedl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automated story generation has been one of the long-standing challenges in
NLP. Among all dimensions of stories, suspense is very common in human-written
stories but relatively under-explored in AI-generated stories. While recent
advances in large language models (LLMs) have greatly promoted language
generation in general, state-of-the-art LLMs are still unreliable when it comes
to suspenseful story generation. We propose a novel iterative-prompting-based
planning method that is grounded in two theoretical foundations of story
suspense from cognitive psychology and narratology. This theory-grounded method
works in a fully zero-shot manner and does not rely on any supervised story
corpora. To the best of our knowledge, this paper is the first attempt at
suspenseful story generation with LLMs. Extensive human evaluations of the
generated suspenseful stories demonstrate the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17120" title="Abstract">arXiv:2402.17120</a> [<a href="/pdf/2402.17120" title="Download PDF">pdf</a>, <a href="/format/2402.17120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable  Machine Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seber%2C+P">Pedro Seber</a>, 
<a href="/search/cs?searchtype=author&query=Braatz%2C+R+D">Richard D. Braatz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Interpretable architectures can have advantages over black-box architectures,
and interpretability is essential for the application of machine learning in
critical settings, such as aviation or medicine. However, the simplest, most
commonly used interpretable architectures (such as LASSO or EN) are limited to
linear predictions and have poor feature selection capabilities. In this work,
we introduce the LASSO-Clip-EN (LCEN) algorithm for the creation of nonlinear,
interpretable machine learning models. LCEN is tested on a wide variety of
artificial and empirical datasets, creating more accurate, sparser models than
other commonly used architectures. These experiments reveal that LCEN is robust
against many issues typically present in datasets and modeling, including
noise, multicollinearity, data scarcity, and hyperparameter variance. LCEN is
also able to rediscover multiple physical laws from empirical data and, for
processes with no known physical laws, LCEN achieves better results than many
other dense and sparse methods -- including using 10.8 times fewer features
than dense methods and 8.1 times fewer features than EN on one dataset, and is
comparable to an ANN on another dataset.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17124" title="Abstract">arXiv:2402.17124</a> [<a href="/pdf/2402.17124" title="Download PDF">pdf</a>, <a href="/format/2402.17124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fact-and-Reflection (FaR) Improves Confidence Calibration of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xinran Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaoman Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wenlin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongshuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianshu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">For a LLM to be trustworthy, its confidence level should be well-calibrated
with its actual performance. While it is now common sense that LLM performances
are greatly impacted by prompts, the confidence calibration in prompting LLMs
has yet to be thoroughly explored. In this paper, we explore how different
prompting strategies influence LLM confidence calibration and how it could be
improved. We conduct extensive experiments on six prompting methods in the
question-answering context and we observe that, while these methods help
improve the expected LLM calibration, they also trigger LLMs to be
over-confident when responding to some instances. Inspired by human cognition,
we propose Fact-and-Reflection (FaR) prompting, which improves the LLM
calibration in two steps. First, FaR elicits the known "facts" that are
relevant to the input prompt from the LLM. And then it asks the model to
"reflect" over them to generate the final answer. Experiments show that FaR
prompting achieves significantly better calibration; it lowers the Expected
Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR
prompting even elicits the capability of verbally expressing concerns in less
confident scenarios, which helps trigger retrieval augmentation for solving
these harder instances.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17127" title="Abstract">arXiv:2402.17127</a> [<a href="/pdf/2402.17127" title="Download PDF">pdf</a>, <a href="/format/2402.17127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Study: Enhancing Voice Spoofing Detection Models with  wav2vec 2.0
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+T">Taein Kang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Soyul Han</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sunmook Choi</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Jaejin Seo</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+S">Sanghyeok Chung</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seungeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Seungsang Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+I">Il-Youp Kwak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Conventional spoofing detection systems have heavily relied on the use of
handcrafted features derived from speech data. However, a notable shift has
recently emerged towards the direct utilization of raw speech waveforms, as
demonstrated by methods like SincNet filters. This shift underscores the demand
for more sophisticated audio sample features. Moreover, the success of deep
learning models, particularly those utilizing large pretrained wav2vec 2.0 as a
featurization front-end, highlights the importance of refined feature encoders.
In response, this research assessed the representational capability of wav2vec
2.0 as an audio feature extractor, modifying the size of its pretrained
Transformer layers through two key adjustments: (1) selecting a subset of
layers starting from the leftmost one and (2) fine-tuning a portion of the
selected layers from the rightmost one. We complemented this analysis with five
spoofing detection back-end models, with a primary focus on AASIST, enabling us
to pinpoint the optimal configuration for the selection and fine-tuning
process. In contrast to conventional handcrafted features, our investigation
identified several spoofing detection systems that achieve state-of-the-art
performance in the ASVspoof 2019 LA dataset. This comprehensive exploration
offers valuable insights into feature selection strategies, advancing the field
of spoofing detection.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17128" title="Abstract">arXiv:2402.17128</a> [<a href="/pdf/2402.17128" title="Download PDF">pdf</a>, <a href="/format/2402.17128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OSCaR: Object State Captioning and State Change Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+N">Nguyen Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+J">Jing Bi</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+A">Ali Vosoughi</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yapeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Fazli%2C+P">Pooyan Fazli</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenliang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The capability of intelligent models to extrapolate and comprehend changes in
object states is a crucial yet demanding aspect of AI research, particularly
through the lens of human interaction in real-world settings. This task
involves describing complex visual environments, identifying active objects,
and interpreting their changes as conveyed through language. Traditional
methods, which isolate object captioning and state change detection, offer a
limited view of dynamic environments. Moreover, relying on a small set of
symbolic words to represent changes has restricted the expressiveness of
language. To address these challenges, in this paper, we introduce the Object
State Captioning and State Change Representation (OSCaR) dataset and benchmark.
OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique
objects from various egocentric video collections. It sets a new testbed for
evaluating multimodal large language models (MLLMs). Our experiments
demonstrate that while MLLMs show some skill, they lack a full understanding of
object state changes. The benchmark includes a fine-tuned model that, despite
initial capabilities, requires significant improvements in accuracy and
generalization ability for effective understanding of these changes. Our code
and dataset are available at https://github.com/nguyennm1024/OSCaR.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17129" title="Abstract">arXiv:2402.17129</a> [<a href="/pdf/2402.17129" title="Download PDF">pdf</a>, <a href="/format/2402.17129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Side Information-Driven Session-based Recommendation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaokun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongfei Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a survey on side information-driven session-based recommendation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The session-based recommendation (SBR) garners increasing attention due to
its ability to predict anonymous user intents within limited interactions.
Emerging efforts incorporate various kinds of side information into their
methods for enhancing task performance. In this survey, we thoroughly review
the side information-driven session-based recommendation from a data-centric
perspective. Our survey commences with an illustration of the motivation and
necessity behind this research topic. This is followed by a detailed
exploration of various benchmarks rich in side information, pivotal for
advancing research in this field. Moreover, we delve into how these diverse
types of side information enhance SBR, underscoring their characteristics and
utility. A systematic review of research progress is then presented, offering
an analysis of the most recent and representative developments within this
topic. Finally, we present the future prospects of this vibrant topic.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17130" title="Abstract">arXiv:2402.17130</a> [<a href="/pdf/2402.17130" title="Download PDF">pdf</a>, <a href="/format/2402.17130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Map-Free Exploration for Confirming the Absence of a  Radioactive Source
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lepowsky%2C+E">Eric Lepowsky</a>, 
<a href="/search/cs?searchtype=author&query=Snyder%2C+D">David Snyder</a>, 
<a href="/search/cs?searchtype=author&query=Glaser%2C+A">Alexander Glaser</a>, 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+A">Anirudha Majumdar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, in submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Performing an inspection task while maintaining the privacy of the inspected
site is a challenging balancing act. In this work, we are motivated by the
future of nuclear arms control verification, which requires both a high level
of privacy and guaranteed correctness. For scenarios with limitations on
sensors and stored information due to the potentially secret nature of
observable features, we propose a robotic verification procedure that provides
map-free exploration to perform a source verification task without requiring,
nor revealing, any task-irrelevant, site-specific information. We provide
theoretical guarantees on the privacy and correctness of our approach,
validated by extensive simulated and hardware experiments.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17131" title="Abstract">arXiv:2402.17131</a> [<a href="/pdf/2402.17131" title="Download PDF">pdf</a>, <a href="/format/2402.17131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers  and RNNs Trained with a New Loss Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seber%2C+P">Pedro Seber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Molecular Networks (q-bio.MN)

</div>
<p class="mathjax">Glycosylation, a protein modification, has multiple essential functional and
structural roles. O-GlcNAcylation, a subtype of glycosylation, has the
potential to be an important target for therapeutics, but methods to reliably
predict O-GlcNAcylation sites had not been available until 2023; a 2021 review
correctly noted that published models were insufficient and failed to
generalize. Moreover, many are no longer usable. In 2023, a considerably better
RNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset
was published. This article first sought to improve these metrics using
transformer encoders. While transformers displayed high performance on this
dataset, their performance was inferior to that of the previously published
RNN. We then created a new loss function, which we call the weighted focal
differentiable MCC, to improve the performance of classification models. RNN
models trained with this new function display superior performance to models
trained using the weighted cross-entropy loss; this new function can also be
used to fine-tune trained models. A two-cell RNN trained with this loss
achieves state-of-the-art performance in O-GlcNAcylation site prediction with
an F$_1$ score of 38.82% and an MCC of 38.21% on that large dataset.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17133" title="Abstract">arXiv:2402.17133</a> [<a href="/pdf/2402.17133" title="Download PDF">pdf</a>, <a href="/format/2402.17133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAM-DiffSR: Structure-Modulated Diffusion Model for Image  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengcheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhiwei Hao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yehui Tang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jianyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion-based super-resolution (SR) models have recently garnered
significant attention due to their potent restoration capabilities. But
conventional diffusion models perform noise sampling from a single
distribution, constraining their ability to handle real-world scenes and
complex textures across semantic regions. With the success of segment anything
model (SAM), generating sufficiently fine-grained region masks can enhance the
detail recovery of diffusion-based SR model. However, directly integrating SAM
into SR models will result in much higher computational cost. In this paper, we
propose the SAM-DiffSR model, which can utilize the fine-grained structure
information from SAM in the process of sampling noise to improve the image
quality without additional computational cost during inference. In the process
of training, we encode structural position information into the segmentation
mask from SAM. Then the encoded mask is integrated into the forward diffusion
process by modulating it to the sampled noise. This adjustment allows us to
independently adapt the noise mean within each corresponding segmentation area.
The diffusion model is trained to estimate this modulated noise. Crucially, our
proposed framework does NOT change the reverse diffusion process and does NOT
require SAM at inference. Experimental results demonstrate the effectiveness of
our proposed method, showcasing superior performance in suppressing artifacts,
and surpassing existing diffusion-based methods by 0.74 dB at the maximum in
terms of PSNR on DIV2K dataset. The code and dataset are available at
https://github.com/lose4578/SAM-DiffSR.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17134" title="Abstract">arXiv:2402.17134</a> [<a href="/pdf/2402.17134" title="Download PDF">pdf</a>, <a href="/format/2402.17134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficiently Leveraging Linguistic Priors for Scene Text Spotting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+N">Nguyen Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yapeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenliang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Incorporating linguistic knowledge can improve scene text recognition, but it
is questionable whether the same holds for scene text spotting, which typically
involves text detection and recognition. This paper proposes a method that
leverages linguistic knowledge from a large text corpus to replace the
traditional one-hot encoding used in auto-regressive scene text spotting and
recognition models. This allows the model to capture the relationship between
characters in the same word. Additionally, we introduce a technique to generate
text distributions that align well with scene text datasets, removing the need
for in-domain fine-tuning. As a result, the newly created text distributions
are more informative than pure one-hot encoding, leading to improved spotting
and recognition performance. Our method is simple and efficient, and it can
easily be integrated into existing auto-regressive-based approaches.
Experimental results show that our method not only improves recognition
accuracy but also enables more accurate localization of words. It significantly
improves both state-of-the-art scene text spotting and recognition pipelines,
achieving state-of-the-art results on several benchmarks.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17135" title="Abstract">arXiv:2402.17135</a> [<a href="/pdf/2402.17135" title="Download PDF">pdf</a>, <a href="/format/2402.17135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Zero-Shot Reinforcement Learning via Functional Reward  Encodings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frans%2C+K">Kevin Frans</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seohong Park</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Can we pre-train a generalist agent from a large amount of unlabeled offline
trajectories such that it can be immediately adapted to any new downstream
tasks in a zero-shot manner? In this work, we present a functional reward
encoding (FRE) as a general, scalable solution to this zero-shot RL problem.
Our main idea is to learn functional representations of any arbitrary tasks by
encoding their state-reward samples using a transformer-based variational
auto-encoder. This functional encoding not only enables the pre-training of an
agent from a wide diversity of general unsupervised reward functions, but also
provides a way to solve any new downstream tasks in a zero-shot manner, given a
small number of reward-annotated samples. We empirically show that FRE agents
trained on diverse random unsupervised reward functions can generalize to solve
novel tasks in a range of simulated robotic benchmarks, often outperforming
previous zero-shot RL and offline RL methods. Code for this project is provided
at: https://github.com/kvfrans/fre
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17139" title="Abstract">arXiv:2402.17139</a> [<a href="/pdf/2402.17139" title="Download PDF">pdf</a>, <a href="/format/2402.17139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video as the New Language for Real-World Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sherry Yang</a>, 
<a href="/search/cs?searchtype=author&query=Walker%2C+J">Jacob Walker</a>, 
<a href="/search/cs?searchtype=author&query=Parker-Holder%2C+J">Jack Parker-Holder</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Bruce%2C+J">Jake Bruce</a>, 
<a href="/search/cs?searchtype=author&query=Barreto%2C+A">Andre Barreto</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Schuurmans%2C+D">Dale Schuurmans</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Both text and video data are abundant on the internet and support large-scale
self-supervised learning through next token or frame prediction. However, they
have not been equally leveraged: language models have had significant
real-world impact, whereas video generation has remained largely limited to
media entertainment. Yet video data captures important information about the
physical world that is difficult to express in language. To address this gap,
we discuss an under-appreciated opportunity to extend video generation to solve
tasks in the real world. We observe how, akin to language, video can serve as a
unified interface that can absorb internet knowledge and represent diverse
tasks. Moreover, we demonstrate how, like language models, video generation can
serve as planners, agents, compute engines, and environment simulators through
techniques such as in-context learning, planning and reinforcement learning. We
identify major impact opportunities in domains such as robotics, self-driving,
and science, supported by recent work that demonstrates how such advanced
capabilities in video generation are plausibly within reach. Lastly, we
identify key challenges in video generation that mitigate progress. Addressing
these challenges will enable video generation models to demonstrate unique
value alongside language models in a wider array of AI applications.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17143" title="Abstract">arXiv:2402.17143</a> [<a href="/pdf/2402.17143" title="Download PDF">pdf</a>, <a href="/format/2402.17143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Efficient Scheduling with Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balkanski%2C+E">Eric Balkanski</a>, 
<a href="/search/cs?searchtype=author&query=Perivier%2C+N">Noemie Perivier</a>, 
<a href="/search/cs?searchtype=author&query=Stein%2C+C">Clifford Stein</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hao-Ting Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">An important goal of modern scheduling systems is to efficiently manage power
usage. In energy-efficient scheduling, the operating system controls the speed
at which a machine is processing jobs with the dual objective of minimizing
energy consumption and optimizing the quality of service cost of the resulting
schedule. Since machine-learned predictions about future requests can often be
learned from historical data, a recent line of work on learning-augmented
algorithms aims to achieve improved performance guarantees by leveraging
predictions. In particular, for energy-efficient scheduling, Bamas et. al.
[BamasMRS20] and Antoniadis et. al. [antoniadis2021novel] designed algorithms
with predictions for the energy minimization with deadlines problem and
achieved an improved competitive ratio when the prediction error is small while
also maintaining worst-case bounds even when the prediction error is
arbitrarily large.
<br />In this paper, we consider a general setting for energy-efficient scheduling
and provide a flexible learning-augmented algorithmic framework that takes as
input an offline and an online algorithm for the desired energy-efficient
scheduling problem. We show that, when the prediction error is small, this
framework gives improved competitive ratios for many different energy-efficient
scheduling problems, including energy minimization with deadlines, while also
maintaining a bounded competitive ratio regardless of the prediction error.
Finally, we empirically demonstrate that this framework achieves an improved
performance on real and synthetic datasets.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17144" title="Abstract">arXiv:2402.17144</a> [<a href="/pdf/2402.17144" title="Download PDF">pdf</a>, <a href="/format/2402.17144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metasql: A Generate-then-Rank Framework for Natural Language to SQL  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yuankai Fan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhenying He</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+T">Tonghui Ren</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Can Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+Y">Yinan Jing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+S">X.Sean Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The Natural Language Interface to Databases (NLIDB) empowers non-technical
users with database access through intuitive natural language (NL)
interactions. Advanced approaches, utilizing neural sequence-to-sequence models
or large-scale language models, typically employ auto-regressive decoding to
generate unique SQL queries sequentially. While these translation models have
greatly improved the overall translation accuracy, surpassing 70% on NLIDB
benchmarks, the use of auto-regressive decoding to generate single SQL queries
may result in sub-optimal outputs, potentially leading to erroneous
translations. In this paper, we propose Metasql, a unified generate-then-rank
framework that can be flexibly incorporated with existing NLIDBs to
consistently improve their translation accuracy. Metasql introduces query
metadata to control the generation of better SQL query candidates and uses
learning-to-rank algorithms to retrieve globally optimized queries.
Specifically, Metasql first breaks down the meaning of the given NL query into
a set of possible query metadata, representing the basic concepts of the
semantics. These metadata are then used as language constraints to steer the
underlying translation model toward generating a set of candidate SQL queries.
Finally, Metasql ranks the candidates to identify the best matching one for the
given NL query. Extensive experiments are performed to study Metasql on two
public NLIDB benchmarks. The results show that the performance of the
translation models can be effectively improved using Metasql.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17151" title="Abstract">arXiv:2402.17151</a> [<a href="/pdf/2402.17151" title="Download PDF">pdf</a>, <a href="/format/2402.17151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Document Parts: Detecting and Characterizing Influence  Campaigns From Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rambow%2C+O">Owen Rambow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose a novel clustering pipeline to detect and characterize influence
campaigns from documents. This approach clusters parts of document, detects
clusters that likely reflect an influence campaign, and then identifies
documents linked to an influence campaign via their association with the
high-influence clusters. Our approach outperforms both the direct
document-level classification and the direct document-level clustering approach
in predicting if a document is part of an influence campaign. We propose
various novel techniques to enhance our pipeline, including using an existing
event factuality prediction system to obtain document parts, and aggregating
multiple clustering experiments to improve the performance of both cluster and
document classification. Classifying documents on the top of clustering not
only accurately extracts the parts of the documents that are relevant to
influence campaigns, but also capture influence campaigns as a coordinated and
holistic phenomenon. Our approach makes possible more fine-grained and
interpretable characterizations of influence campaigns from documents.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17152" title="Abstract">arXiv:2402.17152</a> [<a href="/pdf/2402.17152" title="Download PDF">pdf</a>, <a href="/format/2402.17152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Actions Speak Louder than Words: Trillion-Parameter Sequential  Transducers for Generative Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+J">Jiaqi Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+L">Lucy Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yueming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xuan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Leon Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zhaojie Gong</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+F">Fangda Gu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+M">Michael He</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yinghai Lu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yu Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full technical report to follow
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Large-scale recommendation systems are characterized by their reliance on
high cardinality, heterogeneous features and the need to handle tens of
billions of user actions on a daily basis. Despite being trained on huge volume
of data with thousands of features, most Deep Learning Recommendation Models
(DLRMs) in industry fail to scale with compute.
<br />Inspired by success achieved by Transformers in language and vision domains,
we revisit fundamental design choices in recommendation systems. We reformulate
recommendation problems as sequential transduction tasks within a generative
modeling framework (``Generative Recommenders''), and propose a new
architecture, HSTU, designed for high cardinality, non-stationary streaming
recommendation data.
<br />HSTU outperforms baselines over synthetic and public datasets by up to 65.8\%
in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on
8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion
parameters, improve metrics in online A/B tests by 12.4\% and have been
deployed on multiple surfaces of a large internet platform with billions of
users. More importantly, the model quality of Generative Recommenders
empirically scales as a power-law of training compute across three orders of
magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for
future model developments, and further paves the way for the first foundational
models in recommendations.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17156" title="Abstract">arXiv:2402.17156</a> [<a href="/pdf/2402.17156" title="Download PDF">pdf</a>, <a href="/format/2402.17156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zongying%2C+L">Lin Zongying</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+L">Li Hao</a>, 
<a href="/search/cs?searchtype=author&query=Liuzhenghao%2C+L">Lv Liuzhenghao</a>, 
<a href="/search/cs?searchtype=author&query=Bin%2C+L">Lin Bin</a>, 
<a href="/search/cs?searchtype=author&query=Junwu%2C+Z">Zhang Junwu</a>, 
<a href="/search/cs?searchtype=author&query=Yu-Chian%2C+C+C">Chen Calvin Yu-Chian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yonghong%2C+T">Tian Yonghong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Designing protein sequences with specific biological functions and structural
stability is crucial in biology and chemistry. Generative models already
demonstrated their capabilities for reliable protein design. However, previous
models are limited to the unconditional generation of protein sequences and
lack the controllable generation ability that is vital to biological tasks. In
this work, we propose TaxDiff, a taxonomic-guided diffusion model for
controllable protein sequence generation that combines biological species
information with the generative capabilities of diffusion models to generate
structurally stable proteins within the sequence space. Specifically, taxonomic
control information is inserted into each layer of the transformer block to
achieve fine-grained control. The combination of global and local attention
ensures the sequence consistency and structural foldability of
taxonomic-specific proteins. Extensive experiments demonstrate that TaxDiff can
consistently achieve better performance on multiple protein sequence generation
benchmarks in both taxonomic-guided controllable generation and unconditional
generation. Remarkably, the sequences generated by TaxDiff even surpass those
produced by direct-structure-generation models in terms of confidence based on
predicted structures and require only a quarter of the time of models based on
the diffusion model. The code for generating proteins and training new versions
of TaxDiff is available at:https://github.com/Linzy19/TaxDiff.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17157" title="Abstract">arXiv:2402.17157</a> [<a href="/pdf/2402.17157" title="Download PDF">pdf</a>, <a href="/format/2402.17157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Learning for Forecasting the Dynamics of Complex Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Han Gao</a>, 
<a href="/search/cs?searchtype=author&query=Kaltenbach%2C+S">Sebastian Kaltenbach</a>, 
<a href="/search/cs?searchtype=author&query=Koumoutsakos%2C+P">Petros Koumoutsakos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce generative models for accelerating simulations of complex
systems through learning and evolving their effective dynamics. In the proposed
Generative Learning of Effective Dynamics (G-LED), instances of high
dimensional data are down sampled to a lower dimensional manifold that is
evolved through an auto-regressive attention mechanism. In turn, Bayesian
diffusion models, that map this low-dimensional manifold onto its corresponding
high-dimensional space, capture the statistics of the system dynamics. We
demonstrate the capabilities and drawbacks of G-LED in simulations of several
benchmark systems, including the Kuramoto-Sivashinsky (KS) equation,
two-dimensional high Reynolds number flow over a backward-facing step, and
simulations of three-dimensional turbulent channel flow. The results
demonstrate that generative learning offers new frontiers for the accurate
forecasting of the statistical properties of complex systems at a reduced
computational cost.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17159" title="Abstract">arXiv:2402.17159</a> [<a href="/pdf/2402.17159" title="Download PDF">pdf</a>, <a href="/format/2402.17159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NocPlace: Nocturnal Visual Place Recognition Using Generative and  Inherited Knowledge Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiqun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+H">Huaqi Tao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tingjun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+F">Fulin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yihong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jinqiang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages,9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual Place Recognition (VPR) is crucial in computer vision, aiming to
retrieve database images similar to a query image from an extensive collection
of known images. However, like many vision-related tasks, learning-based VPR
often experiences a decline in performance during nighttime due to the scarcity
of nighttime images. Specifically, VPR needs to address the cross-domain
problem of night-to-day rather than just the issue of a single nighttime
domain. In response to these issues, we present NocPlace, which leverages a
generated large-scale, multi-view, nighttime VPR dataset to embed resilience
against dazzling lights and extreme darkness in the learned global descriptor.
Firstly, we establish a day-night urban scene dataset called NightCities,
capturing diverse nighttime scenarios and lighting variations across 60 cities
globally. Following this, an unpaired image-to-image translation network is
trained on this dataset. Using this trained translation network, we process an
existing VPR dataset, thereby obtaining its nighttime version. The NocPlace is
then fine-tuned using night-style images, the original labels, and descriptors
inherited from the Daytime VPR model. Comprehensive experiments on various
nighttime VPR test sets reveal that NocPlace considerably surpasses previous
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17160" title="Abstract">arXiv:2402.17160</a> [<a href="/pdf/2402.17160" title="Download PDF">pdf</a>, <a href="/format/2402.17160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Choosing Behind the Veil: Tight Bounds for Identity-Blind Online  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ezra%2C+T">Tomer Ezra</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+M">Michal Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z+G">Zhihao Gavin Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Abstract shortened for arXiv
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In Bayesian online settings, every element has a value that is drawn from a
known underlying distribution, which we refer to as the element's identity. The
elements arrive sequentially. Upon the arrival of an element, its value is
revealed, and the decision maker needs to decide, immediately and irrevocably,
whether to accept it or not. While most previous work has assumed that the
decision maker, upon observing the element's value, also becomes aware of its
identity -- namely, its distribution -- practical scenarios frequently demand
that decisions be made based solely on the element's value, without considering
its identity. This necessity arises either from the algorithm's ignorance of
the element's identity or due to the pursuit of fairness. We call such
algorithms identity-blind algorithms, and propose the identity-blindness gap as
a metric to evaluate the performance loss caused by identity-blindness. This
gap is defined as the maximum ratio between the expected performance of an
identity-blind online algorithm and an optimal online algorithm that knows the
arrival order, thus also the identities.
<br />We study the identity-blindness gap in the paradigmatic prophet inequality
problem, under the two objectives of maximizing the expected value, and
maximizing the probability to obtain the highest value. For the max-expectation
objective, the celebrated prophet inequality establishes a single-threshold
algorithm that gives at least 1/2 of the offline optimum, thus also an
identity-blindness gap of at least 1/2. We show that this bound is tight. For
the max-probability objective, while the competitive ratio is tightly 1/e, we
provide a deterministic single-threshold algorithm that gives an
identity-blindness gap of $\sim 0.562$ under the assumption that there are no
large point masses. Moreover, we show that this bound is tight with respect to
deterministic algorithms.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17161" title="Abstract">arXiv:2402.17161</a> [<a href="/pdf/2402.17161" title="Download PDF">pdf</a>, <a href="/format/2402.17161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model for Participatory Urban Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhilun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuming Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Depeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2402.01698">arXiv:2402.01698</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Participatory urban planning is the mainstream of modern urban planning that
involves the active engagement of residents. However, the traditional
participatory paradigm requires experienced planning experts and is often
time-consuming and costly. Fortunately, the emerging Large Language Models
(LLMs) have shown considerable ability to simulate human-like agents, which can
be used to emulate the participatory process easily. In this work, we introduce
an LLM-based multi-agent collaboration framework for participatory urban
planning, which can generate land-use plans for urban regions considering the
diverse needs of residents. Specifically, we construct LLM agents to simulate a
planner and thousands of residents with diverse profiles and backgrounds. We
first ask the planner to carry out an initial land-use plan. To deal with the
different facilities needs of residents, we initiate a discussion among the
residents in each community about the plan, where residents provide feedback
based on their profiles. Furthermore, to improve the efficiency of discussion,
we adopt a fishbowl discussion mechanism, where part of the residents discuss
and the rest of them act as listeners in each round. Finally, we let the
planner modify the plan based on residents' feedback. We deploy our method on
two real-world regions in Beijing. Experiments show that our method achieves
state-of-the-art performance in residents satisfaction and inclusion metrics,
and also outperforms human experts in terms of service accessibility and
ecology metrics.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17165" title="Abstract">arXiv:2402.17165</a> [<a href="/pdf/2402.17165" title="Download PDF">pdf</a>, <a href="/format/2402.17165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-shot adaptation for morphology-independent cell instance  segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaveri%2C+R+J">Ram J. Zaveri</a>, 
<a href="/search/cs?searchtype=author&query=Brume%2C+V">Voke Brume</a>, 
<a href="/search/cs?searchtype=author&query=Doretto%2C+G">Gianfranco Doretto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Microscopy data collections are becoming larger and more frequent. Accurate
and precise quantitative analysis tools like cell instance segmentation are
necessary to benefit from them. This is challenging due to the variability in
the data, which requires retraining the segmentation model to maintain high
accuracy on new collections. This is needed especially for segmenting cells
with elongated and non-convex morphology like bacteria. We propose to reduce
the amount of annotation and computing power needed for retraining the model by
introducing a few-shot domain adaptation approach that requires annotating only
one to five cells of the new data to process and that quickly adapts the model
to maintain high accuracy. Our results show a significant boost in accuracy
after adaptation to very challenging bacteria datasets.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17167" title="Abstract">arXiv:2402.17167</a> [<a href="/pdf/2402.17167" title="Download PDF">pdf</a>, <a href="/ps/2402.17167" title="Download PostScript">ps</a>, <a href="/format/2402.17167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Converse Barrier Certificates for Finite-time Safety Verification of  Continuous-time Perturbed Deterministic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yonghan Li</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+C">Chenyu Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+T">Taoran Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shijie Wang</a>, 
<a href="/search/eess?searchtype=author&query=Xue%2C+B">Bai Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we investigate the problem of verifying the finite-time safety
of continuous-time perturbed deterministic systems represented by ordinary
differential equations in the presence of measurable disturbances. Given a
finite time horizon, if the system is safe, it, starting from a compact initial
set, will remain within an open and bounded safe region throughout the
specified time horizon, regardless of the disturbances. The main contribution
of this work is to uncover that there exists a time-dependent barrier
certificate if and only if the system is safe. This barrier certificate
satisfies the following conditions: negativity over the initial set at the
initial time instant, non-negativity over the boundary of the safe set, and
non-increasing behavior along the system dynamics over the specified finite
time horizon. The existence problem is explored using a Hamilton-Jacobi
differential equation, which has a unique Lipschitz viscosity solution.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17168" title="Abstract">arXiv:2402.17168</a> [<a href="/pdf/2402.17168" title="Download PDF">pdf</a>, <a href="/format/2402.17168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Data Science Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Q">Qiyang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xingyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuqing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">Kan Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source code and data are available at <a href="https://github.com/MetaCopilot/dseval">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In the era of data-driven decision-making, the complexity of data analysis
necessitates advanced expertise and tools of data science, presenting
significant challenges even for specialists. Large Language Models (LLMs) have
emerged as promising aids as data science agents, assisting humans in data
analysis and processing. Yet their practical efficacy remains constrained by
the varied demands of real-world applications and complicated analytical
process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as
well as a series of innovative benchmarks tailored for assessing the
performance of these agents throughout the entire data science lifecycle.
Incorporating a novel bootstrapped annotation method, we streamline dataset
preparation, improve the evaluation coverage, and expand benchmarking
comprehensiveness. Our findings uncover prevalent obstacles and provide
critical insights to inform future advancements in the field.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17169" title="Abstract">arXiv:2402.17169</a> [<a href="/pdf/2402.17169" title="Download PDF">pdf</a>, <a href="/format/2402.17169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Umbra: A Generative Approach for Sunlight Access Computation in  Urban Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Omar%2C+K+S">Kazi Shahrukh Omar</a>, 
<a href="/search/cs?searchtype=author&query=Moreira%2C+G">Gustavo Moreira</a>, 
<a href="/search/cs?searchtype=author&query=Hodczak%2C+D">Daniel Hodczak</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+M">Maryam Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Colaninno%2C+N">Nicola Colaninno</a>, 
<a href="/search/cs?searchtype=author&query=Lage%2C+M">Marcos Lage</a>, 
<a href="/search/cs?searchtype=author&query=Miranda%2C+F">Fabio Miranda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE Transactions on Big Data. Deep Umbra is available at <a href="https://urbantk.org/shadows">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Sunlight and shadow play critical roles in how urban spaces are utilized,
thrive, and grow. While access to sunlight is essential to the success of urban
environments, shadows can provide shaded places to stay during the hot seasons,
mitigate heat island effect, and increase pedestrian comfort levels. Properly
quantifying sunlight access and shadows in large urban environments is key in
tackling some of the important challenges facing cities today. In this paper,
we propose Deep Umbra, a novel computational framework that enables the
quantification of sunlight access and shadows at a global scale. Our framework
is based on a conditional generative adversarial network that considers the
physical form of cities to compute high-resolution spatial information of
accumulated sunlight access for the different seasons of the year. We use data
from seven different cities to train our model, and show, through an extensive
set of experiments, its low overall RMSE (below 0.1) as well as its
extensibility to cities that were not part of the training set. Additionally,
we contribute a set of case studies and a comprehensive dataset with sunlight
access information for more than 100 cities across six continents of the world.
Deep Umbra is available at https://urbantk.org/shadows.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17171" title="Abstract">arXiv:2402.17171</a> [<a href="/pdf/2402.17171" title="Download PDF">pdf</a>, <a href="/format/2402.17171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free  Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yiming Ren</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chengfeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuexin Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">For human-centric large-scale scenes, fine-grained modeling for 3D human
global pose and shape is significant for scene understanding and can benefit
many real-world applications. In this paper, we present LiveHPS, a novel
single-LiDAR-based approach for scene-level human pose and shape estimation
without any limitation of light conditions and wearable devices. In particular,
we design a distillation mechanism to mitigate the distribution-varying effect
of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic
information existing in consecutive frames to solve the occlusion and noise
disturbance. LiveHPS, with its efficient configuration and high-quality output,
is well-suited for real-world applications. Moreover, we propose a huge human
motion dataset, named FreeMotion, which is collected in various scenarios with
diverse human poses, shapes and translations. It consists of multi-modal and
multi-view acquisition data from calibrated and synchronized LiDARs, cameras,
and IMUs. Extensive experiments on our new dataset and other public datasets
demonstrate the SOTA performance and robustness of our approach. We will
release our code and dataset soon.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17172" title="Abstract">arXiv:2402.17172</a> [<a href="/pdf/2402.17172" title="Download PDF">pdf</a>, <a href="/format/2402.17172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lane2Seq: Towards Unified Lane Detection via Sequence Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kunyang Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR2024 acceptance
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we present a novel sequence generation-based framework for
lane detection, called Lane2Seq. It unifies various lane detection formats by
casting lane detection as a sequence generation task. This is different from
previous lane detection methods, which depend on well-designed task-specific
head networks and corresponding loss functions. Lane2Seq only adopts a plain
transformer-based encoder-decoder architecture with a simple cross-entropy
loss. Additionally, we propose a new multi-format model tuning based on
reinforcement learning to incorporate the task-specific knowledge into
Lane2Seq. Experimental results demonstrate that such a simple sequence
generation paradigm not only unifies lane detection but also achieves
competitive performance on benchmarks. For example, Lane2Seq gets 97.95\% and
97.42\% F1 score on Tusimple and LLAMAS datasets, establishing a new
state-of-the-art result for two benchmarks.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17173" title="Abstract">arXiv:2402.17173</a> [<a href="/pdf/2402.17173" title="Download PDF">pdf</a>, <a href="/format/2402.17173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted EF1 and PO Allocations with Few Types of Agents or Chores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+J">Jugal Garg</a>, 
<a href="/search/cs?searchtype=author&query=Murhekar%2C+A">Aniket Murhekar</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">John Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We investigate the existence of fair and efficient allocations of indivisible
chores to asymmetric agents who have unequal entitlements or weights. We
consider the fairness notion of weighted envy-freeness up to one chore (wEF1)
and the efficiency notion of Pareto-optimality (PO). The existence of EF1 and
PO allocations of chores to symmetric agents is a major open problem in
discrete fair division, and positive results are known only for certain
structured instances. In this paper, we study this problem for a more general
setting of asymmetric agents and show that an allocation that is wEF1 and PO
exists and can be computed in polynomial time for instances with:
<br />- Three types of agents, where agents with the same type have identical
preferences but can have different weights.
<br />- Two types of chores, where the chores can be partitioned into two sets,
each containing copies of the same chore. For symmetric agents, our results
establish that EF1 and PO allocations exist for three types of agents and also
generalize known results for three agents, two types of agents, and two types
of chores.
<br />Our algorithms use a weighted picking sequence algorithm as a subroutine; we
expect this idea and our analysis to be of independent interest.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17176" title="Abstract">arXiv:2402.17176</a> [<a href="/pdf/2402.17176" title="Download PDF">pdf</a>, <a href="/format/2402.17176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hongyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yici Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhizhen Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 14 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Model-X knockoff, among various feature selection methods, received much
attention recently due to its guarantee on false discovery rate (FDR) control.
Subsequent to its introduction in parametric design, knockoff is advanced to
handle arbitrary data distributions using deep learning-based generative
modeling. However, we observed that current implementations of the deep Model-X
knockoff framework exhibit limitations. Notably, the "swap property" that
knockoffs necessitate frequently encounter challenges on sample level, leading
to a diminished selection power. To overcome, we develop "Deep Dependency
Regularized Knockoff (DeepDRK)", a distribution-free deep learning method that
strikes a balance between FDR and power. In DeepDRK, a generative model
grounded in a transformer architecture is introduced to better achieve the
"swap property". Novel efficient regularization techniques are also proposed to
reach higher power. Our model outperforms other benchmarks in synthetic,
semi-synthetic, and real-world data, especially when sample size is small and
data distribution is complex.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17177" title="Abstract">arXiv:2402.17177</a> [<a href="/pdf/2402.17177" title="Download PDF">pdf</a>, <a href="/format/2402.17177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sora: A Review on Background, Technology, Limitations, and Opportunities  of Large Vision Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiling Yan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chujie Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruoxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhengqing Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hanchi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lifang He</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 18 figures; Our GitHub Homepage: <a href="https://github.com/lichao-sun/SoraReview">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Sora is a text-to-video generative AI model, released by OpenAI in February
2024. The model is trained to generate videos of realistic or imaginative
scenes from text instructions and show potential in simulating the physical
world. Based on public technical reports and reverse engineering, this paper
presents a comprehensive review of the model's background, related
technologies, applications, remaining challenges, and future directions of
text-to-video AI models. We first trace Sora's development and investigate the
underlying technologies used to build this "world simulator". Then, we describe
in detail the applications and potential impact of Sora in multiple industries
ranging from film-making and education to marketing. We discuss the main
challenges and limitations that need to be addressed to widely deploy Sora,
such as ensuring safe and unbiased video generation. Lastly, we discuss the
future development of Sora and video generation models in general, and how
advancements in the field could enable new ways of human-AI interaction,
boosting productivity and creativity of video generation.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17178" title="Abstract">arXiv:2402.17178</a> [<a href="/pdf/2402.17178" title="Download PDF">pdf</a>, <a href="/format/2402.17178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuralSI: Neural Design of Semantic Interaction for Interactive Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yali Bian</a>, 
<a href="/search/cs?searchtype=author&query=Faust%2C+R">Rebecca Faust</a>, 
<a href="/search/cs?searchtype=author&query=North%2C+C">Chris North</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">An increasing number of studies have utilized interactive deep learning as
the analytic model of visual analytics systems for complex sensemaking tasks.
In these systems, traditional interactive dimensionality reduction (DR) models
are commonly utilized to build a bi-directional bridge between high-dimensional
deep learning representations and low-dimensional visualizations. While these
systems better capture analysts' intents in the context of human-in-the-loop
interactive deep learning, traditional DR cannot support several desired
properties for visual analytics, including out-of-sample extensions, stability,
and real-time inference. To avoid this issue, we propose the neural design
framework of semantic interaction for interactive deep learning. In our
framework, we replace the traditional DR with a neural projection network and
append it to the deep learning model as the task-specific output layer.
Therefore, the analytic model (deep learning) and visualization method
(interactive DR) form one integrated end-to-end trainable deep neural network.
In order to understand the performance of the neural design in comparison to
the state-of-the-art, we systematically performed two complementary studies, a
human-centered qualitative case study and an algorithm-centered
simulation-based quantitative experiment. The results of these studies indicate
that the neural design can give semantic interaction systems substantial
advantages while still keeping comparable inference ability compared to the
state-of-the-art model.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17179" title="Abstract">arXiv:2402.17179</a> [<a href="/pdf/2402.17179" title="Download PDF">pdf</a>, <a href="/format/2402.17179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-Space Optimization: Improved Molecule Sequence Design by Latent  Prompt Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Deqian Kong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jianwen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Honig%2C+E">Edouardo Honig</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Ming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Shuanghong Xue</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+P">Pei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sanping Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Sheng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y+N">Ying Nian Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Designing molecules with desirable properties, such as drug-likeliness and
high binding affinities towards protein targets, is a challenging problem. In
this paper, we propose the Dual-Space Optimization (DSO) method that integrates
latent space sampling and data space selection to solve this problem. DSO
iteratively updates a latent space generative model and a synthetic dataset in
an optimization process that gradually shifts the generative model and the
synthetic data towards regions of desired property values. Our generative model
takes the form of a Latent Prompt Transformer (LPT) where the latent vector
serves as the prompt of a causal transformer. Our extensive experiments
demonstrate effectiveness of the proposed method, which sets new performance
benchmarks across single-objective, multi-objective and constrained molecule
design tasks.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17180" title="Abstract">arXiv:2402.17180</a> [<a href="/pdf/2402.17180" title="Download PDF">pdf</a>, <a href="/ps/2402.17180" title="Download PostScript">ps</a>, <a href="/format/2402.17180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance analysis of MUSIC-type imaging without diagonal elements of  multi-static response matrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Park%2C+W">Won-Kwang Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Generally, to apply the MUltiple SIgnal Classification (MUSIC) algorithm for
the rapid imaging of small inhomogeneities, the complete elements of the
multi-static response (MSR) matrix must be collected. However, in real-world
applications such as microwave imaging or bistatic measurement configuration,
diagonal elements of the MSR matrix are unknown. Nevertheless, it is possible
to obtain imaging results using a traditional approach but theoretical reason
of the applicability has not been investigated yet. In this paper, we establish
mathematical structures of the imaging function of MUSIC from an MSR matrix
without diagonal elements in both transverse magnetic (TM) and transverse
electric (TE) polarizations. The established structures demonstrate why the
shape of the location of small inhomogeneities can be retrieved via MUSIC
without the diagonal elements of the MSR matrix. In addition, they reveal the
intrinsic properties of imaging and the fundamental limitations. Results of
numerical simulations are also provided to support the identified structures.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17184" title="Abstract">arXiv:2402.17184</a> [<a href="/pdf/2402.17184" title="Download PDF">pdf</a>, <a href="/format/2402.17184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme Encoder Output Frame Rate Reduction: Improving Computational  Latencies of Large End-to-End Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhavalkar%2C+R">Rohit Prabhavalkar</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zhong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Stooke%2C+A">Adam Stooke</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xingyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yanzhang He</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+A">Arun Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+D">Dongseong Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Sainath%2C+T+N">Tara N. Sainath</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+P+J">Pedro J. Moreno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The accuracy of end-to-end (E2E) automatic speech recognition (ASR) models
continues to improve as they are scaled to larger sizes, with some now reaching
billions of parameters. Widespread deployment and adoption of these models,
however, requires computationally efficient strategies for decoding. In the
present work, we study one such strategy: applying multiple frame reduction
layers in the encoder to compress encoder outputs into a small number of output
frames. While similar techniques have been investigated in previous work, we
achieve dramatically more reduction than has previously been demonstrated
through the use of multiple funnel reduction layers. Through ablations, we
study the impact of various architectural choices in the encoder to identify
the most effective strategies. We demonstrate that we can generate one encoder
output frame for every 2.56 sec of input speech, without significantly
affecting word error rate on a large-scale voice search task, while improving
encoder and decoder latencies by 48% and 92% respectively, relative to a strong
but computationally expensive baseline.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17185" title="Abstract">arXiv:2402.17185</a> [<a href="/pdf/2402.17185" title="Download PDF">pdf</a>, <a href="/format/2402.17185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inpainting Computational Fluid Dynamics with Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+D">Dule Shu</a>, 
<a href="/search/cs?searchtype=author&query=Zhen%2C+W">Wilson Zhen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijie Li</a>, 
<a href="/search/cs?searchtype=author&query=Farimani%2C+A+B">Amir Barati Farimani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Fluid data completion is a research problem with high potential benefit for
both experimental and computational fluid dynamics. An effective fluid data
completion method reduces the required number of sensors in a fluid dynamics
experiment, and allows a coarser and more adaptive mesh for a Computational
Fluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid
data completion problem makes it prohibitively difficult to obtain a
theoretical solution and presents high numerical uncertainty and instability
for a data-driven approach (e.g., a neural network model). To address these
challenges, we leverage recent advancements in computer vision, employing the
vector quantization technique to map both complete and incomplete fluid data
spaces onto discrete-valued lower-dimensional representations via a two-stage
learning procedure. We demonstrated the effectiveness of our approach on
Kolmogorov flow data (Reynolds number: 1000) occluded by masks of different
size and arrangement. Experimental results show that our proposed model
consistently outperforms benchmark models under different occlusion settings in
terms of point-wise reconstruction accuracy as well as turbulent energy
spectrum and vorticity distribution.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17188" title="Abstract">arXiv:2402.17188</a> [<a href="/pdf/2402.17188" title="Download PDF">pdf</a>, <a href="/format/2402.17188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptMM: Multi-Modal Knowledge Distillation for Recommendation with  Prompt-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiabin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yangqin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited
from the incorporation of multimedia (e.g., visual, textual, and acoustic)
content into their personal recommender systems. These modalities provide
intuitive semantics that facilitate modality-aware user preference modeling.
However, two key challenges in multi-modal recommenders remain unresolved: i)
The introduction of multi-modal encoders with a large number of additional
parameters causes overfitting, given high-dimensional multi-modal features
provided by extractors (e.g., ViT, BERT). ii) Side information inevitably
introduces inaccuracies and redundancies, which skew the modality-interaction
dependency from reflecting true user preference. To tackle these problems, we
propose to simplify and empower recommenders through Multi-modal Knowledge
Distillation (PromptMM) with the prompt-tuning that enables adaptive quality
distillation. Specifically, PromptMM conducts model compression through
distilling u-i edge relationship and multi-modal node content from cumbersome
teachers to relieve students from the additional feature reduction parameters.
To bridge the semantic gap between multi-modal context and collaborative
signals for empowering the overfitting teacher, soft prompt-tuning is
introduced to perform student task-adaptive. Additionally, to adjust the impact
of inaccuracies in multimedia data, a disentangled multi-modal list-wise
distillation is developed with modality-aware re-weighting mechanism.
Experiments on real-world data demonstrate PromptMM's superiority over existing
techniques. Ablation tests confirm the effectiveness of key components.
Additional tests show the efficiency and effectiveness.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17189" title="Abstract">arXiv:2402.17189</a> [<a href="/pdf/2402.17189" title="Download PDF">pdf</a>, <a href="/ps/2402.17189" title="Download PostScript">ps</a>, <a href="/format/2402.17189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Effective Mixture-Of-Experts Approach For Code-Switching Speech  Recognition Leveraging Encoder Disentanglement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tzu-Ting Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hsin-Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi-Cheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chi-Han Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Berlin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">With the massive developments of end-to-end (E2E) neural networks, recent
years have witnessed unprecedented breakthroughs in automatic speech
recognition (ASR). However, the codeswitching phenomenon remains a major
obstacle that hinders ASR from perfection, as the lack of labeled data and the
variations between languages often lead to degradation of ASR performance. In
this paper, we focus exclusively on improving the acoustic encoder of E2E ASR
to tackle the challenge caused by the codeswitching phenomenon. Our main
contributions are threefold: First, we introduce a novel disentanglement loss
to enable the lower-layer of the encoder to capture inter-lingual acoustic
information while mitigating linguistic confusion at the higher-layer of the
encoder. Second, through comprehensive experiments, we verify that our proposed
method outperforms the prior-art methods using pretrained dual-encoders,
meanwhile having access only to the codeswitching corpus and consuming half of
the parameterization. Third, the apparent differentiation of the encoders'
output features also corroborates the complementarity between the
disentanglement loss and the mixture-of-experts (MoE) architecture.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17191" title="Abstract">arXiv:2402.17191</a> [<a href="/pdf/2402.17191" title="Download PDF">pdf</a>, <a href="/ps/2402.17191" title="Download PostScript">ps</a>, <a href="/format/2402.17191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Driven Anonymization: Protecting Personal Data Privacy While  Leveraging Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Le Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+M">Miao Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+D">Duan Xin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qishuo Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiajian Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The development of artificial intelligence has significantly transformed
people's lives. However, it has also posed a significant threat to privacy and
security, with numerous instances of personal information being exposed online
and reports of criminal attacks and theft. Consequently, the need to achieve
intelligent protection of personal information through machine learning
algorithms has become a paramount concern. Artificial intelligence leverages
advanced algorithms and technologies to effectively encrypt and anonymize
personal data, enabling valuable data analysis and utilization while
safeguarding privacy. This paper focuses on personal data privacy protection
and the promotion of anonymity as its core research objectives. It achieves
personal data privacy protection and detection through the use of machine
learning's differential privacy protection algorithm. The paper also addresses
existing challenges in machine learning related to privacy and personal data
protection, offers improvement suggestions, and analyzes factors impacting
datasets to enable timely personal data privacy detection and protection.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17192" title="Abstract">arXiv:2402.17192</a> [<a href="/pdf/2402.17192" title="Download PDF">pdf</a>, <a href="/format/2402.17192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Biomechanics Unlocks Opportunities for Markerless Motion  Capture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cotton%2C+R+J">R. James Cotton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent developments have created differentiable physics simulators designed
for machine learning pipelines that can be accelerated on a GPU. While these
can simulate biomechanical models, these opportunities have not been exploited
for biomechanics research or markerless motion capture. We show that these
simulators can be used to fit inverse kinematics to markerless motion capture
data, including scaling the model to fit the anthropomorphic measurements of an
individual. This is performed end-to-end with an implicit representation of the
movement trajectory, which is propagated through the forward kinematic model to
minimize the error from the 3D markers reprojected into the images. The
differential optimizer yields other opportunities, such as adding bundle
adjustment during trajectory optimization to refine the extrinsic camera
parameters or meta-optimization to improve the base model jointly over
trajectories from multiple participants. This approach improves the
reprojection error from markerless motion capture over prior methods and
produces accurate spatial step parameters compared to an instrumented walkway
for control and clinical populations.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17193" title="Abstract">arXiv:2402.17193</a> [<a href="/pdf/2402.17193" title="Download PDF">pdf</a>, <a href="/format/2402.17193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Scaling Meets LLM Finetuning: The Effect of Data, Model and  Finetuning Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Biao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhongtao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cherry%2C+C">Colin Cherry</a>, 
<a href="/search/cs?searchtype=author&query=Firat%2C+O">Orhan Firat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While large language models (LLMs) often adopt finetuning to unlock their
capabilities for downstream applications, our understanding on the inductive
biases (especially the scaling properties) of different finetuning methods is
still limited. To fill this gap, we conduct systematic experiments studying
whether and how different scaling factors, including LLM model size,
pretraining data size, new finetuning parameter size and finetuning data size,
affect the finetuning performance. We consider two types of finetuning --
full-model tuning (FMT) and parameter efficient tuning (PET, including prompt
tuning and LoRA), and explore their scaling behaviors in the data-limited
regime where the LLM model size substantially outweighs the finetuning data
size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and
experiments on bilingual machine translation and multilingual summarization
benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative
joint scaling law between finetuning data size and each other scaling factor;
2) LLM finetuning benefits more from LLM model scaling than pretraining data
scaling, and PET parameter scaling is generally ineffective; and 3) the optimal
finetuning method is highly task- and finetuning data-dependent. We hope our
findings could shed light on understanding, selecting and developing LLM
finetuning methods.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17200" title="Abstract">arXiv:2402.17200</a> [<a href="/pdf/2402.17200" title="Download PDF">pdf</a>, <a href="/format/2402.17200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Quality of Compressed Images by Mitigating Enhancement Bias  Towards Compression Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+Q">Qunliang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shengxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Meisong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huaida Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Ying Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Existing quality enhancement methods for compressed images focus on aligning
the enhancement domain with the raw domain to yield realistic images. However,
these methods exhibit a pervasive enhancement bias towards the compression
domain, inadvertently regarding it as more realistic than the raw domain. This
bias makes enhanced images closely resemble their compressed counterparts, thus
degrading their perceptual quality. In this paper, we propose a simple yet
effective method to mitigate this bias and enhance the quality of compressed
images. Our method employs a conditional discriminator with the compressed
image as a key condition, and then incorporates a domain-divergence
regularization to actively distance the enhancement domain from the compression
domain. Through this dual strategy, our method enables the discrimination
against the compression domain, and brings the enhancement domain closer to the
raw domain. Comprehensive quality evaluations confirm the superiority of our
method over other state-of-the-art methods without incurring inference
overheads.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17201" title="Abstract">arXiv:2402.17201</a> [<a href="/pdf/2402.17201" title="Download PDF">pdf</a>, <a href="/ps/2402.17201" title="Download PostScript">ps</a>, <a href="/format/2402.17201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Decentralized Market Mechanism for Energy Communities under Operating  Envelopes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alahmed%2C+A+S">Ahmed S. Alahmed</a>, 
<a href="/search/eess?searchtype=author&query=Cavraro%2C+G">Guido Cavraro</a>, 
<a href="/search/eess?searchtype=author&query=Bernstein%2C+A">Andrey Bernstein</a>, 
<a href="/search/eess?searchtype=author&query=Tong%2C+L">Lang Tong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Theoretical Economics (econ.TH); Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose an operating envelopes (OEs) aware energy community market
mechanism that dynamically charges/rewards its members based on two-part
pricing. The OEs are imposed exogenously by a regulated distribution system
operator (DSO) on the energy community's revenue meter that is subject to a
generalized net energy metering (NEM) tariff design. By formulating the
interaction of the community manager and its members as a Stackelberg game, we
show that the proposed two-part pricing achieves a Nash equilibrium and
maximizes the community's social welfare in a decentralized fashion while
ensuring that the community's operation abides by the OEs. The market mechanism
conforms with the cost-causation principle and guarantees community members a
surplus level no less than their maximum surplus when they autonomously face
the DSO. The dynamic and uniform community price is a monotonically decreasing
function of the community's aggregate renewable generation. We also analyze the
impact of exogenous parameters such as NEM rates and OEs on the value of
joining the community. Lastly, through numerical studies, we showcase the
community's welfare, pricing, and compare its members' surplus to customers
under the DSO and other OEs arrangements.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17202" title="Abstract">arXiv:2402.17202</a> [<a href="/pdf/2402.17202" title="Download PDF">pdf</a>, <a href="/format/2402.17202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedBRB: An Effective Solution to the Small-to-Large Scenario in  Device-Heterogeneity Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Ziyue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingfeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+T">Tianchi Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chuan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, the success of large models has demonstrated the importance of
scaling up model size. This has spurred interest in exploring collaborative
training of large-scale models from federated learning perspective. Due to
computational constraints, many institutions struggle to train a large-scale
model locally. Thus, training a larger global model using only smaller local
models has become an important scenario (i.e., the \textbf{small-to-large
scenario}). Although recent device-heterogeneity federated learning approaches
have started to explore this area, they face limitations in fully covering the
parameter space of the global model. In this paper, we propose a method called
\textbf{FedBRB} (\underline{B}lock-wise \underline{R}olling and weighted
\underline{B}roadcast) based on the block concept. FedBRB can uses small local
models to train all blocks of the large global model, and broadcasts the
trained parameters to the entire space for faster information interaction.
Experiments demonstrate FedBRB yields substantial performance gains, achieving
state-of-the-art results in this scenario. Moreover, FedBRB using only minimal
local models can even surpass baselines using larger local models.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17204" title="Abstract">arXiv:2402.17204</a> [<a href="/pdf/2402.17204" title="Download PDF">pdf</a>, <a href="/format/2402.17204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Generative Model Evaluation: A Novel Algorithm for Realistic  Image Synthesis and Comparison in OCR System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Memari%2C+M">Majid Memari</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+K+R">Khaled R. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Rahimi%2C+S">Shahram Rahimi</a>, 
<a href="/search/cs?searchtype=author&query=Golilarz%2C+N+A">Noorbakhsh Amiri Golilarz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> My manuscript entitled "Advancing Generative Model Evaluation: A Novel Algorithm for Realistic Image Synthesis and Comparison in OCR Systems" has been submitted on 29-Jan-2024 to IEEE Access and is presently being given full consideration for publication in IEEE Access
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This research addresses a critical challenge in the field of generative
models, particularly in the generation and evaluation of synthetic images.
Given the inherent complexity of generative models and the absence of a
standardized procedure for their comparison, our study introduces a pioneering
algorithm to objectively assess the realism of synthetic images. This approach
significantly enhances the evaluation methodology by refining the Fr\'echet
Inception Distance (FID) score, allowing for a more precise and subjective
assessment of image quality. Our algorithm is particularly tailored to address
the challenges in generating and evaluating realistic images of Arabic
handwritten digits, a task that has traditionally been near-impossible due to
the subjective nature of realism in image generation. By providing a systematic
and objective framework, our method not only enables the comparison of
different generative models but also paves the way for improvements in their
design and output. This breakthrough in evaluation and comparison is crucial
for advancing the field of OCR, especially for scripts that present unique
complexities, and sets a new standard in the generation and assessment of
high-quality synthetic images.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17205" title="Abstract">arXiv:2402.17205</a> [<a href="/pdf/2402.17205" title="Download PDF">pdf</a>, <a href="/format/2402.17205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Vision-Language STEM Skills of Neural Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianhao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Mirzoyan%2C+S">Srbuhi Mirzoyan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Ming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenguang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a new challenge to test the STEM skills of neural models. The
problems in the real world often require solutions, combining knowledge from
STEM (science, technology, engineering, and math). Unlike existing datasets,
our dataset requires the understanding of multimodal vision-language
information of STEM. Our dataset features one of the largest and most
comprehensive datasets for the challenge. It includes 448 skills and 1,073,146
questions spanning all STEM subjects. Compared to existing datasets that often
focus on examining expert-level ability, our dataset includes fundamental
skills and questions designed based on the K-12 curriculum. We also add
state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our
benchmark. Results show that the recent model advances only help master a very
limited number of lower grade-level skills (2.5% in the third grade) in our
dataset. In fact, these models are still well below (averaging 54.7%) the
performance of elementary students, not to mention near expert-level
performance. To understand and increase the performance on our dataset, we
teach the models on a training split of our dataset. Even though we observe
improved performance, the model performance remains relatively low compared to
average elementary students. To solve STEM problems, we will need novel
algorithmic innovations from the community.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17206" title="Abstract">arXiv:2402.17206</a> [<a href="/pdf/2402.17206" title="Download PDF">pdf</a>, <a href="/format/2402.17206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Identification of Minimum Undesignable RNA Motifs on Loop-Pair  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianshuo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W+Y">Wei Yu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Mathews%2C+D+H">David H. Mathews</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liang Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Motivation: RNA design aims to find at least one sequence that folds with the
highest probability into a designated target structure, but some structures are
undesignable in the sense that no sequence folds into them. Identifying
undesignable structures is useful in delineating and understanding the limit of
RNA designability, but has received little attention until recently. In
addition, existing methods on undesignability are not scalable and not
interpretable.
<br />Results: We introduce a novel graph representation and a new general
algorithmic framework to efficiently identify undesignable motifs in a
secondary structure. The proposed algorithm enumerates minimal motifs based on
the loop-pair graph representation of a structure and establishes the
undesignability of a motif by proposing rival substructure(s). Our work can
also identify unique minimum undesignable motifs across different structures.
Our implemented algorithms successfully identify 26 unique minimum undesignable
motifs among 18 undesignable puzzles from the benchmark Eterna100.
Additionally, our algorithm is so efficient that it scales to natural
structures of 16S and 23S Ribosomal RNAs (about 1,500 and 3,000 nucleotides,
resp.), and finds all of those structures in the widely used ArchiveII database
to be undesignable, with 73 unique minimum undesignable motifs, under the
standard Turner energy model in ViennaRNA.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17207" title="Abstract">arXiv:2402.17207</a> [<a href="/pdf/2402.17207" title="Download PDF">pdf</a>, <a href="/format/2402.17207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deployment Prior Injection for Run-time Calibratable Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiding Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+V+M">Vishal M. Patel</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+G">Gang Hua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With a strong alignment between the training and test distributions, object
relation as a context prior facilitates object detection. Yet, it turns into a
harmful but inevitable training set bias upon test distributions that shift
differently across space and time. Nevertheless, the existing detectors cannot
incorporate deployment context prior during the test phase without parameter
update. Such kind of capability requires the model to explicitly learn
disentangled representations with respect to context prior. To achieve this, we
introduce an additional graph input to the detector, where the graph represents
the deployment context prior, and its edge values represent object relations.
Then, the detector behavior is trained to bound to the graph with a modified
training objective. As a result, during the test phase, any suitable deployment
context prior can be injected into the detector via graph edits, hence
calibrating, or "re-biasing" the detector towards the given prior at run-time
without parameter update. Even if the deployment prior is unknown, the detector
can self-calibrate using deployment prior approximated using its own
predictions. Comprehensive experimental results on the COCO dataset, as well as
cross-dataset testing on the Objects365 dataset, demonstrate the effectiveness
of the run-time calibratable detector.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17210" title="Abstract">arXiv:2402.17210</a> [<a href="/pdf/2402.17210" title="Download PDF">pdf</a>, <a href="/format/2402.17210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Purified and Unified Steganographic Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guobiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zicong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhenxing Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinpeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures, Accepted at CVPR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Steganography is the art of hiding secret data into the cover media for
covert communication. In recent years, more and more deep neural network
(DNN)-based steganographic schemes are proposed to train steganographic
networks for secret embedding and recovery, which are shown to be promising.
Compared with the handcrafted steganographic tools, steganographic networks
tend to be large in size. It raises concerns on how to imperceptibly and
effectively transmit these networks to the sender and receiver to facilitate
the covert communication. To address this issue, we propose in this paper a
Purified and Unified Steganographic Network (PUSNet). It performs an ordinary
machine learning task in a purified network, which could be triggered into
steganographic networks for secret embedding or recovery using different keys.
We formulate the construction of the PUSNet into a sparse weight filling
problem to flexibly switch between the purified and steganographic networks. We
further instantiate our PUSNet as an image denoising network with two
steganographic networks concealed for secret image embedding and recovery.
Comprehensive experiments demonstrate that our PUSNet achieves good performance
on secret image embedding, secret image recovery, and image denoising in a
single architecture. It is also shown to be capable of imperceptibly carrying
the steganographic networks in a purified network. Code is available at
\url{https://github.com/albblgb/PUSNet}
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17213" title="Abstract">arXiv:2402.17213</a> [<a href="/pdf/2402.17213" title="Download PDF">pdf</a>, <a href="/format/2402.17213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VCD: Knowledge Base Guided Visual Commonsense Discovery in Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiangqing Shen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yurun Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Rui Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Visual commonsense contains knowledge about object properties, relationships,
and behaviors in visual data. Discovering visual commonsense can provide a more
comprehensive and richer understanding of images, and enhance the reasoning and
decision-making capabilities of computer vision systems. However, the visual
commonsense defined in existing visual commonsense discovery studies is
coarse-grained and incomplete. In this work, we draw inspiration from a
commonsense knowledge base ConceptNet in natural language processing, and
systematically define the types of visual commonsense. Based on this, we
introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract
fine-grained commonsense of different types contained within different objects
in the image. We accordingly construct a dataset (VCDD) from Visual Genome and
ConceptNet for VCD, featuring over 100,000 images and 14 million
object-commonsense pairs. We furthermore propose a generative model (VCDM) that
integrates a vision-language model with instruction tuning to tackle VCD.
Automatic and human evaluations demonstrate VCDM's proficiency in VCD,
particularly outperforming GPT-4V in implicit commonsense discovery. The value
of VCD is further demonstrated by its application to two downstream tasks,
including visual commonsense evaluation and visual question answering. The data
and code will be made available on GitHub.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17214" title="Abstract">arXiv:2402.17214</a> [<a href="/pdf/2402.17214" title="Download PDF">pdf</a>, <a href="/format/2402.17214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CharacterGen: Efficient 3D Character Generation from Single Images with  Multi-View Pose Canonicalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao-Yang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jia-Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Meng-Hao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan-Pei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shi-Min Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the field of digital content creation, generating high-quality 3D
characters from single images is challenging, especially given the complexities
of various body poses and the issues of self-occlusion and pose ambiguity. In
this paper, we present CharacterGen, a framework developed to efficiently
generate 3D characters. CharacterGen introduces a streamlined generation
pipeline along with an image-conditioned multi-view diffusion model. This model
effectively calibrates input poses to a canonical form while retaining key
attributes of the input image, thereby addressing the challenges posed by
diverse poses. A transformer-based, generalizable sparse-view reconstruction
model is the other core component of our approach, facilitating the creation of
detailed 3D models from multi-view images. We also adopt a
texture-back-projection strategy to produce high-quality texture maps.
Additionally, we have curated a dataset of anime characters, rendered in
multiple poses and views, to train and evaluate our model. Our approach has
been thoroughly evaluated through quantitative and qualitative experiments,
showing its proficiency in generating 3D characters with high-quality shapes
and textures, ready for downstream applications such as rigging and animation.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17215" title="Abstract">arXiv:2402.17215</a> [<a href="/pdf/2402.17215" title="Download PDF">pdf</a>, <a href="/format/2402.17215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multidimensional unstructured sparse recovery via eigenmatrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ying%2C+L">Lexing Ying</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2311.16609">arXiv:2311.16609</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This note considers the multidimensional unstructured sparse recovery
problems. Examples include Fourier inversion and sparse deconvolution. The
eigenmatrix is a data-driven construction with desired approximate eigenvalues
and eigenvectors proposed for the one-dimensional problems. This note extends
the eigenmatrix approach to multidimensional problems. Numerical results are
provided to demonstrate the performance of the proposed method.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17216" title="Abstract">arXiv:2402.17216</a> [<a href="/pdf/2402.17216" title="Download PDF">pdf</a>, <a href="/ps/2402.17216" title="Download PostScript">ps</a>, <a href="/format/2402.17216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of Machine Learning Optimization in Cloud Computing Resource  Scheduling and Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yulu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiaxin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+W">Weixiang Wan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, cloud computing has been widely used. Cloud computing refers
to the centralized computing resources, users through the access to the
centralized resources to complete the calculation, the cloud computing center
will return the results of the program processing to the user. Cloud computing
is not only for individual users, but also for enterprise users. By purchasing
a cloud server, users do not have to buy a large number of computers, saving
computing costs. According to a report by China Economic News Network, the
scale of cloud computing in China has reached 209.1 billion yuan. At present,
the more mature cloud service providers in China are Ali Cloud, Baidu Cloud,
Huawei Cloud and so on. Therefore, this paper proposes an innovative approach
to solve complex problems in cloud computing resource scheduling and management
using machine learning optimization techniques. Through in-depth study of
challenges such as low resource utilization and unbalanced load in the cloud
environment, this study proposes a comprehensive solution, including
optimization methods such as deep learning and genetic algorithm, to improve
system performance and efficiency, and thus bring new breakthroughs and
progress in the field of cloud computing resource management.Rational
allocation of resources plays a crucial role in cloud computing. In the
resource allocation of cloud computing, the cloud computing center has limited
cloud resources, and users arrive in sequence. Each user requests the cloud
computing center to use a certain number of cloud resources at a specific time.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17217" title="Abstract">arXiv:2402.17217</a> [<a href="/pdf/2402.17217" title="Download PDF">pdf</a>, <a href="/format/2402.17217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Logic Specification-Conditioned Decision Transformer for  Offline Safe Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zijian Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Weichao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenchao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline safe reinforcement learning (RL) aims to train a constraint
satisfaction policy from a fixed dataset. Current state-of-the-art approaches
are based on supervised learning with a conditioned policy. However, these
approaches fall short in real-world applications that involve complex tasks
with rich temporal and logical structures. In this paper, we propose temporal
logic Specification-conditioned Decision Transformer (SDT), a novel framework
that harnesses the expressive power of signal temporal logic (STL) to specify
complex temporal rules that an agent should follow and the sequential modeling
capability of Decision Transformer (DT). Empirical evaluations on the DSRL
benchmarks demonstrate the better capacity of SDT in learning safe and
high-reward policies compared with existing approaches. In addition, SDT shows
good alignment with respect to different desired degrees of satisfaction of the
STL specification that it is conditioned on.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17218" title="Abstract">arXiv:2402.17218</a> [<a href="/pdf/2402.17218" title="Download PDF">pdf</a>, <a href="/format/2402.17218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Viblio: Introducing Credibility Signals and Citations to Video-Sharing  Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hughes%2C+E">Emelia Hughes</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Renee Wang</a>, 
<a href="/search/cs?searchtype=author&query=Juneja%2C+P">Prerna Juneja</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tony Li</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+T">Tanu Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Amy Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">As more users turn to video-sharing platforms like YouTube as an information
source, they may consume misinformation despite their best efforts. In this
work, we investigate ways that users can better assess the credibility of
videos by first exploring how users currently determine credibility using
existing signals on platforms and then by introducing and evaluating new
credibility-based signals. We conducted 12 contextual inquiry interviews with
YouTube users, determining that participants used a combination of existing
signals, such as the channel name, the production quality, and prior knowledge,
to evaluate credibility, yet sometimes stumbled in their efforts to do so. We
then developed Viblio, a prototype system that enables YouTube users to view
and add citations and related information while watching a video based on our
participants' needs. From an evaluation with 12 people, all participants found
Viblio to be intuitive and useful in the process of evaluating a video's
credibility and could see themselves using Viblio in the future.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17219" title="Abstract">arXiv:2402.17219</a> [<a href="/pdf/2402.17219" title="Download PDF">pdf</a>, <a href="/format/2402.17219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blockchain for Finance: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hanjie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Q">Qian Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenguang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Butian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yuan Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Huayun Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+E">Erwu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">As an innovative technology for enhancing authenticity, security, and risk
management, blockchain is being widely adopted in trade and finance systems.
The unique capabilities of blockchain, such as immutability and transparency,
enable new business models of distributed data storage, point-to-point
transactions, and decentralized autonomous organizations. In this paper, we
focus on blockchain-based securities trading, in which blockchain technology
plays a vital role in financial services as it ultimately lifts trust and frees
the need for third-party verification by using consensus-based verification. We
investigate the 12 most popular blockchain platforms and elaborate on 6
platforms that are related to finance, seeking to provide a panorama of
securities trading practices. Meanwhile, this survey provides a comprehensive
summary of blockchain-based securities trading applications. We gather numerous
practical applications of blockchain-based securities trading and categorize
them into four distinct categories. For each category, we introduce a typical
example and explain how blockchain contributes to solving the key problems
faced by FinTech companies and researchers. Finally, we provide interesting
observations ranging from mainstream blockchain-based financial institutions to
security issues of decentralized finance applications, aiming to picture the
current blockchain ecosystem in finance.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17223" title="Abstract">arXiv:2402.17223</a> [<a href="/pdf/2402.17223" title="Download PDF">pdf</a>, <a href="/ps/2402.17223" title="Download PostScript">ps</a>, <a href="/format/2402.17223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Restricted Double-Spending Attack on PoW-based Blockchains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yiming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiangfan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures. arXiv admin note: text overlap with <a href="/abs/2304.09965">arXiv:2304.09965</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Numerous blockchain applications are designed with tasks that naturally have
finite durations, and hence, a double-spending attack (DSA) on such blockchain
applications leans towards being conducted within a finite timeframe,
specifically before the completion of their tasks. Furthermore, existing
research suggests that practical attackers typically favor executing a DSA
within a finite timeframe due to their limited computational resources. These
observations serve as the impetus for this paper to investigate a
time-restricted DSA (TR-DSA) model on Proof-of-Work based blockchains. In this
TR-DSA model, an attacker only mines its branch within a finite timeframe, and
the TR-DSA is considered unsuccessful if the attacker's branch fails to surpass
the honest miners' branch when the honest miners' branch has grown by a
specific number of blocks. First, we developed a general closed-form expression
for the success probability of a TR-DSA. This developed probability not only
can assist in evaluating the risk of a DSA on blockchain applications with
timely tasks, but also can enable practical attackers with limited
computational resources to assess the feasibility and expected reward of
launching a TR-DSA. In addition, we provide rigorous proof that the success
probability of a TR-DSA is no greater than that of a time-unrestricted DSA
where the attacker indefinitely mines its branch. This result implies that
blockchain applications with timely tasks are less vulnerable to DSAs than
blockchain applications that provide attackers with an unlimited timeframe for
their attacks. Furthermore, we show that the success probability of a TR-DSA is
always smaller than one even though the attacker controls more than half of the
hash rate in the network. This result alerts attackers that there is still a
risk of failure in launching a TR-DSA even if they amass a majority of the hash
rate in the network.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17226" title="Abstract">arXiv:2402.17226</a> [<a href="/pdf/2402.17226" title="Download PDF">pdf</a>, <a href="/format/2402.17226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning in Conversation: Solving Subjective Tasks through Dialogue  Simulation for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanchi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fuwen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved remarkable performance in
objective tasks such as open-domain question answering and mathematical
reasoning, which can often be solved through recalling learned factual
knowledge or chain-of-thought style reasoning. However, we find that the
performance of LLMs in subjective tasks is still unsatisfactory, such as
metaphor recognition, dark humor detection, etc. Compared to objective tasks,
subjective tasks focus more on interpretation or emotional response rather than
a universally accepted reasoning pathway. Based on the characteristics of the
tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC
(Reasoning in Conversation), a method that focuses on solving subjective tasks
through dialogue simulation. The motivation of RiC is to mine useful contextual
information by simulating dialogues instead of supplying chain-of-thought style
rationales, thereby offering potential useful knowledge behind dialogues for
giving the final answers. We evaluate both API-based and open-source LLMs
including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental
results show that RiC can yield significant improvement compared with various
baselines.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17227" title="Abstract">arXiv:2402.17227</a> [<a href="/pdf/2402.17227" title="Download PDF">pdf</a>, <a href="/format/2402.17227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Backpropagation with Variance-Controlled Adaptive Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziteng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sampling-based algorithms, which eliminate ''unimportant'' computations
during forward and/or back propagation (BP), offer potential solutions to
accelerate neural network training. However, since sampling introduces
approximations to training, such algorithms may not consistently maintain
accuracy across various tasks. In this work, we introduce a variance-controlled
adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an
unbiased stochastic gradient with fine-grained layerwise importance sampling in
data dimension for activation gradient calculation and leverage score sampling
in token dimension for weight gradient calculation. To preserve accuracy, we
control the additional variance by learning the sample ratio jointly with model
parameters during training. We assessed VCAS on multiple fine-tuning and
pre-training tasks in both vision and natural language domains. On all the
tasks, VCAS can preserve the original training loss trajectory and validation
accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction
of the whole training process. The implementation is available at
https://github.com/thu-ml/VCAS .
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17228" title="Abstract">arXiv:2402.17228</a> [<a href="/pdf/2402.17228" title="Download PDF">pdf</a>, <a href="/format/2402.17228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Re-Embedding: Towards Foundation Model-Level Performance in  Computational Pathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wenhao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fengtao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multiple instance learning (MIL) is the most widely used framework in
computational pathology, encompassing sub-typing, diagnosis, prognosis, and
more. However, the existing MIL paradigm typically requires an offline instance
feature extractor, such as a pre-trained ResNet or a foundation model. This
approach lacks the capability for feature fine-tuning within the specific
downstream tasks, limiting its adaptability and performance. To address this
issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding
the instance features online, which captures fine-grained local features and
establishes connections across different regions. Unlike existing works that
focus on pre-training powerful feature extractor or designing sophisticated
instance aggregator, R$^2$T is tailored to re-embed instance features online.
It serves as a portable module that can seamlessly integrate into mainstream
MIL models. Extensive experimental results on common computational pathology
tasks validate that: 1) feature re-embedding improves the performance of MIL
models based on ResNet-50 features to the level of foundation model features,
and further enhances the performance of foundation model features; 2) the
R$^2$T can introduce more significant performance improvements to various MIL
models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest
methods by a large margin. The code is available
at:~\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17229" title="Abstract">arXiv:2402.17229</a> [<a href="/pdf/2402.17229" title="Download PDF">pdf</a>, <a href="/format/2402.17229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preserving Fairness Generalization in Deepfake Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Li Lin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xinan He</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+Y">Yan Ju</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+F">Feng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Although effective deepfake detection models have been developed in recent
years, recent studies have revealed that these models can result in unfair
performance disparities among demographic groups, such as race and gender. This
can lead to particular groups facing unfair targeting or exclusion from
detection, potentially allowing misclassified deepfakes to manipulate public
opinion and undermine trust in the model. The existing method for addressing
this problem is providing a fair loss function. It shows good fairness
performance for intra-domain evaluation but does not maintain fairness for
cross-domain testing. This highlights the significance of fairness
generalization in the fight against deepfakes. In this work, we propose the
first method to address the fairness generalization problem in deepfake
detection by simultaneously considering features, loss, and optimization
aspects. Our method employs disentanglement learning to extract demographic and
domain-agnostic forgery features, fusing them to encourage fair learning across
a flattened loss landscape. Extensive experiments on prominent deepfake
datasets demonstrate our method's effectiveness, surpassing state-of-the-art
approaches in preserving fairness during cross-domain deepfake detection. The
code is available at https://github.com/Purdue-M2/Fairness-Generalization
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17230" title="Abstract">arXiv:2402.17230</a> [<a href="/pdf/2402.17230" title="Download PDF">pdf</a>, <a href="/format/2402.17230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Thought Prompting of Large Language Models for Discovering and  Fixing Software Vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nong%2C+Y">Yu Nong</a>, 
<a href="/search/cs?searchtype=author&query=Aldeen%2C+M">Mohammed Aldeen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Long Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hongxin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Haipeng Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Security vulnerabilities are increasingly prevalent in modern software and
they are widely consequential to our society. Various approaches to defending
against these vulnerabilities have been proposed, among which those leveraging
deep learning (DL) avoid major barriers with other techniques hence attracting
more attention in recent years. However, DL-based approaches face critical
challenges including the lack of sizable and quality-labeled task-specific
datasets and their inability to generalize well to unseen, real-world
scenarios. Lately, large language models (LLMs) have demonstrated impressive
potential in various domains by overcoming those challenges, especially through
chain-of-thought (CoT) prompting. In this paper, we explore how to leverage
LLMs and CoT to address three key software vulnerability analysis tasks:
identifying a given type of vulnerabilities, discovering vulnerabilities of any
type, and patching detected vulnerabilities. We instantiate the general CoT
methodology in the context of these tasks through VSP , our unified,
vulnerability-semantics-guided prompting approach, and conduct extensive
experiments assessing VSP versus five baselines for the three tasks against
three LLMs and two datasets. Results show substantial superiority of our
CoT-inspired prompting (553.3%, 36.5%, and 30.8% higher F1 accuracy for
vulnerability identification, discovery, and patching, respectively, on CVE
datasets) over the baselines. Through in-depth case studies analyzing VSP
failures, we also reveal current gaps in LLM/CoT for challenging vulnerability
cases, while proposing and validating respective improvements.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17231" title="Abstract">arXiv:2402.17231</a> [<a href="/pdf/2402.17231" title="Download PDF">pdf</a>, <a href="/format/2402.17231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+D">Debrup Das</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+D">Debopriyo Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Aditya%2C+S">Somak Aditya</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Ashish Kulkarni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Tool-augmented Large Language Models (TALM) are known to enhance the skillset
of large language models (LLM), thereby, leading to their improved reasoning
abilities across many tasks. While, TALMs have been successfully employed in
different question-answering benchmarks, their efficacy on complex mathematical
reasoning benchmarks, and the potential complimentary benefits offered by tools
for knowledge retrieval and mathematical equation solving, are open research
questions. In this work, we present MATHSENSEI, a tool-augmented large language
model for mathematical reasoning. Augmented with tools for knowledge retrieval
(Bing Web Search), program execution (Python), and symbolic equation solving
(Wolfram-Alpha), we study the complimentary benefits of these tools through
evaluations on mathematical reasoning datasets. We perform exhaustive ablations
on MATH,a popular dataset for evaluating mathematical reasoning on diverse
mathematical disciplines. We also conduct experiments involving well-known tool
planners to study the impact of tool sequencing on the model performance.
MATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with
chain-of-thought on the MATH dataset. We further observe that TALMs are not as
effective for simpler math word problems (in GSM-8k), and the benefit increases
as the complexity and required knowledge increases (progressively over AQuA,
MMLU-Math, and higher level complex questions in MATH). The code and data are
available at https://github.com/Debrup-61/MathSensei.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17232" title="Abstract">arXiv:2402.17232</a> [<a href="/pdf/2402.17232" title="Download PDF">pdf</a>, <a href="/format/2402.17232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-scale Neural Networks for Partial Differential Equations with Small  Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhuang%2C+Q">Qiao Zhuang</a>, 
<a href="/search/math?searchtype=author&query=Yao%2C+C+Z">Chris Ziyi Yao</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zhongqiang Zhang</a>, 
<a href="/search/math?searchtype=author&query=Karniadakis%2C+G+E">George Em Karniadakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We propose a two-scale neural network method for solving partial differential
equations (PDEs) with small parameters using physics-informed neural networks
(PINNs). We directly incorporate the small parameters into the architecture of
neural networks. The proposed method enables solving PDEs with small parameters
in a simple fashion, without adding Fourier features or other computationally
taxing searches of truncation parameters. Various numerical examples
demonstrate reasonable accuracy in capturing features of large derivatives in
the solutions caused by small parameters.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17233" title="Abstract">arXiv:2402.17233</a> [<a href="/pdf/2402.17233" title="Download PDF">pdf</a>, <a href="/format/2402.17233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Square Neural ODE Causal Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+B+J">Bob Junyi Zou</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+M+E">Matthew E. Levine</a>, 
<a href="/search/cs?searchtype=author&query=Zaharieva%2C+D+P">Dessi P. Zaharieva</a>, 
<a href="/search/cs?searchtype=author&query=Johari%2C+R">Ramesh Johari</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+E+B">Emily B. Fox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Methodology (stat.ME)

</div>
<p class="mathjax">Hybrid models combine mechanistic ODE-based dynamics with flexible and
expressive neural network components. Such models have grown rapidly in
popularity, especially in scientific domains where such ODE-based modeling
offers important interpretability and validated causal grounding (e.g., for
counterfactual reasoning). The incorporation of mechanistic models also
provides inductive bias in standard blackbox modeling approaches, critical when
learning from small datasets or partially observed, complex systems.
Unfortunately, as hybrid models become more flexible, the causal grounding
provided by the mechanistic model can quickly be lost. We address this problem
by leveraging another common source of domain knowledge: ranking of treatment
effects for a set of interventions, even if the precise treatment effect is
unknown. We encode this information in a causal loss that we combine with the
standard predictive loss to arrive at a hybrid loss that biases our learning
towards causally valid hybrid models. We demonstrate our ability to achieve a
win-win -- state-of-the-art predictive performance and causal validity -- in
the challenging task of modeling glucose dynamics during exercise.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17235" title="Abstract">arXiv:2402.17235</a> [<a href="/pdf/2402.17235" title="Download PDF">pdf</a>, <a href="/format/2402.17235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Gradient Succeeds for Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jincheng Mei</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zixin Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bo Dai</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Alekh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Szepesvari%2C+C">Csaba Szepesvari</a>, 
<a href="/search/cs?searchtype=author&query=Schuurmans%2C+D">Dale Schuurmans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages; Correction for a previous version published at ICML 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We show that the \emph{stochastic gradient} bandit algorithm converges to a
\emph{globally optimal} policy at an $O(1/t)$ rate, even with a \emph{constant}
step size. Remarkably, global convergence of the stochastic gradient bandit
algorithm has not been previously established, even though it is an old
algorithm known to be applicable to bandits. The new result is achieved by
establishing two novel technical findings: first, the noise of the stochastic
updates in the gradient bandit algorithm satisfies a strong ``growth
condition'' property, where the variance diminishes whenever progress becomes
small, implying that additional noise control via diminishing step sizes is
unnecessary; second, a form of ``weak exploration'' is automatically achieved
through the stochastic gradient updates, since they prevent the action
probabilities from decaying faster than $O(1/t)$, thus ensuring that every
action is sampled infinitely often with probability $1$. These two findings can
be used to show that the stochastic gradient update is already ``sufficient''
for bandits in the sense that exploration versus exploitation is automatically
balanced in a manner that ensures almost sure convergence to a global optimum.
These novel theoretical findings are further verified by experimental results.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17236" title="Abstract">arXiv:2402.17236</a> [<a href="/pdf/2402.17236" title="Download PDF">pdf</a>, <a href="/ps/2402.17236" title="Download PostScript">ps</a>, <a href="/format/2402.17236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Data Mining in Personalized Education: Current Trends and  Future Prospects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhang Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuofan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+W">Wenge Rong</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+Y">Yuanxin Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Zhang Xiong, Haoxuan Li, Zhuang Liu, Zhuofan Chen, Hao Zhou, Wenge
  Rong, Yuanxin Ouyang. A Review of Data Mining in Personalized Education:
  Current Trends and Future Prospects. Frontiers of Digital Education, 2024
  ,1(1): 26-50
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Personalized education, tailored to individual student needs, leverages
educational technology and artificial intelligence (AI) in the digital age to
enhance learning effectiveness. The integration of AI in educational platforms
provides insights into academic performance, learning preferences, and
behaviors, optimizing the personal learning process. Driven by data mining
techniques, it not only benefits students but also provides educators and
institutions with tools to craft customized learning experiences. To offer a
comprehensive review of recent advancements in personalized educational data
mining, this paper focuses on four primary scenarios: educational
recommendation, cognitive diagnosis, knowledge tracing, and learning analysis.
This paper presents a structured taxonomy for each area, compiles commonly used
datasets, and identifies future research directions, emphasizing the role of
data mining in enhancing personalized education and paving the way for future
exploration and innovation.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17237" title="Abstract">arXiv:2402.17237</a> [<a href="/pdf/2402.17237" title="Download PDF">pdf</a>, <a href="/format/2402.17237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image-Text Matching with Multi-View Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Rui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Wanqing Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Existing two-stream models for image-text matching show good performance
while ensuring retrieval speed and have received extensive attention from
industry and academia. These methods use a single representation to encode
image and text separately and get a matching score with cosine similarity or
the inner product of vectors. However, the performance of the two-stream model
is often sub-optimal. On the one hand, a single representation is challenging
to cover complex content comprehensively. On the other hand, in this framework
of lack of interaction, it is challenging to match multiple meanings which
leads to information being ignored. To address the problems mentioned above and
facilitate the performance of the two-stream model, we propose a multi-view
attention approach for two-stream image-text matching MVAM
(\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first
learns multiple image and text representations by diverse attention heads with
different view codes. And then concatenate these representations into one for
matching. A diversity objective is also used to promote diversity between
attention heads. With this method, models are able to encode images and text
from different views and attend to more key points. So we can get
representations that contain more information. When doing retrieval tasks, the
matching scores between images and texts can be calculated from different
aspects, leading to better matching performance. Experiment results on MSCOCO
and Flickr30K show that our proposed model brings improvements over existing
models. Further case studies show that different attention heads can focus on
different contents and finally obtain a more comprehensive representation.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17238" title="Abstract">arXiv:2402.17238</a> [<a href="/pdf/2402.17238" title="Download PDF">pdf</a>, <a href="/format/2402.17238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Negative Sampling Matter? A Review with Insights into its Theory  and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tinglin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+Y">Yukuo Cen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Junshuai Song</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Negative sampling has swiftly risen to prominence as a focal point of
research, with wide-ranging applications spanning machine learning, computer
vision, natural language processing, data mining, and recommender systems. This
growing interest raises several critical questions: Does negative sampling
really matter? Is there a general framework that can incorporate all existing
negative sampling methods? In what fields is it applied? Addressing these
questions, we propose a general framework that leverages negative sampling.
Delving into the history of negative sampling, we trace the development of
negative sampling through five evolutionary paths. We dissect and categorize
the strategies used to select negative sample candidates, detailing global,
local, mini-batch, hop, and memory-based approaches. Our review categorizes
current negative sampling methods into five types: static, hard, GAN-based,
Auxiliary-based, and In-batch methods, providing a clear structure for
understanding negative sampling. Beyond detailed categorization, we highlight
the application of negative sampling in various areas, offering insights into
its practical benefits. Finally, we briefly discuss open problems and future
directions for negative sampling.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17241" title="Abstract">arXiv:2402.17241</a> [<a href="/pdf/2402.17241" title="Download PDF">pdf</a>, <a href="/format/2402.17241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HardTaint: Production-Run Dynamic Taint Analysis via Selective Hardware  Tracing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yueyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yun Qi</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kai Ji</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoliang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuandong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+Z">Zhiqiang Zuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
<p class="mathjax">Dynamic taint analysis (DTA), as a fundamental analysis technique, is widely
used in security, privacy, and diagnosis, etc. As DTA demands to collect and
analyze massive taint data online, it suffers extremely high runtime overhead.
Over the past decades, numerous attempts have been made to lower the overhead
of DTA. Unfortunately, the reductions they achieved are marginal, causing DTA
only applicable to the debugging/testing scenarios. In this paper, we propose
and implement HardTaint, a system that can realize production-run dynamic taint
tracking. HardTaint adopts a hybrid and systematic design which combines static
analysis, selective hardware tracing and parallel graph processing techniques.
The comprehensive evaluations demonstrate that HardTaint introduces only around
9% runtime overhead which is an order of magnitude lower than the
state-of-the-arts, while without sacrificing any taint detection capability.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17242" title="Abstract">arXiv:2402.17242</a> [<a href="/pdf/2402.17242" title="Download PDF">pdf</a>, <a href="/format/2402.17242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Community Search with Accuracy Guarantee on Attributed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Shuzhan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaoliang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yuxia Geng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhenghe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+X">Xiangyu Ke</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianxing Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Given an attributed graph $G$ and a query node $q$, \underline{C}ommunity
\underline{S}earch over \underline{A}ttributed \underline{G}raphs (CS-AG) aims
to find a structure- and attribute-cohesive subgraph from $G$ that contains
$q$. Although CS-AG has been widely studied, they still face three challenges.
(1) Exact methods based on graph traversal are time-consuming, especially for
large graphs. Some tailored indices can improve efficiency, but introduce
nonnegligible storage and maintenance overhead. (2) Approximate methods with a
loose approximation ratio only provide a coarse-grained evaluation of a
community's quality, rather than a reliable evaluation with an accuracy
guarantee in runtime. (3) Attribute cohesiveness metrics often ignores the
important correlation with the query node $q$. We formally define our CS-AG
problem atop a $q$-centric attribute cohesiveness metric considering both
textual and numerical attributes, for $k$-core model on homogeneous graphs. We
show the problem is NP-hard. To solve it, we first propose an exact baseline
with three pruning strategies. Then, we propose an index-free
sampling-estimation-based method to quickly return an approximate community
with an accuracy guarantee, in the form of a confidence interval. Once a good
result satisfying a user-desired error bound is reached, we terminate it early.
We extend it to heterogeneous graphs, $k$-truss model, and size-bounded CS.
Comprehensive experimental studies on ten real-world datasets show its
superiority, e.g., at least 1.54$\times$ (41.1$\times$ on average) faster in
response time and a reliable relative error (within a user-specific error
bound) of attribute cohesiveness is achieved.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17243" title="Abstract">arXiv:2402.17243</a> [<a href="/pdf/2402.17243" title="Download PDF">pdf</a>, <a href="/ps/2402.17243" title="Download PostScript">ps</a>, <a href="/format/2402.17243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spike up Prime Interest in Science and Technology through  Constructionist Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrovi%C4%8D%2C+P">Pavel Petrovi&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Agarshev%2C+F">Fedir Agarshev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was co-funded by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338 Open Access Data discussed in the article is available at <a href="https://robotika.sk/spike">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of EDULEARN23 Conference 3rd-5th July 2023, Palma,
  Mallorca, Spain, pp. 5562-5570, ISBN: 978-84-09-52151-7
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Robotics sets have been successfully used in elementary and secondary schools
in conformance with the 'learning through play' philosophy fostered by LEGO
Education, while utilizing the Constructionism didactic approach. Learners
discover and acquire knowledge through first-hand tangible experiences,
building their own representations in a constructivist learning process. Usual
pedagogical goals of the activities include introduction to the principles of
control, mechanics, programming, and robotics [1]. They are organized as
hands-on learning situations with teamwork cooperation of learners,
project-based learning, sharing and presentations of the learners group
experiences. Arriving from this tradition, we focus on a slightly different
scenarios: employing the robotics sets and the named approaches when learning
Physics, Mathematics, Art, Science, and other subjects. In carefully designed
projects, learners build interactive models that demonstrate concepts,
principles, and phenomena, perform experiments, and modify them in elaboration
phases with the aim to connect, create associations and links to the actual
underlying theoretical curriculum. In this way, they are collecting practical
experiences which are prerequisite to successful learning process. Based on
feedback from children, we continue upon two previous sets of activities that
focused on Physics and Mathematics, this time with projects built around games.
Learners play various games with physical artifacts in the real-world - with
the models they build. They acquire skills while playing the games, analyze
them, and learn about the underlying principles. They modify the game rules,
strategies, create extensions, and interact with each other in an entertaining
and engaging settings. This time we have designed the activities together with
the children, students of applied robotics seminar, and a student of Applied
Informatics.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17245" title="Abstract">arXiv:2402.17245</a> [<a href="/pdf/2402.17245" title="Download PDF">pdf</a>, <a href="/format/2402.17245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in  Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daiqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Kamko%2C+A">Aleks Kamko</a>, 
<a href="/search/cs?searchtype=author&query=Akhgari%2C+E">Ehsan Akhgari</a>, 
<a href="/search/cs?searchtype=author&query=Sabet%2C+A">Ali Sabet</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Linmiao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Doshi%2C+S">Suhail Doshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Model weights: <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this work, we share three insights for achieving state-of-the-art
aesthetic quality in text-to-image generative models. We focus on three
critical aspects for model improvement: enhancing color and contrast, improving
generation across multiple aspect ratios, and improving human-centric fine
details. First, we delve into the significance of the noise schedule in
training a diffusion model, demonstrating its profound impact on realism and
visual fidelity. Second, we address the challenge of accommodating various
aspect ratios in image generation, emphasizing the importance of preparing a
balanced bucketed dataset. Lastly, we investigate the crucial role of aligning
model outputs with human preferences, ensuring that generated images resonate
with human perceptual expectations. Through extensive analysis and experiments,
Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic
quality under various conditions and aspect ratios, outperforming both
widely-used open-source models like SDXL and Playground v2, and closed-source
commercial systems such as DALLE 3 and Midjourney v5.2. Our model is
open-source, and we hope the development of Playground v2.5 provides valuable
guidelines for researchers aiming to elevate the aesthetic quality of
diffusion-based image generation models.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17247" title="Abstract">arXiv:2402.17247</a> [<a href="/pdf/2402.17247" title="Download PDF">pdf</a>, <a href="/ps/2402.17247" title="Download PostScript">ps</a>, <a href="/format/2402.17247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Optimal Control for Linear Quadratic Tracking with Unknown  Target States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yao Li</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+C">Chengpu Yu</a>, 
<a href="/search/eess?searchtype=author&query=Fang%2C+H">Hao Fang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jie Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper addresses the inverse optimal control for the linear quadratic
tracking problem with a fixed but unknown target state, which aims to estimate
the possible triplets comprising the target state, the state weight matrix, and
the input weight matrix from observed optimal control input and the
corresponding state trajectories. Sufficient conditions have been provided for
the unique determination of both the linear quadratic cost function as well as
the target state. A computationally efficient and numerically reliable
parameter identification algorithm is proposed by equating optimal control
strategies with a system of linear equations, and the associated relative error
upper bound is derived in terms of data volume and signal-to-noise ratio.
Moreover, the proposed inverse optimal control algorithm is applied for the
joint cluster coordination and intent identification of a multi-agent system.
By incorporating the structural constraint of the Laplace matrix, the relative
error upper bound can be reduced accordingly. Finally, the algorithm's
efficiency and accuracy are validated by a vehicle-on-a-lever example and a
multi-agent formation control example.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17249" title="Abstract">arXiv:2402.17249</a> [<a href="/pdf/2402.17249" title="Download PDF">pdf</a>, <a href="/format/2402.17249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-Based Speech and Vision Synthesis to Improve Phishing  Attack Detection through a Multi-layer Adaptive Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ige%2C+T">Tosin Ige</a>, 
<a href="/search/cs?searchtype=author&query=Kiekintveld%2C+C">Christopher Kiekintveld</a>, 
<a href="/search/cs?searchtype=author&query=Piplai%2C+A">Aritran Piplai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The ever-evolving ways attacker continues to im prove their phishing
techniques to bypass existing state-of-the-art phishing detection methods pose
a mountain of challenges to researchers in both industry and academia research
due to the inability of current approaches to detect complex phishing attack.
Thus, current anti-phishing methods remain vulnerable to complex phishing
because of the increasingly sophistication tactics adopted by attacker coupled
with the rate at which new tactics are being developed to evade detection. In
this research, we proposed an adaptable framework that combines Deep learning
and Randon Forest to read images, synthesize speech from deep-fake videos, and
natural language processing at various predictions layered to significantly
increase the performance of machine learning models for phishing attack
detection.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17251" title="Abstract">arXiv:2402.17251</a> [<a href="/pdf/2402.17251" title="Download PDF">pdf</a>, <a href="/format/2402.17251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-based and Diversity-driven Specificity in Compositional  Zero-Shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yun Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Compositional Zero-Shot Learning (CZSL) aims to recognize unseen
attribute-object pairs based on a limited set of observed examples. Current
CZSL methodologies, despite their advancements, tend to neglect the distinct
specificity levels present in attributes. For instance, given images of sliced
strawberries, they may fail to prioritize `Sliced-Strawberry' over a generic
`Red-Strawberry', despite the former being more informative. They also suffer
from ballooning search space when shifting from Close-World (CW) to Open-World
(OW) CZSL. To address the issues, we introduce the Context-based and
Diversity-driven Specificity learning framework for CZSL (CDS-CZSL). Our
framework evaluates the specificity of attributes by considering the diversity
of objects they apply to and their related context. This novel approach allows
for more accurate predictions by emphasizing specific attribute-object pairs
and improves composition filtering in OW-CZSL. We conduct experiments in both
CW and OW scenarios, and our model achieves state-of-the-art results across
three datasets.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17256" title="Abstract">arXiv:2402.17256</a> [<a href="/pdf/2402.17256" title="Download PDF">pdf</a>, <a href="/format/2402.17256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pei Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yejie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoshuai Song</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+Y">Yutao Mou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Yunsen Xian</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xunliang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Out-of-domain (OOD) intent detection aims to examine whether the user's query
falls outside the predefined domain of the system, which is crucial for the
proper functioning of task-oriented dialogue (TOD) systems. Previous methods
address it by fine-tuning discriminative models. Recently, some studies have
been exploring the application of large language models (LLMs) represented by
ChatGPT to various downstream tasks, but it is still unclear for their ability
on OOD detection task.This paper conducts a comprehensive evaluation of LLMs
under various experimental settings, and then outline the strengths and
weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot
capabilities, but is still at a disadvantage compared to models fine-tuned with
full resource. More deeply, through a series of additional analysis
experiments, we discuss and summarize the challenges faced by LLMs and provide
guidance for future work including injecting domain knowledge, strengthening
knowledge transfer from IND(In-domain) to OOD, and understanding long
instructions.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17257" title="Abstract">arXiv:2402.17257</a> [<a href="/pdf/2402.17257" title="Download PDF">pdf</a>, <a href="/format/2402.17257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIME: Robust Preference-based Reinforcement Learning with Noisy  Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jie Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+G">Gang Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xingyuan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+Q">Qinghai Miao</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yisheng Lv</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei-Yue Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Preference-based Reinforcement Learning (PbRL) avoids the need for reward
engineering by harnessing human preferences as the reward signal. However,
current PbRL algorithms over-reliance on high-quality feedback from domain
experts, which results in a lack of robustness. In this paper, we present RIME,
a robust PbRL algorithm for effective reward learning from noisy preferences.
Our method incorporates a sample selection-based discriminator to dynamically
filter denoised preferences for robust training. To mitigate the accumulated
error caused by incorrect selection, we propose to warm start the reward model,
which additionally bridges the performance gap during transition from
pre-training to online training in PbRL. Our experiments on robotic
manipulation and locomotion tasks demonstrate that RIME significantly enhances
the robustness of the current state-of-the-art PbRL method. Ablation studies
further demonstrate that the warm start is crucial for both robustness and
feedback-efficiency in limited-feedback cases.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17259" title="Abstract">arXiv:2402.17259</a> [<a href="/pdf/2402.17259" title="Download PDF">pdf</a>, <a href="/format/2402.17259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EDTC: enhance depth of text comprehension in automated audio captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+L">Liwen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Modality discrepancies have perpetually posed significant challenges within
the realm of Automated Audio Captioning (AAC) and across all multi-modal
domains. Facilitating models in comprehending text information plays a pivotal
role in establishing a seamless connection between the two modalities of text
and audio. While recent research has focused on closing the gap between these
two modalities through contrastive learning, it is challenging to bridge the
difference between both modalities using only simple contrastive loss. This
paper introduces Enhance Depth of Text Comprehension (EDTC), which enhances the
model's understanding of text information from three different perspectives.
First, we propose a novel fusion module, FUSER, which aims to extract shared
semantic information from different audio features through feature fusion. We
then introduced TRANSLATOR, a novel alignment module designed to align audio
features and text features along the tensor level. Finally, the weights are
updated by adding momentum to the twin structure so that the model can learn
information about both modalities at the same time. The resulting method
achieves state-of-the-art performance on AudioCaps datasets and demonstrates
results comparable to the state-of-the-art on Clotho datasets.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17262" title="Abstract">arXiv:2402.17262</a> [<a href="/pdf/2402.17262" title="Download PDF">pdf</a>, <a href="/format/2402.17262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speak Out of Turn: Safety Vulnerability of Large Language Models in  Multi-turn Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhenhong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jiuyang Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haopeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Quan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zherui Li</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+S">Sen Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> working in progress 23pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have been demonstrated to generate illegal or
unethical responses, particularly when subjected to "jailbreak." Research on
jailbreak has highlighted the safety issues of LLMs. However, prior studies
have predominantly focused on single-turn dialogue, ignoring the potential
complexities and risks presented by multi-turn dialogue, a crucial mode through
which humans derive information from LLMs. In this paper, we argue that humans
could exploit multi-turn dialogue to induce LLMs into generating harmful
information. LLMs may not intend to reject cautionary or borderline unsafe
queries, even if each turn is closely served for one malicious purpose in a
multi-turn dialogue. Therefore, by decomposing an unsafe query into several
sub-queries for multi-turn dialogue, we induced LLMs to answer harmful
sub-questions incrementally, culminating in an overall harmful response. Our
experiments, conducted across a wide range of LLMs, indicate current
inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our
findings expose vulnerabilities of LLMs in complex scenarios involving
multi-turn dialogue, presenting new challenges for the safety of LLMs.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17263" title="Abstract">arXiv:2402.17263</a> [<a href="/pdf/2402.17263" title="Download PDF">pdf</a>, <a href="/format/2402.17263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+P">Pengjie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chengshun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shiguang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhumin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+J">Jiahuan Pei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring
pre-trained large language models (LLMs), especially as the models' scale and
the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the
idea that the adaptation process is intrinsically low-dimensional, i.e.,
significant model changes can be represented with relatively few parameters.
However, decreasing the rank encounters challenges with generalization errors
for specific tasks when compared to full-parameter fine-tuning. We present
MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters
while maintaining a higher rank, thereby offering improved performance
potential. The core idea is to freeze original pretrained weights and train a
group of mini LoRAs with only a small number of parameters. This can capture a
significant degree of diversity among mini LoRAs, thus promoting better
generalization ability. We conduct a theoretical analysis and empirical studies
on various NLP tasks. Our experimental results show that, compared to LoRA,
MELoRA achieves better performance with 8 times fewer trainable parameters on
natural language understanding tasks and 36 times fewer trainable parameters on
instruction following tasks, which demonstrates the effectiveness of MELoRA.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17264" title="Abstract">arXiv:2402.17264</a> [<a href="/pdf/2402.17264" title="Download PDF">pdf</a>, <a href="/format/2402.17264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit Interaction for Fusion-Based Place Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingyi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Junyi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zijie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xieyuanli Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+L">Ling Pei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Fusion-based place recognition is an emerging technique jointly utilizing
multi-modal perception data, to recognize previously visited places in
GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based
place recognition methods combine multi-modal features in implicit manners.
While achieving remarkable results, they do not explicitly consider what the
individual modality affords in the fusion system. Therefore, the benefit of
multi-modal feature fusion may not be fully explored. In this paper, we propose
a novel fusion-based network, dubbed EINet, to achieve explicit interaction of
the two modalities. EINet uses LiDAR ranges to supervise more robust vision
features for long time spans, and simultaneously uses camera RGB data to
improve the discrimination of LiDAR point clouds. In addition, we develop a new
benchmark for the place recognition task based on the nuScenes dataset. To
establish this benchmark for future research with comprehensive comparisons, we
introduce both supervised and self-supervised training schemes alongside
evaluation protocols. We conduct extensive experiments on the proposed
benchmark, and the experimental results show that our EINet exhibits better
recognition performance as well as solid generalization ability compared to the
state-of-the-art fusion-based place recognition approaches. Our open-source
code and benchmark are released at: https://github.com/BIT-XJY/EINet.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17268" title="Abstract">arXiv:2402.17268</a> [<a href="/pdf/2402.17268" title="Download PDF">pdf</a>, <a href="/format/2402.17268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning Based Robust Volt/Var Control in Active  Distribution Networks With Imprecisely Known Delay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cheng%2C+H">Hong Cheng</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+H">Huan Luo</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+W">Weitao Li</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Q">Qiyue Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Active distribution networks (ADNs) incorporating massive photovoltaic (PV)
devices encounter challenges of rapid voltage fluctuations and potential
violations. Due to the fluctuation and intermittency of PV generation, the
state gap, arising from time-inconsistent states and exacerbated by imprecisely
known system delays, significantly impacts the accuracy of voltage control.
This paper addresses this challenge by introducing a framework for delay
adaptive Volt/Var control (VVC) in the presence of imprecisely known system
delays to regulate the reactive power of PV inverters. The proposed approach
formulates the voltage control, based on predicted system operation states, as
a robust VVC problem. It employs sample selection from the state prediction
interval to promptly identify the worst-performing system operation state.
Furthermore, we leverage the decentralized partially observable Markov decision
process (Dec-POMDP) to reformulate the robust VVC problem. We design Multiple
Policy Networks and employ Multiple Policy Networks and Reward Shaping-based
Multi-agent Twin Delayed Deep Deterministic Policy Gradient (MPNRS-MATD3)
algorithm to efficiently address and solve the Dec-POMDP model-based problem.
Simulation results show the delay adaption characteristic of our proposed
framework, and the MPNRS-MATD3 outperforms other multi-agent reinforcement
learning algorithms in robust voltage control.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17269" title="Abstract">arXiv:2402.17269</a> [<a href="/pdf/2402.17269" title="Download PDF">pdf</a>, <a href="/format/2402.17269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C+T">Cam-Van Thi Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C">Cao-Bach Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+Q">Quang-Thuy Ha</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D">Duc-Trong Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Emotion recognition in conversation (ERC) is a crucial task in natural
language processing and affective computing. This paper proposes MultiDAG+CL, a
novel approach for Multimodal Emotion Recognition in Conversation (ERC) that
employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual
features within a unified framework. The model is enhanced by Curriculum
Learning (CL) to address challenges related to emotional shifts and data
imbalance. Curriculum learning facilitates the learning process by gradually
presenting training samples in a meaningful order, thereby improving the
model's performance in handling emotional variations and data imbalance.
Experimental results on the IEMOCAP and MELD datasets demonstrate that the
MultiDAG+CL models outperform baseline models.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17270" title="Abstract">arXiv:2402.17270</a> [<a href="/pdf/2402.17270" title="Download PDF">pdf</a>, <a href="/format/2402.17270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social  Dilemmas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+C">Chunjiang Mu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shuyue Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">The study of cooperation within social dilemmas has long been a fundamental
topic across various disciplines, including computer science and social
science. Recent advancements in Artificial Intelligence (AI) have significantly
reshaped this field, offering fresh insights into understanding and enhancing
cooperation. This survey examines three key areas at the intersection of AI and
cooperation in social dilemmas. First, focusing on multi-agent cooperation, we
review the intrinsic and external motivations that support cooperation among
rational agents, and the methods employed to develop effective strategies
against diverse opponents. Second, looking into human-agent cooperation, we
discuss the current AI algorithms for cooperating with humans and the human
biases towards AI agents. Third, we review the emergent field of leveraging AI
agents to enhance cooperation among humans. We conclude by discussing future
research avenues, such as using large language models, establishing unified
theoretical frameworks, revisiting existing theories of human cooperation, and
exploring multiple real-world applications.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17273" title="Abstract">arXiv:2402.17273</a> [<a href="/pdf/2402.17273" title="Download PDF">pdf</a>, <a href="/format/2402.17273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time tracking of moving objects from scattering matrix in  real-world microwave imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Son%2C+S">Seeing-Ho Son</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+K">Kwang-Jae Lee</a>, 
<a href="/search/math?searchtype=author&query=Park%2C+W">Won-Kwang Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The problem of the real-time microwave imaging of small, moving objects from
a scattering matrix, whose elements are measured scattering parameters, without
diagonal elements is considered herein. An imaging algorithm based on a
Kirchhoff migration operated at single frequency is designed, and its
mathematical structure is investigated by establishing a relationship with an
infinite series of Bessel functions of integer order and antenna configuration.
This is based on the application of the Born approximation to the scattering
parameters of small objects. The structure explains the reason for the
detection of moving objects via a designed imaging function and supplies its
some properties. To demonstrate the strengths and weaknesses of the proposed
algorithm, various simulations with real-data are conducted.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17275" title="Abstract">arXiv:2402.17275</a> [<a href="/pdf/2402.17275" title="Download PDF">pdf</a>, <a href="/format/2402.17275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Shot Structure-Aware Stylized Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hansam Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jonghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Seunggyu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+Y">Yonghyun Jeong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While GAN-based models have been successful in image stylization tasks, they
often struggle with structure preservation while stylizing a wide range of
input images. Recently, diffusion models have been adopted for image
stylization but still lack the capability to maintain the original quality of
input images. Building on this, we propose OSASIS: a novel one-shot stylization
method that is robust in structure preservation. We show that OSASIS is able to
effectively disentangle the semantics from the structure of an image, allowing
it to control the level of content and style implemented to a given input. We
apply OSASIS to various experimental settings, including stylization with
out-of-domain reference images and stylization with text-driven manipulation.
Results show that OSASIS outperforms other stylization methods, especially for
input images that were rarely encountered during training, providing a
promising solution to stylization via diffusion models.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17277" title="Abstract">arXiv:2402.17277</a> [<a href="/pdf/2402.17277" title="Download PDF">pdf</a>, <a href="/format/2402.17277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RISAR: RIS-assisted Human Activity Recognition with Commercial Wi-Fi  Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Junshuo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yunlong Huang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+W">Wei Yang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/eess?searchtype=author&query=Xiong%2C+R">Rujing Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Mi%2C+T">Tiebin Mi</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+X">Xin Shi</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+R+C">Robert C. Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Human activity recognition (HAR) is integral to smart homes, security, and
healthcare. Existing systems face challenges such as the absence of
line-of-sight links, insufficient details regarding the sensing subject, and
inefficiencies in noise reduction and feature extraction from channel state
information. To address these issues, this study builds a reconfigurable
intelligent surface (RIS)-assisted passive human activity recognition (RISAR)
system, compatible with commercial Wi-Fi devices. RISAR employs a RIS to
augment the orientation and spatial diversity of Wi-Fi signals, thereby
facilitating improved detection capabilities. A new denoising and feature
extraction technique, the high-dimensional factor model, based on random matrix
theory, is proposed for noise elimination and salient temporal feature
extraction. On this basis, a dual-stream spatial-temporal attention network
model is developed for assigning variable weights to different characteristics
and sequences, mimicking human cognitive processes in prioritizing essential
information. Experimental analysis shows that RISAR significantly outperforms
existing HAR systems in accuracy and efficiency, achieving an average accuracy
of 97.26%. These findings not only highlight the adaptability of the RISAR
system but also underscore its potential as a robust solution for activity
recognition across complex environments.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17279" title="Abstract">arXiv:2402.17279</a> [<a href="/pdf/2402.17279" title="Download PDF">pdf</a>, <a href="/format/2402.17279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiFashion: Towards Personalized Outfit Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yiyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunshan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jizhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiangnan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The evolution of Outfit Recommendation (OR) in the realm of fashion has
progressed through two distinct phases: Pre-defined Outfit Recommendation and
Personalized Outfit Composition. Despite these advancements, both phases face
limitations imposed by existing fashion products, hindering their effectiveness
in meeting users' diverse fashion needs. The emergence of AI-generated content
has paved the way for OR to overcome these constraints, demonstrating the
potential for personalized outfit generation.
<br />In pursuit of this, we introduce an innovative task named Generative Outfit
Recommendation (GOR), with the goal of synthesizing a set of fashion images and
assembling them to form visually harmonious outfits customized to individual
users. The primary objectives of GOR revolve around achieving high fidelity,
compatibility, and personalization of the generated outfits. To accomplish
these, we propose DiFashion, a generative outfit recommender model that
harnesses exceptional diffusion models for the simultaneous generation of
multiple fashion images. To ensure the fulfillment of these objectives, three
types of conditions are designed to guide the parallel generation process and
Classifier-Free-Guidance are employed to enhance the alignment between
generated images and conditions. DiFashion is applied to both personalized
Fill-In-The-Blank and GOR tasks, and extensive experiments are conducted on the
iFashion and Polyvore-U datasets. The results of quantitative and
human-involved qualitative evaluations highlight the superiority of DiFashion
over competitive baselines.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17285" title="Abstract">arXiv:2402.17285</a> [<a href="/pdf/2402.17285" title="Download PDF">pdf</a>, <a href="/format/2402.17285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder  Super-resolution Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Hao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Maoguo Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to
effectively capture the complex spectral-spatial relationships and low-level
details, while diffusion models represent a promising generative model known
for their exceptional performance in modeling complex relations and learning
high and low-level visual features. The direct application of diffusion models
to HSI SR is hampered by challenges such as difficulties in model convergence
and protracted inference time. In this work, we introduce a novel
Group-Autoencoder (GAE) framework that synergistically combines with the
diffusion model to construct a highly effective HSI SR model (DMGASR). Our
proposed GAE framework encodes high-dimensional HSI data into low-dimensional
latent space where the diffusion model works, thereby alleviating the
difficulty of training the diffusion model while maintaining band correlation
and considerably reducing inference time. Experimental results on both natural
and remote sensing hyperspectral datasets demonstrate that the proposed method
is superior to other state-of-the-art methods both visually and metrically.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17286" title="Abstract">arXiv:2402.17286</a> [<a href="/pdf/2402.17286" title="Download PDF">pdf</a>, <a href="/format/2402.17286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Constraint-based Mathematical Modeling Library in Prolog with Answer  Constraint Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fages%2C+F">Fran&#xe7;ois Fages</a> (Lifeware)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Mathematical Software (cs.MS); Programming Languages (cs.PL)

</div>
<p class="mathjax">Constraint logic programming emerged in the late 80's as a highly declarative
class of programming languages based on first-order logic and theories with
decidable constraint languages, thereby subsuming Prolog restricted to equality
constraints over the Herbrand's term domain. This approach has proven extremely
successfull in solving combinatorial problems in the industry which quickly led
to the development of a variety of constraint solving libraries in standard
programming languages. Later came the design of a purely declarative front-end
constraint-based modeling language, MiniZinc, independent of the constraint
solvers, in order to compare their performances and create model benchmarks.
Beyond that purpose, the use of a high-level modeling language such as MiniZinc
to develop complete applications, or to teach constraint programming, is
limited by the impossibility to program search strategies, or new constraint
solvers, in a modeling language, as well as by the absence of an integrated
development environment for both levels of constraint-based modeling and
constraint solving. In this paper, we propose to solve those issues by taking
Prolog with its constraint solving libraries, as a unified relation-based
modeling and programming language. We present a Prolog library for high-level
constraint-based mathematical modeling, inspired by MiniZinc, using subscripted
variables (arrays) in addition to lists and terms, quantifiers and iterators in
addition to recursion, together with a patch of constraint libraries in order
to allow array functional notations in constraints. We show that this approach
does not come with a significant computation time overhead, and presents
several advantages in terms of the possibility of focussing on mathematical
modeling, getting answer constraints in addition to ground solutions,
programming search or constraint solvers if needed, and debugging models within
a unique modeling and programming environment.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17287" title="Abstract">arXiv:2402.17287</a> [<a href="/pdf/2402.17287" title="Download PDF">pdf</a>, <a href="/format/2402.17287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interpretable Evaluation of Entropy-based Novelty of Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C+T">Cheuk Ting Li</a>, 
<a href="/search/cs?searchtype=author&query=Farnia%2C+F">Farzan Farnia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">The massive developments of generative model frameworks and architectures
require principled methods for the evaluation of a model's novelty compared to
a reference dataset or baseline generative models. While the recent literature
has extensively studied the evaluation of the quality, diversity, and
generalizability of generative models, the assessment of a model's novelty
compared to a baseline model has not been adequately studied in the machine
learning community. In this work, we focus on the novelty assessment under
multi-modal generative models and attempt to answer the following question:
Given the samples of a generative model $\mathcal{G}$ and a reference dataset
$\mathcal{S}$, how can we discover and count the modes expressed by
$\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral
approach to the described task and propose the Kernel-based Entropic Novelty
(KEN) score to quantify the mode-based novelty of distribution $P_\mathcal{G}$
with respect to distribution $P_\mathcal{S}$. We analytically interpret the
behavior of the KEN score under mixture distributions with sub-Gaussian
components. Next, we develop a method based on Cholesky decomposition to
compute the KEN score from observed samples. We support the KEN-based
quantification of novelty by presenting several numerical results on synthetic
and real image distributions. Our numerical results indicate the success of the
proposed approach in detecting the novel modes and the comparison of
state-of-the-art generative models.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17289" title="Abstract">arXiv:2402.17289</a> [<a href="/pdf/2402.17289" title="Download PDF">pdf</a>, <a href="/format/2402.17289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active propulsion noise shaping for multi-rotor aircraft localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gabriele%2C+S">Serussi Gabriele</a>, 
<a href="/search/cs?searchtype=author&query=Tamir%2C+S">Shor Tamir</a>, 
<a href="/search/cs?searchtype=author&query=Tom%2C+H">Hirshberg Tom</a>, 
<a href="/search/cs?searchtype=author&query=Chaim%2C+B">Baskin Chaim</a>, 
<a href="/search/cs?searchtype=author&query=Alex%2C+B">Bronstein Alex</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for
navigation purposes. However, visual localization and odometry techniques
suffer from poor performance in low or direct sunlight, a limited field of
view, and vulnerability to occlusions. Acoustic sensing can serve as a
complementary or even alternative modality for vision in many situations, and
it also has the added benefits of lower system cost and energy footprint, which
is especially important for micro aircraft. This paper proposes actively
controlling and shaping the aircraft propulsion noise generated by the rotors
to benefit localization tasks, rather than considering it a harmful nuisance.
We present a neural network architecture for selfnoise-based localization in a
known environment. We show that training it simultaneously with learning
time-varying rotor phase modulation achieves accurate and robust localization.
The proposed methods are evaluated using a computationally affordable
simulation of MAV rotor noise in 2D acoustic environments that is fitted to
real recordings of rotor pressure fields.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17290" title="Abstract">arXiv:2402.17290</a> [<a href="/pdf/2402.17290" title="Download PDF">pdf</a>, <a href="/ps/2402.17290" title="Download PostScript">ps</a>, <a href="/format/2402.17290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Lower Bounds for Block-Structured Integer Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hunkenschr%C3%B6der%2C+C">Christoph Hunkenschr&#xf6;der</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+K">Kim-Manuel Klein</a>, 
<a href="/search/cs?searchtype=author&query=Kouteck%C3%BD%2C+M">Martin Kouteck&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=Lassota%2C+A">Alexandra Lassota</a>, 
<a href="/search/cs?searchtype=author&query=Levin%2C+A">Asaf Levin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">We study fundamental block-structured integer programs called tree-fold and
multi-stage IPs. Tree-fold IPs admit a constraint matrix with independent
blocks linked together by few constraints in a recursive pattern; and
transposing their constraint matrix yields multi-stage IPs. The
state-of-the-art algorithms to solve these IPs have an exponential gap in their
running times, making it natural to ask whether this gap is inherent. We answer
this question affirmative. Assuming the Exponential Time Hypothesis, we prove
lower bounds showing that the exponential difference is necessary, and that the
known algorithms are near optimal. Moreover, we prove unconditional lower
bounds on the norms of the Graver basis, a fundamental building block of all
known algorithms to solve these IPs. This shows that none of the current
approaches can be improved beyond this bound.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17292" title="Abstract">arXiv:2402.17292</a> [<a href="/pdf/2402.17292" title="Download PDF">pdf</a>, <a href="/format/2402.17292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DivAvatar: Diverse 3D Avatar Generation with a Single Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+W">Weijing Tao</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+B">Biwen Lei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kunhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+M">Miaomiao Cui</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuansong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+C">Chunyan Miao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-Avatar generation has recently made significant strides due to
advancements in diffusion models. However, most existing work remains
constrained by limited diversity, producing avatars with subtle differences in
appearance for a given text prompt. We design DivAvatar, a novel framework that
generates diverse avatars, empowering 3D creatives with a multitude of distinct
and richly varied 3D avatars from a single text prompt. Different from most
existing work that exploits scene-specific 3D representations such as NeRF,
DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse
avatar generation from simply noise sampling in inference time. DivAvatar has
two key designs that help achieve generation diversity and visual quality. The
first is a noise sampling technique during training phase which is critical in
generating diverse appearances. The second is a semantic-aware zoom mechanism
and a novel depth loss, the former producing appearances of high textual
fidelity by separate fine-tuning of specific body parts and the latter
improving geometry quality greatly by smoothing the generated mesh in the
features space. Extensive experiments show that DivAvatar is highly versatile
in generating avatars of diverse appearances.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17296" title="Abstract">arXiv:2402.17296</a> [<a href="/pdf/2402.17296" title="Download PDF">pdf</a>, <a href="/format/2402.17296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Exposure Correction in Dynamic Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chuanming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Huiyuan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huadong Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Capturing videos with wrong exposure usually produces unsatisfactory visual
effects. While image exposure correction is a popular topic, the video
counterpart is less explored in the literature. Directly applying prior
image-based methods to input videos often results in temporal incoherence with
low visual quality. Existing research in this area is also limited by the lack
of high-quality benchmark datasets. To address these issues, we construct the
first real-world paired video dataset, including both underexposure and
overexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR
cameras and a beam splitter to simultaneously capture improper and normal
exposure videos. In addition, we propose a Video Exposure Correction Network
(VECNet) based on Retinex theory, which incorporates a two-stream illumination
learning mechanism to enhance the overexposure and underexposure factors,
respectively. The estimated multi-frame reflectance and dual-path illumination
components are fused at both feature and image levels, leading to visually
appealing results. Experimental results demonstrate that the proposed method
outperforms existing image exposure correction and underexposed video
enhancement methods. The code and dataset will be available soon.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17298" title="Abstract">arXiv:2402.17298</a> [<a href="/pdf/2402.17298" title="Download PDF">pdf</a>, <a href="/format/2402.17298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArcSin: Adaptive ranged cosine Similarity injected noise for  Language-Driven Visual Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaomin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gongyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bergeles%2C+C">Christos Bergeles</a>, 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+P">Prokar Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Granados%2C+A">Alejandro Granados</a>, 
<a href="/search/cs?searchtype=author&query=Ourselin%2C+S">Sebastien Ourselin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this study, we address the challenging task of bridging the modality gap
between learning from language and inference for visual tasks, including Visual
Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE). We
train models for these tasks in a zero-shot cross-modal transfer setting, a
domain where the previous state-of-the-art method relied on the fixed scale
noise injection, often compromising the semantic content of the original
modality embedding. To combat it, we propose a novel method called Adaptive
ranged cosine Similarity injected noise (ArcSin). First, we introduce an
innovative adaptive noise scale that effectively generates the textual elements
with more variability while preserving the original text feature's integrity.
Second, a similarity pool strategy is employed, expanding the domain
generalization potential by broadening the overall noise scale. This dual
strategy effectively widens the scope of the original domain while safeguarding
content integrity. Our empirical results demonstrate that these models closely
rival those trained on images in terms of performance. Specifically, our method
exhibits substantial improvements over the previous state-of-the-art, achieving
gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively.
Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and
1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries
of what is achievable within the constraints of image-trained model benchmarks.
The code will be released.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17302" title="Abstract">arXiv:2402.17302</a> [<a href="/pdf/2402.17302" title="Download PDF">pdf</a>, <a href="/format/2402.17302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in  Indonesian and Sundanese
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Putri%2C+R+A">Rifki Afina Putri</a>, 
<a href="/search/cs?searchtype=author&query=Haznitrama%2C+F+G">Faiz Ghifari Haznitrama</a>, 
<a href="/search/cs?searchtype=author&query=Adhista%2C+D">Dea Adhista</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+A">Alice Oh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators. Our experiments show that the current best-performing LLM, GPT-4
Turbo, is capable of generating questions with adequate knowledge in Indonesian
but not in Sundanese, highlighting the performance discrepancy between medium-
and lower-resource languages. We also benchmark various LLMs on our generated
datasets and find that they perform better on the LLM-generated datasets
compared to those created by humans.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17304" title="Abstract">arXiv:2402.17304</a> [<a href="/pdf/2402.17304" title="Download PDF">pdf</a>, <a href="/format/2402.17304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing Multimodal Large Language Models for Global and Local Semantic  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Mingxu Tao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Quzhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yansong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongyan Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by LREC-COLING 2024 as a short paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The success of large language models has inspired researchers to transfer
their exceptional representing ability to other modalities. Several recent
works leverage image-caption alignment datasets to train multimodal large
language models (MLLMs), which achieve state-of-the-art performance on
image-to-text tasks. However, there are very few studies exploring whether
MLLMs truly understand the complete image information, i.e., global
information, or if they can only capture some local object information. In this
study, we find that the intermediate layers of models can encode more global
semantic information, whose representation vectors perform better on
visual-language entailment tasks, rather than the topmost layers. We further
probe models for local semantic representation through object detection tasks.
And we draw a conclusion that the topmost layers may excessively focus on local
information, leading to a diminished ability to encode global information.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17306" title="Abstract">arXiv:2402.17306</a> [<a href="/pdf/2402.17306" title="Download PDF">pdf</a>, <a href="/format/2402.17306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Second Round: Diverse Paths Towards Software Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hyrynsalmi%2C+S">Sonja Hyrynsalmi</a>, 
<a href="/search/cs?searchtype=author&query=Peltonen%2C+E">Ella Peltonen</a>, 
<a href="/search/cs?searchtype=author&query=Vainionp%C3%A4%C3%A4%2C+F">Fanny Vainionp&#xe4;&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Hyrynsalmi%2C+S">Sami Hyrynsalmi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2024 ACMIEEE Workshop on Gender Equality, Diversity, and Inclusion in Software Engineering, April 20, 2024, Lisbon, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In the extant literature, there has been discussion on the drivers and
motivations of minorities to enter the software industry. For example,
universities have invested in more diverse imagery for years to attract a more
diverse pool of students. However, in our research, we consider whether we
understand why students choose their current major and how they did in the
beginning decided to apply to study software engineering. We were also
interested in learning if there could be some signs that would help us in
marketing to get more women into tech. We approached the topic via an online
survey (N = 78) sent to the university students of software engineering in
Finland. Our results show that, on average, women apply later to software
engineering studies than men, with statistically significant differences
between genders. Additionally, we found that marketing actions have different
impacts based on gender: personal guidance in live events or platforms is most
influential for women, whereas teachers and social media have a more
significant impact on men. The results also indicate two main paths into the
field: the traditional linear educational pathway and the adult career change
pathway, each significantly varying by gender
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17309" title="Abstract">arXiv:2402.17309</a> [<a href="/pdf/2402.17309" title="Download PDF">pdf</a>, <a href="/ps/2402.17309" title="Download PostScript">ps</a>, <a href="/format/2402.17309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotically constant-free and polynomial-degree-robust a posteriori  error estimates for time-harmonic Maxwell&#x27;s equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chaumont-Frelet%2C+T">T. Chaumont-Frelet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a novel a posteriori error estimator for the N\'ed\'elec finite
element discretization of time-harmonic Maxwell's equations. After the
approximation of the electric field is computed, we propose a fully localized
algorithm to reconstruct approximations to the electric displacement and the
magnetic field, with such approximations respectively fulfilling suitable
divergence and curl constraints. These reconstructed fields are in turn used to
construct an a posteriori error estimator which is shown to be reliable and
efficient. Specifically, the estimator controls the error from above up to a
constant that tends to one as the mesh is refined and/or the polynomial degree
is increased, and from below up to constant independent of $p$. Both bounds are
also fully-robust in the low-frequency regime. The properties of the proposed
estimator are illustrated on a set of numerical examples.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17310" title="Abstract">arXiv:2402.17310</a> [<a href="/pdf/2402.17310" title="Download PDF">pdf</a>, <a href="/format/2402.17310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Method of Tracking and Analysis of Fluorescent-Labeled Cells Using  Automatic Thresholding and Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fukasawa%2C+M">Mizuki Fukasawa</a> (1), 
<a href="/search/cs?searchtype=author&query=Fukuda%2C+T">Tomokazu Fukuda</a> (1), 
<a href="/search/cs?searchtype=author&query=Akashi%2C+T">Takuya Akashi</a> (1) ((1) Iwate University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">High-throughput screening using cell images is an efficient method for
screening new candidates for pharmaceutical drugs. To complete the screening
process, it is essential to have an efficient process for analyzing cell
images. This paper presents a new method for efficiently tracking cells and
quantitatively detecting the signal ratio between cytoplasm and nuclei.
Existing methods include those that use image processing techniques and those
that utilize artificial intelligence (AI). However, these methods do not
consider the correspondence of cells between images, or require a significant
amount of new learning data to train AI. Therefore, our method uses automatic
thresholding and labeling algorithms to compare the position of each cell
between images, and continuously measure and analyze the signal ratio of cells.
This paper describes the algorithm of our method. Using the method, we
experimented to investigate the effect of the number of opening and closing
operations during the binarization process on the tracking of the cells.
Through the experiment, we determined the appropriate number of opening and
closing processes.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17311" title="Abstract">arXiv:2402.17311</a> [<a href="/pdf/2402.17311" title="Download PDF">pdf</a>, <a href="/format/2402.17311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=To%2C+H+Q">Huy Quoc To</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+H">Hung-Nghiep Tran</a>, 
<a href="/search/cs?searchtype=author&query=Greiner-Petter%2C+A">Andr&#x27;e Greiner-Petter</a>, 
<a href="/search/cs?searchtype=author&query=Beierle%2C+F">Felix Beierle</a>, 
<a href="/search/cs?searchtype=author&query=Aizawa%2C+A">Akiko Aizawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Summarization for scientific text has shown significant benefits both for the
research community and human society. Given the fact that the nature of
scientific text is distinctive and the input of the multi-document
summarization task is substantially long, the task requires sufficient
embedding generation and text truncation without losing important information.
To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid
framework for multi-document scientific summarization (MDSS). We leverage the
Sentence-Transformer version of Scientific Paper Embeddings using
Citation-Informed Transformers (SPECTER) to encode and represent textual
sentences, allowing for efficient extractive summarization using k-means
clustering. We employ the T5 family of models to generate abstractive summaries
using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on
the Multi-XScience dataset. Through extensive experiments and evaluation, we
showcase the benefits of our model by using less complicated models to achieve
remarkable results, thereby highlighting its potential in advancing the field
of multi-document summarization for scientific text.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17316" title="Abstract">arXiv:2402.17316</a> [<a href="/pdf/2402.17316" title="Download PDF">pdf</a>, <a href="/format/2402.17316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via  Selective Entropy Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yaofo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+S">Shuaicheng Niu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shoukai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Hengjie Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Mingkui Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The conventional deep learning paradigm often involves training a deep model
on a server and then deploying the model or its distilled ones to
resource-limited edge devices. Usually, the models shall remain fixed once
deployed (at least for some period) due to the potential high cost of model
adaptation for both the server and edge sides. However, in many real-world
scenarios, the test environments may change dynamically (known as distribution
shifts), which often results in degraded performance. Thus, one has to adapt
the edge models promptly to attain promising performance. Moreover, with the
increasing data collected at the edge, this paradigm also fails to further
adapt the cloud model for better performance. To address these, we encounter
two primary challenges: 1) the edge model has limited computation power and may
only support forward propagation; 2) the data transmission budget between cloud
and edge devices is limited in latency-sensitive scenarios. In this paper, we
establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the
edge models only need to perform forward propagation and the edge models can be
adapted online. In our CEMA, to reduce the communication burden, we devise two
criteria to exclude unnecessary samples from uploading to the cloud, i.e.,
dynamic unreliable and low-informative sample exclusion. Based on the uploaded
samples, we update and distribute the affine parameters of normalization layers
by distilling from the stronger foundation model to the edge model with a
sample replay strategy. Extensive experimental results on ImageNet-C and
ImageNet-R verify the effectiveness of our CEMA.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17318" title="Abstract">arXiv:2402.17318</a> [<a href="/pdf/2402.17318" title="Download PDF">pdf</a>, <a href="/format/2402.17318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Supervised Local Learning with Augmented Auxiliary Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenxiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jibin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+C">Chenyang Si</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K+C">Kay Chen Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep neural networks are typically trained using global error signals that
backpropagate (BP) end-to-end, which is not only biologically implausible but
also suffers from the update locking problem and requires huge memory
consumption. Local learning, which updates each layer independently with a
gradient-isolated auxiliary network, offers a promising alternative to address
the above problems. However, existing local learning methods are confronted
with a large accuracy gap with the BP counterpart, particularly for large-scale
networks. This is due to the weak coupling between local layers and their
subsequent network layers, as there is no gradient communication across layers.
To tackle this issue, we put forward an augmented local learning method, dubbed
AugLocal. AugLocal constructs each hidden layer's auxiliary network by
uniformly selecting a small subset of layers from its subsequent network layers
to enhance their synergy. We also propose to linearly reduce the depth of
auxiliary networks as the hidden layer goes deeper, ensuring sufficient network
capacity while reducing the computational cost of auxiliary networks. Our
extensive experiments on four image classification datasets (i.e., CIFAR-10,
SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up
to tens of local layers with a comparable accuracy to BP-trained networks while
reducing GPU memory usage by around 40%. The proposed AugLocal method,
therefore, opens up a myriad of opportunities for training high-performance
deep neural networks on resource-constrained platforms.Code is available at
https://github.com/ChenxiangMA/AugLocal.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17319" title="Abstract">arXiv:2402.17319</a> [<a href="/pdf/2402.17319" title="Download PDF">pdf</a>, <a href="/format/2402.17319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to  1st VCL Challenge -- Multi-Task Robustness Track
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zehui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiuchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shanghang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Feng Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this report, we present our solution to the multi-task robustness track of
the 1st Visual Continual Learning (VCL) Challenge at ICCV 2023 Workshop. We
propose a vanilla framework named UniNet that seamlessly combines various
visual perception algorithms into a multi-task model. Specifically, we choose
DETR3D, Mask2Former, and BinsFormer for 3D object detection, instance
segmentation, and depth estimation tasks, respectively. The final submission is
a single model with InternImage-L backbone, and achieves a 49.6 overall score
(29.5 Det mAP, 80.3 mTPS, 46.4 Seg mAP, and 7.93 silog) on SHIFT validation
set. Besides, we provide some interesting observations in our experiments which
may facilitate the development of multi-task learning in dense visual
prediction.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17322" title="Abstract">arXiv:2402.17322</a> [<a href="/pdf/2402.17322" title="Download PDF">pdf</a>, <a href="/format/2402.17322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enclosing Points with Geometric Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+M">Timothy M. Chan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qizheng He</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jie Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In SoCG'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Let $X$ be a set of points in $\mathbb{R}^2$ and $\mathcal{O}$ be a set of
geometric objects in $\mathbb{R}^2$, where $|X| + |\mathcal{O}| = n$. We study
the problem of computing a minimum subset $\mathcal{O}^* \subseteq \mathcal{O}$
that encloses all points in $X$. Here a point $x \in X$ is enclosed by
$\mathcal{O}^*$ if it lies in a bounded connected component of $\mathbb{R}^2
\backslash (\bigcup_{O \in \mathcal{O}^*} O)$. We propose two algorithmic
frameworks to design polynomial-time approximation algorithms for the problem.
The first framework is based on sparsification and min-cut, which results in
$O(1)$-approximation algorithms for unit disks, unit squares, etc. The second
framework is based on LP rounding, which results in an $O(\alpha(n)\log
n)$-approximation algorithm for segments, where $\alpha(n)$ is the inverse
Ackermann function, and an $O(\log n)$-approximation algorithm for disks.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17323" title="Abstract">arXiv:2402.17323</a> [<a href="/pdf/2402.17323" title="Download PDF">pdf</a>, <a href="/format/2402.17323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDDGR: Stable Diffusion-based Deep Generative Replay for Class  Incremental Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hoseong Cho</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jihyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Tiruneh%2C+Y+Y">Yihalem Yimolal Tiruneh</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+S">Seungryul Baek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2024. We will post a camera-ready version later
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the field of class incremental learning (CIL), genera- tive replay has
become increasingly prominent as a method to mitigate the catastrophic
forgetting, alongside the con- tinuous improvements in generative models.
However, its application in class incremental object detection (CIOD) has been
significantly limited, primarily due to the com- plexities of scenes involving
multiple labels. In this paper, we propose a novel approach called stable
diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a
diffusion-based generative model with pre-trained text- to-diffusion networks
to generate realistic and diverse syn- thetic images. SDDGR incorporates an
iterative refinement strategy to produce high-quality images encompassing old
classes. Additionally, we adopt an L2 knowledge distilla- tion technique to
improve the retention of prior knowledge in synthetic images. Furthermore, our
approach includes pseudo-labeling for old objects within new task images, pre-
venting misclassification as background elements. Exten- sive experiments on
the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing
algorithms, achieving a new state-of-the-art in various CIOD scenarios. The
source code will be made available to the public.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17327" title="Abstract">arXiv:2402.17327</a> [<a href="/pdf/2402.17327" title="Download PDF">pdf</a>, <a href="/format/2402.17327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Efficient Learning via Clustering-Based Sensitivity Sampling:  Foundation Models and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Axiotis%2C+K">Kyriakos Axiotis</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Addad%2C+V">Vincent Cohen-Addad</a>, 
<a href="/search/cs?searchtype=author&query=Henzinger%2C+M">Monika Henzinger</a>, 
<a href="/search/cs?searchtype=author&query=Jerome%2C+S">Sammy Jerome</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>, 
<a href="/search/cs?searchtype=author&query=Saulpic%2C+D">David Saulpic</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D">David Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Wunder%2C+M">Michael Wunder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study the data selection problem, whose aim is to select a small
representative subset of data that can be used to efficiently train a machine
learning model. We present a new data selection approach based on $k$-means
clustering and sensitivity sampling. Assuming access to an embedding
representation of the data with respect to which the model loss is H\"older
continuous, our approach provably allows selecting a set of ``typical'' $k +
1/\varepsilon^2$ elements whose average loss corresponds to the average loss of
the whole dataset, up to a multiplicative $(1\pm\varepsilon)$ factor and an
additive $\varepsilon \lambda \Phi_k$, where $\Phi_k$ represents the $k$-means
cost for the input embeddings and $\lambda$ is the H\"older constant.
<br />We furthermore demonstrate the performance and scalability of our approach on
fine-tuning foundation models and show that it outperforms state-of-the-art
methods. We also show how it can be applied on linear regression, leading to a
new sampling strategy that surprisingly matches the performances of leverage
score sampling, while being conceptually simpler and more scalable.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17333" title="Abstract">arXiv:2402.17333</a> [<a href="/pdf/2402.17333" title="Download PDF">pdf</a>, <a href="/format/2402.17333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised multiple choices question answering via universal corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+H">Hao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaojun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meng Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figures, published to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Unsupervised question answering is a promising yet challenging task, which
alleviates the burden of building large-scale annotated data in a new domain.
It motivates us to study the unsupervised multiple-choice question answering
(MCQA) problem. In this paper, we propose a novel framework designed to
generate synthetic MCQA data barely based on contexts from the universal domain
without relying on any form of manual annotation. Possible answers are
extracted and used to produce related questions, then we leverage both named
entities (NE) and knowledge graphs to discover plausible distractors to form
complete synthetic samples. Experiments on multiple MCQA datasets demonstrate
the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17334" title="Abstract">arXiv:2402.17334</a> [<a href="/pdf/2402.17334" title="Download PDF">pdf</a>, <a href="/format/2402.17334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiVRec: Bidirectional View-based Multimodal Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiaxi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jingtong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuehong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+M">Ming He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zitao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The integration of multimodal information into sequential recommender systems
has attracted significant attention in recent research. In the initial stages
of multimodal sequential recommendation models, the mainstream paradigm was
ID-dominant recommendations, wherein multimodal information was fused as side
information. However, due to their limitations in terms of transferability and
information intrusion, another paradigm emerged, wherein multimodal features
were employed directly for recommendation, enabling recommendation across
datasets. Nonetheless, it overlooked user ID information, resulting in low
information utilization and high training costs. To this end, we propose an
innovative framework, BivRec, that jointly trains the recommendation tasks in
both ID and multimodal views, leveraging their synergistic relationship to
enhance recommendation performance bidirectionally. To tackle the information
heterogeneity issue, we first construct structured user interest
representations and then learn the synergistic relationship between them.
Specifically, BivRec comprises three modules: Multi-scale Interest Embedding,
comprehensively modeling user interests by expanding user interaction sequences
with multi-scale patching; Intra-View Interest Decomposition, constructing
highly structured interest representations using carefully designed Gaussian
attention and Cluster attention; and Cross-View Interest Learning, learning the
synergistic relationship between the two recommendation views through
coarse-grained overall semantic similarity and fine-grained interest allocation
similarity BiVRec achieves state-of-the-art performance on five datasets and
showcases various practical advantages.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17336" title="Abstract">arXiv:2402.17336</a> [<a href="/pdf/2402.17336" title="Download PDF">pdf</a>, <a href="/format/2402.17336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outdoor Environment Reconstruction with Deep Learning on Radio  Propagation Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khachatrian%2C+H">Hrant Khachatrian</a>, 
<a href="/search/cs?searchtype=author&query=Mkrtchyan%2C+R">Rafayel Mkrtchyan</a>, 
<a href="/search/cs?searchtype=author&query=Raptis%2C+T+P">Theofanis P. Raptis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Work partly supported by the RA Science Committee grant No. 22rl-052 (DISTAL) and the EU under Italian National Recovery and Resilience Plan of NextGenerationEU on "Telecommunications of the Future" (PE00000001 - program "RESTART")
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Conventional methods for outdoor environment reconstruction rely
predominantly on vision-based techniques like photogrammetry and LiDAR, facing
limitations such as constrained coverage, susceptibility to environmental
conditions, and high computational and energy demands. These challenges are
particularly pronounced in applications like augmented reality navigation,
especially when integrated with wearable devices featuring constrained
computational resources and energy budgets. In response, this paper proposes a
novel approach harnessing ambient wireless signals for outdoor environment
reconstruction. By analyzing radio frequency (RF) data, the paper aims to
deduce the environmental characteristics and digitally reconstruct the outdoor
surroundings. Investigating the efficacy of selected deep learning (DL)
techniques on the synthetic RF dataset WAIR-D, the study endeavors to address
the research gap in this domain. Two DL-driven approaches are evaluated
(convolutional U-Net and CLIP+ based on vision transformers), with performance
assessed using metrics like intersection-over-union (IoU), Hausdorff distance,
and Chamfer distance. The results demonstrate promising performance of the
RF-based reconstruction method, paving the way towards lightweight and scalable
reconstruction solutions.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17337" title="Abstract">arXiv:2402.17337</a> [<a href="/pdf/2402.17337" title="Download PDF">pdf</a>, <a href="/format/2402.17337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Massive parallelization and performance enhancement of an immersed  boundary method based unsteady flow solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sundar%2C+R">Rahul Sundar</a>, 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+D">Dipanjan Majumdar</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+C+L">Chhote Lal Shah</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Sunetra Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">High-fidelity simulations of unsteady fluid flow are now possible with
advancements in high-performance computing hardware and software frameworks.
Since computational fluid dynamics (CFD) computations are dominated by linear
algebraic routines, they can be significantly accelerated through massive
parallelization on graphics processing units (GPUs). Thus, GPU implementation
of high-fidelity CFD solvers is essential in reducing the turnaround time for
quicker design space exploration. In the present work, an immersed boundary
method (IBM) based in-house flow solver has been ported to the GPU using
OpenACC, a compiler directive-based heterogeneous parallel programming
framework. Out of various GPU porting pathways available, OpenACC was chosen
because of its minimum code intrusion, low development time, and striking
similarity with OpenMP, a similar directive-based shared memory programming
framework. A detailed validation study and performance analysis of the parallel
solver implementations on the CPU and GPU are presented. The GPU implementation
shows a speedup up to the order $O(10)$ over the CPU parallel version and up to
the order $O(10^2)$ over the serial code. The GPU implementation also scales
well with increasing mesh size owing to the efficient utilization of the GPU
processor cores.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17339" title="Abstract">arXiv:2402.17339</a> [<a href="/pdf/2402.17339" title="Download PDF">pdf</a>, <a href="/format/2402.17339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned  Latents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+W">Wei Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Haoteng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaogang Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pedestrian trajectory prediction is the key technology in many applications
for providing insights into human behavior and anticipating human future
motions. Most existing empirical models are explicitly formulated by observed
human behaviors using explicable mathematical terms with a deterministic
nature, while recent work has focused on developing hybrid models combined with
learning-based techniques for powerful expressiveness while maintaining
explainability. However, the deterministic nature of the learned steering
behaviors from the empirical models limits the models' practical performance.
To address this issue, this work proposes the social conditional variational
autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs
a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE
learns socially reasonable motion randomness by utilizing a socially
explainable interaction energy map as the CVAE's condition, which illustrates
the future occupancy of each pedestrian's local neighborhood area. The energy
map is generated using an energy-based interaction model, which anticipates the
energy cost (i.e., repulsion intensity) of pedestrians' interactions with
neighbors. Experimental results on two public benchmarks including 25 scenes
demonstrate that SocialCVAE significantly improves prediction accuracy compared
with the state-of-the-art methods, with up to 16.85% improvement in Average
Displacement Error (ADE) and 69.18% improvement in Final Displacement Error
(FDE).
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17342" title="Abstract">arXiv:2402.17342</a> [<a href="/pdf/2402.17342" title="Download PDF">pdf</a>, <a href="/format/2402.17342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scalable Multi-Layered Blockchain Architecture for Enhanced EHR  Sharing and Drug Supply Chain Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javan%2C+R">Reza Javan</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+M">Mehrzad Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Beheshti-Atashgah%2C+M">Mohammad Beheshti-Atashgah</a>, 
<a href="/search/cs?searchtype=author&query=Aref%2C+M+R">Mohammad Reza Aref</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In recent years, the healthcare sector's shift to online platforms has
spotlighted challenges concerning data security, privacy, and scalability.
Blockchain technology, known for its decentralized, secure, and immutable
nature, emerges as a viable solution for these pressing issues. This article
presents an innovative Electronic Health Records (EHR) sharing and drug supply
chain management framework tailored to address scalability, security, data
integrity, traceability, and secure data sharing. The framework introduces five
layers and transactions, prioritizing patient-centric healthcare by granting
patients comprehensive access control over their health information. This
access facilitates smoother processes, such as insurance claims, while
maintaining robust security measures. Notably, our implementation of
parallelism significantly bolsters scalability and transaction throughput while
minimizing network traffic. Performance evaluations conducted through the
Caliper benchmark indicate a slight increase in processor consumption during
specific transactions, mitigated effectively by parallelization. RAM
requirements remain largely stable. Additionally, our approach notably reduces
network traffic while tripling transaction throughput. The framework ensures
patient privacy, data integrity, access control, and interoperability, aligning
with traditional healthcare systems. Moreover, it provides transparency and
real-time drug supply monitoring, empowering decision-makers with actionable
insights. As healthcare evolves, our framework sets a crucial precedent for
innovative, scalable, and secure systems. Future enhancements could focus on
scalability, real-world deployment, standardized data formats, reinforced
security protocols, privacy preservation, and IoT integration to comply with
regulations and meet evolving industry needs.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17343" title="Abstract">arXiv:2402.17343</a> [<a href="/pdf/2402.17343" title="Download PDF">pdf</a>, <a href="/format/2402.17343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Bayesian Optimization via Preferential Modeling of Abstract  Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=V%2C+A+K+A">Arun Kumar A V</a>, 
<a href="/search/cs?searchtype=author&query=Shilton%2C+A">Alistair Shilton</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Sunil Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+S">Santu Rana</a>, 
<a href="/search/cs?searchtype=author&query=Greenhill%2C+S">Stewart Greenhill</a>, 
<a href="/search/cs?searchtype=author&query=Venkatesh%2C+S">Svetha Venkatesh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 Pages, 6 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Experimental (design) optimization is a key driver in designing and
discovering new products and processes. Bayesian Optimization (BO) is an
effective tool for optimizing expensive and black-box experimental design
processes. While Bayesian optimization is a principled data-driven approach to
experimental optimization, it learns everything from scratch and could greatly
benefit from the expertise of its human (domain) experts who often reason about
systems at different abstraction levels using physical properties that are not
necessarily directly measured (or measurable). In this paper, we propose a
human-AI collaborative Bayesian framework to incorporate expert preferences
about unmeasured abstract properties into the surrogate modeling to further
boost the performance of BO. We provide an efficient strategy that can also
handle any incorrect/misleading expert bias in preferential judgments. We
discuss the convergence behavior of our proposed framework. Our experimental
results involving synthetic functions and real-world datasets show the
superiority of our method against the baselines.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17345" title="Abstract">arXiv:2402.17345</a> [<a href="/pdf/2402.17345" title="Download PDF">pdf</a>, <a href="/format/2402.17345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LocalGCL: Local-aware Contrastive Learning for Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haojun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiawei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chentao Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph representation learning (GRL) makes considerable progress recently,
which encodes graphs with topological structures into low-dimensional
embeddings. Meanwhile, the time-consuming and costly process of annotating
graph labels manually prompts the growth of self-supervised learning (SSL)
techniques. As a dominant approach of SSL, Contrastive learning (CL) learns
discriminative representations by differentiating between positive and negative
samples. However, when applied to graph data, it overemphasizes global patterns
while neglecting local structures. To tackle the above issue, we propose
\underline{Local}-aware \underline{G}raph \underline{C}ontrastive
\underline{L}earning (\textbf{\methnametrim}), a self-supervised learning
framework that supplementarily captures local graph information with
masking-based modeling compared with vanilla contrastive learning. Extensive
experiments validate the superiority of \methname against state-of-the-art
methods, demonstrating its promise as a comprehensive graph representation
learner.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17350" title="Abstract">arXiv:2402.17350</a> [<a href="/pdf/2402.17350" title="Download PDF">pdf</a>, <a href="/ps/2402.17350" title="Download PostScript">ps</a>, <a href="/format/2402.17350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards an Enforceable GDPR Specification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hublet%2C+F">Fran&#xe7;ois Hublet</a>, 
<a href="/search/cs?searchtype=author&query=Kvamme%2C+A">Alexander Kvamme</a>, 
<a href="/search/cs?searchtype=author&query=Krsti%C4%87%2C+S">Sr&#x111;an Krsti&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">While Privacy by Design (PbD) is prescribed by modern privacy regulations
such as the EU's GDPR, achieving PbD in real software systems is a notoriously
difficult task. One emerging technique to realize PbD is Runtime enforcement
(RE), in which an enforcer, loaded with a specification of a system's privacy
requirements, observes the actions performed by the system and instructs it to
perform actions that will ensure compliance with these requirements at all
times. To be able to use RE techniques for PbD, privacy regulations first need
to be translated into an enforceable specification. In this paper, we report on
our ongoing work in formalizing the GDPR. We first present a set of
requirements and an iterative methodology for creating enforceable formal
specifications of legal provisions. Then, we report on a preliminary case study
in which we used our methodology to derive an enforceable specification of part
of the GDPR. Our case study suggests that our methodology can be effectively
used to develop accurate enforceable specifications.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17351" title="Abstract">arXiv:2402.17351</a> [<a href="/pdf/2402.17351" title="Download PDF">pdf</a>, <a href="/format/2402.17351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICP-Flow: LiDAR Scene Flow Estimation with ICP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yancong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Caesar%2C+H">Holger Caesar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scene flow characterizes the 3D motion between two LiDAR scans captured by an
autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow
as point-wise unconstrained flow vectors that can be learned by either
large-scale training beforehand or time-consuming optimization at inference.
However, these methods do not take into account that objects in autonomous
driving often move rigidly. We incorporate this rigid-motion assumption into
our design, where the goal is to associate objects over scans and then estimate
the locally rigid transformations. We propose ICP-Flow, a learning-free flow
estimator. The core of our design is the conventional Iterative Closest Point
(ICP) algorithm, which aligns the objects over time and outputs the
corresponding rigid transformations. Crucially, to aid ICP, we propose a
histogram-based initialization that discovers the most likely translation, thus
providing a good starting point for ICP. The complete scene flow is then
recovered from the rigid transformations. We outperform state-of-the-art
baselines, including supervised models, on the Waymo dataset and perform
competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward
neural network, supervised by the pseudo labels from our model, and achieve top
performance among all models capable of real-time inference. We validate the
advantage of our model on scene flow estimation with longer temporal gaps, up
to 0.5 seconds where other models fail to deliver meaningful results.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17355" title="Abstract">arXiv:2402.17355</a> [<a href="/pdf/2402.17355" title="Download PDF">pdf</a>, <a href="/format/2402.17355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RECOST: External Knowledge Guided Data-efficient Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haobo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junbo Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the current landscape of large language models (LLMs), the process of
instruction tuning serves as an essential step. Considering the high computing
power overhead, data-efficient instruction tuning was proposed to reduce the
training data size in this process, aiming at selecting high-quality
instructional data. Nevertheless, we argue that most current data-efficient
instruction-tuning methods are highly dependent on the quality of the original
instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a
common scenario in this field, dirty samples will even be selected with a
higher probability than other samples. To address these challenges, we utilized
external knowledge (relevant examples or paragraphs) to evaluate those samples
synthesized by LLMs with an in-context-based relative predictive entropy. Based
on the new metric, we proposed a framework, dubbed as \textbf{RECOST}, which
integrates external-knowledge-base re-ranking and diversity-consistent sampling
into a single pipeline. Through extensive experiments on several synthetic
datasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our
method and achieve even better results with only \textbf{1\%} of the full
dataset.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17357" title="Abstract">arXiv:2402.17357</a> [<a href="/pdf/2402.17357" title="Download PDF">pdf</a>, <a href="/ps/2402.17357" title="Download PostScript">ps</a>, <a href="/format/2402.17357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A robust parameterized enhanced shift-splitting preconditioner for  three-by-three block saddle point problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ahmad%2C+S+S">Sk. Safique Ahmad</a>, 
<a href="/search/math?searchtype=author&query=Khatun%2C+P">Pinki Khatun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper proposes a new parameterized enhanced shift-splitting {\it (PESS)}
preconditioner to solve the three-by-three block saddle point problem ({\it
SPP}). In addition, necessary and sufficient criteria are established for the
convergence of the proposed {\it PESS} iterative process for any random initial
guess. Furthermore, we meticulously investigate the spectral bounds of the {\it
PESS} preconditioned matrix. Moreover, empirical investigation has been
performed for the sensitivity analysis of the system, revealing the robustness
of the proposed {\it PESS} preconditioner. Numerical experiments are carried
out to demonstrate the enhanced efficiency and robustness of the proposed {\it
PESS} preconditioner compared to the existing block diagonal and
shift-splitting preconditioners.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17358" title="Abstract">arXiv:2402.17358</a> [<a href="/pdf/2402.17358" title="Download PDF">pdf</a>, <a href="/format/2402.17358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoFA: Shielded On-the-fly Alignment via Priority Rule Following
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xinyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bowen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yaojie Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hongyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Le Sun</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xianpei Han</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The alignment problem in Large Language Models (LLMs) involves adapting them
to the broad spectrum of human values. This requirement challenges existing
alignment methods due to diversity of preferences and regulatory standards.
This paper introduces a novel alignment paradigm, priority rule following,
which defines rules as the primary control mechanism in each dialog,
prioritizing them over user instructions. Our preliminary analysis reveals that
even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding
and prioritizing the rules. Therefore, we present PriorityDistill, a
semi-automated approach for distilling priority following signals from LLM
simulations to ensure robust rule integration and adherence. Our experiments
show that this method not only effectively minimizes misalignments utilizing
only one general rule but also adapts smoothly to various unseen rules,
ensuring they are shielded from hijacking and that the model responds
appropriately.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17360" title="Abstract">arXiv:2402.17360</a> [<a href="/pdf/2402.17360" title="Download PDF">pdf</a>, <a href="/format/2402.17360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAPT: Category-level Articulation Estimation from a Single Point Cloud  Using Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Lian Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+R">Ryoichi Ishikawa</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+Y">Yoshihiro Sato</a>, 
<a href="/search/cs?searchtype=author&query=Oishi%2C+T">Takeshi Oishi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">The ability to estimate joint parameters is essential for various
applications in robotics and computer vision. In this paper, we propose CAPT:
category-level articulation estimation from a point cloud using Transformer.
CAPT uses an end-to-end transformer-based architecture for joint parameter and
state estimation of articulated objects from a single point cloud. The proposed
CAPT methods accurately estimate joint parameters and states for various
articulated objects with high precision and robustness. The paper also
introduces a motion loss approach, which improves articulation estimation
performance by emphasizing the dynamic features of articulated objects.
Additionally, the paper presents a double voting strategy to provide the
framework with coarse-to-fine parameter estimation. Experimental results on
several category datasets demonstrate that our methods outperform existing
alternatives for articulation estimation. Our research provides a promising
solution for applying Transformer-based architectures in articulated object
analysis.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17363" title="Abstract">arXiv:2402.17363</a> [<a href="/pdf/2402.17363" title="Download PDF">pdf</a>, <a href="/format/2402.17363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CGGM: A conditional graph generation model with adaptive sparsity for  node anomaly detection in IoT networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xianshi Su</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Munan Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Tongbang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+H">Hao Long</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Dynamic graphs are extensively employed for detecting anomalous behavior in
nodes within the Internet of Things (IoT). Generative models are often used to
address the issue of imbalanced node categories in dynamic graphs.
Nevertheless, the constraints it faces include the monotonicity of adjacency
relationships, the difficulty in constructing multi-dimensional features for
nodes, and the lack of a method for end-to-end generation of multiple
categories of nodes. This paper presents a novel graph generation model, called
CGGM, designed specifically to generate a larger number of nodes belonging to
the minority class. The mechanism for generating an adjacency matrix, through
adaptive sparsity, enhances flexibility in its structure. The feature
generation module, called multidimensional features generator (MFG) to generate
node features along with topological information. Labels are transformed into
embedding vectors, serving as conditional constraints to control the generation
of synthetic data across multiple categories. Using a multi-stage loss, the
distribution of synthetic data is adjusted to closely resemble that of real
data. In extensive experiments, we show that CGGM's synthetic data outperforms
state-of-the-art methods across various metrics. Our results demonstrate
efficient generation of diverse data categories, robustly enhancing
multi-category classification model performance.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17364" title="Abstract">arXiv:2402.17364</a> [<a href="/pdf/2402.17364" title="Download PDF">pdf</a>, <a href="/format/2402.17364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruobing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Congying Han</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tiande Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent works in implicit representations, such as Neural Radiance Fields
(NeRF), have advanced the generation of realistic and animatable head avatars
from video sequences. These implicit methods are still confronted by visual
artifacts and jitters, since the lack of explicit geometric constraints poses a
fundamental challenge in accurately modeling complex facial deformations. In
this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid
representation that encodes explicit dynamic meshes by neural networks to
ensure geometric consistency across various motions and viewpoints. DynTet is
parameterized by the coordinate-based networks which learn signed distance,
deformation, and material texture, anchoring the training data into a
predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently
decodes textured meshes with a consistent topology, enabling fast rendering
through a differentiable rasterizer and supervision via a pixel loss. To
enhance training efficiency, we incorporate classical 3D Morphable Models to
facilitate geometry learning and define a canonical space for simplifying
texture learning. These advantages are readily achievable owing to the
effective geometric representation employed in DynTet. Compared with prior
works, DynTet demonstrates significant improvements in fidelity, lip
synchronization, and real-time performance according to various metrics. Beyond
producing stable and visually appealing synthesis videos, our method also
outputs the dynamic meshes which is promising to enable many emerging
applications.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17369" title="Abstract">arXiv:2402.17369</a> [<a href="/pdf/2402.17369" title="Download PDF">pdf</a>, <a href="/ps/2402.17369" title="Download PostScript">ps</a>, <a href="/format/2402.17369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Functions of Symmetric Hierarchically Semiseparable Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Casulli%2C+A+A">Angelo A. Casulli</a>, 
<a href="/search/math?searchtype=author&query=Kressner%2C+D">Daniel Kressner</a>, 
<a href="/search/math?searchtype=author&query=Robol%2C+L">Leonardo Robol</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The aim of this work is to develop a fast algorithm for approximating the
matrix function $f(A)$ of a square matrix $A$ that is symmetric and has
hierarchically semiseparable (HSS) structure. Appearing in a wide variety of
applications, often in the context of discretized (fractional) differential and
integral operators, HSS matrices have a number of attractive properties
facilitating the development of fast algorithms. In this work, we use an
unconventional telescopic decomposition of $A$, inspired by recent work of
Levitt and Martinsson on approximating an HSS matrix from matrix-vector
products with a few random vectors. This telescopic decomposition allows us to
approximate $f(A)$ by recursively performing low-rank updates with rational
Krylov subspaces while keeping the size of the matrices involved in the
rational Krylov subspaces small. In particular, no large-scale linear system
needs to be solved, which yields favorable complexity estimates and reduced
execution times compared to existing methods, including an existing
divide-and-conquer strategy. The advantages of our newly proposed algorithms
are demonstrated for a number of examples from the literature, featuring the
exponential, the inverse square root, and the sign function of a matrix. Even
for matrix inversion, our algorithm exhibits superior performance, even if not
specifically designed for this task.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17370" title="Abstract">arXiv:2402.17370</a> [<a href="/pdf/2402.17370" title="Download PDF">pdf</a>, <a href="/format/2402.17370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient MLP-based Point-guided Segmentation Network for Ore Images  with Ambiguous Boundary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guodong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yuting Peng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Le Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengya Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">An Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hongliang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The precise segmentation of ore images is critical to the successful
execution of the beneficiation process. Due to the homogeneous appearance of
the ores, which leads to low contrast and unclear boundaries, accurate
segmentation becomes challenging, and recognition becomes problematic. This
paper proposes a lightweight framework based on Multi-Layer Perceptron (MLP),
which focuses on solving the problem of edge burring. Specifically, we
introduce a lightweight backbone better suited for efficiently extracting
low-level features. Besides, we design a feature pyramid network consisting of
two MLP structures that balance local and global information thus enhancing
detection accuracy. Furthermore, we propose a novel loss function that guides
the prediction points to match the instance edge points to achieve clear object
boundaries. We have conducted extensive experiments to validate the efficacy of
our proposed method. Our approach achieves a remarkable processing speed of
over 27 frames per second (FPS) with a model size of only 73 MB. Moreover, our
method delivers a consistently high level of accuracy, with impressive
performance scores of 60.4 and 48.9 in~$AP_{50}^{box}$ and~$AP_{50}^{mask}$
respectively, as compared to the currently available state-of-the-art
techniques, when tested on the ore image dataset. The source code will be
released at \url{https://github.com/MVME-HBUT/ORENEXT}.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17371" title="Abstract">arXiv:2402.17371</a> [<a href="/pdf/2402.17371" title="Download PDF">pdf</a>, <a href="/format/2402.17371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toker%2C+M">Michael Toker</a>, 
<a href="/search/cs?searchtype=author&query=Mishali%2C+O">Oren Mishali</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCnz-Manor%2C+O">Ophir M&#xfc;nz-Manor</a>, 
<a href="/search/cs?searchtype=author&query=Kimelfeld%2C+B">Benny Kimelfeld</a>, 
<a href="/search/cs?searchtype=author&query=Belinkov%2C+Y">Yonatan Belinkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024. Project webpage: <a href="https://tokeron.github.io/metaphor/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">There is a large volume of late antique and medieval Hebrew texts. They
represent a crucial linguistic and cultural bridge between Biblical and modern
Hebrew. Poetry is prominent in these texts and one of its main haracteristics
is the frequent use of metaphor. Distinguishing figurative and literal language
use is a major task for scholars of the Humanities, especially in the fields of
literature, linguistics, and hermeneutics. This paper presents a new,
challenging dataset of late antique and medieval Hebrew poetry with expert
annotations of metaphor, as well as some baseline results, which we hope will
facilitate further research in this area.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17372" title="Abstract">arXiv:2402.17372</a> [<a href="/pdf/2402.17372" title="Download PDF">pdf</a>, <a href="/format/2402.17372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bastico%2C+M">Matteo Bastico</a>, 
<a href="/search/cs?searchtype=author&query=Decenci%C3%A8re%2C+E">Etienne Decenci&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Cort%C3%A9%2C+L">Laurent Cort&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Tillier%2C+Y">Yannick Tillier</a>, 
<a href="/search/cs?searchtype=author&query=Ryckelynck%2C+D">David Ryckelynck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted at Computer Vision and Patter Recognition (CVPR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Point cloud matching, a crucial technique in computer vision, medical and
robotics fields, is primarily concerned with finding correspondences between
pairs of point clouds or voxels. In some practical scenarios, emphasizing local
differences is crucial for accurately identifying a correct match, thereby
enhancing the overall robustness and reliability of the matching process.
Commonly used shape descriptors have several limitations and often fail to
provide meaningful local insights on the paired geometries. In this work, we
propose a new technique, based on graph Laplacian eigenmaps, to match point
clouds by taking into account fine local structures. To deal with the order and
sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called
Coupled Laplacian, that allows to easily generate aligned eigenspaces for
multiple rigidly-registered geometries. We show that the similarity between
those aligned high-dimensional spaces provides a locally meaningful score to
match shapes. We initially evaluate the performance of the proposed technique
in a point-wise manner, specifically focusing on the task of object anomaly
localization using the MVTec 3D-AD dataset. Additionally, we define a new
medical task, called automatic Bone Side Estimation (BSE), which we address
through a global similarity score derived from coupled eigenspaces. In order to
test it, we propose a benchmark collecting bone surface structures from various
public datasets. Our matching technique, based on Coupled Laplacian,
outperforms other methods by reaching an impressive accuracy on both tasks. The
code to reproduce our experiments is publicly available at
https://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary
Code.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17375" title="Abstract">arXiv:2402.17375</a> [<a href="/pdf/2402.17375" title="Download PDF">pdf</a>, <a href="/format/2402.17375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Computation in Integral Reinforcement Learning for  Continuous-Time Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cao%2C+W">Wenhan Cao</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+W">Wei Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Integral reinforcement learning (IntRL) demands the precise computation of
the utility function's integral at its policy evaluation (PEV) stage. This is
achieved through quadrature rules, which are weighted sums of utility functions
evaluated from state samples obtained in discrete time. Our research reveals a
critical yet underexplored phenomenon: the choice of the computational method
-- in this case, the quadrature rule -- can significantly impact control
performance. This impact is traced back to the fact that computational errors
introduced in the PEV stage can affect the policy iteration's convergence
behavior, which in turn affects the learned controller. To elucidate how
computation impacts control, we draw a parallel between IntRL's policy
iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation.
In this light, computational error in PEV manifests as an extra error term in
each iteration of Newton's method, with its upper bound proportional to the
computational error. Further, we demonstrate that when the utility function
resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is
achievable by employing Bayesian quadrature with the RKHS-inducing kernel
function. We prove that the local convergence rates for IntRL using the
trapezoidal rule and Bayesian quadrature with a Mat\'ern kernel to be
$O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples
and $b$ is the Mat\'ern kernel's smoothness parameter. These theoretical
findings are finally validated by two canonical control tasks.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17376" title="Abstract">arXiv:2402.17376</a> [<a href="/pdf/2402.17376" title="Download PDF">pdf</a>, <a href="/format/2402.17376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Diffusion Sampling with Optimized Time Steps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Shuchen Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaoqiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shifeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2024. Under camera-ready revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion probabilistic models (DPMs) have shown remarkable performance in
high-resolution image synthesis, but their sampling efficiency is still to be
desired due to the typically large number of sampling steps. Recent
advancements in high-order numerical ODE solvers for DPMs have enabled the
generation of high-quality images with much fewer sampling steps. While this is
a significant development, most sampling methods still employ uniform time
steps, which is not optimal when using a small number of steps. To address this
issue, we propose a general framework for designing an optimization problem
that seeks more appropriate time steps for a specific numerical ODE solver for
DPMs. This optimization problem aims to minimize the distance between the
ground-truth solution to the ODE and an approximate solution corresponding to
the numerical solver. It can be efficiently solved using the constrained trust
region method, taking less than $15$ seconds. Our extensive experiments on both
unconditional and conditional sampling using pixel- and latent-space DPMs
demonstrate that, when combined with the state-of-the-art sampling method
UniPC, our optimized time steps significantly improve image generation
performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet,
compared to using uniform time steps.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17377" title="Abstract">arXiv:2402.17377</a> [<a href="/pdf/2402.17377" title="Download PDF">pdf</a>, <a href="/ps/2402.17377" title="Download PostScript">ps</a>, <a href="/format/2402.17377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KoDialogBench: Evaluating Conversational Understanding of Language  Models with Korean Dialogue Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+S">Seongbo Jang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seonghyeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hwanjo Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As language models are often deployed as chatbot assistants, it becomes a
virtue for models to engage in conversations in a user's first language. While
these models are trained on a wide range of languages, a comprehensive
evaluation of their proficiency in low-resource languages such as Korean has
been lacking. In this work, we introduce KoDialogBench, a benchmark designed to
assess language models' conversational capabilities in Korean. To this end, we
collect native Korean dialogues on daily topics from public sources, or
translate dialogues from other languages. We then structure these conversations
into diverse test datasets, spanning from dialogue comprehension to response
selection tasks. Leveraging the proposed benchmark, we conduct extensive
evaluations and analyses of various language models to measure a foundational
understanding of Korean dialogues. Experimental results indicate that there
exists significant room for improvement in models' conversation skills.
Furthermore, our in-depth comparisons across different language models
highlight the effectiveness of recent training techniques in enhancing
conversational proficiency. We anticipate that KoDialogBench will promote the
progress towards conversation-aware Korean language models.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17385" title="Abstract">arXiv:2402.17385</a> [<a href="/pdf/2402.17385" title="Download PDF">pdf</a>, <a href="/format/2402.17385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Determinants of LLM-assisted Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eigner%2C+E">Eva Eigner</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4ndler%2C+T">Thorsten H&#xe4;ndler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Decision-making is a fundamental capability in everyday life. Large Language
Models (LLMs) provide multifaceted support in enhancing human decision-making
processes. However, understanding the influencing factors of LLM-assisted
decision-making is crucial for enabling individuals to utilize LLM-provided
advantages and minimize associated risks in order to make more informed and
better decisions. This study presents the results of a comprehensive literature
analysis, providing a structural overview and detailed analysis of determinants
impacting decision-making with LLM support. In particular, we explore the
effects of technological aspects of LLMs, including transparency and prompt
engineering, psychological factors such as emotions and decision-making styles,
as well as decision-specific determinants such as task difficulty and
accountability. In addition, the impact of the determinants on the
decision-making process is illustrated via multiple application scenarios.
Drawing from our analysis, we develop a dependency framework that systematizes
possible interactions in terms of reciprocal interdependencies between these
determinants. Our research reveals that, due to the multifaceted interactions
with various determinants, factors such as trust in or reliance on LLMs, the
user's mental model, and the characteristics of information processing are
identified as significant aspects influencing LLM-assisted decision-making
processes. Our findings can be seen as crucial for improving decision quality
in human-AI collaboration, empowering both users and organizations, and
designing more effective LLM interfaces. Additionally, our work provides a
foundation for future empirical investigations on the determinants of
decision-making assisted by LLMs.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17387" title="Abstract">arXiv:2402.17387</a> [<a href="/pdf/2402.17387" title="Download PDF">pdf</a>, <a href="/format/2402.17387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RACP: Risk-Aware Contingency Planning with Multi-Modal Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+K+A">Khaled A. Mustafa</a>, 
<a href="/search/cs?searchtype=author&query=Ornia%2C+D+J">Daniel Jarne Ornia</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">For an autonomous vehicle to operate reliably within real-world traffic
scenarios, it is imperative to assess the repercussions of its prospective
actions by anticipating the uncertain intentions exhibited by other
participants in the traffic environment. Driven by the pronounced multi-modal
nature of human driving behavior, this paper presents an approach that
leverages Bayesian beliefs over the distribution of potential policies of other
road users to construct a novel risk-aware probabilistic motion planning
framework. In particular, we propose a novel contingency planner that outputs
long-term contingent plans conditioned on multiple possible intents for other
actors in the traffic scene. The Bayesian belief is incorporated into the
optimization cost function to influence the behavior of the short-term plan
based on the likelihood of other agents' policies. Furthermore, a probabilistic
risk metric is employed to fine-tune the balance between efficiency and
robustness. Through a series of closed-loop safety-critical simulated traffic
scenarios shared with human-driven vehicles, we demonstrate the practical
efficacy of our proposed approach that can handle multi-vehicle scenarios.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17389" title="Abstract">arXiv:2402.17389</a> [<a href="/pdf/2402.17389" title="Download PDF">pdf</a>, <a href="/format/2402.17389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairBelief - Assessing Harmful Beliefs in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Setzu%2C+M">Mattia Setzu</a>, 
<a href="/search/cs?searchtype=author&query=Manerba%2C+M+M">Marta Marchiori Manerba</a>, 
<a href="/search/cs?searchtype=author&query=Minervini%2C+P">Pasquale Minervini</a>, 
<a href="/search/cs?searchtype=author&query=Nozza%2C+D">Debora Nozza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language Models (LMs) have been shown to inherit undesired biases that might
hurt minorities and underrepresented groups if such systems were integrated
into real-world applications without careful fairness auditing. This paper
proposes FairBelief, an analytical approach to capture and assess beliefs,
i.e., propositions that an LM may embed with different degrees of confidence
and that covertly influence its predictions. With FairBelief, we leverage
prompting to study the behavior of several state-of-the-art LMs across
different previously neglected axes, such as model scale and likelihood,
assessing predictions on a fairness dataset specifically designed to quantify
LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative
assessment of the beliefs emitted by the models. We apply FairBelief to English
LMs, revealing that, although these architectures enable high performances on
diverse natural language processing tasks, they show hurtful beliefs about
specific genders. Interestingly, training procedure and dataset, model scale,
and architecture induce beliefs of different degrees of hurtfulness.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17390" title="Abstract">arXiv:2402.17390</a> [<a href="/pdf/2402.17390" title="Download PDF">pdf</a>, <a href="/format/2402.17390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness-Congruent Adversarial Training for Secure Machine Learning  Model Updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Angioni%2C+D">Daniele Angioni</a>, 
<a href="/search/cs?searchtype=author&query=Demetrio%2C+L">Luca Demetrio</a>, 
<a href="/search/cs?searchtype=author&query=Pintor%2C+M">Maura Pintor</a>, 
<a href="/search/cs?searchtype=author&query=Oneto%2C+L">Luca Oneto</a>, 
<a href="/search/cs?searchtype=author&query=Anguita%2C+D">Davide Anguita</a>, 
<a href="/search/cs?searchtype=author&query=Biggio%2C+B">Battista Biggio</a>, 
<a href="/search/cs?searchtype=author&query=Roli%2C+F">Fabio Roli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Machine-learning models demand for periodic updates to improve their average
accuracy, exploiting novel architectures and additional data. However, a
newly-updated model may commit mistakes that the previous model did not make.
Such misclassifications are referred to as negative flips, and experienced by
users as a regression of performance. In this work, we show that this problem
also affects robustness to adversarial examples, thereby hindering the
development of secure model update practices. In particular, when updating a
model to improve its adversarial robustness, some previously-ineffective
adversarial examples may become misclassified, causing a regression in the
perceived security of the system. We propose a novel technique, named
robustness-congruent adversarial training, to address this issue. It amounts to
fine-tuning a model with adversarial training, while constraining it to retain
higher robustness on the adversarial examples that were correctly classified
before the update. We show that our algorithm and, more generally, learning
with non-regression constraints, provides a theoretically-grounded framework to
train consistent estimators. Our experiments on robust models for computer
vision confirm that (i) both accuracy and robustness, even if improved after
model update, can be affected by negative flips, and (ii) our
robustness-congruent adversarial training can mitigate the problem,
outperforming competing baseline methods.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17392" title="Abstract">arXiv:2402.17392</a> [<a href="/pdf/2402.17392" title="Download PDF">pdf</a>, <a href="/format/2402.17392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and  Humans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gromov%2C+V+A">Vasilii A. Gromov</a>, 
<a href="/search/cs?searchtype=author&query=Kogan%2C+A+S">Alexandra S. Kogan</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Gromov V.A., Kogan A.S. Spot the Bot: Coarse-Grained Partition of
  Semantic Paths for Bots and Humans // Pattern Recognition and Machine
  Intelligence, 2023. pp. 348--355
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Nowadays, technology is rapidly advancing: bots are writing comments,
articles, and reviews. Due to this fact, it is crucial to know if the text was
written by a human or by a bot. This paper focuses on comparing structures of
the coarse-grained partitions of semantic paths for human-written and
bot-generated texts. We compare the clusterizations of datasets of n-grams from
literary texts and texts generated by several bots. The hypothesis is that the
structures and clusterizations are different. Our research supports the
hypothesis. As the semantic structure may be different for different languages,
we investigate Russian, English, German, and Vietnamese languages.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17393" title="Abstract">arXiv:2402.17393</a> [<a href="/pdf/2402.17393" title="Download PDF">pdf</a>, <a href="/ps/2402.17393" title="Download PostScript">ps</a>, <a href="/format/2402.17393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing Chatbots to Support Victims and Survivors of Domestic Abuse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saglam%2C+R+B">Rahime Belen Saglam</a>, 
<a href="/search/cs?searchtype=author&query=Nurse%2C+J+R+C">Jason R. C. Nurse</a>, 
<a href="/search/cs?searchtype=author&query=Sugiura%2C+L">Lisa Sugiura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Objective: Domestic abuse cases have risen significantly over the last four
years, in part due to the COVID-19 pandemic and the challenges for victims and
survivors in accessing support. In this study, we investigate the role that
chatbots - Artificial Intelligence (AI) and rule-based - may play in supporting
victims/survivors in situations such as these or where direct access to help is
limited. Methods: Interviews were conducted with experts working in domestic
abuse support services and organizations (e.g., charities, law enforcement) and
the content of websites of related support-service providers was collected.
Thematic content analysis was then applied to assess and extract insights from
the interview data and the content on victim-support websites. We also reviewed
pertinent chatbot literature to reflect on studies that may inform design
principles and interaction patterns for agents used to support
victims/survivors. Results: From our analysis, we outlined a set of design
considerations/practices for chatbots that consider potential use cases and
target groups, dialog structure, personality traits that might be useful for
chatbots to possess, and finally, safety and privacy issues that should be
addressed. Of particular note are situations where AI systems (e.g., ChatGPT,
CoPilot, Gemini) are not recommended for use, the value of conveying emotional
support, the importance of transparency, and the need for a safe and
confidential space. Conclusion: It is our hope that these
considerations/practices will stimulate debate among chatbots and AI developers
and service providers and - for situations where chatbots are deemed
appropriate for use - inspire efficient use of chatbots in the support of
survivors of domestic abuse.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17394" title="Abstract">arXiv:2402.17394</a> [<a href="/pdf/2402.17394" title="Download PDF">pdf</a>, <a href="/format/2402.17394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Network Protocol Fuzzing: Model, Techniques and Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shihao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongfang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Long Luo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Gang Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">As one of the most successful and effective software testing techniques in
recent years, fuzz testing has uncovered numerous bugs and vulnerabilities in
modern software, including network protocol software. In contrast to other
fuzzing targets, network protocol software exhibits its distinct
characteristics and challenges, introducing a plethora of research questions
that need to be addressed in the design and implementation of network protocol
fuzzers. While some research work has evaluated and systematized the knowledge
of general fuzzing techniques at a high level, there is a lack of similar
analysis and summarization for fuzzing research specific to network protocols.
This paper offers a comprehensive exposition of network protocol software's
fuzzing-related features and conducts a systematic review of some
representative advancements in network protocol fuzzing since its inception. We
summarize state-of-the-art strategies and solutions in various aspects, propose
a unified protocol fuzzing process model, and introduce the techniques involved
in each stage of the model. At the same time, this paper also summarizes the
promising research directions in the landscape of protocol fuzzing to foster
exploration within the community for more efficient and intelligent modern
network protocol fuzzing techniques.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17396" title="Abstract">arXiv:2402.17396</a> [<a href="/pdf/2402.17396" title="Download PDF">pdf</a>, <a href="/format/2402.17396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of  Prompting Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petruzzellis%2C+F">Flavio Petruzzellis</a>, 
<a href="/search/cs?searchtype=author&query=Testolin%2C+A">Alberto Testolin</a>, 
<a href="/search/cs?searchtype=author&query=Sperduti%2C+A">Alessandro Sperduti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing thanks to their ability to reuse knowledge acquired on
massive text corpora on a wide variety of downstream tasks, with minimal (if
any) tuning steps. At the same time, it has been repeatedly shown that LLMs
lack systematic generalization, which allows to extrapolate the learned
statistical regularities outside the training distribution. In this work, we
offer a systematic benchmarking of GPT-4, one of the most advanced LLMs
available, on three algorithmic tasks characterized by the possibility to
control the problem difficulty with two parameters. We compare the performance
of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the
Transformer-Encoder architecture recently introduced to solve similar tasks,
the Neural Data Router. We find that the deployment of advanced prompting
techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating
that state-of-the-art LLMs constitute a very strong baseline also in
challenging tasks that require systematic generalization.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17400" title="Abstract">arXiv:2402.17400</a> [<a href="/pdf/2402.17400" title="Download PDF">pdf</a>, <a href="/format/2402.17400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Continual Pretraining in Large Language Models: Insights  and Implications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Y%C4%B1ld%C4%B1z%2C+%C3%87">&#xc7;a&#x11f;atay Y&#x131;ld&#x131;z</a>, 
<a href="/search/cs?searchtype=author&query=Ravichandran%2C+N+K">Nishaanth Kanna Ravichandran</a>, 
<a href="/search/cs?searchtype=author&query=Punia%2C+P">Prishruit Punia</a>, 
<a href="/search/cs?searchtype=author&query=Bethge%2C+M">Matthias Bethge</a>, 
<a href="/search/cs?searchtype=author&query=Ermis%2C+B">Beyza Ermis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper studies the evolving domain of Continual Learning (CL) in large
language models (LLMs), with a focus on developing strategies for efficient and
sustainable training. Our primary emphasis is on continual domain-adaptive
pretraining, a process designed to equip LLMs with the ability to integrate new
information from various domains while retaining previously learned knowledge
and enhancing cross-domain knowledge transfer without relying on
domain-specific identification. Unlike previous studies, which mostly
concentrate on a limited selection of tasks or domains and primarily aim to
address the issue of forgetting, our research evaluates the adaptability and
capabilities of LLMs to changing data landscapes in practical scenarios. To
this end, we introduce a new benchmark designed to measure the adaptability of
LLMs to these evolving data environments, offering a comprehensive framework
for evaluation. We examine the impact of model size on learning efficacy and
forgetting, as well as how the progression and similarity of emerging domains
affect the knowledge transfer within these models. Our findings uncover several
key insights: (i) when the sequence of domains shows semantic similarity,
continual pretraining enables LLMs to better specialize in the current domain
compared to stand-alone fine-tuning, (ii) training across a diverse range of
domains enhances both backward and forward knowledge transfer, and (iii)
smaller models are particularly sensitive to continual pretraining, showing the
most significant rates of both forgetting and learning. We posit that our
research marks a shift towards establishing a more realistic benchmark for
investigating CL in LLMs, and has the potential to play a key role in guiding
the direction of future research in the field.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17403" title="Abstract">arXiv:2402.17403</a> [<a href="/pdf/2402.17403" title="Download PDF">pdf</a>, <a href="/format/2402.17403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sora Generates Videos with Stunning Geometrical Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuanyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Daquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenxu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Shaodong Wei</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Q">Qibin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recently developed Sora model [1] has exhibited remarkable capabilities
in video generation, sparking intense discussions regarding its ability to
simulate real-world phenomena. Despite its growing popularity, there is a lack
of established metrics to evaluate its fidelity to real-world physics
quantitatively. In this paper, we introduce a new benchmark that assesses the
quality of the generated videos based on their adherence to real-world physics
principles. We employ a method that transforms the generated videos into 3D
models, leveraging the premise that the accuracy of 3D reconstruction is
heavily contingent on the video quality. From the perspective of 3D
reconstruction, we use the fidelity of the geometric constraints satisfied by
the constructed 3D models as a proxy to gauge the extent to which the generated
videos conform to real-world physics rules. Project page:
https://sora-geometrical-consistency.github.io/
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17405" title="Abstract">arXiv:2402.17405</a> [<a href="/pdf/2402.17405" title="Download PDF">pdf</a>, <a href="/ps/2402.17405" title="Download PostScript">ps</a>, <a href="/format/2402.17405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Underwater Acoustic Source Seeking Using Time-Difference-of-Arrival  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandi%C4%87%2C+F">Filip Mandi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Mi%C5%A1kovi%C4%87%2C+N">Nikola Mi&#x161;kovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Lon%C4%8Dar%2C+I">Ivan Lon&#x10d;ar</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Journal of Oceanic Engineering, vol. 45, no. 3, pp. 759-771,
  July 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The research presented in this paper is aimed at developing a control
algorithm for an autonomous surface system carrying a two-sensor array
consisting of two acoustic receivers, capable of measuring the
time-difference-of-arrival (TDOA) of a quasiperiodic underwater acoustic signal
and utilizing this value to steer the system toward the acoustic source in the
horizontal plane. Stability properties of the proposed algorithm are analyzed
using the Lie bracket approximation technique. Furthermore, simulation results
are presented, where particular attention is given to the relationship between
the time difference of arrival measurement noise and the sensor baseline - the
distance between the two acoustic receivers. Also, the influence of a constant
disturbance caused by sea currents is considered. Finally, experimental results
in which the algorithm was deployed on two autonomous surface vehicles, each
equipped with a single acoustic receiver, are presented. The algorithm
successfully steers the vehicle formation toward the acoustic source, despite
the measurement noise and intermittent measurements, thus showing the
feasibility of the proposed algorithm in real-life conditions.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17406" title="Abstract">arXiv:2402.17406</a> [<a href="/pdf/2402.17406" title="Download PDF">pdf</a>, <a href="/format/2402.17406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+S">Shentong Mo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yansen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xufang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Visual Prompt Tuning (VPT) techniques have gained prominence for their
capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual
tasks using specialized learnable tokens termed as prompts. Contemporary VPT
methodologies, especially when employed with self-supervised vision
transformers, often default to the introduction of new learnable prompts or
gated prompt tokens predominantly sourced from the model's previous block. A
pivotal oversight in such approaches is their failure to harness the potential
of long-range previous blocks as sources of prompts within each self-supervised
ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning
(LSPT) - a revolutionary approach to visual representation learning. Drawing
inspiration from the intricacies of the human brain, LSPT ingeniously
incorporates long-term gated prompts. This feature serves as temporal coding,
curbing the risk of forgetting parameters acquired from earlier blocks. Further
enhancing its prowess, LSPT brings into play patch tokens, serving as spatial
coding. This is strategically designed to perpetually amass class-conscious
features, thereby fortifying the model's prowess in distinguishing and
identifying visual categories. To validate the efficacy of our proposed method,
we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.
Our empirical findings underscore the superiority of LSPT, showcasing its
ability to set new benchmarks in visual prompt tuning performance.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17407" title="Abstract">arXiv:2402.17407</a> [<a href="/pdf/2402.17407" title="Download PDF">pdf</a>, <a href="/format/2402.17407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neural Rewriting System to Solve Algorithmic Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petruzzellis%2C+F">Flavio Petruzzellis</a>, 
<a href="/search/cs?searchtype=author&query=Testolin%2C+A">Alberto Testolin</a>, 
<a href="/search/cs?searchtype=author&query=Sperduti%2C+A">Alessandro Sperduti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Modern neural network architectures still struggle to learn algorithmic
procedures that require to systematically apply compositional rules to solve
out-of-distribution problem instances. In this work, we propose an original
approach to learn algorithmic tasks inspired by rewriting systems, a classic
framework in symbolic artificial intelligence. We show that a rewriting system
can be implemented as a neural architecture composed by specialized modules:
the Selector identifies the target sub-expression to process, the Solver
simplifies the sub-expression by computing the corresponding result, and the
Combiner produces a new version of the original expression by replacing the
sub-expression with the solution provided. We evaluate our model on three types
of algorithmic tasks that require simplifying symbolic formulas involving
lists, arithmetic, and algebraic expressions. We test the extrapolation
capabilities of the proposed architecture using formulas involving a higher
number of operands and nesting levels than those seen during training, and we
benchmark its performance against the Neural Data Router, a recent model
specialized for systematic generalization, and a state-of-the-art large
language model (GPT-4) probed with advanced prompting strategies.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17409" title="Abstract">arXiv:2402.17409</a> [<a href="/pdf/2402.17409" title="Download PDF">pdf</a>, <a href="/ps/2402.17409" title="Download PostScript">ps</a>, <a href="/format/2402.17409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Programmable Drone in Educational Projects and Competitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrovi%C4%8D%2C+P">Pavel Petrovi&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Ver%C4%8Dim%C3%A1k%2C+P">Peter Ver&#x10d;im&#xe1;k</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was co-funded by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338. Open Access code and data described in the paper are available at: <a href="https://doi.org/10.5281/zenodo.10715699">this https URL</a> and <a href="https://github.com/RoboCup-Junior-Slovensko/softverova-podpora/tree/master/drone-robocup2023">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of EDULEARN23 Conference 3rd-5th July 2023, Palma,
  Mallorca, Spain, ISBN: 978-84-09-52151-7
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The mainstream of educational robotics platforms orbits the various versions
of versatile robotics sets and kits, while interesting outliers add new
opportunities and extend the possible learning situations. Examples of such are
reconfigurable robots, rolling sphere robots, humanoids, swimming, or
underwater robots. Another kind within this category are flying drones. While
remotely controlled drones were a very attractive target for hobby model makers
for quite a long time already, they were seldom used in educational scenarios
as robots that are programmed by children to perform various simple tasks. A
milestone was reached with the introduction of the educational drone Tello,
which can be programmed even in Scratch, or some general-purpose languages such
as Node.js or Python. The programs can even have access to the robot sensors
that are used by the underlying layers of the controller. In addition, they
have the option to acquire images from the drone camera and perform actions
based on processing the frames applying computer vision algorithms. We have
been using this drone in an educational robotics competition for three years
without camera, and after our students have developed several successful
projects that utilized a camera, we prepared a new competition challenge that
requires the use of the camera. In the article, we summarize related efforts
and our experiences with educational drones, and their use in the student
projects and competition.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17410" title="Abstract">arXiv:2402.17410</a> [<a href="/pdf/2402.17410" title="Download PDF">pdf</a>, <a href="/ps/2402.17410" title="Download PostScript">ps</a>, <a href="/format/2402.17410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel image space formalism of Fourier domain interpolation neural  networks for noise propagation analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dawood%2C+P">Peter Dawood</a>, 
<a href="/search/cs?searchtype=author&query=Breuer%2C+F">Felix Breuer</a>, 
<a href="/search/cs?searchtype=author&query=Homolya%2C+I">Istvan Homolya</a>, 
<a href="/search/cs?searchtype=author&query=Stebani%2C+J">Jannik Stebani</a>, 
<a href="/search/cs?searchtype=author&query=Gram%2C+M">Maximilian Gram</a>, 
<a href="/search/cs?searchtype=author&query=Jakob%2C+P+M">Peter M. Jakob</a>, 
<a href="/search/cs?searchtype=author&query=Zaiss%2C+M">Moritz Zaiss</a>, 
<a href="/search/cs?searchtype=author&query=Blaimer%2C+M">Martin Blaimer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Purpose: To develop an image space formalism of multi-layer convolutional
neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions
and analytically estimate noise propagation during CNN inference. Theory and
Methods: Nonlinear activations in the Fourier domain (also known as k-space)
using complex-valued Rectifier Linear Units are expressed as elementwise
multiplication with activation masks. This operation is transformed into a
convolution in the image space. After network training in k-space, this
approach provides an algebraic expression for the derivative of the
reconstructed image with respect to the aliased coil images, which serve as the
input tensors to the network in the image space. This allows the variance in
the network inference to be estimated analytically and to be used to describe
noise characteristics. Monte-Carlo simulations and numerical approaches based
on auto-differentiation were used for validation. The framework was tested on
retrospectively undersampled invivo brain images. Results: Inferences conducted
in the image domain are quasi-identical to inferences in the k-space,
underlined by corresponding quantitative metrics. Noise variance maps obtained
from the analytical expression correspond with those obtained via Monte-Carlo
simulations, as well as via an auto-differentiation approach. The noise
resilience is well characterized, as in the case of classical Parallel Imaging.
Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes
in variance maps obtained via Monte-Carlo simulations. Conclusion: The
quasi-equivalent image space formalism for neural networks for k-space
interpolation enables fast and accurate description of the noise
characteristics during CNN inference, analogous to geometry-factor maps in
traditional parallel imaging methods.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17411" title="Abstract">arXiv:2402.17411</a> [<a href="/pdf/2402.17411" title="Download PDF">pdf</a>, <a href="/format/2402.17411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistency Matters: Explore LLMs Consistency From a Black-Box  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fufangchen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+G">Guoqiang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiaheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+F">Fei Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Nowadays both commercial and open-source academic LLM have become the
mainstream models of NLP. However, there is still a lack of research on LLM
consistency, meaning that throughout the various stages of LLM research and
deployment, its internal parameters and capabilities should remain unchanged.
This issue exists in both the industrial and academic sectors. The solution to
this problem is often time-consuming and labor-intensive, and there is also an
additional cost of secondary deployment, resulting in economic and time losses.
To fill this gap, we build an LLM consistency task dataset and design several
baselines. Additionally, we choose models of diverse scales for the main
experiments. Specifically, in the LightGBM experiment, we used traditional NLG
metrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training.
The final result exceeds the manual evaluation and GPT3.5 as well as other
models in the main experiment, achieving the best performance. In the end, we
use the best performing LightGBM model as the base model to build the
evaluation tool, which can effectively assist in the deployment of business
models. Our code and tool demo are available at
https://github.com/heavenhellchen/Consistency.git
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17412" title="Abstract">arXiv:2402.17412</a> [<a href="/pdf/2402.17412" title="Download PDF">pdf</a>, <a href="/format/2402.17412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized  Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marjit%2C+S">Shyam Marjit</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+H">Harshit Singh</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+N">Nityanand Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+S">Sayak Paul</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chia-Mu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: \href{https://diffusekrona.github.io/}{<a href="https://diffusekrona.github.io/">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the realm of subject-driven text-to-image (T2I) generative models, recent
developments like DreamBooth and BLIP-Diffusion have led to impressive results
yet encounter limitations due to their intensive fine-tuning demands and
substantial parameter requirements. While the low-rank adaptation (LoRA) module
within DreamBooth offers a reduction in trainable parameters, it introduces a
pronounced sensitivity to hyperparameters, leading to a compromise between
parameter efficiency and the quality of T2I personalized image synthesis.
Addressing these constraints, we introduce \textbf{\textit{DiffuseKronA}}, a
novel Kronecker product-based adaptation module that not only significantly
reduces the parameter count by 35\% and 99.947\% compared to LoRA-DreamBooth
and the original DreamBooth, respectively, but also enhances the quality of
image synthesis. Crucially, \textit{DiffuseKronA} mitigates the issue of
hyperparameter sensitivity, delivering consistent high-quality generations
across a wide range of hyperparameters, thereby diminishing the necessity for
extensive fine-tuning. Furthermore, a more controllable decomposition makes
\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\%
reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse
and complex input images and text prompts, \textit{DiffuseKronA} consistently
outperforms existing models, producing diverse images of higher quality with
improved fidelity and a more accurate color distribution of objects, all the
while upholding exceptional parameter efficiency, thus presenting a substantial
advancement in the field of T2I generative modeling. Our project page,
consisting of links to the code, and pre-trained checkpoints, is available at
\href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17414" title="Abstract">arXiv:2402.17414</a> [<a href="/pdf/2402.17414" title="Download PDF">pdf</a>, <a href="/format/2402.17414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Video Compression with Feature Modulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiahao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yan Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2024. Codes are at <a href="https://github.com/microsoft/DCVC">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The emerging conditional coding-based neural video codec (NVC) shows
superiority over commonly-used residual coding-based codec and the latest NVC
already claims to outperform the best traditional codec. However, there still
exist critical problems blocking the practicality of NVC. In this paper, we
propose a powerful conditional coding-based NVC that solves two critical
problems via feature modulation. The first is how to support a wide quality
range in a single model. Previous NVC with this capability only supports about
3.8 dB PSNR range on average. To tackle this limitation, we modulate the latent
feature of the current frame via the learnable quantization scaler. During the
training, we specially design the uniform quantization parameter sampling
mechanism to improve the harmonization of encoding and quantization. This
results in a better learning of the quantization scaler and helps our NVC
support about 11.4 dB PSNR range. The second is how to make NVC still work
under a long prediction chain. We expose that the previous SOTA NVC has an
obvious quality degradation problem when using a large intra-period setting. To
this end, we propose modulating the temporal feature with a periodically
refreshing mechanism to boost the quality. %Besides solving the above two
problems, we also design a single model that can support both RGB and YUV
colorspaces. Notably, under single intra-frame setting, our codec can achieve
29.7\% bitrate saving over previous SOTA NVC with 16\% MACs reduction. Our
codec serves as a notable landmark in the journey of NVC evolution. The codes
are at https://github.com/microsoft/DCVC.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17417" title="Abstract">arXiv:2402.17417</a> [<a href="/pdf/2402.17417" title="Download PDF">pdf</a>, <a href="/format/2402.17417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARZero: Cross-Attention Alignment for Radiology Zero-Shot  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+H">Haoran Lai</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Q">Qingsong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zihang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rongsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiyang He</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaodong Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S+K">S. Kevin Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The advancement of Zero-Shot Learning in the medical domain has been driven
forward by using pre-trained models on large-scale image-text pairs, focusing
on image-text alignment. However, existing methods primarily rely on cosine
similarity for alignment, which may not fully capture the complex relationship
between medical images and reports. To address this gap, we introduce a novel
approach called Cross-Attention Alignment for Radiology Zero-Shot
Classification (CARZero). Our approach innovatively leverages cross-attention
mechanisms to process image and report features, creating a Similarity
Representation that more accurately reflects the intricate relationships in
medical semantics. This representation is then linearly projected to form an
image-text similarity matrix for cross-modality alignment. Additionally,
recognizing the pivotal role of prompt selection in zero-shot learning, CARZero
incorporates a Large Language Model-based prompt alignment strategy. This
strategy standardizes diverse diagnostic expressions into a unified format for
both training and inference phases, overcoming the challenges of manual prompt
design. Our approach is simple yet effective, demonstrating state-of-the-art
performance in zero-shot classification on five official chest radiograph
diagnostic test sets, including remarkable results on datasets with long-tail
distributions of rare diseases. This achievement is attributed to our new
image-text alignment strategy, which effectively addresses the complex
relationship between medical images and reports.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17420" title="Abstract">arXiv:2402.17420</a> [<a href="/pdf/2402.17420" title="Download PDF">pdf</a>, <a href="/format/2402.17420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PANDAS: Prototype-based Novel Class Discovery and Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayes%2C+T+L">Tyler L. Hayes</a>, 
<a href="/search/cs?searchtype=author&query=de+Souza%2C+C+R">C&#xe9;sar R. de Souza</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Namil Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jiwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Volpi%2C+R">Riccardo Volpi</a>, 
<a href="/search/cs?searchtype=author&query=Larlus%2C+D">Diane Larlus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Object detectors are typically trained once and for all on a fixed set of
classes. However, this closed-world assumption is unrealistic in practice, as
new classes will inevitably emerge after the detector is deployed in the wild.
In this work, we look at ways to extend a detector trained for a set of base
classes so it can i) spot the presence of novel classes, and ii) automatically
enrich its repertoire to be able to detect those newly discovered classes
together with the base ones. We propose PANDAS, a method for novel class
discovery and detection. It discovers clusters representing novel classes from
unlabeled data, and represents old and new classes with prototypes. During
inference, a distance-based classifier uses these prototypes to assign a label
to each detected object instance. The simplicity of our method makes it widely
applicable. We experimentally demonstrate the effectiveness of PANDAS on the
VOC 2012 and COCO-to-LVIS benchmarks. It performs favorably against the state
of the art for this task while being computationally more affordable.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17423" title="Abstract">arXiv:2402.17423</a> [<a href="/pdf/2402.17423" title="Download PDF">pdf</a>, <a href="/format/2402.17423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforced In-Context Black-Box Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Lei Song</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chenxiao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+K">Ke Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dong Li</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zongzhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chao Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Black-Box Optimization (BBO) has found successful applications in many fields
of science and engineering. Recently, there has been a growing interest in
meta-learning particular components of BBO algorithms to speed up optimization
and get rid of tedious hand-crafted heuristics. As an extension, learning the
entire algorithm from data requires the least labor from experts and can
provide the most flexibility. In this paper, we propose RIBBO, a method to
reinforce-learn a BBO algorithm from offline data in an end-to-end fashion.
RIBBO employs expressive sequence models to learn the optimization histories
produced by multiple behavior algorithms and tasks, leveraging the in-context
learning ability of large models to extract task information and make decisions
accordingly. Central to our method is to augment the optimization histories
with regret-to-go tokens, which are designed to represent the performance of an
algorithm based on cumulative regret of the histories. The integration of
regret-to-go tokens enables RIBBO to automatically generate sequences of query
points that satisfy the user-desired regret, which is verified by its
universally good empirical performance on diverse problems, including BBOB
functions, hyper-parameter optimization and robot control problems.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17424" title="Abstract">arXiv:2402.17424</a> [<a href="/pdf/2402.17424" title="Download PDF">pdf</a>, <a href="/ps/2402.17424" title="Download PostScript">ps</a>, <a href="/format/2402.17424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViTaL: An Advanced Framework for Automated Plant Disease Identification  in Leaf Images Using Vision Transformers and Linear Projection For Feature  Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sebastian%2C+A">Abhishek Sebastian</a>, 
<a href="/search/cs?searchtype=author&query=A%2C+A+F">Annis Fathima A</a>, 
<a href="/search/cs?searchtype=author&query=R%2C+P">Pragna R</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+M+K">Madhan Kumar S</a>, 
<a href="/search/cs?searchtype=author&query=G%2C+Y+K">Yaswanth Kannan G</a>, 
<a href="/search/cs?searchtype=author&query=Murali%2C+V">Vinay Murali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Our paper introduces a robust framework for the automated identification of
diseases in plant leaf images. The framework incorporates several key stages to
enhance disease recognition accuracy. In the pre-processing phase, a thumbnail
resizing technique is employed to resize images, minimizing the loss of
critical image details while ensuring computational efficiency. Normalization
procedures are applied to standardize image data before feature extraction.
Feature extraction is facilitated through a novel framework built upon Vision
Transformers, a state-of-the-art approach in image analysis. Additionally,
alternative versions of the framework with an added layer of linear projection
and blockwise linear projections are explored. This comparative analysis allows
for the evaluation of the impact of linear projection on feature extraction and
overall model performance. To assess the effectiveness of the proposed
framework, various Convolutional Neural Network (CNN) architectures are
utilized, enabling a com- prehensive evaluation of linear projection's
influence on key evaluation metrics. The findings demonstrate the efficacy of
the proposed framework, with the top- performing model achieving a Hamming loss
of 0.054. Furthermore, we propose a novel hardware design specifically tailored
for scanning diseased leaves in an omnidirectional fashion. The hardware
implementation utilizes a Raspberry Pi Compute Module to address low-memory
configurations, ensuring practicality and affordability. This innovative
hardware solution enhances the overall feasibility and accessibility of the
proposed automated disease identification system. This research contributes to
the field of agriculture by offering valuable insights and tools for the early
detection and management of plant diseases, potentially leading to improved
crop yields and enhanced food security.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17427" title="Abstract">arXiv:2402.17427</a> [<a href="/pdf/2402.17427" title="Download PDF">pdf</a>, <a href="/format/2402.17427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiaqi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianzhuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiyong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiayue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yangdi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaofei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Songcen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Youliang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenming Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2024. Project website: <a href="https://vastgaussian.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing NeRF-based methods for large scene reconstruction often have
limitations in visual quality and rendering speed. While the recent 3D Gaussian
Splatting works well on small-scale and object-centric scenes, scaling it up to
large scenes poses challenges due to limited video memory, long optimization
time, and noticeable appearance variations. To address these challenges, we
present VastGaussian, the first method for high-quality reconstruction and
real-time rendering on large scenes based on 3D Gaussian Splatting. We propose
a progressive partitioning strategy to divide a large scene into multiple
cells, where the training cameras and point cloud are properly distributed with
an airspace-aware visibility criterion. These cells are merged into a complete
scene after parallel optimization. We also introduce decoupled appearance
modeling into the optimization process to reduce appearance variations in the
rendered images. Our approach outperforms existing NeRF-based methods and
achieves state-of-the-art results on multiple large scene datasets, enabling
fast optimization and high-fidelity real-time rendering.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17428" title="Abstract">arXiv:2402.17428</a> [<a href="/pdf/2402.17428" title="Download PDF">pdf</a>, <a href="/format/2402.17428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shortest cover after edit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitani%2C+K">Kazuki Mitani</a>, 
<a href="/search/cs?searchtype=author&query=Mieno%2C+T">Takuya Mieno</a>, 
<a href="/search/cs?searchtype=author&query=Seto%2C+K">Kazuhisa Seto</a>, 
<a href="/search/cs?searchtype=author&query=Horiyama%2C+T">Takashi Horiyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">This paper investigates the (quasi-)periodicity of a string when the string
is edited. A string $C$ is called a cover (as known as a quasi-period) of a
string $T$ if each character of $T$ lies within some occurrence of $C$. By
definition, a cover of $T$ must be a border of $T$; that is, it occurs both as
a prefix and as a suffix of $T$. In this paper, we focus on the changes in the
longest border and the shortest cover of a string when the string is edited
only once. We propose a data structure of size $O(n)$ that computes the longest
border and the shortest cover of the string in $O(\ell + \log n)$ time after an
edit operation (either insertion, deletion, or substitution of some string) is
applied to the input string $T$ of length $n$, where $\ell$ is the length of
the string being inserted or substituted. The data structure can be constructed
in $O(n)$ time given string $T$.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17430" title="Abstract">arXiv:2402.17430</a> [<a href="/pdf/2402.17430" title="Download PDF">pdf</a>, <a href="/format/2402.17430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Enhanced Queries of Point Sets for Vectorized Map  Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guangwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Ji Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Ningyi Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In autonomous driving, the high-definition (HD) map plays a crucial role in
localization and planning. Recently, several methods have facilitated
end-to-end online map construction in DETR-like frameworks. However, little
attention has been paid to the potential capabilities of exploring the query
mechanism. This paper introduces MapQR, an end-to-end method with an emphasis
on enhancing query capabilities for constructing online vectorized maps.
Although the map construction is essentially a point set prediction task, MapQR
utilizes instance queries rather than point queries. These instance queries are
scattered for the prediction of point sets and subsequently gathered for the
final matching. This query design, called the scatter-and-gather query, shares
content information in the same map element and avoids possible inconsistency
of content information in point queries. We further exploit prior information
to enhance an instance query by adding positional information embedded from
their reference points. Together with a simple and effective improvement of a
BEV encoder, the proposed MapQR achieves the best mean average precision (mAP)
and maintains good efficiency on both nuScenes and Argoverse 2. In addition,
integrating our query design into other models can boost their performance
significantly. The code will be available at https://github.com/HXMap/MapQR.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17431" title="Abstract">arXiv:2402.17431</a> [<a href="/pdf/2402.17431" title="Download PDF">pdf</a>, <a href="/format/2402.17431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning  with Kandinsky Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorello%2C+L+S">Luca Salvatore Lorello</a>, 
<a href="/search/cs?searchtype=author&query=Lippi%2C+M">Marco Lippi</a>, 
<a href="/search/cs?searchtype=author&query=Melacci%2C+S">Stefano Melacci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Artificial intelligence is continuously seeking novel challenges and
benchmarks to effectively measure performance and to advance the
state-of-the-art. In this paper we introduce KANDY, a benchmarking framework
that can be used to generate a variety of learning and reasoning tasks inspired
by Kandinsky patterns. By creating curricula of binary classification tasks
with increasing complexity and with sparse supervisions, KANDY can be used to
implement benchmarks for continual and semi-supervised learning, with a
specific focus on symbol compositionality. Classification rules are also
provided in the ground truth to enable analysis of interpretable solutions.
Together with the benchmark generation pipeline, we release two curricula, an
easier and a harder one, that we propose as new challenges for the research
community. With a thorough experimental evaluation, we show how both
state-of-the-art neural models and purely symbolic approaches struggle with
solving most of the tasks, thus calling for the application of advanced
neuro-symbolic methods trained over time.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17433" title="Abstract">arXiv:2402.17433</a> [<a href="/pdf/2402.17433" title="Download PDF">pdf</a>, <a href="/format/2402.17433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing EEG-to-Text Decoding through Transferable Representations from  Pre-trained Contrastive EEG-Text Masked Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhenxi Song</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhengyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiguo Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Reconstructing natural language from non-invasive electroencephalography
(EEG) holds great promise as a language decoding technology for brain-computer
interfaces (BCIs). However, EEG-based language decoding is still in its nascent
stages, facing several technical issues such as: 1) Absence of a hybrid
strategy that can effectively integrate cross-modality (between EEG and text)
self-learning with intra-modality self-reconstruction of EEG features or
textual sequences; 2) Under-utilization of large language models (LLMs) to
enhance EEG-based language decoding. To address above issues, we propose the
Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that
orchestrates compound self-supervised learning across and within EEG and text
through a dedicated multi-stream encoder. Furthermore, we develop a framework
called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable
Representations), which leverages pre-trained modules alongside the EEG stream
from CET-MAE and further enables an LLM (specifically BART) to decode text from
EEG sequences. Comprehensive experiments conducted on the popular text-evoked
EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms
the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%,
respectively. These results indicate significant advancements in the field and
underscores the proposed framework's potential to enable more powerful and
widespread BCI applications.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17434" title="Abstract">arXiv:2402.17434</a> [<a href="/pdf/2402.17434" title="Download PDF">pdf</a>, <a href="/format/2402.17434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Passive Aligning Physical Interaction of Fully-Actuated Aerial Vehicles  for Pushing Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hui%2C+T">Tong Hui</a>, 
<a href="/search/cs?searchtype=author&query=Cuniato%2C+E">Eugenio Cuniato</a>, 
<a href="/search/cs?searchtype=author&query=Pantic%2C+M">Michael Pantic</a>, 
<a href="/search/cs?searchtype=author&query=Tognon%2C+M">Marco Tognon</a>, 
<a href="/search/cs?searchtype=author&query=Fumagalli%2C+M">Matteo Fumagalli</a>, 
<a href="/search/cs?searchtype=author&query=Siegwart%2C+R">Roland Siegwart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 2024 IEEE International Conference on Robotics and Automation (ICRA2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Recently, the utilization of aerial manipulators for performing pushing tasks
in non-destructive testing (NDT) applications has seen significant growth. Such
operations entail physical interactions between the aerial robotic system and
the environment. End-effectors with multiple contact points are often used for
placing NDT sensors in contact with a surface to be inspected. Aligning the NDT
sensor and the work surface while preserving contact, requires that all
available contact points at the end-effector tip are in contact with the work
surface. With a standard full-pose controller, attitude errors often occur due
to perturbations caused by modeling uncertainties, sensor noise, and
environmental uncertainties. Even small attitude errors can cause a loss of
contact points between the end-effector tip and the work surface. To preserve
full alignment amidst these uncertainties, we propose a control strategy which
selectively deactivates angular motion control and enables direct force control
in specific directions. In particular, we derive two essential conditions to be
met, such that the robot can passively align with flat work surfaces achieving
full alignment through the rotation along non-actively controlled axes.
Additionally, these conditions serve as hardware design and control guidelines
for effectively integrating the proposed control method for practical usage.
Real world experiments are conducted to validate both the control design and
the guidelines.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17436" title="Abstract">arXiv:2402.17436</a> [<a href="/pdf/2402.17436" title="Download PDF">pdf</a>, <a href="/format/2402.17436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wykorzystanie rekonfigurowalnych matryc antenowych wraz z informacj&#x105;  kontekstow&#x105;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ku%C5%82acz%2C+%C5%81">&#x141;ukasz Ku&#x142;acz</a>, 
<a href="/search/cs?searchtype=author&query=Kliks%2C+A">Adrian Kliks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, in Polish language, 5 figures, presented during conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconfigurable intelligent surfaces can be successfully used to control the
radio environment. Simple control of the reflection angle of the signal from
the surface allows maximization or minimization of the received power in
specific places. The paper presents simulations where it is possible to receive
a signal in a place where it was not possible, to detect the occupancy of the
spectrum in a place where the sensor was unable to make correct detection or to
minimize interference in a specific receiver.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17437" title="Abstract">arXiv:2402.17437</a> [<a href="/pdf/2402.17437" title="Download PDF">pdf</a>, <a href="/format/2402.17437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Emotion-Semantic Correlations for Empathetic Response  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhou Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaofei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhihao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Tiecheng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yunbing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yisong Su</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+S">Sibo Ju</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+X">Xiangwen Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures, Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Empathetic response generation aims to generate empathetic responses by
understanding the speaker's emotional feelings from the language of dialogue.
Recent methods capture emotional words in the language of communicators and
construct them as static vectors to perceive nuanced emotions. However,
linguistic research has shown that emotional words in language are dynamic and
have correlations with other grammar semantic roles, i.e., words with semantic
meanings, in grammar. Previous methods overlook these two characteristics,
which easily lead to misunderstandings of emotions and neglect of key
semantics. To address this issue, we propose a dynamical Emotion-Semantic
Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM
constructs dynamic emotion-semantic vectors through the interaction of context
and emotions. We introduce dependency trees to reflect the correlations between
emotions and semantics. Based on dynamic emotion-semantic vectors and
dependency trees, we propose a dynamic correlation graph convolutional network
to guide the model in learning context meanings in dialogue and generating
empathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset
show that ESCM understands semantics and emotions more accurately and expresses
fluent and informative empathetic responses. Our analysis results also indicate
that the correlations between emotions and semantics are frequently used in
dialogues, which is of great significance for empathetic perception and
expression.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17440" title="Abstract">arXiv:2402.17440</a> [<a href="/pdf/2402.17440" title="Download PDF">pdf</a>, <a href="/format/2402.17440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Principled Architecture-aware Scaling of Hyperparameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wuyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junru Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hanin%2C+B">Boris Hanin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Training a high-quality deep neural network requires choosing suitable
hyperparameters, which is a non-trivial and expensive process. Current works
try to automatically optimize or design principles of hyperparameters, such
that they can generalize to diverse unseen scenarios. However, most designs or
optimization methods are agnostic to the choice of network structures, and thus
largely ignore the impact of neural architectures on hyperparameters. In this
work, we precisely characterize the dependence of initializations and maximal
learning rates on the network architecture, which includes the network depth,
width, convolutional kernel size, and connectivity patterns. By pursuing every
parameter to be maximally updated with the same mean squared change in
pre-activations, we can generalize our initialization and learning rates across
MLPs (multi-layer perception) and CNNs (convolutional neural network) with
sophisticated graph topologies. We verify our principles with comprehensive
experiments. More importantly, our strategy further sheds light on advancing
current benchmarks for architecture design. A fair comparison of AutoML
algorithms requires accurate network rankings. However, we demonstrate that
network rankings can be easily changed by better training networks in
benchmarks with our architecture-aware learning rates and initialization.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17442" title="Abstract">arXiv:2402.17442</a> [<a href="/pdf/2402.17442" title="Download PDF">pdf</a>, <a href="/format/2402.17442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ansible Lightspeed: A Code Generation Service for IT Automation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+P">Priyam Sahoo</a>, 
<a href="/search/cs?searchtype=author&query=Pujar%2C+S">Saurabh Pujar</a>, 
<a href="/search/cs?searchtype=author&query=Nalawade%2C+G">Ganesh Nalawade</a>, 
<a href="/search/cs?searchtype=author&query=Gebhardt%2C+R">Richard Gebhardt</a>, 
<a href="/search/cs?searchtype=author&query=Mandel%2C+L">Louis Mandel</a>, 
<a href="/search/cs?searchtype=author&query=Buratti%2C+L">Luca Buratti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
<p class="mathjax">The availability of Large Language Models (LLMs) which can generate code, has
made it possible to create tools that improve developer productivity.
Integrated development environments or IDEs which developers use to write
software are often used as an interface to interact with LLMs. Although many
such tools have been released, almost all of them focus on general-purpose
programming languages. Domain-specific languages, such as those crucial for IT
automation, have not received much attention. Ansible is one such YAML-based IT
automation-specific language. Red Hat Ansible Lightspeed with IBM Watson Code
Assistant, further referred to as Ansible Lightspeed, is an LLM-based service
designed explicitly for natural language to Ansible code generation.
<br />In this paper, we describe the design and implementation of the Ansible
Lightspeed service and analyze feedback from thousands of real users. We
examine diverse performance indicators, classified according to both immediate
and extended utilization patterns along with user sentiments. The analysis
shows that the user acceptance rate of Ansible Lightspeed suggestions is higher
than comparable tools that are more general and not specific to a programming
language. This remains true even after we use much more stringent criteria for
what is considered an accepted model suggestion, discarding suggestions which
were heavily edited after being accepted. The relatively high acceptance rate
results in higher-than-expected user retention and generally positive user
feedback. This paper provides insights on how a comparatively small, dedicated
model performs on a domain-specific language and more importantly, how it is
received by users.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17447" title="Abstract">arXiv:2402.17447</a> [<a href="/pdf/2402.17447" title="Download PDF">pdf</a>, <a href="/format/2402.17447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Based Named Entity Recognition Models for Recipes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goel%2C+M">Mansi Goel</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Ayush Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Shubham Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Kapuriya%2C+J">Janak Kapuriya</a>, 
<a href="/search/cs?searchtype=author&query=Konam%2C+A+V">Akhil Vamshi Konam</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rishabh Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rastogi%2C+S">Shrey Rastogi</a>, 
<a href="/search/cs?searchtype=author&query=Niharika">Niharika</a>, 
<a href="/search/cs?searchtype=author&query=Bagler%2C+G">Ganesh Bagler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 main figures and 2 in appendices, and 3 main tables; Accepted for publication in LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Food touches our lives through various endeavors, including flavor,
nourishment, health, and sustainability. Recipes are cultural capsules
transmitted across generations via unstructured text. Automated protocols for
recognizing named entities, the building blocks of recipe text, are of immense
value for various applications ranging from information extraction to novel
recipe generation. Named entity recognition is a technique for extracting
information from unstructured or semi-structured data with known labels.
Starting with manually-annotated data of 6,611 ingredient phrases, we created
an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we
systematically cleaned and analyzed ingredient phrases from RecipeDB, the
gold-standard recipe data repository, and annotated them using the Stanford
NER. Based on the analysis, we sampled a subset of 88,526 phrases using a
clustering-based approach while preserving the diversity to create the
machine-annotated dataset. A thorough investigation of NER approaches on these
three datasets involving statistical, fine-tuning of deep learning-based
language models and few-shot prompting on large language models (LLMs) provides
deep insights. We conclude that few-shot prompting on LLMs has abysmal
performance, whereas the fine-tuned spaCy-transformer emerges as the best model
with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,
augmented, and machine-annotated datasets, respectively.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17453" title="Abstract">arXiv:2402.17453</a> [<a href="/pdf/2402.17453" title="Download PDF">pdf</a>, <a href="/format/2402.17453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DS-Agent: Automated Data Science by Empowering Large Language Models  with Case-Based Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Siyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Cheng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hechang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we investigate the potential of large language models (LLMs)
based agents to automate data science tasks, with the goal of comprehending
task requirements, then building and training the best-fit machine learning
models. Despite their widespread success, existing LLM agents are hindered by
generating unreasonable experiment plans within this scenario. To this end, we
present DS-Agent, a novel automatic framework that harnesses LLM agent and
case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR
framework to structure an automatic iteration pipeline, which can flexibly
capitalize on the expert knowledge from Kaggle, and facilitate consistent
performance improvement through the feedback mechanism. Moreover, DS-Agent
implements a low-resource deployment stage with a simplified CBR paradigm to
adapt past successful solutions from the development stage for direct code
generation, significantly reducing the demand on foundational capabilities of
LLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success
rate in the development stage, while attaining 36% improvement on average one
pass rate across alternative LLMs in the deployment stage. In both stages,
DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per
run with GPT-4, respectively.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17456" title="Abstract">arXiv:2402.17456</a> [<a href="/pdf/2402.17456" title="Download PDF">pdf</a>, <a href="/format/2402.17456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to  Assist Adolescent Cyberbullying Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hedderich%2C+M+A">Michael A. Hedderich</a>, 
<a href="/search/cs?searchtype=author&query=Bazarova%2C+N+N">Natalie N. Bazarova</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+W">Wenting Zou</a>, 
<a href="/search/cs?searchtype=author&query=Shim%2C+R">Ryun Shim</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinda Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qian Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Cyberbullying harms teenagers' mental health, and teaching them upstanding
intervention is crucial. Wizard-of-Oz studies show chatbots can scale up
personalized and interactive cyberbullying education, but implementing such
chatbots is a challenging and delicate task. We created a no-code chatbot
design tool for K-12 teachers. Using large language models and prompt chaining,
our tool allows teachers to prototype bespoke dialogue flows and chatbot
utterances. In offering this tool, we explore teachers' distinctive needs when
designing chatbots to assist their teaching, and how chatbot design tools might
better support them. Our findings reveal that teachers welcome the tool
enthusiastically. Moreover, they see themselves as playwrights guiding both the
students' and the chatbot's behaviors, while allowing for some improvisation.
Their goal is to enable students to rehearse both desirable and undesirable
reactions to cyberbullying in a safe environment. We discuss the design
opportunities LLM-Chains offer for empowering teachers and the research
opportunities this work opens up.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17457" title="Abstract">arXiv:2402.17457</a> [<a href="/pdf/2402.17457" title="Download PDF">pdf</a>, <a href="/format/2402.17457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why do Learning Rates Transfer? Reconciling Optimization and Scaling  Limits for Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noci%2C+L">Lorenzo Noci</a>, 
<a href="/search/cs?searchtype=author&query=Meterez%2C+A">Alexandru Meterez</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Orvieto%2C+A">Antonio Orvieto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, there has been growing evidence that if the width and depth of a
neural network are scaled toward the so-called rich feature learning limit
($\mu$P and its depth extension), then some hyperparameters - such as the
learning rate - exhibit transfer from small to very large models, thus reducing
the cost of hyperparameter tuning. From an optimization perspective, this
phenomenon is puzzling, as it implies that the loss landscape is remarkably
consistent across very different model sizes. In this work, we find empirical
evidence that learning rate transfer can be attributed to the fact that under
$\mu$P and its depth extension, the largest eigenvalue of the training loss
Hessian (i.e. the sharpness) is largely independent of the width and depth of
the network for a sustained period of training time. On the other hand, we show
that under the neural tangent kernel (NTK) regime, the sharpness exhibits very
different dynamics at different scales, thus preventing learning rate transfer.
But what causes these differences in the sharpness dynamics? Through a
connection between the spectra of the Hessian and the NTK matrix, we argue that
the cause lies in the presence (for $\mu$P) or progressive absence (for the NTK
regime) of feature learning, which results in a different evolution of the NTK,
and thus of the sharpness. We corroborate our claims with a substantial suite
of experiments, covering a wide range of datasets and architectures: from
ResNets and Vision Transformers trained on benchmark vision datasets to
Transformers-based language models trained on WikiText
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17459" title="Abstract">arXiv:2402.17459</a> [<a href="/pdf/2402.17459" title="Download PDF">pdf</a>, <a href="/format/2402.17459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PureLottery: Fair and Bias-Resistant Leader Election with a Novel  Single-Elimination Tournament Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ballweg%2C+J">Jonas Ballweg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Leader Election (LE) is crucial in distributed systems and blockchain
technology, ensuring one participant acts as the leader. Traditional LE methods
often depend on distributed random number generation (RNG), facing issues like
vulnerability to manipulation, lack of fairness, and the need for complex
procedures such as verifiable delay functions (VDFs) and publicly-verifiable
secret sharing (PVSS). This Bachelor's thesis presents a novel approach to
randomized LE, leveraging a game-theoretic assumption that participants, aiming
to be chosen as leaders, will naturally avoid actions that diminish their
chances. This perspective simplifies LE by eliminating the need for
decentralized RNG. Introducing PureLottery, inspired by single-elimination
sports tournaments, this method offers a fair, bias-resistant, and efficient LE
solution for blockchain environments. It operates on the principle of two
participants competing in each match, rendering collusion efforts useless.
PureLottery stands out for its low computational and communication complexity,
suitable for smart contract implementation. It provides strong game-theoretic
incentives for honesty and is robust against adversaries, ensuring no increase
in election chances through dishonesty. The protocol guarantees that each
honest player has at least a 1/n chance of winning, irrespective of adversary
manipulation among the other n-1 participants. PureLottery can also address
related problems like participant ranking, electing multiple leaders, and
leader aversion, showcasing its versatility across various applications,
including lotteries and blockchain protocols. An open-source implementation is
made available for public use.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17463" title="Abstract">arXiv:2402.17463</a> [<a href="/pdf/2402.17463" title="Download PDF">pdf</a>, <a href="/format/2402.17463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training-Free Long-Context Scaling of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+C">Chenxin An</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shansan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The ability of Large Language Models (LLMs) to process and generate coherent
text is markedly weakened when the number of input tokens exceeds their
pretraining length. Given the expensive overhead of finetuning large-scale
models with longer sequences, we propose Dual Chunk Attention (DCA), which
enables Llama2 70B to support context windows of more than 100k tokens without
continual training. By decomposing the attention computation for long sequences
into chunk-based modules, DCA manages to effectively capture the relative
positional information of tokens within the same chunk (Intra-Chunk) and across
distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash
Attention. In addition to its impressive extrapolation capability, DCA achieves
performance on practical long-context tasks that is comparable to or even
better than that of finetuned models. When compared with proprietary models,
our training-free 70B model attains 94% of the performance of gpt-3.5-16k,
indicating it is a viable open-source alternative. All code and data used in
this work are released at \url{https://github.com/HKUNLP/ChunkLlama}.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17464" title="Abstract">arXiv:2402.17464</a> [<a href="/pdf/2402.17464" title="Download PDF">pdf</a>, <a href="/format/2402.17464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bi&#x27;an Du</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+R">Renjie Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Generative 3D part assembly involves understanding part relationships and
predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work
often focus on the geometry of individual parts, neglecting part-whole
hierarchies of objects. Leveraging two key observations: 1) super-part poses
provide strong hints about part poses, and 2) predicting super-part poses is
easier due to fewer superparts, we propose a part-whole-hierarchy message
passing network for efficient 3D part assembly. We first introduce super-parts
by grouping geometrically similar parts without any semantic labels. Then we
employ a part-whole hierarchical encoder, wherein a super-part encoder predicts
latent super-part poses based on input parts. Subsequently, we transform the
point cloud using the latent poses, feeding it to the part encoder for
aggregating super-part information and reasoning about part relationships to
predict all part poses. In training, only ground-truth part poses are required.
During inference, the predicted latent poses of super-parts enhance
interpretability. Experimental results on the PartNet dataset show that our
method achieves state-of-the-art performance in part and connectivity accuracy
and enables an interpretable hierarchical part assembly.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17465" title="Abstract">arXiv:2402.17465</a> [<a href="/pdf/2402.17465" title="Download PDF">pdf</a>, <a href="/format/2402.17465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model X-ray:Detect Backdoored Models via Decision Boundary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yanghao Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Ting Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep neural networks (DNNs) have revolutionized various industries, leading
to the rise of Machine Learning as a Service (MLaaS). In this paradigm,
well-trained models are typically deployed through APIs. However, DNNs are
susceptible to backdoor attacks, which pose significant risks to their
applications. This vulnerability necessitates a method for users to ascertain
whether an API is compromised before usage. Although many backdoor detection
methods have been developed, they often operate under the assumption that the
defender has access to specific information such as details of the attack, soft
predictions from the model API, and even the knowledge of the model parameters,
limiting their practicality in MLaaS scenarios. To address it, in this paper,
we begin by presenting an intriguing observation: the decision boundary of the
backdoored model exhibits a greater degree of closeness than that of the clean
model. Simultaneously, if only one single label is infected, a larger portion
of the regions will be dominated by the attacked label. Building upon this
observation, we propose Model X-ray, a novel backdoor detection approach for
MLaaS through the analysis of decision boundaries. Model X-ray can not only
identify whether the target API is infected by backdoor attacks but also
determine the target attacked label under the all-to-one attack strategy.
Importantly, it accomplishes this solely by the hard prediction of clean
inputs, regardless of any assumptions about attacks and prior knowledge of the
training details of the model. Extensive experiments demonstrated that Model
X-ray can be effective for MLaaS across diverse backdoor attacks, datasets, and
architectures.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17466" title="Abstract">arXiv:2402.17466</a> [<a href="/pdf/2402.17466" title="Download PDF">pdf</a>, <a href="/ps/2402.17466" title="Download PostScript">ps</a>, <a href="/format/2402.17466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Estimation and Control for LTI systems under Finite-Time  Agreement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fioravanti%2C+C">Camilla Fioravanti</a>, 
<a href="/search/eess?searchtype=author&query=Makridis%2C+E">Evagoras Makridis</a>, 
<a href="/search/eess?searchtype=author&query=Oliva%2C+G">Gabriele Oliva</a>, 
<a href="/search/eess?searchtype=author&query=Vrakopoulou%2C+M">Maria Vrakopoulou</a>, 
<a href="/search/eess?searchtype=author&query=Charalambous%2C+T">Themistoklis Charalambous</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper considers a strongly connected network of agents, each capable of
partially observing and controlling a discrete-time linear time-invariant (LTI)
system that is jointly observable and controllable. Additionally, agents
collaborate to achieve a shared estimated state, computed as the average of
their local state estimates. Recent studies suggest that increasing the number
of average consensus steps between state estimation updates allows agents to
choose from a wider range of state feedback controllers, thereby potentially
enhancing control performance. However, such approaches require that agents
know the input matrices of all other nodes, and the selection of control gains
is, in general, centralized. Motivated by the limitations of such approaches,
we propose a new technique where: (i) estimation and control gain design is
fully distributed and finite-time, and (ii) agent coordination involves a
finite-time exact average consensus algorithm, allowing arbitrary selection of
estimation convergence rate despite the estimator's distributed nature. We
verify our methodology's effectiveness using illustrative numerical
simulations.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17467" title="Abstract">arXiv:2402.17467</a> [<a href="/pdf/2402.17467" title="Download PDF">pdf</a>, <a href="/format/2402.17467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Processing Methods for Symbolic Music Generation and  Information Retrieval: a Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+D">Dinh-Viet-Toan Le</a>, 
<a href="/search/cs?searchtype=author&query=Bigo%2C+L">Louis Bigo</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+M">Mikaela Keller</a>, 
<a href="/search/cs?searchtype=author&query=Herremans%2C+D">Dorien Herremans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Several adaptations of Transformers models have been developed in various
domains since its breakthrough in Natural Language Processing (NLP). This trend
has spread into the field of Music Information Retrieval (MIR), including
studies processing music data. However, the practice of leveraging NLP tools
for symbolic music data is not novel in MIR. Music has been frequently compared
to language, as they share several similarities, including sequential
representations of text and music. These analogies are also reflected through
similar tasks in MIR and NLP. This survey reviews NLP methods applied to
symbolic music generation and information retrieval studies following two axes.
We first propose an overview of representations of symbolic music adapted from
natural language sequential representations. Such representations are designed
by considering the specificities of symbolic music. These representations are
then processed by models. Such models, possibly originally developed for text
and adapted for symbolic music, are trained on various tasks. We describe these
models, in particular deep learning models, through different prisms,
highlighting music-specialized mechanisms. We finally present a discussion
surrounding the effective use of NLP tools for symbolic music data. This
includes technical issues regarding NLP methods and fundamental differences
between text and music, which may open several doors for further research into
more effectively adapting NLP tools to symbolic MIR.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17470" title="Abstract">arXiv:2402.17470</a> [<a href="/pdf/2402.17470" title="Download PDF">pdf</a>, <a href="/format/2402.17470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bit Distribution Study and Implementation of Spatial Quality Map in the  JPEG-AI Standardization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Panqi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jue Mao</a>, 
<a href="/search/cs?searchtype=author&query=Koyuncu%2C+E">Esin Koyuncu</a>, 
<a href="/search/cs?searchtype=author&query=Koyuncu%2C+A+B">A. Burakhan Koyuncu</a>, 
<a href="/search/cs?searchtype=author&query=Solovyev%2C+T">Timofey Solovyev</a>, 
<a href="/search/cs?searchtype=author&query=Karabutov%2C+A">Alexander Karabutov</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Alshina%2C+E">Elena Alshina</a>, 
<a href="/search/cs?searchtype=author&query=Kaup%2C+A">Andre Kaup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Currently, there is a high demand for neural network-based image compression
codecs. These codecs employ non-linear transforms to create compact bit
representations and facilitate faster coding speeds on devices compared to the
hand-crafted transforms used in classical frameworks. The scientific and
industrial communities are highly interested in these properties, leading to
the standardization effort of JPEG-AI. The JPEG-AI verification model has been
released and is currently under development for standardization. Utilizing
neural networks, it can outperform the classic codec VVC intra by over 10%
BD-rate operating at base operation point. Researchers attribute this success
to the flexible bit distribution in the spatial domain, in contrast to VVC
intra's anchor that is generated with a constant quality point. However, our
study reveals that VVC intra displays a more adaptable bit distribution
structure through the implementation of various block sizes. As a result of our
observations, we have proposed a spatial bit allocation method to optimize the
JPEG-AI verification model's bit distribution and enhance the visual quality.
Furthermore, by applying the VVC bit distribution strategy, the objective
performance of JPEG-AI verification mode can be further improved, resulting in
a maximum gain of 0.45 dB in PSNR-Y.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17472" title="Abstract">arXiv:2402.17472</a> [<a href="/pdf/2402.17472" title="Download PDF">pdf</a>, <a href="/format/2402.17472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fraud Detection with Binding Global and Local Relational Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haolin Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shuyang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lifeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S">Siyuan Du</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+G">Guangnan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+H">Hongfeng Chai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review for SIGKDD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph Neural Network has been proved to be effective for fraud detection for
its capability to encode node interaction and aggregate features in a holistic
view. Recently, Transformer network with great sequence encoding ability, has
also outperformed other GNN-based methods in literatures. However, both
GNN-based and Transformer-based networks only encode one perspective of the
whole graph, while GNN encodes global features and Transformer network encodes
local ones. Furthermore, previous works ignored encoding global interaction
features of the heterogeneous graph with separate networks, thus leading to
suboptimal performance. In this work, we present a novel framework called
Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds
local and global features into a target node. The simple yet effective network
applies a modified GAGA module where each transformer layer is followed by a
cross-relation aggregation layer, to encode local embeddings and node
interactions across different relations. Apart from the Transformer-based
network, we further introduce a Relation-Aware GNN module to learn global
embeddings, which is later merged into the local embeddings by an attention
fusion module and a skip connection. Extensive experiments on two popular
public datasets and an industrial dataset demonstrate that RAGFormer achieves
the state-of-the-art performance. Substantial analysis experiments validate the
effectiveness of each submodule of RAGFormer and its high efficiency in
utilizing small-scale data and low hyper-parameter sensitivity.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17478" title="Abstract">arXiv:2402.17478</a> [<a href="/pdf/2402.17478" title="Download PDF">pdf</a>, <a href="/format/2402.17478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda  Spans in News Articles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasanain%2C+M">Maram Hasanain</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Fatema Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+F">Firoj Alam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a full paper at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The use of propaganda has spiked on mainstream and social media, aiming to
manipulate or mislead users. While efforts to automatically detect propaganda
techniques in textual, visual, or multimodal content have increased, most of
them primarily focus on English content. The majority of the recent initiatives
targeting medium to low-resource languages produced relatively small annotated
datasets, with a skewed distribution, posing challenges for the development of
sophisticated propaganda detection models. To address this challenge, we
carefully develop the largest propaganda dataset to date, ArPro, comprised of
8K paragraphs from newspaper articles, labeled at the text span level following
a taxonomy of 23 propagandistic techniques. Furthermore, our work offers the
first attempt to understand the performance of large language models (LLMs),
using GPT-4, for fine-grained propaganda detection from text. Results showed
that GPT-4's performance degrades as the task moves from simply classifying a
paragraph as propagandistic or not, to the fine-grained task of detecting
propaganda techniques and their manifestation in text. Compared to models
fine-tuned on the dataset for propaganda detection at different classification
granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a
dataset consisting of six other languages for span detection, and results
suggest that the model struggles with the task across languages. Our dataset
and resources will be released to the community.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17481" title="Abstract">arXiv:2402.17481</a> [<a href="/pdf/2402.17481" title="Download PDF">pdf</a>, <a href="/format/2402.17481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Schemes for 3-Wave Kinetic Equations: A Complete Treatment of  the Collision Operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Walton%2C+S">Steven Walton</a>, 
<a href="/search/math?searchtype=author&query=Tran%2C+M">Minh-Binh Tran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In our previous work, numerical schemes for a simplified version of 3-wave
kinetic equations, in which only the simple forward-cascade terms of the
collision operators are kept, have been successfully designed, especially to
capture the long time dynamics of the equation given the multiple blow-up time
phenomenon. In this second work in the series, we propose numerical treatments
for the complete 3-wave kinetic equations, in which the complete, much more
complicated collision operators are fully considered based on a novel
conservative form of the equation. We then derive an implicit finite volume
scheme to solve the equation. The new discretization uses an adaptive
time-stepping method which allows for the simulations to be carried to very
long times. Our computed solutions are compared with previously derived
long-time asymptotic estimates for the decay rate of total energy of
time-dependent solutions of 3-wave kinetic equations and found to be in
excellent agreement.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17482" title="Abstract">arXiv:2402.17482</a> [<a href="/pdf/2402.17482" title="Download PDF">pdf</a>, <a href="/ps/2402.17482" title="Download PostScript">ps</a>, <a href="/format/2402.17482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Classification of Phonetic Segments in Child Speech Using Raw  Ultrasound Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ani%2C+S+A">Saja Al Ani</a>, 
<a href="/search/cs?searchtype=author&query=Cleland%2C+J">Joanne Cleland</a>, 
<a href="/search/cs?searchtype=author&query=Zoha%2C+A">Ahmed Zoha</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 17th International Joint Conference on
  Biomedical Engineering Systems and Technologies - Volume 1: BIOIMAGING, 2024,
  pages 326-331
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speech sound disorder (SSD) is defined as a persistent impairment in speech
sound production leading to reduced speech intelligibility and hindered verbal
communication. Early recognition and intervention of children with SSD and
timely referral to speech and language therapists (SLTs) for treatment are
crucial. Automated detection of speech impairment is regarded as an efficient
method for examining and screening large populations. This study focuses on
advancing the automatic diagnosis of SSD in early childhood by proposing a
technical solution that integrates ultrasound tongue imaging (UTI) with
deep-learning models. The introduced FusionNet model combines UTI data with the
extracted texture features to classify UTI. The overarching aim is to elevate
the accuracy and efficiency of UTI analysis, particularly for classifying
speech sounds associated with SSD. This study compared the FusionNet approach
with standard deep-learning methodologies, highlighting the excellent
improvement results of the FusionNet model in UTI classification and the
potential of multi-learning in improving UTI classification in speech therapy
clinics.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17483" title="Abstract">arXiv:2402.17483</a> [<a href="/pdf/2402.17483" title="Download PDF">pdf</a>, <a href="/format/2402.17483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera  Joint Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+T">Tao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guangrun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+Y">Yixing Lao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Liang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kaicheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Neural implicit fields have been a de facto standard in novel view synthesis.
Recently, there exist some methods exploring fusing multiple modalities within
a single field, aiming to share implicit features from different modalities to
enhance reconstruction performance. However, these modalities often exhibit
misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely
affect another, like camera performance, and vice versa. In this work, we
conduct comprehensive analyses on the multimodal implicit field of LiDAR-camera
joint synthesis, revealing the underlying issue lies in the misalignment of
different sensors. Furthermore, we introduce AlignMiF, a geometrically aligned
multimodal implicit field with two proposed modules: Geometry-Aware Alignment
(GAA) and Shared Geometry Initialization (SGI). These modules effectively align
the coarse geometry across different modalities, significantly enhancing the
fusion process between LiDAR and camera data. Through extensive experiments
across various datasets and scenes, we demonstrate the effectiveness of our
approach in facilitating better interaction between LiDAR and camera modalities
within a unified neural field. Specifically, our proposed AlignMiF, achieves
remarkable improvement over recent implicit fusion methods (+2.01 and +3.11
image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses
single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer
Distance on the respective datasets).
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17485" title="Abstract">arXiv:2402.17485</a> [<a href="/pdf/2402.17485" title="Download PDF">pdf</a>, <a href="/format/2402.17485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with  Audio2Video Diffusion Model under Weak Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+L">Linrui Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bo%2C+L">Liefeng Bo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, we tackle the challenge of enhancing the realism and
expressiveness in talking head video generation by focusing on the dynamic and
nuanced relationship between audio cues and facial movements. We identify the
limitations of traditional techniques that often fail to capture the full
spectrum of human expressions and the uniqueness of individual facial styles.
To address these issues, we propose EMO, a novel framework that utilizes a
direct audio-to-video synthesis approach, bypassing the need for intermediate
3D models or facial landmarks. Our method ensures seamless frame transitions
and consistent identity preservation throughout the video, resulting in highly
expressive and lifelike animations. Experimental results demonsrate that EMO is
able to produce not only convincing speaking videos but also singing videos in
various styles, significantly outperforming existing state-of-the-art
methodologies in terms of expressiveness and realism.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17486" title="Abstract">arXiv:2402.17486</a> [<a href="/pdf/2402.17486" title="Download PDF">pdf</a>, <a href="/format/2402.17486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MGE: A Training-Free and Efficient Model Generation and Enhancement  Scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Z">Zeshan Pang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuliang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xuehu Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">To provide a foundation for the research of deep learning models, the
construction of model pool is an essential step. This paper proposes a
Training-Free and Efficient Model Generation and Enhancement Scheme (MGE). This
scheme primarily considers two aspects during the model generation process: the
distribution of model parameters and model performance. Experiments result
shows that generated models are comparable to models obtained through normal
training, and even superior in some cases. Moreover, the time consumed in
generating models accounts for only 1\% of the time required for normal model
training. More importantly, with the enhancement of Evolution-MGE, generated
models exhibits competitive generalization ability in few-shot tasks. And the
behavioral dissimilarity of generated models has the potential of adversarial
defense.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17487" title="Abstract">arXiv:2402.17487</a> [<a href="/pdf/2402.17487" title="Download PDF">pdf</a>, <a href="/format/2402.17487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Panqi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Koyuncu%2C+A+B">A. Burakhan Koyuncu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jue Mao</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Z">Ze Cui</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tiansheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Solovyev%2C+T">Timofey Solovyev</a>, 
<a href="/search/cs?searchtype=author&query=Karabutov%2C+A">Alexander Karabutov</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Alshina%2C+E">Elena Alshina</a>, 
<a href="/search/cs?searchtype=author&query=Kaup%2C+A">Andre Kaup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at (IEEE) PCS 2024; 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The research on neural network (NN) based image compression has shown
superior performance compared to classical compression frameworks. Unlike the
hand-engineered transforms in the classical frameworks, NN-based models learn
the non-linear transforms providing more compact bit representations, and
achieve faster coding speed on parallel devices over their classical
counterparts. Those properties evoked the attention of both scientific and
industrial communities, resulting in the standardization activity JPEG-AI. The
verification model for the standardization process of JPEG-AI is already in
development and has surpassed the advanced VVC intra codec. To generate
reconstructed images with the desired bits per pixel and assess the BD-rate
performance of both the JPEG-AI verification model and VVC intra, bit rate
matching is employed. However, the current state of the JPEG-AI verification
model experiences significant slowdowns during bit rate matching, resulting in
suboptimal performance due to an unsuitable model. The proposed methodology
offers a gradual algorithmic optimization for matching bit rates, resulting in
a fourfold acceleration and over 1% improvement in BD-rate at the base
operation point. At the high operation point, the acceleration increases up to
sixfold.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17488" title="Abstract">arXiv:2402.17488</a> [<a href="/pdf/2402.17488" title="Download PDF">pdf</a>, <a href="/format/2402.17488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity Assessment of Analog Security Primitives Using the Disentropy  of Autocorrelation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jimenez%2C+P">Paul Jimenez</a>, 
<a href="/search/cs?searchtype=author&query=Cardoso%2C+R">Raphael Cardoso</a>, 
<a href="/search/cs?searchtype=author&query=de+Queiroz%2C+M+G">Maur&#xec;cio Gomes de Queiroz</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+M">Mohab Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Marchand%2C+C">C&#xe9;dric Marchand</a>, 
<a href="/search/cs?searchtype=author&query=Letartre%2C+X">Xavier Letartre</a>, 
<a href="/search/cs?searchtype=author&query=Pavanello%2C+F">Fabio Pavanello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The study of regularity in signals can be of great importance, typically in
medicine to analyse electrocardiogram (ECG) or electromyography (EMG) signals,
but also in climate studies, finance or security. In this work we focus on
security primitives such as Physical Unclonable Functions (PUFs) or
Pseudo-Random Number Generators (PRNGs). Such primitives must have a high level
of complexity or entropy in their responses to guarantee enough security for
their applications. There are several ways of assessing the complexity of their
responses, especially in the binary domain. With the development of analog PUFs
such as optical (photonic) PUFs, it would be useful to be able to assess their
complexity in the analog domain when designing them, for example, before
converting analog signals into binary. In this numerical study, we decided to
explore the potential of the disentropy of autocorrelation as a measure of
complexity for security primitives as PUFs or PRNGs with analog output or
responses. We compare this metric to others used to assess regularities in
analog signals such as Approximate Entropy (ApEn) and Fuzzy Entropy (FuzEn). We
show that the disentropy of autocorrelation is able to differentiate between
well-known PRNGs and non-optimised or bad PRNGs in the analog and binary domain
with a better contrast than ApEn and FuzEn. Next, we show that the disentropy
of autocorrelation is able to detect small patterns injected in PUFs responses
and then we applied it to photonic PUFs simulations.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17489" title="Abstract">arXiv:2402.17489</a> [<a href="/pdf/2402.17489" title="Download PDF">pdf</a>, <a href="/format/2402.17489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSRESF: Sensitivity-aware Single-particle Radiation Effects Simulation  Framework in SoC Platforms based on SVM Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Meng Liu</a> (1), 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a> (1), 
<a href="/search/cs?searchtype=author&query=Xiao%2C+F">Fei Xiao</a> (1), 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruijie Wang</a> (1), 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chunxue Liu</a> (2), 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a> (2) ((1) Faculty of Information Technology, School of Microelectronics, Beijing University of Technology, Beijing, China, (2) Beijing Microelectronics Technology Institute, Beijing, China)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 61th ACM/IEEE Design Automation conference (DAC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">The ever-expanding scale of integrated circuits has brought about a
significant rise in the design risks associated with radiation-resistant
integrated circuit chips. Traditional single-particle experimental methods,
with their iterative design approach, are increasingly ill-suited for the
challenges posed by large-scale integrated circuits. In response, this article
introduces a novel sensitivity-aware single-particle radiation effects
simulation framework tailored for System-on-Chip platforms. Based on SVM
algorithm we have implemented fast finding and classification of sensitive
circuit nodes. Additionally, the methodology automates soft error analysis
across the entire software stack. The study includes practical experiments
focusing on RISC-V architecture, encompassing core components, buses, and
memory systems. It culminates in the establishment of databases for Single
Event Upsets (SEU) and Single Event Transients (SET), showcasing the practical
efficacy of the proposed methodology in addressing radiation-induced challenges
at the scale of contemporary integrated circuits. Experimental results have
shown up to 12.78X speed-up on the basis of achieving 94.58% accuracy.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17490" title="Abstract">arXiv:2402.17490</a> [<a href="/pdf/2402.17490" title="Download PDF">pdf</a>, <a href="/ps/2402.17490" title="Download PostScript">ps</a>, <a href="/format/2402.17490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Mechanical Turkness: Tactical Media Art and the Critique of  Corporate AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grba%2C+D">Dejan Grba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Matthes, J\"org, Damian Trilling, Ljubi\v{s}a Boji\'c and Simona \v{Z}iki\'c, eds. 2024. Navigating the Digital Age: An In-Depth Exploration into the Intersection of Modern Technologies and Societal Transformation. Vienna and Belgrade: Institute for Philosophy and Social Theory and University of Belgrade and Department of Communication, University of Vienna
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The extensive industrialization of artificial intelligence (AI) since the
mid-2010s has increasingly motivated artists to address its economic and
sociopolitical consequences. In this chapter, I discuss interrelated art
practices that thematize creative agency, crowdsourced labor, and delegated
artmaking to reveal the social rootage of AI technologies and underline the
productive human roles in their development. I focus on works whose poetic
features indicate broader issues of contemporary AI-influenced science,
technology, economy, and society. By exploring the conceptual, methodological,
and ethical aspects of their effectiveness in disrupting the political regime
of corporate AI, I identify several problems that affect their tactical impact
and outline potential avenues for tackling the challenges and advancing the
field.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17493" title="Abstract">arXiv:2402.17493</a> [<a href="/pdf/2402.17493" title="Download PDF">pdf</a>, <a href="/ps/2402.17493" title="Download PostScript">ps</a>, <a href="/format/2402.17493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prescribing Large Language Models for Perioperative Care: What&#x27;s The  Right Dose for Pre-trained Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Bing Xue</a>, 
<a href="/search/cs?searchtype=author&query=Alba%2C+C">Charles Alba</a>, 
<a href="/search/cs?searchtype=author&query=Abraham%2C+J">Joanna Abraham</a>, 
<a href="/search/cs?searchtype=author&query=Kannampallil%2C+T">Thomas Kannampallil</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chenyang Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Supplemental file available at: <a href="http://tinyurl.com/mszmjna9">this http URL</a>; models publicly available at: <a href="https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT">this https URL</a> AND <a href="https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Postoperative risk predictions can inform effective perioperative care
management and planning. We aimed to assess whether clinical large language
models (LLMs) can predict postoperative risks using clinical texts with various
training strategies. The main cohort involved 84,875 records from Barnes Jewish
Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth
Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up
based on the length of postoperative ICU stay less than 7 days. For the BJH
dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and
pneumonia. Three domain adaptation and finetuning strategies were implemented
for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives;
incorporating labels with semi-supervised fine-tuning; and foundational
modelling through multi-task learning. Model performance was compared using the
area under the receiver operating characteristic curve (AUROC) and the area
under the precision recall curve (AUPRC) for classification tasks, and mean
squared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed
traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and
14% for AUPRC. Adapting models further improved performance: (1)
self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2)
semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to
self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and
2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical
LLMs offer opportunities for postoperative risk predictions in unforeseen data,
with peaks in foundational models indicating the potential of task-agnostic
learning towards the generalizability of LLMs in perioperative care.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17495" title="Abstract">arXiv:2402.17495</a> [<a href="/pdf/2402.17495" title="Download PDF">pdf</a>, <a href="/ps/2402.17495" title="Download PostScript">ps</a>, <a href="/format/2402.17495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Unwanted Dissemination of Science: The Usage of Academic Articles as  Ammunition in Contested Discursive Arenas on Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Richard Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Horv%C3%A1t%2C+E">Em&#x151;ke-&#xc1;gnes Horv&#xe1;t</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 tables, submitted to CSCW '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Twitter is a common site of offensive language. Prior literature has shown
that the emotional content of tweets can heavily impact their diffusion when
discussing political topics. We extend prior work to look at offensive tweets
that link to academic articles. Using a mixed methods approach, we identify
three findings: firstly, offensive language is common in tweets that refer to
academic articles, and vary widely by subject matter. Secondly, discourse
analysis reveals that offensive tweets commonly use academic articles to
promote or attack political ideologies. Lastly, we show that offensive tweets
reach a smaller audience than their non-offensive counterparts. Our analysis of
these offensive tweets reveal how academic articles are being shared on Twitter
not for the sake of disseminating new knowledge, but rather to as argumentative
tools in controversial and combative discourses.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17496" title="Abstract">arXiv:2402.17496</a> [<a href="/pdf/2402.17496" title="Download PDF">pdf</a>, <a href="/format/2402.17496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emotional Voice Messages (EMOVOME) database: emotion recognition in  spontaneous voice messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaragoz%C3%A1%2C+L+G">Luc&#xed;a G&#xf3;mez Zaragoz&#xe1;</a> (1), 
<a href="/search/cs?searchtype=author&query=del+Amor%2C+R">Roc&#xed;o del Amor</a> (1), 
<a href="/search/cs?searchtype=author&query=Vargas%2C+E+P">Elena Parra Vargas</a> (1), 
<a href="/search/cs?searchtype=author&query=Naranjo%2C+V">Valery Naranjo</a> (1), 
<a href="/search/cs?searchtype=author&query=Raya%2C+M+A">Mariano Alca&#xf1;iz Raya</a> (1), 
<a href="/search/cs?searchtype=author&query=Mar%C3%ADn-Morales%2C+J">Javier Mar&#xed;n-Morales</a> (1) ((1) HUMAN-tech Institute, Universitat Polit&#xe8;nica de Val&#xe8;ncia, Valencia, Spain)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, submitted to Scientific Data
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing
999 audio messages from real conversations on a messaging app from 100 Spanish
speakers, gender balanced. Voice messages were produced in-the-wild conditions
before participants were recruited, avoiding any conscious bias due to
laboratory environment. Audios were labeled in valence and arousal dimensions
by three non-experts and two experts, which were then combined to obtain a
final label per dimension. The experts also provided an extra label
corresponding to seven emotion categories. To set a baseline for future
investigations using EMOVOME, we implemented emotion recognition models using
both speech and audio transcriptions. For speech, we used the standard eGeMAPS
feature set and support vector machines, obtaining 49.27% and 44.71% unweighted
accuracy for valence and arousal respectively. For text, we fine-tuned a
multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for
valence and arousal respectively. This database will significantly contribute
to research on emotion recognition in the wild, while also providing a unique
natural and freely accessible resource for Spanish.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17497" title="Abstract">arXiv:2402.17497</a> [<a href="/pdf/2402.17497" title="Download PDF">pdf</a>, <a href="/format/2402.17497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Ruiyang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Considering the limited internal parametric knowledge, retrieval-augmented
generation (RAG) has been widely used to extend the knowledge scope of large
language models (LLMs). Despite the extensive efforts on RAG research, in
existing methods, LLMs cannot precisely assess the relevance of retrieved
documents, thus likely leading to misleading or even incorrect utilization of
external knowledge (i.e., retrieved documents). To address this issue, in this
paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for
open-domain question answering (QA). As the key motivation, we aim to enhance
the self-awareness of source relevance for LLMs, so as to adaptively utilize
external knowledge in RAG systems. Specially, we develop a new architecture for
LLM based RAG system, by incorporating a specially designed rank head that
precisely assesses the relevance of retrieved documents. Furthermore, we
propose an improved training method based on bi-granularity relevance fusion
and noise-resistant training. By combining the improvements in both
architecture and training, our proposed REAR can better utilize external
knowledge by effectively perceiving the relevance of retrieved documents.
Experiments on four open-domain QA tasks show that REAR significantly
outperforms previous a number of competitive RAG approaches. Our code and data
can be accessed at https://github.com/RUCAIBox/REAR.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17501" title="Abstract">arXiv:2402.17501</a> [<a href="/pdf/2402.17501" title="Download PDF">pdf</a>, <a href="/format/2402.17501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intensive Care as One Big Sequence Modeling Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liventsev%2C+V">Vadim Liventsev</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+T">Tobias Fritz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement Learning in Healthcare is typically concerned with narrow
self-contained tasks such as sepsis prediction or anesthesia control. However,
previous research has demonstrated the potential of generalist models (the
prime example being Large Language Models) to outperform task-specific
approaches due to their capability for implicit transfer learning. To enable
training of foundation models for Healthcare as well as leverage the
capabilities of state of the art Transformer architectures, we propose the
paradigm of Healthcare as Sequence Modeling, in which interaction between the
patient and the healthcare provider is represented as an event stream and tasks
like diagnosis and treatment selection are modeled as prediction of future
events in the stream. To explore this paradigm experimentally we develop
MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous
clinical records from MIMIC-IV dataset into a uniform event stream format,
train a baseline model and explore its capabilities.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17502" title="Abstract">arXiv:2402.17502</a> [<a href="/pdf/2402.17502" title="Download PDF">pdf</a>, <a href="/format/2402.17502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedLPPA: Learning Personalized Prompt and Aggregation for Federated  Weakly-supervised Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Li Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiewei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Pujin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhiyuan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K+K+Y">Kenneth K. Y. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Federated learning (FL) effectively mitigates the data silo challenge brought
about by policies and privacy concerns, implicitly harnessing more data for
deep model training. However, traditional centralized FL models grapple with
diverse multi-center data, especially in the face of significant data
heterogeneity, notably in medical contexts. In the realm of medical image
segmentation, the growing imperative to curtail annotation costs has amplified
the importance of weakly-supervised techniques which utilize sparse annotations
such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate
diverse annotation formats across different sites, which research topic remains
under-investigated. In such context, we propose a novel personalized FL
framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage
heterogeneous weak supervision for medical image segmentation. In FedLPPA, a
learnable universal knowledge prompt is maintained, complemented by multiple
learnable personalized data distribution prompts and prompts representing the
supervision sparsity. Integrated with sample features through a dual-attention
mechanism, those prompts empower each local task decoder to adeptly adjust to
both the local distribution and the supervision form. Concurrently, a
dual-decoder strategy, predicated on prompt similarity, is introduced for
enhancing the generation of pseudo-labels in weakly-supervised learning,
alleviating overfitting and noise accumulation inherent to local data, while an
adaptable aggregation method is employed to customize the task decoder on a
parameter-wise basis. Extensive experiments on three distinct medical image
segmentation tasks involving different modalities underscore the superiority of
FedLPPA, with its efficacy closely parallels that of fully supervised
centralized training. Our code and data will be available.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17504" title="Abstract">arXiv:2402.17504</a> [<a href="/pdf/2402.17504" title="Download PDF">pdf</a>, <a href="/format/2402.17504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Estimation of Relative Pose for UAVs Using a Dual-Channel  Feature Association
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoying Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Wei Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Leveraging multiple cameras on Unmanned Aerial Vehicles (UAVs) to form a
variable-baseline stereo camera for collaborative perception is highly
promising. The critical steps include high-rate cross-camera feature
association and frame-rate relative pose estimation of multiple UAVs. To
accelerate the feature association rate to match the frame rate, we propose a
dual-channel structure to decouple the time-consuming feature detection and
match from the high-rate image stream. The novel design of periodic guidance
and fast prediction effectively utilizes each image frame to achieve a
frame-rate feature association. Real-world experiments are executed using
SuperPoint and SuperGlue on the NVIDIA NX 8G platform with a 30 Hz image
stream. Using single-channel SuperPoint and SuperGlue can only achieve 13 Hz
feature association. The proposed dual-channel method can improve the rate of
feature association from 13 Hz to 30 Hz, supporting the frame-rate requirement.
To accommodate the proposed feature association, we develop a Multi-State
Constrained Kalman Filter (MSCKF)-based relative pose estimator in the back-end
by fusing the local odometry from two UAVs together with the measurements of
common features. Experiments show that the dual-channel feature association
improves the rate of visual observation and enhances the real-time performance
of back-end estimator compared to the existing methods. Video -
https://youtu.be/UBAR1iP0GPk Supplementary video - https://youtu.be/nPq8EpVzJZM
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17505" title="Abstract">arXiv:2402.17505</a> [<a href="/pdf/2402.17505" title="Download PDF">pdf</a>, <a href="/format/2402.17505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BASES: Large-scale Web Search User Simulation with Large Language Model  based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Ruiyang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+P">Peng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yingqi Qu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hua Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haifeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Due to the excellent capacities of large language models (LLMs), it becomes
feasible to develop LLM-based agents for reliable user simulation. Considering
the scarcity and limit (e.g., privacy issues) of real user data, in this paper,
we conduct large-scale user simulation for web search, to improve the analysis
and modeling of user search behavior. Specially, we propose BASES, a novel user
simulation framework with LLM-based agents, designed to facilitate
comprehensive simulations of web search user behaviors. Our simulation
framework can generate unique user profiles at scale, which subsequently leads
to diverse search behaviors. To demonstrate the effectiveness of BASES, we
conduct evaluation experiments based on two human benchmarks in both Chinese
and English, demonstrating that BASES can effectively simulate large-scale
human-like search behaviors. To further accommodate the research on web search,
we develop WARRIORS, a new large-scale dataset encompassing web search user
behaviors, including both Chinese and English versions, which can greatly
bolster research in the field of information retrieval. Our code and data will
be publicly released soon.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17507" title="Abstract">arXiv:2402.17507</a> [<a href="/pdf/2402.17507" title="Download PDF">pdf</a>, <a href="/format/2402.17507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Multi-Head Self-Attention with Linear Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Hankyul Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+J">Jongbin Ryu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose an efficient interactive method for multi-head self-attention via
decomposition. For existing methods using multi-head self-attention, the
attention operation of each head is computed independently. However, we show
that the interactions between cross-heads of the attention matrix enhance the
information flow of the attention operation. Considering that the attention
matrix of each head can be seen as a feature of networks, it is beneficial to
establish connectivity between them to capture interactions better. However, a
straightforward approach to capture the interactions between the cross-heads is
computationally prohibitive as the complexity grows substantially with the high
dimension of an attention matrix. In this work, we propose an effective method
to decompose the attention operation into query- and key-less components. This
will result in a more manageable size for the attention matrix, specifically
for the cross-head interactions. Expensive experimental results show that the
proposed cross-head interaction approach performs favorably against existing
efficient attention methods and state-of-the-art backbone models.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17509" title="Abstract">arXiv:2402.17509</a> [<a href="/pdf/2402.17509" title="Download PDF">pdf</a>, <a href="/format/2402.17509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme Miscalibration and the Illusion of Adversarial Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raina%2C+V">Vyas Raina</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Samson Tan</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>, 
<a href="/search/cs?searchtype=author&query=Rawal%2C+A">Aditya Rawal</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+S">Sheng Zha</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Deep learning-based Natural Language Processing (NLP) models are vulnerable
to adversarial attacks, where small perturbations can cause a model to
misclassify. Adversarial Training (AT) is often used to increase model
robustness. However, we have discovered an intriguing phenomenon: deliberately
or accidentally miscalibrating models masks gradients in a way that interferes
with adversarial attack search methods, giving rise to an apparent increase in
robustness. We show that this observed gain in robustness is an illusion of
robustness (IOR), and demonstrate how an adversary can perform various forms of
test-time temperature calibration to nullify the aforementioned interference
and allow the adversarial attack to find adversarial examples. Hence, we urge
the NLP community to incorporate test-time temperature scaling into their
robustness evaluations to ensure that any observed gains are genuine. Finally,
we show how the temperature can be scaled during \textit{training} to improve
genuine robustness.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17510" title="Abstract">arXiv:2402.17510</a> [<a href="/pdf/2402.17510" title="Download PDF">pdf</a>, <a href="/format/2402.17510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demonstrating and Reducing Shortcuts in Vision-Language Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bleeker%2C+M">Maurits Bleeker</a>, 
<a href="/search/cs?searchtype=author&query=Hendriksen%2C+M">Mariya Hendriksen</a>, 
<a href="/search/cs?searchtype=author&query=Yates%2C+A">Andrew Yates</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Vision-language models (VLMs) mainly rely on contrastive training to learn
general-purpose representations of images and captions. We focus on the
situation when one image is associated with several captions, each caption
containing both information shared among all captions and unique information
per caption about the scene depicted in the image. In such cases, it is unclear
whether contrastive losses are sufficient for learning task-optimal
representations that contain all the information provided by the captions or
whether the contrastive learning setup encourages the learning of a simple
shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for
vision-language: a training and evaluation framework where we inject synthetic
shortcuts into image-text data. We show that contrastive VLMs trained from
scratch or fine-tuned with data containing these synthetic shortcuts mainly
learn features that represent the shortcut. Hence, contrastive losses are not
sufficient to learn task-optimal representations, i.e., representations that
contain all task-relevant information shared between the image and associated
captions. We examine two methods to reduce shortcut learning in our training
and evaluation framework: (i) latent target decoding and (ii) implicit feature
modification. We show empirically that both methods improve performance on the
evaluation task, but only partly reduce shortcut learning when training and
evaluating with our shortcut learning framework. Hence, we show the difficulty
and challenge of our shortcut learning framework for contrastive
vision-language representation learning.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17511" title="Abstract">arXiv:2402.17511</a> [<a href="/pdf/2402.17511" title="Download PDF">pdf</a>, <a href="/format/2402.17511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Mutual Information for Language Conditioned Skill Discovery  on Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+Z">Zhaoxun Ju</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fuchun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language-conditioned robot behavior plays a vital role in executing complex
tasks by associating human commands or instructions with perception and
actions. The ability to compose long-horizon tasks based on unconstrained
language instructions necessitates the acquisition of a diverse set of
general-purpose skills. However, acquiring inherent primitive skills in a
coupled and long-horizon environment without external rewards or human
supervision presents significant challenges. In this paper, we evaluate the
relationship between skills and language instructions from a mathematical
perspective, employing two forms of mutual information within the framework of
language-conditioned policy learning. To maximize the mutual information
between language and skills in an unsupervised manner, we propose an end-to-end
imitation learning approach known as Language Conditioned Skill Discovery
(LCSD). Specifically, we utilize vector quantization to learn discrete latent
skills and leverage skill sequences of trajectories to reconstruct high-level
semantic instructions. Through extensive experiments on language-conditioned
robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and
CALVIN, we demonstrate the superiority of our method over prior works. Our
approach exhibits enhanced generalization capabilities towards unseen tasks,
improved skill interpretability, and notably higher rates of task completion
success.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17512" title="Abstract">arXiv:2402.17512</a> [<a href="/pdf/2402.17512" title="Download PDF">pdf</a>, <a href="/format/2402.17512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Attention for Linear Time Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dolga%2C+R">Rares Dolga</a>, 
<a href="/search/cs?searchtype=author&query=Cobzarenco%2C+M">Marius Cobzarenco</a>, 
<a href="/search/cs?searchtype=author&query=Barber%2C+D">David Barber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The time complexity of the standard attention mechanism in a transformer
scales quadratically with the length of the sequence. We introduce a method to
reduce this to linear scaling with time, based on defining attention via latent
vectors. The method is readily usable as a drop-in replacement for the standard
attention mechanism. Our "Latte Transformer" model can be implemented for both
bidirectional and unidirectional tasks, with the causal version allowing a
recurrent implementation which is memory and time-efficient during inference of
language generation tasks. Whilst next token prediction scales linearly with
the sequence length for a standard transformer, a Latte Transformer requires
constant time to compute the next token. The empirical performance of our
method is comparable to standard attention, yet allows scaling to context
windows much larger than practical in standard attention.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17514" title="Abstract">arXiv:2402.17514</a> [<a href="/pdf/2402.17514" title="Download PDF">pdf</a>, <a href="/format/2402.17514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Unsupervised Crowd Counting and Localization with Adaptive  Resolution SAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Jia Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiangqiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A+B">Antoni B. Chan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The existing crowd counting models require extensive training data, which is
time-consuming to annotate. To tackle this issue, we propose a simple yet
effective crowd counting method by utilizing the Segment-Everything-Everywhere
Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to
generate pseudo-labels for training crowd counting models. However, our initial
investigation reveals that SEEM's performance in dense crowd scenes is limited,
primarily due to the omission of many persons in high-density areas. To
overcome this limitation, we propose an adaptive resolution SEEM to handle the
scale variations, occlusions, and overlapping of people within crowd scenes.
Alongside this, we introduce a robust localization method, based on Gaussian
Mixture Models, for predicting the head positions in the predicted people
masks. Given the mask and point pseudo-labels, we propose a robust loss
function, which is designed to exclude uncertain regions based on SEEM's
predictions, thereby enhancing the training process of the counting networks.
Finally, we propose an iterative method for generating pseudo-labels. This
method aims at improving the quality of the segmentation masks by identifying
more tiny persons in high-density regions, which are often missed in the first
pseudo-labeling stage. Overall, our proposed method achieves the best
unsupervised performance in crowd counting, while also being comparable results
to some supervised methods. This makes it a highly effective and versatile tool
for crowd counting, especially in situations where labeled data is not
available.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17516" title="Abstract">arXiv:2402.17516</a> [<a href="/pdf/2402.17516" title="Download PDF">pdf</a>, <a href="/format/2402.17516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QUCE: The Minimisation and Quantification of Path-Based Uncertainty for  Generative Counterfactual Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duell%2C+J">Jamie Duell</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Hsuan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Seisenberger%2C+M">Monika Seisenberger</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiuyi Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep Neural Networks (DNNs) stand out as one of the most prominent approaches
within the Machine Learning (ML) domain. The efficacy of DNNs has surged
alongside recent increases in computational capacity, allowing these approaches
to scale to significant complexities for addressing predictive challenges in
big data. However, as the complexity of DNN models rises, interpretability
diminishes. In response to this challenge, explainable models such as
Adversarial Gradient Integration (AGI) leverage path-based gradients provided
by DNNs to elucidate their decisions. Yet the performance of path-based
explainers can be compromised when gradients exhibit irregularities during
out-of-distribution path traversal. In this context, we introduce Quantified
Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate
out-of-distribution traversal by minimizing path uncertainty. QUCE not only
quantifies uncertainty when presenting explanations but also generates more
certain counterfactual examples. We showcase the performance of the QUCE method
by comparing it with competing methods for both path-based explanations and
generative counterfactual examples. The code repository for the QUCE method is
available at: https://github.com/jamie-duell/QUCE.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17517" title="Abstract">arXiv:2402.17517</a> [<a href="/pdf/2402.17517" title="Download PDF">pdf</a>, <a href="/format/2402.17517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-Noise Robust Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Na%2C+B">Byeonghu Na</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yeongmin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+H">HeeSun Bae</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+H">Jung Hyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+S+J">Se Jung Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wanmo Kang</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+I">Il-Chul Moon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Conditional diffusion models have shown remarkable performance in various
generative tasks, but training them requires large-scale datasets that often
contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to
condition mismatch and quality degradation of generated data. This paper
proposes Transition-aware weighted Denoising Score Matching (TDSM) for training
conditional diffusion models with noisy labels, which is the first study in the
line of diffusion models. The TDSM objective contains a weighted sum of score
networks, incorporating instance-wise and time-dependent label transition
probabilities. We introduce a transition-aware weight estimator, which
leverages a time-dependent noisy-label classifier distinctively customized to
the diffusion process. Through experiments across various datasets and noisy
label settings, TDSM improves the quality of generated samples aligned with
given conditions. Furthermore, our method improves generation performance even
on prevalent benchmark datasets, which implies the potential noisy labels and
their risk of generative model learning. Finally, we show the improved
performance of TDSM on top of conventional noisy label corrections, which
empirically proving its contribution as a part of label-noise robust generative
models. Our code is available at: https://github.com/byeonghu-na/tdsm.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17521" title="Abstract">arXiv:2402.17521</a> [<a href="/pdf/2402.17521" title="Download PDF">pdf</a>, <a href="/format/2402.17521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongcheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Dingkang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dingyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xingyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhikang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yingying Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Efficient downsampling plays a crucial role in point cloud learning,
particularly for large-scale 3D scenes. Existing downsampling methods either
require a huge computational burden or sacrifice fine-grained geometric
information. This paper presents an advanced sampler that achieves both high
accuracy and efficiency. The proposed method utilizes voxel-based sampling as a
foundation, but effectively addresses the challenges regarding voxel size
determination and the preservation of critical geometric cues. Specifically, we
propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the
reference of point-based downsampling ratio. This ensures the sampling results
exhibit a favorable distribution for comprehending various 3D objects or
scenes. Additionally, we introduce a network compatible with arbitrary voxel
sizes for sampling and feature extraction while maintaining high efficiency.
Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet
benchmarks with promising efficiency. Code will be available at
https://github.com/yhc2021/AVS-Net.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17524" title="Abstract">arXiv:2402.17524</a> [<a href="/pdf/2402.17524" title="Download PDF">pdf</a>, <a href="/format/2402.17524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Highway Discretionary Lane-change Decision and Control Using Model  Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zheng%2C+Z">Zishun Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yuan Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">To enable vehicles to perform automatic lane change amidst the random traffic
flow on highways, this paper introduces a decision-making and control method
for vehicle lane-change based on Model Predictive Control (MPC). This approach
divides the driving control of vehicles on highways into two parts: lane-change
decision and lane-change control, both of which are solved using the MPC
method. In the lane-change decision module, the minimum driving costs for each
lane are computed and compared by solving the MPC problem to make lane-change
decisions. In the lane-change control module, a dynamic bicycle model is
incorporated, and a multi-objective cost function is designed to obtain the
optimal control inputs for the lane-change process. The proposed lane-change
decision and control methods are simulated and validated within the SUMO
platform under random highway traffic conditions.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17525" title="Abstract">arXiv:2402.17525</a> [<a href="/pdf/2402.17525" title="Download PDF">pdf</a>, <a href="/format/2402.17525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Model-Based Image Editing: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiancheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yifan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Mingfu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+J">Jiaxi Lv</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianzhuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wei Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">He Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Liangliang Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Denoising diffusion models have emerged as a powerful tool for various image
generation and editing tasks, facilitating the synthesis of visual content in
an unconditional or input-conditional manner. The core idea behind them is
learning to reverse the process of gradually adding noise to images, allowing
them to generate high-quality samples from a complex distribution. In this
survey, we provide an exhaustive overview of existing methods using diffusion
models for image editing, covering both theoretical and practical aspects in
the field. We delve into a thorough analysis and categorization of these works
from multiple perspectives, including learning strategies, user-input
conditions, and the array of specific editing tasks that can be accomplished.
In addition, we pay special attention to image inpainting and outpainting, and
explore both earlier traditional context-driven and current multimodal
conditional methods, offering a comprehensive analysis of their methodologies.
To further evaluate the performance of text-guided image editing algorithms, we
propose a systematic benchmark, EditEval, featuring an innovative metric, LMM
Score. Finally, we address current limitations and envision some potential
directions for future research. The accompanying repository is released at
https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17527" title="Abstract">arXiv:2402.17527</a> [<a href="/pdf/2402.17527" title="Download PDF">pdf</a>, <a href="/format/2402.17527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predict the Next Word: &lt;Humans exhibit uncertainty in this task and  language models _____&gt;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ilia%2C+E">Evgenia Ilia</a>, 
<a href="/search/cs?searchtype=author&query=Aziz%2C+W">Wilker Aziz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models (LMs) are statistical models trained to assign probability to
human-generated text. As such, it is reasonable to question whether they
approximate linguistic variability exhibited by humans well. This form of
statistical assessment is difficult to perform at the passage level, for it
requires acceptability judgements (i.e., human evaluation) or a robust
automated proxy (which is non-trivial). At the word level, however, given some
context, samples from an LM can be assessed via exact matching against a
prerecorded dataset of alternative single-word continuations of the available
context. We exploit this fact and evaluate the LM's ability to reproduce
variability that humans (in particular, a population of English speakers)
exhibit in the 'next word prediction' task. This can be seen as assessing a
form of calibration, which, in the context of text classification, Baan et al.
(2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and
ChatGPT and find that they exhibit fairly low calibration to human uncertainty.
We also verify the failure of expected calibration error (ECE) to reflect this,
and as such, advise the community against relying on it in this setting.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17531" title="Abstract">arXiv:2402.17531</a> [<a href="/pdf/2402.17531" title="Download PDF">pdf</a>, <a href="/format/2402.17531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+K">Kaikai An</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fangkai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liqun Li</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhixing Ren</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Pu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hua Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Rajmohan%2C+S">Saravan Rajmohan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Effective incident management is pivotal for the smooth operation of
enterprises-level cloud services. In order to expedite incident mitigation,
service teams compile troubleshooting knowledge into Troubleshooting Guides
(TSGs) accessible to on-call engineers (OCEs). While automated pipelines are
enabled to resolve the most frequent and easy incidents, there still exist
complex incidents that require OCEs' intervention. However, TSGs are often
unstructured and incomplete, which requires manual interpretation by OCEs,
leading to on-call fatigue and decreased productivity, especially among
new-hire OCEs. In this work, we propose Nissist which leverages TSGs and
incident mitigation histories to provide proactive suggestions, reducing human
intervention. Leveraging Large Language Models (LLM), Nissist extracts insights
from unstructured TSGs and historical incident mitigation discussions, forming
a comprehensive knowledge base. Its multi-agent system design enhances
proficiency in precisely discerning user queries, retrieving relevant
information, and delivering systematic plans consecutively. Through our user
case and experiment, we demonstrate that Nissist significant reduce Time to
Mitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs
and improving service reliability. Our demo is available at
https://aka.ms/nissist_demo.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17532" title="Abstract">arXiv:2402.17532</a> [<a href="/pdf/2402.17532" title="Download PDF">pdf</a>, <a href="/format/2402.17532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval is Accurate Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+B">Bowen Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Leyang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xuxin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yuexian Zou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Standard language models generate text by selecting tokens from a fixed,
finite, and standalone vocabulary. We introduce a novel method that selects
context-aware phrases from a collection of supporting documents. One of the
most significant challenges for this paradigm shift is determining the training
oracles, because a string of text can be segmented in various ways and each
segment can be retrieved from numerous possible documents. To address this, we
propose to initialize the training oracles using linguistic heuristics and,
more importantly, bootstrap the oracles through iterative self-reinforcement.
Extensive experiments show that our model not only outperforms standard
language models on a variety of knowledge-intensive tasks but also demonstrates
improved generation quality in open-ended text generation. For instance,
compared to the standard language model counterpart, our model raises the
accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from
42.61% to 81.58% in open-ended text generation. Remarkably, our model also
achieves the best performance and the lowest latency among several
retrieval-augmented baselines. In conclusion, we assert that retrieval is more
accurate generation and hope that our work will encourage further research on
this new paradigm shift.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17533" title="Abstract">arXiv:2402.17533</a> [<a href="/pdf/2402.17533" title="Download PDF">pdf</a>, <a href="/format/2402.17533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Black-box Adversarial Attacks Against Image Quality Assessment Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ran%2C+Y">Yu Ran</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Ao-Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Weixuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuan-Gen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the
perceptual quality of an image in line with its subjective evaluation. To put
the NR-IQA models into practice, it is essential to study their potential
loopholes for model refinement. This paper makes the first attempt to explore
the black-box adversarial attacks on NR-IQA models. Specifically, we first
formulate the attack problem as maximizing the deviation between the estimated
quality scores of original and perturbed images, while restricting the
perturbed image distortions for visual quality preservation. Under such
formulation, we then design a Bi-directional loss function to mislead the
estimated quality scores of adversarial examples towards an opposite direction
with maximum deviation. On this basis, we finally develop an efficient and
effective black-box attack method against NR-IQA models. Extensive experiments
reveal that all the evaluated NR-IQA models are vulnerable to the proposed
attack method. And the generated perturbations are not transferable, enabling
them to serve the investigation of specialities of disparate IQA models.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17535" title="Abstract">arXiv:2402.17535</a> [<a href="/pdf/2402.17535" title="Download PDF">pdf</a>, <a href="/format/2402.17535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Hendriksen%2C+M">Mariya Hendriksen</a>, 
<a href="/search/cs?searchtype=author&query=Yates%2C+A">Andrew Yates</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, accepted as a full paper at ECIR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Learned sparse retrieval (LSR) is a family of neural methods that encode
queries and documents into sparse lexical vectors that can be indexed and
retrieved efficiently with an inverted index. We explore the application of LSR
to the multi-modal domain, with a focus on text-image retrieval. While LSR has
seen success in text retrieval, its application in multimodal retrieval remains
underexplored. Current approaches like LexLIP and STAIR require complex
multi-step training on massive datasets. Our proposed approach efficiently
transforms dense vectors from a frozen dense model into sparse lexical vectors.
We address issues of high dimension co-activation and semantic deviation
through a new training algorithm, using Bernoulli random variables to control
query expansion. Experiments with two dense models (BLIP, ALBEF) and two
datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively
reduces co-activation and semantic deviation. Our best-performing sparsified
model outperforms state-of-the-art text-image LSR models with a shorter
training time and lower GPU memory requirements. Our approach offers an
effective solution for training LSR retrieval models in multimodal settings.
Our code and model checkpoints are available at
github.com/thongnt99/lsr-multimodal
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17538" title="Abstract">arXiv:2402.17538</a> [<a href="/pdf/2402.17538" title="Download PDF">pdf</a>, <a href="/ps/2402.17538" title="Download PostScript">ps</a>, <a href="/format/2402.17538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A TDM-based Analog Front-End for Ear-EEG Recording with 83-G$&#x3a9;$  Input Impedance, 384-mV DC Tolerance and 0.47-$&#x3bc;$Vrms Input-Referred Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Huiyong Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper presents the design of a time-division multiplexed
capacitively-coupled chopper analog front end with a novel impedance boost loop
(IBL) and a novel DC servo loop (DSL). The proposed IBL boosts the input
impedance of the analog front end to up to several tens of G$\Omega$. It
firstly utilizes an external IBL to prevent the total input impedance from
degradation caused by parasitic capacitance from the ESD pad and external
interconnections, and secondly relies on an internal IBL to compensate for the
leakage current introduced by the chopper. The proposed DSL consists of a
coarse DSL driven by square waveforms and a fine DSL driven by five
phase-interleaving PWM waveforms, which up modulate the harmonics 5 times
higher. An edge-pursuit comparator (EPC) is utilized to monitor the residual
electrode offset voltage (EDO) at the LNA's output. Designed in a 0.18-$\mu$m
CMOS process, the AFE consumes 4.5 $\mu$A from a 1.2-V supply. The simulated
input referred noise is 0.47 $\mu$Vrms from 0.5 to 100 Hz in the presence of a
384-mV EDO. The proposed AFE achieves a high input impedance of 83 G$\Omega$ at
1 Hz and 9.3 G$\Omega$ at 100 Hz even with the presence of 20-pF parasitic
capacitance.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17546" title="Abstract">arXiv:2402.17546</a> [<a href="/pdf/2402.17546" title="Download PDF">pdf</a>, <a href="/format/2402.17546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COCOA: CBT-based Conversational Counseling Agent using Memory  Specialized in Cognitive Distortions and Dynamic Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Suyeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jieun Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Harim Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+K">Kyoung-Mee Chung</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongha Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yeo%2C+J">Jinyoung Yeo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The demand for conversational agents that provide mental health care is
consistently increasing. In this work, we develop a psychological counseling
agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT)
techniques to identify and address cognitive distortions inherent in the
client's statements. Specifically, we construct a memory system to efficiently
manage information necessary for counseling while extracting high-level
insights about the client from their utterances. Additionally, to ensure that
the counseling agent generates appropriate responses, we introduce dynamic
prompting to flexibly apply CBT techniques and facilitate the appropriate
retrieval of information. We conducted dialogues between CoCoA and characters
from Character.ai, creating a dataset for evaluation. Then, we asked GPT to
evaluate the constructed counseling dataset, and our model demonstrated a
statistically significant difference from other models.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17549" title="Abstract">arXiv:2402.17549</a> [<a href="/pdf/2402.17549" title="Download PDF">pdf</a>, <a href="/format/2402.17549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlipHash: A Constant-Time Consistent Range-Hashing Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masson%2C+C">Charles Masson</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H+K">Homin K. Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Consistent range-hashing is a technique used in distributed systems, either
directly or as a subroutine for consistent hashing, commonly to realize an even
and stable data distribution over a variable number of resources. We introduce
FlipHash, a consistent range-hashing algorithm with constant time complexity
and low memory requirements. Like Jump Consistent Hash, FlipHash is intended
for applications where resources can be indexed sequentially. Under this
condition, it ensures that keys are hashed evenly across resources and that
changing the number of resources only causes keys to be remapped from a removed
resource or to an added one, but never shuffled across persisted ones. FlipHash
differentiates itself with its low computational cost, achieving constant-time
complexity. We show that FlipHash beats Jump Consistent Hash's cost, which is
logarithmic in the number of resources, both theoretically and in experiments
over practical settings.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17550" title="Abstract">arXiv:2402.17550</a> [<a href="/pdf/2402.17550" title="Download PDF">pdf</a>, <a href="/format/2402.17550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergency Caching: Coded Caching-based Reliable Map Transmission in  Emergency Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zeyu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lianming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+A">Aiguo Fei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Many rescue missions demand effective perception and real-time decision
making, which highly rely on effective data collection and processing. In this
study, we propose a three-layer architecture of emergency caching networks
focusing on data collection and reliable transmission, by leveraging efficient
perception and edge caching technologies. Based on this architecture, we
propose a disaster map collection framework that integrates coded caching
technologies. Our framework strategically caches coded fragments of maps across
unmanned aerial vehicles (UAVs), fostering collaborative uploading for
augmented transmission reliability. Additionally, we establish a comprehensive
probability model to assess the effective recovery area of disaster maps.
Towards the goal of utility maximization, we propose a deep reinforcement
learning (DRL) based algorithm that jointly makes decisions about cooperative
UAVs selection, bandwidth allocation and coded caching parameter adjustment,
accommodating the real-time map updates in a dynamic disaster situation. Our
proposed scheme is more effective than the non-coding caching scheme, as
validated by simulation.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17553" title="Abstract">arXiv:2402.17553</a> [<a href="/pdf/2402.17553" title="Download PDF">pdf</a>, <a href="/format/2402.17553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist  Autonomous Agents for Desktop and Web
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+R">Raghav Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Butala%2C+Y+P">Yash Parag Butala</a>, 
<a href="/search/cs?searchtype=author&query=Russak%2C+M">Melisa Russak</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+J+Y">Jing Yu Koh</a>, 
<a href="/search/cs?searchtype=author&query=Kamble%2C+K">Kiran Kamble</a>, 
<a href="/search/cs?searchtype=author&query=Alshikh%2C+W">Waseem Alshikh</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">For decades, human-computer interaction has fundamentally been manual. Even
today, almost all productive work done on the computer necessitates human input
at every step. Autonomous virtual agents represent an exciting step in
automating many of these menial tasks. Virtual agents would empower users with
limited technical proficiency to harness the full possibilities of computer
systems. They could also enable the efficient streamlining of numerous computer
tasks, ranging from calendar management to complex travel bookings, with
minimal human intervention. In this paper, we introduce OmniACT, the
first-of-a-kind dataset and benchmark for assessing an agent's capability to
generate executable programs to accomplish computer tasks. Our scope extends
beyond traditional web automation, covering a diverse range of desktop
applications. The dataset consists of fundamental tasks such as "Play the next
song", as well as longer horizon tasks such as "Send an email to John Doe
mentioning the time and place to meet". Specifically, given a pair of screen
image and a visually-grounded natural language task, the goal is to generate a
script capable of fully executing the task. We run several strong baseline
language model agents on our benchmark. The strongest baseline, GPT-4, performs
the best on our benchmark However, its performance level still reaches only 15%
of the human proficiency in generating executable scripts capable of completing
the task, demonstrating the challenge of our task for conventional web agents.
Our benchmark provides a platform to measure and evaluate the progress of
language model agents in automating computer tasks and motivates future work
towards building multimodal models that bridge large language models and the
visual grounding of computer screens.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17554" title="Abstract">arXiv:2402.17554</a> [<a href="/pdf/2402.17554" title="Download PDF">pdf</a>, <a href="/ps/2402.17554" title="Download PostScript">ps</a>, <a href="/format/2402.17554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of Predictive Reliability to Foster Trust in Artificial  Intelligence. A case study in Multiple Sclerosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peracchio%2C+L">Lorenzo Peracchio</a>, 
<a href="/search/cs?searchtype=author&query=Nicora%2C+G">Giovanna Nicora</a>, 
<a href="/search/cs?searchtype=author&query=Parimbelli%2C+E">Enea Parimbelli</a>, 
<a href="/search/cs?searchtype=author&query=Buonocore%2C+T+M">Tommaso Mario Buonocore</a>, 
<a href="/search/cs?searchtype=author&query=Bergamaschi%2C+R">Roberto Bergamaschi</a>, 
<a href="/search/cs?searchtype=author&query=Tavazzi%2C+E">Eleonora Tavazzi</a>, 
<a href="/search/cs?searchtype=author&query=Dagliati%2C+A">Arianna Dagliati</a>, 
<a href="/search/cs?searchtype=author&query=Bellazzi%2C+R">Riccardo Bellazzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical
contexts, such as medicine, requires the implementation of safety measures to
reduce risks of harm in case of prediction errors. Spotting ML failures is of
paramount importance when ML predictions are used to drive clinical decisions.
ML predictive reliability measures the degree of trust of a ML prediction on a
new instance, thus allowing decision-makers to accept or reject it based on its
reliability. To assess reliability, we propose a method that implements two
principles. First, our approach evaluates whether an instance to be classified
is coming from the same distribution of the training set. To do this, we
leverage Autoencoders (AEs) ability to reconstruct the training set with low
error. An instance is considered Out-of-Distribution (OOD) if the AE
reconstructs it with a high error. Second, it is evaluated whether the ML
classifier has good performances on samples similar to the newly classified
instance by using a proxy model. We show that this approach is able to assess
reliability both in a simulated scenario and on a model trained to predict
disease progression of Multiple Sclerosis patients. We also developed a Python
package, named relAI, to embed reliability measures into ML pipelines. We
propose a simple approach that can be used in the deployment phase of any ML
model to suggest whether to trust predictions or not. Our method holds the
promise to provide effective support to clinicians by spotting potential ML
failures during deployment.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17555" title="Abstract">arXiv:2402.17555</a> [<a href="/pdf/2402.17555" title="Download PDF">pdf</a>, <a href="/format/2402.17555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised  Semantic Segmentation with Its Class Label
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hangzhou He</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lujia Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yanye Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scribble-based weakly-supervised semantic segmentation using sparse scribble
supervision is gaining traction as it reduces annotation costs when compared to
fully annotated alternatives. Existing methods primarily generate pseudo-labels
by diffusing labeled pixels to unlabeled ones with local cues for supervision.
However, this diffusion process fails to exploit global semantics and
class-specific cues, which are important for semantic segmentation. In this
study, we propose a class-driven scribble promotion network, which utilizes
both scribble annotations and pseudo-labels informed by image-level classes and
global semantics for supervision. Directly adopting pseudo-labels might
misguide the segmentation model, thus we design a localization rectification
module to correct foreground representations in the feature space. To further
combine the advantages of both supervisions, we also introduce a distance
entropy loss for uncertainty reduction, which adapts per-pixel confidence
weights according to the reliable region determined by the scribble and
pseudo-label's boundary. Experiments on the ScribbleSup dataset with different
qualities of scribble annotations outperform all the previous methods,
demonstrating the superiority and robustness of our method.The code is
available at
https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17559" title="Abstract">arXiv:2402.17559</a> [<a href="/pdf/2402.17559" title="Download PDF">pdf</a>, <a href="/format/2402.17559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphMatch: Subgraph Query Processing on FPGAs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dann%2C+J">Jonas Dann</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B6tz%2C+T">Tobias G&#xf6;tz</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+D">Daniel Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Giceva%2C+J">Jana Giceva</a>, 
<a href="/search/cs?searchtype=author&query=Fr%C3%B6ning%2C+H">Holger Fr&#xf6;ning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Efficiently finding subgraph embeddings in large graphs is crucial for many
application areas like biology and social network analysis. Set intersections
are the predominant and most challenging aspect of current join-based subgraph
query processing systems for CPUs. Previous work has shown the viability of
utilizing FPGAs for acceleration of graph and join processing.
<br />In this work, we propose GraphMatch, the first genearl-purpose stand-alone
subgraph query processing accelerator based on worst-case optimal joins (WCOJ)
that is fully designed for modern, field programmable gate array (FPGA)
hardware. For efficient processing of various graph data sets and query graph
patterns, it leverages a novel set intersection approach, called AllCompare,
tailor-made for FPGAs. We show that this set intersection approach efficiently
solves multi-set intersections in subgraph query processing, superior to
CPU-based approaches. Overall, GraphMatch achieves a speedup of over 2.68x and
5.16x, compared to the state-of-the-art systems GraphFlow and RapidMatch,
respectively.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17561" title="Abstract">arXiv:2402.17561</a> [<a href="/pdf/2402.17561" title="Download PDF">pdf</a>, <a href="/format/2402.17561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PHNet: Patch-based Normalization for Portrait Harmonization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Efremyan%2C+K">Karen Efremyan</a>, 
<a href="/search/cs?searchtype=author&query=Petrova%2C+E">Elizaveta Petrova</a>, 
<a href="/search/cs?searchtype=author&query=Kaskov%2C+E">Evgeny Kaskov</a>, 
<a href="/search/cs?searchtype=author&query=Kapitanov%2C+A">Alexander Kapitanov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Image harmonization, Patch-based normalization, Portrait harmonization
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A common problem for composite images is the incompatibility of their
foreground and background components. Image harmonization aims to solve this
problem, making the whole image look more authentic and coherent. Most existing
solutions predict lookup tables (LUTs) or reconstruct images, utilizing various
attributes of composite images. Recent approaches have primarily focused on
employing global transformations like normalization and color curve rendering
to achieve visual consistency, and they often overlook the importance of local
visual coherence. We present a patch-based harmonization network consisting of
novel Patch-based normalization (PN) blocks and a feature extractor based on
statistical color transfer. Extensive experiments demonstrate the network's
high generalization capability for different domains. Our network achieves
state-of-the-art results on the iHarmony4 dataset. Also, we created a new human
portrait harmonization dataset based on FFHQ and checked the proposed method to
show the generalization ability by achieving the best metrics on it. The
benchmark experiments confirm that the suggested patch-based normalization
block and feature extractor effectively improve the network's capability to
harmonize portraits. Our code and model baselines are publicly available.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17562" title="Abstract">arXiv:2402.17562</a> [<a href="/pdf/2402.17562" title="Download PDF">pdf</a>, <a href="/format/2402.17562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of the Generalization Ability of Lidar 3D Object  Detectors to Unseen Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eskandar%2C+G">George Eskandar</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongzhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+A">Abhishek Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Guirguis%2C+K">Karim Guirguis</a>, 
<a href="/search/cs?searchtype=author&query=Sayed%2C+M">Mohamed Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bin Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D Object Detectors (3D-OD) are crucial for understanding the environment in
many robotic tasks, especially autonomous driving. Including 3D information via
Lidar sensors improves accuracy greatly. However, such detectors perform poorly
on domains they were not trained on, i.e. different locations, sensors,
weather, etc., limiting their reliability in safety-critical applications.
There exist methods to adapt 3D-ODs to these domains; however, these methods
treat 3D-ODs as a black box, neglecting underlying architectural decisions and
source-domain training strategies. Instead, we dive deep into the details of
3D-ODs, focusing our efforts on fundamental factors that influence robustness
prior to domain adaptation.
<br />We systematically investigate four design choices (and the interplay between
them) often overlooked in 3D-OD robustness and domain adaptation: architecture,
voxel encoding, data augmentations, and anchor strategies. We assess their
impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks
encompassing three types of domain gaps - sensor type, weather, and location.
<br />Our main findings are: (1) transformer backbones with local point features
are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial
for adaptation across geographical locations, significantly boosting scores
without retraining, (3) source-domain augmentations allow the model to
generalize to low-resolution sensors, and (4) surprisingly, robustness to bad
weather is improved when training directly on more clean weather data than on
training with bad weather data. We outline our main conclusions and findings to
provide practical guidance on developing more robust 3D-ODs.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17563" title="Abstract">arXiv:2402.17563</a> [<a href="/pdf/2402.17563" title="Download PDF">pdf</a>, <a href="/format/2402.17563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Guided Adversarial Training of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Ling Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+H">Haotian Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhilong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+B">Bin Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models have demonstrated exceptional efficacy in various generative
applications. While existing models focus on minimizing a weighted sum of
denoising score matching losses for data distribution modeling, their training
primarily emphasizes instance-level optimization, overlooking valuable
structural information within each mini-batch, indicative of pair-wise
relationships among samples. To address this limitation, we introduce
Structure-guided Adversarial training of Diffusion Models (SADM). In this
pioneering approach, we compel the model to learn manifold structures between
samples in each training batch. To ensure the model captures authentic manifold
structures in the data distribution, we advocate adversarial training of the
diffusion generator against a novel structure discriminator in a minimax game,
distinguishing real manifold structures from the generated ones. SADM
substantially improves existing diffusion transformers (DiT) and outperforms
existing methods in image generation and cross-domain fine-tuning tasks across
12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on
ImageNet for class-conditional image generation at resolutions of 256x256 and
512x512, respectively.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17564" title="Abstract">arXiv:2402.17564</a> [<a href="/pdf/2402.17564" title="Download PDF">pdf</a>, <a href="/format/2402.17564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Potential of Large Language Models as Prompt Optimizers:  An Analogical Analysis with Gradient-based Model Optimizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xinyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Siyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic prompt optimization is an important approach to improving the
performance of large language models (LLMs). Recent research demonstrates the
potential of using LLMs as prompt optimizers, which can generate improved task
prompts via iterative refinement. In this paper, we propose a novel perspective
to investigate the design of LLM-based prompt optimizers, by drawing an analogy
with gradient-based model optimizers. To connect these two approaches, we
identify two pivotal factors in model parameter learning: update direction and
update method. Focused on the two aspects, we borrow the theoretical framework
and learning methods from gradient-based optimization to design improved
strategies for LLM-based prompt optimizers. By systematically analyzing a rich
set of improvement strategies, we further develop a capable Gradient-inspired
LLM-based Prompt Optimizer called GPO. At each step, it first retrieves
relevant prompts from the optimization trajectory as the update direction.
Then, it utilizes the generation-based refinement strategy to perform the
update, while controlling the edit distance through a cosine-based decay
strategy. Extensive experiments demonstrate the effectiveness and efficiency of
GPO. In particular, GPO brings an additional improvement of up to 56.8% on
Big-Bench Hard and 55.3% on MMLU compared to baseline methods.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17569" title="Abstract">arXiv:2402.17569</a> [<a href="/pdf/2402.17569" title="Download PDF">pdf</a>, <a href="/format/2402.17569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backpropagation-Based Analytical Derivatives of EKF Covariance for  Active Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benhamou%2C+J">Jonas Benhamou</a>, 
<a href="/search/cs?searchtype=author&query=Bonnabel%2C+S">Silv&#xe8;re Bonnabel</a>, 
<a href="/search/cs?searchtype=author&query=Chapdelaine%2C+C">Camille Chapdelaine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted at IORS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">To enhance accuracy of robot state estimation, perception-aware (or active
sensing) methods seek trajectories that minimize uncertainty. To this aim, one
possibility is to seek trajectories that minimize the final covariance of an
extended Kalman filter (EKF), w.r.t. its control inputs over a given horizon.
However, this can be computationally demanding. In this article, we derive
novel backpropagation analytical formulas for the derivatives of the final
covariance of an EKF w.r.t. its inputs. We then leverage the obtained gradients
as an enabling technology to derive perception-aware optimal motion plans.
Simulations validate the approach, showcasing improvements in both estimation
accuracy and execution time. Experimental results on a real large ground
vehicle also support the method.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17570" title="Abstract">arXiv:2402.17570</a> [<a href="/pdf/2402.17570" title="Download PDF">pdf</a>, <a href="/format/2402.17570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Variational Contaminated Noise Gaussian Process Regression for  Forecasting Geomagnetic Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iong%2C+D">Daniel Iong</a>, 
<a href="/search/cs?searchtype=author&query=McAnear%2C+M">Matthew McAnear</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yuezhou Qu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+S">Shasha Zou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G+T+Y">Gabor Toth Yang Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Methodology (stat.ME)

</div>
<p class="mathjax">Gaussian Processes (GP) have become popular machine learning methods for
kernel based learning on datasets with complicated covariance structures. In
this paper, we present a novel extension to the GP framework using a
contaminated normal likelihood function to better account for heteroscedastic
variance and outlier noise. We propose a scalable inference algorithm based on
the Sparse Variational Gaussian Process (SVGP) method for fitting sparse
Gaussian process regression models with contaminated normal noise on large
datasets. We examine an application to geomagnetic ground perturbations, where
the state-of-art prediction model is based on neural networks. We show that our
approach yields shorter predictions intervals for similar coverage and accuracy
when compared to an artificial dense neural network baseline.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17572" title="Abstract">arXiv:2402.17572</a> [<a href="/pdf/2402.17572" title="Download PDF">pdf</a>, <a href="/format/2402.17572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperdimensional computing: a fast, robust and interpretable paradigm  for biological data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stock%2C+M">Michiel Stock</a>, 
<a href="/search/cs?searchtype=author&query=Boeckaerts%2C+D">Dimitri Boeckaerts</a>, 
<a href="/search/cs?searchtype=author&query=Dewulf%2C+P">Pieter Dewulf</a>, 
<a href="/search/cs?searchtype=author&query=Taelman%2C+S">Steff Taelman</a>, 
<a href="/search/cs?searchtype=author&query=Van+Haeverbeke%2C+M">Maxime Van Haeverbeke</a>, 
<a href="/search/cs?searchtype=author&query=Van+Criekinge%2C+W">Wim Van Criekinge</a>, 
<a href="/search/cs?searchtype=author&query=De+Baets%2C+B">Bernard De Baets</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Advances in bioinformatics are primarily due to new algorithms for processing
diverse biological data sources. While sophisticated alignment algorithms have
been pivotal in analyzing biological sequences, deep learning has substantially
transformed bioinformatics, addressing sequence, structure, and functional
analyses. However, these methods are incredibly data-hungry, compute-intensive
and hard to interpret. Hyperdimensional computing (HDC) has recently emerged as
an intriguing alternative. The key idea is that random vectors of high
dimensionality can represent concepts such as sequence identity or phylogeny.
These vectors can then be combined using simple operators for learning,
reasoning or querying by exploiting the peculiar properties of high-dimensional
spaces. Our work reviews and explores the potential of HDC for bioinformatics,
emphasizing its efficiency, interpretability, and adeptness in handling
multimodal and structured data. HDC holds a lot of potential for various omics
data searching, biosignal analysis and health applications.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17573" title="Abstract">arXiv:2402.17573</a> [<a href="/pdf/2402.17573" title="Download PDF">pdf</a>, <a href="/format/2402.17573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HBF MU-MIMO with Interference-Aware Beam Pair Link Allocation for  Beyond-5G mm-Wave Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ichkov%2C+A">Aleksandar Ichkov</a>, 
<a href="/search/cs?searchtype=author&query=Wietfeld%2C+A">Alexander Wietfeld</a>, 
<a href="/search/cs?searchtype=author&query=Petrova%2C+M">Marina Petrova</a>, 
<a href="/search/cs?searchtype=author&query=Simi%C4%87%2C+L">Ljiljana Simi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 11 figures. This work has been submitted to IEEE for possible publication (copyright may be transferred without notice, after which this version may no longer be accessible)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Hybrid beamforming (HBF) multi-user multiple-input multiple-output (MU-MIMO)
is a key technology for unlocking the directional millimeter-wave (mm-wave)
nature for spatial multiplexing beyond current codebook-based 5G-NR networks.
In order to suppress co-scheduled users' interference, HBF MU-MIMO is
predicated on having sufficient radio frequency chains and accurate channel
state information (CSI), which can otherwise lead to performance losses due to
imperfect interference cancellation. In this work, we propose IABA, a 5G-NR
standard-compliant beam pair link (BPL) allocation scheme for mitigating
spatial interference in practical HBF MU-MIMO networks. IABA solves the network
sum throughput optimization via either a distributed or a centralized BPL
allocation using dedicated CSI reference signals for candidate BPL monitoring.
We present a comprehensive study of practical multi-cell mm-wave networks and
demonstrate that HBF MU-MIMO without interference-aware BPL allocation
experiences strong residual interference which limits the achievable network
performance. Our results show that IABA offers significant performance gains
over the default interference-agnostic 5G-NR BPL allocation, and even allows
HBF MU-MIMO to outperform the fully digital MU-MIMO baseline, by facilitating
allocation of secondary BPLs other than the strongest BPL found during initial
access. We further demonstrate the scalability of IABA with increased gNB
antennas and densification for beyond-5G mm-wave networks.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17574" title="Abstract">arXiv:2402.17574</a> [<a href="/pdf/2402.17574" title="Download PDF">pdf</a>, <a href="/format/2402.17574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agent-Pro: Learning to Evolve via Policy-Level Reflection and  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Ke Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengna Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yongliang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+G">Guiyang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zeqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yueting Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Weiming Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LLM-based Agent
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17580" title="Abstract">arXiv:2402.17580</a> [<a href="/pdf/2402.17580" title="Download PDF">pdf</a>, <a href="/format/2402.17580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A highly efficient computational approach for part-scale microstructure  predictions in Ti-6Al-4V additive manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Proell%2C+S+D">Sebastian D. Proell</a>, 
<a href="/search/cs?searchtype=author&query=Brotz%2C+J">Julian Brotz</a>, 
<a href="/search/cs?searchtype=author&query=Kronbichler%2C+M">Martin Kronbichler</a>, 
<a href="/search/cs?searchtype=author&query=Wall%2C+W+A">Wolfgang A. Wall</a>, 
<a href="/search/cs?searchtype=author&query=Meier%2C+C">Christoph Meier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Fast and efficient simulations of metal additive manufacturing (AM) processes
are highly relevant to exploring the full potential of this promising
manufacturing technique. The microstructure composition plays an important role
in characterizing the part quality and deriving mechanical properties. When
complete parts are simulated, one often needs to resort to strong
simplifications such as layer-wise heating due to the large number of simulated
time steps compared to the small time step sizes. This article proposes a
scan-resolved approach to the coupled thermo-microstructural problem. Building
on a highly efficient thermal model, we discuss the implementation of a
phenomenological microstructure model for the evolution of the three main
constituents of Ti-6Al-4V: stable $\alpha_s$-phase, martensite $\alpha_m$-phase
and $\beta$-phase. The implementation is tailored to modern hardware features
using vectorization and fast approximations of transcendental functions. A
performance model and numerical examples verify the high degree of
optimization. We demonstrate the applicability and predictive power of the
approach and the influence of scan strategy and geometry. Depending on the
specific example, results can be obtained with moderate computational resources
in a few hours to days. The numerical examples include a prediction of the
microstructure on the full NIST AM Benchmark cantilever specimen.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17583" title="Abstract">arXiv:2402.17583</a> [<a href="/pdf/2402.17583" title="Download PDF">pdf</a>, <a href="/format/2402.17583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in  Large-scale Cloud Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junjie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuangbin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhihan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=LI%2C+Y">Yichen LI</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiazhen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Cong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zengyin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yongqiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Postmortem analysis is essential in the management of incidents within cloud
systems, which provides valuable insights to improve system's reliability and
robustness. At CloudA, fault pattern profiling is performed during the
postmortem phase, which involves the classification of incidents' faults into
unique categories, referred to as fault pattern. By aggregating and analyzing
these fault patterns, engineers can discern common faults, vulnerable
components and emerging fault trends. However, this process is currently
conducted by manual labeling, which has inherent drawbacks. On the one hand,
the sheer volume of incidents means only the most severe ones are analyzed,
causing a skewed overview of fault patterns. On the other hand, the complexity
of the task demands extensive domain knowledge, which leads to errors and
inconsistencies. To address these limitations, we propose an automated
approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.
It leverages hierarchy-guided contrastive learning to train a hierarchy-aware
incident encoder and predicts fault patterns with enhanced incident
representations. We evaluate FaultProfIT using the production incidents from
CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art
methods. Our ablation study and analysis also verify the effectiveness of
hierarchy-guided contrastive learning. Additionally, we have deployed
FaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+
incidents from 30+ cloud services, successfully revealing several fault trends
that have informed system improvements.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17585" title="Abstract">arXiv:2402.17585</a> [<a href="/pdf/2402.17585" title="Download PDF">pdf</a>, <a href="/format/2402.17585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Constrained STL Task Decomposition through Convex  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Marchesini%2C+G">Gregorio Marchesini</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Siyuan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lindemann%2C+L">Lars Lindemann</a>, 
<a href="/search/eess?searchtype=author&query=Dimarogonas%2C+D+V">Dimos V. Dimarogonas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted at 2024 American Control Conference (ACC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this work, we propose a method to decompose signal temporal logic (STL)
tasks for multi-agent systems subject to constraints imposed by the
communication graph. Specifically, we propose to decompose tasks defined over
multiple agents which require multi-hop communication, by a set of sub-tasks
defined over the states of agents with 1-hop distance over the communication
graph. To this end, we parameterize the predicates of the tasks to be
decomposed as suitable hyper-rectangles. Then, we show that by solving a
constrained convex optimization, optimal parameters maximising the volume of
the predicate's super-level sets can be computed for the decomposed tasks. In
addition, we provide a formal definition of conflicting conjunctions of tasks
for the considered STL fragment and a formal procedure to exclude such
conjunctions from the solution set of possible decompositions. The proposed
approach is demonstrated through simulations.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17587" title="Abstract">arXiv:2402.17587</a> [<a href="/pdf/2402.17587" title="Download PDF">pdf</a>, <a href="/format/2402.17587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instance-aware Exploration-Verification-Exploitation for Instance  ImageGoal Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+X">Xiaohan Lei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Min Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wengang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Houqiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to
navigate to a specified object depicted by a goal image in an unexplored
environment.
<br />The main challenge of this task lies in identifying the target object from
different viewpoints while rejecting similar distractors.
<br />Existing ImageGoal Navigation methods usually adopt the simple
Exploration-Exploitation framework and ignore the identification of specific
instance during navigation.
<br />In this work, we propose to imitate the human behaviour of ``getting closer
to confirm" when distinguishing objects from a distance.
<br />Specifically, we design a new modular navigation framework named
Instance-aware Exploration-Verification-Exploitation (IEVE) for instance-level
image goal navigation.
<br />Our method allows for active switching among the exploration, verification,
and exploitation actions, thereby facilitating the agent in making reasonable
decisions under different situations.
<br />On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our
method surpasses previous state-of-the-art work, with a classical segmentation
model (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).
Our code will be made publicly available at https://github.com/XiaohanLei/IEVE.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17588" title="Abstract">arXiv:2402.17588</a> [<a href="/pdf/2402.17588" title="Download PDF">pdf</a>, <a href="/format/2402.17588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chronicles of CI/CD: A Deep Dive into its Usage Over Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Gi%C3%A3o%2C+H">Hugo da Gi&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Flores%2C+A">Andr&#xe9; Flores</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+R">Rui Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Cunha%2C+J">J&#xe1;come Cunha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">DevOps is a combination of methodologies and tools that improves the software
development, build, deployment, and monitoring processes by shortening its
lifecycle and improving software quality. Part of this process is CI/CD, which
embodies mostly the first parts, right up to the deployment. Despite the many
benefits of DevOps and CI/CD, it still presents many challenges promoted by the
tremendous proliferation of different tools, languages, and syntaxes, which
makes the field quite challenging to learn and keep up to date. Software
repositories contain data regarding various software practices, tools, and
uses. This data can help gather multiple insights that inform technical and
academic decision-making. GitHub is currently the most popular software hosting
platform and provides a search API that lets users query its repositories. Our
goal with this paper is to gain insights into the technologies developers use
for CI/CD by analyzing GitHub repositories. Using a list of the
state-of-the-art CI/CD technologies, we use the GitHub search API to find
repositories using each of these technologies. We also use the API to extract
various insights regarding those repositories. We then organize and analyze the
data collected. From our analysis, we provide an overview of the use of CI/CD
technologies in our days, but also what happened in the last 12 years. We also
show developers use several technologies simultaneously in the same project and
that the change between technologies is quite common. From these insights, we
find several research paths, from how to support the use of multiple
technologies, both in terms of techniques, but also in terms of human-computer
interaction, to aiding developers in evolving their CI/CD pipelines, again
considering the various dimensions of the problem.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17589" title="Abstract">arXiv:2402.17589</a> [<a href="/pdf/2402.17589" title="Download PDF">pdf</a>, <a href="/format/2402.17589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Beitong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Cheng Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, the application of Contrastive Representation Learning (CRL) in
learning with noisy labels (LNL) has shown promising advancements due to its
remarkable ability to learn well-distributed representations for better
distinguishing noisy labels. However, CRL is mainly used as a pre-training
technique, leading to a complicated multi-stage training pipeline. We also
observed that trivially combining CRL with supervised LNL methods decreases
performance. Using different images from the same class as negative pairs in
CRL creates optimization conflicts between CRL and the supervised loss. To
address these two issues, we propose an end-to-end PLReMix framework that
avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR)
contrastive loss to alleviate the conflicts between losses. This PLR loss
constructs a reliable negative set of each sample by filtering out its
inappropriate negative pairs that overlap at the top k indices of prediction
probabilities, leading to more compact semantic clusters than vanilla CRL.
Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to
distinguish clean and noisy samples by leveraging semantic information and
model outputs simultaneously, which is expanded on the previously widely used
one-dimensional form. The PLR loss and a semi-supervised loss are
simultaneously applied to train on the GMM divided clean and noisy samples.
Experiments on multiple benchmark datasets demonstrate the effectiveness of the
proposed method. Our proposed PLR loss is scalable, which can be easily
integrated into other LNL methods and boost their performance. Codes will be
available.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17593" title="Abstract">arXiv:2402.17593</a> [<a href="/pdf/2402.17593" title="Download PDF">pdf</a>, <a href="/format/2402.17593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Shuttle Operation for Vulnerable Populations: Lessons and  Experiences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+R">Ren Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zhaofeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jinghui Liao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weisong Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The increasing shortage of drivers poses a significant threat to vulnerable
populations, particularly seniors and disabled individuals who heavily depend
on public transportation for accessing healthcare services and social events.
Autonomous Vehicles (AVs) emerge as a promising alternative, offering potential
improvements in accessibility and independence for these groups. However,
current designs and studies often overlook the unique needs and experiences of
these populations, leading to potential accessibility barriers. This paper
presents a detailed case study of an autonomous shuttle test specifically
tailored for seniors and disabled individuals, conducted during the early
stages of the COVID-19 pandemic. The service, which lasted 13 weeks, catered to
approximately 1500 passengers in an urban setting, aiming to facilitate access
to essential services. Drawing from the safety operator's experiences and
direct observations, we identify critical user experience and safety challenges
faced by vulnerable passengers. Based on our findings, we propose targeted
initiatives to enhance the safety, accessibility, and user education of AV
technology for seniors and disabled individuals. These include increasing
educational opportunities to familiarize these groups with AV technology,
designing AVs with a focus on diversity and inclusion, and improving training
programs for AV operators to address the unique needs of vulnerable
populations. Through these initiatives, we aim to bridge the gap in AV
accessibility and ensure that these technologies benefit all members of
society.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17595" title="Abstract">arXiv:2402.17595</a> [<a href="/pdf/2402.17595" title="Download PDF">pdf</a>, <a href="/format/2402.17595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Regularization via Spectral Neural Networks and Non-linear  Matrix Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+H+T+M">Hong T.M. Chu</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Subhro Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+C+T">Chi Thanh Lam</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S+S">Soumendu Sundar Mukherjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
<p class="mathjax">The phenomenon of implicit regularization has attracted interest in recent
years as a fundamental aspect of the remarkable generalizing ability of neural
networks. In a nutshell, it entails that gradient descent dynamics in many
neural nets, even without any explicit regularizer in the loss function,
converges to the solution of a regularized learning problem. However, known
results attempting to theoretically explain this phenomenon focus
overwhelmingly on the setting of linear neural nets, and the simplicity of the
linear structure is particularly crucial to existing arguments. In this paper,
we explore this problem in the context of more realistic neural networks with a
general class of non-linear activation functions, and rigorously demonstrate
the implicit regularization phenomenon for such networks in the setting of
matrix sensing problems, together with rigorous rate guarantees that ensure
exponentially fast convergence of gradient descent.In this vein, we contribute
a network architecture called Spectral Neural Networks (abbrv. SNN) that is
particularly suitable for matrix learning problems. Conceptually, this entails
coordinatizing the space of matrices by their singular values and singular
vectors, as opposed to by their entries, a potentially fruitful perspective for
matrix learning. We demonstrate that the SNN architecture is inherently much
more amenable to theoretical analysis than vanilla neural nets and confirm its
effectiveness in the context of matrix sensing, via both mathematical
guarantees and empirical investigations. We believe that the SNN architecture
has the potential to be of wide applicability in a broad class of matrix
learning scenarios.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17596" title="Abstract">arXiv:2402.17596</a> [<a href="/pdf/2402.17596" title="Download PDF">pdf</a>, <a href="/format/2402.17596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corridor MPC for Multi-Agent Inspection of Orbiting Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marchesini%2C+G">Gregorio Marchesini</a>, 
<a href="/search/cs?searchtype=author&query=Roque%2C+P">Pedro Roque</a>, 
<a href="/search/cs?searchtype=author&query=Dimarogonas%2C+D+V">Dimos V. Dimarogonas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted at Conference on Decision and Control 2023 (CDC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this work, we propose an extension of the previously introduced Corridor
Model Predictive Control scheme for high-order and distributed systems, with an
application for on-orbit inspection. To this end, we leverage high order
control barrier function (HOCBF) constraints as a suitable control approach to
maintain each agent in the formation within a safe corridor from its reference
trajectory. The recursive feasibility of the designed MPC scheme is tested
numerically, while suitable modifications of the classical HOCBF constraint
definition are introduced such that safety is guaranteed both in sampled and
continuous time. The designed controller is validated through computer
simulation in a realistic inspection scenario of the International Space
Station.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17597" title="Abstract">arXiv:2402.17597</a> [<a href="/pdf/2402.17597" title="Download PDF">pdf</a>, <a href="/ps/2402.17597" title="Download PostScript">ps</a>, <a href="/format/2402.17597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A duality for nonabelian group codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wentworth-Nice%2C+P">Prairie Wentworth-Nice</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Group Theory (math.GR)

</div>
<p class="mathjax">In 1962, Jesse MacWilliams published a set of formulas for linear and abelian
group codes that among other applications, were incredibly valuable in the
study of self-dual codes. Now called the MacWilliams Identities, her results
relate the weight enumerator and complete weight enumerator of a code to those
of its dual code. A similar set of MacWilliams identities has been proven to
exist for many other types of codes. In 2013, Dougherty, Sol\'{e}, and Kim
published a list of fundamental open questions in coding theory. Among them,
Open Question 4.3: "Is there a duality and MacWilliams formula for codes over
non-Abelian groups?" In this paper, we propose a duality for nonabelian group
codes in terms of the irreducible representations of the group. We show that
there is a Greene's Theorem and MacWilliams Identities which hold for this
notion of duality. When the group is abelian, our results are equivalent to
existing formulas in the literature.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17599" title="Abstract">arXiv:2402.17599</a> [<a href="/pdf/2402.17599" title="Download PDF">pdf</a>, <a href="/format/2402.17599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DAGnosis: Localized Identification of Data Inconsistencies using  Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huynh%2C+N">Nicolas Huynh</a>, 
<a href="/search/cs?searchtype=author&query=Berrevoets%2C+J">Jeroen Berrevoets</a>, 
<a href="/search/cs?searchtype=author&query=Seedat%2C+N">Nabeel Seedat</a>, 
<a href="/search/cs?searchtype=author&query=Crabb%C3%A9%2C+J">Jonathan Crabb&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhaozhi Qian</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Identification and appropriate handling of inconsistencies in data at
deployment time is crucial to reliably use machine learning models. While
recent data-centric methods are able to identify such inconsistencies with
respect to the training set, they suffer from two key limitations: (1)
suboptimality in settings where features exhibit statistical independencies,
due to their usage of compressive representations and (2) lack of localization
to pin-point why a sample might be flagged as inconsistent, which is important
to guide future data collection. We solve these two fundamental limitations
using directed acyclic graphs (DAGs) to encode the training set's features
probability distribution and independencies as a structure. Our method, called
DAGnosis, leverages these structural interactions to bring valuable and
insightful data-centric conclusions. DAGnosis unlocks the localization of the
causes of inconsistencies on a DAG, an aspect overlooked by previous
approaches. Moreover, we show empirically that leveraging these interactions
(1) leads to more accurate conclusions in detecting inconsistencies, as well as
(2) provides more detailed insights into why some samples are flagged.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17601" title="Abstract">arXiv:2402.17601</a> [<a href="/pdf/2402.17601" title="Download PDF">pdf</a>, <a href="/format/2402.17601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing sleep detection by modelling weak label sets: A novel weakly  supervised learning approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boeker%2C+M">Matthias Boeker</a>, 
<a href="/search/cs?searchtype=author&query=Thambawita%2C+V">Vajira Thambawita</a>, 
<a href="/search/cs?searchtype=author&query=Riegler%2C+M">Michael Riegler</a>, 
<a href="/search/cs?searchtype=author&query=Halvorsen%2C+P">P&#xe5;l Halvorsen</a>, 
<a href="/search/cs?searchtype=author&query=Hammer%2C+H+L">Hugo L. Hammer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Understanding sleep and activity patterns plays a crucial role in physical
and mental health. This study introduces a novel approach for sleep detection
using weakly supervised learning for scenarios where reliable ground truth
labels are unavailable. The proposed method relies on a set of weak labels,
derived from the predictions generated by conventional sleep detection
algorithms. Introducing a novel approach, we suggest a novel generalised
non-linear statistical model in which the number of weak sleep labels is
modelled as outcome of a binomial distribution. The probability of sleep in the
binomial distribution is linked to the outcomes of neural networks trained to
detect sleep based on actigraphy. We show that maximizing the likelihood
function of the model, is equivalent to minimizing the soft cross-entropy loss.
Additionally, we explored the use of the Brier score as a loss function for
weak labels. The efficacy of the suggested modelling framework was demonstrated
using the Multi-Ethnic Study of Atherosclerosis dataset. A \gls{lstm} trained
on the soft cross-entropy outperformed conventional sleep detection algorithms,
other neural network architectures and loss functions in accuracy and model
calibration. This research not only advances sleep detection techniques in
scenarios where ground truth data is scarce but also contributes to the broader
field of weakly supervised learning by introducing innovative approach in
modelling sets of weak labels.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17603" title="Abstract">arXiv:2402.17603</a> [<a href="/pdf/2402.17603" title="Download PDF">pdf</a>, <a href="/format/2402.17603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRIPs-Py: Techniques for Regularization of Inverse Problems in Python
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pasha%2C+M">Mirjeta Pasha</a>, 
<a href="/search/math?searchtype=author&query=Gazzola%2C+S">Silvia Gazzola</a>, 
<a href="/search/math?searchtype=author&query=Sanderford%2C+C">Connor Sanderford</a>, 
<a href="/search/math?searchtype=author&query=Ugwu%2C+U+O">Ugochukwu O. Ugwu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 10 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we describe TRIPs-Py, a new Python package of linear discrete
inverse problems solvers and test problems. The goal of the package is
two-fold: 1) to provide tools for solving small and large-scale inverse
problems, and 2) to introduce test problems arising from a wide range of
applications. The solvers available in TRIPs-Py include direct regularization
methods (such as truncated singular value decomposition and Tikhonov) and
iterative regularization techniques (such as Krylov subspace methods and recent
solvers for $\ell_p$-$\ell_q$ formulations, which enforce sparse or
edge-preserving solutions and handle different noise types). All our solvers
have built-in strategies to define the regularization parameter(s). Some of the
test problems in TRIPs-Py arise from simulated image deblurring and
computerized tomography, while other test problems model realistic problems in
dynamic computerized tomography. Numerical examples are included to illustrate
the usage as well as the performance of the described methods on the provided
test problems. To the best of our knowledge, TRIPs-Py is the first Python
software package of this kind, which may serve both research and didactical
purposes.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17604" title="Abstract">arXiv:2402.17604</a> [<a href="/pdf/2402.17604" title="Download PDF">pdf</a>, <a href="/ps/2402.17604" title="Download PostScript">ps</a>, <a href="/format/2402.17604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant ideals of polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Arka Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Lasota%2C+S">S&#x142;awomir Lasota</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">We study existence and computability of finite bases for ideals of
polynomials over infinitely many variables. In our setting, variables come from
a countable logical structure A, and embeddings from A to A act on polynomials
by renaming variables. First, we give a sufficient and necessary condition for
A to guarantee the following generalisation of Hilbert's Basis Theorem: every
polynomial ideal which is equivariant, i.e. invariant under renaming of
variables, is finitely generated. Second, we develop an extension of classical
Buchberger's algorithm to compute a Gr\"obner basis of a given equivariant
ideal. This implies decidability of the membership problem for equivariant
ideals. Finally, we sketch upon various applications of these results to
register automata, Petri nets with data, orbit-finitely generated vector
spaces, and orbit-finite systems of linear equations.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17606" title="Abstract">arXiv:2402.17606</a> [<a href="/pdf/2402.17606" title="Download PDF">pdf</a>, <a href="/format/2402.17606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Topological Representations with Bidirectional Graph Attention  Network for Solving Job Shop Scheduling Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yaoxin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Wen Song</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jing Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing learning-based methods for solving job shop scheduling problem
(JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and
neglect the rich and meaningful topological structures of disjunctive graphs
(DGs). This paper proposes the topology-aware bidirectional graph attention
network (TBGAT), a novel GNN architecture based on the attention mechanism, to
embed the DG for solving JSSP in a local search framework. Specifically, TBGAT
embeds the DG from a forward and a backward view, respectively, where the
messages are propagated by following the different topologies of the views and
aggregated via graph attention. Then, we propose a novel operator based on the
message-passing mechanism to calculate the forward and backward topological
sorts of the DG, which are the features for characterizing the topological
structures and exploited by our model. In addition, we theoretically and
experimentally show that TBGAT has linear computational complexity to the
number of jobs and machines, respectively, which strengthens the practical
value of our method. Besides, extensive experiments on five synthetic datasets
and seven classic benchmarks show that TBGAT achieves new SOTA results by
outperforming a wide range of neural methods by a large margin.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17608" title="Abstract">arXiv:2402.17608</a> [<a href="/pdf/2402.17608" title="Download PDF">pdf</a>, <a href="/format/2402.17608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miaschi%2C+A">Alessio Miaschi</a>, 
<a href="/search/cs?searchtype=author&query=Dell%27Orletta%2C+F">Felice Dell&#x27;Orletta</a>, 
<a href="/search/cs?searchtype=author&query=Venturi%2C+G">Giulia Venturi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we explore the impact of augmenting pre-trained
Encoder-Decoder models, specifically T5, with linguistic knowledge for the
prediction of a target task. In particular, we investigate whether fine-tuning
a T5 model on an intermediate task that predicts structural linguistic
properties of sentences modifies its performance in the target task of
predicting sentence-level complexity. Our study encompasses diverse experiments
conducted on Italian and English datasets, employing both monolingual and
multilingual T5 models at various sizes. Results obtained for both languages
and in cross-lingual configurations show that linguistically motivated
intermediate fine-tuning has generally a positive impact on target task
performance, especially when applied to smaller models and in scenarios with
limited data availability.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17611" title="Abstract">arXiv:2402.17611</a> [<a href="/pdf/2402.17611" title="Download PDF">pdf</a>, <a href="/format/2402.17611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Large-scale Evaluation of Pretraining Paradigms for the Detection of  Defects in Electroluminescence Solar Cell Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Torpey%2C+D">David Torpey</a>, 
<a href="/search/cs?searchtype=author&query=Pratt%2C+L">Lawrence Pratt</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+R">Richard Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Pretraining has been shown to improve performance in many domains, including
semantic segmentation, especially in domains with limited labelled data. In
this work, we perform a large-scale evaluation and benchmarking of various
pretraining methods for Solar Cell Defect Detection (SCDD) in
electroluminescence images, a field with limited labelled datasets. We cover
supervised training with semantic segmentation, semi-supervised learning, and
two self-supervised techniques. We also experiment with both in-distribution
and out-of-distribution (OOD) pretraining and observe how this affects
downstream performance. The results suggest that supervised training on a large
OOD dataset (COCO), self-supervised pretraining on a large OOD dataset
(ImageNet), and semi-supervised pretraining (CCT) all yield statistically
equivalent performance for mean Intersection over Union (mIoU). We achieve a
new state-of-the-art for SCDD and demonstrate that certain pretraining schemes
result in superior performance on underrepresented classes. Additionally, we
provide a large-scale unlabelled EL image dataset of $22000$ images, and a
$642$-image labelled semantic segmentation EL dataset, for further research in
developing self- and semi-supervised training techniques in this domain.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17613" title="Abstract">arXiv:2402.17613</a> [<a href="/pdf/2402.17613" title="Download PDF">pdf</a>, <a href="/format/2402.17613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Automated Writing Evaluation with Corrective Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+I+X">Izia Xiaoxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xihan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Coates%2C+E">Edith Coates</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+M">Min Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+J">Jiexin Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+M">Mengyang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jungyeul Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the system demonstration track at NAACL-HLT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The utilization of technology in second language learning and teaching has
become ubiquitous. For the assessment of writing specifically, automated
writing evaluation (AWE) and grammatical error correction (GEC) have become
immensely popular and effective methods for enhancing writing proficiency and
delivering instant and individualized feedback to learners. By leveraging the
power of natural language processing (NLP) and machine learning algorithms, AWE
and GEC systems have been developed separately to provide language learners
with automated corrective feedback and more accurate and unbiased scoring that
would otherwise be subject to examiners. In this paper, we propose an
integrated system for automated writing evaluation with corrective feedback as
a means of bridging the gap between AWE and GEC results for second language
learners. This system enables language learners to simulate the essay writing
tests: a student writes and submits an essay, and the system returns the
assessment of the writing along with suggested grammatical error corrections.
Given that automated scoring and grammatical correction are more efficient and
cost-effective than human grading, this integrated system would also alleviate
the burden of manually correcting innumerable essays.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17614" title="Abstract">arXiv:2402.17614</a> [<a href="/pdf/2402.17614" title="Download PDF">pdf</a>, <a href="/format/2402.17614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herzog%2C+J">Jonas Herzog</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Few-shot segmentation performance declines substantially when facing images
from a domain different than the training domain, effectively limiting
real-world use cases. To alleviate this, recently cross-domain few-shot
segmentation (CD-FSS) has emerged. Works that address this task mainly
attempted to learn segmentation on a source domain in a manner that generalizes
across domains. Surprisingly, we can outperform these approaches while
eliminating the training stage and removing their main segmentation network. We
show test-time task-adaption is the key for successful CD-FSS instead.
Task-adaption is achieved by appending small networks to the feature pyramid of
a conventionally classification-pretrained backbone. To avoid overfitting to
the few labeled samples in supervised fine-tuning, consistency across augmented
views of input images serves as guidance while learning the parameters of the
attached layers. Despite our self-restriction not to use any images other than
the few labeled samples at test time, we achieve new state-of-the-art
performance in CD-FSS, evidencing the need to rethink approaches for the task.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17615" title="Abstract">arXiv:2402.17615</a> [<a href="/pdf/2402.17615" title="Download PDF">pdf</a>, <a href="/format/2402.17615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Agent Model for Opinion Evolution under Cognitive Biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alvim%2C+M+S">M&#xe1;rio S. Alvim</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+A+G">Artur Gaspar da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Knight%2C+S">Sophia Knight</a>, 
<a href="/search/cs?searchtype=author&query=Valencia%2C+F">Frank Valencia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We generalize the DeGroot model for opinion dynamics to better capture
realistic social scenarios. We introduce a model where each agent has their own
individual cognitive biases. Society is represented as a directed graph whose
edges indicate how much agents influence one another. Biases are represented as
the functions in the square region $[-1,1]^2$ and categorized into four
sub-regions based on the potential reactions they may elicit in an agent during
instances of opinion disagreement. Under the assumption that each bias of every
agent is a continuous function within the region of receptive but resistant
reactions ($\mathbf{R}$), we show that the society converges to a consensus if
the graph is strongly connected. Under the same assumption, we also establish
that the entire society converges to a unanimous opinion if and only if the
source components of the graph-namely, strongly connected components with no
external influence-converge to that opinion. We illustrate that convergence is
not guaranteed for strongly connected graphs when biases are either
discontinuous functions in $\mathbf{R}$ or not included in $\mathbf{R}$. We
showcase our model through a series of examples and simulations, offering
insights into how opinions form in social networks under cognitive biases.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17622" title="Abstract">arXiv:2402.17622</a> [<a href="/pdf/2402.17622" title="Download PDF">pdf</a>, <a href="/format/2402.17622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Williams%2C+D+S+W">David S. W. Williams</a>, 
<a href="/search/cs?searchtype=author&query=Gadd%2C+M">Matthew Gadd</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+P">Paul Newman</a>, 
<a href="/search/cs?searchtype=author&query=De+Martini%2C+D">Daniele De Martini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This work proposes a semantic segmentation network that produces high-quality
uncertainty estimates in a single forward pass. We exploit general
representations from foundation models and unlabelled datasets through a Masked
Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters
and simpler than previous techniques. For neural networks used in
safety-critical applications, bias in the training data can lead to errors;
therefore it is crucial to understand a network's limitations at run time and
act accordingly. To this end, we test our proposed method on a number of test
domains including the SAX Segmentation benchmark, which includes labelled test
data from dense urban, rural and off-road driving domains. The proposed method
consistently outperforms uncertainty estimation and Out-of-Distribution (OoD)
techniques on this difficult benchmark.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17624" title="Abstract">arXiv:2402.17624</a> [<a href="/pdf/2402.17624" title="Download PDF">pdf</a>, <a href="/format/2402.17624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CustomSketching: Sketch Concept Extraction for Sketch-based Image  Synthesis and Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chufeng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Hongbo Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Personalization techniques for large text-to-image (T2I) models allow users
to incorporate new concepts from reference images. However, existing methods
primarily rely on textual descriptions, leading to limited control over
customized images and failing to support fine-grained and local editing (e.g.,
shape, pose, and details). In this paper, we identify sketches as an intuitive
and versatile representation that can facilitate such control, e.g., contour
lines capturing shape information and flow lines representing texture. This
motivates us to explore a novel task of sketch concept extraction: given one or
more sketch-image pairs, we aim to extract a special sketch concept that
bridges the correspondence between the images and sketches, thus enabling
sketch-based image synthesis and editing at a fine-grained level. To accomplish
this, we introduce CustomSketching, a two-stage framework for extracting novel
sketch concepts. Considering that an object can often be depicted by a contour
for general shapes and additional strokes for internal details, we introduce a
dual-sketch representation to reduce the inherent ambiguity in sketch
depiction. We employ a shape loss and a regularization loss to balance fidelity
and editability during optimization. Through extensive experiments, a user
study, and several applications, we show our method is effective and superior
to the adapted baselines.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17630" title="Abstract">arXiv:2402.17630</a> [<a href="/pdf/2402.17630" title="Download PDF">pdf</a>, <a href="/format/2402.17630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Grained Natural Language Inference Based Faithfulness Evaluation  for Diverse Summarisation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huajian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yumo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Beltrachini%2C+L">Laura Perez-Beltrachini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We study existing approaches to leverage off-the-shelf Natural Language
Inference (NLI) models for the evaluation of summary faithfulness and argue
that these are sub-optimal due to the granularity level considered for premises
and hypotheses. That is, the smaller content unit considered as hypothesis is a
sentence and premises are made up of a fixed number of document sentences. We
propose a novel approach, namely InFusE, that uses a variable premise size and
simplifies summary sentences into shorter hypotheses. Departing from previous
studies which focus on single short document summarisation, we analyse NLI
based faithfulness evaluation for diverse summarisation tasks. We introduce
DiverSumm, a new benchmark comprising long form summarisation (long documents
and summaries) and diverse summarisation tasks (e.g., meeting and
multi-document summarisation). In experiments, InFusE obtains superior
performance across the different summarisation tasks. Our code and data are
available at https://github.com/HJZnlp/infuse.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17631" title="Abstract">arXiv:2402.17631</a> [<a href="/pdf/2402.17631" title="Download PDF">pdf</a>, <a href="/format/2402.17631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Cache-Oblivious Funnelselect
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brodal%2C+G+S">Gerth St&#xf8;lting Brodal</a>, 
<a href="/search/cs?searchtype=author&query=Wild%2C+S">Sebastian Wild</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In the multiple-selection problem one is given an unsorted array $S$ of $N$
elements and an array of $q$ query ranks $r_1&lt;\cdots&lt;r_q$, and the task is to
return, in sorted order, the $q$ elements in $S$ of rank $r_1, \ldots, r_q$,
respectively. The asymptotic deterministic comparison complexity of the problem
was settled by Dobkin and Munro [JACM 1981]. In the I/O model an optimal I/O
complexity was achieved by Hu et al. [SPAA 2014]. Recently [ESA 2023], we
presented a cache-oblivious algorithm with matching I/O complexity, named
funnelselect, since it heavily borrows ideas from the cache-oblivious sorting
algorithm funnelsort from the seminal paper by Frigo, Leiserson, Prokop and
Ramachandran [FOCS 1999]. Funnelselect is inherently randomized as it relies on
sampling for cheaply finding many good pivots. In this paper we present
deterministic funnelselect, achieving the same optional I/O complexity
cache-obliviously without randomization. Our new algorithm essentially replaces
a single (in expectation) reversed-funnel computation using random pivots by a
recursive algorithm using multiple reversed-funnel computations. To meet the
I/O bound, this requires a carefully chosen subproblem size based on the
entropy of the sequence of query ranks; deterministic funnelselect thus raises
distinct technical challenges not met by randomized funnelselect. The resulting
worst-case I/O bound is $O\bigl(\sum_{i=1}^{q+1} \frac{\Delta_i}{B} \cdot
\log_{M/B} \frac{N}{\Delta_i} + \frac{N}{B}\bigr)$, where $B$ is the external
memory block size, $M\geq B^{1+\epsilon}$ is the internal memory size, for some
constant $\epsilon&gt;0$, and $\Delta_i = r_{i} - r_{i-1}$ (assuming $r_0=0$ and
$r_{q+1}=N + 1$).
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17632" title="Abstract">arXiv:2402.17632</a> [<a href="/pdf/2402.17632" title="Download PDF">pdf</a>, <a href="/ps/2402.17632" title="Download PostScript">ps</a>, <a href="/format/2402.17632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Securing OPEN-RAN Equipment Using Blockchain-Based Supply Chain  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehrban%2C+A">Ali Mehrban</a>, 
<a href="/search/cs?searchtype=author&query=Jani%2C+M">Mostafa Jani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The disaggregated and multi-vendor nature of OPEN-RAN networks introduces new
supply chain security risks, making equipment authenticity and integrity
crucial challenges. Robust solutions are needed to mitigate vulnerabilities in
manufacturing and integration. This paper puts forth a novel blockchain-based
approach to secure OPEN-RAN equipment through its lifecycle. By combining
firmware authentication codes, a permissioned blockchain ledger, and equipment
node validators, we architect a tamper-resistant ecosystem to track provenance.
The outlined design, while conceptual, establishes a foundation and roadmap for
future realization. Through careful implementation planning, development of
core components like firmware signed hashes and smart contracts, and rigorous
performance evaluation, this paper can evolve from concept to practice. There
is a vivid potential to make OPEN-RAN supply chains corner to corner secure,
igniting further research and real-world deployment.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17633" title="Abstract">arXiv:2402.17633</a> [<a href="/pdf/2402.17633" title="Download PDF">pdf</a>, <a href="/format/2402.17633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Text Segmentation to Smart Chaptering: A Novel Benchmark for  Structuring Video Transcriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Retkowski%2C+F">Fabian Retkowski</a>, 
<a href="/search/cs?searchtype=author&query=Waibel%2C+A">Alexander Waibel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text segmentation is a fundamental task in natural language processing, where
documents are split into contiguous sections. However, prior research in this
area has been constrained by limited datasets, which are either small in scale,
synthesized, or only contain well-structured documents. In this paper, we
address these limitations by introducing a novel benchmark YTSeg focusing on
spoken content that is inherently more unstructured and both topically and
structurally diverse. As part of this work, we introduce an efficient
hierarchical segmentation model MiniSeg, that outperforms state-of-the-art
baselines. Lastly, we expand the notion of text segmentation to a more
practical "smart chaptering" task that involves the segmentation of
unstructured content, the generation of meaningful segment titles, and a
potential real-time application of the models.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17640" title="Abstract">arXiv:2402.17640</a> [<a href="/pdf/2402.17640" title="Download PDF">pdf</a>, <a href="/format/2402.17640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exergetic Port-Hamiltonian Systems Modeling Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lohmayer%2C+M">Markus Lohmayer</a>, 
<a href="/search/eess?searchtype=author&query=Lynch%2C+O">Owen Lynch</a>, 
<a href="/search/eess?searchtype=author&query=Leyendecker%2C+S">Sigrid Leyendecker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">Mathematical modeling of real-world physical systems requires the consistent
combination of a multitude of physical laws and phenomenological models. This
challenging task can be greatly simplified by hierarchically decomposing
systems. Moreover, the use of diagrams for expressing such decompositions helps
make the process more intuitive and facilitates communication, even with
non-experts. As an important requirement, models have to respect fundamental
physical laws such as the first and the second law of thermodynamics. While
some existing modeling frameworks can make such guarantees based on structural
properties of their models, they lack a formal graphical syntax. We present a
compositional and thermodynamically consistent modeling language with a
graphical syntax. As its semantics, port-Hamiltonian systems are endowed with
further structural properties and a fixed physical interpretation such that
thermodynamic consistency is ensured in a way that is closely related to the
GENERIC framework. While port-Hamiltonian systems are inspired by graphical
modeling with bond graphs, neither the link between the two, nor bond graphs
themselves, can be easily formalized. In contrast, our syntax is based on a
refinement of the well-studied operad of undirected wiring diagrams. The
language effectively decouples the construction of complex models via the
graphical syntax from physical concerns, which are dealt with only at the level
of primitive subsystems that represent elementary physical behaviors. As a
consequence, reuse of models and substitution of their parts becomes possible.
Finally, by construction, systems interact by exchanging exergy, i.e. energy
that is available for doing work, so the language is particularly well suited
for thermodynamic analysis and optimization.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17641" title="Abstract">arXiv:2402.17641</a> [<a href="/pdf/2402.17641" title="Download PDF">pdf</a>, <a href="/format/2402.17641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Learning is Effective for Large Deep Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yuesong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Daheim%2C+N">Nico Daheim</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+B">Bai Cong</a>, 
<a href="/search/cs?searchtype=author&query=Nickl%2C+P">Peter Nickl</a>, 
<a href="/search/cs?searchtype=author&query=Marconi%2C+G+M">Gian Maria Marconi</a>, 
<a href="/search/cs?searchtype=author&query=Bazan%2C+C">Clement Bazan</a>, 
<a href="/search/cs?searchtype=author&query=Yokota%2C+R">Rio Yokota</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>, 
<a href="/search/cs?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+E">Mohammad Emtiyaz Khan</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6llenhoff%2C+T">Thomas M&#xf6;llenhoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally. Code is available here: <a href="https://github.com/team-approx-bayes/ivon">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We give extensive empirical evidence against the common belief that
variational learning is ineffective for large neural networks. We show that an
optimizer called Improved Variational Online Newton (IVON) consistently matches
or outperforms Adam for training large networks such as GPT-2 and ResNets from
scratch. IVON's computational costs are nearly identical to Adam but its
predictive uncertainty is better. We show several new use cases of IVON where
we improve fine-tuning and model merging in Large Language Models, accurately
predict generalization error, and faithfully estimate sensitivity to data. We
find overwhelming evidence in support of effectiveness of variational learning.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17644" title="Abstract">arXiv:2402.17644</a> [<a href="/pdf/2402.17644" title="Download PDF">pdf</a>, <a href="/format/2402.17644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are LLMs Capable of Data-based Statistical and Causal Reasoning?  Benchmarking Advanced Quantitative Reasoning with Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zirui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xueqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yansong Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://xxxiaol.github.io/QRData/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Quantitative reasoning is a critical skill to analyze data, yet the
assessment of such ability remains limited. To address this gap, we introduce
the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate
Large Language Models' capability in statistical and causal reasoning with
real-world data. The benchmark comprises a carefully constructed dataset of 411
questions accompanied by data sheets from textbooks, online learning materials,
and academic papers. To compare models' quantitative reasoning abilities on
data and text, we enrich the benchmark with an auxiliary set of 290 text-only
questions, namely QRText. We evaluate natural language reasoning, program-based
reasoning, and agent reasoning methods including Chain-of-Thought,
Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models.
The strongest model GPT-4 achieves an accuracy of 58%, which has a large room
for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM
pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals
that models encounter difficulties in data analysis and causal reasoning, and
struggle in using causal knowledge and provided data simultaneously. Code and
data are in https://github.com/xxxiaol/QRData.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17645" title="Abstract">arXiv:2402.17645</a> [<a href="/pdf/2402.17645" title="Download PDF">pdf</a>, <a href="/format/2402.17645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SongComposer: A Large Language Model for Lyric and Melody Composition in  Song Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shuangrui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaoyi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+R">Rui Qian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Conghui He</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> project page: <a href="https://pjlab-songcomposer.github.io/">this https URL</a> code: <a href="https://github.com/pjlab-songcomposer/songcomposer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We present SongComposer, an innovative LLM designed for song composition. It
could understand and generate melodies and lyrics in symbolic song
representations, by leveraging the capability of LLM. Existing music-related
LLM treated the music as quantized audio signals, while such implicit encoding
leads to inefficient encoding and poor flexibility. In contrast, we resort to
symbolic song representation, the mature and efficient way humans designed for
music, and enable LLM to explicitly compose songs like humans. In practice, we
design a novel tuple design to format lyric and three note attributes (pitch,
duration, and rest duration) in the melody, which guarantees the correct LLM
understanding of musical symbols and realizes precise alignment between lyrics
and melody. To impart basic music understanding to LLM, we carefully collected
SongCompose-PT, a large-scale song pretraining dataset that includes lyrics,
melodies, and paired lyrics-melodies in either Chinese or English. After
adequate pre-training, 10K carefully crafted QA pairs are used to empower the
LLM with the instruction-following capability and solve diverse tasks. With
extensive experiments, SongComposer demonstrates superior performance in
lyric-to-melody generation, melody-to-lyric generation, song continuation, and
text-to-song creation, outperforming advanced LLMs like GPT-4.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17649" title="Abstract">arXiv:2402.17649</a> [<a href="/pdf/2402.17649" title="Download PDF">pdf</a>, <a href="/format/2402.17649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond prompt brittleness: Evaluating the reliability and consistency of  political worldviews in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ceron%2C+T">Tanise Ceron</a>, 
<a href="/search/cs?searchtype=author&query=Falk%2C+N">Neele Falk</a>, 
<a href="/search/cs?searchtype=author&query=Bari%C4%87%2C+A">Ana Bari&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaev%2C+D">Dmitry Nikolaev</a>, 
<a href="/search/cs?searchtype=author&query=Pad%C3%B3%2C+S">Sebastian Pad&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Due to the widespread use of large language models (LLMs) in ubiquitous
systems, we need to understand whether they embed a specific worldview and what
these views reflect. Recent studies report that, prompted with political
questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear
whether these leanings are reliable (robust to prompt variations) and whether
the leaning is consistent across policies and political leaning. We propose a
series of tests which assess the reliability and consistency of LLMs' stances
on political statements based on a dataset of voting-advice questionnaires
collected from seven EU countries and annotated for policy domains. We study
LLMs ranging in size from 7B to 70B parameters and find that their reliability
increases with parameter count. Larger models show overall stronger alignment
with left-leaning parties but differ among policy programs: They evince a
(left-wing) positive stance towards environment protection, social welfare but
also (right-wing) law and order, with no consistent preferences in foreign
policy, migration, and economy.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17651" title="Abstract">arXiv:2402.17651</a> [<a href="/pdf/2402.17651" title="Download PDF">pdf</a>, <a href="/format/2402.17651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CSI-Free Optimization of Reconfigurable Intelligent Surfaces with  Interference by Using Multiport Network Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abrardo%2C+A">A. Abrardo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconfigurable Intelligent Surfaces (RIS) will play a pivotal role in
next-generation wireless systems. Despite efforts to minimize pilot overhead
associated with channel estimation, the necessity of configuring the RIS
multiple times before obtaining reliable Channel State Information (CSI) may
significantly diminish their benefits. Therefore, we propose a CSI-free
approach that explores the feasibility of optimizing the RIS for the uplink of
a communication system in the presence of interfering users without relying on
CSI estimation but leveraging solely some a priori statistical knowledge of the
channel. In this context, we consider a multiport network model that accounts
for several aspects overlooked by traditional RIS models used in Communication
Theory, such as mutual coupling among scattering elements and the presence of
structural scattering. The proposed approach targets the maximization of the
average achievable rate and is shown to achieve performance that, in some
cases, can be very close to the case where the RIS is optimized leveraging
perfect CSI.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17652" title="Abstract">arXiv:2402.17652</a> [<a href="/pdf/2402.17652" title="Download PDF">pdf</a>, <a href="/format/2402.17652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigator: A Decentralized Scheduler for Latency-Sensitive ML Workflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuting Yang</a>, 
<a href="/search/cs?searchtype=author&query=Merlina%2C+A">Andrea Merlina</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Weijia Song</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T">Tiancheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Birman%2C+K">Ken Birman</a>, 
<a href="/search/cs?searchtype=author&query=Vitenberg%2C+R">Roman Vitenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We consider ML query processing in distributed systems where GPU-enabled
workers coordinate to execute complex queries: a computing style often seen in
applications that interact with users in support of image processing and
natural language processing. In such systems, coscheduling of GPU memory
management and task placement represents a promising opportunity. We propose
Navigator, a novel framework that unifies these functions to reduce job latency
while using resources efficiently, placing tasks where data dependencies will
be satisfied, collocating tasks from the same job (when this will not overload
the host or its GPU), and efficiently managing GPU memory. Comparison with
other state of the art schedulers shows a significant reduction in completion
times while requiring the same amount or even fewer resources. In one case,
just half the servers were needed for processing the same workload.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17653" title="Abstract">arXiv:2402.17653</a> [<a href="/pdf/2402.17653" title="Download PDF">pdf</a>, <a href="/format/2402.17653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Distributional Shift in Semantic Segmentation via Uncertainty  Estimation from Unlabelled Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Williams%2C+D+S+W">David S. W. Williams</a>, 
<a href="/search/cs?searchtype=author&query=De+Martini%2C+D">Daniele De Martini</a>, 
<a href="/search/cs?searchtype=author&query=Gadd%2C+M">Matthew Gadd</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+P">Paul Newman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Transactions on Robotics (T-RO)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Knowing when a trained segmentation model is encountering data that is
different to its training data is important. Understanding and mitigating the
effects of this play an important part in their application from a performance
and assurance perspective - this being a safety concern in applications such as
autonomous vehicles (AVs). This work presents a segmentation network that can
detect errors caused by challenging test domains without any additional
annotation in a single forward pass. As annotation costs limit the diversity of
labelled datasets, we use easy-to-obtain, uncurated and unlabelled data to
learn to perform uncertainty estimation by selectively enforcing consistency
over data augmentation. To this end, a novel segmentation benchmark based on
the SAX Dataset is used, which includes labelled test data spanning three
autonomous-driving domains, ranging in appearance from dense urban to off-road.
The proposed method, named Gamma-SSL, consistently outperforms uncertainty
estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark
- by up to 10.7% in area under the receiver operating characteristic (ROC)
curve and 19.2% in area under the precision-recall (PR) curve in the most
challenging of the three scenarios.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17655" title="Abstract">arXiv:2402.17655</a> [<a href="/pdf/2402.17655" title="Download PDF">pdf</a>, <a href="/format/2402.17655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidence-Aware Multi-Field Model Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuhan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qinglin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jia Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+L">Libin Zong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhenhua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Accurately predicting the probabilities of user feedback, such as clicks and
conversions, is critical for ad ranking and bidding. However, there often exist
unwanted mismatches between predicted probabilities and true likelihoods due to
the shift of data distributions and intrinsic model biases. Calibration aims to
address this issue by post-processing model predictions, and field-aware
calibration can adjust model output on different feature field values to
satisfy fine-grained advertising demands. Unfortunately, the observed samples
corresponding to certain field values can be too limited to make confident
calibrations, which may yield bias amplification and online disturbance. In
this paper, we propose a confidence-aware multi-field calibration method, which
adaptively adjusts the calibration intensity based on the confidence levels
derived from sample statistics. It also utilizes multiple feature fields for
joint model calibration with awareness of their importance to mitigate the data
sparsity effect of a single field. Extensive offline and online experiments
show the superiority of our method in boosting advertising performance and
reducing prediction deviations.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17659" title="Abstract">arXiv:2402.17659</a> [<a href="/pdf/2402.17659" title="Download PDF">pdf</a>, <a href="/ps/2402.17659" title="Download PostScript">ps</a>, <a href="/format/2402.17659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: Cryptocurrency Wallets -- A Security Review and Classification  based on Authentication Factors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Homoliak%2C+I">Ivan Homoliak</a>, 
<a href="/search/cs?searchtype=author&query=Pere%C5%A1%C3%ADni%2C+M">Martin Pere&#x161;&#xed;ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1812.03598">arXiv:1812.03598</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In this work, we review existing cryptocurrency wallet solutions with regard
to authentication methods and factors from the user's point of view. In
particular, we distinguish between authentication factors that are verified
against the blockchain and the ones verified locally (or against a centralized
party). With this in mind, we define notions for $k-factor$ authentication
against the blockchain and $k-factor$ authentication against the authentication
factors. Based on these notions, we propose a classification of authentication
schemes. We extend our classification to accommodate the threshold signatures
and signing transactions by centralized parties (such as exchanges or
co-signing services). Finally, we apply our classification to existing wallet
solutions, which we compare based on various security and key-management
features.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17660" title="Abstract">arXiv:2402.17660</a> [<a href="/pdf/2402.17660" title="Download PDF">pdf</a>, <a href="/format/2402.17660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular  Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pelaez%2C+R+P">Raul P. Pelaez</a>, 
<a href="/search/cs?searchtype=author&query=Simeon%2C+G">Guillem Simeon</a>, 
<a href="/search/cs?searchtype=author&query=Galvelis%2C+R">Raimondas Galvelis</a>, 
<a href="/search/cs?searchtype=author&query=Mirarchi%2C+A">Antonio Mirarchi</a>, 
<a href="/search/cs?searchtype=author&query=Eastman%2C+P">Peter Eastman</a>, 
<a href="/search/cs?searchtype=author&query=Doerr%2C+S">Stefan Doerr</a>, 
<a href="/search/cs?searchtype=author&query=Th%C3%B6lke%2C+P">Philipp Th&#xf6;lke</a>, 
<a href="/search/cs?searchtype=author&query=Markland%2C+T+E">Thomas E. Markland</a>, 
<a href="/search/cs?searchtype=author&query=De+Fabritiis%2C+G">Gianni De Fabritiis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biological Physics (physics.bio-ph); Chemical Physics (physics.chem-ph); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Achieving a balance between computational speed, prediction accuracy, and
universal applicability in molecular simulations has been a persistent
challenge. This paper presents substantial advancements in the TorchMD-Net
software, a pivotal step forward in the shift from conventional force fields to
neural network-based potentials. The evolution of TorchMD-Net into a more
comprehensive and versatile framework is highlighted, incorporating
cutting-edge architectures such as TensorNet. This transformation is achieved
through a modular design approach, encouraging customized applications within
the scientific community. The most notable enhancement is a significant
improvement in computational efficiency, achieving a very remarkable
acceleration in the computation of energy and forces for TensorNet models, with
performance gains ranging from 2-fold to 10-fold over previous iterations.
Other enhancements include highly optimized neighbor search algorithms that
support periodic boundary conditions and the smooth integration with existing
molecular dynamics frameworks. Additionally, the updated version introduces the
capability to integrate physical priors, further enriching its application
spectrum and utility in research. The software is available at
https://github.com/torchmd/torchmd-net.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17664" title="Abstract">arXiv:2402.17664</a> [<a href="/pdf/2402.17664" title="Download PDF">pdf</a>, <a href="/format/2402.17664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Differentiable Physics for Cloth Digitalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+D">Deshan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+N">Ningtao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures, to be published in CVPR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a new method for cloth digitalization. Deviating from existing
methods which learn from data captured under relatively casual settings, we
propose to learn from data captured in strictly tested measuring protocols, and
find plausible physical parameters of the cloths. However, such data is
currently absent, so we first propose a new dataset with accurate cloth
measurements. Further, the data size is considerably smaller than the ones in
current deep learning, due to the nature of the data capture process. To learn
from small data, we propose a new Bayesian differentiable cloth model to
estimate the complex material heterogeneity of real cloths. It can provide
highly accurate digitalization from very limited data samples. Through
exhaustive evaluation and comparison, we show our method is accurate in cloth
digitalization, efficient in learning from limited data samples, and general in
capturing material variations. Code and data are available
https://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17666" title="Abstract">arXiv:2402.17666</a> [<a href="/pdf/2402.17666" title="Download PDF">pdf</a>, <a href="/format/2402.17666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Deep Reinforcement Learning for Distributed Satellite  Routing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lozano-Cuadra%2C+F">Federico Lozano-Cuadra</a>, 
<a href="/search/cs?searchtype=author&query=Soret%2C+B">Beatriz Soret</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL)
approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each
satellite is an independent decision-making agent with a partial knowledge of
the environment, and supported by feedback received from the nearby agents.
Building on our previous work that introduced a Q-routing solution, the
contribution of this paper is to extend it to a deep learning framework able to
quickly adapt to the network and traffic changes, and based on two phases: (1)
An offline exploration learning phase that relies on a global Deep Neural
Network (DNN) to learn the optimal paths at each possible position and
congestion level; (2) An online exploitation phase with local, on-board,
pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes
offline that are then loaded for an efficient distributed routing online.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17671" title="Abstract">arXiv:2402.17671</a> [<a href="/pdf/2402.17671" title="Download PDF">pdf</a>, <a href="/format/2402.17671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Securing Reliability: A Brief Overview on Enhancing In-Context Learning  for Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yunpeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yaonan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhihong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaorun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoxing Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As foundation models (FMs) continue to shape the landscape of AI, the
in-context learning (ICL) paradigm thrives but also encounters issues such as
toxicity, hallucination, disparity, adversarial vulnerability, and
inconsistency. Ensuring the reliability and responsibility of FMs is crucial
for the sustainable development of the AI ecosystem. In this concise overview,
we investigate recent advancements in enhancing the reliability and
trustworthiness of FMs within ICL frameworks, focusing on four key
methodologies, each with its corresponding subgoals. We sincerely hope this
paper can provide valuable insights for researchers and practitioners
endeavoring to build safe and dependable FMs and foster a stable and consistent
ICL environment, thereby unlocking their vast potential.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17672" title="Abstract">arXiv:2402.17672</a> [<a href="/pdf/2402.17672" title="Download PDF">pdf</a>, <a href="/format/2402.17672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alkhatib%2C+M+Q">Mohammed Q. Alkhatib</a>, 
<a href="/search/cs?searchtype=author&query=Zitouni%2C+M+S">M. Sami Zitouni</a>, 
<a href="/search/cs?searchtype=author&query=Al-Saad%2C+M">Mina Al-Saad</a>, 
<a href="/search/cs?searchtype=author&query=Aburaed%2C+N">Nour Aburaed</a>, 
<a href="/search/cs?searchtype=author&query=Al-Ahmad%2C+H">Hussain Al-Ahmad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Polarimetric synthetic aperture radar (PolSAR) images encompass valuable
information that can facilitate extensive land cover interpretation and
generate diverse output products. Extracting meaningful features from PolSAR
data poses challenges distinct from those encountered in optical imagery. Deep
learning (DL) methods offer effective solutions for overcoming these challenges
in PolSAR feature extraction. Convolutional neural networks (CNNs) play a
crucial role in capturing PolSAR image characteristics by leveraging kernel
capabilities to consider local information and the complex-valued nature of
PolSAR data. In this study, a novel three-branch fusion of complex-valued CNN,
named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for
PolSAR image classification. To validate the performance of the proposed
method, classification results are compared against multiple state-of-the-art
approaches using the airborne synthetic aperture radar (AIRSAR) datasets of
Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The
results indicate that the proposed approach demonstrates improvements in
overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a
0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data
underscore the effectiveness of the SDF2Net model, revealing a promising
overall accuracy of 96.01% even with only a 1% sampling ratio.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17674" title="Abstract">arXiv:2402.17674</a> [<a href="/pdf/2402.17674" title="Download PDF">pdf</a>, <a href="/format/2402.17674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Highly-Efficient Persistent FIFO Queues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatourou%2C+P">Panagiota Fatourou</a>, 
<a href="/search/cs?searchtype=author&query=Giachoudis%2C+N">Nikos Giachoudis</a>, 
<a href="/search/cs?searchtype=author&query=Mallis%2C+G">George Mallis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In this paper, we study the question whether techniques employed, in a
conventional system, by state-of-the-art concurrent algorithms to avoid
contended hot spots are still efficient for recoverable computing in settings
with Non-Volatile Memory (NVM). We focus on concurrent FIFO queues that have
two end-points, head and tail, which are highly contended.
<br />We present a persistent FIFO queue implementation that performs a pair of
persistence instructions per operation (enqueue or dequeue). The algorithm
achieves to perform these instructions on variables of low contention by
employing Fetch&amp;Increment and using the state-of-the-art queue implementation
by Afek and Morrison (PPoPP'13). These result in performance that is up to 2x
faster than state-of-the-art persistent FIFO queue implementations.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17678" title="Abstract">arXiv:2402.17678</a> [<a href="/pdf/2402.17678" title="Download PDF">pdf</a>, <a href="/format/2402.17678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise  Sketch Instance Guided Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+S">Mohammad Sadil Khan</a>, 
<a href="/search/cs?searchtype=author&query=Dupont%2C+E">Elona Dupont</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S+A">Sk Aziz Ali</a>, 
<a href="/search/cs?searchtype=author&query=Cherenkova%2C+K">Kseniya Cherenkova</a>, 
<a href="/search/cs?searchtype=author&query=Kacem%2C+A">Anis Kacem</a>, 
<a href="/search/cs?searchtype=author&query=Aouada%2C+D">Djamila Aouada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Reverse engineering in the realm of Computer-Aided Design (CAD) has been a
longstanding aspiration, though not yet entirely realized. Its primary aim is
to uncover the CAD process behind a physical object given its 3D scan. We
propose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture to
recover the design history of a CAD model represented as a sequence of
sketch-and-extrusion from an input point cloud. Our model learns
visual-language representations by layer-wise cross-attention between point
cloud and CAD language embedding. In particular, a new Sketch instance Guided
Attention (SGA) module is proposed in order to reconstruct the fine-grained
details of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet not
only reconstructs a unique full design history of the corresponding CAD model
given an input point cloud but also provides multiple plausible design choices.
This allows for an interactive reverse engineering scenario by providing
designers with multiple next-step choices along with the design process.
Extensive experiments on publicly available CAD datasets showcase the
effectiveness of our approach against existing baseline models in two settings,
namely, full design history recovery and conditional auto-completion from point
clouds.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17679" title="Abstract">arXiv:2402.17679</a> [<a href="/pdf/2402.17679" title="Download PDF">pdf</a>, <a href="/ps/2402.17679" title="Download PostScript">ps</a>, <a href="/format/2402.17679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Emergence of Large Language Models in Static Analysis: A First Look  through Micro-Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venkatesh%2C+A+P+S">Ashwin Prasad Shivarpatna Venkatesh</a>, 
<a href="/search/cs?searchtype=author&query=Sabu%2C+S">Samkutty Sabu</a>, 
<a href="/search/cs?searchtype=author&query=Mir%2C+A+M">Amir M. Mir</a>, 
<a href="/search/cs?searchtype=author&query=Reis%2C+S">Sofia Reis</a>, 
<a href="/search/cs?searchtype=author&query=Bodden%2C+E">Eric Bodden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in: ICSE FORGE 2024 (AI Foundation Models and Software Engineering)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The application of Large Language Models (LLMs) in software engineering,
particularly in static analysis tasks, represents a paradigm shift in the
field. In this paper, we investigate the role that current LLMs can play in
improving callgraph analysis and type inference for Python programs. Using the
PyCG, HeaderGen, and TypeEvalPy micro-benchmarks, we evaluate 26 LLMs,
including OpenAI's GPT series and open-source models such as LLaMA. Our study
reveals that LLMs show promising results in type inference, demonstrating
higher accuracy than traditional methods, yet they exhibit limitations in
callgraph analysis. This contrast emphasizes the need for specialized
fine-tuning of LLMs to better suit specific static analysis tasks. Our findings
provide a foundation for further research towards integrating LLMs for static
analysis tasks.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17680" title="Abstract">arXiv:2402.17680</a> [<a href="/pdf/2402.17680" title="Download PDF">pdf</a>, <a href="/format/2402.17680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning  for Multimodal Video Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Huiyu Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lanxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Heqian Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Taijin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+B">Benliu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongliang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">To address the problem of catastrophic forgetting due to the invisibility of
old categories in sequential input, existing work based on relatively simple
categorization tasks has made some progress. In contrast, video captioning is a
more complex task in multimodal scenario, which has not been explored in the
field of incremental learning. After identifying this stability-plasticity
problem when analyzing video with sequential input, we originally propose a
method to Mitigate Catastrophic Forgetting in class-incremental learning for
multimodal Video Captioning (MCF-VC). As for effectively maintaining good
performance on old tasks at the macro level, we design Fine-grained Sensitivity
Selection (FgSS) based on the Mask of Linear's Parameters and Fisher
Sensitivity to pick useful knowledge from old tasks. Further, in order to
better constrain the knowledge characteristics of old and new tasks at the
specific feature level, we have created the Two-stage Knowledge Distillation
(TsKD), which is able to learn the new task well while weighing the old task.
Specifically, we design two distillation losses, which constrain the cross
modal semantic information of semantic attention feature map and the textual
information of the final outputs respectively, so that the inter-model and
intra-model stylized knowledge of the old class is retained while learning the
new class. In order to illustrate the ability of our model to resist
forgetting, we designed a metric CIDER_t to detect the stage forgetting rate.
Our experiments on the public dataset MSR-VTT show that the proposed method
significantly resists the forgetting of previous tasks without replaying old
samples, and performs well on the new task.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17682" title="Abstract">arXiv:2402.17682</a> [<a href="/pdf/2402.17682" title="Download PDF">pdf</a>, <a href="/format/2402.17682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NextLevelBERT: Investigating Masked Language Modeling with Higher-Level  Representations for Long Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Czinczoll%2C+T">Tamara Czinczoll</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6nes%2C+C">Christoph H&#xf6;nes</a>, 
<a href="/search/cs?searchtype=author&query=Schall%2C+M">Maximilian Schall</a>, 
<a href="/search/cs?searchtype=author&query=de+Melo%2C+G">Gerard de Melo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While (large) language models have significantly improved over the last
years, they still struggle to sensibly process long sequences found, e.g., in
books, due to the quadratic scaling of the underlying attention mechanism. To
address this, we propose NextLevelBERT, a Masked Language Model operating not
on tokens, but on higher-level semantic representations in the form of text
embeddings. We pretrain NextLevelBERT to predict the vector representation of
entire masked text chunks and evaluate the effectiveness of the resulting
document vectors on three task types: 1) Semantic Textual Similarity via
zero-shot document embeddings, 2) Long document classification, 3)
Multiple-choice question answering. We find that next level Masked Language
Modeling is an effective technique to tackle long-document use cases and can
outperform much larger embedding models as long as the required level of detail
is not too high. We make model and code available.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17688" title="Abstract">arXiv:2402.17688</a> [<a href="/pdf/2402.17688" title="Download PDF">pdf</a>, <a href="/format/2402.17688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel spectral methods for shock capturing and the removal of tygers in  computational fluid dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kolluru%2C+V+S+S">Venkata Sai Swetha Kolluru</a>, 
<a href="/search/math?searchtype=author&query=Besse%2C+N">Nicolas Besse</a>, 
<a href="/search/math?searchtype=author&query=Pandit%2C+R">Rahul Pandit</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Spectral methods yield numerical solutions of the Galerkin-truncated versions
of nonlinear partial differential equations involved especially in fluid
dynamics. In the presence of discontinuities, such as shocks, spectral
approximations develop Gibbs oscillations near the discontinuity. This causes
the numerical solution to deviate quickly from the true solution. For spectral
approximations of the 1D inviscid Burgers equation, nonlinear wave resonances
lead to the formation of tygers in well-resolved areas of the flow, far from
the shock. Recently, Besse(to be published) has proposed novel spectral
relaxation (SR) and spectral purging (SP) schemes for the removal of tygers and
Gibbs oscillations in spectral approximations of nonlinear conservation laws.
For the 1D inviscid Burgers equation, it is shown that the novel SR and SP
approximations of the solution converge strongly in L2 norm to the entropic
weak solution, under an appropriate choice of kernels and related parameters.
In this work, we carry out a detailed numerical investigation of SR and SP
schemes when applied to the 1D inviscid Burgers equation and report the
efficiency of shock capture and the removal of tygers. We then extend our study
to systems of nonlinear hyperbolic conservation laws - such as the 2x2 system
of the shallow water equations and the standard 3x3 system of 1D compressible
Euler equations. For the latter, we generalise the implementation of SR methods
to non-periodic problems using Chebyshev polynomials. We then turn to singular
flow in the 1D wall approximation of the 3D-axisymmetric wall-bounded
incompressible Euler equation. Here, in order to determine the blowup time of
the solution, we compare the decay of the width of the analyticity strip,
obtained from the pure pseudospectral method, with the improved estimate
obtained using the novel spectral relaxation scheme.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17689" title="Abstract">arXiv:2402.17689</a> [<a href="/pdf/2402.17689" title="Download PDF">pdf</a>, <a href="/format/2402.17689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QoS prediction in radio vehicular environments via prior user  information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ain%2C+N+U">Noor Ul Ain</a>, 
<a href="/search/cs?searchtype=author&query=Hernang%C3%B3mez%2C+R">Rodrigo Hernang&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Palaios%2C+A">Alexandros Palaios</a>, 
<a href="/search/cs?searchtype=author&query=Kasparick%2C+M">Martin Kasparick</a>, 
<a href="/search/cs?searchtype=author&query=Sta%C5%84czak%2C+S">S&#x142;awomir Sta&#x144;czak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reliable wireless communications play an important role in the automotive
industry as it helps to enhance current use cases and enable new ones such as
connected autonomous driving, platooning, cooperative maneuvering, teleoperated
driving, and smart navigation. These and other use cases often rely on specific
quality of service (QoS) levels for communication. Recently, the area of
predictive quality of service (QoS) has received a great deal of attention as a
key enabler to forecast communication quality well enough in advance. However,
predicting QoS in a reliable manner is a notoriously difficult task. In this
paper, we evaluate ML tree-ensemble methods to predict QoS in the range of
minutes with data collected from a cellular test network. We discuss radio
environment characteristics and we showcase how these can be used to improve ML
performance and further support the uptake of ML in commercial networks.
Specifically, we use the correlations of the measurements coming from the radio
environment by including information of prior vehicles to enhance the
prediction of the target vehicles. Moreover, we are extending prior art by
showing how longer prediction horizons can be supported.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17690" title="Abstract">arXiv:2402.17690</a> [<a href="/pdf/2402.17690" title="Download PDF">pdf</a>, <a href="/format/2402.17690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Vehicles: Evolution of Artificial Intelligence and Learning  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shetiya%2C+S+S">Sneha Sudhir Shetiya</a>, 
<a href="/search/cs?searchtype=author&query=Garikapati%2C+D">Divya Garikapati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The advent of autonomous vehicles has heralded a transformative era in
transportation, reshaping the landscape of mobility through cutting-edge
technologies. Central to this evolu- tion is the integration of Artificial
Intelligence (AI) and learning algorithms, propelling vehicles into realms of
unprecedented autonomy. This paper provides a comprehensive exploration of the
evolutionary trajectory of AI within autonomous vehicles, tracing the journey
from foundational principles to the most recent advancements. Commencing with a
current landscape overview, the paper delves into the fundamental role of AI in
shaping the autonomous decision-making capabilities of vehicles. It elucidates
the steps involved in the AI-powered development life cycle in vehicles,
addressing ethical considerations and bias in AI-driven software development
for autonomous vehicles. The study presents statis- tical insights into the
usage and types of AI/learning algorithms over the years, showcasing the
evolving research landscape within the automotive industry. Furthermore, the
paper highlights the pivotal role of parameters in refining algorithms for both
trucks and cars, facilitating vehicles to adapt, learn, and improve performance
over time. It concludes by outlining different levels of autonomy, elucidating
the nuanced usage of AI and learning algorithms, and automating key tasks at
each level. Additionally, the document discusses the variation in software
package sizes across different autonomy levels
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17695" title="Abstract">arXiv:2402.17695</a> [<a href="/pdf/2402.17695" title="Download PDF">pdf</a>, <a href="/format/2402.17695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometric Deep Learning for Computer-Aided Design: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heidari%2C+N">Negar Heidari</a>, 
<a href="/search/cs?searchtype=author&query=Iosifidis%2C+A">Alexandros Iosifidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 14 figures, journal article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Geometric Deep Learning techniques have become a transformative force in the
field of Computer-Aided Design (CAD), and have the potential to revolutionize
how designers and engineers approach and enhance the design process. By
harnessing the power of machine learning-based methods, CAD designers can
optimize their workflows, save time and effort while making better informed
decisions, and create designs that are both innovative and practical. The
ability to process the CAD designs represented by geometric data and to analyze
their encoded features enables the identification of similarities among diverse
CAD models, the proposition of alternative designs and enhancements, and even
the generation of novel design alternatives. This survey offers a comprehensive
overview of learning-based methods in computer-aided design across various
categories, including similarity analysis and retrieval, 2D and 3D CAD model
synthesis, and CAD generation from point clouds. Additionally, it provides a
complete list of benchmark datasets and their characteristics, along with
open-source codes that have propelled research in this domain. The final
discussion delves into the challenges prevalent in this field, followed by
potential future research directions in this rapidly evolving field.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17698" title="Abstract">arXiv:2402.17698</a> [<a href="/pdf/2402.17698" title="Download PDF">pdf</a>, <a href="/format/2402.17698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning reduced-order Quadratic-Linear models in Process Engineering  using Operator Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gosea%2C+I+V">Ion Victor Gosea</a>, 
<a href="/search/math?searchtype=author&query=Peterson%2C+L">Luisa Peterson</a>, 
<a href="/search/math?searchtype=author&query=Goyal%2C+P">Pawan Goyal</a>, 
<a href="/search/math?searchtype=author&query=Bremer%2C+J">Jens Bremer</a>, 
<a href="/search/math?searchtype=author&query=Sundmacher%2C+K">Kai Sundmacher</a>, 
<a href="/search/math?searchtype=author&query=Benner%2C+P">Peter Benner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we address the challenge of efficiently modeling dynamical
systems in process engineering. We use reduced-order model learning,
specifically operator inference. This is a non-intrusive, data-driven method
for learning dynamical systems from time-domain data. The application in our
study is carbon dioxide methanation, an important reaction within the
Power-to-X framework, to demonstrate its potential. The numerical results show
the ability of the reduced-order models constructed with operator inference to
provide a reduced yet accurate surrogate solution. This represents an important
milestone towards the implementation of fast and reliable digital twin
architectures.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17699" title="Abstract">arXiv:2402.17699</a> [<a href="/pdf/2402.17699" title="Download PDF">pdf</a>, <a href="/format/2402.17699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-based Discrete Sampling with Automatic Cyclical Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pynadath%2C+P">Patrick Pynadath</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+R">Riddhiman Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Hariharan%2C+A">Arun Hariharan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Discrete distributions, particularly in high-dimensional deep models, are
often highly multimodal due to inherent discontinuities. While gradient-based
discrete sampling has proven effective, it is susceptible to becoming trapped
in local modes due to the gradient information. To tackle this challenge, we
propose an automatic cyclical scheduling, designed for efficient and accurate
sampling in multimodal discrete distributions. Our method contains three key
components: (1) a cyclical step size schedule where large steps discover new
modes and small steps exploit each mode; (2) a cyclical balancing schedule,
ensuring ``balanced" proposals for given step sizes and high efficiency of the
Markov chain; and (3) an automatic tuning scheme for adjusting the
hyperparameters in the cyclical schedules, allowing adaptability across diverse
datasets with minimal tuning. We prove the non-asymptotic convergence and
inference guarantee for our method in general discrete distributions. Extensive
experiments demonstrate the superiority of our method in sampling complex
multimodal discrete distributions.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17700" title="Abstract">arXiv:2402.17700</a> [<a href="/pdf/2402.17700" title="Download PDF">pdf</a>, <a href="/format/2402.17700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAVEL: Evaluating Interpretability Methods on Disentangling Language  Model Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhengxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>, 
<a href="/search/cs?searchtype=author&query=Geiger%2C+A">Atticus Geiger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Individual neurons participate in the representation of multiple high-level
concepts. To what extent can different interpretability methods successfully
disentangle these roles? To help address this question, we introduce RAVEL
(Resolving Attribute-Value Entanglements in Language Models), a dataset that
enables tightly controlled, quantitative comparisons between a variety of
existing interpretability methods. We use the resulting conceptual framework to
define the new method of Multi-task Distributed Alignment Search (MDAS), which
allows us to find distributed representations satisfying multiple causal
criteria. With Llama2-7B as the target language model, MDAS achieves
state-of-the-art results on RAVEL, demonstrating the importance of going beyond
neuron-level analyses to identify features distributed across activations. We
release our benchmark at https://github.com/explanare/ravel.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17703" title="Abstract">arXiv:2402.17703</a> [<a href="/pdf/2402.17703" title="Download PDF">pdf</a>, <a href="/ps/2402.17703" title="Download PostScript">ps</a>, <a href="/format/2402.17703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Free Deep Deterministic Policy Gradient Controller for Setpoint  Tracking of Non-minimum Phase Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tavakkoli%2C+F">Fatemeh Tavakkoli</a>, 
<a href="/search/eess?searchtype=author&query=Sarhadi%2C+P">Pouria Sarhadi</a>, 
<a href="/search/eess?searchtype=author&query=Clement%2C+B">Benoit Clement</a>, 
<a href="/search/eess?searchtype=author&query=Naeem%2C+W">Wasif Naeem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Deep Reinforcement Learning (DRL) techniques have received significant
attention in control and decision-making algorithms. Most applications involve
complex decision-making systems, justified by the algorithms' computational
power and cost. While model-based versions are emerging, model-free DRL
approaches are intriguing for their independence from models, yet they remain
relatively less explored in terms of performance, particularly in applied
control. This study conducts a thorough performance analysis comparing the
data-driven DRL paradigm with a classical state feedback controller, both
designed based on the same cost (reward) function of the linear quadratic
regulator (LQR) problem. Twelve additional performance criteria are introduced
to assess the controllers' performance, independent of the LQR problem for
which they are designed. Two Deep Deterministic Policy Gradient (DDPG)-based
controllers are developed, leveraging DDPG's widespread reputation. These
controllers are aimed at addressing a challenging setpoint tracking problem in
a Non-Minimum Phase (NMP) system. The performance and robustness of the
controllers are assessed in the presence of operational challenges, including
disturbance, noise, initial conditions, and model uncertainties. The findings
suggest that the DDPG controller demonstrates promising behavior under rigorous
test conditions. Nevertheless, further improvements are necessary for the DDPG
controller to outperform classical methods in all criteria. While DRL
algorithms may excel in complex environments owing to the flexibility in the
reward function definition, this paper offers practical insights and a
comparison framework specifically designed to evaluate these algorithms within
the context of control engineering.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17705" title="Abstract">arXiv:2402.17705</a> [<a href="/pdf/2402.17705" title="Download PDF">pdf</a>, <a href="/format/2402.17705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning for Estimating Heterogeneous Treatment Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makhija%2C+D">Disha Makhija</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+J">Joydeep Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yejin Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning methods for estimating heterogeneous treatment effects (HTE)
facilitate large-scale personalized decision-making across various domains such
as healthcare, policy making, education, and more. Current machine learning
approaches for HTE require access to substantial amounts of data per treatment,
and the high costs associated with interventions makes centrally collecting so
much data for each intervention a formidable challenge. To overcome this
obstacle, in this work, we propose a novel framework for collaborative learning
of HTE estimators across institutions via Federated Learning. We show that even
under a diversity of interventions and subject populations across clients, one
can jointly learn a common feature representation, while concurrently and
privately learning the specific predictive functions for outcomes under
distinct interventions across institutions. Our framework and the associated
algorithm are based on this insight, and leverage tabular transformers to map
multiple input data to feature representations which are then used for outcome
prediction via multi-task learning. We also propose a novel way of federated
training of personalised transformers that can work with heterogeneous input
feature spaces. Experimental results on real-world clinical trial data
demonstrate the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17706" title="Abstract">arXiv:2402.17706</a> [<a href="/pdf/2402.17706" title="Download PDF">pdf</a>, <a href="/format/2402.17706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive quantization with mixed-precision based on low-cost proxy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junzhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+S">Senmao Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shunli Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by icassp2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">It is critical to deploy complicated neural network models on hardware with
limited resources. This paper proposes a novel model quantization method, named
the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ),
which contains three key modules. The hardware-aware module is designed by
considering the hardware limitations, while an adaptive mixed-precision
quantization module is developed to evaluate the quantization sensitivity by
using the Hessian matrix and Pareto frontier techniques. Integer linear
programming is used to fine-tune the quantization across different layers. Then
the low-cost proxy neural architecture search module efficiently explores the
ideal quantization hyperparameters. Experiments on the ImageNet demonstrate
that the proposed LCPAQ achieves comparable or superior quantization accuracy
to existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the search
time compared with existing methods, which provides a shortcut in practical
quantization use for resource-limited devices.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17709" title="Abstract">arXiv:2402.17709</a> [<a href="/pdf/2402.17709" title="Download PDF">pdf</a>, <a href="/format/2402.17709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Case-Based or Rule-Based: How Do Transformers Do the Math?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaojuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haotong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite the impressive performance in a variety of complex tasks, modern
large language models (LLMs) still have trouble dealing with some math problems
that are simple and intuitive for humans, such as addition. While we can easily
learn basic rules of addition and apply them to new problems of any length,
LLMs struggle to do the same. Instead, they may rely on similar "cases" seen in
the training corpus for help. We define these two different reasoning
mechanisms as "rule-based reasoning" and "case-based reasoning". Since
rule-based reasoning is essential for acquiring the systematic generalization
ability, we aim to explore exactly whether transformers use rule-based or
case-based reasoning for math problems. Through carefully designed intervention
experiments on five math tasks, we confirm that transformers are performing
case-based reasoning, no matter whether scratchpad is used, which aligns with
the previous observations that transformers use subgraph matching/shortcut
learning to reason. To mitigate such problems, we propose a Rule-Following
Fine-Tuning (RFFT) technique to teach transformers to perform rule-based
reasoning. Specifically, we provide explicit rules in the input and then
instruct transformers to recite and follow the rules step by step. Through
RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to
generalize to up to 12-digit addition with over 95% accuracy, which is over 40%
higher than scratchpad. The significant improvement demonstrates that teaching
LLMs to explicitly use rules helps them learn rule-based reasoning and
generalize better in length.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17710" title="Abstract">arXiv:2402.17710</a> [<a href="/pdf/2402.17710" title="Download PDF">pdf</a>, <a href="/format/2402.17710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Neural Network Binarization with Forward and Backward  Proximal Quantizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yiwei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yaoliang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Nia%2C+V+P">Vahid Partovi Nia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In neural network binarization, BinaryConnect (BC) and its variants are
considered the standard. These methods apply the sign function in their forward
pass and their respective gradients are backpropagated to update the weights.
However, the derivative of the sign function is zero whenever defined, which
consequently freezes training. Therefore, implementations of BC (e.g., BNN)
usually replace the derivative of sign in the backward computation with
identity or other approximate gradient alternatives. Although such practice
works well empirically, it is largely a heuristic or ''training trick.'' We aim
at shedding some light on these training tricks from the optimization
perspective. Building from existing theory on ProxConnect (PC, a generalization
of BC), we (1) equip PC with different forward-backward quantizers and obtain
ProxConnect++ (PC++) that includes existing binarization techniques as special
cases; (2) derive a principled way to synthesize forward-backward quantizers
with automatic theoretical guarantees; (3) illustrate our theory by proposing
an enhanced binarization algorithm BNN++; (4) conduct image classification
experiments on CNNs and vision transformers, and empirically verify that BNN++
generally achieves competitive results on binarizing these models.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17711" title="Abstract">arXiv:2402.17711</a> [<a href="/pdf/2402.17711" title="Download PDF">pdf</a>, <a href="/format/2402.17711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interior penalty discontinuous Galerkin methods for the nearly  incompressible elasticity eigenvalue problem with heterogeneous media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Khan%2C+A">Arbaz Khan</a>, 
<a href="/search/math?searchtype=author&query=Lepe%2C+F">Felipe Lepe</a>, 
<a href="/search/math?searchtype=author&query=Vellojin%2C+J">Jesus Vellojin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper studies the family of interior penalty discontinuous Galerkin
methods for solving the Herrmann formulation of the linear elasticity
eigenvalue problem in heterogeneous media. By employing a weighted Lam\'e
coefficient norm within the framework of non-compact operators theory, we prove
convergence of both continuous and discrete eigenvalue problems as the mesh
size approaches zero, independently of the Lam\'e constants. Additionally, we
conduct an a posteriori analysis and propose a reliable and efficient
estimator. The theoretical findings are supported by numerical experiments.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17712" title="Abstract">arXiv:2402.17712</a> [<a href="/pdf/2402.17712" title="Download PDF">pdf</a>, <a href="/format/2402.17712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A $p$-version of convolution quadrature in wave propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rieder%2C+A">Alexander Rieder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider a novel way of discretizing wave scattering problems using the
general formalism of convolution quadrature, but instead of reducing the
timestep size ($h$-method), we achieve accuracy by increasing the order of the
method ($p$-method). We base this method on discontinuous Galerkin timestepping
and use the Z-transform. We show that for a certain class of incident waves,
the resulting schemes observes(root)-exponential convergence rate with respect
to the number of boundary integral operators that need to be applied. Numerical
experiments confirm the findings.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17713" title="Abstract">arXiv:2402.17713</a> [<a href="/pdf/2402.17713" title="Download PDF">pdf</a>, <a href="/format/2402.17713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An all-frequency stable surface integral equation algorithm for  electromagnetism in 3-D unbounded penetrable media: Continuous and  fully-discrete model analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ganesh%2C+M">Mahadevan Ganesh</a>, 
<a href="/search/math?searchtype=author&query=Hawkins%2C+S+C">Stuart C. Hawkins</a>, 
<a href="/search/math?searchtype=author&query=Volkov%2C+D">Darko Volkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">We use the time-harmonic Maxwell partial differential equations (PDEs) to
model the wave propagation in 3-D space, which comprises a closed penetrable
scatterer and its unbounded free-space complement. Surface integral equations
(SIEs) that are equivalent to the time-harmonic Maxwell PDEs provide an
efficient framework to directly model the surface electromagnetic fields and
hence the RCS.The equivalent SIE system on the interface has the advantages
that: (a) it avoids truncation of the unbounded region and the solution exactly
satisfies the radiation condition; and (b) the surface-fields solution yields
the unknowns in the Maxwell PDEs through surface potential representations of
the interior and exterior fields. The Maxwell PDE system has been proven
(several decades ago) to be stable for all frequencies, that is, (i) it does
not possess eigenfrequencies (it is well-posed); and (ii) it does not suffer
from low-frequency. However, weakly-singular SIE reformulations of the PDE
satisfying these two properties, subject to a stabilization constraint, were
derived and mathematically proven only about a decade ago (see {J. Math. Anal.
Appl. 412 (2014) 277-300}). The aim of this article is two-fold: (I) To effect
a robust coupling of the stabilization constraint to the weakly singular SIE
and use mathematical analysis to establish that the resulting continuous
weakly-singular second-kind self-adjoint SIE system (without constraints)
retains all-frequency stability; and (II) To apply a fully-discrete spectral
algorithm for the all-frequency-stable weakly-singular second-kind SIE, and
prove spectral accuracy of the algorithm. We numerically demonstrate the
high-order accuracy of the algorithm using several dielectric and absorbing
benchmark scatterers with curved surfaces.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17715" title="Abstract">arXiv:2402.17715</a> [<a href="/pdf/2402.17715" title="Download PDF">pdf</a>, <a href="/ps/2402.17715" title="Download PostScript">ps</a>, <a href="/format/2402.17715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Central Primitives for Quantum Cryptography with Classical  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+K">Kai-Min Chung</a>, 
<a href="/search/cs?searchtype=author&query=Goldin%2C+E">Eli Goldin</a>, 
<a href="/search/cs?searchtype=author&query=Gray%2C+M">Matthew Gray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Recent work has introduced the "Quantum-Computation Classical-Communication"
(QCCC) (Chung et. al.) setting for cryptography. There has been some evidence
that One Way Puzzles (OWPuzz) are the natural central cryptographic primitive
for this setting (Khurana and Tomer). For a primitive to be considered central
it should have several characteristics. It should be well behaved (which for
this paper we will think of as having amplification, combiners, and universal
constructions); it should be implied by a wide variety of other primitives; and
it should be equivalent to some class of useful primitives. We present
combiners, correctness and security amplification, and a universal construction
for OWPuzz. Our proof of security amplification uses a new and cleaner version
construction of EFI from OWPuzz (in comparison to the result of Khurana and
Tomer) that generalizes to weak OWPuzz and is the most technically involved
section of the paper. It was previously known that OWPuzz are implied by other
primitives of interest including commitments, symmetric key encryption, one way
state generators (OWSG), and therefore pseudorandom states (PRS). However we
are able to rule out OWPuzz's equivalence to many of these primitives by
showing a black box separation between general OWPuzz and a restricted class of
OWPuzz (those with efficient verification, which we call EV-OWPuzz). We then
show that EV-OWPuzz are also implied by most of these primitives, which
separates them from OWPuzz as well. This separation also separates extending
PRS from highly compressing PRS answering an open question of Ananth et. al.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17717" title="Abstract">arXiv:2402.17717</a> [<a href="/pdf/2402.17717" title="Download PDF">pdf</a>, <a href="/format/2402.17717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AmbigNLG: Addressing Task Ambiguity in Instruction for NLG
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niwa%2C+A">Ayana Niwa</a>, 
<a href="/search/cs?searchtype=author&query=Iso%2C+H">Hayate Iso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this study, we introduce AmbigNLG, a new task designed to tackle the
challenge of task ambiguity in instructions for Natural Language Generation
(NLG) tasks. Despite the impressive capabilities of Large Language Models
(LLMs) in understanding and executing a wide range of tasks through natural
language interaction, their performance is significantly hindered by the
ambiguity present in real-world instructions. To address this, AmbigNLG seeks
to identify and mitigate such ambiguities, aiming to refine instructions to
match user expectations better. We introduce a dataset, AmbigSNI-NLG,
consisting of 2,500 instances, and develop an ambiguity taxonomy for
categorizing and annotating instruction ambiguities. Our approach demonstrates
substantial improvements in text generation quality, highlighting the critical
role of clear and specific instructions in enhancing LLM performance in NLG
tasks.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17718" title="Abstract">arXiv:2402.17718</a> [<a href="/pdf/2402.17718" title="Download PDF">pdf</a>, <a href="/ps/2402.17718" title="Download PostScript">ps</a>, <a href="/format/2402.17718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Digital Twin Framework in Additive Manufacturing: Machine  Learning and Bayesian Optimization for Time Series Process Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karkaria%2C+V">Vispi Karkaria</a>, 
<a href="/search/cs?searchtype=author&query=Goeckner%2C+A">Anthony Goeckner</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+R">Rujing Zha</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianjing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jian Cao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R+X">Robert X. Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 Pages, 10 Figures, 1 Table, NAMRC Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Laser-directed-energy deposition (DED) offers advantages in additive
manufacturing (AM) for creating intricate geometries and material grading. Yet,
challenges like material inconsistency and part variability remain, mainly due
to its layer-wise fabrication. A key issue is heat accumulation during DED,
which affects the material microstructure and properties. While closed-loop
control methods for heat management are common in DED research, few integrate
real-time monitoring, physics-based modeling, and control in a unified
framework. Our work presents a digital twin (DT) framework for real-time
predictive control of DED process parameters to meet specific design
objectives. We develop a surrogate model using Long Short-Term Memory
(LSTM)-based machine learning with Bayesian Inference to predict temperatures
in DED parts. This model predicts future temperature states in real time. We
also introduce Bayesian Optimization (BO) for Time Series Process Optimization
(BOTSPO), based on traditional BO but featuring a unique time series process
profile generator with reduced dimensions. BOTSPO dynamically optimizes
processes, identifying optimal laser power profiles to attain desired
mechanical properties. The established process trajectory guides online
optimizations, aiming to enhance performance. This paper outlines the digital
twin framework's components, promoting its integration into a comprehensive
system for AM.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17720" title="Abstract">arXiv:2402.17720</a> [<a href="/pdf/2402.17720" title="Download PDF">pdf</a>, <a href="/format/2402.17720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The SMART approach to instance-optimal online learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Siddhartha Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+A">Alankrita Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C+L">Christina Lee Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Information Theory (cs.IT)

</div>
<p class="mathjax">We devise an online learning algorithm -- titled Switching via Monotone
Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret
that is instance optimal, i.e., simultaneously competitive on every input
sequence compared to the performance of the follow-the-leader (FTL) policy and
the worst case guarantee of any other input policy. We show that the regret of
the SMART policy on any input sequence is within a multiplicative factor
$e/(e-1) \approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the
sequence, and 2) the upper bound on regret guaranteed by the given worst-case
policy. This implies a strictly stronger guarantee than typical
`best-of-both-worlds' bounds as the guarantee holds for every input sequence
regardless of how it is generated. SMART is simple to implement as it begins by
playing FTL and switches at most once during the time horizon to the worst-case
algorithm. Our approach and results follow from an operational reduction of
instance optimal online learning to competitive analysis for the ski-rental
problem. We complement our competitive ratio upper bounds with a fundamental
lower bound showing that over all input sequences, no algorithm can get better
than a $1.43$-fraction of the minimum regret achieved by FTL and the
minimax-optimal policy. We also present a modification of SMART that combines
FTL with a ``small-loss" algorithm to achieve instance optimality between the
regret of FTL and the small loss regret bound.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17721" title="Abstract">arXiv:2402.17721</a> [<a href="/pdf/2402.17721" title="Download PDF">pdf</a>, <a href="/format/2402.17721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content-Centric Prototyping of Generative AI Applications: Emerging  Approaches and Challenges in Collaborative Software Teams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramonyam%2C+H">Hari Subramonyam</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+D">Divy Thakkar</a>, 
<a href="/search/cs?searchtype=author&query=Dieber%2C+J">J&#xfc;rgen Dieber</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Anoop Sinha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Generative AI models are increasingly powering software applications,
offering the capability to produce expressive content across varied contexts.
However, unlike previous iterations of human-AI design, the emerging design
process for generative capabilities primarily hinges on prompt engineering
strategies. Given this fundamental shift in approach, our work aims to
understand how collaborative software teams set up and apply design guidelines
and values, iteratively prototype prompts, and evaluate prompts to achieve
desired outcomes. We conducted design studies with 39 industry professionals,
including designers, software engineers, and product managers. Our findings
reveal a content-centric prototyping approach in which teams begin with the
content they want to generate, then identify specific attributes, constraints,
and values, and explore methods to give users the ability to influence and
interact with those attributes. Based on associated challenges, such as the
lack of model interpretability and overfitting the design to examples, we
outline considerations for generative AI prototyping.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17723" title="Abstract">arXiv:2402.17723</a> [<a href="/pdf/2402.17723" title="Download PDF">pdf</a>, <a href="/format/2402.17723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion  Latent Aligners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yazhou Xing</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yingqing He</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zeyue Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CVPR 2024. Project website: <a href="https://yzxing87.github.io/Seeing-and-Hearing/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video and audio content creation serves as the core technique for the movie
industry and professional users. Recently, existing diffusion-based methods
tackle video and audio generation separately, which hinders the technique
transfer from academia to industry. In this work, we aim at filling the gap,
with a carefully designed optimization-based framework for cross-visual-audio
and joint-visual-audio generation. We observe the powerful generation ability
of off-the-shelf video or audio generation models. Thus, instead of training
the giant models from scratch, we propose to bridge the existing strong models
with a shared latent representation space. Specifically, we propose a
multimodality latent aligner with the pre-trained ImageBind model. Our latent
aligner shares a similar core as the classifier guidance that guides the
diffusion denoising process during inference time. Through carefully designed
optimization strategy and loss functions, we show the superior performance of
our method on joint video-audio generation, visual-steered audio generation,
and audio-steered visual generation tasks. The project website can be found at
https://yzxing87.github.io/Seeing-and-Hearing/
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17724" title="Abstract">arXiv:2402.17724</a> [<a href="/pdf/2402.17724" title="Download PDF">pdf</a>, <a href="/ps/2402.17724" title="Download PostScript">ps</a>, <a href="/format/2402.17724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elliptic Reconstruction and A Posteriori Error Estimates for Parabolic  Variational Inequalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Antil%2C+H">Harbir Antil</a>, 
<a href="/search/math?searchtype=author&query=Khandelwal%2C+R">Rohit Khandelwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Optimization and Control (math.OC)

</div>
<p class="mathjax">Elliptic reconstruction property, originally introduced by Makridakis and
Nochetto for linear parabolic problems, is a well-known tool to derive optimal
a posteriori error estimates. No such results are known for nonlinear and
nonsmooth problems such as parabolic variational inequalities (VIs). This
article establishes the elliptic reconstruction property for parabolic VIs and
derives a posteriori error estimates in $L^{\infty}(0,T;L^{2}(\Omega))$ and
$L^{\infty}(0,T;L^{\infty}(\Omega))$, respectively. As an application, the
residual-type error estimates are presented.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17726" title="Abstract">arXiv:2402.17726</a> [<a href="/pdf/2402.17726" title="Download PDF">pdf</a>, <a href="/format/2402.17726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VRP-SAM: SAM with Visual Reference Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanpeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiahui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+E">Errui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingdong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zechao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2024; It is not the camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that
empowers the Segment Anything Model (SAM) to utilize annotated reference images
as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM
can utilize annotated reference images to comprehend specific objects and
perform segmentation of specific objects in target image. It is note that the
VRP encoder can support a variety of annotation formats for reference images,
including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}.
VRP-SAM achieves a breakthrough within the SAM framework by extending its
versatility and applicability while preserving SAM's inherent strengths, thus
enhancing user-friendliness. To enhance the generalization ability of VRP-SAM,
the VRP encoder adopts a meta-learning strategy. To validate the effectiveness
of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO
datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual
reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM
demonstrates strong generalization capabilities, allowing it to perform
segmentation of unseen objects and enabling cross-domain segmentation.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17729" title="Abstract">arXiv:2402.17729</a> [<a href="/pdf/2402.17729" title="Download PDF">pdf</a>, <a href="/format/2402.17729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fairness-Aware Adversarial Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanghao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianle Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+R">Ronghui Mu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaowei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+W">Wenjie Ruan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work will appear in the CVPR 2024 conference proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Although adversarial training (AT) has proven effective in enhancing the
model's robustness, the recently revealed issue of fairness in robustness has
not been well addressed, i.e. the robust accuracy varies significantly among
different categories. In this paper, instead of uniformly evaluating the
model's average class performance, we delve into the issue of robust fairness,
by considering the worst-case distribution across various classes. We propose a
novel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a
generalization of conventional AT, we re-define the problem of adversarial
training as a min-max-max framework, to ensure both robustness and fairness of
the trained model. Specifically, by taking advantage of distributional robust
optimization, our method aims to find the worst distribution among different
categories, and the solution is guaranteed to obtain the upper bound
performance with high probability. In particular, FAAL can fine-tune an unfair
robust model to be fair within only two epochs, without compromising the
overall clean and robust accuracies. Extensive experiments on various image
datasets validate the superior performance and efficiency of the proposed FAAL
compared to other state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17730" title="Abstract">arXiv:2402.17730</a> [<a href="/pdf/2402.17730" title="Download PDF">pdf</a>, <a href="/format/2402.17730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Markovletics: Methods and A Novel Application for Learning  Continuous-Time Markov Chain Mixtures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spaeh%2C+F">Fabian Spaeh</a>, 
<a href="/search/cs?searchtype=author&query=Tsourakakis%2C+C+E">Charalampos E. Tsourakakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sequential data naturally arises from user engagement on digital platforms
like social media, music streaming services, and web navigation, encapsulating
evolving user preferences and behaviors through continuous information streams.
A notable unresolved query in stochastic processes is learning mixtures of
continuous-time Markov chains (CTMCs). While there is progress in learning
mixtures of discrete-time Markov chains with recovery guarantees
[GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored
challenges. The intrigue in CTMC mixtures stems from their potential to model
intricate continuous-time stochastic processes prevalent in various fields
including social media, finance, and biology.
<br />In this study, we introduce a novel framework for exploring CTMCs,
emphasizing the influence of observed trails' length and mixture parameters on
problem regimes, which demands specific algorithms. Through thorough
experimentation, we examine the impact of discretizing continuous-time trails
on the learnability of the continuous-time mixture, given that these processes
are often observed via discrete, resource-demanding observations. Our
comparative analysis with leading methods explores sample complexity and the
trade-off between the number of trails and their lengths, offering crucial
insights for method selection in different problem instances. We apply our
algorithms on an extensive collection of Lastfm's user-generated trails
spanning three years, demonstrating the capability of our algorithms to
differentiate diverse user preferences. We pioneer the use of CTMC mixtures on
a basketball passing dataset to unveil intricate offensive tactics of NBA
teams. This underscores the pragmatic utility and versatility of our proposed
framework. All results presented in this study are replicable, and we provide
the implementations to facilitate reproducibility.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17733" title="Abstract">arXiv:2402.17733</a> [<a href="/pdf/2402.17733" title="Download PDF">pdf</a>, <a href="/format/2402.17733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tower: An Open Multilingual Large Language Model for Translation-Related  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alves%2C+D+M">Duarte M. Alves</a>, 
<a href="/search/cs?searchtype=author&query=Pombal%2C+J">Jos&#xe9; Pombal</a>, 
<a href="/search/cs?searchtype=author&query=Guerreiro%2C+N+M">Nuno M. Guerreiro</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+P+H">Pedro H. Martins</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+J">Jo&#xe3;o Alves</a>, 
<a href="/search/cs?searchtype=author&query=Farajian%2C+A">Amin Farajian</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+B">Ben Peters</a>, 
<a href="/search/cs?searchtype=author&query=Rei%2C+R">Ricardo Rei</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+P">Patrick Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Sweta Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>, 
<a href="/search/cs?searchtype=author&query=de+Souza%2C+J+G+C">Jos&#xe9; G.C. de Souza</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+A+F+T">Andr&#xe9; F.T. Martins</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While general-purpose large language models (LLMs) demonstrate proficiency on
multiple tasks within the domain of translation, approaches based on open LLMs
are competitive only when specializing on a single task. In this paper, we
propose a recipe for tailoring LLMs to multiple tasks present in translation
workflows. We perform continued pretraining on a multilingual mixture of
monolingual and parallel data, creating TowerBase, followed by finetuning on
instructions relevant for translation processes, creating TowerInstruct. Our
final model surpasses open alternatives on several tasks relevant to
translation workflows and is competitive with general-purpose closed LLMs. To
facilitate future research, we release the Tower models, our specialization
dataset, an evaluation framework for LLMs focusing on the translation
ecosystem, and a collection of model generations, including ours, on our
benchmark.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17736" title="Abstract">arXiv:2402.17736</a> [<a href="/pdf/2402.17736" title="Download PDF">pdf</a>, <a href="/format/2402.17736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-Based Algorithms for Graph Searching Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DePavia%2C+A+F">Adela Frances DePavia</a>, 
<a href="/search/cs?searchtype=author&query=Tani%2C+E">Erasmo Tani</a>, 
<a href="/search/cs?searchtype=author&query=Vakilian%2C+A">Ali Vakilian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of graph searching with prediction recently
introduced by Banerjee et al. (2022). In this problem, an agent, starting at
some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a
hidden goal node $g$ while minimizing the total distance travelled. We study a
setting in which at any node $v$, the agent receives a noisy estimate of the
distance from $v$ to $g$. We design algorithms for this search task on unknown
graphs. We establish the first formal guarantees on unknown weighted graphs and
provide lower bounds showing that the algorithms we propose have optimal or
nearly-optimal dependence on the prediction error. Further, we perform
numerical experiments demonstrating that in addition to being robust to
adversarial error, our algorithms perform well in typical instances in which
the error is stochastic. Finally, we provide alternative simpler performance
bounds on the algorithms of Banerjee et al. (2022) for the case of searching on
a known graph, and establish new lower bounds for this setting.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17739" title="Abstract">arXiv:2402.17739</a> [<a href="/pdf/2402.17739" title="Download PDF">pdf</a>, <a href="/format/2402.17739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> reBandit: Random Effects based Online RL algorithm for Reducing Cannabis  Use
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Susobhan Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yongyi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+P">Pei-Yao Hung</a>, 
<a href="/search/cs?searchtype=author&query=Coughlin%2C+L">Lara Coughlin</a>, 
<a href="/search/cs?searchtype=author&query=Bonar%2C+E">Erin Bonar</a>, 
<a href="/search/cs?searchtype=author&query=Nahum-Shani%2C+I">Inbal Nahum-Shani</a>, 
<a href="/search/cs?searchtype=author&query=Walton%2C+M">Maureen Walton</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+S">Susan Murphy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The escalating prevalence of cannabis use, and associated cannabis-use
disorder (CUD), poses a significant public health challenge globally. With a
notably wide treatment gap, especially among emerging adults (EAs; ages 18-25),
addressing cannabis use and CUD remains a pivotal objective within the 2030
United Nations Agenda for Sustainable Development Goals (SDG). In this work, we
develop an online reinforcement learning (RL) algorithm called reBandit which
will be utilized in a mobile health study to deliver personalized mobile health
interventions aimed at reducing cannabis use among EAs. reBandit utilizes
random effects and informative Bayesian priors to learn quickly and efficiently
in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes
and optimization techniques to autonomously update its hyper-parameters online.
To evaluate the performance of our algorithm, we construct a simulation testbed
using data from a prior study, and compare against commonly used algorithms in
mobile health studies. We show that reBandit performs equally well or better
than all the baseline algorithms, and the performance gap widens as population
heterogeneity increases in the simulation environment, proving its adeptness to
adapt to diverse population of study participants.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17743" title="Abstract">arXiv:2402.17743</a> [<a href="/pdf/2402.17743" title="Download PDF">pdf</a>, <a href="/format/2402.17743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rose: Efficient and Extensible Autodiff on the Web
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Estep%2C+S">Sam Estep</a> (Carnegie Mellon University), 
<a href="/search/cs?searchtype=author&query=Rothkopf%2C+R">Raven Rothkopf</a> (Barnard College, Columbia University), 
<a href="/search/cs?searchtype=author&query=Ni%2C+W">Wode Ni</a> (Carnegie Mellon University), 
<a href="/search/cs?searchtype=author&query=Sunshine%2C+J">Joshua Sunshine</a> (Carnegie Mellon University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Automatic differentiation (AD) has become the backbone for a new wave of
optimization-driven domains such as computer graphics and machine learning over
the past decade. However, existing AD systems face limitations, either lacking
support for in-browser development or failing to harness more recent,
compilerbased approaches to achieve both expressiveness and size-preserving
differentiation. This work introduces Rose, a portable, extensible AD library
that runs on the web. Rose allows users to write opaque functions with custom
derivatives and supports dynamic construction of AD functions. We integrated
Rose into two differentiable simulations and a diagram authoring tool to
demonstrate the utility of Rose's design. Finally, we show that Rose is 173x as
fast as TensorFlow.js in compiling and running a benchmark suite of optimized
diagrams.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17744" title="Abstract">arXiv:2402.17744</a> [<a href="/pdf/2402.17744" title="Download PDF">pdf</a>, <a href="/format/2402.17744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using  Contrastive Learning and Geometric Unfolding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oberstrass%2C+A">Alexander Oberstrass</a>, 
<a href="/search/cs?searchtype=author&query=DeKraker%2C+J">Jordan DeKraker</a>, 
<a href="/search/cs?searchtype=author&query=Palomero-Gallagher%2C+N">Nicola Palomero-Gallagher</a>, 
<a href="/search/cs?searchtype=author&query=Muenzing%2C+S+E+A">Sascha E. A. Muenzing</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+A+C">Alan C. Evans</a>, 
<a href="/search/cs?searchtype=author&query=Axer%2C+M">Markus Axer</a>, 
<a href="/search/cs?searchtype=author&query=Amunts%2C+K">Katrin Amunts</a>, 
<a href="/search/cs?searchtype=author&query=Dickscheid%2C+T">Timo Dickscheid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Understanding the cortical organization of the human brain requires
interpretable descriptors for distinct structural and functional imaging data.
3D polarized light imaging (3D-PLI) is an imaging modality for visualizing
fiber architecture in postmortem brains with high resolution that also captures
the presence of cell bodies, for example, to identify hippocampal subfields.
The rich texture in 3D-PLI images, however, makes this modality particularly
difficult to analyze and best practices for characterizing architectonic
patterns still need to be established. In this work, we demonstrate a novel
method to analyze the regional organization of the human hippocampus in 3D-PLI
by combining recent advances in unfolding methods with deep texture features
obtained using a self-supervised contrastive learning approach. We identify
clusters in the representations that correspond well with classical
descriptions of hippocampal subfields, lending validity to the developed
methodology.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17747" title="Abstract">arXiv:2402.17747</a> [<a href="/pdf/2402.17747" title="Download PDF">pdf</a>, <a href="/format/2402.17747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Your AI Deceives You: Challenges with Partial Observability of  Human Evaluators in Reward Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lang%2C+L">Leon Lang</a>, 
<a href="/search/cs?searchtype=author&query=Foote%2C+D">Davis Foote</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+S">Stuart Russell</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A">Anca Dragan</a>, 
<a href="/search/cs?searchtype=author&query=Jenner%2C+E">Erik Jenner</a>, 
<a href="/search/cs?searchtype=author&query=Emmons%2C+S">Scott Emmons</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Past analyses of reinforcement learning from human feedback (RLHF) assume
that the human fully observes the environment. What happens when human feedback
is based only on partial observations? We formally define two failure cases:
deception and overjustification. Modeling the human as Boltzmann-rational
w.r.t. a belief over trajectories, we prove conditions under which RLHF is
guaranteed to result in policies that deceptively inflate their performance,
overjustify their behavior to make an impression, or both. To help address
these issues, we mathematically characterize how partial observability of the
environment translates into (lack of) ambiguity in the learned return function.
In some cases, accounting for partial observability makes it theoretically
possible to recover the return function and thus the optimal policy, while in
other cases, there is irreducible ambiguity. We caution against blindly
applying RLHF in partially observable settings and propose research directions
to help tackle these challenges.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17748" title="Abstract">arXiv:2402.17748</a> [<a href="/pdf/2402.17748" title="Download PDF">pdf</a>, <a href="/format/2402.17748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Market Dynamics of Liquid Staking Derivatives (LSDs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+X">Xihan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhipeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Staking has emerged as a crucial concept following Ethereum's transition to
Proof-of-Stake consensus. The introduction of Liquid Staking Derivatives (LSDs)
has effectively addressed the illiquidity issue associated with solo staking,
gaining significant market attention. This paper analyzes the LSD market
dynamics from the perspectives of both liquidity takers (LTs) and liquidity
providers (LPs). We first quantify the price discrepancy between the LSD
primary and secondary markets. Then we investigate and empirically measure how
LTs can leverage such discrepancy to exploit arbitrage opportunities, unveiling
the potential barriers to LSD arbitrages. In addition, we evaluate the
financial profit and losses experienced by LPs who supply LSDs for liquidity
provision. Our findings reveal that 66% LSD liquidity provision positions yield
an Annual Percentage Rate (APR) lower than simply holding the corresponding
LSDs.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17751" title="Abstract">arXiv:2402.17751</a> [<a href="/pdf/2402.17751" title="Download PDF">pdf</a>, <a href="/format/2402.17751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Eye Gaze Heatmap Analysis of Uncertainty Head-Up Display Designs for  Conditional Automated Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gerber%2C+M+A">Michael A. Gerber</a>, 
<a href="/search/cs?searchtype=author&query=Schroeter%2C+R">Ronald Schroeter</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+D">Daniel Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Janssen%2C+C+P">Christian P. Janssen</a>, 
<a href="/search/cs?searchtype=author&query=Rakotonirainy%2C+A">Andry Rakotonirainy</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+J">Jonny Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Lenne%2C+M+G">Mike G. Lenne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 2024 ACM Conference on Human Factors in Computing Systems (CHI'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper reports results from a high-fidelity driving simulator study
(N=215) about a head-up display (HUD) that conveys a conditional automated
vehicle's dynamic "uncertainty" about the current situation while fallback
drivers watch entertaining videos. We compared (between-group) three design
interventions: display (a bar visualisation of uncertainty close to the video),
interruption (interrupting the video during uncertain situations), and
combination (a combination of both), against a baseline (video-only). We
visualised eye-tracking data to conduct a heatmap analysis of the four groups'
gaze behaviour over time. We found interruptions initiated a phase during which
participants interleaved their attention between monitoring and entertainment.
This improved monitoring behaviour was more pronounced in combination compared
to interruption, suggesting pre-warning interruptions have positive effects.
The same addition had negative effects without interruptions (comparing
baseline &amp; display). Intermittent interruptions may have safety benefits over
placing additional peripheral displays without compromising usability.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17753" title="Abstract">arXiv:2402.17753</a> [<a href="/pdf/2402.17753" title="Download PDF">pdf</a>, <a href="/format/2402.17753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Very Long-Term Conversational Memory of LLM Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maharana%2C+A">Adyasha Maharana</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dong-Ho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Sergey Tulyakov</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Barbieri%2C+F">Francesco Barbieri</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuwei Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages; Project page: <a href="https://snap-research.github.io/locomo/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing works on long-term open-domain dialogues focus on evaluating model
responses within contexts spanning no more than five chat sessions. Despite
advancements in long-context large language models (LLMs) and retrieval
augmented generation (RAG) techniques, their efficacy in very long-term
dialogues remains unexplored. To address this research gap, we introduce a
machine-human pipeline to generate high-quality, very long-term dialogues by
leveraging LLM-based agent architectures and grounding their dialogues on
personas and temporal event graphs. Moreover, we equip each agent with the
capability of sharing and reacting to images. The generated conversations are
verified and edited by human annotators for long-range consistency and
grounding to the event graphs. Using this pipeline, we collect LoCoMo, a
dataset of very long-term conversations, each encompassing 300 turns and 9K
tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a
comprehensive evaluation benchmark to measure long-term memory in models,
encompassing question answering, event summarization, and multi-modal dialogue
generation tasks. Our experimental results indicate that LLMs exhibit
challenges in understanding lengthy conversations and comprehending long-range
temporal and causal dynamics within dialogues. Employing strategies like
long-context LLMs or RAG can offer improvements but these models still
substantially lag behind human performance.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17754" title="Abstract">arXiv:2402.17754</a> [<a href="/pdf/2402.17754" title="Download PDF">pdf</a>, <a href="/ps/2402.17754" title="Download PostScript">ps</a>, <a href="/format/2402.17754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increasing the Diversity of Investment Portfolio with Integration of  Gamified Components in the FinTech Applications Lifecycle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=PourMohammadBagher%2C+L">Latifeh PourMohammadBagher</a>, 
<a href="/search/cs?searchtype=author&query=Safarabadi%2C+N+S">Najmieh Sadat Safarabadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Gamification has the potential to make significant contributions to financial
product delivery, Fintech services, and inclusive growth. The integration of
gamification into FinTech applications has shown a positive correlation with
the social impact theory. Utilizing gamification in a sustainable and effective
manner can be crucial for long-term prospects in the FinTech industry.
Therefore, it is essential to develop efficient and modern financial software
that improves the customer experience. The current literature aims to
contribute to this area by highlighting the relationship between interrelated
theories and the key factors to consider when designing a gamified element.
This study aims to explore the effects of gamification on altering user
intention and its significant influence on customer value propositions.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17756" title="Abstract">arXiv:2402.17756</a> [<a href="/pdf/2402.17756" title="Download PDF">pdf</a>, <a href="/format/2402.17756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustly Learning Single-Index Models via Alignment Sharpness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zarifis%2C+N">Nikos Zarifis</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Puqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Diakonikolas%2C+I">Ilias Diakonikolas</a>, 
<a href="/search/cs?searchtype=author&query=Diakonikolas%2C+J">Jelena Diakonikolas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Optimization and Control (math.OC); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of learning Single-Index Models under the $L_2^2$ loss
in the agnostic model. We give an efficient learning algorithm, achieving a
constant factor approximation to the optimal loss, that succeeds under a range
of distributions (including log-concave distributions) and a broad class of
monotone and Lipschitz link functions. This is the first efficient constant
factor approximate agnostic learner, even for Gaussian data and for any
nontrivial class of link functions. Prior work for the case of unknown link
function either works in the realizable setting or does not attain constant
factor approximation. The main technical ingredient enabling our algorithm and
analysis is a novel notion of a local error bound in optimization that we term
alignment sharpness and that may be of broader interest.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17758" title="Abstract">arXiv:2402.17758</a> [<a href="/pdf/2402.17758" title="Download PDF">pdf</a>, <a href="/ps/2402.17758" title="Download PostScript">ps</a>, <a href="/format/2402.17758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily  Living
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zakour%2C+M">Marsil Zakour</a>, 
<a href="/search/cs?searchtype=author&query=Nath%2C+P+P">Partha Pratim Nath</a>, 
<a href="/search/cs?searchtype=author&query=Lohmer%2C+L">Ludwig Lohmer</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B6k%C3%A7e%2C+E+F">Emre Faik G&#xf6;k&#xe7;e</a>, 
<a href="/search/cs?searchtype=author&query=Piccolrovazzi%2C+M">Martin Piccolrovazzi</a>, 
<a href="/search/cs?searchtype=author&query=Patsch%2C+C">Constantin Patsch</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuankai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+R">Rahul Chaudhari</a>, 
<a href="/search/cs?searchtype=author&query=Steinbach%2C+E">Eckehard Steinbach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Hand-Object Interactions (HOIs) are conditioned on spatial and temporal
contexts like surrounding objects, pre- vious actions, and future intents (for
example, grasping and handover actions vary greatly based on objects proximity
and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over
time) are limited to one subject inter- acting with one object only. This
restricts the generalization of learning-based HOI methods trained on those
datasets. We introduce ADL4D, a dataset of up to two subjects inter- acting
with different sets of objects performing Activities of Daily Living (ADL) like
breakfast or lunch preparation ac- tivities. The transition between multiple
objects to complete a certain task over time introduces a unique context
lacking in existing datasets. Our dataset consists of 75 sequences with a total
of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action
annotations. We develop an automatic system for multi-view multi-hand 3D pose
an- notation capable of tracking hand poses over time. We inte- grate and test
it against publicly available datasets. Finally, we evaluate our dataset on the
tasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17759" title="Abstract">arXiv:2402.17759</a> [<a href="/pdf/2402.17759" title="Download PDF">pdf</a>, <a href="/format/2402.17759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Optimal Learning of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuxian Gu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Li Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yaru Hao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qingxiu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This work studies the general principles of improving the learning of
language models (LMs), which aims at reducing the necessary training steps for
achieving superior performance. Specifically, we present a theory for the
optimal learning of LMs. We first propose an objective that optimizes LM
learning by maximizing the data compression ratio in an
"LM-training-as-lossless-compression" view. Then, we derive a theorem, named
Learning Law, to reveal the properties of the dynamics in the optimal learning
process under our objective. The theorem is then validated by experiments on a
linear classification and a real-world language modeling task. Finally, we
empirically verify that the optimal learning of LMs essentially stems from the
improvement of the coefficients in the scaling law of LMs, indicating great
promise and significance for designing practical learning acceleration methods.
Our code can be found at https://aka.ms/LearningLaw.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17762" title="Abstract">arXiv:2402.17762</a> [<a href="/pdf/2402.17762" title="Download PDF">pdf</a>, <a href="/format/2402.17762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Massive Activations in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingjie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinlei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J. Zico Kolter</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website at <a href="https://eric-mingjie.github.io/massive-activations/index.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We observe an empirical phenomenon in Large Language Models (LLMs) -- very
few activations exhibit significantly larger values than others (e.g., 100,000
times larger). We call them massive activations. First, we demonstrate the
widespread existence of massive activations across various LLMs and
characterize their locations. Second, we find their values largely stay
constant regardless of the input, and they function as indispensable bias terms
in LLMs. Third, these massive activations lead to the concentration of
attention probabilities to their corresponding tokens, and further, implicit
bias terms in the self-attention output. Last, we also study massive
activations in Vision Transformers.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17763" title="Abstract">arXiv:2402.17763</a> [<a href="/pdf/2402.17763" title="Download PDF">pdf</a>, <a href="/ps/2402.17763" title="Download PostScript">ps</a>, <a href="/format/2402.17763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Unnecessary Alerts in Pedestrian Protection Systems Based on  P2V Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soto%2C+I">Ignacio Soto</a>, 
<a href="/search/cs?searchtype=author&query=Jimenez%2C+F">Felipe Jimenez</a>, 
<a href="/search/cs?searchtype=author&query=Calderon%2C+M">Maria Calderon</a>, 
<a href="/search/cs?searchtype=author&query=Naranjo%2C+J+E">Jose E. Naranjo</a>, 
<a href="/search/cs?searchtype=author&query=Anaya%2C+J+J">Jose J. Anaya</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Electronics 8(3):360, 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">There are different proposals in the literature on how to protect pedestrians
using warning systems to alert drivers of their presence. They can be based on
onboard perception systems or wireless communications. The evaluation of these
systems has been focused on testing their ability to detect pedestrians. A
problem that has received much less attention is the possibility of generating
too many alerts in the warning systems. In this paper, we propose and analyze
four different algorithms to take the decision on generating alerts in a
warning system that is based on direct wireless communications between vehicles
and pedestrians. With the algorithms, we explore different strategies to reduce
unnecessary alerts. The feasibility of the implementation of the algorithms was
evaluated with a deployment using real equipment, and tests were carried out to
verify their behavior in real scenarios. The ability of each algorithm to
reduce unnecessary alerts was evaluated with realistic simulations in an urban
scenario, using a traffic simulator with vehicular and pedestrian flows. The
results show the importance of tackling the problem of driver overload in
warning systems, and that it is not straightforward to predict the load of
alerts generated by an algorithm in a large-scale deployment, in which there
are multiple interactions between vehicles and pedestrians.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17764" title="Abstract">arXiv:2402.17764</a> [<a href="/pdf/2402.17764" title="Download PDF">pdf</a>, <a href="/format/2402.17764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shuming Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lingxiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaohan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Li Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruiping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jilong Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent research, such as BitNet, is paving the way for a new era of 1-bit
Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,
namely BitNet b1.58, in which every single parameter (or weight) of the LLM is
ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)
Transformer LLM with the same model size and training tokens in terms of both
perplexity and end-task performance, while being significantly more
cost-effective in terms of latency, memory, throughput, and energy consumption.
More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for
training new generations of LLMs that are both high-performance and
cost-effective. Furthermore, it enables a new computation paradigm and opens
the door for designing specific hardware optimized for 1-bit LLMs.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17766" title="Abstract">arXiv:2402.17766</a> [<a href="/pdf/2402.17766" title="Download PDF">pdf</a>, <a href="/format/2402.17766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShapeLLM: Universal 3D Object Understanding for Embodied Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zekun Qi</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+R">Runpei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaochen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+H">Haoran Geng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chunrui Han</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Z">Zheng Ge</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Li Yi</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kaisheng Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents ShapeLLM, the first 3D Multimodal Large Language Model
(LLM) designed for embodied interaction, exploring a universal 3D object
understanding with 3D point clouds and languages. ShapeLLM is built upon an
improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view
image distillation for enhanced geometry understanding. By utilizing ReCon++ as
the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed
instruction-following data and tested on our newly human-curated evaluation
benchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance
in 3D geometry understanding and language-unified 3D interaction tasks, such as
embodied visual grounding.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17767" title="Abstract">arXiv:2402.17767</a> [<a href="/pdf/2402.17767" title="Download PDF">pdf</a>, <a href="/format/2402.17767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opening Cabinets and Drawers in the Real World using a Commodity Mobile  Manipulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Arjun Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Michelle Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sathua%2C+R">Rishik Sathua</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Saurabh Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project webpage: <a href="https://arjung128.github.io/opening-cabinets-and-drawers">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Pulling open cabinets and drawers presents many difficult technical
challenges in perception (inferring articulation parameters for objects from
onboard sensors), planning (producing motion plans that conform to tight task
constraints), and control (making and maintaining contact while applying forces
on the environment). In this work, we build an end-to-end system that enables a
commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in
diverse previously unseen real world environments. We conduct 4 days of real
world testing of this system spanning 31 different objects from across 13
different real world environments. Our system achieves a success rate of 61% on
opening novel cabinets and drawers in unseen environments zero-shot. An
analysis of the failure modes suggests that errors in perception are the most
significant challenge for our system. We will open source code and models for
others to replicate and build upon our system.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17768" title="Abstract">arXiv:2402.17768</a> [<a href="/pdf/2402.17768" title="Download PDF">pdf</a>, <a href="/format/2402.17768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Matthew Chang</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Pranav Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Saurabh Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> for project website with video, see <a href="https://sites.google.com/view/diffusion-meets-dagger">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">A common failure mode for policies trained with imitation is compounding
execution errors at test time. When the learned policy encounters states that
were not present in the expert demonstrations, the policy fails, leading to
degenerate behavior. The Dataset Aggregation, or DAgger approach to this
problem simply collects more data to cover these failure states. However, in
practice, this is often prohibitively expensive. In this work, we propose
Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without
the cost for eye-in-hand imitation learning problems. Instead of collecting new
samples to cover out-of-distribution states, DMD uses recent advances in
diffusion models to create these samples with diffusion models. This leads to
robust performance from few demonstrations. In experiments conducted for
non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a
success rate of 80% with as few as 8 expert demonstrations, where naive
behavior cloning reaches only 20%. DMD also outperform competing NeRF-based
augmentation schemes by 50%.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17769" title="Abstract">arXiv:2402.17769</a> [<a href="/pdf/2402.17769" title="Download PDF">pdf</a>, <a href="/format/2402.17769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factors that Affect Personalization of Robots for Older Adults
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stegner%2C+L">Laura Stegner</a>, 
<a href="/search/cs?searchtype=author&query=Senft%2C+E">Emmanuel Senft</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+B">Bilge Mutlu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at CONCATENATE Workshop at HRI 2023 in Stockholm, Sweden
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We introduce a taxonomy of important factors to consider when designing
interactions with an assistive robot in a senior living facility. These factors
are derived from our reflection on two field studies and are grouped into the
following high-level categories: primary user (residents), care partners,
robot, facility and external circumstances. We outline how multiple factors in
these categories impact different aspects of personalization, such as adjusting
interactions based on the unique needs of a resident or modifying alerts about
the robot's status for different care partners. This preliminary taxonomy
serves as a framework for considering how to deploy personalized assistive
robots in the complex caregiving ecosystem.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Wed, 28 Feb 24</h3>
<dl>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00997" title="Abstract">arXiv:2312.00997</a> (cross-list from quant-ph) [<a href="/pdf/2312.00997" title="Download PDF">pdf</a>, <a href="/format/2312.00997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Whole-Chip QAOA for Higher-Order Ising Spin Glass Models on  Heavy-Hex Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pelofske%2C+E">Elijah Pelofske</a>, 
<a href="/search/quant-ph?searchtype=author&query=B%C3%A4rtschi%2C+A">Andreas B&#xe4;rtschi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cincio%2C+L">Lukasz Cincio</a>, 
<a href="/search/quant-ph?searchtype=author&query=Golden%2C+J">John Golden</a>, 
<a href="/search/quant-ph?searchtype=author&query=Eidenbenz%2C+S">Stephan Eidenbenz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Data Structures and Algorithms (cs.DS); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">We show through numerical simulation that the Quantum Alternating Operator
Ansatz (QAOA) for higher-order, random-coefficient, heavy-hex compatible spin
glass Ising models has strong parameter concentration across problem sizes from
$16$ up to $127$ qubits for $p=1$ up to $p=5$, which allows for
straight-forward transfer learning of QAOA angles on instance sizes where
exhaustive grid-search is prohibitive even for $p&gt;1$. We use Matrix Product
State (MPS) simulation at different bond dimensions to obtain confidence in
these results, and we obtain the optimal solutions to these combinatorial
optimization problems using CPLEX. In order to assess the ability of current
noisy quantum hardware to exploit such parameter concentration, we execute
short-depth QAOA circuits (with a CNOT depth of 6 per $p$, resulting in
circuits which contain $1420$ two qubit gates for $127$ qubit $p=5$ QAOA) on
$100$ higher-order (cubic term) Ising models on IBM quantum superconducting
processors with $16, 27, 127$ qubits using QAOA angles learned from a single
$16$-qubit instance. We show that (i) the best quantum processors generally
find lower energy solutions up to $p=3$ for 27 qubit systems and up to $p=2$
for 127 qubit systems and are overcome by noise at higher values of $p$, (ii)
the best quantum processors find mean energies that are about a factor of two
off from the noise-free numerical simulation results. Additional insights from
our experiments are that large performance differences exist among different
quantum processors even of the same generation and that dynamical decoupling
significantly improve performance for some, but decrease performance for other
quantum processors. Lastly we show $p=1$ QAOA angle mean energy landscapes
computed using up to a $414$ qubit quantum computer, showing that the mean QAOA
energy landscapes remain very similar as the problem size changes.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16854" title="Abstract">arXiv:2402.16854</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.16854" title="Download PDF">pdf</a>, <a href="/ps/2402.16854" title="Download PostScript">ps</a>, <a href="/format/2402.16854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention Based Molecule Generation via Hierarchical Variational  Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Sivanesan%2C+D">Divahar Sivanesan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Molecule generation is a task made very difficult by the complex ways in
which we represent molecules computationally. A common technique used in
molecular generative modeling is to use SMILES strings with recurrent neural
networks built into variational autoencoders - but these suffer from a myriad
of issues: vanishing gradients, long-range forgetting, and invalid molecules.
In this work, we show that by combining recurrent neural networks with
convolutional networks in a hierarchical manner, we are able to both extract
autoregressive information from SMILES strings while maintaining signal and
long-range dependencies. This allows for generations with very high validity
rates on the order of 95% when reconstructing known molecules. We also observe
an average Tanimoto similarity of .6 between test set and reconstructed
molecules, which suggests our method is able to map between SMILES strings and
their learned representations in a more effective way than prior works using
similar methods.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16865" title="Abstract">arXiv:2402.16865</a> (cross-list from eess.IV) [<a href="/pdf/2402.16865" title="Download PDF">pdf</a>, <a href="/format/2402.16865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improve Robustness of Eye Disease Detection by including Learnable  Probabilistic Discrete Latent Variables into Machine Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Prabhakaran%2C+A">Anirudh Prabhakaran</a>, 
<a href="/search/eess?searchtype=author&query=Xiao%2C+Y">YeKun Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Cheng%2C+C">Ching-Yu Cheng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+D">Dianbo Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Ocular diseases, ranging from diabetic retinopathy to glaucoma, present a
significant public health challenge due to their prevalence and potential for
causing vision impairment. Early and accurate diagnosis is crucial for
effective treatment and management.In recent years, deep learning models have
emerged as powerful tools for analysing medical images, including ocular
imaging . However, challenges persist in model interpretability and uncertainty
estimation, which are critical for clinical decision-making. This study
introduces a novel application of GFlowOut, leveraging the probabilistic
framework of Generative Flow Networks (GFlowNets) to learn the posterior
distribution over dropout masks, for the classification and analysis of ocular
diseases using eye fundus images. We develop a robust and generalizable method
that utilizes GFlowOut integrated with ResNet18 and ViT models as backbone in
identifying various ocular conditions. This study employs a unique set of
dropout masks - none, random, bottomup, and topdown - to enhance model
performance in analyzing ocular images. Our results demonstrate that the
bottomup GFlowOut mask significantly improves accuracy, outperforming the
traditional dropout approach.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16882" title="Abstract">arXiv:2402.16882</a> (cross-list from physics.chem-ph) [<a href="/pdf/2402.16882" title="Download PDF">pdf</a>, <a href="/format/2402.16882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Substrate Scope Contrastive Learning: Repurposing Human Bias to Learn  Atomic Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gao%2C+W">Wenhao Gao</a>, 
<a href="/search/physics?searchtype=author&query=Raghavan%2C+P">Priyanka Raghavan</a>, 
<a href="/search/physics?searchtype=author&query=Shprints%2C+R">Ron Shprints</a>, 
<a href="/search/physics?searchtype=author&query=Coley%2C+C+W">Connor W. Coley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Learning molecular representation is a critical step in molecular machine
learning that significantly influences modeling success, particularly in
data-scarce situations. The concept of broadly pre-training neural networks has
advanced fields such as computer vision, natural language processing, and
protein engineering. However, similar approaches for small organic molecules
have not achieved comparable success. In this work, we introduce a novel
pre-training strategy, substrate scope contrastive learning, which learns
atomic representations tailored to chemical reactivity. This method considers
the grouping of substrates and their yields in published substrate scope tables
as a measure of their similarity or dissimilarity in terms of chemical
reactivity. We focus on 20,798 aryl halides in the CAS Content Collection
spanning thousands of publications to learn a representation of aryl halide
reactivity. We validate our pre-training approach through both intuitive
visualizations and comparisons to traditional reactivity descriptors and
physical organic chemistry principles. The versatility of these embeddings is
further evidenced in their application to yield prediction, regioselectivity
prediction, and the diverse selection of new substrates. This work not only
presents a chemistry-tailored neural network pre-training strategy to learn
reactivity-aligned atomic representations, but also marks a first-of-its-kind
approach to benefit from the human bias in substrate scope design.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16901" title="Abstract">arXiv:2402.16901</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.16901" title="Download PDF">pdf</a>, <a href="/format/2402.16901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Duan%2C+C">ChenRui Duan</a>, 
<a href="/search/q-bio?searchtype=author&query=Zang%2C+Z">Zelin Zang</a>, 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+Y">Yongjie Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=He%2C+H">Hang He</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Song%2C+Z">Zijia Song</a>, 
<a href="/search/q-bio?searchtype=author&query=Zheng%2C+J">Ju-Sheng Zheng</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Metagenomic data, comprising mixed multi-species genomes, are prevalent in
diverse environments like oceans and soils, significantly impacting human
health and ecological functions. However, current research relies on K-mer
representations, limiting the capture of structurally relevant gene contexts.
To address these limitations and further our understanding of complex
relationships between metagenomic sequences and their functions, we introduce a
protein-based gene representation as a context-aware and structure-relevant
tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene
group-level pre-training, providing insights into inter-gene contextual
information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for
gene-level pre-training to model gene sequence-function relationships. MGM and
TEM-CL constitute our novel metagenomic language model {\NAME}, pre-trained on
100 million metagenomic sequences. We demonstrate the superiority of our
proposed {\NAME} on eight datasets.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16907" title="Abstract">arXiv:2402.16907</a> (cross-list from eess.IV) [<a href="/pdf/2402.16907" title="Download PDF">pdf</a>, <a href="/format/2402.16907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Posterior Proximal Sampling for Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+H">Hongjie Wu</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+L">Linchao He</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+M">Mingqin Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+D">Dongdong Chen</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+K">Kunming Luo</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+M">Mengting Luo</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Ji-Zhe Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+H">Hu Chen</a>, 
<a href="/search/eess?searchtype=author&query=Lv%2C+J">Jiancheng Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion models have demonstrated remarkable efficacy in generating
high-quality samples. Existing diffusion-based image restoration algorithms
exploit pre-trained diffusion models to leverage data priors, yet they still
preserve elements inherited from the unconditional generation paradigm. These
strategies initiate the denoising process with pure white noise and incorporate
random noise at each generative step, leading to over-smoothed results. In this
paper, we introduce a refined paradigm for diffusion-based image restoration.
Specifically, we opt for a sample consistent with the measurement identity at
each generative step, exploiting the sampling selection as an avenue for output
stability and enhancement. Besides, we start the restoration process with an
initialization combined with the measurement signal, providing supplementary
information to better align the generative process. Extensive experimental
results and analyses validate the effectiveness of our proposed approach across
diverse image restoration tasks.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16930" title="Abstract">arXiv:2402.16930</a> (cross-list from physics.chem-ph) [<a href="/pdf/2402.16930" title="Download PDF">pdf</a>, <a href="/format/2402.16930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrustMol: Trustworthy Inverse Molecular Design via Alignment with  Molecular Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wijaya%2C+K+T">Kevin Tirta Wijaya</a>, 
<a href="/search/physics?searchtype=author&query=Ansari%2C+N">Navid Ansari</a>, 
<a href="/search/physics?searchtype=author&query=Seidel%2C+H">Hans-Peter Seidel</a>, 
<a href="/search/physics?searchtype=author&query=Babaei%2C+V">Vahid Babaei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Data-driven generation of molecules with desired properties, also known as
inverse molecular design (IMD), has attracted significant attention in recent
years. Despite the significant progress in the accuracy and diversity of
solutions, existing IMD methods lag behind in terms of trustworthiness. The
root issue is that the design process of these methods is increasingly more
implicit and indirect, and this process is also isolated from the native
forward process (NFP), the ground-truth function that models the molecular
dynamics. Following this insight, we propose TrustMol, an IMD method built to
be trustworthy. For this purpose, TrustMol relies on a set of technical
novelties including a new variational autoencoder network. Moreover, we propose
a latent-property pairs acquisition method to effectively navigate the
complexities of molecular latent optimization, a process that seems intuitive
yet challenging due to the high-frequency and discontinuous nature of molecule
space. TrustMol also integrates uncertainty-awareness into molecular latent
optimization. These lead to improvements in both explainability and reliability
of the IMD process. We validate the trustworthiness of TrustMol through a wide
range of experiments.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16978" title="Abstract">arXiv:2402.16978</a> (cross-list from math.OC) [<a href="/pdf/2402.16978" title="Download PDF">pdf</a>, <a href="/format/2402.16978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An inexact Bregman proximal point method and its acceleration version  for unbalanced optimal transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+F">Faqiang Wang</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/math?searchtype=author&query=Cui%2C+L">Li Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Unbalanced Optimal Transport (UOT) problem plays increasingly important
roles in computational biology, computational imaging and deep learning.
Scaling algorithm is widely used to solve UOT due to its convenience and good
convergence properties. However, this algorithm has lower accuracy for large
regularization parameters, and due to stability issues, small regularization
parameters can easily lead to numerical overflow. We address this challenge by
developing an inexact Bregman proximal point method for solving UOT. This
algorithm approximates the proximal operator using the Scaling algorithm at
each iteration. The algorithm (1) converges to the true solution of UOT, (2)
has theoretical guarantees and robust regularization parameter selection, (3)
mitigates numerical stability issues, and (4) can achieve comparable
computational complexity to the Scaling algorithm in specific practice.
Building upon this, we develop an accelerated version of inexact Bregman
proximal point method for solving UOT by using acceleration techniques of
Bregman proximal point method and provide theoretical guarantees and
experimental validation of convergence and acceleration.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16991" title="Abstract">arXiv:2402.16991</a> (cross-list from stat.ML) [<a href="/pdf/2402.16991" title="Download PDF">pdf</a>, <a href="/format/2402.16991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Phase Transition in Diffusion Models Reveals the Hierarchical Nature  of Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sclocchi%2C+A">Antonio Sclocchi</a>, 
<a href="/search/stat?searchtype=author&query=Favero%2C+A">Alessandro Favero</a>, 
<a href="/search/stat?searchtype=author&query=Wyart%2C+M">Matthieu Wyart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding the structure of real data is paramount in advancing modern
deep-learning methodologies. Natural data such as images are believed to be
composed of features organised in a hierarchical and combinatorial manner,
which neural networks capture during learning. Recent advancements show that
diffusion models can generate high-quality images, hinting at their ability to
capture this underlying structure. We study this phenomenon in a hierarchical
generative model of data. We find that the backward diffusion process acting
after a time $t$ is governed by a phase transition at some threshold time,
where the probability of reconstructing high-level features, like the class of
an image, suddenly drops. Instead, the reconstruction of low-level features,
such as specific details of an image, evolves smoothly across the whole
diffusion process. This result implies that at times beyond the transition, the
class has changed but the generated sample may still be composed of low-level
elements of the initial image. We validate these theoretical insights through
numerical experiments on class-unconditional ImageNet diffusion models. Our
analysis characterises the relationship between time and scale in diffusion
models and puts forward generative models as powerful tools to model
combinatorial data properties.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17036" title="Abstract">arXiv:2402.17036</a> (cross-list from stat.ML) [<a href="/pdf/2402.17036" title="Download PDF">pdf</a>, <a href="/format/2402.17036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Anderka%2C+R">Rafael Anderka</a>, 
<a href="/search/stat?searchtype=author&query=Deisenroth%2C+M+P">Marc Peter Deisenroth</a>, 
<a href="/search/stat?searchtype=author&query=Takao%2C+S">So Takao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Data assimilation (DA) methods use priors arising from differential equations
to robustly interpolate and extrapolate data. Popular techniques such as
ensemble methods that handle high-dimensional, nonlinear PDE priors focus
mostly on state estimation, however can have difficulty learning the parameters
accurately. On the other hand, machine learning based approaches can naturally
learn the state and parameters, but their applicability can be limited, or
produce uncertainties that are hard to interpret. Inspired by the Integrated
Nested Laplace Approximation (INLA) method in spatial statistics, we propose an
alternative approach to DA based on iteratively linearising the dynamical
model. This produces a Gaussian Markov random field at each iteration, enabling
one to use INLA to infer the state and parameters. Our approach can be used for
arbitrary nonlinear systems, while retaining interpretability, and is
furthermore demonstrated to outperform existing methods on the DA task. By
providing a more nuanced approach to handling nonlinear PDE priors, our
methodology offers improved accuracy and robustness in predictions, especially
where data sparsity is prevalent.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17039" title="Abstract">arXiv:2402.17039</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.17039" title="Download PDF">pdf</a>, <a href="/format/2402.17039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incorporating climate change effects into the European power system  adequacy assessment using a post-processing method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Harang%2C+I">In&#xe8;s Harang</a>, 
<a href="/search/physics?searchtype=author&query=Heymann%2C+F">Fabian Heymann</a>, 
<a href="/search/physics?searchtype=author&query=Stoop%2C+L+P">Laurens P. Stoop</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SEGAN, 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Computers and Society (cs.CY); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">The demand-supply balance of electricity systems is fundamentally linked to
climate conditions. In light of this, the present study aims to model the
effect of climate change on the European electricity system, specifically on
its long-term reliability. A resource adequate power system -- a system where
electricity supply covers demand -- is sensitive to generation capacity, demand
patterns, and the network structure and capacity. Climate change is foreseen to
affect each of these components.
<br />In this analysis, we focused on two drivers of power system adequacy: the
impact of temperature variations on electricity demand, and of water inflows
changes on hydro generation. Using a post-processing approach, based on results
found in the literature, the inputs of a large-scale electricity market model
covering the European region were modified. The results show that climate
change may decrease total LOLE (Loss of Load Expectation) hours in Europe by
more than 50%, as demand will largely decrease because of a higher temperatures
during winter. We found that the climate change impact on demand tends to
decrease LOLE values, while the climate change effects on hydrological
conditions tend to increase LOLE values.
<br />The study is built on a limited amount of open-source data and can flexibly
incorporate various sets of assumptions. Outcomes also show the current
difficulties to reliably model the effects of climate change on power system
adequacy. Overall, our presented method displays the relevance of climate
change effects in electricity network studies.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17040" title="Abstract">arXiv:2402.17040</a> (cross-list from math.OC) [<a href="/pdf/2402.17040" title="Download PDF">pdf</a>, <a href="/format/2402.17040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Radiotherapy Planning with Spatially Based Uncertainty Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Goldberg%2C+N">Noam Goldberg</a>, 
<a href="/search/math?searchtype=author&query=Langer%2C+M+P">Mark P. Langer</a>, 
<a href="/search/math?searchtype=author&query=Shtern%2C+S">Shimrit Shtern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Radiotherapy treatment planning is a challenging large-scale optimization
problem plagued by uncertainty. Following the robust optimization methodology,
we propose a novel, spatially based uncertainty set for robust modeling of
radiotherapy planning, producing solutions that are immune to unexpected
changes in biological conditions. Our proposed uncertainty set realistically
captures biological radiosensitivity patterns that are observed using recent
advances in imaging, while its parameters can be personalized for individual
patients. We exploit the structure of this set to devise a compact
reformulation of the robust model. We develop a row-generation scheme to solve
real, large-scale instances of the robust model. This method is then extended
to a relaxation-based scheme for enforcing challenging, yet clinically
important, dose-volume cardinality constraints. The computational performance
of our algorithms, as well as the quality and robustness of the computed
treatment plans, are demonstrated on simulated and real imaging data. Based on
accepted performance measures, such as minimal target dose and homogeneity,
these examples demonstrate that the spatially robust model achieves almost the
same performance as the nominal model in the nominal scenario, and otherwise,
the spatial model outperforms both the nominal and the box-uncertainty models.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17042" title="Abstract">arXiv:2402.17042</a> (cross-list from stat.ME) [<a href="/pdf/2402.17042" title="Download PDF">pdf</a>, <a href="/ps/2402.17042" title="Download PostScript">ps</a>, <a href="/format/2402.17042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Generalizing Inferences from Trials to Target Populations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Huang%2C+M+Y">Melody Y Huang</a>, 
<a href="/search/stat?searchtype=author&query=Robertson%2C+S+E">Sarah E Robertson</a>, 
<a href="/search/stat?searchtype=author&query=Parikh%2C+H">Harsh Parikh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
<p class="mathjax">Randomized Controlled Trials (RCTs) are pivotal in generating internally
valid estimates with minimal assumptions, serving as a cornerstone for
researchers dedicated to advancing causal inference methods. However, extending
these findings beyond the experimental cohort to achieve externally valid
estimates is crucial for broader scientific inquiry. This paper delves into the
forefront of addressing these external validity challenges, encapsulating the
essence of a multidisciplinary workshop held at the Institute for Computational
and Experimental Research in Mathematics (ICERM), Brown University, in Fall
2023. The workshop congregated experts from diverse fields including social
science, medicine, public health, statistics, computer science, and education,
to tackle the unique obstacles each discipline faces in extrapolating
experimental findings. Our study presents three key contributions: we integrate
ongoing efforts, highlighting methodological synergies across fields; provide
an exhaustive review of generalizability and transportability based on the
workshop's discourse; and identify persistent hurdles while suggesting avenues
for future research. By doing so, this paper aims to enhance the collective
understanding of the generalizability and transportability of causal effects,
fostering cross-disciplinary collaboration and offering valuable insights for
researchers working on refining and applying causal inference methods.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17067" title="Abstract">arXiv:2402.17067</a> (cross-list from math.ST) [<a href="/pdf/2402.17067" title="Download PDF">pdf</a>, <a href="/ps/2402.17067" title="Download PostScript">ps</a>, <a href="/format/2402.17067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Independent Samples Along the Langevin Diffusion and the Unadjusted  Langevin Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+J">Jiaming Liang</a>, 
<a href="/search/math?searchtype=author&query=Mitra%2C+S">Siddharth Mitra</a>, 
<a href="/search/math?searchtype=author&query=Wibisono%2C+A">Andre Wibisono</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the rate at which the initial and current random variables become
independent along a Markov chain, focusing on the Langevin diffusion in
continuous time and the Unadjusted Langevin Algorithm (ULA) in discrete time.
We measure the dependence between random variables via their mutual
information. For the Langevin diffusion, we show the mutual information
converges to $0$ exponentially fast when the target is strongly log-concave,
and at a polynomial rate when the target is weakly log-concave. These rates are
analogous to the mixing time of the Langevin diffusion under similar
assumptions. For the ULA, we show the mutual information converges to $0$
exponentially fast when the target is strongly log-concave and smooth. We prove
our results by developing the mutual version of the mixing time analyses of
these Markov chains. We also provide alternative proofs based on strong data
processing inequalities for the Langevin diffusion and the ULA, and by showing
regularity results for these processes in mutual information.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17086" title="Abstract">arXiv:2402.17086</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.17086" title="Download PDF">pdf</a>, <a href="/format/2402.17086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An optimal transport model for dynamical shapes, collective motion and  cellular aggregates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Diez%2C+A">Antoine Diez</a>, 
<a href="/search/q-bio?searchtype=author&query=Feydy%2C+J">Jean Feydy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Dynamical Systems (math.DS); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Many biological systems such as cell aggregates, tissues or bacterial
colonies behave as unconventional systems of particles that are strongly
constrained by volume exclusion and shape interactions. Understanding how these
constraints lead to macroscopic self-organized structures is a fundamental
question in e.g. developmental biology. To this end, various types of
computational models have been developed: phase fields, cellular automata,
vertex models, level-set, finite element simulations, etc. We introduce a new
framework based on optimal transport theory to model particle systems with
arbitrary dynamical shapes and deformability. Our method builds upon the
pioneering work of Brenier on incompressible fluids and its recent applications
to materials science. It lets us specify the shapes of individual cells and
supports a wide range of interaction mechanisms, while automatically taking
care of the volume exclusion constraint at an affordable numerical cost. We
showcase the versatility of this approach by reproducing several classical
systems in computational biology. Our Python code is freely available at:
www.github.com/antoinediez/ICeShOT
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17087" title="Abstract">arXiv:2402.17087</a> (cross-list from stat.ML) [<a href="/pdf/2402.17087" title="Download PDF">pdf</a>, <a href="/ps/2402.17087" title="Download PostScript">ps</a>, <a href="/format/2402.17087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Note on Bayesian Networks with Latent Root Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zaffalon%2C+M">Marco Zaffalon</a>, 
<a href="/search/stat?searchtype=author&query=Antonucci%2C+A">Alessandro Antonucci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We characterise the likelihood function computed from a Bayesian network with
latent variables as root nodes. We show that the marginal distribution over the
remaining, manifest, variables also factorises as a Bayesian network, which we
call empirical. A dataset of observations of the manifest variables allows us
to quantify the parameters of the empirical Bayesian net. We prove that (i) the
likelihood of such a dataset from the original Bayesian network is dominated by
the global maximum of the likelihood from the empirical one; and that (ii) such
a maximum is attained if and only if the parameters of the Bayesian network are
consistent with those of the empirical model.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17089" title="Abstract">arXiv:2402.17089</a> (cross-list from stat.ML) [<a href="/pdf/2402.17089" title="Download PDF">pdf</a>, <a href="/format/2402.17089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning high-dimensional targets by two-parameter models and gradient  flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yarotsky%2C+D">Dmitry Yarotsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We explore the theoretical possibility of learning $d$-dimensional targets
with $W$-parameter models by gradient flow (GF) when $W&lt;d$. Our main result
shows that if the targets are described by a particular $d$-dimensional
probability distribution, then there exist models with as few as two parameters
that can learn the targets with arbitrarily high success probability. On the
other hand, we show that for $W&lt;d$ there is necessarily a large subset of
GF-non-learnable targets. In particular, the set of learnable targets is not
dense in $\mathbb R^d$, and any subset of $\mathbb R^d$ homeomorphic to the
$W$-dimensional sphere contains non-learnable targets. Finally, we observe that
the model in our main theorem on almost guaranteed two-parameter learning is
constructed using a hierarchical procedure and as a result is not expressible
by a single elementary function. We show that this limitation is essential in
the sense that such learnability can be ruled out for a large class of
elementary functions.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17105" title="Abstract">arXiv:2402.17105</a> (cross-list from math.CO) [<a href="/pdf/2402.17105" title="Download PDF">pdf</a>, <a href="/ps/2402.17105" title="Download PostScript">ps</a>, <a href="/format/2402.17105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimum length word-representants of graph products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Srinivasan%2C+E">Eshwar Srinivasan</a>, 
<a href="/search/math?searchtype=author&query=Hariharasubramanian%2C+R">Ramesh Hariharasubramanian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, submitted to DAM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">A graph $G = (V, E)$ is said to be word-representable if a word $w$ can be
formed using the letters of the alphabet $V$ such that for every pair of
vertices $x$ and $y$, $xy \in E$ if and only if $x$ and $y$ alternate in $w$.
Gaetz and Ji have recently introduced the notion of minimum length
word-representants for word-representable graphs. They have also determined the
minimum possible length of the word-representants for certain classes of
graphs, such as trees and cycles. It is know that Cartesian and Rooted products
preserve word-representability. Moreover, Broere constructed a uniform word
representing the Cartesian product of $G$ and $K_n$ using occurrence based
functions.
<br />In this paper, we study the minimum length of word-representants for
Cartesian and Rooted products using morphism and occurrence based function,
respectively. Also, we solve an open problem posed by Broere in his master
thesis. This problem asks to construct a word for the Cartesian product of two
arbitrary word-representable graphs.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17106" title="Abstract">arXiv:2402.17106</a> (cross-list from stat.ML) [<a href="/pdf/2402.17106" title="Download PDF">pdf</a>, <a href="/format/2402.17106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset Fairness: Achievable Fairness on Your Data With Utility  Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Taufiq%2C+M+F">Muhammad Faaiz Taufiq</a>, 
<a href="/search/stat?searchtype=author&query=Ton%2C+J">Jean-Francois Ton</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">In machine learning fairness, training models which minimize disparity across
different sensitive groups often leads to diminished accuracy, a phenomenon
known as the fairness-accuracy trade-off. The severity of this trade-off
fundamentally depends on dataset characteristics such as dataset imbalances or
biases. Therefore using a uniform fairness requirement across datasets remains
questionable and can often lead to models with substantially low utility. To
address this, we present a computationally efficient approach to approximate
the fairness-accuracy trade-off curve tailored to individual datasets, backed
by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO)
framework, our approach mitigates the computational burden of having to train
multiple models when approximating the trade-off curve. Moreover, we quantify
the uncertainty in our approximation by introducing confidence intervals around
this curve, offering a statistically grounded perspective on the acceptable
range of fairness violations for any given accuracy threshold. Our empirical
evaluation spanning tabular, image and language datasets underscores that our
approach provides practitioners with a principled framework for
dataset-specific fairness decisions across various data modalities.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17148" title="Abstract">arXiv:2402.17148</a> (cross-list from quant-ph) [<a href="/pdf/2402.17148" title="Download PDF">pdf</a>, <a href="/format/2402.17148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time series generation for option pricing on quantum computers using  tensor network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kobayashi%2C+N">Nozomu Kobayashi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Suimon%2C+Y">Yoshiyuki Suimon</a>, 
<a href="/search/quant-ph?searchtype=author&query=Miyamoto%2C+K">Koichi Miyamoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP)

</div>
<p class="mathjax">Finance, especially option pricing, is a promising industrial field that
might benefit from quantum computing. While quantum algorithms for option
pricing have been proposed, it is desired to devise more efficient
implementations of costly operations in the algorithms, one of which is
preparing a quantum state that encodes a probability distribution of the
underlying asset price. In particular, in pricing a path-dependent option, we
need to generate a state encoding a joint distribution of the underlying asset
price at multiple time points, which is more demanding. To address these
issues, we propose a novel approach using Matrix Product State (MPS) as a
generative model for time series generation. To validate our approach, taking
the Heston model as a target, we conduct numerical experiments to generate time
series in the model. Our findings demonstrate the capability of the MPS model
to generate paths in the Heston model, highlighting its potential for
path-dependent option pricing on quantum computers.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17187" title="Abstract">arXiv:2402.17187</a> (cross-list from eess.IV) [<a href="/pdf/2402.17187" title="Download PDF">pdf</a>, <a href="/format/2402.17187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary  Embolism Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guo%2C+Z">Zhaoxin Guo</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhipeng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ge%2C+R">Ruiquan Ge</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+J">Jianxun Yu</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+F">Feiwei Qin</a>, 
<a href="/search/eess?searchtype=author&query=Tian%2C+Y">Yuan Tian</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+Y">Yuqing Peng</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yonghong Li</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Changmiao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The early detection of a pulmonary embolism (PE) is critical for enhancing
patient survival rates. Both image-based and non-image-based features are of
utmost importance in medical classification tasks. In a clinical setting,
physicians tend to rely on the contextual information provided by Electronic
Medical Records (EMR) to interpret medical imaging. However, very few models
effectively integrate clinical information with imaging data. To address this
shortcoming, we suggest a multimodal fusion methodology, termed PE-MVCNet,
which capitalizes on Computed Tomography Pulmonary Angiography imaging and EMR
data. This method comprises the Image-only module with an integrated multi-view
block, the EMR-only module, and the Cross-modal Attention Fusion (CMAF) module.
These modules cooperate to extract comprehensive features that subsequently
generate predictions for PE. We conducted experiments using the publicly
accessible Stanford University Medical Center dataset, achieving an AUROC of
94.1%, an accuracy rate of 90.2%, and an F1 score of 90.6%. Our proposed model
outperforms existing methodologies, corroborating that our multimodal fusion
model excels compared to models that use a single data modality.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17194" title="Abstract">arXiv:2402.17194</a> (cross-list from q-fin.TR) [<a href="/pdf/2402.17194" title="Download PDF">pdf</a>, <a href="/ps/2402.17194" title="Download PostScript">ps</a>, <a href="/format/2402.17194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Random Forest Model for Analyzing and Forecasting the US Stock  Market in the Context of Smart Finance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Zheng%2C+J">Jiajian Zheng</a>, 
<a href="/search/q-fin?searchtype=author&query=Xin%2C+D">Duan Xin</a>, 
<a href="/search/q-fin?searchtype=author&query=Cheng%2C+Q">Qishuo Cheng</a>, 
<a href="/search/q-fin?searchtype=author&query=Tian%2C+M">Miao Tian</a>, 
<a href="/search/q-fin?searchtype=author&query=Yang%2C+L">Le Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Trading and Market Microstructure (q-fin.TR)</span>; Computational Engineering, Finance, and Science (cs.CE); Portfolio Management (q-fin.PM)

</div>
<p class="mathjax">The stock market is a crucial component of the financial market, playing a
vital role in wealth accumulation for investors, financing costs for listed
companies, and the stable development of the national macroeconomy. Significant
fluctuations in the stock market can damage the interests of stock investors
and cause an imbalance in the industrial structure, which can interfere with
the macro level development of the national economy. The prediction of stock
price trends is a popular research topic in academia. Predicting the three
trends of stock pricesrising, sideways, and falling can assist investors in
making informed decisions about buying, holding, or selling stocks.
Establishing an effective forecasting model for predicting these trends is of
substantial practical importance. This paper evaluates the predictive
performance of random forest models combined with artificial intelligence on a
test set of four stocks using optimal parameters. The evaluation considers both
predictive accuracy and time efficiency.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17196" title="Abstract">arXiv:2402.17196</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.17196" title="Download PDF">pdf</a>, <a href="/format/2402.17196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with  Uncertainty Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Abduallah%2C+Y">Yasser Abduallah</a>, 
<a href="/search/astro-ph?searchtype=author&query=Alobaid%2C+K+A">Khalid A. Alobaid</a>, 
<a href="/search/astro-ph?searchtype=author&query=Wang%2C+J+T+L">Jason T. L. Wang</a>, 
<a href="/search/astro-ph?searchtype=author&query=Wang%2C+H">Haimin Wang</a>, 
<a href="/search/astro-ph?searchtype=author&query=Jordanova%2C+V+K">Vania K. Jordanova</a>, 
<a href="/search/astro-ph?searchtype=author&query=Yurchyshyn%2C+V">Vasyl Yurchyshyn</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cavus%2C+H">Huseyin Cavus</a>, 
<a href="/search/astro-ph?searchtype=author&query=Jing%2C+J">Ju Jing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a novel deep learning framework, named SYMHnet, which employs a
graph neural network and a bidirectional long short-term memory network to
cooperatively learn patterns from solar wind and interplanetary magnetic field
parameters for short-term forecasts of the SYM-H index based on 1-minute and
5-minute resolution data. SYMHnet takes, as input, the time series of the
parameters' values provided by NASA's Space Science Data Coordinated Archive
and predicts, as output, the SYM-H index value at time point t + w hours for a
given time point t where w is 1 or 2. By incorporating Bayesian inference into
the learning framework, SYMHnet can quantify both aleatoric (data) uncertainty
and epistemic (model) uncertainty when predicting future SYM-H indices.
Experimental results show that SYMHnet works well at quiet time and storm time,
for both 1-minute and 5-minute resolution data. The results also show that
SYMHnet generally performs better than related machine learning methods. For
example, SYMHnet achieves a forecast skill score (FSS) of 0.343 compared to the
FSS of 0.074 of a recent gradient boosting machine (GBM) method when predicting
SYM-H indices (1 hour in advance) in a large storm (SYM-H = -393 nT) using
5-minute resolution data. When predicting the SYM-H indices (2 hours in
advance) in the large storm, SYMHnet achieves an FSS of 0.553 compared to the
FSS of 0.087 of the GBM method. In addition, SYMHnet can provide results for
both data and model uncertainty quantification, whereas the related methods
cannot.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17220" title="Abstract">arXiv:2402.17220</a> (cross-list from math.PR) [<a href="/pdf/2402.17220" title="Download PDF">pdf</a>, <a href="/ps/2402.17220" title="Download PostScript">ps</a>, <a href="/format/2402.17220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the probability of a Pareto record
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fill%2C+J+A">James Allen Fill</a> (1), 
<a href="/search/math?searchtype=author&query=Sun%2C+A">Ao Sun</a> (1) ((1) The Johns Hopkins University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Given a sequence of independent random vectors taking values in ${\mathbb
R}^d$ and having common continuous distribution $F$, say that the $n^{\rm
\scriptsize th}$ observation sets a (Pareto) record if it is not dominated (in
every coordinate) by any preceding observation. Let $p_n(F) \equiv p_{n, d}(F)$
denote the probability that the $n^{\rm \scriptsize th}$ observation sets a
record. There are many interesting questions to address concerning $p_n$ and
multivariate records more generally, but this short paper focuses on how $p_n$
varies with $F$, particularly if, under $F$, the coordinates exhibit negative
dependence or positive dependence (rather than independence, a more-studied
case). We introduce new notions of negative and positive dependence ideally
suited for such a study, called negative record-setting probability dependence
(NRSPD) and positive record-setting probability dependence (PRSPD), relate
these notions to existing notions of dependence, and for fixed $d \geq 2$ and
$n \geq 1$ prove that the image of the mapping $p_n$ on the domain of NRSPD
(respectively, PRSPD) distributions is $[p^*_n, 1]$ (resp., $[n^{-1}, p^*_n]$),
where $p^*_n$ is the record-setting probability for any $F$ governing
independent coordinates.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17221" title="Abstract">arXiv:2402.17221</a> (cross-list from math.PR) [<a href="/pdf/2402.17221" title="Download PDF">pdf</a>, <a href="/ps/2402.17221" title="Download PostScript">ps</a>, <a href="/format/2402.17221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharpened localization of the trailing point of the Pareto record  frontier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fill%2C+J+A">James Allen Fill</a> (1), 
<a href="/search/math?searchtype=author&query=Naiman%2C+D">Daniel Naiman</a> (1), 
<a href="/search/math?searchtype=author&query=Sun%2C+A">Ao Sun</a> (1) ((1) The Johns Hopkins University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 2 figures. arXiv admin note: text overlap with <a href="/abs/1901.05621">arXiv:1901.05621</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">For $d\ge2$ and iid $d$-dimensional observations $X^{(1)},X^{(2)},\dots$ with
independent Exponential$(1)$ coordinates, we revisit the study by Fill and
Naiman (Electron. J. Probab., 2020) of the boundary (relative to the closed
positive orthant), or "frontier", $F_n$ of the closed Pareto record-setting
(RS) region \[ \mbox{RS}_n:=\{0\le x\in{\mathbb R}^d:x\not\prec X^{(i)}\mbox{\
for all $1\le i\le n$}\} \] at time $n$, where $0\le x$ means that $0\le x_j$
for $1\le j\le d$ and $x\prec y$ means that $x_j&lt;y_j$ for $1\le j\le d$. With
$x_+:=\sum_{j=1}^d x_j$, let \[ F_n^-:=\min\{x_+:x\in F_n\}\quad\mbox{and}\quad
F_n^+:=\max\{x_+:x\in F_n\}. \] Almost surely, there are for each $n$ unique
vectors $\lambda_n\in F_n$ and $\tau_n\in F_n$ such that $F_n^+=(\lambda_n)_+$
and $F_n^-=(\tau_n)_+$; we refer to $\lambda_n$ and $\tau_n$ as the leading and
trailing points, respectively, of the frontier. Fill and Naiman provided rather
sharp information about the typical and almost sure behavior of $F^+$, but
somewhat crude information about $F^-$, namely, that for any $\varepsilon &gt;0$
and $c_n\to\infty$ we have \[ {\mathbb P}(F_n^- -\ln n\in
(-(2+\varepsilon)\ln\ln\ln n,c_n))\to 1 \] (describing typical behavior) and
almost surely \[ \limsup \frac{F_n^- - \ln n}{\ln \ln n} \le 0 \quad \mbox{and}
\quad \liminf \frac{F_n^- - \ln n}{\ln \ln \ln n} \in [-2, -1]. \]
<br />In this paper we use the theory of generators (minima of $F_n$) together with
the first- and second-moment methods to improve considerably the trailing-point
location results to \[ F_n^- - (\ln n - \ln \ln \ln n)
\overset{\mathrm{P}}{\longrightarrow} - \ln(d - 1) \] (describing typical
behavior) and, for $d \ge 3$, almost surely \begin{align*} &amp;\limsup [F_n^- -
(\ln n - \ln \ln \ln n)] \leq -\ln(d - 2) + \ln 2 \\ \mbox{and }&amp;\liminf [F_n^-
- (\ln n - \ln \ln \ln n)] \ge - \ln d - \ln 2. \end{align*}
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17222" title="Abstract">arXiv:2402.17222</a> (cross-list from math.OC) [<a href="/pdf/2402.17222" title="Download PDF">pdf</a>, <a href="/ps/2402.17222" title="Download PostScript">ps</a>, <a href="/format/2402.17222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deadzone-Adapted Disturbance Suppression Control for Strict-Feedback  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Karafyllis%2C+I">Iasson Karafyllis</a>, 
<a href="/search/math?searchtype=author&query=Krstic%2C+M">Miroslav Krstic</a>, 
<a href="/search/math?searchtype=author&query=Aslanidis%2C+A">Alexandros Aslanidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 6 figures. arXiv admin note: text overlap with <a href="/abs/2311.07938">arXiv:2311.07938</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper we extend our recently proposed Deadzone-Adapted Disturbance
Suppression (DADS) Control approach from systems with matched uncertainties to
general systems in parametric strict feedback form. The DADS approach prevents
gain and state drift regardless of the size of the disturbance and unknown
parameter and achieves an attenuation of the plant output to an assignable
small level, despite the presence of persistent disturbances and unknown
parameters of arbitrary and unknown bounds. The controller is designed by means
of a step-by-step backstepping procedure which can be applied in an algorithmic
fashion. Examples are provided which illustrate the efficiency of the DADS
controller compared to existing adaptive control schemes.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17246" title="Abstract">arXiv:2402.17246</a> (cross-list from eess.IV) [<a href="/pdf/2402.17246" title="Download PDF">pdf</a>, <a href="/format/2402.17246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion  Classification Using 3D Multi-Phase Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lou%2C+M">Meng Lou</a>, 
<a href="/search/eess?searchtype=author&query=Ying%2C+H">Hanning Ying</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xiaoqing Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+H">Hong-Yu Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yuqing Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+Y">Yizhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Automated classification of liver lesions in multi-phase CT and MR scans is
of clinical significance but challenging. This study proposes a novel Siamese
Dual-Resolution Transformer (SDR-Former) framework, specifically designed for
liver lesion classification in 3D multi-phase CT and MR imaging with varying
phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural
Network (SNN) to process multi-phase imaging inputs, possessing robust feature
representations while maintaining computational efficiency. The weight-sharing
feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer
(DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored
3D Transformer for processing high- and low-resolution images, respectively.
This hybrid sub-architecture excels in capturing detailed local features and
understanding global contextual information, thereby, boosting the SNN's
feature extraction capabilities. Additionally, a novel Adaptive Phase Selection
Module (APSM) is introduced, promoting phase-specific intercommunication and
dynamically adjusting each phase's influence on the diagnostic outcome. The
proposed SDR-Former framework has been validated through comprehensive
experiments on two clinical datasets: a three-phase CT dataset and an
eight-phase MR dataset. The experimental results affirm the efficacy of the
proposed framework. To support the scientific community, we are releasing our
extensive multi-phase MR dataset for liver lesion analysis to the public. This
pioneering dataset, being the first publicly available multi-phase MR dataset
in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is
accessible at:https://bit.ly/3IyYlgN.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17295" title="Abstract">arXiv:2402.17295</a> (cross-list from quant-ph) [<a href="/pdf/2402.17295" title="Download PDF">pdf</a>, <a href="/format/2402.17295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Distance Approximation for Persistence Diagrams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ameneyro%2C+B">Bernardo Ameneyro</a>, 
<a href="/search/quant-ph?searchtype=author&query=Herrman%2C+R">Rebekah Herrman</a>, 
<a href="/search/quant-ph?searchtype=author&query=Siopsis%2C+G">George Siopsis</a>, 
<a href="/search/quant-ph?searchtype=author&query=Maroulas%2C+V">Vasileios Maroulas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 11 figures, submitted to SIAM Journal on Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Topological Data Analysis methods can be useful for classification and
clustering tasks in many different fields as they can provide two dimensional
persistence diagrams that summarize important information about the shape of
potentially complex and high dimensional data sets. The space of persistence
diagrams can be endowed with various metrics such as the Wasserstein distance
which admit a statistical structure and allow to use these summaries for
machine learning algorithms. However, computing the distance between two
persistence diagrams involves finding an optimal way to match the points of the
two diagrams and may not always be an easy task for classical computers. In
this work we explore the potential of quantum computers to estimate the
distance between persistence diagrams, in particular we propose variational
quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$
distance. Our implementation is a weighted version of the Quantum Approximate
Optimization Algorithm that relies on control clauses to encode the constraints
of the optimization problem.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17307" title="Abstract">arXiv:2402.17307</a> (cross-list from eess.IV) [<a href="/pdf/2402.17307" title="Download PDF">pdf</a>, <a href="/format/2402.17307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Models for Inpainting of Healthy Brain Tissue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Durrer%2C+A">Alicia Durrer</a>, 
<a href="/search/eess?searchtype=author&query=Cattin%2C+P+C">Philippe C. Cattin</a>, 
<a href="/search/eess?searchtype=author&query=Wolleb%2C+J">Julia Wolleb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, MICCAI challenge submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper is a contribution to the "BraTS 2023 Local Synthesis of Healthy
Brain Tissue via Inpainting Challenge". The task of this challenge is to
transform tumor tissue into healthy tissue in brain magnetic resonance (MR)
images. This idea originates from the problem that MR images can be evaluated
using automatic processing tools, however, many of these tools are optimized
for the analysis of healthy tissue. By solving the given inpainting task, we
enable the automatic analysis of images featuring lesions, and further
downstream tasks. Our approach builds on denoising diffusion probabilistic
models. We use a 2D model that is trained using slices in which healthy tissue
was cropped out and is learned to be inpainted again. This allows us to use the
ground truth healthy tissue during training. In the sampling stage, we replace
the slices containing diseased tissue in the original 3D volume with the slices
containing the healthy tissue inpainting. With our approach, we achieve
comparable results to the competing methods. On the validation set our model
achieves a mean SSIM of 0.7804, a PSNR of 20.3525 and a MSE of 0.0113. In
future we plan to extend our 2D model to a 3D model, allowing to inpaint the
region of interest as a whole without losing context information of neighboring
slices.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17317" title="Abstract">arXiv:2402.17317</a> (cross-list from eess.IV) [<a href="/pdf/2402.17317" title="Download PDF">pdf</a>, <a href="/format/2402.17317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced  Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ferreira%2C+A">Andr&#xe9; Ferreira</a>, 
<a href="/search/eess?searchtype=author&query=Solak%2C+N">Naida Solak</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jianning Li</a>, 
<a href="/search/eess?searchtype=author&query=Dammann%2C+P">Philipp Dammann</a>, 
<a href="/search/eess?searchtype=author&query=Kleesiek%2C+J">Jens Kleesiek</a>, 
<a href="/search/eess?searchtype=author&query=Alves%2C+V">Victor Alves</a>, 
<a href="/search/eess?searchtype=author&query=Egger%2C+J">Jan Egger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep Learning is the state-of-the-art technology for segmenting brain
tumours. However, this requires a lot of high-quality data, which is difficult
to obtain, especially in the medical field. Therefore, our solutions address
this problem by using unconventional mechanisms for data augmentation.
Generative adversarial networks and registration are used to massively increase
the amount of available samples for training three different deep learning
models for brain tumour segmentation, the first task of the BraTS2023
challenge. The first model is the standard nnU-Net, the second is the Swin
UNETR and the third is the winning solution of the BraTS 2021 Challenge. The
entire pipeline is built on the nnU-Net implementation, except for the
generation of the synthetic data. The use of convolutional algorithms and
transformers is able to fill each other's knowledge gaps. Using the new metric,
our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95
14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the
validation set.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17346" title="Abstract">arXiv:2402.17346</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.17346" title="Download PDF">pdf</a>, <a href="/format/2402.17346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the training of PINNs for unsteady flow past a plunging  foil through the lens of input subdomain level loss function gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sundar%2C+R">Rahul Sundar</a>, 
<a href="/search/physics?searchtype=author&query=Lucor%2C+D">Didier Lucor</a>, 
<a href="/search/physics?searchtype=author&query=Sarkar%2C+S">Sunetra Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently immersed boundary method-inspired physics-informed neural networks
(PINNs) including the moving boundary-enabled PINNs (MB-PINNs) have shown the
ability to accurately reconstruct velocity and recover pressure as a hidden
variable for unsteady flow past moving bodies. Considering flow past a plunging
foil, MB-PINNs were trained with global physics loss relaxation and also in
conjunction with a physics-based undersampling method, obtaining good accuracy.
The purpose of this study was to investigate which input spatial subdomain
contributes to the training under the effect of physics loss relaxation and
physics-based undersampling. In the context of MB-PINNs training, three spatial
zones: the moving body, wake, and outer zones were defined. To quantify which
spatial zone drives the training, two novel metrics are computed from the zonal
loss component gradient statistics and the proportion of sample points in each
zone. Results confirm that the learning indeed depends on the combined effect
of the zonal loss component gradients and the proportion of points in each
zone. Moreover, the dominant input zones are also the ones that have the
strongest solution gradients in some sense.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17386" title="Abstract">arXiv:2402.17386</a> (cross-list from hep-ph) [<a href="/pdf/2402.17386" title="Download PDF">pdf</a>, <a href="/format/2402.17386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A case study of sending graph neural networks back to the test bench for  applications in high-energy particle physics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Pfeffer%2C+E">Emanuel Pfeffer</a>, 
<a href="/search/hep-ph?searchtype=author&query=Wa%C3%9Fmer%2C+M">Michael Wa&#xdf;mer</a>, 
<a href="/search/hep-ph?searchtype=author&query=Cung%2C+Y">Yee-Ying Cung</a>, 
<a href="/search/hep-ph?searchtype=author&query=Wolf%2C+R">Roger Wolf</a>, 
<a href="/search/hep-ph?searchtype=author&query=Husemann%2C+U">Ulrich Husemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Artificial Intelligence (cs.AI); High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">In high-energy particle collisions, the primary collision products usually
decay further resulting in tree-like, hierarchical structures with a priori
unknown multiplicity. At the stable-particle level all decay products of a
collision form permutation invariant sets of final state objects. The analogy
to mathematical graphs gives rise to the idea that graph neural networks
(GNNs), which naturally resemble these properties, should be best-suited to
address many tasks related to high-energy particle physics. In this paper we
describe a benchmark test of a typical GNN against neural networks of the
well-established deep fully-connected feed-forward architecture. We aim at
performing this comparison maximally unbiased in terms of nodes, hidden layers,
or trainable parameters of the neural networks under study. As physics case we
use the classification of the final state X produced in association with top
quark-antiquark pairs in proton-proton collisions at the Large Hadron Collider
at CERN, where X stands for a bottom quark-antiquark pair produced either
non-resonantly or through the decay of an intermediately produced Z or Higgs
boson.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17398" title="Abstract">arXiv:2402.17398</a> (cross-list from quant-ph) [<a href="/pdf/2402.17398" title="Download PDF">pdf</a>, <a href="/format/2402.17398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Mohanty%2C+N">Nishikanta Mohanty</a>, 
<a href="/search/quant-ph?searchtype=author&query=Behera%2C+B+K">Bikash K. Behera</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ferrie%2C+C">Christopher Ferrie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 22 Figures, 2 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The paper proposes the Quantum-SMOTE method, a novel solution that uses
quantum computing techniques to solve the prevalent problem of class imbalance
in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority
Oversampling Technique (SMOTE), generates synthetic data points using quantum
processes such as swap tests and quantum rotation. The process varies from the
conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean
distances, enabling synthetic instances to be generated from minority class
data points without relying on neighbor proximity. The algorithm asserts
greater control over the synthetic data generation process by introducing
hyperparameters such as rotation angle, minority percentage, and splitting
factor, which allow for customization to specific dataset requirements. The
approach is tested on a public dataset of TelecomChurn and evaluated alongside
two prominent classification algorithms, Random Forest and Logistic Regression,
to determine its impact along with varying proportions of synthetic data.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17402" title="Abstract">arXiv:2402.17402</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.17402" title="Download PDF">pdf</a>, <a href="/format/2402.17402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beacon, a lightweight deep reinforcement learning benchmark library for  flow control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Viquerat%2C+J">Jonathan Viquerat</a>, 
<a href="/search/physics?searchtype=author&query=Meliga%2C+P">Philippe Meliga</a>, 
<a href="/search/physics?searchtype=author&query=Jeken%2C+P">Pablo Jeken</a>, 
<a href="/search/physics?searchtype=author&query=Hachem%2C+E">Elie Hachem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">Recently, the increasing use of deep reinforcement learning for flow control
problems has led to a new area of research, focused on the coupling and the
adaptation of the existing algorithms to the control of numerical fluid
dynamics environments. Although still in its infancy, the field has seen
multiple successes in a short time span, and its fast development pace can
certainly be partly imparted to the open-source effort that drives the
expansion of the community. Yet, this emerging domain still misses a common
ground to (i) ensure the reproducibility of the results, and (ii) offer a
proper ad-hoc benchmarking basis. To this end, we propose Beacon, an
open-source benchmark library composed of seven lightweight 1D and 2D flow
control problems with various characteristics, action and observation space
characteristics, and CPU requirements. In this contribution, the seven
considered problems are described, and reference control solutions are
provided. The sources for the following work are available at
https://github.com/jviquerat/beacon.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17438" title="Abstract">arXiv:2402.17438</a> (cross-list from eess.IV) [<a href="/pdf/2402.17438" title="Download PDF">pdf</a>, <a href="/format/2402.17438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal  Correspondence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bongratz%2C+F">Fabian Bongratz</a>, 
<a href="/search/eess?searchtype=author&query=Fecht%2C+J">Jan Fecht</a>, 
<a href="/search/eess?searchtype=author&query=Rickmann%2C+A">Anne-Marie Rickmann</a>, 
<a href="/search/eess?searchtype=author&query=Wachinger%2C+C">Christian Wachinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Reconstructing the cortex from longitudinal MRI is indispensable for
analyzing morphological changes in the human brain. Despite the recent
disruption of cortical surface reconstruction with deep learning, challenges
arising from longitudinal data are still persistent. Especially the lack of
strong spatiotemporal point correspondence hinders downstream analyses due to
the introduced noise. To address this issue, we present V2C-Long, the first
dedicated deep learning-based cortex reconstruction method for longitudinal
MRI. In contrast to existing methods, V2C-Long surfaces are directly comparable
in a cross-sectional and longitudinal manner. We establish strong inherent
spatiotemporal correspondences via a novel composition of two deep mesh
deformation networks and fast aggregation of feature-enhanced within-subject
templates. The results on internal and external test data demonstrate that
V2C-Long yields cortical surfaces with improved accuracy and consistency
compared to previous methods. Finally, this improvement manifests in higher
sensitivity to regional cortical atrophy in Alzheimer's disease.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17454" title="Abstract">arXiv:2402.17454</a> (cross-list from physics.med-ph) [<a href="/pdf/2402.17454" title="Download PDF">pdf</a>, <a href="/ps/2402.17454" title="Download PostScript">ps</a>, <a href="/format/2402.17454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment anything model for head and neck tumor segmentation with CT, PET  and MRI multi-modality images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ren%2C+J">Jintao Ren</a>, 
<a href="/search/physics?searchtype=author&query=Rasmussen%2C+M">Mathis Rasmussen</a>, 
<a href="/search/physics?searchtype=author&query=Nijkamp%2C+J">Jasper Nijkamp</a>, 
<a href="/search/physics?searchtype=author&query=Eriksen%2C+J+G">Jesper Grau Eriksen</a>, 
<a href="/search/physics?searchtype=author&query=Korreman%2C+S">Stine Korreman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep learning presents novel opportunities for the auto-segmentation of gross
tumor volume (GTV) in head and neck cancer (HNC), yet fully automatic methods
usually necessitate significant manual refinement. This study investigates the
Segment Anything Model (SAM), recognized for requiring minimal human prompting
and its zero-shot generalization ability across natural images. We specifically
examine MedSAM, a version of SAM fine-tuned with large-scale public medical
images. Despite its progress, the integration of multi-modality images (CT,
PET, MRI) for effective GTV delineation remains a challenge. Focusing on SAM's
application in HNC GTV segmentation, we assess its performance in both
zero-shot and fine-tuned scenarios using single (CT-only) and fused
multi-modality images. Our study demonstrates that fine-tuning SAM
significantly enhances its segmentation accuracy, building upon the already
effective zero-shot results achieved with bounding box prompts. These findings
open a promising avenue for semi-automatic HNC GTV segmentation.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17474" title="Abstract">arXiv:2402.17474</a> (cross-list from math.PR) [<a href="/pdf/2402.17474" title="Download PDF">pdf</a>, <a href="/format/2402.17474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Holistic Approach for Bitcoin Confirmation Times &amp; Optimal Fee  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=G%C3%BCndlach%2C+R">Rowel G&#xfc;ndlach</a>, 
<a href="/search/math?searchtype=author&query=Stoepker%2C+I+V">Ivo V. Stoepker</a>, 
<a href="/search/math?searchtype=author&query=Kapodistria%2C+S">Stella Kapodistria</a>, 
<a href="/search/math?searchtype=author&query=Resing%2C+J+A+C">Jacques A. C. Resing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Bitcoin is currently subject to a significant pay-for-speed trade-off. This
is caused by lengthy and highly variable transaction confirmation times,
especially during times of congestion. Users can reduce their transaction
confirmation times by increasing their transaction fee. In this paper, based on
the inner workings of Bitcoin, we propose a model-based approach (based on the
Cram\'er-Lundberg model) that can be used to determine the optimal fee, via,
for example, the mean or quantiles, and models accurately the confirmation time
distribution for a given fee. The proposed model is highly suitable as it
arises as the limiting model for the mempool process (that tracks the
unconfirmed transactions), which we rigorously show via a fluid limit and we
extend this to the diffusion limit (an approximation of the Cram\'er-Lundberg
model for fast computations in highly congested instances). We also propose
methods (incorporating the real-time data) to estimate the model parameters,
thereby combining model and data-driven approaches. The model-based approach is
validated on real-world data and the resulting transaction fees outperform, in
most instances, the data-driven ones.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17492" title="Abstract">arXiv:2402.17492</a> (cross-list from astro-ph.CO) [<a href="/pdf/2402.17492" title="Download PDF">pdf</a>, <a href="/format/2402.17492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> syren-halofit: A fast, interpretable, high-precision formula for the  $&#x39b;$CDM nonlinear matter power spectrum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Bartlett%2C+D+J">Deaglan J. Bartlett</a>, 
<a href="/search/astro-ph?searchtype=author&query=Wandelt%2C+B+D">Benjamin D. Wandelt</a>, 
<a href="/search/astro-ph?searchtype=author&query=Zennaro%2C+M">Matteo Zennaro</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ferreira%2C+P+G">Pedro G. Ferreira</a>, 
<a href="/search/astro-ph?searchtype=author&query=Desmond%2C+H">Harry Desmond</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures. Submitted to A&amp;A
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$,
as a function of cosmological parameters and redshift is of fundamental
importance in cosmology. Analytic approximations provide an interpretable
solution, yet current approximations are neither fast nor accurate relative to
black-box numerical emulators. We use symbolic regression to obtain simple
analytic approximations to the nonlinear scale, $k_\sigma$, the effective
spectral index, $n_{\rm eff}$, and the curvature, $C$, which are required for
the halofit model. We then re-optimise the coefficients of halofit to fit a
wide range of cosmologies and redshifts. We then again exploit symbolic
regression to explore the space of analytic expressions to fit the residuals
between $P(k)$ and the optimised predictions of halofit. All methods are
validated against $N$-body simulations. Our symbolic expressions for
$k_\sigma$, $n_{\rm eff}$ and $C$ have root mean squared fractional errors of
0.8%, 0.2% and 0.3%, respectively, for redshifts below 3 and a wide range of
cosmologies. The re-optimised halofit parameters reduce the root mean squared
fractional error from 3% to below 2% for wavenumbers $k=9\times10^{-3}-9 \,
h{\rm Mpc^{-1}}$. We introduce syren-halofit (symbolic-regression-enhanced
halofit), an extension to halofit containing a short symbolic correction which
improves this error to 1%. Our method is 2350 and 3170 times faster than
current halofit and hmcode implementations, respectively, and 2680 and 64 times
faster than EuclidEmulator2 (which requires running class) and the BACCO
emulator. We obtain comparable accuracy to EuclidEmulator2 and the BACCO
emulator when tested on $N$-body simulations. Our work greatly increases the
speed and accuracy of symbolic approximations to $P(k)$, making them
significantly faster than their numerical counterparts without loss of
accuracy.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17500" title="Abstract">arXiv:2402.17500</a> (cross-list from nlin.AO) [<a href="/pdf/2402.17500" title="Download PDF">pdf</a>, <a href="/format/2402.17500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Instability in Complex Oscillator Networks: Limitations and  Potentials of Network Measures and Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Nauck%2C+C">Christian Nauck</a>, 
<a href="/search/nlin?searchtype=author&query=Lindner%2C+M">Michael Lindner</a>, 
<a href="/search/nlin?searchtype=author&query=Molkenthin%2C+N">Nora Molkenthin</a>, 
<a href="/search/nlin?searchtype=author&query=Kurths%2C+J">J&#xfc;rgen Kurths</a>, 
<a href="/search/nlin?searchtype=author&query=Sch%C3%B6ll%2C+E">Eckehard Sch&#xf6;ll</a>, 
<a href="/search/nlin?searchtype=author&query=Raisch%2C+J">J&#xf6;rg Raisch</a>, 
<a href="/search/nlin?searchtype=author&query=Hellmann%2C+F">Frank Hellmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages (16 pages main section), 15 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Adaptation and Self-Organizing Systems (nlin.AO)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">A central question of network science is how functional properties of systems
arise from their structure. For networked dynamical systems, structure is
typically quantified with network measures. A functional property that is of
theoretical and practical interest for oscillatory systems is the stability of
synchrony to localized perturbations. Recently, Graph Neural Networks (GNNs)
have been shown to predict this stability successfully; at the same time,
network measures have struggled to paint a clear picture. Here we collect 46
relevant network measures and find that no small subset can reliably predict
stability. The performance of GNNs can only be matched by combining all network
measures and nodewise machine learning. However, unlike GNNs, this approach
fails to extrapolate from network ensembles to several real power grid
topologies. This suggests that correlations of network measures and function
may be misleading, and that GNNs capture the causal relationship between
structure and stability substantially better.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17506" title="Abstract">arXiv:2402.17506</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.17506" title="Download PDF">pdf</a>, <a href="/format/2402.17506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thermodynamics-informed super-resolution of scarce temporal dynamics  data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Bermejo-Barbanoj%2C+C">Carlos Bermejo-Barbanoj</a>, 
<a href="/search/physics?searchtype=author&query=Moya%2C+B">Beatriz Moya</a>, 
<a href="/search/physics?searchtype=author&query=Bad%C3%ADas%2C+A">Alberto Bad&#xed;as</a>, 
<a href="/search/physics?searchtype=author&query=Chinesta%2C+F">Francisco Chinesta</a>, 
<a href="/search/physics?searchtype=author&query=Cueto%2C+E">El&#xed;as Cueto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a method to increase the resolution of measurements of a physical
system and subsequently predict its time evolution using thermodynamics-aware
neural networks. Our method uses adversarial autoencoders, which reduce the
dimensionality of the full order model to a set of latent variables that are
enforced to match a prior, for example a normal distribution. Adversarial
autoencoders are seen as generative models, and they can be trained to generate
high-resolution samples from low-resoution inputs, meaning they can address the
so-called super-resolution problem. Then, a second neural network is trained to
learn the physical structure of the latent variables and predict their temporal
evolution. This neural network is known as an structure-preserving neural
network. It learns the metriplectic-structure of the system and applies a
physical bias to ensure that the first and second principles of thermodynamics
are fulfilled. The integrated trajectories are decoded to their original
dimensionality, as well as to the higher dimensionality space produced by the
adversarial autoencoder and they are compared to the ground truth solution. The
method is tested with two examples of flow over a cylinder, where the fluid
properties are varied between both examples.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17539" title="Abstract">arXiv:2402.17539</a> (cross-list from math.OC) [<a href="/pdf/2402.17539" title="Download PDF">pdf</a>, <a href="/ps/2402.17539" title="Download PostScript">ps</a>, <a href="/format/2402.17539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The optimizing mode classification stabilization of sampled stochastic  jump systems via an improved hill-climbing algorithm based on Q-learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+G">Guoliang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Statistics Theory (math.ST)

</div>
<p class="mathjax">This paper addresses the stabilization problem of stochastic jump systems
(SJSs) closed by a generally sampled controller. Because of the controller's
switching and state both sampled, it is challenging to study its stabilization.
A new stabilizing method deeply depending on the mode classifications is
proposed to deal with the above sampling situation, whose quantity is equal to
a Stirling number of the second kind. For the sake of finding the best
stabilization effect among all the classifications, a convex optimization
problem is developed, whose globally solution is proved to be existent and can
be computed by an augmented Lagrangian function. More importantly, in order to
further reduce the computation complexity but retaining a better performance as
much as possible, a novelly improved hill-climbing algorithm is established by
applying the Q-learning technique to provide an optimal attenuation
coefficient. A numerical example is offered so as to verify the effectiveness
and superiority of the methods proposed in this study.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17544" title="Abstract">arXiv:2402.17544</a> (cross-list from eess.IV) [<a href="/pdf/2402.17544" title="Download PDF">pdf</a>, <a href="/format/2402.17544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Learned Image Codecs to Screen Content via Adjustable  Transformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dogaroglu%2C+H+B">H. Burak Dogaroglu</a>, 
<a href="/search/eess?searchtype=author&query=Koyuncu%2C+A+B">A. Burakhan Koyuncu</a>, 
<a href="/search/eess?searchtype=author&query=Boev%2C+A">Atanas Boev</a>, 
<a href="/search/eess?searchtype=author&query=Alshina%2C+E">Elena Alshina</a>, 
<a href="/search/eess?searchtype=author&query=Steinbach%2C+E">Eckehard Steinbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">As learned image codecs (LICs) become more prevalent, their low coding
efficiency for out-of-distribution data becomes a bottleneck for some
applications. To improve the performance of LICs for screen content (SC) images
without breaking backwards compatibility, we propose to introduce parameterized
and invertible linear transformations into the coding pipeline without changing
the underlying baseline codec's operation flow. We design two neural networks
to act as prefilters and postfilters in our setup to increase the coding
efficiency and help with the recovery from coding artifacts. Our end-to-end
trained solution achieves up to 10% bitrate savings on SC compression compared
to the baseline LICs while introducing only 1% extra parameters.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17621" title="Abstract">arXiv:2402.17621</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.17621" title="Download PDF">pdf</a>, <a href="/ps/2402.17621" title="Download PostScript">ps</a>, <a href="/format/2402.17621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised machine learning for microbiomics: bridging the gap between  current and best practices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Dudek%2C+N+K">Natasha K. Dudek</a>, 
<a href="/search/q-bio?searchtype=author&query=Chakhvadze%2C+M">Mariam Chakhvadze</a>, 
<a href="/search/q-bio?searchtype=author&query=Kobakhidze%2C+S">Saba Kobakhidze</a>, 
<a href="/search/q-bio?searchtype=author&query=Kantidze%2C+O">Omar Kantidze</a>, 
<a href="/search/q-bio?searchtype=author&query=Gankin%2C+Y">Yuriy Gankin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning (ML) is set to accelerate innovations in clinical
microbiomics, such as in disease diagnostics and prognostics. This will require
high-quality, reproducible, interpretable workflows whose predictive
capabilities meet or exceed the high thresholds set for clinical tools by
regulatory agencies. Here, we capture a snapshot of current practices in the
application of supervised ML to microbiomics data, through an in-depth analysis
of 100 peer-reviewed journal articles published in 2021-2022. We apply a
data-driven approach to steer discussion of the merits of varied approaches to
experimental design, including key considerations such as how to mitigate the
effects of small dataset size while avoiding data leakage. We further provide
guidance on how to avoid common experimental design pitfalls that can hurt
model performance, trustworthiness, and reproducibility. Discussion is
accompanied by an interactive online tutorial that demonstrates foundational
principles of ML experimental design, tailored to the microbiomics community.
Formalizing community best practices for supervised ML in microbiomics is an
important step towards improving the success and efficiency of clinical
research, to the benefit of patients and other stakeholders.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17686" title="Abstract">arXiv:2402.17686</a> (cross-list from physics.chem-ph) [<a href="/pdf/2402.17686" title="Download PDF">pdf</a>, <a href="/format/2402.17686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Vazquez-Salazar%2C+L+I">Luis Itza Vazquez-Salazar</a>, 
<a href="/search/physics?searchtype=author&query=K%C3%A4ser%2C+S">Silvan K&#xe4;ser</a>, 
<a href="/search/physics?searchtype=author&query=Meuwly%2C+M">Markus Meuwly</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Uncertainty quantification (UQ) to detect samples with large expected errors
(outliers) is applied to reactive molecular potential energy surfaces (PESs).
Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian
Mixture Models (GMM) - were applied to the H-transfer reaction between ${\it
syn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble
models provide the best results for detecting outliers, followed by GMM. For
example, from a pool of 1000 structures with the largest uncertainty, the
detection quality for outliers is $\sim 90$ \% and $\sim 50$ \%, respectively,
if 25 or 1000 structures with large errors are sought. On the contrary, the
limitations of the statistical assumptions of DER greatly impacted its
prediction capabilities. Finally, a structure-based indicator was found to be
correlated with large average error, which may help to rapidly classify new
structures into those that provide an advantage for refining the neural
network.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17693" title="Abstract">arXiv:2402.17693</a> (cross-list from quant-ph) [<a href="/pdf/2402.17693" title="Download PDF">pdf</a>, <a href="/format/2402.17693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Complete Graphical Language for Linear Optical Circuits with  Finite-Photon-Number Sources and Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Heurtel%2C+N">Nicolas Heurtel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Linear optical circuits can be used to manipulate the quantum states of
photons as they pass through components including beam splitters and phase
shifters. Those photonic states possess a particularly high level of
expressiveness, as they reside within the bosonic Fock space, an
infinite-dimensional Hilbert space. However, in the domain of linear optical
quantum computation, these basic components may not be sufficient to
efficiently perform all computations of interest, such as universal quantum
computation. To address this limitation it is common to add auxiliary sources
and detectors, which enable projections onto auxiliary photonic states and thus
increase the versatility of the processes. In this paper, we introduce the
$\textbf{LO}_{fi}$-calculus, a graphical language to reason on the
infinite-dimensional bosonic Fock space with circuits composed of four core
elements of linear optics: the phase shifter, the beam splitter, and auxiliary
sources and detectors with bounded photon number. We present an equational
theory that we prove to be complete: two $\textbf{LO}_{fi}$-circuits represent
the same quantum process if and only if one can be transformed into the other
with the rules of the $\textbf{LO}_{fi}$-calculus. We give a unique and compact
universal form for such circuits.
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17694" title="Abstract">arXiv:2402.17694</a> (cross-list from math.OC) [<a href="/pdf/2402.17694" title="Download PDF">pdf</a>, <a href="/format/2402.17694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Control Barrier Functions: Maximizing the Action Space Subject  to Control Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Beaver%2C+L+E">Logan E. Beaver</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This letter addresses the constraint compatibility problem of control barrier
functions (CBFs), which occurs when a safety-critical CBF requires a system to
apply more control effort than it is capable of generating. This inevitably
leads to a safety violation, which transitions the system to an unsafe (and
possibly dangerous) trajectory. We resolve the constraint compatibility problem
by constructing a control barrier function that maximizes the feasible action
space for first and second-order constraints, and we prove that the optimal CBF
encodes a dynamical motion primitive. Furthermore, we show that this dynamical
motion primitive contains an implicit model for the future trajectory for
time-varying components of the system. We validate our optimal CBF in
simulation, and compare its behavior with a linear CBF.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17701" title="Abstract">arXiv:2402.17701</a> (cross-list from eess.AS) [<a href="/pdf/2402.17701" title="Download PDF">pdf</a>, <a href="/format/2402.17701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Low-latency Music Source Separation using Hybrid  Spectrogram-TasNet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Venkatesh%2C+S">Satvik Venkatesh</a>, 
<a href="/search/eess?searchtype=author&query=Benilov%2C+A">Arthur Benilov</a>, 
<a href="/search/eess?searchtype=author&query=Coleman%2C+P">Philip Coleman</a>, 
<a href="/search/eess?searchtype=author&query=Roskam%2C+F">Frederic Roskam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">There have been significant advances in deep learning for music demixing in
recent years. However, there has been little attention given to how these
neural networks can be adapted for real-time low-latency applications, which
could be helpful for hearing aids, remixing audio streams and live shows. In
this paper, we investigate the various challenges involved in adapting current
demixing models in the literature for this use case. Subsequently, inspired by
the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain
Audio Separation Network HS-TasNet, which utilises the advantages of spectral
and waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall
signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases
to 5.55 with additional training data. These results demonstrate the potential
of efficient demixing for real-time low-latency music applications.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17704" title="Abstract">arXiv:2402.17704</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.17704" title="Download PDF">pdf</a>, <a href="/format/2402.17704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer Learning Bayesian Optimization to Design Competitor DNA  Molecules for Use in Diagnostic Assays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Sedgwick%2C+R">Ruby Sedgwick</a>, 
<a href="/search/q-bio?searchtype=author&query=Goertz%2C+J+P">John P. Goertz</a>, 
<a href="/search/q-bio?searchtype=author&query=Stevens%2C+M+M">Molly M. Stevens</a>, 
<a href="/search/q-bio?searchtype=author&query=Misener%2C+R">Ruth Misener</a>, 
<a href="/search/q-bio?searchtype=author&query=van+der+Wilk%2C+M">Mark van der Wilk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">With the rise in engineered biomolecular devices, there is an increased need
for tailor-made biological sequences. Often, many similar biological sequences
need to be made for a specific application meaning numerous, sometimes
prohibitively expensive, lab experiments are necessary for their optimization.
This paper presents a transfer learning design of experiments workflow to make
this development feasible. By combining a transfer learning surrogate model
with Bayesian optimization, we show how the total number of experiments can be
reduced by sharing information between optimization tasks. We demonstrate the
reduction in the number of experiments using data from the development of DNA
competitors for use in an amplification-based diagnostic assay. We use
cross-validation to compare the predictive accuracy of different transfer
learning models, and then compare the performance of the models for both single
objective and penalized optimization tasks.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17722" title="Abstract">arXiv:2402.17722</a> (cross-list from math.OC) [<a href="/pdf/2402.17722" title="Download PDF">pdf</a>, <a href="/format/2402.17722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming Nonconvex Stochastic Mirror Descent with General Bregman  Divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fatkhullin%2C+I">Ilyas Fatkhullin</a>, 
<a href="/search/math?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the
contemporary nonconvex optimization setting. Existing results for batch-free
nonconvex SMD restrict the choice of the distance generating function (DGF) to
be differentiable with Lipschitz continuous gradients, thereby excluding
important setups such as Shannon entropy. In this work, we present a new
convergence analysis of nonconvex SMD supporting general DGF, that overcomes
the above limitations and relies solely on the standard assumptions. Moreover,
our convergence is established with respect to the Bregman Forward-Backward
envelope, which is a stronger measure than the commonly used squared norm of
gradient mapping. We further extend our results to guarantee high probability
convergence under sub-Gaussian noise and global convergence under the
generalized Bregman Proximal Polyak-{\L}ojasiewicz condition. Additionally, we
illustrate the advantages of our improved SMD theory in various nonconvex
machine learning tasks by harnessing nonsmooth DGFs. Notably, in the context of
nonconvex differentially private (DP) learning, our theory yields a simple
algorithm with a (nearly) dimension-independent utility bound. For the problem
of training linear neural networks, we develop provably convergent stochastic
algorithms.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17725" title="Abstract">arXiv:2402.17725</a> (cross-list from eess.IV) [<a href="/pdf/2402.17725" title="Download PDF">pdf</a>, <a href="/format/2402.17725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedContext: Learning Contextual Cues for Efficient Volumetric Medical  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gani%2C+H">Hanan Gani</a>, 
<a href="/search/eess?searchtype=author&query=Naseer%2C+M">Muzammal Naseer</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+F">Fahad Khan</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+S">Salman Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/hananshafi/MedContext">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Volumetric medical segmentation is a critical component of 3D medical image
analysis that delineates different semantic regions. Deep neural networks have
significantly improved volumetric medical segmentation, but they generally
require large-scale annotated data to achieve better performance, which can be
expensive and prohibitive to obtain. To address this limitation, existing works
typically perform transfer learning or design dedicated pretraining-finetuning
stages to learn representative features. However, the mismatch between the
source and target domain can make it challenging to learn optimal
representation for volumetric data, while the multi-stage training demands
higher compute as well as careful selection of stage-specific design choices.
In contrast, we propose a universal training framework called MedContext that
is architecture-agnostic and can be incorporated into any existing training
framework for 3D medical segmentation. Our approach effectively learns self
supervised contextual cues jointly with the supervised voxel segmentation task
without requiring large-scale annotated volumetric medical data or dedicated
pretraining-finetuning stages. The proposed approach induces contextual
knowledge in the network by learning to reconstruct the missing organ or parts
of an organ in the output segmentation space. The effectiveness of MedContext
is validated across multiple 3D medical datasets and four state-of-the-art
model architectures. Our approach demonstrates consistent gains in segmentation
performance across datasets and different architectures even in few-shot data
scenarios. Our code and pretrained models are available at
https://github.com/hananshafi/MedContext
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17732" title="Abstract">arXiv:2402.17732</a> (cross-list from math.ST) [<a href="/pdf/2402.17732" title="Download PDF">pdf</a>, <a href="/format/2402.17732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Batched Nonparametric Contextual Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jiang%2C+R">Rong Jiang</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+C">Cong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study nonparametric contextual bandits under batch constraints, where the
expected reward for each action is modeled as a smooth function of covariates,
and the policy updates are made at the end of each batch of observations. We
establish a minimax regret lower bound for this setting and propose Batched
Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal
regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the
covariate space into smaller bins, carefully aligning their widths with the
batch size. We also show the suboptimality of static binning under batch
constraints, highlighting the necessity of dynamic binning. Additionally, our
results suggest that a nearly constant number of policy updates can attain
optimal regret in the fully online setting.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17735" title="Abstract">arXiv:2402.17735</a> (cross-list from eess.AS) [<a href="/pdf/2402.17735" title="Download PDF">pdf</a>, <a href="/format/2402.17735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Fidelity Neural Phonetic Posteriorgrams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Churchwell%2C+C">Cameron Churchwell</a>, 
<a href="/search/eess?searchtype=author&query=Morrison%2C+M">Max Morrison</a>, 
<a href="/search/eess?searchtype=author&query=Pardo%2C+B">Bryan Pardo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024 Workshop on Explainable Machine Learning for Speech and Audio
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">A phonetic posteriorgram (PPG) is a time-varying categorical distribution
over acoustic units of speech (e.g., phonemes). PPGs are a popular
representation in speech generation due to their ability to disentangle
pronunciation features from speaker identity, allowing accurate reconstruction
of pronunciation (e.g., voice conversion) and coarse-grained pronunciation
editing (e.g., foreign accent conversion). In this paper, we demonstrably
improve the quality of PPGs to produce a state-of-the-art interpretable PPG
representation. We train an off-the-shelf speech synthesizer using our PPG
representation and show that high-quality PPGs yield independent control over
pitch and pronunciation. We further demonstrate novel uses of PPGs, such as an
acoustic pronunciation distance and fine-grained pronunciation control.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17745" title="Abstract">arXiv:2402.17745</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.17745" title="Download PDF">pdf</a>, <a href="/format/2402.17745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoDIP: Low light phase retrieval with deep image prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Manekar%2C+R">Raunak Manekar</a>, 
<a href="/search/physics?searchtype=author&query=Negrini%2C+E">Elisa Negrini</a>, 
<a href="/search/physics?searchtype=author&query=Pham%2C+M">Minh Pham</a>, 
<a href="/search/physics?searchtype=author&query=Jacobs%2C+D">Daniel Jacobs</a>, 
<a href="/search/physics?searchtype=author&query=Srivastava%2C+J">Jaideep Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Computer Vision and Pattern Recognition (cs.CV); Optics (physics.optics)

</div>
<p class="mathjax">Phase retrieval (PR) is a fundamental challenge in scientific imaging,
enabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging
at low radiation doses becomes important in applications where samples are
susceptible to radiation damage. However, most PR methods struggle in low dose
scenario due to the presence of very high shot noise. Advancements in the
optical data acquisition setup, exemplified by in-situ CDI, have shown
potential for low-dose imaging. But these depend on a time series of
measurements, rendering them unsuitable for single-image applications.
Similarly, on the computational front, data-driven phase retrieval techniques
are not readily adaptable to the single-image context. Deep learning based
single-image methods, such as deep image prior, have been effective for various
imaging tasks but have exhibited limited success when applied to PR. In this
work, we propose LoDIP which combines the in-situ CDI setup with the power of
implicit neural priors to tackle the problem of single-image low-dose phase
retrieval. Quantitative evaluations demonstrate the superior performance of
LoDIP on this task as well as applicability to real experimental scenarios.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17750" title="Abstract">arXiv:2402.17750</a> (cross-list from physics.optics) [<a href="/pdf/2402.17750" title="Download PDF">pdf</a>, <a href="/format/2402.17750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling on-chip photonic neural processors using arbitrarily  programmable wave propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Onodera%2C+T">Tatsuhiro Onodera</a>, 
<a href="/search/physics?searchtype=author&query=Stein%2C+M+M">Martin M. Stein</a>, 
<a href="/search/physics?searchtype=author&query=Ash%2C+B+A">Benjamin A. Ash</a>, 
<a href="/search/physics?searchtype=author&query=Sohoni%2C+M+M">Mandar M. Sohoni</a>, 
<a href="/search/physics?searchtype=author&query=Bosch%2C+M">Melissa Bosch</a>, 
<a href="/search/physics?searchtype=author&query=Yanagimoto%2C+R">Ryotatsu Yanagimoto</a>, 
<a href="/search/physics?searchtype=author&query=Jankowski%2C+M">Marc Jankowski</a>, 
<a href="/search/physics?searchtype=author&query=McKenna%2C+T+P">Timothy P. McKenna</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+T">Tianyu Wang</a>, 
<a href="/search/physics?searchtype=author&query=Shvets%2C+G">Gennady Shvets</a>, 
<a href="/search/physics?searchtype=author&query=Shcherbakov%2C+M+R">Maxim R. Shcherbakov</a>, 
<a href="/search/physics?searchtype=author&query=Wright%2C+L+G">Logan G. Wright</a>, 
<a href="/search/physics?searchtype=author&query=McMahon%2C+P+L">Peter L. McMahon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
<p class="mathjax">On-chip photonic processors for neural networks have potential benefits in
both speed and energy efficiency but have not yet reached the scale at which
they can outperform electronic processors. The dominant paradigm for designing
on-chip photonics is to make networks of relatively bulky discrete components
connected by one-dimensional waveguides. A far more compact alternative is to
avoid explicitly defining any components and instead sculpt the continuous
substrate of the photonic processor to directly perform the computation using
waves freely propagating in two dimensions. We propose and demonstrate a device
whose refractive index as a function of space, $n(x,z)$, can be rapidly
reprogrammed, allowing arbitrary control over the wave propagation in the
device. Our device, a 2D-programmable waveguide, combines photoconductive gain
with the electro-optic effect to achieve massively parallel modulation of the
refractive index of a slab waveguide, with an index modulation depth of
$10^{-3}$ and approximately $10^4$ programmable degrees of freedom. We used a
prototype device with a functional area of $12\,\text{mm}^2$ to perform
neural-network inference with up to 49-dimensional input vectors in a single
pass, achieving 96% accuracy on vowel classification and 86% accuracy on $7
\times 7$-pixel MNIST handwritten-digit classification. This is a scale beyond
that of previous photonic chips relying on discrete components, illustrating
the benefit of the continuous-waves paradigm. In principle, with large enough
chip area, the reprogrammability of the device's refractive index distribution
enables the reconfigurable realization of any passive, linear photonic circuit
or device. This promises the development of more compact and versatile photonic
systems for a wide range of applications, including optical processing, smart
sensing, spectroscopy, and optical communications.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.17760" title="Abstract">arXiv:2402.17760</a> (cross-list from quant-ph) [<a href="/pdf/2402.17760" title="Download PDF">pdf</a>, <a href="/format/2402.17760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Program Variational Quantum Circuits with Fast Weights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+S+Y">Samuel Yen-Chi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Quantum Machine Learning (QML) has surfaced as a pioneering framework
addressing sequential control tasks and time-series modeling. It has
demonstrated empirical quantum advantages notably within domains such as
Reinforcement Learning (RL) and time-series prediction. A significant
advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically
tailored for memory-intensive tasks encompassing partially observable
environments and non-linear time-series prediction. Nevertheless, QRNN-based
models encounter challenges, notably prolonged training duration stemming from
the necessity to compute quantum gradients using backpropagation-through-time
(BPTT). This predicament exacerbates when executing the complete model on
quantum devices, primarily due to the substantial demand for circuit evaluation
arising from the parameter-shift rule. This paper introduces the Quantum Fast
Weight Programmers (QFWP) as a solution to the temporal or sequential learning
challenge. The QFWP leverages a classical neural network (referred to as the
'slow programmer') functioning as a quantum programmer to swiftly modify the
parameters of a variational quantum circuit (termed the 'fast programmer').
Instead of completely overwriting the fast programmer at each time-step, the
slow programmer generates parameter changes or updates for the quantum circuit
parameters. This approach enables the fast programmer to incorporate past
observations or information. Notably, the proposed QFWP model achieves learning
of temporal dependencies without necessitating the use of quantum recurrent
neural networks. Numerical simulations conducted in this study showcase the
efficacy of the proposed QFWP model in both time-series prediction and RL
tasks. The model exhibits performance levels either comparable to or surpassing
those achieved by QLSTM-based models.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Wed, 28 Feb 24</h3>
<dl>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1810.13306" title="Abstract">arXiv:1810.13306</a> (replaced) [<a href="/pdf/1810.13306" title="Download PDF">pdf</a>, <a href="/format/1810.13306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Machine Learning: From Principles to Practices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhenqian Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+L">Lanning Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Q">Quanming Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a preliminary and will be kept updated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1908.05040" title="Abstract">arXiv:1908.05040</a> (replaced) [<a href="/pdf/1908.05040" title="Download PDF">pdf</a>, <a href="/format/1908.05040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Generalized Max Pooling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christlein%2C+V">Vincent Christlein</a>, 
<a href="/search/cs?searchtype=author&query=Spranger%2C+L">Lukas Spranger</a>, 
<a href="/search/cs?searchtype=author&query=Seuret%2C+M">Mathias Seuret</a>, 
<a href="/search/cs?searchtype=author&query=Nicolaou%2C+A">Anguelos Nicolaou</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%A1l%2C+P">Pavel Kr&#xe1;l</a>, 
<a href="/search/cs?searchtype=author&query=Maier%2C+A">Andreas Maier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICDAR'19 (v2: fixed Fig. 1)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2019 International Conference on Document Analysis and Recognition
  (ICDAR), Sydney, NSW, Australia, 2019, pp. 1090-1096
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.09603" title="Abstract">arXiv:2103.09603</a> (replaced) [<a href="/pdf/2103.09603" title="Download PDF">pdf</a>, <a href="/format/2103.09603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoubleML -- An Object-Oriented Implementation of Double Machine Learning  in R
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bach%2C+P">Philipp Bach</a>, 
<a href="/search/stat?searchtype=author&query=Chernozhukov%2C+V">Victor Chernozhukov</a>, 
<a href="/search/stat?searchtype=author&query=Kurz%2C+M+S">Malte S. Kurz</a>, 
<a href="/search/stat?searchtype=author&query=Spindler%2C+M">Martin Spindler</a>, 
<a href="/search/stat?searchtype=author&query=Klaassen%2C+S">Sven Klaassen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 56 pages, 8 Figures, 1 Table; Updated version for DoubleML 1.0.0
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Statistical Software 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.07324" title="Abstract">arXiv:2104.07324</a> (replaced) [<a href="/pdf/2104.07324" title="Download PDF">pdf</a>, <a href="/format/2104.07324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OneLog: Towards End-to-End Training in Software Log Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+S">Shayan Hashemi</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A4ntyl%C3%A4%2C+M">Mika M&#xe4;ntyl&#xe4;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06682" title="Abstract">arXiv:2106.06682</a> (replaced) [<a href="/pdf/2106.06682" title="Download PDF">pdf</a>, <a href="/format/2106.06682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving PDEs on Unknown Manifolds with Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+S">Senwei Liang</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+S+W">Shixiao W. Jiang</a>, 
<a href="/search/math?searchtype=author&query=Harlim%2C+J">John Harlim</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+H">Haizhao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.02473" title="Abstract">arXiv:2109.02473</a> (replaced) [<a href="/pdf/2109.02473" title="Download PDF">pdf</a>, <a href="/format/2109.02473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robust Cybersecurity Topic Classification Tool
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pelofske%2C+E">Elijah Pelofske</a>, 
<a href="/search/cs?searchtype=author&query=Liebrock%2C+L+M">Lorie M. Liebrock</a>, 
<a href="/search/cs?searchtype=author&query=Urias%2C+V">Vincent Urias</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Improved formatting
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.10275" title="Abstract">arXiv:2111.10275</a> (replaced) [<a href="/pdf/2111.10275" title="Download PDF">pdf</a>, <a href="/format/2111.10275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Composite Goodness-of-fit Tests with Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Key%2C+O">Oscar Key</a>, 
<a href="/search/stat?searchtype=author&query=Gretton%2C+A">Arthur Gretton</a>, 
<a href="/search/stat?searchtype=author&query=Briol%2C+F">Fran&#xe7;ois-Xavier Briol</a>, 
<a href="/search/stat?searchtype=author&query=Fernandez%2C+T">Tamara Fernandez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.13841" title="Abstract">arXiv:2111.13841</a> (replaced) [<a href="/pdf/2111.13841" title="Download PDF">pdf</a>, <a href="/format/2111.13841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Perturbation for Adversarial Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhaoyan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+S">Shiguang Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 18 pages, 7 figures, 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.01633" title="Abstract">arXiv:2203.01633</a> (replaced) [<a href="/pdf/2203.01633" title="Download PDF">pdf</a>, <a href="/format/2203.01633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical method for feasible and approximately optimal solutions of  multi-marginal optimal transport beyond discrete measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Neufeld%2C+A">Ariel Neufeld</a>, 
<a href="/search/math?searchtype=author&query=Xiang%2C+Q">Qikun Xiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.15862" title="Abstract">arXiv:2205.15862</a> (replaced) [<a href="/pdf/2205.15862" title="Download PDF">pdf</a>, <a href="/ps/2205.15862" title="Download PostScript">ps</a>, <a href="/format/2205.15862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Snapture -- A Novel Neural Architecture for Combined Static and Dynamic  Hand Gesture Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+H">Hassan Ali</a>, 
<a href="/search/cs?searchtype=author&query=Jirak%2C+D">Doreen Jirak</a>, 
<a href="/search/cs?searchtype=author&query=Wermter%2C+S">Stefan Wermter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Cognitive Computation(Accepted:30/06/2023, Published:17/07/2023),20 pages,20 figures,4 tables;Please find the published version/info to cite: <a href="https://doi.org/10.1007/s12559-023-10174-z">this https URL</a>;Repositories: <a href="https://zenodo.org/doi/10.5281/zenodo.10679196">this https URL</a>, <a href="https://zenodo.org/doi/10.5281/zenodo.10693816">this https URL</a>;This work was co-funded by Horizon Europe project TERAIS under Grant agreement number 101079338
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Cognitive Computation 15, 2014-2033 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.02902" title="Abstract">arXiv:2206.02902</a> (replaced) [<a href="/pdf/2206.02902" title="Download PDF">pdf</a>, <a href="/format/2206.02902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Goal-Space Planning with Subgoal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+C">Chunlok Lo</a>, 
<a href="/search/cs?searchtype=author&query=Roice%2C+K">Kevin Roice</a>, 
<a href="/search/cs?searchtype=author&query=Panahi%2C+P+M">Parham Mohammad Panahi</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+S">Scott Jordan</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+A">Adam White</a>, 
<a href="/search/cs?searchtype=author&query=Mihucz%2C+G">Gabor Mihucz</a>, 
<a href="/search/cs?searchtype=author&query=Aminmansour%2C+F">Farzane Aminmansour</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+M">Martha White</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.09107" title="Abstract">arXiv:2206.09107</a> (replaced) [<a href="/pdf/2206.09107" title="Download PDF">pdf</a>, <a href="/format/2206.09107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree-Guided Rare Feature Selection and Logic Aggregation with Electronic  Health Records Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianmin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Aseltine%2C+R+H">Robert H. Aseltine</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.01183" title="Abstract">arXiv:2209.01183</a> (replaced) [<a href="/pdf/2209.01183" title="Download PDF">pdf</a>, <a href="/format/2209.01183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Indoor Positioning in 5G-Advanced: Challenges and Solution towards  Centimeter-level Accuracy with Carrier Phase Enhancements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikonowicz%2C+J">Jakub Nikonowicz</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+A">Aamir Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Ashraf%2C+M+I">Muhammad Ikram Ashraf</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rnson%2C+E">Emil Bj&#xf6;rnson</a>, 
<a href="/search/cs?searchtype=author&query=Gidlund%2C+M">Mikael Gidlund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures, 1 table, accepted for publication in IEEE Wireless Communications Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.13938" title="Abstract">arXiv:2209.13938</a> (replaced) [<a href="/pdf/2209.13938" title="Download PDF">pdf</a>, <a href="/format/2209.13938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combinatorics of Correlated Equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Brandenburg%2C+M">Marie-Charlotte Brandenburg</a>, 
<a href="/search/math?searchtype=author&query=Hollering%2C+B">Benjamin Hollering</a>, 
<a href="/search/math?searchtype=author&query=Portakal%2C+I">Irem Portakal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computer Science and Game Theory (cs.GT); Algebraic Geometry (math.AG)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11194" title="Abstract">arXiv:2210.11194</a> (replaced) [<a href="/pdf/2210.11194" title="Download PDF">pdf</a>, <a href="/format/2210.11194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controller-Guided Partial Label Consistency Regularization with  Unlabeled Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qian-Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bowen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mingyan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zimo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shu-Tao Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15658" title="Abstract">arXiv:2210.15658</a> (replaced) [<a href="/pdf/2210.15658" title="Download PDF">pdf</a>, <a href="/format/2210.15658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All the Feels: A dexterous hand with large-area tactile sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhirangi%2C+R">Raunaq Bhirangi</a>, 
<a href="/search/cs?searchtype=author&query=DeFranco%2C+A">Abigail DeFranco</a>, 
<a href="/search/cs?searchtype=author&query=Adkins%2C+J">Jacob Adkins</a>, 
<a href="/search/cs?searchtype=author&query=Majidi%2C+C">Carmel Majidi</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhinav Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Hellebrekers%2C+T">Tess Hellebrekers</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vikash Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.17264" title="Abstract">arXiv:2210.17264</a> (replaced) [<a href="/e-print/2210.17264" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-lingual Text-To-Speech with Flow-based Voice Conversion for  Improved Pronunciation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ellinas%2C+N">Nikolaos Ellinas</a>, 
<a href="/search/cs?searchtype=author&query=Vamvoukakis%2C+G">Georgios Vamvoukakis</a>, 
<a href="/search/cs?searchtype=author&query=Markopoulos%2C+K">Konstantinos Markopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Maniati%2C+G">Georgia Maniati</a>, 
<a href="/search/cs?searchtype=author&query=Kakoulidis%2C+P">Panos Kakoulidis</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+J+S">June Sig Sung</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+I">Inchul Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Raptis%2C+S">Spyros Raptis</a>, 
<a href="/search/cs?searchtype=author&query=Chalamandaris%2C+A">Aimilios Chalamandaris</a>, 
<a href="/search/cs?searchtype=author&query=Tsiakoulis%2C+P">Pirros Tsiakoulis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fundamental changes to the model described and experimental procedure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.01723" title="Abstract">arXiv:2211.01723</a> (replaced) [<a href="/pdf/2211.01723" title="Download PDF">pdf</a>, <a href="/format/2211.01723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Checking for First-Order Logic with Disjoint Paths Predicates in  Proper Minor-Closed Graph Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golovach%2C+P+A">Petr A. Golovach</a>, 
<a href="/search/cs?searchtype=author&query=Stamoulis%2C+G">Giannos Stamoulis</a>, 
<a href="/search/cs?searchtype=author&query=Thilikos%2C+D+M">Dimitrios M. Thilikos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An extended abstract of this paper appeared in the Proceedings of the 34th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Data Structures and Algorithms (cs.DS); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.02920" title="Abstract">arXiv:2211.02920</a> (replaced) [<a href="/pdf/2211.02920" title="Download PDF">pdf</a>, <a href="/format/2211.02920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GmGM: a Fast Multi-Axis Gaussian Graphical Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Andrew%2C+B">Bailey Andrew</a>, 
<a href="/search/stat?searchtype=author&query=Westhead%2C+D">David Westhead</a>, 
<a href="/search/stat?searchtype=author&query=Cutillo%2C+L">Luisa Cutillo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages (33 additional in supplementary material), 19 figures, accepted at AIStats 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06617" title="Abstract">arXiv:2211.06617</a> (replaced) [<a href="/pdf/2211.06617" title="Download PDF">pdf</a>, <a href="/format/2211.06617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Risk Minimization with Relative Entropy Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Perlaza%2C+S+M">Samir M. Perlaza</a>, 
<a href="/search/math?searchtype=author&query=Bisson%2C+G">Gaetan Bisson</a>, 
<a href="/search/math?searchtype=author&query=Esnaola%2C+I">I&#xf1;aki Esnaola</a>, 
<a href="/search/math?searchtype=author&query=Jean-Marie%2C+A">Alain Jean-Marie</a>, 
<a href="/search/math?searchtype=author&query=Rini%2C+S">Stefano Rini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the IEEE Transactions on Information Theory: Submitted June 2023. Revised in October 2023. Accepted January 2024. Also available as: Research Report, INRIA, No. RR-9454, Centre Inria d'Universit\'e C\^ote d'Azur, Sophia Antipolis, France, Feb., 2022. Last version: Version 7
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07900" title="Abstract">arXiv:2211.07900</a> (replaced) [<a href="/pdf/2211.07900" title="Download PDF">pdf</a>, <a href="/ps/2211.07900" title="Download PostScript">ps</a>, <a href="/format/2211.07900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Inapproximability of the Minimum Distance Problem over all  Fields and the Shortest Vector Problem in all $\ell_p$ Norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bennett%2C+H">Huck Bennett</a>, 
<a href="/search/cs?searchtype=author&query=Cheraghchi%2C+M">Mahdi Cheraghchi</a>, 
<a href="/search/cs?searchtype=author&query=Guruswami%2C+V">Venkatesan Guruswami</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+J">Jo&#xe3;o Ribeiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages. Revised exposition. To appear at SICOMP. Short conference version in STOC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.01118" title="Abstract">arXiv:2212.01118</a> (replaced) [<a href="/pdf/2212.01118" title="Download PDF">pdf</a>, <a href="/format/2212.01118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The medial axis of closed bounded sets is Lipschitz stable with respect  to the Hausdorff distance under ambient diffeomorphisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kou%C5%99imsk%C3%A1%2C+H+D+P">Hana Dal Poz Kou&#x159;imsk&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Lieutier%2C+A">Andr&#xe9; Lieutier</a>, 
<a href="/search/cs?searchtype=author&query=Wintraecken%2C+M">Mathijs Wintraecken</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02723" title="Abstract">arXiv:2212.02723</a> (replaced) [<a href="/pdf/2212.02723" title="Download PDF">pdf</a>, <a href="/format/2212.02723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applying Opponent Modeling for Automatic Bidding in Online Repeated  Auctions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yudong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Congying Han</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tiande Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Hao Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07892" title="Abstract">arXiv:2212.07892</a> (replaced) [<a href="/pdf/2212.07892" title="Download PDF">pdf</a>, <a href="/format/2212.07892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Multimodal Data for Joint Generative Modeling of Complex  Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brenner%2C+M">Manuel Brenner</a>, 
<a href="/search/cs?searchtype=author&query=Hess%2C+F">Florian Hess</a>, 
<a href="/search/cs?searchtype=author&query=Koppe%2C+G">Georgia Koppe</a>, 
<a href="/search/cs?searchtype=author&query=Durstewitz%2C+D">Daniel Durstewitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A previous version was published as a workshop paper for the AAAI 2023 Workshop MLmDS under the name "Multimodal Teacher Forcing for Reconstructing Nonlinear Dynamical Systems"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS); Chaotic Dynamics (nlin.CD)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08147" title="Abstract">arXiv:2212.08147</a> (replaced) [<a href="/pdf/2212.08147" title="Download PDF">pdf</a>, <a href="/format/2212.08147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small-Signal Stability Impacts of Load and Network Dynamics on  Grid-Forming Inverters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Henriquez-Auba%2C+R">Rodrigo Henriquez-Auba</a>, 
<a href="/search/eess?searchtype=author&query=Lara%2C+J+D">Jose Daniel Lara</a>, 
<a href="/search/eess?searchtype=author&query=Callaway%2C+D+S">Duncan S. Callaway</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted on IEEE 2024 Conference on Innovative Smart Grid Technologies, North America (ISGT NA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08426" title="Abstract">arXiv:2212.08426</a> (replaced) [<a href="/pdf/2212.08426" title="Download PDF">pdf</a>, <a href="/format/2212.08426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistically consistent inverse optimal control for discrete-time  indefinite linear-quadratic systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/math?searchtype=author&query=Ringh%2C+A">Axel Ringh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages; 1 figure. Revision; in particular, somewhat larger updates in sections 2, 5 and 6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10002" title="Abstract">arXiv:2212.10002</a> (replaced) [<a href="/pdf/2212.10002" title="Download PDF">pdf</a>, <a href="/format/2212.10002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Against Disinformation Attacks in Open-Domain Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weller%2C+O">Orion Weller</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Aleem Khan</a>, 
<a href="/search/cs?searchtype=author&query=Weir%2C+N">Nathaniel Weir</a>, 
<a href="/search/cs?searchtype=author&query=Lawrie%2C+D">Dawn Lawrie</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.06727" title="Abstract">arXiv:2301.06727</a> (replaced) [<a href="/pdf/2301.06727" title="Download PDF">pdf</a>, <a href="/ps/2301.06727" title="Download PostScript">ps</a>, <a href="/format/2301.06727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Roadmap for Unconventional Computing with Nanotechnology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Finocchio%2C+G">Giovanni Finocchio</a>, 
<a href="/search/cs?searchtype=author&query=Incorvia%2C+J+A+C">Jean Anne C. Incorvia</a>, 
<a href="/search/cs?searchtype=author&query=Friedman%2C+J+S">Joseph S. Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Giordano%2C+A">Anna Giordano</a>, 
<a href="/search/cs?searchtype=author&query=Grollier%2C+J">Julie Grollier</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hyunsoo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ciubotaru%2C+F">Florin Ciubotaru</a>, 
<a href="/search/cs?searchtype=author&query=Chumak%2C+A">Andrii Chumak</a>, 
<a href="/search/cs?searchtype=author&query=Naeemi%2C+A+J">Azad J. Naeemi</a>, 
<a href="/search/cs?searchtype=author&query=Cotofana%2C+S+D">Sorin D. Cotofana</a>, 
<a href="/search/cs?searchtype=author&query=Tomasello%2C+R">Riccardo Tomasello</a>, 
<a href="/search/cs?searchtype=author&query=Panagopoulos%2C+C">Christos Panagopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Carpentieri%2C+M">Mario Carpentieri</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+P">Peng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J+J">J. Joshua Yang</a>, 
<a href="/search/cs?searchtype=author&query=Todri-Sanial%2C+A">Aida Todri-Sanial</a>, 
<a href="/search/cs?searchtype=author&query=Boschetto%2C+G">Gabriele Boschetto</a>, 
<a href="/search/cs?searchtype=author&query=Makasheva%2C+K">Kremena Makasheva</a>, 
<a href="/search/cs?searchtype=author&query=Sangwan%2C+V+K">Vinod K. Sangwan</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A+R">Amit Ranjan Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Hersam%2C+M+C">Mark C. Hersam</a>, 
<a href="/search/cs?searchtype=author&query=Camsari%2C+K+Y">Kerem Y. Camsari</a>, 
<a href="/search/cs?searchtype=author&query=McMahon%2C+P+L">Peter L. McMahon</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+S">Supriyo Datta</a>, 
<a href="/search/cs?searchtype=author&query=Koiller%2C+B">Belita Koiller</a>, 
<a href="/search/cs?searchtype=author&query=Aguilar%2C+G+H">Gabriel H. Aguilar</a>, 
<a href="/search/cs?searchtype=author&query=Tempor%C3%A3o%2C+G+P">Guilherme P. Tempor&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+D+R">Davi R. Rodrigues</a>, 
<a href="/search/cs?searchtype=author&query=Sunada%2C+S">Satoshi Sunada</a>, 
<a href="/search/cs?searchtype=author&query=Everschor-Sitte%2C+K">Karin Everschor-Sitte</a>, 
<a href="/search/cs?searchtype=author&query=Tatsumura%2C+K">Kosuke Tatsumura</a>, 
<a href="/search/cs?searchtype=author&query=Goto%2C+H">Hayato Goto</a>, 
<a href="/search/cs?searchtype=author&query=Puliafito%2C+V">Vito Puliafito</a>, 
<a href="/search/cs?searchtype=author&query=%C3%85kerman%2C+J">Johan &#xc5;kerman</a>, 
<a href="/search/cs?searchtype=author&query=Takesue%2C+H">Hiroki Takesue</a>, 
<a href="/search/cs?searchtype=author&query=Di+Ventra%2C+M">Massimiliano Di Ventra</a>, 
<a href="/search/cs?searchtype=author&query=Pershin%2C+Y+V">Yuriy V. Pershin</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+S">Saibal Mukhopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+I">I-Ting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+B+K">Brajesh Kumar Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Hasler%2C+J">Jennifer Hasler</a>,  et al. (5 additional authors not shown)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 80 pages accepted in Nano Futures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Nano Futures (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Applied Physics (physics.app-ph)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00922" title="Abstract">arXiv:2302.00922</a> (replaced) [<a href="/pdf/2302.00922" title="Download PDF">pdf</a>, <a href="/ps/2302.00922" title="Download PostScript">ps</a>, <a href="/format/2302.00922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An algebraic algorithm for the ParaTuck-2 decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Usevich%2C+K">Konstantin Usevich</a> (CRAN)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Rings and Algebras (math.RA)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01622" title="Abstract">arXiv:2302.01622</a> (replaced) [<a href="/pdf/2302.01622" title="Download PDF">pdf</a>, <a href="/format/2302.01622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private, fair and accurate: Training large-scale, privacy-preserving AI  models in medical imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Arasteh%2C+S+T">Soroosh Tayebi Arasteh</a>, 
<a href="/search/eess?searchtype=author&query=Ziller%2C+A">Alexander Ziller</a>, 
<a href="/search/eess?searchtype=author&query=Kuhl%2C+C">Christiane Kuhl</a>, 
<a href="/search/eess?searchtype=author&query=Makowski%2C+M">Marcus Makowski</a>, 
<a href="/search/eess?searchtype=author&query=Nebelung%2C+S">Sven Nebelung</a>, 
<a href="/search/eess?searchtype=author&query=Braren%2C+R">Rickmer Braren</a>, 
<a href="/search/eess?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>, 
<a href="/search/eess?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>, 
<a href="/search/eess?searchtype=author&query=Kaissis%2C+G">Georgios Kaissis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Communications Medicine. Nature Portfolio
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Commun Med 4 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02887" title="Abstract">arXiv:2302.02887</a> (replaced) [<a href="/pdf/2302.02887" title="Download PDF">pdf</a>, <a href="/format/2302.02887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UVDoc: Neural Grid-based Document Unwarping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verhoeven%2C+F">Floor Verhoeven</a>, 
<a href="/search/cs?searchtype=author&query=Magne%2C+T">Tanguy Magne</a>, 
<a href="/search/cs?searchtype=author&query=Sorkine-Hornung%2C+O">Olga Sorkine-Hornung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, published in SIGGRAPH Asia 2023 Conference Papers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05059" title="Abstract">arXiv:2302.05059</a> (replaced) [<a href="/pdf/2302.05059" title="Download PDF">pdf</a>, <a href="/format/2302.05059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effects of noise on the overparametrization of quantum neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Garc%C3%ADa-Mart%C3%ADn%2C+D">Diego Garc&#xed;a-Mart&#xed;n</a>, 
<a href="/search/quant-ph?searchtype=author&query=Larocca%2C+M">Martin Larocca</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cerezo%2C+M">M. Cerezo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 + 6 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06701" title="Abstract">arXiv:2302.06701</a> (replaced) [<a href="/pdf/2302.06701" title="Download PDF">pdf</a>, <a href="/format/2302.06701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Efficient Federated Bilevel Optimization with Local and  Global Lower Level Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Feihu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 version (Algorithm 1 is updated to be more concise)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07558" title="Abstract">arXiv:2302.07558</a> (replaced) [<a href="/pdf/2302.07558" title="Download PDF">pdf</a>, <a href="/format/2302.07558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Initialisation from lattice Boltzmann to multi-step Finite Difference  methods: modified equations and discrete observability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bellotti%2C+T">Thomas Bellotti</a> (CMAP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11919" title="Abstract">arXiv:2302.11919</a> (replaced) [<a href="/pdf/2302.11919" title="Download PDF">pdf</a>, <a href="/format/2302.11919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEM: Perception Error Model for Virtual Testing of Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piazzoni%2C+A">Andrea Piazzoni</a>, 
<a href="/search/cs?searchtype=author&query=Cherian%2C+J">Jim Cherian</a>, 
<a href="/search/cs?searchtype=author&query=Dauwels%2C+J">Justin Dauwels</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+L">Lap-Pui Chau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures. This is a preprint, and version 2 only updates the title and the reference to the final published article, which can be found at DOI: 10.1109/TITS.2023.3311633
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Intelligent Transportation Systems, vol. 25,
  no. 1, pp. 670-681, Jan. 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12822" title="Abstract">arXiv:2302.12822</a> (replaced) [<a href="/pdf/2302.12822" title="Download PDF">pdf</a>, <a href="/format/2302.12822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Prompt Augmentation and Selection with Chain-of-Thought from  Labeled Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shum%2C+K">KaShun Shum</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00402" title="Abstract">arXiv:2303.00402</a> (replaced) [<a href="/pdf/2303.00402" title="Download PDF">pdf</a>, <a href="/format/2303.00402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On discrete ground states of rotating Bose-Einstein condensates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Henning%2C+P">Patrick Henning</a>, 
<a href="/search/math?searchtype=author&query=Yadav%2C+M">Mahima Yadav</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00968" title="Abstract">arXiv:2303.00968</a> (replaced) [<a href="/pdf/2303.00968" title="Download PDF">pdf</a>, <a href="/format/2303.00968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic fairness-aware recommendation through multi-agent social choice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aird%2C+A">Amanda Aird</a>, 
<a href="/search/cs?searchtype=author&query=Farastu%2C+P">Paresha Farastu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Joshua Sun</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0tefancov%C3%A1%2C+E">Elena &#x160;tefancov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=All%2C+C">Cassidy All</a>, 
<a href="/search/cs?searchtype=author&query=Voida%2C+A">Amy Voida</a>, 
<a href="/search/cs?searchtype=author&query=Mattei%2C+N">Nicholas Mattei</a>, 
<a href="/search/cs?searchtype=author&query=Burke%2C+R">Robin Burke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01230" title="Abstract">arXiv:2303.01230</a> (replaced) [<a href="/pdf/2303.01230" title="Download PDF">pdf</a>, <a href="/format/2303.01230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic Data: Methods, Use Cases, and Risks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Cristofaro%2C+E">Emiliano De Cristofaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear in IEEE Security and Privacy Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02041" title="Abstract">arXiv:2303.02041</a> (replaced) [<a href="/pdf/2303.02041" title="Download PDF">pdf</a>, <a href="/ps/2303.02041" title="Download PostScript">ps</a>, <a href="/format/2303.02041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Likes and Fragments: Examining Perceptions of Time Spent on TikTok
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goetzen%2C+A">Angelica Goetzen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruizhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Redmiles%2C+E+M">Elissa M. Redmiles</a>, 
<a href="/search/cs?searchtype=author&query=Zannettou%2C+S">Savvas Zannettou</a>, 
<a href="/search/cs?searchtype=author&query=Ayalon%2C+O">Oshrat Ayalon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06183" title="Abstract">arXiv:2303.06183</a> (replaced) [<a href="/pdf/2303.06183" title="Download PDF">pdf</a>, <a href="/format/2303.06183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient simulation of individual-based population models: the R  Package IBMPopSim
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Giorgi%2C+D">Daphn&#xe9; Giorgi</a>, 
<a href="/search/q-bio?searchtype=author&query=Kaakai%2C+S">Sarah Kaakai</a>, 
<a href="/search/q-bio?searchtype=author&query=Lemaire%2C+V">Vincent Lemaire</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Mathematical Software (cs.MS); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08064" title="Abstract">arXiv:2303.08064</a> (replaced) [<a href="/pdf/2303.08064" title="Download PDF">pdf</a>, <a href="/format/2303.08064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Neural Path Guiding with Normalized Anisotropic Spherical  Gaussians
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiawei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Iizuka%2C+A">Akito Iizuka</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Hajime Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Komura%2C+T">Taku Komura</a>, 
<a href="/search/cs?searchtype=author&query=Kitamura%2C+Y">Yoshifumi Kitamura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09044" title="Abstract">arXiv:2303.09044</a> (replaced) [<a href="/pdf/2303.09044" title="Download PDF">pdf</a>, <a href="/format/2303.09044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLo-CAM: Class Activation Mapping for Object Co-Localization in  Weakly-Labeled Unconstrained Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belharbi%2C+S">Soufiane Belharbi</a>, 
<a href="/search/cs?searchtype=author&query=Murtaza%2C+S">Shakeeb Murtaza</a>, 
<a href="/search/cs?searchtype=author&query=Pedersoli%2C+M">Marco Pedersoli</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+I+B">Ismail Ben Ayed</a>, 
<a href="/search/cs?searchtype=author&query=McCaffrey%2C+L">Luke McCaffrey</a>, 
<a href="/search/cs?searchtype=author&query=Granger%2C+E">Eric Granger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14069" title="Abstract">arXiv:2303.14069</a> (replaced) [<a href="/pdf/2303.14069" title="Download PDF">pdf</a>, <a href="/format/2303.14069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dancing the Quantum Waltz: Compiling Three-Qubit Gates on Four Level  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Litteken%2C+A">Andrew Litteken</a> (1), 
<a href="/search/quant-ph?searchtype=author&query=Seifert%2C+L+M">Lennart Maximilian Seifert</a> (1), 
<a href="/search/quant-ph?searchtype=author&query=Chadwick%2C+J+D">Jason D. Chadwick</a> (1), 
<a href="/search/quant-ph?searchtype=author&query=Nottingham%2C+N">Natalia Nottingham</a> (1), 
<a href="/search/quant-ph?searchtype=author&query=Roy%2C+T">Tanay Roy</a> (1 and 2), 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+Z">Ziqian Li</a> (1 and 3), 
<a href="/search/quant-ph?searchtype=author&query=Schuster%2C+D">David Schuster</a> (1 and 3), 
<a href="/search/quant-ph?searchtype=author&query=Chong%2C+F+T">Frederic T. Chong</a> (1), 
<a href="/search/quant-ph?searchtype=author&query=Baker%2C+J+M">Jonathan M. Baker</a> (4) ((1) University of Chicago, (2) Fermilab, (3) Stanford University, (4) Duke University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures, to be published at ISCA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14537" title="Abstract">arXiv:2303.14537</a> (replaced) [<a href="/pdf/2303.14537" title="Download PDF">pdf</a>, <a href="/format/2303.14537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Augmentation: Self-Supervised Learning with Transformations in  Activation Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Br%C3%BCel-Gabrielsson%2C+R">Rickard Br&#xfc;el-Gabrielsson</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tongzhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Baradad%2C+M">Manel Baradad</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+J">Justin Solomon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15963" title="Abstract">arXiv:2303.15963</a> (replaced) [<a href="/pdf/2303.15963" title="Download PDF">pdf</a>, <a href="/ps/2303.15963" title="Download PostScript">ps</a>, <a href="/format/2303.15963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal and multicontrast image fusion via deep generative models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dimitri%2C+G+M">Giovanna Maria Dimitri</a>, 
<a href="/search/cs?searchtype=author&query=Spasov%2C+S">Simeon Spasov</a>, 
<a href="/search/cs?searchtype=author&query=Duggento%2C+A">Andrea Duggento</a>, 
<a href="/search/cs?searchtype=author&query=Passamonti%2C+L">Luca Passamonti</a>, 
<a href="/search/cs?searchtype=author&query=Li%60o%2C+P">Pietro Li`o</a>, 
<a href="/search/cs?searchtype=author&query=Toschi%2C+N">Nicola Toschi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16564" title="Abstract">arXiv:2303.16564</a> (replaced) [<a href="/e-print/2303.16564" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a  Bayesian Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stone%2C+R+S">Rebecca S Stone</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+N">Nishant Ravikumar</a>, 
<a href="/search/cs?searchtype=author&query=Bulpitt%2C+A+J">Andrew J Bulpitt</a>, 
<a href="/search/cs?searchtype=author&query=Hogg%2C+D+C">David C Hogg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We are revising this paper with significant changes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01899" title="Abstract">arXiv:2304.01899</a> (replaced) [<a href="/pdf/2304.01899" title="Download PDF">pdf</a>, <a href="/format/2304.01899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Class Feature Augmentation for Class Incremental Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaeyoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bohyung Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05930" title="Abstract">arXiv:2304.05930</a> (replaced) [<a href="/pdf/2304.05930" title="Download PDF">pdf</a>, <a href="/format/2304.05930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Multiscale Encoder-Decoder Transformer for Video Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karim%2C+R">Rezaul Karim</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wildes%2C+R+P">Richard P. Wildes</a>, 
<a href="/search/cs?searchtype=author&query=Siam%2C+M">Mennatullah Siam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extension of CVPR'23 paper for journal submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12046" title="Abstract">arXiv:2304.12046</a> (replaced) [<a href="/pdf/2304.12046" title="Download PDF">pdf</a>, <a href="/format/2304.12046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When to Replan? An Adaptive Replanning Strategy for Autonomous  Navigation using Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Honda%2C+K">Kohei Honda</a>, 
<a href="/search/cs?searchtype=author&query=Yonetani%2C+R">Ryo Yonetani</a>, 
<a href="/search/cs?searchtype=author&query=Nishimura%2C+M">Mai Nishimura</a>, 
<a href="/search/cs?searchtype=author&query=Kozuno%2C+T">Tadashi Kozuno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12501" title="Abstract">arXiv:2304.12501</a> (replaced) [<a href="/pdf/2304.12501" title="Download PDF">pdf</a>, <a href="/format/2304.12501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The cross-sectional stock return predictions via quantum neural network  and tensor network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+N">Nozomu Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Suimon%2C+Y">Yoshiyuki Suimon</a>, 
<a href="/search/cs?searchtype=author&query=Miyamoto%2C+K">Koichi Miyamoto</a>, 
<a href="/search/cs?searchtype=author&query=Mitarai%2C+K">Kosuke Mitarai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Quantum Mach. Intell. 5, 46 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Finance (q-fin.CP); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13689" title="Abstract">arXiv:2304.13689</a> (replaced) [<a href="/pdf/2304.13689" title="Download PDF">pdf</a>, <a href="/format/2304.13689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HeySQuAD: A Spoken Question Answering Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yijing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Rallabandi%2C+S">SaiKrishna Rallabandi</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasamurthy%2C+R">Ravisutha Srinivasamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Dakle%2C+P+P">Parag Pravin Dakle</a>, 
<a href="/search/cs?searchtype=author&query=Gon%2C+A">Alolika Gon</a>, 
<a href="/search/cs?searchtype=author&query=Raghavan%2C+P">Preethi Raghavan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02547" title="Abstract">arXiv:2305.02547</a> (replaced) [<a href="/pdf/2305.02547" title="Download PDF">pdf</a>, <a href="/format/2305.02547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PersonaLLM: Investigating the Ability of Large Language Models to  Express Personality Traits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiajie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xubo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Breazeal%2C+C">Cynthia Breazeal</a>, 
<a href="/search/cs?searchtype=author&query=Kabbara%2C+J">Jad Kabbara</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Deb Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First version uploaded at IC2S2 in May 2023. Full paper submitted in Nov. 2023 and updated Feb. 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03374" title="Abstract">arXiv:2305.03374</a> (replaced) [<a href="/pdf/2305.03374" title="Download PDF">pdf</a>, <a href="/format/2305.03374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven  Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Simin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+X">Xuguang Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuwei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenwu Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07180" title="Abstract">arXiv:2305.07180</a> (replaced) [<a href="/pdf/2305.07180" title="Download PDF">pdf</a>, <a href="/format/2305.07180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Saliency-Aware Distillation for Few-shot Fine-grained Visual  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+L+P">C. L. Philip Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xinrong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Multimedia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07614" title="Abstract">arXiv:2305.07614</a> (replaced) [<a href="/pdf/2305.07614" title="Download PDF">pdf</a>, <a href="/format/2305.07614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NevIR: Negation in Neural Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weller%2C+O">Orion Weller</a>, 
<a href="/search/cs?searchtype=author&query=Lawrie%2C+D">Dawn Lawrie</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10880" title="Abstract">arXiv:2305.10880</a> (replaced) [<a href="/e-print/2305.10880" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional sufficient dimension reduction through information  maximization with application to classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+X">Xinyu Li</a>, 
<a href="/search/stat?searchtype=author&query=Xu%2C+J">Jianjun Xu</a>, 
<a href="/search/stat?searchtype=author&query=Cui%2C+W">Wenquan Cui</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+H">Haoyang Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There are some problems with the methodology or experimental design in the article that make it impossible to produce reliable conclusions, so we have decided to withdraw it
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12077" title="Abstract">arXiv:2305.12077</a> (replaced) [<a href="/pdf/2305.12077" title="Download PDF">pdf</a>, <a href="/format/2305.12077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer in  Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+K">Kaige Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoliang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junda Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Handong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mahadik%2C+K">Kanak Mahadik</a>, 
<a href="/search/cs?searchtype=author&query=Nenkova%2C+A">Ani Nenkova</a>, 
<a href="/search/cs?searchtype=author&query=Riedl%2C+M">Mark Riedl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12599" title="Abstract">arXiv:2305.12599</a> (replaced) [<a href="/pdf/2305.12599" title="Download PDF">pdf</a>, <a href="/format/2305.12599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Abstract Meaning Representation-Based Logic-Driven Data Augmentation for  Logical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+Q">Qiming Bao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+A+Y">Alex Yuxuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhenyun Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Gendron%2C+G">Gael Gendron</a>, 
<a href="/search/cs?searchtype=author&query=Pistotti%2C+T">Timothy Pistotti</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+N">Neset Tan</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+N">Nathan Young</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yonghua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Denny%2C+P">Paul Denny</a>, 
<a href="/search/cs?searchtype=author&query=Witbrock%2C+M">Michael Witbrock</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiamou Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The short version (v2) was accepted for oral presentation at the first LLM@IJCAI 2023 non-archival symposium; the full version is under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12675" title="Abstract">arXiv:2305.12675</a> (replaced) [<a href="/pdf/2305.12675" title="Download PDF">pdf</a>, <a href="/format/2305.12675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Frustratingly Simple Decoding Method for Neural Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huayang Li</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LREC-Coling 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13252" title="Abstract">arXiv:2305.13252</a> (replaced) [<a href="/pdf/2305.13252" title="Download PDF">pdf</a>, <a href="/format/2305.13252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;According to ...&quot;: Prompting Language Models Improves Quoting from  Pre-Training Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weller%2C+O">Orion Weller</a>, 
<a href="/search/cs?searchtype=author&query=Marone%2C+M">Marc Marone</a>, 
<a href="/search/cs?searchtype=author&query=Weir%2C+N">Nathaniel Weir</a>, 
<a href="/search/cs?searchtype=author&query=Lawrie%2C+D">Dawn Lawrie</a>, 
<a href="/search/cs?searchtype=author&query=Khashabi%2C+D">Daniel Khashabi</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13300" title="Abstract">arXiv:2305.13300</a> (replaced) [<a href="/pdf/2305.13300" title="Download PDF">pdf</a>, <a href="/format/2305.13300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large  Language Models in Knowledge Conflicts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+R">Renze Lou</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15698" title="Abstract">arXiv:2305.15698</a> (replaced) [<a href="/pdf/2305.15698" title="Download PDF">pdf</a>, <a href="/format/2305.15698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Diversity in Deep Neural Network Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jihye Choi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17588" title="Abstract">arXiv:2305.17588</a> (replaced) [<a href="/pdf/2305.17588" title="Download PDF">pdf</a>, <a href="/format/2305.17588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnosing Transformers: Illuminating Feature Spaces for Clinical  Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+A+R">Aliyah R. Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Cherapanamjeri%2C+Y">Yeshwanth Cherapanamjeri</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Briton Park</a>, 
<a href="/search/cs?searchtype=author&query=Naumann%2C+T">Tristan Naumann</a>, 
<a href="/search/cs?searchtype=author&query=Odisho%2C+A+Y">Anobel Y. Odisho</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18801" title="Abstract">arXiv:2305.18801</a> (replaced) [<a href="/pdf/2305.18801" title="Download PDF">pdf</a>, <a href="/format/2305.18801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global minimization of polynomial integral functionals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fantuzzi%2C+G">Giovanni Fantuzzi</a>, 
<a href="/search/math?searchtype=author&query=Fuentes%2C+F">Federico Fuentes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Improved main results, added section on computational complexity. 27 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18874" title="Abstract">arXiv:2305.18874</a> (replaced) [<a href="/pdf/2305.18874" title="Download PDF">pdf</a>, <a href="/format/2305.18874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast evaluation of derivatives of B&#xe9;zier curves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chudy%2C+F">Filip Chudy</a>, 
<a href="/search/math?searchtype=author&query=Wo%C5%BAny%2C+P">Pawe&#x142; Wo&#x17a;ny</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19044" title="Abstract">arXiv:2305.19044</a> (replaced) [<a href="/pdf/2305.19044" title="Download PDF">pdf</a>, <a href="/format/2305.19044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Promise and Limits of Real-Time Recurrent Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Irie%2C+K">Kazuki Irie</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+A">Anand Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Schmidhuber%2C+J">J&#xfc;rgen Schmidhuber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19891" title="Abstract">arXiv:2305.19891</a> (replaced) [<a href="/pdf/2305.19891" title="Download PDF">pdf</a>, <a href="/format/2305.19891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Neighborhood Construction for Structured Large Discrete Action  Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akkerman%2C+F">Fabian Akkerman</a>, 
<a href="/search/cs?searchtype=author&query=Luy%2C+J">Julius Luy</a>, 
<a href="/search/cs?searchtype=author&query=van+Heeswijk%2C+W">Wouter van Heeswijk</a>, 
<a href="/search/cs?searchtype=author&query=Schiffer%2C+M">Maximilian Schiffer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Camera ready version. <a href="https://openreview.net/forum?id=80wh3jjCZf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00003" title="Abstract">arXiv:2306.00003</a> (replaced) [<a href="/pdf/2306.00003" title="Download PDF">pdf</a>, <a href="/format/2306.00003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Heart Disease from Multi-View Ultrasound Images via Supervised  Attention Multiple Instance Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+Z">Zhe Huang</a>, 
<a href="/search/eess?searchtype=author&query=Wessler%2C+B+S">Benjamin S. Wessler</a>, 
<a href="/search/eess?searchtype=author&query=Hughes%2C+M+C">Michael C.Hughes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Echocardiogram; multiple-instance learning; self-supervised learning; semi-supervised learning; medical imaging
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> MLHC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01195" title="Abstract">arXiv:2306.01195</a> (replaced) [<a href="/pdf/2306.01195" title="Download PDF">pdf</a>, <a href="/format/2306.01195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistency-guided Prompt Learning for Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Shuvendu Roy</a>, 
<a href="/search/cs?searchtype=author&query=Etemad%2C+A">Ali Etemad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02066" title="Abstract">arXiv:2306.02066</a> (replaced) [<a href="/pdf/2306.02066" title="Download PDF">pdf</a>, <a href="/format/2306.02066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Gaussian Process Diffusion Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Prakhar Verma</a>, 
<a href="/search/cs?searchtype=author&query=Adam%2C+V">Vincent Adam</a>, 
<a href="/search/cs?searchtype=author&query=Solin%2C+A">Arno Solin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Artificial Intelligence and Statistics (AISTATS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05812" title="Abstract">arXiv:2306.05812</a> (replaced) [<a href="/pdf/2306.05812" title="Download PDF">pdf</a>, <a href="/format/2306.05812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HRTF upsampling with a generative adversarial network using a gnomonic  equiangular projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hogg%2C+A+O+T">Aidan O. T. Hogg</a>, 
<a href="/search/eess?searchtype=author&query=Jenkins%2C+M">Mads Jenkins</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">He Liu</a>, 
<a href="/search/eess?searchtype=author&query=Squires%2C+I">Isaac Squires</a>, 
<a href="/search/eess?searchtype=author&query=Cooper%2C+S+J">Samuel J. Cooper</a>, 
<a href="/search/eess?searchtype=author&query=Picinali%2C+L">Lorenzo Picinali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures, Preprint (Accepted to IEEE/ACM Transactions on Audio, Speech, and Language Processing on the 15 Feb 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Sound (cs.SD); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08649" title="Abstract">arXiv:2306.08649</a> (replaced) [<a href="/pdf/2306.08649" title="Download PDF">pdf</a>, <a href="/format/2306.08649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delfosse%2C+Q">Quentin Delfosse</a>, 
<a href="/search/cs?searchtype=author&query=Bl%C3%BCml%2C+J">Jannis Bl&#xfc;ml</a>, 
<a href="/search/cs?searchtype=author&query=Gregori%2C+B">Bjarne Gregori</a>, 
<a href="/search/cs?searchtype=author&query=Sztwiertnia%2C+S">Sebastian Sztwiertnia</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 8 main paper pages, 36 appendix pages. In main paper: 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09783" title="Abstract">arXiv:2306.09783</a> (replaced) [<a href="/pdf/2306.09783" title="Download PDF">pdf</a>, <a href="/format/2306.09783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MementoHash: A Stateful, Minimal Memory, Best Performing Consistent Hash  Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coluzzi%2C+M">Massimo Coluzzi</a>, 
<a href="/search/cs?searchtype=author&query=Brocco%2C+A">Amos Brocco</a>, 
<a href="/search/cs?searchtype=author&query=Antonucci%2C+A">Alessandro Antonucci</a>, 
<a href="/search/cs?searchtype=author&query=Leidi%2C+T">Tiziano Leidi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11339" title="Abstract">arXiv:2306.11339</a> (replaced) [<a href="/pdf/2306.11339" title="Download PDF">pdf</a>, <a href="/format/2306.11339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masking Augmentation for Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heo%2C+B">Byeongho Heo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taekyung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongyoon Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11698" title="Abstract">arXiv:2306.11698</a> (replaced) [<a href="/pdf/2306.11698" title="Download PDF">pdf</a>, <a href="/format/2306.11698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+H">Hengzhi Pei</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chulin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Mintong Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chejian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zidi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+R">Ritik Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Schaeffer%2C+R">Rylan Schaeffer</a>, 
<a href="/search/cs?searchtype=author&query=Truong%2C+S+T">Sang T. Truong</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Simran Arora</a>, 
<a href="/search/cs?searchtype=author&query=Mazeika%2C+M">Mantas Mazeika</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zinan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Outstanding Paper (Datasets and Benchmarks Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12644" title="Abstract">arXiv:2306.12644</a> (replaced) [<a href="/pdf/2306.12644" title="Download PDF">pdf</a>, <a href="/format/2306.12644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Chance-constrained Game for Coordinating Renewable Microgrids with  Service Delivery Risk: A Bayesian Optimization Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ding%2C+Y">Yifu Ding</a>, 
<a href="/search/math?searchtype=author&query=Hobbs%2C+B">Benjamin Hobbs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14287" title="Abstract">arXiv:2306.14287</a> (replaced) [<a href="/pdf/2306.14287" title="Download PDF">pdf</a>, <a href="/format/2306.14287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Contextformer: Spatio-Channel Window Attention for Fast  Context Modeling in Learned Image Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Koyuncu%2C+A+B">A. Burakhan Koyuncu</a>, 
<a href="/search/eess?searchtype=author&query=Jia%2C+P">Panqi Jia</a>, 
<a href="/search/eess?searchtype=author&query=Boev%2C+A">Atanas Boev</a>, 
<a href="/search/eess?searchtype=author&query=Alshina%2C+E">Elena Alshina</a>, 
<a href="/search/eess?searchtype=author&query=Steinbach%2C+E">Eckehard Steinbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for IEEE TCSVT (14 pages, 10 figures, 9 tables)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15924" title="Abstract">arXiv:2306.15924</a> (replaced) [<a href="/pdf/2306.15924" title="Download PDF">pdf</a>, <a href="/ps/2306.15924" title="Download PostScript">ps</a>, <a href="/format/2306.15924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The curse of dimensionality in operator learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanthaler%2C+S">Samuel Lanthaler</a>, 
<a href="/search/cs?searchtype=author&query=Stuart%2C+A+M">Andrew M. Stuart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17492" title="Abstract">arXiv:2306.17492</a> (replaced) [<a href="/pdf/2306.17492" title="Download PDF">pdf</a>, <a href="/format/2306.17492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preference Ranking Optimization for Human Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+F">Feifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bowen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Houfeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00676" title="Abstract">arXiv:2307.00676</a> (replaced) [<a href="/pdf/2307.00676" title="Download PDF">pdf</a>, <a href="/format/2307.00676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for  Robust 3D Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jingjie Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weitong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sinclair%2C+M">Matthew Sinclair</a>, 
<a href="/search/cs?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by MICCAI BTSD-1001AI workshop. (Oral presentation).<a href="https://btsdmiccai.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00831" title="Abstract">arXiv:2307.00831</a> (replaced) [<a href="/pdf/2307.00831" title="Download PDF">pdf</a>, <a href="/format/2307.00831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Satisfiability of Local First-Order Logics with Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bollig%2C+B">Benedikt Bollig</a>, 
<a href="/search/cs?searchtype=author&query=Sangnier%2C+A">Arnaud Sangnier</a>, 
<a href="/search/cs?searchtype=author&query=Stietel%2C+O">Olivier Stietel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2209.10309">arXiv:2209.10309</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04661" title="Abstract">arXiv:2307.04661</a> (replaced) [<a href="/pdf/2307.04661" title="Download PDF">pdf</a>, <a href="/ps/2307.04661" title="Download PostScript">ps</a>, <a href="/format/2307.04661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the power of graph neural networks and the role of the activation  function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalife%2C+S">Sammy Khalife</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+A">Amitabh Basu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08202" title="Abstract">arXiv:2307.08202</a> (replaced) [<a href="/pdf/2307.08202" title="Download PDF">pdf</a>, <a href="/ps/2307.08202" title="Download PostScript">ps</a>, <a href="/format/2307.08202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Next-Generation Urban Connectivity: Is the Integrated  HAPS-Terrestrial Network a Solution?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shamsabadi%2C+A+A">Afsoon Alidadi Shamsabadi</a>, 
<a href="/search/eess?searchtype=author&query=Yadav%2C+A">Animesh Yadav</a>, 
<a href="/search/eess?searchtype=author&query=Yanikomeroglu%2C+H">Halim Yanikomeroglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08816" title="Abstract">arXiv:2307.08816</a> (replaced) [<a href="/pdf/2307.08816" title="Download PDF">pdf</a>, <a href="/format/2307.08816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Cutting-Plane Algorithms via Reinforcement Learning  Surrogates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mana%2C+K">Kyle Mana</a>, 
<a href="/search/cs?searchtype=author&query=Acero%2C+F">Fernando Acero</a>, 
<a href="/search/cs?searchtype=author&query=Mak%2C+S">Stephen Mak</a>, 
<a href="/search/cs?searchtype=author&query=Zehtabi%2C+P">Parisa Zehtabi</a>, 
<a href="/search/cs?searchtype=author&query=Cashmore%2C+M">Michael Cashmore</a>, 
<a href="/search/cs?searchtype=author&query=Magazzeni%2C+D">Daniele Magazzeni</a>, 
<a href="/search/cs?searchtype=author&query=Veloso%2C+M">Manuela Veloso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version (includes Supplementary Material). Accepted at AAAI 24 Main Track with Oral Presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10683" title="Abstract">arXiv:2307.10683</a> (replaced) [<a href="/pdf/2307.10683" title="Download PDF">pdf</a>, <a href="/format/2307.10683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fractional Denoising for 3D Molecular Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Feng%2C+S">Shikun Feng</a>, 
<a href="/search/q-bio?searchtype=author&query=Ni%2C+Y">Yuyan Ni</a>, 
<a href="/search/q-bio?searchtype=author&query=Lan%2C+Y">Yanyan Lan</a>, 
<a href="/search/q-bio?searchtype=author&query=Ma%2C+Z">Zhi-Ming Ma</a>, 
<a href="/search/q-bio?searchtype=author&query=Ma%2C+W">Wei-Ying Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12499" title="Abstract">arXiv:2307.12499</a> (replaced) [<a href="/pdf/2307.12499" title="Download PDF">pdf</a>, <a href="/format/2307.12499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xuelong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kaisheng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+B">Bin Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14723" title="Abstract">arXiv:2307.14723</a> (replaced) [<a href="/pdf/2307.14723" title="Download PDF">pdf</a>, <a href="/format/2307.14723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EFLNet: Enhancing Feature Learning for Infrared Small Target Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingliang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pi%2C+Y">Yangjun Pi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Geoscience and Remote Sensing 19 February
  2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15237" title="Abstract">arXiv:2307.15237</a> (replaced) [<a href="/pdf/2307.15237" title="Download PDF">pdf</a>, <a href="/format/2307.15237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weather Sensitive High Spatio-Temporal Resolution Transportation  Electric Load Profiles For Multiple Decarbonization Pathways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Acharya%2C+S">Samrat Acharya</a>, 
<a href="/search/eess?searchtype=author&query=Ghosal%2C+M">Malini Ghosal</a>, 
<a href="/search/eess?searchtype=author&query=Thurber%2C+T">Travis Thurber</a>, 
<a href="/search/eess?searchtype=author&query=Burleyson%2C+C+D">Casey D. Burleyson</a>, 
<a href="/search/eess?searchtype=author&query=Ou%2C+Y">Yang Ou</a>, 
<a href="/search/eess?searchtype=author&query=Campbell%2C+A">Allison Campbell</a>, 
<a href="/search/eess?searchtype=author&query=Iyer%2C+G">Gokul Iyer</a>, 
<a href="/search/eess?searchtype=author&query=Voisin%2C+N">Nathalie Voisin</a>, 
<a href="/search/eess?searchtype=author&query=Fuller%2C+J">Jason Fuller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16446" title="Abstract">arXiv:2307.16446</a> (replaced) [<a href="/pdf/2307.16446" title="Download PDF">pdf</a>, <a href="/format/2307.16446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Power Transfer between Two Antenna Arrays in the Near Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tiwari%2C+K+K">Krishan Kumar Tiwari</a>, 
<a href="/search/eess?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00177" title="Abstract">arXiv:2308.00177</a> (replaced) [<a href="/pdf/2308.00177" title="Download PDF">pdf</a>, <a href="/format/2308.00177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretrained deep models outperform GBDTs in Learning-To-Rank under label  scarcity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+C">Charlie Hou</a>, 
<a href="/search/cs?searchtype=author&query=Thekumparampil%2C+K+K">Kiran Koshy Thekumparampil</a>, 
<a href="/search/cs?searchtype=author&query=Shavlovsky%2C+M">Michael Shavlovsky</a>, 
<a href="/search/cs?searchtype=author&query=Fanti%2C+G">Giulia Fanti</a>, 
<a href="/search/cs?searchtype=author&query=Dattatreya%2C+Y">Yesh Dattatreya</a>, 
<a href="/search/cs?searchtype=author&query=Sanghavi%2C+S">Sujay Sanghavi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML-MFPL 2023 Workshop Oral
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00214" title="Abstract">arXiv:2308.00214</a> (replaced) [<a href="/pdf/2308.00214" title="Download PDF">pdf</a>, <a href="/ps/2308.00214" title="Download PostScript">ps</a>, <a href="/format/2308.00214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Loss Functions and Scene Representations for 3D/2D  Registration on Single-view Fluoroscopic X-ray Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chaochao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Faruqui%2C+S+H+A">Syed Hasib Akhter Faruqui</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Abhinav Patel</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+R+N">Ramez N. Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Hurley%2C+M+C">Michael C. Hurley</a>, 
<a href="/search/cs?searchtype=author&query=Shaibani%2C+A">Ali Shaibani</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+M+B">Matthew B. Potts</a>, 
<a href="/search/cs?searchtype=author&query=Jahromi%2C+B+S">Babak S. Jahromi</a>, 
<a href="/search/cs?searchtype=author&query=Ansari%2C+S+A">Sameer A. Ansari</a>, 
<a href="/search/cs?searchtype=author&query=Cantrell%2C+D+R">Donald R. Cantrell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00566" title="Abstract">arXiv:2308.00566</a> (replaced) [<a href="/pdf/2308.00566" title="Download PDF">pdf</a>, <a href="/format/2308.00566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic positional embeddings improve masked image modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bar%2C+A">Amir Bar</a>, 
<a href="/search/cs?searchtype=author&query=Bordes%2C+F">Florian Bordes</a>, 
<a href="/search/cs?searchtype=author&query=Shocher%2C+A">Assaf Shocher</a>, 
<a href="/search/cs?searchtype=author&query=Assran%2C+M">Mahmoud Assran</a>, 
<a href="/search/cs?searchtype=author&query=Vincent%2C+P">Pascal Vincent</a>, 
<a href="/search/cs?searchtype=author&query=Ballas%2C+N">Nicolas Ballas</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Globerson%2C+A">Amir Globerson</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and models available in <a href="https://github.com/amirbar/StoP">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03550" title="Abstract">arXiv:2308.03550</a> (replaced) [<a href="/pdf/2308.03550" title="Download PDF">pdf</a>, <a href="/format/2308.03550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feasible approximation of matching equilibria for large-scale matching  for teams problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Neufeld%2C+A">Ariel Neufeld</a>, 
<a href="/search/math?searchtype=author&query=Xiang%2C+Q">Qikun Xiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05254" title="Abstract">arXiv:2308.05254</a> (replaced) [<a href="/pdf/2308.05254" title="Download PDF">pdf</a>, <a href="/format/2308.05254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Intra-Autonomous Systems Graph Generator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dadauto%2C+C+V">Caio Vinicius Dadauto</a>, 
<a href="/search/cs?searchtype=author&query=da+Fonseca%2C+N+L+S">Nelson Luis Saldanha da Fonseca</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva+Torres%2C+R">Ricardo da Silva Torres</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06466" title="Abstract">arXiv:2308.06466</a> (replaced) [<a href="/pdf/2308.06466" title="Download PDF">pdf</a>, <a href="/ps/2308.06466" title="Download PostScript">ps</a>, <a href="/format/2308.06466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Split-State Non-Malleable Codes and Secret Sharing Schemes for Quantum  Messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boddu%2C+N+G">Naresh Goud Boddu</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+V">Vipul Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rahul Jain</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+J">Jo&#xe3;o Ribeiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12532" title="Abstract">arXiv:2308.12532</a> (replaced) [<a href="/pdf/2308.12532" title="Download PDF">pdf</a>, <a href="/format/2308.12532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedSOL: Stabilized Orthogonal Learning in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gihun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+M">Minchan Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangmook Kim</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Jaehoon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024 (CVPR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15939" title="Abstract">arXiv:2308.15939</a> (replaced) [<a href="/pdf/2308.15939" title="Download PDF">pdf</a>, <a href="/format/2308.15939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bootstrap Fine-Grained Vision-Language Alignment for Unified Zero-Shot  Anomaly Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Hanqiu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+J">Jinan Bao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingyu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01657" title="Abstract">arXiv:2309.01657</a> (replaced) [<a href="/pdf/2309.01657" title="Download PDF">pdf</a>, <a href="/format/2309.01657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Stationary Graph Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Canbolat%2C+A">Abdullah Canbolat</a>, 
<a href="/search/stat?searchtype=author&query=Vural%2C+E">Elif Vural</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05388" title="Abstract">arXiv:2309.05388</a> (replaced) [<a href="/pdf/2309.05388" title="Download PDF">pdf</a>, <a href="/format/2309.05388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Single Rotation Averaging Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+H">Seong Hun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Civera%2C+J">Javier Civera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08532" title="Abstract">arXiv:2309.08532</a> (replaced) [<a href="/pdf/2309.08532" title="Download PDF">pdf</a>, <a href="/format/2309.08532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connecting Large Language Models with Evolutionary Algorithms Yields  Powerful Prompt Optimizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qingyan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Junliang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bei Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaitao Song</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guoqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Learning Representations (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08541" title="Abstract">arXiv:2309.08541</a> (replaced) [<a href="/pdf/2309.08541" title="Download PDF">pdf</a>, <a href="/format/2309.08541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When do Generative Query and Document Expansions Fail? A Comprehensive  Study Across Methods, Retrievers, and Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weller%2C+O">Orion Weller</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+K">Kyle Lo</a>, 
<a href="/search/cs?searchtype=author&query=Wadden%2C+D">David Wadden</a>, 
<a href="/search/cs?searchtype=author&query=Lawrie%2C+D">Dawn Lawrie</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Soldaini%2C+L">Luca Soldaini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10831" title="Abstract">arXiv:2309.10831</a> (replaced) [<a href="/pdf/2309.10831" title="Download PDF">pdf</a>, <a href="/format/2309.10831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Actively Learning Reinforcement Learning: A Stochastic Optimal Control  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramadan%2C+M+S">Mohammad S. Ramadan</a>, 
<a href="/search/cs?searchtype=author&query=Hayajnh%2C+M+A">Mahmoud A. Hayajnh</a>, 
<a href="/search/cs?searchtype=author&query=Tolley%2C+M+T">Michael T. Tolley</a>, 
<a href="/search/cs?searchtype=author&query=Vamvoudakis%2C+K+G">Kyriakos G. Vamvoudakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13633" title="Abstract">arXiv:2309.13633</a> (replaced) [<a href="/pdf/2309.13633" title="Download PDF">pdf</a>, <a href="/format/2309.13633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvalLM: Interactive Evaluation of Large Language Model Prompts on  User-Defined Criteria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T+S">Tae Soo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yoonjoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jamin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Ho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14065" title="Abstract">arXiv:2309.14065</a> (replaced) [<a href="/pdf/2309.14065" title="Download PDF">pdf</a>, <a href="/format/2309.14065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile  Platform Real-Time RGB-D Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+S">Siqi Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weixi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Renzhong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shengjun Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16589" title="Abstract">arXiv:2309.16589</a> (replaced) [<a href="/pdf/2309.16589" title="Download PDF">pdf</a>, <a href="/ps/2309.16589" title="Download PostScript">ps</a>, <a href="/format/2309.16589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connecting Space Missions Through NGSO Constellations: Feasibility Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chougrani%2C+H">Houcine Chougrani</a>, 
<a href="/search/eess?searchtype=author&query=Kodheli%2C+O">Oltjon Kodheli</a>, 
<a href="/search/eess?searchtype=author&query=Georganaki%2C+A">Ali Georganaki</a>, 
<a href="/search/eess?searchtype=author&query=Thoemel%2C+J">Jan Thoemel</a>, 
<a href="/search/eess?searchtype=author&query=Turtoro%2C+C+V">Chiara Vittoria Turtoro</a>, 
<a href="/search/eess?searchtype=author&query=Zeppenfeldt%2C+F">Frank Zeppenfeldt</a>, 
<a href="/search/eess?searchtype=author&query=Pissias%2C+P">Petros Pissias</a>, 
<a href="/search/eess?searchtype=author&query=Hofmann%2C+M">Mahulena Hofmann</a>, 
<a href="/search/eess?searchtype=author&query=Chatzinotas%2C+S">Symeon Chatzinotas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16595" title="Abstract">arXiv:2309.16595</a> (replaced) [<a href="/pdf/2309.16595" title="Download PDF">pdf</a>, <a href="/format/2309.16595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Effectively Leverage Graph Structural Information through  Prompts, and Why?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingjian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaqi Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17264" title="Abstract">arXiv:2309.17264</a> (replaced) [<a href="/pdf/2309.17264" title="Download PDF">pdf</a>, <a href="/format/2309.17264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Foundation Model for General Moving Object Segmentation in Medical  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhongnuo Yan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tong Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Han Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiongquan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenlong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+D">Dong Ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 7 figures, 3 tables. This paper has been accepted by ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02174" title="Abstract">arXiv:2310.02174</a> (replaced) [<a href="/pdf/2310.02174" title="Download PDF">pdf</a>, <a href="/format/2310.02174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask Again, Then Fail: Large Language Models&#x27; Vacillations in Judgement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qiming Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zengzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Rui Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update mitigation results of fine-tuning the model on synthesized high-quality preference data with DPO algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02875" title="Abstract">arXiv:2310.02875</a> (replaced) [<a href="/pdf/2310.02875" title="Download PDF">pdf</a>, <a href="/format/2310.02875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Robot Configuration Spaces with few Convex Sets using  Clique Covers of Visibility Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Werner%2C+P">Peter Werner</a>, 
<a href="/search/cs?searchtype=author&query=Amice%2C+A">Alexandre Amice</a>, 
<a href="/search/cs?searchtype=author&query=Marcucci%2C+T">Tobia Marcucci</a>, 
<a href="/search/cs?searchtype=author&query=Rus%2C+D">Daniela Rus</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, accepted for publication at ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04311" title="Abstract">arXiv:2310.04311</a> (replaced) [<a href="/pdf/2310.04311" title="Download PDF">pdf</a>, <a href="/format/2310.04311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Deep Joint Source-Channel Coding with Decoder-Only Side  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yilmaz%2C+S+F">Selim F. Yilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Ozyilkan%2C+E">Ezgi Ozyilkan</a>, 
<a href="/search/cs?searchtype=author&query=Gunduz%2C+D">Deniz Gunduz</a>, 
<a href="/search/cs?searchtype=author&query=Erkip%2C+E">Elza Erkip</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE International Conference on Machine Learning for Communication and Networking (ICMLCN) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04671" title="Abstract">arXiv:2310.04671</a> (replaced) [<a href="/pdf/2310.04671" title="Download PDF">pdf</a>, <a href="/format/2310.04671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Abductive Reasoning Meets Driving Hazard Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charoenpitaks%2C+K">Korawat Charoenpitaks</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V">Van-Quang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Suganuma%2C+M">Masanori Suganuma</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+M">Masahiro Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Niihara%2C+R">Ryoma Niihara</a>, 
<a href="/search/cs?searchtype=author&query=Okatani%2C+T">Takayuki Okatani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main Paper: 10 pages, Supplementary Materials: 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05797" title="Abstract">arXiv:2310.05797</a> (replaced) [<a href="/pdf/2310.05797" title="Download PDF">pdf</a>, <a href="/format/2310.05797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Post Hoc Explainers?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kroeger%2C+N">Nicholas Kroeger</a>, 
<a href="/search/cs?searchtype=author&query=Ley%2C+D">Dan Ley</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+S">Satyapriya Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+C">Chirag Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06823" title="Abstract">arXiv:2310.06823</a> (replaced) [<a href="/pdf/2310.06823" title="Download PDF">pdf</a>, <a href="/format/2310.06823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NECO: NEural Collapse Based Out-of-distribution detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ammar%2C+M+B">Mou&#xef;n Ben Ammar</a>, 
<a href="/search/stat?searchtype=author&query=Belkhir%2C+N">Nacim Belkhir</a>, 
<a href="/search/stat?searchtype=author&query=Popescu%2C+S">Sebastian Popescu</a>, 
<a href="/search/stat?searchtype=author&query=Manzanera%2C+A">Antoine Manzanera</a>, 
<a href="/search/stat?searchtype=author&query=Franchi%2C+G">Gianni Franchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06958" title="Abstract">arXiv:2310.06958</a> (replaced) [<a href="/pdf/2310.06958" title="Download PDF">pdf</a>, <a href="/format/2310.06958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing the Robustness of Modern No-Reference Image- and Video-Quality  Metrics to Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Antsiferova%2C+A">Anastasia Antsiferova</a>, 
<a href="/search/cs?searchtype=author&query=Abud%2C+K">Khaled Abud</a>, 
<a href="/search/cs?searchtype=author&query=Gushchin%2C+A">Aleksandr Gushchin</a>, 
<a href="/search/cs?searchtype=author&query=Shumitskaya%2C+E">Ekaterina Shumitskaya</a>, 
<a href="/search/cs?searchtype=author&query=Lavrushkin%2C+S">Sergey Lavrushkin</a>, 
<a href="/search/cs?searchtype=author&query=Vatolin%2C+D">Dmitriy Vatolin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Multimedia (cs.MM); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08586" title="Abstract">arXiv:2310.08586</a> (replaced) [<a href="/pdf/2310.08586" title="Download PDF">pdf</a>, <a href="/format/2310.08586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PonderV2: Pave the Way for 3D Foundation Model with A Universal  Pre-training Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haoyi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Honghui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Di Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sha Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xianglong He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tong He</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2301.00157">arXiv:2301.00157</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09650" title="Abstract">arXiv:2310.09650</a> (replaced) [<a href="/pdf/2310.09650" title="Download PDF">pdf</a>, <a href="/ps/2310.09650" title="Download PostScript">ps</a>, <a href="/format/2310.09650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Federated Learning in Healthcare: a Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thrasher%2C+J">Jacob Thrasher</a>, 
<a href="/search/cs?searchtype=author&query=Devkota%2C+A">Alina Devkota</a>, 
<a href="/search/cs?searchtype=author&query=Siwakotai%2C+P">Prasiddha Siwakotai</a>, 
<a href="/search/cs?searchtype=author&query=Chivukula%2C+R">Rohit Chivukula</a>, 
<a href="/search/cs?searchtype=author&query=Poudel%2C+P">Pranav Poudel</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chaunbo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+B">Binod Bhattarai</a>, 
<a href="/search/cs?searchtype=author&query=Gyawali%2C+P">Prashnna Gyawali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10556" title="Abstract">arXiv:2310.10556</a> (replaced) [<a href="/pdf/2310.10556" title="Download PDF">pdf</a>, <a href="/format/2310.10556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample Complexity of Preference-Based Nonparametric Off-Policy  Evaluation with Deep Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minshuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11714" title="Abstract">arXiv:2310.11714</a> (replaced) [<a href="/pdf/2310.11714" title="Download PDF">pdf</a>, <a href="/format/2310.11714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Evaluation of Generative Models in Distributed Learning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Farnia%2C+F">Farzan Farnia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhenghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yunheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12613" title="Abstract">arXiv:2310.12613</a> (replaced) [<a href="/pdf/2310.12613" title="Download PDF">pdf</a>, <a href="/ps/2310.12613" title="Download PostScript">ps</a>, <a href="/format/2310.12613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Normalization of Linear Temporal Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esparza%2C+J">Javier Esparza</a>, 
<a href="/search/cs?searchtype=author&query=Rubio%2C+R">Rub&#xe9;n Rubio</a>, 
<a href="/search/cs?searchtype=author&query=Sickert%2C+S">Salomon Sickert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in J. ACM. arXiv admin note: text overlap with <a href="/abs/2304.08872">arXiv:2304.08872</a>, <a href="/abs/2005.00472">arXiv:2005.00472</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14341" title="Abstract">arXiv:2310.14341</a> (replaced) [<a href="/pdf/2310.14341" title="Download PDF">pdf</a>, <a href="/format/2310.14341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">YeXin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages,3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14901" title="Abstract">arXiv:2310.14901</a> (replaced) [<a href="/pdf/2310.14901" title="Download PDF">pdf</a>, <a href="/format/2310.14901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Series of Hessian-Vector Products for Tractable Saddle-Free Newton  Optimisation of Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oldewage%2C+E+T">Elre T. Oldewage</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+R+M">Ross M. Clarke</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 10 figures, 5 tables. To appear in TMLR. First two authors' order randomised
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15310" title="Abstract">arXiv:2310.15310</a> (replaced) [<a href="/pdf/2310.15310" title="Download PDF">pdf</a>, <a href="/format/2310.15310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A direct solution to the interpolative inverse non-uniform fast Fourier  transform problem for spectral analyses of non-equidistant time-series data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Armstrong%2C+M+S">Michael Sorochan Armstrong</a>, 
<a href="/search/math?searchtype=author&query=P%C3%A9rez-Gir%C3%B3n%2C+J+C">Jos&#xe9; Carlos P&#xe9;rez-Gir&#xf3;n</a>, 
<a href="/search/math?searchtype=author&query=Camacho%2C+J">Jos&#xe9; Camacho</a>, 
<a href="/search/math?searchtype=author&query=Zamora%2C+R">Regino Zamora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18338" title="Abstract">arXiv:2310.18338</a> (replaced) [<a href="/pdf/2310.18338" title="Download PDF">pdf</a>, <a href="/format/2310.18338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small Language Models Fine-tuned to Coordinate Larger Language Models  improve Complex Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Juneja%2C+G">Gurusha Juneja</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Subhabrata Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarti%2C+S">Soumen Chakrabarti</a>, 
<a href="/search/cs?searchtype=author&query=Manchanda%2C+S">Sunny Manchanda</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Typos corrected)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19608" title="Abstract">arXiv:2310.19608</a> (replaced) [<a href="/pdf/2310.19608" title="Download PDF">pdf</a>, <a href="/format/2310.19608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Feynman--Kac training of partial Bayesian neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mair%2C+S">Sebastian Mair</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6n%2C+T+B">Thomas B. Sch&#xf6;n</a>, 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6lund%2C+J">Jens Sj&#xf6;lund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19683" title="Abstract">arXiv:2310.19683</a> (replaced) [<a href="/pdf/2310.19683" title="Download PDF">pdf</a>, <a href="/format/2310.19683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Online Bootstrap for Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Palm%2C+N">Nicolai Palm</a>, 
<a href="/search/stat?searchtype=author&query=Nagler%2C+T">Thomas Nagler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19944" title="Abstract">arXiv:2310.19944</a> (replaced) [<a href="/pdf/2310.19944" title="Download PDF">pdf</a>, <a href="/format/2310.19944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Unscented Autoencoders for Trajectory Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Janjo%C5%A1%2C+F">Faris Janjo&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Hallgarten%2C+M">Marcel Hallgarten</a>, 
<a href="/search/cs?searchtype=author&query=Knittel%2C+A">Anthony Knittel</a>, 
<a href="/search/cs?searchtype=author&query=Dolgov%2C+M">Maxim Dolgov</a>, 
<a href="/search/cs?searchtype=author&query=Zell%2C+A">Andreas Zell</a>, 
<a href="/search/cs?searchtype=author&query=Z%C3%B6llner%2C+J+M">J. Marius Z&#xf6;llner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20426" title="Abstract">arXiv:2310.20426</a> (replaced) [<a href="/pdf/2310.20426" title="Download PDF">pdf</a>, <a href="/format/2310.20426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Pareto Set Learning with Structure Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02189" title="Abstract">arXiv:2311.02189</a> (replaced) [<a href="/pdf/2311.02189" title="Download PDF">pdf</a>, <a href="/format/2311.02189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness  Learning Using Segment Anything Model with Fair Error-Bound Scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Min Shi</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Kouhana%2C+A">Ava Kouhana</a>, 
<a href="/search/cs?searchtype=author&query=Elze%2C+T">Tobias Elze</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengyu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024; Codes available at <a href="https://github.com/Harvard-Ophthalmology-AI-Lab/FairSeg">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03004" title="Abstract">arXiv:2311.03004</a> (replaced) [<a href="/pdf/2311.03004" title="Download PDF">pdf</a>, <a href="/format/2311.03004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Degrees-of-Freedom Limit of Holographic MIMO  Communications: A 3-D Antenna Array Topology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S+S+A">Shuai S. A. Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongjing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tengjiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Da Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chongwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Sheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shilie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianmin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+E">Er-Ping Li</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+W+E+I">Wei E. I. Sha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Applied Physics (physics.app-ph)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05227" title="Abstract">arXiv:2311.05227</a> (replaced) [<a href="/pdf/2311.05227" title="Download PDF">pdf</a>, <a href="/ps/2311.05227" title="Download PostScript">ps</a>, <a href="/format/2311.05227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kantian Deontology Meets AI Alignment: Towards Morally Grounded Fairness  Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mougan%2C+C">Carlos Mougan</a>, 
<a href="/search/cs?searchtype=author&query=Brand%2C+J">Joshua Brand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06144" title="Abstract">arXiv:2311.06144</a> (replaced) [<a href="/pdf/2311.06144" title="Download PDF">pdf</a>, <a href="/format/2311.06144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Reinforcement Learning for the Low-Level Control of a  Quadrotor UAV
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Beomyeol Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T">Taeyoung Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06310" title="Abstract">arXiv:2311.06310</a> (replaced) [<a href="/pdf/2311.06310" title="Download PDF">pdf</a>, <a href="/format/2311.06310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Labor Space: A Unifying Representation of the Labor Market via Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kim%2C+S">Seongwoon Kim</a>, 
<a href="/search/physics?searchtype=author&query=Ahn%2C+Y">Yong-Yeol Ahn</a>, 
<a href="/search/physics?searchtype=author&query=Park%2C+J">Jaehyuk Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07499" title="Abstract">arXiv:2311.07499</a> (replaced) [<a href="/pdf/2311.07499" title="Download PDF">pdf</a>, <a href="/format/2311.07499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Sim-to-Real Gap with Dynamic Compliance Tuning for  Industrial Insertion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICRA 24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07511" title="Abstract">arXiv:2311.07511</a> (replaced) [<a href="/pdf/2311.07511" title="Download PDF">pdf</a>, <a href="/ps/2311.07511" title="Download PostScript">ps</a>, <a href="/format/2311.07511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty estimation in satellite precipitation interpolation with  machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Papacharalampous%2C+G">Georgia Papacharalampous</a>, 
<a href="/search/stat?searchtype=author&query=Tyralis%2C+H">Hristos Tyralis</a>, 
<a href="/search/stat?searchtype=author&query=Doulamis%2C+N">Nikolaos Doulamis</a>, 
<a href="/search/stat?searchtype=author&query=Doulamis%2C+A">Anastasios Doulamis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph); Applications (stat.AP); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07634" title="Abstract">arXiv:2311.07634</a> (replaced) [<a href="/pdf/2311.07634" title="Download PDF">pdf</a>, <a href="/format/2311.07634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ActiveDC: Distribution Calibration for Active Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenshuai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhenghui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+J">Jinzhou Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CVPR 2024 Accept
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08600" title="Abstract">arXiv:2311.08600</a> (replaced) [<a href="/pdf/2311.08600" title="Download PDF">pdf</a>, <a href="/format/2311.08600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Derivation of sixth-order exponential Runge--Kutta methods for stiff  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luan%2C+V+T">Vu Thai Luan</a>, 
<a href="/search/math?searchtype=author&query=Alhsmy%2C+T">Trky Alhsmy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Applied Mathematics Letters
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Mathematics Letters (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08640" title="Abstract">arXiv:2311.08640</a> (replaced) [<a href="/pdf/2311.08640" title="Download PDF">pdf</a>, <a href="/format/2311.08640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multistage Collaborative Knowledge Distillation from a Large Language  Model for Semi-Supervised Sequence Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiachen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenlong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Drozdov%2C+A">Andrew Drozdov</a>, 
<a href="/search/cs?searchtype=author&query=Rozonoyer%2C+B">Benjamin Rozonoyer</a>, 
<a href="/search/cs?searchtype=author&query=Sultan%2C+M+A">Md Arafat Sultan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jay-Yoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Iyyer%2C+M">Mohit Iyyer</a>, 
<a href="/search/cs?searchtype=author&query=McCallum%2C+A">Andrew McCallum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08850" title="Abstract">arXiv:2311.08850</a> (replaced) [<a href="/pdf/2311.08850" title="Download PDF">pdf</a>, <a href="/format/2311.08850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling the Output of a Generative Model by Latent Feature Vector  Shifting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belanec%2C+R">R&#xf3;bert Belanec</a>, 
<a href="/search/cs?searchtype=author&query=Lacko%2C+P">Peter Lacko</a>, 
<a href="/search/cs?searchtype=author&query=Malinovsk%C3%A1%2C+K">Krist&#xed;na Malinovsk&#xe1;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, presented on DISA2023 conference in Ko\v{s}ice, Source code: <a href="https://doi.org/10.5281/zenodo.10708459">this https URL</a>, Accepted: 7.8.2023. Published: 15.11.2023. This work was funded by the Horizon-Widera-2021 European Twinning project TERAIS G.A. n. 101079338
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 World Symposium on Digital Intelligence for Systems and
  Machines (DISA), Ko\v{s}ice, Slovakia, 2023, pp. 24-30
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08894" title="Abstract">arXiv:2311.08894</a> (replaced) [<a href="/pdf/2311.08894" title="Download PDF">pdf</a>, <a href="/format/2311.08894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing  Supervised Models with In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patidar%2C+M">Mayur Patidar</a>, 
<a href="/search/cs?searchtype=author&query=Sawhney%2C+R">Riya Sawhney</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Avinash Singh</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+B">Biswajit Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+I">Indrajit Bhattacharya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09675" title="Abstract">arXiv:2311.09675</a> (replaced) [<a href="/pdf/2311.09675" title="Download PDF">pdf</a>, <a href="/format/2311.09675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where Do People Tell Stories Online? Story Detection Across Online  Communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Antoniak%2C+M">Maria Antoniak</a>, 
<a href="/search/cs?searchtype=author&query=Mire%2C+J">Joel Mire</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>, 
<a href="/search/cs?searchtype=author&query=Ash%2C+E">Elliott Ash</a>, 
<a href="/search/cs?searchtype=author&query=Piper%2C+A">Andrew Piper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10801" title="Abstract">arXiv:2311.10801</a> (replaced) [<a href="/pdf/2311.10801" title="Download PDF">pdf</a>, <a href="/format/2311.10801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning with Maskable Stock Representation for Portfolio  Management in Customizable Stock Pools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Zhang%2C+W">Wentao Zhang</a>, 
<a href="/search/q-fin?searchtype=author&query=Zhao%2C+Y">Yilei Zhao</a>, 
<a href="/search/q-fin?searchtype=author&query=Sun%2C+S">Shuo Sun</a>, 
<a href="/search/q-fin?searchtype=author&query=Ying%2C+J">Jie Ying</a>, 
<a href="/search/q-fin?searchtype=author&query=Xie%2C+Y">Yonggang Xie</a>, 
<a href="/search/q-fin?searchtype=author&query=Song%2C+Z">Zitao Song</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+X">Xinrun Wang</a>, 
<a href="/search/q-fin?searchtype=author&query=An%2C+B">Bo An</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Portfolio Management (q-fin.PM)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14402" title="Abstract">arXiv:2311.14402</a> (replaced) [<a href="/pdf/2311.14402" title="Download PDF">pdf</a>, <a href="/format/2311.14402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TEA: Test-time Energy Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yige Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bingbing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Liang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR 2024). Code is available at <a href="https://github.com/yuanyige/tea">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15328" title="Abstract">arXiv:2311.15328</a> (replaced) [<a href="/pdf/2311.15328" title="Download PDF">pdf</a>, <a href="/format/2311.15328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BS-Diff: Effective Bone Suppression Using Conditional Diffusion Models  from Chest X-Ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhanghao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+Y">Yifei Sun</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+W">Wenjian Qin</a>, 
<a href="/search/eess?searchtype=author&query=Ge%2C+R">Ruiquan Ge</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+C">Cheng Pan</a>, 
<a href="/search/eess?searchtype=author&query=Deng%2C+W">Wenming Deng</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhou Liu</a>, 
<a href="/search/eess?searchtype=author&query=Min%2C+W">Wenwen Min</a>, 
<a href="/search/eess?searchtype=author&query=Elazab%2C+A">Ahmed Elazab</a>, 
<a href="/search/eess?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Changmiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, accepted by ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15537" title="Abstract">arXiv:2311.15537</a> (replaced) [<a href="/pdf/2311.15537" title="Download PDF">pdf</a>, <a href="/format/2311.15537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+B">Bin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiale Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yanwei Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17456" title="Abstract">arXiv:2311.17456</a> (replaced) [<a href="/pdf/2311.17456" title="Download PDF">pdf</a>, <a href="/format/2311.17456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with  Iterative Diffusion-Based Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiuming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guangming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Weicai Ye</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chaokang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jinru Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+D">Dalong Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hesheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2024. Codes will be released on <a href="https://github.com/IRMVLab/DifFlow3D">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17463" title="Abstract">arXiv:2311.17463</a> (replaced) [<a href="/pdf/2311.17463" title="Download PDF">pdf</a>, <a href="/format/2311.17463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructing Optimal $L_{\infty}$ Star Discrepancy Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cl%C3%A9ment%2C+F">Fran&#xe7;ois Cl&#xe9;ment</a>, 
<a href="/search/cs?searchtype=author&query=Doerr%2C+C">Carola Doerr</a>, 
<a href="/search/cs?searchtype=author&query=Klamroth%2C+K">Kathrin Klamroth</a>, 
<a href="/search/cs?searchtype=author&query=Paquete%2C+L">Lu&#xed;s Paquete</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated old version with improved plots and a correction on general position
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18460" title="Abstract">arXiv:2311.18460</a> (replaced) [<a href="/pdf/2311.18460" title="Download PDF">pdf</a>, <a href="/format/2311.18460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Fairness under Unobserved Confounding: A Neural Sensitivity  Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schr%C3%B6der%2C+M">Maresa Schr&#xf6;der</a>, 
<a href="/search/cs?searchtype=author&query=Frauen%2C+D">Dennis Frauen</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00058" title="Abstract">arXiv:2312.00058</a> (replaced) [<a href="/pdf/2312.00058" title="Download PDF">pdf</a>, <a href="/ps/2312.00058" title="Download PostScript">ps</a>, <a href="/format/2312.00058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform estimates for a fully discrete scheme integrating the linear  heat equation on a bounded interval with pure Neumann boundary conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dujardin%2C+G">Guillaume Dujardin</a> (Paradyse), 
<a href="/search/math?searchtype=author&query=Lafitte%2C+P">Pauline Lafitte</a> (MICS, FR3487)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01655" title="Abstract">arXiv:2312.01655</a> (replaced) [<a href="/pdf/2312.01655" title="Download PDF">pdf</a>, <a href="/format/2312.01655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Polar Metric Learning: Efficient Classically Learned Quantum  Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Sharma%2C+V">Vinayak Sharma</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shrivastava%2C+A">Aviral Shrivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02209" title="Abstract">arXiv:2312.02209</a> (replaced) [<a href="/pdf/2312.02209" title="Download PDF">pdf</a>, <a href="/format/2312.02209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute  Decomposition and Indexing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaosheng He</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhongang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Si Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+G">Guosheng Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by CVPR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05814" title="Abstract">arXiv:2312.05814</a> (replaced) [<a href="/pdf/2312.05814" title="Download PDF">pdf</a>, <a href="/format/2312.05814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Speech Embeddings for Speech Synthesis Based on Deep Generative  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seo-Hyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Young-Eun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Soowon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+B">Byung-Kwan Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jun-Young Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05849" title="Abstract">arXiv:2312.05849</a> (replaced) [<a href="/pdf/2312.05849" title="Download PDF">pdf</a>, <a href="/format/2312.05849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoe%2C+J+T">Jiun Tian Hoe</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xudong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+C+S">Chee Seng Chan</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y">Yap-Peng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Weipeng Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://jiuntian.github.io/interactdiffusion.">this https URL</a> Accepted at CVPR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06185" title="Abstract">arXiv:2312.06185</a> (replaced) [<a href="/pdf/2312.06185" title="Download PDF">pdf</a>, <a href="/format/2312.06185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KnowGPT: Knowledge Injection for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Junnan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+D">Daochen Zha</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zailiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06454" title="Abstract">arXiv:2312.06454</a> (replaced) [<a href="/pdf/2312.06454" title="Download PDF">pdf</a>, <a href="/format/2312.06454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point Transformer with Federated Learning for Predicting Breast Cancer  HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+B">Bao Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhenyu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Shao%2C+L">Lizhi Shao</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+B">Bensheng Qiu</a>, 
<a href="/search/eess?searchtype=author&query=Bu%2C+H">Hong Bu</a>, 
<a href="/search/eess?searchtype=author&query=Tian%2C+J">Jie Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07783" title="Abstract">arXiv:2312.07783</a> (replaced) [<a href="/pdf/2312.07783" title="Download PDF">pdf</a>, <a href="/format/2312.07783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BarraCUDA: GPUs do Leak DNN Weights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Horvath%2C+P">Peter Horvath</a>, 
<a href="/search/cs?searchtype=author&query=Chmielewski%2C+L">Lukasz Chmielewski</a>, 
<a href="/search/cs?searchtype=author&query=Weissbart%2C+L">Leo Weissbart</a>, 
<a href="/search/cs?searchtype=author&query=Batina%2C+L">Lejla Batina</a>, 
<a href="/search/cs?searchtype=author&query=Yarom%2C+Y">Yuval Yarom</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07920" title="Abstract">arXiv:2312.07920</a> (replaced) [<a href="/pdf/2312.07920" title="Download PDF">pdf</a>, <a href="/format/2312.07920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic  Autonomous Driving Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaoyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhiwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+X">Xiaojun Shan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongtao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Deqing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08616" title="Abstract">arXiv:2312.08616</a> (replaced) [<a href="/pdf/2312.08616" title="Download PDF">pdf</a>, <a href="/format/2312.08616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generalized Neural Diffusion Framework on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yibo Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongrui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11462" title="Abstract">arXiv:2312.11462</a> (replaced) [<a href="/pdf/2312.11462" title="Download PDF">pdf</a>, <a href="/format/2312.11462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascade Speculative Drafting for Even Faster LLM Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaocong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiacheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chenkai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11468" title="Abstract">arXiv:2312.11468</a> (replaced) [<a href="/pdf/2312.11468" title="Download PDF">pdf</a>, <a href="/format/2312.11468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bias-Reduced Neural Networks for Parameter Estimation in Quantitative  MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Mao%2C+A">Andrew Mao</a>, 
<a href="/search/physics?searchtype=author&query=Flassbeck%2C+S">Sebastian Flassbeck</a>, 
<a href="/search/physics?searchtype=author&query=Assl%C3%A4nder%2C+J">Jakob Assl&#xe4;nder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12131" title="Abstract">arXiv:2312.12131</a> (replaced) [<a href="/pdf/2312.12131" title="Download PDF">pdf</a>, <a href="/format/2312.12131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elliptic Curve Pairing Stealth Address Protocols
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mikic%2C+M">Marija Mikic</a>, 
<a href="/search/cs?searchtype=author&query=Srbakoski%2C+M">Mihajlo Srbakoski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12470" title="Abstract">arXiv:2312.12470</a> (replaced) [<a href="/pdf/2312.12470" title="Download PDF">pdf</a>, <a href="/format/2312.12470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rotated Multi-Scale Interaction Network for Referring Remote Sensing  Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yiwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiayi Ji</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoshuai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CVPR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14713" title="Abstract">arXiv:2312.14713</a> (replaced) [<a href="/pdf/2312.14713" title="Download PDF">pdf</a>, <a href="/format/2312.14713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Transfer Multiobjective Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+Y">Yew-Soon Ong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00499" title="Abstract">arXiv:2401.00499</a> (replaced) [<a href="/pdf/2401.00499" title="Download PDF">pdf</a>, <a href="/ps/2401.00499" title="Download PostScript">ps</a>, <a href="/format/2401.00499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating High-Precision Force Fields for Molecular Dynamics  Simulations to Study Chemical Reaction Mechanisms using Molecular  Configuration Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Yuan%2C+S">Sihao Yuan</a>, 
<a href="/search/physics?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Xie%2C+Z">Zhaoxin Xie</a>, 
<a href="/search/physics?searchtype=author&query=Fan%2C+C">Cheng Fan</a>, 
<a href="/search/physics?searchtype=author&query=Xiao%2C+Y">Yunlong Xiao</a>, 
<a href="/search/physics?searchtype=author&query=Gao%2C+Y+Q">Yi Qin Gao</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+Y+I">Yi Issac Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Soft Condensed Matter (cond-mat.soft); Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03797" title="Abstract">arXiv:2401.03797</a> (replaced) [<a href="/pdf/2401.03797" title="Download PDF">pdf</a>, <a href="/ps/2401.03797" title="Download PostScript">ps</a>, <a href="/format/2401.03797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anatomy of Neural Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saleh%2C+M">Majd Saleh</a>, 
<a href="/search/cs?searchtype=author&query=Paquelet%2C+S">St&#xe9;phane Paquelet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 Pages; 25 Figures; some typos and notation errors are corrected in this version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05857" title="Abstract">arXiv:2401.05857</a> (replaced) [<a href="/e-print/2401.05857" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure Dynamic Event-triggered Consensus Under Asynchronous Denial of  Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Azarbahram%2C+A">Ali Azarbahram</a>, 
<a href="/search/eess?searchtype=author&query=Amini%2C+A">Amir Amini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work needs to be revised fundamentally with a greater emphasis on the nonlinear dynamics and the destructive effects of independent DoS attacks over the communication links between actual and auxiliary states
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07644" title="Abstract">arXiv:2401.07644</a> (replaced) [<a href="/pdf/2401.07644" title="Download PDF">pdf</a>, <a href="/format/2401.07644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource Allocation in STAR-RIS-Aided SWIPT with RSMA via Meta-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amiri%2C+M">Mojtaba Amiri</a>, 
<a href="/search/cs?searchtype=author&query=Vaezpour%2C+E">Elaheh Vaezpour</a>, 
<a href="/search/cs?searchtype=author&query=Javadi%2C+S">Sepideh Javadi</a>, 
<a href="/search/cs?searchtype=author&query=Mili%2C+M+R">Mohammad Robat Mili</a>, 
<a href="/search/cs?searchtype=author&query=Yanikomeroglu%2C+H">Halim Yanikomeroglu</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08610" title="Abstract">arXiv:2401.08610</a> (replaced) [<a href="/pdf/2401.08610" title="Download PDF">pdf</a>, <a href="/format/2401.08610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities  and Risks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Xiong%2C+X">Xihan Xiong</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+Z">Zhipeng Wang</a>, 
<a href="/search/q-fin?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/q-fin?searchtype=author&query=Knottenbelt%2C+W">William Knottenbelt</a>, 
<a href="/search/q-fin?searchtype=author&query=Huth%2C+M">Michael Huth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Finance (q-fin.GN)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08860" title="Abstract">arXiv:2401.08860</a> (replaced) [<a href="/pdf/2401.08860" title="Download PDF">pdf</a>, <a href="/format/2401.08860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Level Multi-Instance Distillation for Self-Supervised Fine-Grained  Visual Categorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Q">Qi Bi</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+W">Wei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jingjun Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Haolan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gui-Song Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10731" title="Abstract">arXiv:2401.10731</a> (replaced) [<a href="/pdf/2401.10731" title="Download PDF">pdf</a>, <a href="/format/2401.10731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Removal and Selection: Improving RGB-Infrared Object Detection via  Coarse-to-Fine Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tianyi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Maoxun Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xingxing Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9pages, 7figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10773" title="Abstract">arXiv:2401.10773</a> (replaced) [<a href="/pdf/2401.10773" title="Download PDF">pdf</a>, <a href="/format/2401.10773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel lattice codes from Hurwitz quaternion integers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Souza%2C+J+G+F">Juliana G. F. Souza</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+S+I+R">Sueli I. R. Costa</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+C">Cong Ling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10787" title="Abstract">arXiv:2401.10787</a> (replaced) [<a href="/pdf/2401.10787" title="Download PDF">pdf</a>, <a href="/ps/2401.10787" title="Download PostScript">ps</a>, <a href="/format/2401.10787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Online Certificate Status Protocol with Certificate Revocation  List for Smart Grid Public Key Infrastructure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hong-Sheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhe-Yi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hsuan-Tung Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hung-Min Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11033" title="Abstract">arXiv:2401.11033</a> (replaced) [<a href="/pdf/2401.11033" title="Download PDF">pdf</a>, <a href="/format/2401.11033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for  Large Language Models&#x27; Training?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raza%2C+S">Shaina Raza</a>, 
<a href="/search/cs?searchtype=author&query=Ghuge%2C+S">Shardul Ghuge</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Chen Ding</a>, 
<a href="/search/cs?searchtype=author&query=Dolatabadi%2C+E">Elham Dolatabadi</a>, 
<a href="/search/cs?searchtype=author&query=Pandya%2C+D">Deval Pandya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12413" title="Abstract">arXiv:2401.12413</a> (replaced) [<a href="/pdf/2401.12413" title="Download PDF">pdf</a>, <a href="/format/2401.12413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual  Translation via Tiny Multi-Parallel Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Shaomu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Stap%2C+D">David Stap</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13641" title="Abstract">arXiv:2401.13641</a> (replaced) [<a href="/pdf/2401.13641" title="Download PDF">pdf</a>, <a href="/format/2401.13641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Good is ChatGPT at Face Biometrics? A First Look into Recognition,  Soft Biometrics, and Explainability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DeAndres-Tame%2C+I">Ivan DeAndres-Tame</a>, 
<a href="/search/cs?searchtype=author&query=Tolosana%2C+R">Ruben Tolosana</a>, 
<a href="/search/cs?searchtype=author&query=Vera-Rodriguez%2C+R">Ruben Vera-Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+A">Aythami Morales</a>, 
<a href="/search/cs?searchtype=author&query=Fierrez%2C+J">Julian Fierrez</a>, 
<a href="/search/cs?searchtype=author&query=Ortega-Garcia%2C+J">Javier Ortega-Garcia</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, February 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13941" title="Abstract">arXiv:2401.13941</a> (replaced) [<a href="/pdf/2401.13941" title="Download PDF">pdf</a>, <a href="/ps/2401.13941" title="Download PostScript">ps</a>, <a href="/format/2401.13941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AC-Driven Series Elastic Electrohydraulic Actuator for Stable and Smooth  Displacement Output
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Q">Quan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuanyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dannuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeow%2C+R+C">Raye Chen-Hua Yeow</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14009" title="Abstract">arXiv:2401.14009</a> (replaced) [<a href="/pdf/2401.14009" title="Download PDF">pdf</a>, <a href="/format/2401.14009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Feasibility of Simple Transformer for Dynamic Graph Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuxia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+L">Lizi Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by WWW'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14556" title="Abstract">arXiv:2401.14556</a> (replaced) [<a href="/pdf/2401.14556" title="Download PDF">pdf</a>, <a href="/format/2401.14556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Looking Right is Sometimes Right: Investigating the Capabilities of  Decoder-only LLMs for Sequence Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duki%C4%87%2C+D">David Duki&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0najder%2C+J">Jan &#x160;najder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14591" title="Abstract">arXiv:2401.14591</a> (replaced) [<a href="/pdf/2401.14591" title="Download PDF">pdf</a>, <a href="/format/2401.14591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ricci flow-guided autoencoders in learning time-dependent dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gracyk%2C+A">Andrew Gracyk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15330" title="Abstract">arXiv:2401.15330</a> (replaced) [<a href="/pdf/2401.15330" title="Download PDF">pdf</a>, <a href="/format/2401.15330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Sparse Survival Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+R">Rui Xin</a>, 
<a href="/search/cs?searchtype=author&query=Seltzer%2C+M">Margo Seltzer</a>, 
<a href="/search/cs?searchtype=author&query=Rudin%2C+C">Cynthia Rudin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS2024 camera ready version. arXiv admin note: text overlap with <a href="/abs/2211.14980">arXiv:2211.14980</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15906" title="Abstract">arXiv:2401.15906</a> (replaced) [<a href="/pdf/2401.15906" title="Download PDF">pdf</a>, <a href="/format/2401.15906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean Estimation with User-Level Privacy for Spatio-Temporal IoT Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rameshwar%2C+V+A">V. Arvind Rameshwar</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+A">Anshoo Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Prajjwal Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+N">Novoneel Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Abhay Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17464" title="Abstract">arXiv:2401.17464</a> (replaced) [<a href="/pdf/2401.17464" title="Download PDF">pdf</a>, <a href="/format/2401.17464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Tool Use with Chain-of-Abstraction Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Silin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedi-Yu%2C+J">Jane Dwivedi-Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Ping Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X+E">Xiaoqing Ellen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Pasunuru%2C+R">Ramakanth Pasunuru</a>, 
<a href="/search/cs?searchtype=author&query=Golovneva%2C+O">Olga Golovneva</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+K">Koustuv Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianlu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00849" title="Abstract">arXiv:2402.00849</a> (replaced) [<a href="/pdf/2402.00849" title="Download PDF">pdf</a>, <a href="/format/2402.00849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-based Causal Representation Learning: Linear and General  Transformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Var%C4%B1c%C4%B1%2C+B">Burak Var&#x131;c&#x131;</a>, 
<a href="/search/cs?searchtype=author&query=Acart%C3%BCrk%2C+E">Emre Acart&#xfc;rk</a>, 
<a href="/search/cs?searchtype=author&query=Shanmugam%2C+K">Karthikeyan Shanmugam</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Abhishek Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Tajer%2C+A">Ali Tajer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> (updated literature review) Linear transformations: stronger results than our previous paper Score-based Causal Representation Learning with Interventions (<a href="/abs/2301.08230">arXiv:2301.08230</a>). General transformations: results also appear in our paper General Identifiability and Achievability for Causal Representation Learning (<a href="/abs/2310.15450">arXiv:2310.15450</a>) accepted to AISTATS 2024 (oral). arXiv admin note: text overlap with <a href="/abs/2310.15450">arXiv:2310.15450</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01055" title="Abstract">arXiv:2402.01055</a> (replaced) [<a href="/pdf/2402.01055" title="Download PDF">pdf</a>, <a href="/format/2402.01055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiclass Learning from Noisy Labels for Non-decomposable Performance  Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Shivani Agarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01481" title="Abstract">arXiv:2402.01481</a> (replaced) [<a href="/pdf/2402.01481" title="Download PDF">pdf</a>, <a href="/format/2402.01481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-level protein pre-training with Vabs-Net
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiale Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+W">Wanru Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jia Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shuqi Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01593" title="Abstract">arXiv:2402.01593</a> (replaced) [<a href="/pdf/2402.01593" title="Download PDF">pdf</a>, <a href="/ps/2402.01593" title="Download PostScript">ps</a>, <a href="/format/2402.01593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Accuracy of Approximate Filtering Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carrillo%2C+J+A">J. A. Carrillo</a>, 
<a href="/search/math?searchtype=author&query=Hoffmann%2C+F">F. Hoffmann</a>, 
<a href="/search/math?searchtype=author&query=Stuart%2C+A+M">A. M. Stuart</a>, 
<a href="/search/math?searchtype=author&query=Vaes%2C+U">U. Vaes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ICIAM proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01663" title="Abstract">arXiv:2402.01663</a> (replaced) [<a href="/pdf/2402.01663" title="Download PDF">pdf</a>, <a href="/format/2402.01663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Killer Apps: Low-Speed, Large-Scale AI Weapons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldman%2C+P">Philip Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Dant%2C+A">Aaron Dant</a>, 
<a href="/search/cs?searchtype=author&query=Foulds%2C+J+R">James R. Foulds</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages with 10 pages of appendices. 3 Figures, 2 code listings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01927" title="Abstract">arXiv:2402.01927</a> (replaced) [<a href="/pdf/2402.01927" title="Download PDF">pdf</a>, <a href="/format/2402.01927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathemyths: Leveraging Large Language Models to Teach Mathematical  Language through Child-AI Co-Creative Storytelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuechen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ziska%2C+K">Katherine Ziska</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+S">Soobin Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chi-Lin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Ying Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conditionally Accepted at CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02725" title="Abstract">arXiv:2402.02725</a> (replaced) [<a href="/pdf/2402.02725" title="Download PDF">pdf</a>, <a href="/ps/2402.02725" title="Download PostScript">ps</a>, <a href="/format/2402.02725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cybersickness Detection through Head Movement Patterns: A Promising  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salehi%2C+M">Masoud Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Javadpour%2C+N">Nikoo Javadpour</a>, 
<a href="/search/cs?searchtype=author&query=Beisner%2C+B">Brietta Beisner</a>, 
<a href="/search/cs?searchtype=author&query=Sanaei%2C+M">Mohammadamin Sanaei</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+S+B">Stephen B. Gilbert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 3 Figures, 3 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03091" title="Abstract">arXiv:2402.03091</a> (replaced) [<a href="/pdf/2402.03091" title="Download PDF">pdf</a>, <a href="/format/2402.03091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal rate of convergence in periodic homogenization of viscous  Hamilton-Jacobi equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Qian%2C+J">Jianliang Qian</a>, 
<a href="/search/math?searchtype=author&query=Sprekeler%2C+T">Timo Sprekeler</a>, 
<a href="/search/math?searchtype=author&query=Tran%2C+H+V">Hung V. Tran</a>, 
<a href="/search/math?searchtype=author&query=Yu%2C+Y">Yifeng Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03299" title="Abstract">arXiv:2402.03299</a> (replaced) [<a href="/pdf/2402.03299" title="Download PDF">pdf</a>, <a href="/format/2402.03299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GUARD: Role-playing to Generate Natural-language Jailbreakings to Test  Guideline Adherence of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Haibo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruoxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Andy Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinyin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haohan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 papges
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03563" title="Abstract">arXiv:2402.03563</a> (replaced) [<a href="/pdf/2402.03563" title="Download PDF">pdf</a>, <a href="/format/2402.03563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distinguishing the Knowable from the Unknowable with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahdritz%2C+G">Gustaf Ahdritz</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+T">Tian Qin</a>, 
<a href="/search/cs?searchtype=author&query=Vyas%2C+N">Nikhil Vyas</a>, 
<a href="/search/cs?searchtype=author&query=Barak%2C+B">Boaz Barak</a>, 
<a href="/search/cs?searchtype=author&query=Edelman%2C+B+L">Benjamin L. Edelman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03848" title="Abstract">arXiv:2402.03848</a> (replaced) [<a href="/pdf/2402.03848" title="Download PDF">pdf</a>, <a href="/format/2402.03848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ANLS* -- A Universal Document Processing Metric for Generative Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peer%2C+D">David Peer</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6pf%2C+P">Philemon Sch&#xf6;pf</a>, 
<a href="/search/cs?searchtype=author&query=Nebendahl%2C+V">Volckmar Nebendahl</a>, 
<a href="/search/cs?searchtype=author&query=Rietzler%2C+A">Alexander Rietzler</a>, 
<a href="/search/cs?searchtype=author&query=Stabinger%2C+S">Sebastian Stabinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04249" title="Abstract">arXiv:2402.04249</a> (replaced) [<a href="/pdf/2402.04249" title="Download PDF">pdf</a>, <a href="/format/2402.04249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HarmBench: A Standardized Evaluation Framework for Automated Red Teaming  and Robust Refusal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazeika%2C+M">Mantas Mazeika</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+L">Long Phan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xuwang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Andy Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+N">Norman Mu</a>, 
<a href="/search/cs?searchtype=author&query=Sakhaee%2C+E">Elham Sakhaee</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nathaniel Li</a>, 
<a href="/search/cs?searchtype=author&query=Basart%2C+S">Steven Basart</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Forsyth%2C+D">David Forsyth</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://www.harmbench.org">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04470" title="Abstract">arXiv:2402.04470</a> (replaced) [<a href="/pdf/2402.04470" title="Download PDF">pdf</a>, <a href="/ps/2402.04470" title="Download PostScript">ps</a>, <a href="/format/2402.04470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large language models as probes into latent psychology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhicheng Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05041" title="Abstract">arXiv:2402.05041</a> (replaced) [<a href="/pdf/2402.05041" title="Download PDF">pdf</a>, <a href="/format/2402.05041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-reversible lifts of reversible diffusion processes and relaxation  times
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eberle%2C+A">Andreas Eberle</a>, 
<a href="/search/math?searchtype=author&query=L%C3%B6rler%2C+F">Francis L&#xf6;rler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added definition of strong lift. 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA); Statistics Theory (math.ST); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05408" title="Abstract">arXiv:2402.05408</a> (replaced) [<a href="/pdf/2402.05408" title="Download PDF">pdf</a>, <a href="/format/2402.05408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dewei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">You Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in CVPR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06081" title="Abstract">arXiv:2402.06081</a> (replaced) [<a href="/pdf/2402.06081" title="Download PDF">pdf</a>, <a href="/format/2402.06081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computer Search of Primitive OBZCPs of Lengths up to 49
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kazakov%2C+P">Peter Kazakov</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zilong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Number Theory (math.NT)

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07300" title="Abstract">arXiv:2402.07300</a> (replaced) [<a href="/pdf/2402.07300" title="Download PDF">pdf</a>, <a href="/format/2402.07300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPICA: Interactive Video Content Exploration through Augmented Audio  Descriptions for Blind or Low-Vision Viewers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+Z">Zheng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Wimer%2C+B+L">Brianna L. Wimer</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kaiwen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Keyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ban%2C+J">Jerrick Ban</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yapeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuhang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+J">Toby Jia-Jun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07452" title="Abstract">arXiv:2402.07452</a> (replaced) [<a href="/pdf/2402.07452" title="Download PDF">pdf</a>, <a href="/format/2402.07452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TriAug: Out-of-Distribution Detection for Imbalanced Breast Lesion in  Ultrasound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yinyu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shijing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+D">Dong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruobing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07791" title="Abstract">arXiv:2402.07791</a> (replaced) [<a href="/pdf/2402.07791" title="Download PDF">pdf</a>, <a href="/format/2402.07791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Decision Manifolds to Assure Trusted Autonomous Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Litton%2C+M">Matthew Litton</a>, 
<a href="/search/cs?searchtype=author&query=Drusinsky%2C+D">Doron Drusinsky</a>, 
<a href="/search/cs?searchtype=author&query=Michael%2C+J+B">James Bret Michael</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08164" title="Abstract">arXiv:2402.08164</a> (replaced) [<a href="/pdf/2402.08164" title="Download PDF">pdf</a>, <a href="/ps/2402.08164" title="Download PostScript">ps</a>, <a href="/format/2402.08164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Limitations of the Transformer Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Peng%2C+B">Binghui Peng</a>, 
<a href="/search/stat?searchtype=author&query=Narayanan%2C+S">Srini Narayanan</a>, 
<a href="/search/stat?searchtype=author&query=Papadimitriou%2C+C">Christos Papadimitriou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08496" title="Abstract">arXiv:2402.08496</a> (replaced) [<a href="/pdf/2402.08496" title="Download PDF">pdf</a>, <a href="/format/2402.08496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Review of Data-to-Text NLG
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osuji%2C+C+C">Chinonso Cynthia Osuji</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+T+C">Thiago Castro Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+B">Brian Davis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08774" title="Abstract">arXiv:2402.08774</a> (replaced) [<a href="/pdf/2402.08774" title="Download PDF">pdf</a>, <a href="/ps/2402.08774" title="Download PostScript">ps</a>, <a href="/format/2402.08774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LDTrack: Dynamic People Tracking by Service Robots using Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fung%2C+A">Angus Fung</a>, 
<a href="/search/cs?searchtype=author&query=Benhabib%2C+B">Beno Benhabib</a>, 
<a href="/search/cs?searchtype=author&query=Nejat%2C+G">Goldie Nejat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08931" title="Abstract">arXiv:2402.08931</a> (replaced) [<a href="/pdf/2402.08931" title="Download PDF">pdf</a>, <a href="/format/2402.08931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Depth-aware Volume Attention for Texture-less Stereo Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yintao Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09216" title="Abstract">arXiv:2402.09216</a> (replaced) [<a href="/pdf/2402.09216" title="Download PDF">pdf</a>, <a href="/format/2402.09216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling the Authoring of AutoTutors with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+P">Sankalan Pal Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Zouhar%2C+V">Vil&#xe9;m Zouhar</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10232" title="Abstract">arXiv:2402.10232</a> (replaced) [<a href="/pdf/2402.10232" title="Download PDF">pdf</a>, <a href="/ps/2402.10232" title="Download PostScript">ps</a>, <a href="/format/2402.10232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple, unified analysis of Johnson-Lindenstrauss with applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+Y">Yingru Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10460" title="Abstract">arXiv:2402.10460</a> (replaced) [<a href="/pdf/2402.10460" title="Download PDF">pdf</a>, <a href="/format/2402.10460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A survey of LSM-Tree based Indexes, Data Systems and KV-stores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Supriya Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10573" title="Abstract">arXiv:2402.10573</a> (replaced) [<a href="/pdf/2402.10573" title="Download PDF">pdf</a>, <a href="/format/2402.10573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LinkNER: Linking Local Named Entity Recognition Models to Large Language  Models using Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuhua Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Mengting Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WebConf (WWW'2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10870" title="Abstract">arXiv:2402.10870</a> (replaced) [<a href="/pdf/2402.10870" title="Download PDF">pdf</a>, <a href="/format/2402.10870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best of Three Worlds: Adaptive Experimentation for Digital Marketing in  Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiez%2C+T">Tanner Fiez</a>, 
<a href="/search/cs?searchtype=author&query=Nassif%2C+H">Houssam Nassif</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu-Cheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gamez%2C+S">Sergio Gamez</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+L">Lalit Jain</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Web Conference (WWW), Singapore, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11303" title="Abstract">arXiv:2402.11303</a> (replaced) [<a href="/pdf/2402.11303" title="Download PDF">pdf</a>, <a href="/format/2402.11303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FViT: A Focal Vision Transformer with Gabor Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yulong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongshuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zengqiang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11325" title="Abstract">arXiv:2402.11325</a> (replaced) [<a href="/pdf/2402.11325" title="Download PDF">pdf</a>, <a href="/format/2402.11325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatEarthNet: A Global-Scale Image-Text Dataset Empowering  Vision-Language Geo-Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhenghang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhitong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+L">Lichao Mou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11488" title="Abstract">arXiv:2402.11488</a> (replaced) [<a href="/pdf/2402.11488" title="Download PDF">pdf</a>, <a href="/format/2402.11488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IRFundusSet: An Integrated Retinal Fundus Dataset with a Harmonized  Healthy Label
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Githinji%2C+P+B">P. Bilha Githinji</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Keming Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiantao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+P">Peiwu Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11525" title="Abstract">arXiv:2402.11525</a> (replaced) [<a href="/pdf/2402.11525" title="Download PDF">pdf</a>, <a href="/format/2402.11525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Translation Preference Modeling with RLHF: A Step Towards  Cost-Effective Solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zu%2C+C">Can Zu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sixian Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+W">Wenjuan Qin</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11585" title="Abstract">arXiv:2402.11585</a> (replaced) [<a href="/pdf/2402.11585" title="Download PDF">pdf</a>, <a href="/format/2402.11585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolypNextLSTM: A lightweight and fast polyp video segmentation network  using ConvNext and ConvLSTM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+D">Debayan Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Reuter%2C+K">Konrad Reuter</a>, 
<a href="/search/cs?searchtype=author&query=Behrendnt%2C+F">Finn Behrendnt</a>, 
<a href="/search/cs?searchtype=author&query=Maack%2C+L">Lennart Maack</a>, 
<a href="/search/cs?searchtype=author&query=Grube%2C+S">Sarah Grube</a>, 
<a href="/search/cs?searchtype=author&query=Schlaefer%2C+A">Alexander Schlaefer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11793" title="Abstract">arXiv:2402.11793</a> (replaced) [<a href="/pdf/2402.11793" title="Download PDF">pdf</a>, <a href="/format/2402.11793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Kaleidoscopic Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+H">Harsh Shrivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11996" title="Abstract">arXiv:2402.11996</a> (replaced) [<a href="/pdf/2402.11996" title="Download PDF">pdf</a>, <a href="/format/2402.11996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ISCUTE: Instance Segmentation of Cables Using Text Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kozlovsky%2C+S">Shir Kozlovsky</a>, 
<a href="/search/cs?searchtype=author&query=Joglekar%2C+O">Omkar Joglekar</a>, 
<a href="/search/cs?searchtype=author&query=Di+Castro%2C+D">Dotan Di Castro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12026" title="Abstract">arXiv:2402.12026</a> (replaced) [<a href="/pdf/2402.12026" title="Download PDF">pdf</a>, <a href="/format/2402.12026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acquiring Clean Language Models from Backdoor Poisoned Datasets by  Downscaling Frequency Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zongru Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Pengzhou Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gongshen Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12140" title="Abstract">arXiv:2402.12140</a> (replaced) [<a href="/pdf/2402.12140" title="Download PDF">pdf</a>, <a href="/format/2402.12140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Many-Stage Optimal Stabilized Runge-Kutta Methods for Hyperbolic Partial  Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Doehring%2C+D">Daniel Doehring</a>, 
<a href="/search/math?searchtype=author&query=Gassner%2C+G+J">Gregor J. Gassner</a>, 
<a href="/search/math?searchtype=author&query=Torrilhon%2C+M">Manuel Torrilhon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Classical Analysis and ODEs (math.CA)

</div>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12563" title="Abstract">arXiv:2402.12563</a> (replaced) [<a href="/pdf/2402.12563" title="Download PDF">pdf</a>, <a href="/format/2402.12563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Loka Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yusheng Su</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13233" title="Abstract">arXiv:2402.13233</a> (replaced) [<a href="/pdf/2402.13233" title="Download PDF">pdf</a>, <a href="/format/2402.13233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMORE: Similarity-based Hyperdimensional Domain Adaptation for  Multi-Sensor Time Series Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Faruque%2C+M+A+A">Mohammad Abdullah Al Faruque</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2308.03295">arXiv:2308.03295</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13241" title="Abstract">arXiv:2402.13241</a> (replaced) [<a href="/pdf/2402.13241" title="Download PDF">pdf</a>, <a href="/format/2402.13241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Causal Discovery from Heterogeneous Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Loka Li</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+I">Ignavier Ng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+G">Gongxu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Biwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Bin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13516" title="Abstract">arXiv:2402.13516</a> (replaced) [<a href="/pdf/2402.13516" title="Download PDF">pdf</a>, <a href="/format/2402.13516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity  within Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chenyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengding Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guangli Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13828" title="Abstract">arXiv:2402.13828</a> (replaced) [<a href="/pdf/2402.13828" title="Download PDF">pdf</a>, <a href="/format/2402.13828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Origami: (un)folding the abstraction of recursion schemes for program  synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+M+C">Matheus Campos Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=de+Franca%2C+F+O">Fabricio Olivetti de Franca</a>, 
<a href="/search/cs?searchtype=author&query=Francesquini%2C+E">Emilio Francesquini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13946" title="Abstract">arXiv:2402.13946</a> (replaced) [<a href="/pdf/2402.13946" title="Download PDF">pdf</a>, <a href="/format/2402.13946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gohil%2C+V">Vasudev Gohil</a>, 
<a href="/search/cs?searchtype=author&query=Patnaik%2C+S">Satwik Patnaik</a>, 
<a href="/search/cs?searchtype=author&query=Kalathil%2C+D">Dileep Kalathil</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+J">Jeyavijayan Rajendran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in USENIX Security Symposium, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14026" title="Abstract">arXiv:2402.14026</a> (replaced) [<a href="/pdf/2402.14026" title="Download PDF">pdf</a>, <a href="/ps/2402.14026" title="Download PostScript">ps</a>, <a href="/format/2402.14026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probability Tools for Sequential Random Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+Y">Yingru Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Data Structures and Algorithms (cs.DS); Information Theory (cs.IT); Numerical Analysis (math.NA); Probability (math.PR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14035" title="Abstract">arXiv:2402.14035</a> (replaced) [<a href="/pdf/2402.14035" title="Download PDF">pdf</a>, <a href="/format/2402.14035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wisdom of Committee: Distilling from Foundation Model to Specialized  Application Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zichang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingyun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuening Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+A">Anshumali Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+S">Shuchao Bi</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lichan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhe Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14181" title="Abstract">arXiv:2402.14181</a> (replaced) [<a href="/pdf/2402.14181" title="Download PDF">pdf</a>, <a href="/format/2402.14181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grid Minors and Products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dujmovi%C4%87%2C+V">Vida Dujmovi&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Morin%2C+P">Pat Morin</a>, 
<a href="/search/math?searchtype=author&query=Wood%2C+D+R">David R. Wood</a>, 
<a href="/search/math?searchtype=author&query=Worley%2C+D">David Worley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14228" title="Abstract">arXiv:2402.14228</a> (replaced) [<a href="/pdf/2402.14228" title="Download PDF">pdf</a>, <a href="/format/2402.14228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COPR: Continual Human Preference Learning via Optimal Policy  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yuanzhao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yehong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Bin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14278" title="Abstract">arXiv:2402.14278</a> (replaced) [<a href="/pdf/2402.14278" title="Download PDF">pdf</a>, <a href="/format/2402.14278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locality Bounds for Sampling Hamming Slices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kane%2C+D+M">Daniel M. Kane</a>, 
<a href="/search/cs?searchtype=author&query=Ostuni%2C+A">Anthony Ostuni</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kewen Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minor updates to better reflect past literature. No technical material has been changed
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14392" title="Abstract">arXiv:2402.14392</a> (replaced) [<a href="/pdf/2402.14392" title="Download PDF">pdf</a>, <a href="/format/2402.14392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reading Relevant Feature from Global Representation Memory for Visual  Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pinxue Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lingyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinglun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+W">Weifeng Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9pages,5 figures, accepted by the Thirty-seventh Conference on Neural Information Processing Systems(Neurips 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14432" title="Abstract">arXiv:2402.14432</a> (replaced) [<a href="/pdf/2402.14432" title="Download PDF">pdf</a>, <a href="/format/2402.14432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Influence of Driving Context on Lateral Driving Style  Preferences: A Simulator-Based Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Haselberger%2C+J">Johann Haselberger</a>, 
<a href="/search/eess?searchtype=author&query=B%C3%B6hle%2C+M">Maximilian B&#xf6;hle</a>, 
<a href="/search/eess?searchtype=author&query=Schick%2C+B">Bernhard Schick</a>, 
<a href="/search/eess?searchtype=author&query=M%C3%BCller%2C+S">Steffen M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14611" title="Abstract">arXiv:2402.14611</a> (replaced) [<a href="/pdf/2402.14611" title="Download PDF">pdf</a>, <a href="/format/2402.14611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overcoming Dimensional Collapse in Self-supervised Contrastive Learning  for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassanpour%2C+J">Jamshid Hassanpour</a>, 
<a href="/search/cs?searchtype=author&query=Srivastav%2C+V">Vinkle Srivastav</a>, 
<a href="/search/cs?searchtype=author&query=Mutter%2C+D">Didier Mutter</a>, 
<a href="/search/cs?searchtype=author&query=Padoy%2C+N">Nicolas Padoy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at at ISBI-2024 (<a href="https://biomedicalimaging.org/2024/">this https URL</a>). 4 pages, 2 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14696" title="Abstract">arXiv:2402.14696</a> (replaced) [<a href="/pdf/2402.14696" title="Download PDF">pdf</a>, <a href="/ps/2402.14696" title="Download PostScript">ps</a>, <a href="/format/2402.14696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Schr&#xf6;dingerization based quantum algorithms for linear dynamical  systems with inhomogeneous terms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jin%2C+S">Shi Jin</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+N">Nana Liu</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+C">Chuwen Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14872" title="Abstract">arXiv:2402.14872</a> (replaced) [<a href="/pdf/2402.14872" title="Download PDF">pdf</a>, <a href="/format/2402.14872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts  Against Open-source LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Siyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Han Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aishan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E">Ee-Chien Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15119" title="Abstract">arXiv:2402.15119</a> (replaced) [<a href="/pdf/2402.15119" title="Download PDF">pdf</a>, <a href="/ps/2402.15119" title="Download PostScript">ps</a>, <a href="/format/2402.15119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multidisciplinary framework for deconstructing bots&#x27; pluripotency in  dualistic antagonism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wentao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sasahara%2C+K">Kazutoshi Sasahara</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+J">Jianxun Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenlu Fan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiwen Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15159" title="Abstract">arXiv:2402.15159</a> (replaced) [<a href="/pdf/2402.15159" title="Download PDF">pdf</a>, <a href="/format/2402.15159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Unlearning of Pre-trained Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chien%2C+E">Eli Chien</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Minxin Du</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+X">Xinyao Niu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zezhou Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/yaojin17/Unlearning_LLM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15180" title="Abstract">arXiv:2402.15180</a> (replaced) [<a href="/pdf/2402.15180" title="Download PDF">pdf</a>, <a href="/format/2402.15180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks  with Self-Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heegyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yuk%2C+S">Sehyun Yuk</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hyunsouk Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15183" title="Abstract">arXiv:2402.15183</a> (replaced) [<a href="/pdf/2402.15183" title="Download PDF">pdf</a>, <a href="/format/2402.15183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphEdit: Large Language Models for Graph Structure Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zirui Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yanhua Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zixuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15267" title="Abstract">arXiv:2402.15267</a> (replaced) [<a href="/pdf/2402.15267" title="Download PDF">pdf</a>, <a href="/format/2402.15267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robust Defense against Adversarial Attacks on Deep Learning-based  Malware Detectors via (De)Randomized Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gibert%2C+D">Daniel Gibert</a>, 
<a href="/search/cs?searchtype=author&query=Zizzo%2C+G">Giulio Zizzo</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+Q">Quan Le</a>, 
<a href="/search/cs?searchtype=author&query=Planes%2C+J">Jordi Planes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2308.08906">arXiv:2308.08906</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15674" title="Abstract">arXiv:2402.15674</a> (replaced) [<a href="/pdf/2402.15674" title="Download PDF">pdf</a>, <a href="/format/2402.15674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formally Verified C Code Generation from Hybrid Communicating Sequential  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zekun Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+B">Bohua Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Qiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+N">Naijun Zhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15852" title="Abstract">arXiv:2402.15852</a> (replaced) [<a href="/pdf/2402.15852" title="Download PDF">pdf</a>, <a href="/format/2402.15852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NaVid: Video-based VLM Plans the Next Step for Vision-and-Language  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiazhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kunyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rongtao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Gengze Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yicong Hong</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xiaomeng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15958" title="Abstract">arXiv:2402.15958</a> (replaced) [<a href="/pdf/2402.15958" title="Download PDF">pdf</a>, <a href="/format/2402.15958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the dynamics of three-layer neural networks: initial condensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng-An Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tao Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15968" title="Abstract">arXiv:2402.15968</a> (replaced) [<a href="/pdf/2402.15968" title="Download PDF">pdf</a>, <a href="/format/2402.15968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoDream: Exchanging dreams instead of models for federated aggregation  with heterogeneous models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Abhishek Singh</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+G">Gauri Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kapila%2C+R">Ritvik Kapila</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yichuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+A">Alex Dang</a>, 
<a href="/search/cs?searchtype=author&query=Shankar%2C+S">Sheshank Shankar</a>, 
<a href="/search/cs?searchtype=author&query=Ehab%2C+M">Mohammed Ehab</a>, 
<a href="/search/cs?searchtype=author&query=Raskar%2C+R">Ramesh Raskar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 12 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16040" title="Abstract">arXiv:2402.16040</a> (replaced) [<a href="/pdf/2402.16040" title="Download PDF">pdf</a>, <a href="/format/2402.16040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EHRNoteQA: A Patient-Specific Question Answering Benchmark for  Evaluating Large Language Models in Clinical Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kweon%2C+S">Sunjun Kweon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jiyoun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+H">Heeyoung Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+D">Dongchul Cha</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+H">Hangyul Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kwanghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+S">Seunghyun Won</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16063" title="Abstract">arXiv:2402.16063</a> (replaced) [<a href="/pdf/2402.16063" title="Download PDF">pdf</a>, <a href="/format/2402.16063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Citation-Enhanced Generation for LLM-based Chatbot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junkai Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weizhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16107" title="Abstract">arXiv:2402.16107</a> (replaced) [<a href="/pdf/2402.16107" title="Download PDF">pdf</a>, <a href="/format/2402.16107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuseChat: Knowledge Fusion of Chat Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fanqi Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+L">Longguang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report, work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16311" title="Abstract">arXiv:2402.16311</a> (replaced) [<a href="/pdf/2402.16311" title="Download PDF">pdf</a>, <a href="/format/2402.16311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-domain Chinese Sentence Pattern Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingsi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+C">Cunliang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liner Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Meishan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yujie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haozhe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Erhong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16359" title="Abstract">arXiv:2402.16359</a> (replaced) [<a href="/pdf/2402.16359" title="Download PDF">pdf</a>, <a href="/format/2402.16359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feedback Efficient Online Fine-Tuning of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uehara%2C+M">Masatoshi Uehara</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yulai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+K">Kevin Black</a>, 
<a href="/search/cs?searchtype=author&query=Hajiramezanali%2C+E">Ehsan Hajiramezanali</a>, 
<a href="/search/cs?searchtype=author&query=Scalia%2C+G">Gabriele Scalia</a>, 
<a href="/search/cs?searchtype=author&query=Diamant%2C+N+L">Nathaniel Lee Diamant</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+A+M">Alex M Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>, 
<a href="/search/cs?searchtype=author&query=Biancalani%2C+T">Tommaso Biancalani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review (codes will be released soon)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16377" title="Abstract">arXiv:2402.16377</a> (replaced) [<a href="/pdf/2402.16377" title="Download PDF">pdf</a>, <a href="/ps/2402.16377" title="Download PostScript">ps</a>, <a href="/format/2402.16377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation and perturbations of stable solutions to a stationary mean  field game system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Berry%2C+J">Jules Berry</a> (IRMAR, INSA Rennes, UR), 
<a href="/search/math?searchtype=author&query=Ley%2C+O">Olivier Ley</a> (IRMAR), 
<a href="/search/math?searchtype=author&query=Silva%2C+F+J">Francisco J Silva</a> (XLIM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16384" title="Abstract">arXiv:2402.16384</a> (replaced) [<a href="/pdf/2402.16384" title="Download PDF">pdf</a>, <a href="/format/2402.16384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Superconductor Neuron with Ternary Synaptic Connections for  Ultra-Fast SNN Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Karamuftuoglu%2C+M+A">Mustafa Altay Karamuftuoglu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ucpinar%2C+B+Z">Beyza Zeynep Ucpinar</a>, 
<a href="/search/cond-mat?searchtype=author&query=Fayyazi%2C+A">Arash Fayyazi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Razmkhah%2C+S">Sasan Razmkhah</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kamal%2C+M">Mehdi Kamal</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pedram%2C+M">Massoud Pedram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Superconductivity (cond-mat.supr-con)</span>; Emerging Technologies (cs.ET); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16458" title="Abstract">arXiv:2402.16458</a> (replaced) [<a href="/pdf/2402.16458" title="Download PDF">pdf</a>, <a href="/format/2402.16458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ID-XCB: Data-independent Debiasing for Fair and Accurate  Transformer-based Cyberbullying Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+P">Peiling Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zubiaga%2C+A">Arkaitz Zubiaga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16506" title="Abstract">arXiv:2402.16506</a> (replaced) [<a href="/pdf/2402.16506" title="Download PDF">pdf</a>, <a href="/format/2402.16506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Conditional Diffusion Models for Semantic Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Juyeon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+I">Inho Kong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+D">Dogyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">Hyunwoo J. Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16542" title="Abstract">arXiv:2402.16542</a> (replaced) [<a href="/pdf/2402.16542" title="Download PDF">pdf</a>, <a href="/format/2402.16542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboGrind: Intuitive and Interactive Surface Treatment with Industrial  Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alt%2C+B">Benjamin Alt</a>, 
<a href="/search/cs?searchtype=author&query=St%C3%B6ckl%2C+F">Florian St&#xf6;ckl</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+S">Silvan M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Braun%2C+C">Christopher Braun</a>, 
<a href="/search/cs?searchtype=author&query=Raible%2C+J">Julian Raible</a>, 
<a href="/search/cs?searchtype=author&query=Alhasan%2C+S">Saad Alhasan</a>, 
<a href="/search/cs?searchtype=author&query=Rettig%2C+O">Oliver Rettig</a>, 
<a href="/search/cs?searchtype=author&query=Ringle%2C+L">Lukas Ringle</a>, 
<a href="/search/cs?searchtype=author&query=Katic%2C+D">Darko Katic</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%A4kel%2C+R">Rainer J&#xe4;kel</a>, 
<a href="/search/cs?searchtype=author&query=Beetz%2C+M">Michael Beetz</a>, 
<a href="/search/cs?searchtype=author&query=Strand%2C+M">Marcus Strand</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+M+F">Marco F. Huber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, accepted to the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16569" title="Abstract">arXiv:2402.16569</a> (replaced) [<a href="/pdf/2402.16569" title="Download PDF">pdf</a>, <a href="/format/2402.16569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretrained Visual Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirchhof%2C+M">Michael Kirchhof</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+M">Mark Collier</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S+J">Seong Joon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+E">Enkelejda Kasneci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16726" title="Abstract">arXiv:2402.16726</a> (replaced) [<a href="/pdf/2402.16726" title="Download PDF">pdf</a>, <a href="/format/2402.16726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Grokked Transformers in Complex Modular Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Minegishi%2C+G">Gouki Minegishi</a>, 
<a href="/search/cs?searchtype=author&query=Iwasawa%2C+Y">Yusuke Iwasawa</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/frt03/grok_mod_poly">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16782" title="Abstract">arXiv:2402.16782</a> (replaced) [<a href="/pdf/2402.16782" title="Download PDF">pdf</a>, <a href="/format/2402.16782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiplex measures for higher-order networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lotito%2C+Q+F">Quintino Francesco Lotito</a>, 
<a href="/search/physics?searchtype=author&query=Montresor%2C+A">Alberto Montresor</a>, 
<a href="/search/physics?searchtype=author&query=Battiston%2C+F">Federico Battiston</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16819" title="Abstract">arXiv:2402.16819</a> (replaced) [<a href="/pdf/2402.16819" title="Download PDF">pdf</a>, <a href="/format/2402.16819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nemotron-4 15B Technical Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parmar%2C+J">Jupinder Parmar</a>, 
<a href="/search/cs?searchtype=author&query=Prabhumoye%2C+S">Shrimai Prabhumoye</a>, 
<a href="/search/cs?searchtype=author&query=Jennings%2C+J">Joseph Jennings</a>, 
<a href="/search/cs?searchtype=author&query=Patwary%2C+M">Mostofa Patwary</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+S">Sandeep Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+D">Dan Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+D">Deepak Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Jhunjhunwala%2C+A">Aastha Jhunjhunwala</a>, 
<a href="/search/cs?searchtype=author&query=Dattagupta%2C+A">Ayush Dattagupta</a>, 
<a href="/search/cs?searchtype=author&query=Jawa%2C+V">Vibhu Jawa</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mahabaleshwarkar%2C+A">Ameya Mahabaleshwarkar</a>, 
<a href="/search/cs?searchtype=author&query=Nitski%2C+O">Osvald Nitski</a>, 
<a href="/search/cs?searchtype=author&query=Brundyn%2C+A">Annika Brundyn</a>, 
<a href="/search/cs?searchtype=author&query=Maki%2C+J">James Maki</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M">Miguel Martinez</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+J">Jiaxuan You</a>, 
<a href="/search/cs?searchtype=author&query=Kamalu%2C+J">John Kamalu</a>, 
<a href="/search/cs?searchtype=author&query=LeGresley%2C+P">Patrick LeGresley</a>, 
<a href="/search/cs?searchtype=author&query=Fridman%2C+D">Denys Fridman</a>, 
<a href="/search/cs?searchtype=author&query=Casper%2C+J">Jared Casper</a>, 
<a href="/search/cs?searchtype=author&query=Aithal%2C+A">Ashwath Aithal</a>, 
<a href="/search/cs?searchtype=author&query=Kuchaiev%2C+O">Oleksii Kuchaiev</a>, 
<a href="/search/cs?searchtype=author&query=Shoeybi%2C+M">Mohammad Shoeybi</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+J">Jonathan Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16823" title="Abstract">arXiv:2402.16823</a> (replaced) [<a href="/pdf/2402.16823" title="Download PDF">pdf</a>, <a href="/format/2402.16823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Agents as Optimizable Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuge%2C+M">Mingchen Zhuge</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kirsch%2C+L">Louis Kirsch</a>, 
<a href="/search/cs?searchtype=author&query=Faccio%2C+F">Francesco Faccio</a>, 
<a href="/search/cs?searchtype=author&query=Khizbullin%2C+D">Dmitrii Khizbullin</a>, 
<a href="/search/cs?searchtype=author&query=Schmidhuber%2C+J">J&#xfc;rgen Schmidhuber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Website: <a href="https://gptswarm.org">this https URL</a> ; Github Repo: <a href="https://github.com/metauto-ai/gptswarm">this https URL</a> ; Replace to fix typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16825" title="Abstract">arXiv:2402.16825</a> (replaced) [<a href="/pdf/2402.16825" title="Download PDF">pdf</a>, <a href="/ps/2402.16825" title="Download PostScript">ps</a>, <a href="/format/2402.16825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional  layers for 3D abdominal organ segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenzhao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Albert%2C+S">Steffen Albert</a>, 
<a href="/search/cs?searchtype=author&query=Wichtmann%2C+B+D">Barbara D. Wichtmann</a>, 
<a href="/search/cs?searchtype=author&query=Maurer%2C+A">Angelika Maurer</a>, 
<a href="/search/cs?searchtype=author&query=Attenberger%2C+U">Ulrike Attenberger</a>, 
<a href="/search/cs?searchtype=author&query=Z%C3%B6llner%2C+F+G">Frank G. Z&#xf6;llner</a>, 
<a href="/search/cs?searchtype=author&query=Hesser%2C+J">J&#xfc;rgen Hesser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.16842" title="Abstract">arXiv:2402.16842</a> (replaced) [<a href="/pdf/2402.16842" title="Download PDF">pdf</a>, <a href="/format/2402.16842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetry in Low-Rank Adapters of Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiacheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Greenewald%2C+K">Kristjan Greenewald</a>, 
<a href="/search/cs?searchtype=author&query=Nadjahi%2C+K">Kimia Nadjahi</a>, 
<a href="/search/cs?searchtype=author&query=de+Oc%C3%A1riz+Borde%2C+H+S">Haitz S&#xe1;ez de Oc&#xe1;riz Borde</a>, 
<a href="/search/cs?searchtype=author&query=Gabrielsson%2C+R+B">Rickard Br&#xfc;el Gabrielsson</a>, 
<a href="/search/cs?searchtype=author&query=Choshen%2C+L">Leshem Choshen</a>, 
<a href="/search/cs?searchtype=author&query=Ghassemi%2C+M">Marzyeh Ghassemi</a>, 
<a href="/search/cs?searchtype=author&query=Yurochkin%2C+M">Mikhail Yurochkin</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+J">Justin Solomon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item439">Cross-lists</a></li>
<li><a href="#item494">Replacements</a></li>
</ul>
<small>[ total of 754 entries:  <b>1-754</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
