<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Thu 22 Feb 24  to  Fri 23 Feb 24, announced Mon, 26 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item335">Cross-lists</a></li>
<li><a href="#item371">Replacements</a></li>
</ul>
<small>[ total of 629 entries:  <b>1-629</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Mon, 26 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14821" title="Abstract">arXiv:2402.14821</a> [<a href="/pdf/2402.14821" title="Download PDF">pdf</a>, <a href="/format/2402.14821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constraint Propagation on GPU: A Case Study for the Bin Packing  Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tardivo%2C+F">Fabio Tardivo</a>, 
<a href="/search/cs?searchtype=author&query=Michel%2C+L">Laurent Michel</a>, 
<a href="/search/cs?searchtype=author&query=Pontelli%2C+E">Enrico Pontelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The Bin Packing Problem is one of the most important problems in discrete
optimization, as it captures the requirements of many real-world problems.
Because of its importance, it has been approached with the main theoretical and
practical tools. Resolution approaches based on Linear Programming are the most
effective, while Constraint Programming proves valuable when the Bin Packing
Problem is a component of a larger problem. This work focuses on the Bin
Packing constraint and explores how GPUs can be used to enhance its propagation
algorithm. Two approaches are motivated and discussed, one based on knapsack
reasoning and one using alternative lower bounds. The implementations are
evaluated in comparison with state-of-the-art approaches on different
benchmarks from the literature. The results indicate that the GPU-accelerated
lower bounds offers a desirable alternative to tackle large instances.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14822" title="Abstract">arXiv:2402.14822</a> [<a href="/pdf/2402.14822" title="Download PDF">pdf</a>, <a href="/ps/2402.14822" title="Download PostScript">ps</a>, <a href="/format/2402.14822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of an Analog Memory Cell in 0.25 micron CMOS process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barai%2C+P">Paramita Barai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Bachelors Thesis submitted in partial fulfillment of the requirements for the degree of Bachelor of Technology (Honours) in Electronics and Electrical Communication Engineering at the Indian Institute of Technology - Kharagpur (year 2002), by Paramita Barai under the guidance of Prof. Dr. A. S. Dhar
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>

</div>
<p class="mathjax">CMOS VLSI technology is the most dominant integration methodology prevailing
in the world today. Various signal-processing blocks are made using analog or
digital design techniques in MOS VLSI. An important component is the Memory
unit used to store data. In the project a memory cell has been built up using
analog design method. A capacitor is used as the basic storage device. The main
idea behind analog memory is that the analog value of the charge or voltage
stored in the capacitor is the data stored. So the dielectric quality of the
capacitor becomes important here to determine how effectively it can store some
charge. Analog memory is a trade off between hardware cost, chip area and
accuracy or quality of storage. The circuit of analog memory cell was developed
starting from the idea that required voltage will be stored in a capacitor and
MOS transistors were used as switches. A given technology of integration was
used and hence the dielectric property of the capacitor was fixed. By suitable
circuit configuration the analog voltage value was written to the capacitor,
read out when required and the charge loss was also refreshed. The results
obtained are as given in the thesis.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14825" title="Abstract">arXiv:2402.14825</a> [<a href="/pdf/2402.14825" title="Download PDF">pdf</a>, <a href="/format/2402.14825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deepfake Detection and the Impact of Limited Computing Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cantero-Arjona%2C+P">Paloma Cantero-Arjona</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez-Maci%C3%A1n%2C+A">Alfonso S&#xe1;nchez-Maci&#xe1;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The rapid development of technologies and artificial intelligence makes
deepfakes an increasingly sophisticated and challenging-to-identify technique.
To ensure the accuracy of information and control misinformation and mass
manipulation, it is of paramount importance to discover and develop artificial
intelligence models that enable the generic detection of forged videos. This
work aims to address the detection of deepfakes across various existing
datasets in a scenario with limited computing resources. The goal is to analyze
the applicability of different deep learning techniques under these
restrictions and explore possible approaches to enhance their efficiency.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14830" title="Abstract">arXiv:2402.14830</a> [<a href="/pdf/2402.14830" title="Download PDF">pdf</a>, <a href="/format/2402.14830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orca-Math: Unlocking the potential of SLMs in Grade School Math
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitra%2C+A">Arindam Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Khanpour%2C+H">Hamed Khanpour</a>, 
<a href="/search/cs?searchtype=author&query=Rosset%2C+C">Corby Rosset</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A">Ahmed Awadallah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mathematical word problem-solving has long been recognized as a complex task
for small language models (SLMs). A recent study hypothesized that the smallest
model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34
billion parameters. To reach this level of performance with smaller models,
researcher often train SLMs to generate Python code or use tools to help avoid
calculation errors. Additionally, they employ ensembling, where outputs of up
to 100 model runs are combined to arrive at a more accurate result. Result
selection is done using consensus, majority vote or a separate a verifier model
used in conjunction with the SLM. Ensembling provides a substantial boost in
accuracy but at a significant cost increase with multiple calls to the model
(e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).
<br />In this work, we present Orca-Math, a 7-billion-parameter SLM based on the
Mistral-7B, which achieves 86.81% on GSM8k without the need for multiple model
calls or the use of verifiers, code execution or any other external tools. Our
approach has the following key elements: (1) A high quality synthetic dataset
of 200K math problems created using a multi-agent setup where agents
collaborate to create the data, (2) An iterative learning techniques that
enables the SLM to practice solving problems, receive feedback on its solutions
and learn from preference pairs incorporating the SLM solutions and the
feedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves
81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math
achieves 86.81% pass@1. Orca-Math surpasses the performance of significantly
larger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It
also significantly outperforms other smaller models while using much smaller
data (hundreds of thousands vs. millions of problems).
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14832" title="Abstract">arXiv:2402.14832</a> [<a href="/pdf/2402.14832" title="Download PDF">pdf</a>, <a href="/ps/2402.14832" title="Download PostScript">ps</a>, <a href="/format/2402.14832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Simulation Budget Management into Drum-Buffer-Rope: A Study  on Parametrization and Reducing Computational Effort
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bokor%2C+B">Balwin Bokor</a>, 
<a href="/search/cs?searchtype=author&query=Seiringer%2C+W">Wolfgang Seiringer</a>, 
<a href="/search/cs?searchtype=author&query=Altendorfer%2C+K">Klaus Altendorfer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In manufacturing, a bottleneck workstation frequently emerges, complicating
production planning and escalating costs. To address this, Drum-Buffer-Rope
(DBR) is a widely recognized production planning and control method that
fo-cuses on centralizing the bottleneck workstation, thereby improving
production system performance. Although DBR is primarily focused on creating a
bottleneck schedule, the selection of planning parameters is crucial, as they
significantly in-fluence the scheduling process. Conducting a comprehensive
full factorial enu-meration to identify the ideal planning parameters requires
substantial computa-tional effort. Simulation Budget Management (SBM) offers an
effective concept to reduce this effort by skipping less promising parameter
combinations. This publication introduces a method for integrating SBM into
DBR, aimed at deter-mining the optimal planning parameters. Furthermore, we
conduct a simulation study to analyze the effects of different production
system environments, i.e., varying levels of shop load and process uncertainty,
on both the performance and parameterization of DBR and the efficacy of SBM.
Our results show significant reduction in simulation budget for identifying
optimal planning parameters com-pared to traditional full factorial
enumeration.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14833" title="Abstract">arXiv:2402.14833</a> [<a href="/pdf/2402.14833" title="Download PDF">pdf</a>, <a href="/ps/2402.14833" title="Download PostScript">ps</a>, <a href="/format/2402.14833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CliqueParcel: An Approach For Batching LLM Prompts That Jointly  Optimizes Efficiency And Faithfulness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiayi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tinghan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Neville%2C+J">Jennifer Neville</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have become pivotal in recent research. However,
during the inference process, LLMs still require substantial resources. In this
paper, we propose CliqueParcel, a method designed to improve the efficiency of
LLMs via prompt batching. Existing strategies to optimize inference efficiency
often compromise on output quality, leading to a discounted output problem.
This issue might result in reduced accuracy or outputs that are less detailed.
CliqueParcel is our answer to this challenge. While ensuring accuracy and
minimizing deviations from the original outputs (i.e., faithfulness), our
method significantly improves efficiency during inference.
<br />To lay the groundwork, we first redefine efficiency measurements by excluding
the reduction in running time due to shorter lengths. Then, we provide a
comprehensive trade-off between efficiency and faithfulness to clarify the
nature of the 'discounted output' problem. Within the CliqueParcel framework,
we suggest multiple batching sub-methods and discuss the specific scenarios in
which they can be applied. During evaluation, CliqueParcel is tested on eight
widely recognized datasets, which can be classified into three types: reading
comprehension, open-source question-answering, and reasoning. Our experiments
explore the performance of CliqueParcel, including efficiency, faithfulness,
and the trade-off between them. This work provides novel insights into
inference efficiency and demonstrates promising performance.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14834" title="Abstract">arXiv:2402.14834</a> [<a href="/pdf/2402.14834" title="Download PDF">pdf</a>, <a href="/format/2402.14834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MSynFD: Multi-hop Syntax aware Fake News Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Liang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chongyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shoujin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Naseem%2C+U">Usman Naseem</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Liang Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">The proliferation of social media platforms has fueled the rapid
dissemination of fake news, posing threats to our real-life society. Existing
methods use multimodal data or contextual information to enhance the detection
of fake news by analyzing news content and/or its social context. However,
these methods often overlook essential textual news content (articles) and
heavily rely on sequential modeling and global attention to extract semantic
information. These existing methods fail to handle the complex, subtle twists
in news articles, such as syntax-semantics mismatches and prior biases, leading
to lower performance and potential failure when modalities or social context
are missing. To bridge these significant gaps, we propose a novel multi-hop
syntax aware fake news detection (MSynFD) method, which incorporates
complementary syntax information to deal with subtle twists in fake news.
Specifically, we introduce a syntactical dependency graph and design a
multi-hop subgraph aggregation mechanism to capture multi-hop syntax. It
extends the effect of word perception, leading to effective noise filtering and
adjacent relation enhancement. Subsequently, a sequential relative
position-aware Transformer is designed to capture the sequential information,
together with an elaborate keyword debiasing module to mitigate the prior bias.
Extensive experimental results on two public benchmark datasets verify the
effectiveness and superior performance of our proposed MSynFD over
state-of-the-art detection models.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14835" title="Abstract">arXiv:2402.14835</a> [<a href="/pdf/2402.14835" title="Download PDF">pdf</a>, <a href="/format/2402.14835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Miaozeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuanyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+N">Nan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haiyun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Siyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+B">Bozhong Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Multimodal knowledge editing represents a critical advancement in enhancing
the capabilities of Multimodal Large Language Models (MLLMs). Despite its
potential, current benchmarks predominantly focus on coarse-grained knowledge,
leaving the intricacies of fine-grained (FG) multimodal entity knowledge
largely unexplored. This gap presents a notable challenge, as FG entity
recognition is pivotal for the practical deployment and effectiveness of MLLMs
in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a
comprehensive benchmark and dataset specifically designed for the FG multimodal
entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess
different perspectives, including Vanilla Name Answering, Entity-Level Caption,
and Complex-Scenario Recognition. In addition, a new form of knowledge editing,
Multi-step Editing, is introduced to evaluate the editing efficiency. Through
our extensive evaluations, we demonstrate that the current state-of-the-art
methods face significant challenges in tackling our proposed benchmark,
underscoring the complexity of FG knowledge editing in MLLMs. Our findings
spotlight the urgent need for novel approaches in this domain, setting a clear
agenda for future research and development efforts within the community.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14836" title="Abstract">arXiv:2402.14836</a> [<a href="/pdf/2402.14836" title="Download PDF">pdf</a>, <a href="/format/2402.14836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stealthy Attack on Large Language Model based Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinghao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuting Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+G">Guibing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Recently, the powerful large language models (LLMs) have been instrumental in
propelling the progress of recommender systems (RS). However, while these
systems have flourished, their susceptibility to security threats has been
largely overlooked. In this work, we reveal that the introduction of LLMs into
recommendation models presents new security vulnerabilities due to their
emphasis on the textual content of items. We demonstrate that attackers can
significantly boost an item's exposure by merely altering its textual content
during the testing phase, without requiring direct interference with the
model's training process. Additionally, the attack is notably stealthy, as it
does not affect the overall recommendation performance and the modifications to
the text are subtle, making it difficult for users and platforms to detect. Our
comprehensive experiments across four mainstream LLM-based recommendation
models demonstrate the superior efficacy and stealthiness of our approach. Our
work unveils a significant security gap in LLM-based recommendation systems and
paves the way for future research on protecting these systems.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14837" title="Abstract">arXiv:2402.14837</a> [<a href="/pdf/2402.14837" title="Download PDF">pdf</a>, <a href="/format/2402.14837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Categorization of Prompting Techniques for Large Language  Models: A Practitioner&#x27;s Guide
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fagbohun%2C+O">Oluwole Fagbohun</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+R+M">Rachel M. Harrison</a>, 
<a href="/search/cs?searchtype=author&query=Dereventsov%2C+A">Anton Dereventsov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Due to rapid advancements in the development of Large Language Models (LLMs),
programming these models with prompts has recently gained significant
attention. However, the sheer number of available prompt engineering techniques
creates an overwhelming landscape for practitioners looking to utilize these
tools. For the most efficient and effective use of LLMs, it is important to
compile a comprehensive list of prompting techniques and establish a
standardized, interdisciplinary categorization framework. In this survey, we
examine some of the most well-known prompting techniques from both academic and
practical viewpoints and classify them into seven distinct categories. We
present an overview of each category, aiming to clarify their unique
contributions and showcase their practical applications in real-world examples
in order to equip fellow practitioners with a structured framework for
understanding and categorizing prompting techniques tailored to their specific
domains. We believe that this approach will help simplify the complex landscape
of prompt engineering and enable more effective utilization of LLMs in various
applications. By providing practitioners with a systematic approach to prompt
categorization, we aim to assist in navigating the intricacies of effective
prompt design for conversational pre-trained LLMs and inspire new possibilities
in their respective fields.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14838" title="Abstract">arXiv:2402.14838</a> [<a href="/pdf/2402.14838" title="Download PDF">pdf</a>, <a href="/format/2402.14838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic  Features for Distinguishing AI-Generated and Human-Written Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rad%2C+M+H">Mohammad Heydari Rad</a>, 
<a href="/search/cs?searchtype=author&query=Farsi%2C+F">Farhan Farsi</a>, 
<a href="/search/cs?searchtype=author&query=Bali%2C+S">Shayan Bali</a>, 
<a href="/search/cs?searchtype=author&query=Etezadi%2C+R">Romina Etezadi</a>, 
<a href="/search/cs?searchtype=author&query=Shamsfard%2C+M">Mehrnoush Shamsfard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Mohammad Heydari Rad, Farhan Farsi, and Shayan Bali have made equal contributions to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs
have been used to generate texts in different languages and for different
tasks. Additionally, due to the participation of remarkable companies such as
Google and OpenAI, LLMs are now more accessible, and people can easily use
them. However, an important issue is how we can detect AI-generated texts from
human-written ones. In this article, we have investigated the problem of
AI-generated text detection from two different aspects: semantics and syntax.
Finally, we presented an AI model that can distinguish AI-generated texts from
human-written ones with high accuracy on both multilingual and monolingual
tasks using the M4 dataset. According to our results, using a semantic approach
would be more helpful for detection. However, there is a lot of room for
improvement in the syntactic approach, and it would be a good approach for
future work.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14840" title="Abstract">arXiv:2402.14840</a> [<a href="/pdf/2402.14840" title="Download PDF">pdf</a>, <a href="/format/2402.14840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question  Answering and Clinical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Congyun Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Ming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaowei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yujiao%2C+L">Li Yujiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yingbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yabo Jia</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuliang Du</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Cong Fan</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+C">Chenfei Chi</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+X">Xiangguo Lv</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fangzhou Li</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+W">Wei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yiran Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Applications (stat.AP)

</div>
<p class="mathjax">Recent advancements in Large Language Models (LLMs) and Large Multi-modal
Models (LMMs) have shown potential in various medical applications, such as
Intelligent Medical Diagnosis. Although impressive results have been achieved,
we find that existing benchmarks do not reflect the complexity of real medical
reports and specialized in-depth reasoning capabilities. In this work, we
introduced RJUA-MedDQA, a comprehensive benchmark in the field of medical
specialization, which poses several challenges: comprehensively interpreting
imgage content across diverse challenging layouts, possessing numerical
reasoning ability to identify abnormal indicators and demonstrating clinical
reasoning ability to provide statements of disease diagnosis, status and advice
based on medical contexts. We carefully design the data generation pipeline and
proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed
at restoring textual and tabular content in medical report images. This method
substantially enhances annotation efficiency, doubling the productivity of each
annotator, and yields a 26.8% improvement in accuracy. We conduct extensive
evaluations, including few-shot assessments of 5 LMMs which are capable of
solving Chinese medical QA tasks. To further investigate the limitations and
potential of current LMMs, we conduct comparative experiments on a set of
strong LLMs by using image-text generated by ESRA method. We report the
performance of baselines and offer several observations: (1) The overall
performance of existing LMMs is still limited; however LMMs more robust to
low-quality and diverse-structured images compared to LLMs. (3) Reasoning
across context and image content present significant challenges. We hope this
benchmark helps the community make progress on these challenging tasks in
multi-modal medical document understanding and facilitate its application in
healthcare.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14843" title="Abstract">arXiv:2402.14843</a> [<a href="/pdf/2402.14843" title="Download PDF">pdf</a>, <a href="/format/2402.14843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Diffusion with Reinforced Conditioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianchi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaohan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haizhen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Feng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion models have demonstrated exceptional capability in generating
high-quality images, videos, and audio. Due to their adaptiveness in iterative
refinement, they provide a strong potential for achieving better
non-autoregressive sequence generation. However, existing text diffusion models
still fall short in their performance due to a challenge in handling the
discreteness of language. This paper thoroughly analyzes text diffusion models
and uncovers two significant limitations: degradation of self-conditioning
during training and misalignment between training and sampling. Motivated by
our findings, we propose a novel Text Diffusion model called TREC, which
mitigates the degradation with Reinforced Conditioning and the misalignment by
Time-Aware Variance Scaling. Our extensive experiments demonstrate the
competitiveness of TREC against autoregressive, non-autoregressive, and
diffusion baselines. Moreover, qualitative analysis shows its advanced ability
to fully utilize the diffusion process in refining samples.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14845" title="Abstract">arXiv:2402.14845</a> [<a href="/pdf/2402.14845" title="Download PDF">pdf</a>, <a href="/format/2402.14845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Purifying Large Language Models by Ensembling a Small Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The emerging success of large language models (LLMs) heavily relies on
collecting abundant training data from external (untrusted) sources. Despite
substantial efforts devoted to data cleaning and curation, well-constructed
LLMs have been reported to suffer from copyright infringement, data poisoning,
and/or privacy violations, which would impede practical deployment of LLMs. In
this study, we propose a simple and easily implementable method for purifying
LLMs from the negative effects caused by uncurated data, namely, through
ensembling LLMs with benign and small language models (SLMs). Aside from
theoretical guarantees, we perform comprehensive experiments to empirically
confirm the efficacy of ensembling LLMs with SLMs, which can effectively
preserve the performance of LLMs while mitigating issues such as copyright
infringement, data poisoning, and privacy violations.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14846" title="Abstract">arXiv:2402.14846</a> [<a href="/pdf/2402.14846" title="Download PDF">pdf</a>, <a href="/format/2402.14846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stick to your Role! Stability of Personal Values Expressed in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kova%C4%8D%2C+G">Grgur Kova&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Portelas%2C+R">R&#xe9;my Portelas</a>, 
<a href="/search/cs?searchtype=author&query=Sawayama%2C+M">Masataka Sawayama</a>, 
<a href="/search/cs?searchtype=author&query=Dominey%2C+P+F">Peter Ford Dominey</a>, 
<a href="/search/cs?searchtype=author&query=Oudeyer%2C+P">Pierre-Yves Oudeyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The project website and code are available at <a href="https://sites.google.com/view/llmvaluestability">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The standard way to study Large Language Models (LLMs) through benchmarks or
psychology questionnaires is to provide many different queries from similar
minimal contexts (e.g. multiple choice questions). However, due to LLM's highly
context-dependent nature, conclusions from such minimal-context evaluations may
be little informative about the model's behavior in deployment (where it will
be exposed to many new contexts). We argue that context-dependence should be
studied as another dimension of LLM comparison alongside others such as
cognitive abilities, knowledge, or model size. In this paper, we present a
case-study about the stability of value expression over different contexts
(simulated conversations on different topics), and as measured using a standard
psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19
open-sourced LLMs from five families. Reusing methods from psychology, we study
Rank-order stability on the population (interpersonal) level, and Ipsative
stability on the individual (intrapersonal) level. We explore two settings:
with and without instructing LLMs to simulate particular personalities. We
observe similar trends in the stability of models and model families - Mixtral,
Mistral and Qwen families being more stable than LLaMa-2 and Phi - over those
two settings, two different simulated populations, and even in the downstream
behavioral task. When instructed to simulate particular personas, LLMs exhibit
low Rank-Order stability, and this stability further diminishes with
conversation length. This highlights the need for future research directions on
LLMs that can coherently simulate a diversity of personas, as well as how
context-dependence can be studied in more thorough and efficient ways. This
paper provides a foundational step in that direction, and, to our knowledge, it
is the first study of value stability in LLMs.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14848" title="Abstract">arXiv:2402.14848</a> [<a href="/pdf/2402.14848" title="Download PDF">pdf</a>, <a href="/format/2402.14848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Same Task, More Tokens: the Impact of Input Length on the Reasoning  Performance of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levy%2C+M">Mosh Levy</a>, 
<a href="/search/cs?searchtype=author&query=Jacoby%2C+A">Alon Jacoby</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper explores the impact of extending input lengths on the capabilities
of Large Language Models (LLMs). Despite LLMs advancements in recent times,
their performance consistency across different input lengths is not well
understood. We investigate this aspect by introducing a novel QA reasoning
framework, specifically designed to assess the impact of input length. We
isolate the effect of input length using multiple versions of the same sample,
each being extended with padding of different lengths, types and locations. Our
findings show a notable degradation in LLMs' reasoning performance at much
shorter input lengths than their technical maximum. We show that the
degradation trend appears in every version of our dataset, although at
different intensities. Additionally, our study reveals that traditional
perplexity metrics do not correlate with performance of LLMs' in long input
reasoning tasks. We analyse our results and identify failure modes that can
serve as useful guides for future research, potentially informing strategies to
address the limitations observed in LLMs.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14849" title="Abstract">arXiv:2402.14849</a> [<a href="/pdf/2402.14849" title="Download PDF">pdf</a>, <a href="/ps/2402.14849" title="Download PostScript">ps</a>, <a href="/format/2402.14849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous and Segmented Bidirectional Encoding for NMT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingpu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zehua Han</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+M">Mengyu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Helin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuxiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Miao Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the rapid advancement of Neural Machine Translation (NMT), enhancing
translation efficiency and quality has become a focal point of research.
Despite the commendable performance of general models such as the Transformer
in various aspects, they still fall short in processing long sentences and
fully leveraging bidirectional contextual information. This paper introduces an
improved model based on the Transformer, implementing an asynchronous and
segmented bidirectional decoding strategy aimed at elevating translation
efficiency and accuracy. Compared to traditional unidirectional translations
from left-to-right or right-to-left, our method demonstrates heightened
efficiency and improved translation quality, particularly in handling long
sentences. Experimental results on the IWSLT2017 dataset confirm the
effectiveness of our approach in accelerating translation and increasing
accuracy, especially surpassing traditional unidirectional strategies in long
sentence translation. Furthermore, this study analyzes the impact of sentence
length on decoding outcomes and explores the model's performance in various
scenarios. The findings of this research not only provide an effective encoding
strategy for the NMT field but also pave new avenues and directions for future
studies.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14850" title="Abstract">arXiv:2402.14850</a> [<a href="/pdf/2402.14850" title="Download PDF">pdf</a>, <a href="/format/2402.14850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHATATC: Large Language Model-Driven Conversational Agents for  Supporting Strategic Air Traffic Flow Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdulhak%2C+S">Sinan Abdulhak</a>, 
<a href="/search/cs?searchtype=author&query=Hubbard%2C+W">Wayne Hubbard</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+K">Karthik Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M+Z">Max Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative artificial intelligence (AI) and large language models (LLMs) have
gained rapid popularity through publicly available tools such as ChatGPT. The
adoption of LLMs for personal and professional use is fueled by the natural
interactions between human users and computer applications such as ChatGPT,
along with powerful summarization and text generation capabilities. Given the
widespread use of such generative AI tools, in this work we investigate how
these tools can be deployed in a non-safety critical, strategic traffic flow
management setting. Specifically, we train an LLM, CHATATC, based on a large
historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023
and consisting of over 80,000 GDP implementations, revisions, and
cancellations. We test the query and response capabilities of CHATATC,
documenting successes (e.g., providing correct GDP rates, durations, and
reason) and shortcomings (e.g,. superlative questions). We also detail the
design of a graphical user interface for future users to interact and
collaborate with the CHATATC conversational agent.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14851" title="Abstract">arXiv:2402.14851</a> [<a href="/pdf/2402.14851" title="Download PDF">pdf</a>, <a href="/format/2402.14851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Hanchen Xia</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Feng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cunxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Guojiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">Modern LLMs have become increasingly powerful, but they are still facing
challenges in specialized tasks such as Text-to-SQL. We propose SQL-CRAFT, a
framework to advance LLMs' SQL generation Capabilities through inteRActive
reFinemenT and enhanced reasoning. We leverage an Interactive Correction Loop
(IC-Loop) for LLMs to interact with databases automatically, as well as
Python-enhanced reasoning. We conduct experiments on two Text-to-SQL datasets,
Spider and Bird, with performance improvements of up to 5.7% compared to the
naive prompting method. Moreover, our method surpasses the current
state-of-the-art on the Spider Leaderboard, demonstrating the effectiveness of
our framework.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14852" title="Abstract">arXiv:2402.14852</a> [<a href="/pdf/2402.14852" title="Download PDF">pdf</a>, <a href="/ps/2402.14852" title="Download PostScript">ps</a>, <a href="/format/2402.14852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HumanEval on Latest GPT Models -- 2024
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daniel Li</a>, 
<a href="/search/cs?searchtype=author&query=Murr%2C+L">Lincoln Murr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In 2023, we are using the latest models of GPT-4 to advance program
synthesis. The large language models have significantly improved the
state-of-the-art for this purpose. To make these advancements more accessible,
we have created a repository that connects these models to Huamn Eval. This
dataset was initally developed to be used with a language model called CODEGEN
on natural and programming language data. The utility of these trained models
is showcased by demonstrating their competitive performance in zero-shot Python
code generation on HumanEval tasks compared to previous state-of-the-art
solutions. Additionally, this gives way to developing more multi-step paradigm
synthesis. This benchmark features 160 diverse problem sets factorized into
multistep prompts that our analysis shows significantly improves program
synthesis over single-turn inputs. All code is open source at
https://github.com/daniel442li/gpt-human-eval .
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14853" title="Abstract">arXiv:2402.14853</a> [<a href="/pdf/2402.14853" title="Download PDF">pdf</a>, <a href="/format/2402.14853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NL2Formula: Generating Spreadsheet Formulas from Natural Language  Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Z">Zhitao Hou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Haoyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yulei Sui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haidong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets,
is a widespread practice among users performing data analysis. However,
crafting formulas on spreadsheets remains a tedious and error-prone task for
many end-users, particularly when dealing with complex operations. To alleviate
the burden associated with writing spreadsheet formulas, this paper introduces
a novel benchmark task called NL2Formula, with the aim to generate executable
formulas that are grounded on a spreadsheet table, given a Natural Language
(NL) query as input. To accomplish this, we construct a comprehensive dataset
consisting of 70,799 paired NL queries and corresponding spreadsheet formulas,
covering 21,670 tables and 37 types of formula functions. We realize the
NL2Formula task by providing a sequence-to-sequence baseline implementation
called fCoder. Experimental results validate the effectiveness of fCoder,
demonstrating its superior performance compared to the baseline models.
Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e.,
text-davinci-003). Lastly, through in-depth error analysis, we identify
potential challenges in the NL2Formula task and advocate for further
investigation.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14854" title="Abstract">arXiv:2402.14854</a> [<a href="/pdf/2402.14854" title="Download PDF">pdf</a>, <a href="/format/2402.14854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dual-Prompting for Interpretable Mental Health Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyolim Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+D">Dongje Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Daeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+S">Sejung Son</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungbae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jinyoung Han</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the Ninth Workshop on Computational Linguistics and
  Clinical Psychology 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the increasing demand for AI-based mental health monitoring tools,
their practical utility for clinicians is limited by the lack of
interpretability.The CLPsych 2024 Shared Task (Chim et al., 2024) aims to
enhance the interpretability of Large Language Models (LLMs), particularly in
mental health analysis, by providing evidence of suicidality through linguistic
content. We propose a dual-prompting approach: (i) Knowledge-aware evidence
extraction by leveraging the expert identity and a suicide dictionary with a
mental health-specific LLM; and (ii) Evidence summarization by employing an
LLM-based consistency evaluator. Comprehensive experiments demonstrate the
effectiveness of combining domain-specific information, revealing performance
improvements and the approach's potential to aid clinicians in assessing mental
state progression.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14855" title="Abstract">arXiv:2402.14855</a> [<a href="/pdf/2402.14855" title="Download PDF">pdf</a>, <a href="/ps/2402.14855" title="Download PostScript">ps</a>, <a href="/format/2402.14855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An LLM Maturity Model for Reliable and Transparent Text-to-Query
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lei Yu</a> (Expression), 
<a href="/search/cs?searchtype=author&query=Ray%2C+A">Abir Ray</a> (Expression)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recognizing the imperative to address the reliability and transparency issues
of Large Language Models (LLM), this work proposes an LLM maturity model
tailored for text-to-query applications. This maturity model seeks to fill the
existing void in evaluating LLMs in such applications by incorporating
dimensions beyond mere correctness or accuracy. Moreover, this work introduces
a real-world use case from the law enforcement domain and showcases QueryIQ, an
LLM-powered, domain-specific text-to-query assistant to expedite user workflows
and reveal hidden relationship in data.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14856" title="Abstract">arXiv:2402.14856</a> [<a href="/pdf/2402.14856" title="Download PDF">pdf</a>, <a href="/format/2402.14856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Inferential Strategies of Humans and Large Language Models in  Deductive Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondorf%2C+P">Philipp Mondorf</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deductive reasoning plays a pivotal role in the formulation of sound and
cohesive arguments. It allows individuals to draw conclusions that logically
follow, given the truth value of the information provided. Recent progress in
the domain of large language models (LLMs) has showcased their capability in
executing deductive reasoning tasks. Nonetheless, a significant portion of
research primarily assesses the accuracy of LLMs in solving such tasks, often
overlooking a deeper analysis of their reasoning behavior. In this study, we
draw upon principles from cognitive psychology to examine inferential
strategies employed by LLMs, through a detailed evaluation of their responses
to propositional logic problems. Our findings indicate that LLMs display
reasoning patterns akin to those observed in humans, including strategies like
$\textit{supposition following}$ or $\textit{chain construction}$. Moreover,
our research demonstrates that the architecture and scale of the model
significantly affect its preferred method of reasoning, with more advanced
models tending to adopt strategies more frequently than less sophisticated
ones. Importantly, we assert that a model's accuracy, that is the correctness
of its final conclusion, does not necessarily reflect the validity of its
reasoning process. This distinction underscores the necessity for more nuanced
evaluation procedures in the field.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14857" title="Abstract">arXiv:2402.14857</a> [<a href="/pdf/2402.14857" title="Download PDF">pdf</a>, <a href="/format/2402.14857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is the System Message Really Important to Jailbreaks in Large Language  Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+X">Xiaotian Zou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongkang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages,3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The rapid evolution of Large Language Models (LLMs) has rendered them
indispensable in modern society. While security measures are typically in place
to align LLMs with human values prior to release, recent studies have unveiled
a concerning phenomenon named "jailbreak." This term refers to the unexpected
and potentially harmful responses generated by LLMs when prompted with
malicious questions. Existing research focuses on generating jailbreak prompts
but our study aim to answer a different question: Is the system message really
important to jailbreak in LLMs? To address this question, we conducted
experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak
prompts with varying system messages: short, long, and none. We discover that
different system messages have distinct resistances to jailbreak by
experiments. Additionally, we explore the transferability of jailbreak across
LLMs. This finding underscores the significant impact system messages can have
on mitigating LLMs jailbreak. To generate system messages that are more
resistant to jailbreak prompts, we propose System Messages Evolutionary
Algorithms (SMEA). Through SMEA, we can get robust system messages population
that demonstrate up to 98.9% resistance against jailbreak prompts. Our research
not only bolsters LLMs security but also raises the bar for jailbreak,
fostering advancements in this field of study.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14858" title="Abstract">arXiv:2402.14858</a> [<a href="/pdf/2402.14858" title="Download PDF">pdf</a>, <a href="/format/2402.14858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatEL: Entity Linking with Chatbots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yifan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qingkai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Weninger%2C+T">Tim Weninger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Entity Linking (EL) is an essential and challenging task in natural language
processing that seeks to link some text representing an entity within a
document or sentence with its corresponding entry in a dictionary or knowledge
base. Most existing approaches focus on creating elaborate contextual models
that look for clues the words surrounding the entity-text to help solve the
linking problem. Although these fine-tuned language models tend to work, they
can be unwieldy, difficult to train, and do not transfer well to other domains.
Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced
solution to the problems inherent in EL models, but simply naive prompts to
LLMs do not work well. In the present work, we define ChatEL, which is a
three-step framework to prompt LLMs to return accurate results. Overall the
ChatEL framework improves the average F1 performance across 10 datasets by more
than 2%. Finally, a thorough error analysis shows many instances with the
ground truth labels were actually incorrect, and the labels predicted by ChatEL
were actually correct. This indicates that the quantitative results presented
in this paper may be a conservative estimate of the actual performance. All
data and code are available as an open-source package on GitHub at
https://github.com/yifding/In_Context_EL.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14859" title="Abstract">arXiv:2402.14859</a> [<a href="/pdf/2402.14859" title="Download PDF">pdf</a>, <a href="/format/2402.14859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Wolf Within: Covert Injection of Malice into MLLM Societies via an  MLLM Operative
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chengshuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Moraffah%2C+R">Raha Moraffah</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifan Li</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Yu Kong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Due to their unprecedented ability to process and respond to various types of
data, Multimodal Large Language Models (MLLMs) are constantly defining the new
boundary of Artificial General Intelligence (AGI). As these advanced generative
models increasingly form collaborative networks for complex tasks, the
integrity and security of these systems are crucial. Our paper, ``The Wolf
Within'', explores a novel vulnerability in MLLM societies - the indirect
propagation of malicious content. Unlike direct harmful output generation for
MLLMs, our research demonstrates how a single MLLM agent can be subtly
influenced to generate prompts that, in turn, induce other MLLM agents in the
society to output malicious content. This subtle, yet potent method of indirect
influence marks a significant escalation in the security risks associated with
MLLMs. Our findings reveal that, with minimal or even no access to MLLMs'
parameters, an MLLM agent, when manipulated to produce specific prompts or
instructions, can effectively ``infect'' other agents within a society of
MLLMs. This infection leads to the generation and circulation of harmful
outputs, such as dangerous instructions or misinformation, across the society.
We also show the transferability of these indirectly generated prompts,
highlighting their possibility in propagating malice through inter-agent
communication. This research provides a critical insight into a new dimension
of threat posed by MLLMs, where a single agent can act as a catalyst for
widespread malevolent influence. Our work underscores the urgent need for
developing robust mechanisms to detect and mitigate such covert manipulations
within MLLM societies, ensuring their safe and ethical utilization in societal
applications. Our implementation is released at
\url{https://github.com/ChengshuaiZhao0/The-Wolf-Within.git}.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14860" title="Abstract">arXiv:2402.14860</a> [<a href="/pdf/2402.14860" title="Download PDF">pdf</a>, <a href="/format/2402.14860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ranking Large Language Models without Ground Truth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhurandhar%2C+A">Amit Dhurandhar</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+R">Rahul Nair</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Moninder Singh</a>, 
<a href="/search/cs?searchtype=author&query=Daly%2C+E">Elizabeth Daly</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Evaluation and ranking of large language models (LLMs) has become an
important problem with the proliferation of these models and their impact.
Evaluation methods either require human responses which are expensive to
acquire or use pairs of LLMs to evaluate each other which can be unreliable. In
this paper, we provide a novel perspective where, given a dataset of prompts
(viz. questions, instructions, etc.) and a set of LLMs, we rank them without
access to any ground truth or reference responses. Inspired by real life where
both an expert and a knowledgeable person can identify a novice our main idea
is to consider triplets of models, where each one of them evaluates the other
two, correctly identifying the worst model in the triplet with high
probability. We also analyze our idea and provide sufficient conditions for it
to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.
In experiments on different generative tasks (summarization, multiple-choice,
and dialog), our methods reliably recover close to true rankings without
reference data. This points to a viable low-resource mechanism for practical
use.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14861" title="Abstract">arXiv:2402.14861</a> [<a href="/pdf/2402.14861" title="Download PDF">pdf</a>, <a href="/format/2402.14861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CloudNine: Analyzing Meteorological Observation Impact on Weather  Prediction Using Explainable Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyeon-Ju Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jeon-Ho Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+I">In-Hyuk Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+O">O-Joun Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">The impact of meteorological observations on weather forecasting varies with
sensor type, location, time, and other environmental factors. Thus,
quantitative analysis of observation impacts is crucial for effective and
efficient development of weather forecasting systems. However, the existing
impact analysis methods are difficult to be widely applied due to their high
dependencies on specific forecasting systems. Also, they cannot provide
observation impacts at multiple spatio-temporal scales, only global impacts of
observation types. To address these issues, we present a novel system called
``CloudNine,'' which allows analysis of individual observations' impacts on
specific predictions based on explainable graph neural networks (XGNNs).
Combining an XGNN-based atmospheric state estimation model with a numerical
weather prediction model, we provide a web application to search for
observations in the 3D space of the Earth system and to visualize the impact of
individual observations on predictions in specific spatial regions and time
periods.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14862" title="Abstract">arXiv:2402.14862</a> [<a href="/pdf/2402.14862" title="Download PDF">pdf</a>, <a href="/format/2402.14862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SISSA: Real-time Monitoring of Hardware Functional Safety and  Cybersecurity with In-vehicle SOME/IP Ethernet Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Ke Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yufeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchen Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Scalable service-Oriented Middleware over IP (SOME/IP) is an Ethernet
communication standard protocol in the Automotive Open System Architecture
(AUTOSAR), promoting ECU-to-ECU communication over the IP stack. However,
SOME/IP lacks a robust security architecture, making it susceptible to
potential attacks. Besides, random hardware failure of ECU will disrupt SOME/IP
communication. In this paper, we propose SISSA, a SOME/IP communication
traffic-based approach for modeling and analyzing in-vehicle functional safety
and cyber security. Specifically, SISSA models hardware failures with the
Weibull distribution and addresses five potential attacks on SOME/IP
communication, including Distributed Denial-of-Services, Man-in-the-Middle, and
abnormal communication processes, assuming a malicious user accesses the
in-vehicle network. Subsequently, SISSA designs a series of deep learning
models with various backbones to extract features from SOME/IP sessions among
ECUs. We adopt residual self-attention to accelerate the model's convergence
and enhance detection accuracy, determining whether an ECU is under attack,
facing functional failure, or operating normally. Additionally, we have created
and annotated a dataset encompassing various classes, including indicators of
attack, functionality, and normalcy. This contribution is noteworthy due to the
scarcity of publicly accessible datasets with such characteristics.Extensive
experimental results show the effectiveness and efficiency of SISSA.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14863" title="Abstract">arXiv:2402.14863</a> [<a href="/pdf/2402.14863" title="Download PDF">pdf</a>, <a href="/format/2402.14863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of a semi-autonomous attentive listening system with takeover  prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kawai%2C+H">Haruki Kawai</a>, 
<a href="/search/cs?searchtype=author&query=Lala%2C+D">Divesh Lala</a>, 
<a href="/search/cs?searchtype=author&query=Inoue%2C+K">Koji Inoue</a>, 
<a href="/search/cs?searchtype=author&query=Ochi%2C+K">Keiko Ochi</a>, 
<a href="/search/cs?searchtype=author&query=Kawahara%2C+T">Tatsuya Kawahara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The handling of communication breakdowns and loss of engagement is an
important aspect of spoken dialogue systems, particularly for chatting systems
such as attentive listening, where the user is mostly speaking. We presume that
a human is best equipped to handle this task and rescue the flow of
conversation. To this end, we propose a semi-autonomous system, where a remote
operator can take control of an autonomous attentive listening system in
real-time. In order to make human intervention easy and consistent, we
introduce automatic detection of low interest and engagement to provide
explicit takeover prompts to the remote operator. We implement this
semi-autonomous system which detects takeover points for the operator and
compare it to fully tele-operated and fully autonomous attentive listening
systems. We find that the semi-autonomous system is generally perceived more
positively than the autonomous system. The results suggest that identifying
points of conversation when the user starts to lose interest may help us
improve a fully autonomous dialogue system.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14865" title="Abstract">arXiv:2402.14865</a> [<a href="/pdf/2402.14865" title="Download PDF">pdf</a>, <a href="/format/2402.14865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kaijie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qinlin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruochen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report; 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Evaluation of large language models (LLMs) has raised great concerns in the
community due to the issue of data contamination. Existing work designed
evaluation protocols using well-defined algorithms for specific tasks, which
cannot be easily extended to diverse scenarios. Moreover, current evaluation
benchmarks can only provide the overall benchmark results and cannot support a
fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we
propose meta probing agents (MPA), a general dynamic evaluation protocol
inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal
2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs
the probing and judging agents to automatically transform an original
evaluation problem into a new one following psychometric theory on three basic
cognitive abilities: language understanding, problem solving, and domain
knowledge. These basic abilities are also dynamically configurable, allowing
multifaceted analysis. We conducted extensive evaluations using MPA and found
that most LLMs achieve poorer performance, indicating room for improvement. Our
multifaceted analysis demonstrated the strong correlation between the basic
abilities and an implicit Matthew effect on model size, i.e., larger models
possess stronger correlations of the abilities. MPA can also be used as a data
augmentation approach to enhance LLMs.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14866" title="Abstract">arXiv:2402.14866</a> [<a href="/pdf/2402.14866" title="Download PDF">pdf</a>, <a href="/format/2402.14866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APTQ: Attention-aware Post-Training Mixed-Precision Quantization for  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+Z">Ziyi Guan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hantao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yupeng Su</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+N">Ngai Wong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hao Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, published to DAC 2024: 61st IEEE/ACM Design Automation Conference. (DAC'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) have greatly advanced the natural language
processing paradigm. However, the high computational load and huge model sizes
pose a grand challenge for deployment on edge devices. To this end, we propose
APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs,
which considers not only the second-order information of each layer's weights,
but also, for the first time, the nonlinear effect of attention outputs on the
entire model. We leverage the Hessian trace as a sensitivity metric for
mixed-precision quantization, ensuring an informed precision reduction that
retains model performance. Experiments show APTQ surpasses previous
quantization methods, achieving an average of 4 bit width a 5.22 perplexity
nearly equivalent to full precision in the C4 dataset. In addition, APTQ
attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an
average bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating
its effectiveness to produce high-quality quantized LLMs.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14867" title="Abstract">arXiv:2402.14867</a> [<a href="/pdf/2402.14867" title="Download PDF">pdf</a>, <a href="/ps/2402.14867" title="Download PostScript">ps</a>, <a href="/format/2402.14867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effects of term weighting approach with and without stop words removing  on Arabic text classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alhenawi%2C+E">Esra&#x27;a Alhenawi</a>, 
<a href="/search/cs?searchtype=author&query=Khurma%2C+R+A">Ruba Abu Khurma</a>, 
<a href="/search/cs?searchtype=author&query=Castillo%2C+P+A">Pedro A. Castillo</a>, 
<a href="/search/cs?searchtype=author&query=Arenas%2C+M+G">Maribel G. Arenas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Classifying text is a method for categorizing documents into pre-established
groups. Text documents must be prepared and represented in a way that is
appropriate for the algorithms used for data mining prior to classification. As
a result, a number of term weighting strategies have been created in the
literature to enhance text categorization algorithms' functionality. This study
compares the effects of Binary and Term frequency weighting feature
methodologies on the text's classification method when stop words are
eliminated once and when they are not. In recognition of assessing the effects
of prior weighting of features approaches on classification results in terms of
accuracy, recall, precision, and F-measure values, we used an Arabic data set
made up of 322 documents divided into six main topics (agriculture, economy,
health, politics, science, and sport), each of which contains 50 documents,
with the exception of the health category, which contains 61 documents. The
results demonstrate that for all metrics, the term frequency feature weighting
approach with stop word removal outperforms the binary approach, while for
accuracy, recall, and F-Measure, the binary approach outperforms the TF
approach without stop word removal. However, for precision, the two approaches
produce results that are very similar. Additionally, it is clear from the data
that, using the same phrase weighting approach, stop word removing increases
classification accuracy.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14869" title="Abstract">arXiv:2402.14869</a> [<a href="/pdf/2402.14869" title="Download PDF">pdf</a>, <a href="/format/2402.14869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Harmonics for Low-Cost Jamming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ieropoulos%2C+V">Vasilis Ieropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Anthi%2C+E">Eirini Anthi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the Toulouse Hacking Convention 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">The digitalisation of the modern schooling system has led to multiple schools
and organisations buying similar hardware. Electronic equipment like wireless
microphones, projectors, touchscreen displays etc., have been almost
standardised with a few well-known brands leading the market. This has led to
the adoption of common frequency ranges between brands with many sticking
between 600-670 MHz. The popularity of low-cost computing devices like the
Raspberry Pi which has been used in a plethora of applications has also taken
the path of being used as low-cost transmitters. There have been many
implementations where the Raspberry Pi has been used as the target device but
few cases where the PI is the actual threat. In this paper, we explore the use
of the Raspberry Pi as a stealth radio frequency jamming device to disable
wireless conference microphones. Harmonics were used to achieve frequencies
outside the Pi's transmission frequency by taking advantage of its unfiltered
transmission.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14871" title="Abstract">arXiv:2402.14871</a> [<a href="/pdf/2402.14871" title="Download PDF">pdf</a>, <a href="/format/2402.14871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Based Multi-Agent Generation of Semi-structured Documents from  Semantic Templates in the Public Administration Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Musumeci%2C+E">Emanuele Musumeci</a>, 
<a href="/search/cs?searchtype=author&query=Brienza%2C+M">Michele Brienza</a>, 
<a href="/search/cs?searchtype=author&query=Suriani%2C+V">Vincenzo Suriani</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+D">Daniele Nardi</a>, 
<a href="/search/cs?searchtype=author&query=Bloisi%2C+D+D">Domenico Daniele Bloisi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at HCI INTERNATIONAL 2024 - 26th International Conference on Human-Computer Interaction. Washington Hilton Hotel, Washington DC, USA, 29 June - 4 July 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In the last years' digitalization process, the creation and management of
documents in various domains, particularly in Public Administration (PA), have
become increasingly complex and diverse. This complexity arises from the need
to handle a wide range of document types, often characterized by
semi-structured forms. Semi-structured documents present a fixed set of data
without a fixed format. As a consequence, a template-based solution cannot be
used, as understanding a document requires the extraction of the data
structure. The recent introduction of Large Language Models (LLMs) has enabled
the creation of customized text output satisfying user requests. In this work,
we propose a novel approach that combines the LLMs with prompt engineering and
multi-agent systems for generating new documents compliant with a desired
structure. The main contribution of this work concerns replacing the commonly
used manual prompting with a task description generated by semantic retrieval
from an LLM. The potential of this approach is demonstrated through a series of
experiments and case studies, showcasing its effectiveness in real-world PA
scenarios.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14872" title="Abstract">arXiv:2402.14872</a> [<a href="/pdf/2402.14872" title="Download PDF">pdf</a>, <a href="/format/2402.14872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts  Against Open-source LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Siyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Han Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aishan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E">Ee-Chien Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Large Language Models (LLMs), used in creative writing, code generation, and
translation, generate text based on input sequences but are vulnerable to
jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak
prompt methods use a combination of jailbreak templates followed by questions
to ask to create jailbreak prompts. However, existing jailbreak prompt designs
generally suffer from excessive semantic differences, resulting in an inability
to resist defenses that use simple semantic metrics as thresholds. Jailbreak
prompts are semantically more varied than the original questions used for
queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach
that bypasses LLMs by generating jailbreak prompts that are semantically
similar to the original question. We model the search for jailbreak prompts
that satisfy both semantic similarity and jailbreak validity as a
multi-objective optimization problem and employ a standardized set of genetic
algorithms for generating eligible prompts. Compared to the baseline
AutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%
higher without ONION defense and 85.2% higher with ONION defense. SMJ's better
performance in all three semantic meaningfulness metrics of Jailbreak Prompt,
Similarity, and Outlier, also means that SMJ is resistant to defenses that use
those metrics as thresholds.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14873" title="Abstract">arXiv:2402.14873</a> [<a href="/pdf/2402.14873" title="Download PDF">pdf</a>, <a href="/format/2402.14873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical Report on the Checkfor.ai AI-Generated Text Classifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emi%2C+B">Bradley Emi</a>, 
<a href="/search/cs?searchtype=author&query=Spero%2C+M">Max Spero</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present the Checkfor.ai text classifier, a transformer-based neural
network trained to distinguish text written by large language models from text
written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT
as well as leading commercial AI detection tools with over 9 times lower error
rates on a comprehensive benchmark comprised of ten text domains (student
writing, creative writing, scientific writing, books, encyclopedias, news,
email, scientific papers, short-form Q\&amp;A) and 8 open- and closed-source large
language models. We propose a training algorithm, hard negative mining with
synthetic mirrors, that enables our classifier to achieve orders of magnitude
lower false positive rates on high-data domains such as reviews. Finally, we
show that Checkfor.ai is not biased against nonnative English speakers and
generalizes to domains and models unseen during training.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14874" title="Abstract">arXiv:2402.14874</a> [<a href="/pdf/2402.14874" title="Download PDF">pdf</a>, <a href="/format/2402.14874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distillation Contrastive Decoding: Improving LLMs Reasoning with  Contrastive Decoding and Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phan%2C+P">Phuc Phan</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+H">Hieu Tran</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+L">Long Phan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a straightforward approach called Distillation Contrastive
Decoding (DCD) to enhance the reasoning capabilities of Large Language Models
(LLMs) during inference. In contrast to previous approaches that relied on
smaller amateur models or analysis of hidden state differences, DCD employs
Contrastive Chain-of-thought Prompting and advanced distillation techniques,
including Dropout and Quantization. This approach effectively addresses the
limitations of Contrastive Decoding (CD), which typically requires both an
expert and an amateur model, thus increasing computational resource demands. By
integrating contrastive prompts with distillation, DCD obviates the need for an
amateur model and reduces memory usage. Our evaluations demonstrate that DCD
significantly enhances LLM performance across a range of reasoning benchmarks,
surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14875" title="Abstract">arXiv:2402.14875</a> [<a href="/pdf/2402.14875" title="Download PDF">pdf</a>, <a href="/format/2402.14875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s in a Name? Auditing Large Language Models for Race and Gender  Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haim%2C+A">Amit Haim</a>, 
<a href="/search/cs?searchtype=author&query=Salinas%2C+A">Alejandro Salinas</a>, 
<a href="/search/cs?searchtype=author&query=Nyarko%2C+J">Julian Nyarko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 9 tables, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">We employ an audit design to investigate biases in state-of-the-art large
language models, including GPT-4. In our study, we elicit prompt the models for
advice regarding an individual across a variety of scenarios, such as during
car purchase negotiations or election outcome predictions. We find that the
advice systematically disadvantages names that are commonly associated with
racial minorities and women. Names associated with Black women receive the
least advantageous outcomes. The biases are consistent across 42 prompt
templates and several models, indicating a systemic issue rather than isolated
incidents. While providing numerical, decision-relevant anchors in the prompt
can successfully counteract the biases, qualitative details have inconsistent
effects and may even increase disparities. Our findings underscore the
importance of conducting audits at the point of LLM deployment and
implementation to mitigate their potential for harm against marginalized
communities.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14876" title="Abstract">arXiv:2402.14876</a> [<a href="/pdf/2402.14876" title="Download PDF">pdf</a>, <a href="/ps/2402.14876" title="Download PostScript">ps</a>, <a href="/format/2402.14876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pseudo-Random Generator based on a Photonic Neuromorphic Physical  Unclonable Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dermanis%2C+D">Dimitris Dermanis</a>, 
<a href="/search/cs?searchtype=author&query=Rizomiliotis%2C+P">Panagiotis Rizomiliotis</a>, 
<a href="/search/cs?searchtype=author&query=Bogris%2C+A">Adonis Bogris</a>, 
<a href="/search/cs?searchtype=author&query=Mesaritakis%2C+C">Charis Mesaritakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Optics (physics.optics)

</div>
<p class="mathjax">In this work we provide numerical results concerning a silicon-on-insulator
photonic neuromorphic circuit configured as a physical unclonable function. The
proposed scheme is enhanced with the capability to be operated as an
unconventional deterministic pseudo-random number generator, suitable for
cryptographic applications that alleviates the need for key storage in
non-volatile digital media. The proposed photonic neuromorphic scheme is able
to offer NIST test compatible numbers with an extremely low false
positive/negative probability below 10-14. The proposed scheme offers
multi-functional capabilities due to the fact that it can be simultaneously
used as an integrated photonic accelerator for machine-learning applications
and as a hardware root of trust.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14878" title="Abstract">arXiv:2402.14878</a> [<a href="/pdf/2402.14878" title="Download PDF">pdf</a>, <a href="/format/2402.14878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-efficiency Limits on Training AI Systems using Learning-in-Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zihao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Leugering%2C+J">Johannes Leugering</a>, 
<a href="/search/cs?searchtype=author&query=Cauwenberghs%2C+G">Gert Cauwenberghs</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabartty%2C+S">Shantanu Chakrabartty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Learning-in-memory (LIM) is a recently proposed paradigm to overcome
fundamental memory bottlenecks in training machine learning systems. While
compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e.
energy dissipated due to repeated memory read access) they are agnostic to the
energy dissipated due to repeated memory writes at the precision required for
training (the update-wall), and they don't account for the energy dissipated
when transferring information between short-term and long-term memories (the
consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can
be overcome if the energy barrier of physical memories is adaptively modulated
such that the dynamics of memory updates and consolidation match the Lyapunov
dynamics of gradient-descent training of an AI model. In this paper, we derive
new theoretical lower bounds on energy dissipation when training AI systems
using different LIM approaches. The analysis presented here is model-agnostic
and highlights the trade-off between energy efficiency and the speed of
training. The resulting non-equilibrium energy-efficiency bounds have a similar
flavor as that of Landauer's energy-dissipation bounds. We also extend these
limits by taking into account the number of floating-point operations (FLOPs)
used for training, the size of the AI model, and the precision of the training
parameters. Our projections suggest that the energy-dissipation lower-bound to
train a brain scale AI system (comprising of $10^{15}$ parameters) using LIM is
$10^8 \sim 10^9$ Joules, which is on the same magnitude the Landauer's
adiabatic lower-bound and $6$ to $7$ orders of magnitude lower than the
projections obtained using state-of-the-art AI accelerator hardware
lower-bounds.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14879" title="Abstract">arXiv:2402.14879</a> [<a href="/pdf/2402.14879" title="Download PDF">pdf</a>, <a href="/format/2402.14879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Driving Generative Agents With Their Personality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klinkert%2C+L+J">Lawrence J. Klinkert</a>, 
<a href="/search/cs?searchtype=author&query=Buongiorno%2C+S">Stephanie Buongiorno</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+C">Corey Clark</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Pages, 4 figures, Draft
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This research explores the potential of Large Language Models (LLMs) to
utilize psychometric values, specifically personality information, within the
context of video game character development. Affective Computing (AC) systems
quantify a Non-Player character's (NPC) psyche, and an LLM can take advantage
of the system's information by using the values for prompt generation. The
research shows an LLM can consistently represent a given personality profile,
thereby enhancing the human-like characteristics of game characters.
Repurposing a human examination, the International Personality Item Pool (IPIP)
questionnaire, to evaluate an LLM shows that the model can accurately generate
content concerning the personality provided. Results show that the improvement
of LLM, such as the latest GPT-4 model, can consistently utilize and interpret
a personality to represent behavior.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14880" title="Abstract">arXiv:2402.14880</a> [<a href="/pdf/2402.14880" title="Download PDF">pdf</a>, <a href="/format/2402.14880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Histograms: Leveraging Language Models for Text Dataset  Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reif%2C+E">Emily Reif</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Crystal Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wexler%2C+J">James Wexler</a>, 
<a href="/search/cs?searchtype=author&query=Kahng%2C+M">Minsuk Kahng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Making sense of unstructured text datasets is perennially difficult, yet
increasingly relevant with Large Language Models. Data workers often rely on
dataset summaries, especially distributions of various derived features. Some
features, like toxicity or topics, are relevant to many datasets, but many
interesting features are domain specific: instruments and genres for a music
dataset, or diseases and symptoms for a medical dataset. Accordingly, data
workers often run custom analyses for each dataset, which is cumbersome and
difficult. We present AutoHistograms, a visualization tool leveragingLLMs.
AutoHistograms automatically identifies relevant features, visualizes them with
histograms, and allows the user to interactively query the dataset for
categories of entities and create new histograms. In a user study with 10 data
workers (n=10), we observe that participants can quickly identify insights and
explore the data using AutoHistograms, and conceptualize a broad range of
applicable use cases. Together, this tool and user study contributeto the
growing field of LLM-assisted sensemaking tools.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14881" title="Abstract">arXiv:2402.14881</a> [<a href="/pdf/2402.14881" title="Download PDF">pdf</a>, <a href="/format/2402.14881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study on the Vulnerability of Test Questions against ChatGPT-based  Cheating
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ram%2C+S">Shanker Ram</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 International Conference on Machine Learning and Applications (ICMLA)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 International Conference on Machine Learning and Applications
  (ICMLA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">ChatGPT is a chatbot that can answer text prompts fairly accurately, even
performing very well on postgraduate-level questions. Many educators have found
that their take-home or remote tests and exams are vulnerable to ChatGPT-based
cheating because students may directly use answers provided by tools like
ChatGPT. In this paper, we try to provide an answer to an important question:
how well ChatGPT can answer test questions and how we can detect whether the
questions of a test can be answered correctly by ChatGPT. We generated
ChatGPT's responses to the MedMCQA dataset, which contains over 10,000 medical
school entrance exam questions. We analyzed the responses and uncovered certain
types of questions ChatGPT answers more inaccurately than others. In addition,
we have created a basic natural language processing model to single out the
most vulnerable questions to ChatGPT in a collection of questions or a sample
exam. Our tool can be used by test-makers to avoid ChatGPT-vulnerable test
questions.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14882" title="Abstract">arXiv:2402.14882</a> [<a href="/pdf/2402.14882" title="Download PDF">pdf</a>, <a href="/format/2402.14882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms  with Target Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sumin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jihoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+N">Namwoo Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Mechanisms are essential components designed to perform specific tasks in
various mechanical systems. However, designing a mechanism that satisfies
certain kinematic or quasi-static requirements is a challenging task. The
kinematic requirements may include the workspace of a mechanism, while the
quasi-static requirements of a mechanism may include its torque transmission,
which refers to the ability of the mechanism to transfer power and torque
effectively. In this paper, we propose a deep learning-based generative model
for generating multiple crank-rocker four-bar linkage mechanisms that satisfy
both the kinematic and quasi-static requirements aforementioned. The proposed
model is based on a conditional generative adversarial network (cGAN) with
modifications for mechanism synthesis, which is trained to learn the
relationship between the requirements of a mechanism with respect to linkage
lengths. The results demonstrate that the proposed model successfully generates
multiple distinct mechanisms that satisfy specific kinematic and quasi-static
requirements. To evaluate the novelty of our approach, we provide a comparison
of the samples synthesized by the proposed cGAN, traditional cVAE and NSGA-II.
Our approach has several advantages over traditional design methods. It enables
designers to efficiently generate multiple diverse and feasible design
candidates while exploring a large design space. Also, the proposed model
considers both the kinematic and quasi-static requirements, which can lead to
more efficient and effective mechanisms for real-world use, making it a
promising tool for linkage mechanism design.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14883" title="Abstract">arXiv:2402.14883</a> [<a href="/pdf/2402.14883" title="Download PDF">pdf</a>, <a href="/format/2402.14883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shen Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Liuyi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jinyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">To support various applications, business owners often seek the customized
models that are obtained by fine-tuning a pre-trained LLM through the API
provided by LLM owners or cloud servers. However, this process carries a
substantial risk of model misuse, potentially resulting in severe economic
consequences for business owners. Thus, safeguarding the copyright of these
customized models during LLM fine-tuning has become an urgent practical
requirement, but there are limited existing solutions to provide such
protection. To tackle this pressing issue, we propose a novel watermarking
approach named "Double-I watermark". Specifically, based on the instruct-tuning
data, two types of backdoor data paradigms are introduced with trigger in the
instruction and the input, respectively. By leveraging LLM's learning
capability to incorporate customized backdoor samples into the dataset, the
proposed approach effectively injects specific watermarking information into
the customized model during fine-tuning, which makes it easy to inject and
verify watermarks in commercial scenarios. We evaluate the proposed "Double-I
watermark" under various fine-tuning methods, demonstrating its harmlessness,
robustness, uniqueness, imperceptibility, and validity through both theoretical
analysis and experimental verification.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14886" title="Abstract">arXiv:2402.14886</a> [<a href="/pdf/2402.14886" title="Download PDF">pdf</a>, <a href="/ps/2402.14886" title="Download PostScript">ps</a>, <a href="/format/2402.14886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applying Reinforcement Learning to Optimize Traffic Light Cycles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Son%2C+S">Seungah Son</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Juhee Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Manual optimization of traffic light cycles is a complex and time-consuming
task, necessitating the development of automated solutions. In this paper, we
propose the application of reinforcement learning to optimize traffic light
cycles in real-time. We present a case study using the Simulation Urban
Mobility simulator to train a Deep Q-Network algorithm. The experimental
results showed 44.16% decrease in the average number of Emergency stops,
showing the potential of our approach to reduce traffic congestion and improve
traffic flow. Furthermore, we discuss avenues for future research and
enhancements to the reinforcement learning model.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14888" title="Abstract">arXiv:2402.14888</a> [<a href="/pdf/2402.14888" title="Download PDF">pdf</a>, <a href="/format/2402.14888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient data selection employing Semantic Similarity-based Graph  Structures for model training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petcu%2C+R">Roxana Petcu</a>, 
<a href="/search/cs?searchtype=author&query=Maji%2C+S">Subhadeep Maji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2023 Workshop: Sampling and Optimization in Discrete Space
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent developments in natural language processing (NLP) have highlighted the
need for substantial amounts of data for models to capture textual information
accurately. This raises concerns regarding the computational resources and time
required for training such models. This paper introduces Semantics for data
SAliency in Model performance Estimation (SeSaME). It is an efficient data
sampling mechanism solely based on textual information without passing the data
through a compute-heavy model or other intensive pre-processing
transformations. The application of this approach is demonstrated in the use
case of low-resource automated speech recognition (ASR) models, which
excessively rely on text-to-speech (TTS) calls when using augmented data.
SeSaME learns to categorize new incoming data points into speech recognition
difficulty buckets by employing semantic similarity-based graph structures and
discrete ASR information from homophilous neighbourhoods through message
passing. The results indicate reliable projections of ASR performance, with a
93% accuracy increase when using the proposed method compared to random
predictions, bringing non-trivial information on the impact of textual
representations in speech models. Furthermore, a series of experiments show
both the benefits and challenges of using the ASR information on incoming data
to fine-tune the model. We report a 7% drop in validation loss compared to
random sampling, 7% WER drop with non-local aggregation when evaluating against
a highly difficult dataset, and 1.8% WER drop with local aggregation and high
semantic similarity between datasets.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14889" title="Abstract">arXiv:2402.14889</a> [<a href="/pdf/2402.14889" title="Download PDF">pdf</a>, <a href="/ps/2402.14889" title="Download PostScript">ps</a>, <a href="/format/2402.14889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COBIAS: Contextual Reliability in Bias Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Govil%2C+P">Priyanshul Govil</a>, 
<a href="/search/cs?searchtype=author&query=Bonagiri%2C+V+K">Vamshi Krishna Bonagiri</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+M">Manas Gaur</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+S">Sanorita Dey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) are trained on inherently biased data. Previous
works on debiasing models rely on benchmark datasets to measure model
performance. However, these datasets suffer from several pitfalls due to the
extremely subjective understanding of bias, highlighting a critical need for
contextual exploration. We propose understanding the context of user inputs
with consideration of the diverse situations in which input statements are
possible. This approach would allow for frameworks that foster bias awareness
rather than guardrails that hurt user engagement. Our contribution is twofold:
(i) we create a dataset of 2287 stereotyped statements augmented with points
for adding context; (ii) we develop the Context-Oriented Bias Indicator and
Assessment Score (COBIAS) to assess statements' contextual reliability in
measuring bias. Our metric is a significant predictor of the contextual
reliability of bias-benchmark datasets ($\chi^2=71.02, p&lt;2.2 \cdot 10^{-16})$.
COBIAS can be used to create reliable datasets, resulting in an improvement in
bias mitigation works.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14890" title="Abstract">arXiv:2402.14890</a> [<a href="/pdf/2402.14890" title="Download PDF">pdf</a>, <a href="/format/2402.14890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vygotsky Distance: Measure for Benchmark Task Similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Surkov%2C+M+K">Maxim K. Surkov</a>, 
<a href="/search/cs?searchtype=author&query=Yamshchikov%2C+I+P">Ivan P. Yamshchikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Evaluation plays a significant role in modern natural language processing.
Most modern NLP benchmarks consist of arbitrary sets of tasks that neither
guarantee any generalization potential for the model once applied outside the
test set nor try to minimize the resource consumption needed for model
evaluation. This paper presents a theoretical instrument and a practical
algorithm to calculate similarity between benchmark tasks, we call this
similarity measure "Vygotsky distance". The core idea of this similarity
measure is that it is based on relative performance of the "students" on a
given task, rather that on the properties of the task itself. If two tasks are
close to each other in terms of Vygotsky distance the models tend to have
similar relative performance on them. Thus knowing Vygotsky distance between
tasks one can significantly reduce the number of evaluation tasks while
maintaining a high validation quality. Experiments on various benchmarks,
including GLUE, SuperGLUE, CLUE, and RussianSuperGLUE, demonstrate that a vast
majority of NLP benchmarks could be at least 40% smaller in terms of the tasks
included. Most importantly, Vygotsky distance could also be used for the
validation of new tasks thus increasing the generalization potential of the
future NLP models.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14891" title="Abstract">arXiv:2402.14891</a> [<a href="/pdf/2402.14891" title="Download PDF">pdf</a>, <a href="/format/2402.14891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMBind: A Unified Modality-Task Integration Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+P">Peng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Munan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jinfa Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Q">Qi Song</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+M">Mingjun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While recent progress in multimodal large language models tackles various
modality tasks, they posses limited integration capabilities for complex
multi-modality tasks, consequently constraining the development of the field.
In this work, we take the initiative to explore and propose the LLMBind, a
unified framework for modality task integration, which binds Large Language
Models and corresponding pre-trained task models with task-specific tokens.
Consequently, LLMBind can interpret inputs and produce outputs in versatile
combinations of image, text, video, and audio. Specifically, we introduce a
Mixture-of-Experts technique to enable effective learning for different
multimodal tasks through collaboration among diverse experts. Furthermore, we
create a multi-task dataset comprising 400k instruction data, which unlocks the
ability for interactive visual generation and editing tasks. Extensive
experiments show the effectiveness of our framework across various tasks,
including image, video, audio generation, image segmentation, and image
editing. More encouragingly, our framework can be easily extended to other
modality tasks, showcasing the promising potential of creating a unified AI
agent for modeling universal modalities.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14894" title="Abstract">arXiv:2402.14894</a> [<a href="/pdf/2402.14894" title="Download PDF">pdf</a>, <a href="/format/2402.14894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Ground-Fault Location Method in Distribution Power System  With Distributed Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Caporuscio%2C+M">Mauro Caporuscio</a>, 
<a href="/search/eess?searchtype=author&query=Dupuis%2C+A">Antoine Dupuis</a>, 
<a href="/search/eess?searchtype=author&query=L%C3%B6we%2C+W">Welf L&#xf6;we</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">The recent increase in renewable energy penetration at the distribution level
introduces a multi-directional power flow that outdated traditional fault
location techniques. To this extent, the development of new methods is needed
to ensure fast and accurate fault localization and, hence, strengthen power
system reliability. This paper proposes a data-driven ground fault location
method for the power distribution system. An 11-bus 20 kV power system is
modeled in Matlab/Simulink to simulate ground faults. The faults are generated
at different locations and under various system operational states. Time-domain
faulted three-phase voltages at the system substation are then analyzed with
discrete wavelet transform. Statistical quantities of the processed data are
eventually used to train an Artificial Neural Network (ANN) to find a mapping
between computed voltage features and faults. Specifically, three ANNs allow
the prediction of faulted phase, faulted branch, and fault distance from the
system substation separately. According to the results, the method shows good
potential, with a total relative error of 0,4% for fault distance prediction.
The method is applied to datasets with unknown system states to test
robustness.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14895" title="Abstract">arXiv:2402.14895</a> [<a href="/pdf/2402.14895" title="Download PDF">pdf</a>, <a href="/format/2402.14895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation is Dead, Long Live Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piedboeuf%2C+F">Fr&#xe9;d&#xe9;ric Piedboeuf</a>, 
<a href="/search/cs?searchtype=author&query=Langlais%2C+P">Philippe Langlais</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Textual data augmentation (DA) is a prolific field of study where novel
techniques to create artificial data are regularly proposed, and that has
demonstrated great efficiency on small data settings, at least for text
classification tasks. In this paper, we challenge those results, showing that
classical data augmentation is simply a way of performing better fine-tuning,
and that spending more time fine-tuning before applying data augmentation
negates its effect. This is a significant contribution as it answers several
questions that were left open in recent years, namely~: which DA technique
performs best (all of them as long as they generate data close enough to the
training set as to not impair training) and why did DA show positive results
(facilitates training of network). We furthermore show that zero and few-shot
data generation via conversational agents such as ChatGPT or LLama2 can
increase performances, concluding that this form of data augmentation does
still work, even if classical methods do not.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14897" title="Abstract">arXiv:2402.14897</a> [<a href="/pdf/2402.14897" title="Download PDF">pdf</a>, <a href="/ps/2402.14897" title="Download PostScript">ps</a>, <a href="/format/2402.14897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Thought Unfaithfulness as Disguised Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bentham%2C+O">Oliver Bentham</a>, 
<a href="/search/cs?searchtype=author&query=Stringham%2C+N">Nathan Stringham</a>, 
<a href="/search/cs?searchtype=author&query=Marasovi%C4%87%2C+A">Ana Marasovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding the extent to which Chain-of-Thought (CoT) generations align
with a large language model's (LLM) internal computations is critical for
deciding whether to trust an LLM's output. As a proxy for CoT faithfulness,
<a href="/abs/2307.13702">arXiv:2307.13702</a> propose a metric that measures a model's dependence on its CoT
for producing an answer. Within a single family of proprietary models, they
find that LLMs exhibit a scaling-then-inverse-scaling relationship between
model size and their measure of faithfulness, and that a 13 billion parameter
model exhibits increased faithfulness compared to models ranging from 810
million to 175 billion parameters in size. We evaluate whether these results
generalize as a property of all LLMs. We replicate their experimental setup
with three different families of models and, under specific conditions,
successfully reproduce the scaling trends for CoT faithfulness they report.
However, we discover that simply changing the order of answer choices in the
prompt can reduce the metric by 73 percentage points. The faithfulness metric
is also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about
its validity as a construct for evaluating faithfulness.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14899" title="Abstract">arXiv:2402.14899</a> [<a href="/pdf/2402.14899" title="Download PDF">pdf</a>, <a href="/format/2402.14899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning  Meets Adversarial Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zefeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhen Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+F">Fan Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zifeng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Tresp%2C+V">Volker Tresp</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jindong Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand
images. However, like traditional vision models, they are still vulnerable to
adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely
explored on MLLMs, which not only improves model's performance, but also
enhances model's explainability by giving intermediate reasoning steps.
Nevertheless, there is still a lack of study regarding MLLMs' adversarial
robustness with CoT and an understanding of what the rationale looks like when
MLLMs infer wrong answers with adversarial images. Our research evaluates the
adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT
marginally improves adversarial robustness against existing attack methods.
Moreover, we introduce a novel stop-reasoning attack technique that effectively
bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the
alterations in CoT reasoning when MLLMs confront adversarial images, shedding
light on their reasoning process under adversarial attacks.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14901" title="Abstract">arXiv:2402.14901</a> [<a href="/pdf/2402.14901" title="Download PDF">pdf</a>, <a href="/format/2402.14901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Usage-centric Take on Intent Understanding in E-Commerce
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wendi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Vougiouklis%2C+P">Pavlos Vougiouklis</a>, 
<a href="/search/cs?searchtype=author&query=Steedman%2C+M">Mark Steedman</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Identifying and understanding user intents is a pivotal task for E-Commerce.
Despite its popularity, intent understanding has not been consistently defined
or accurately benchmarked. In this paper, we focus on predicative user intents
as "how a customer uses a product", and pose intent understanding as a natural
language reasoning task, independent of product ontologies. We identify two
weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph, that limit
its capacity to reason about user intents and to recommend diverse useful
products. Following these observations, we introduce a Product Recovery
Benchmark including a novel evaluation framework and an example dataset. We
further validate the above FolkScope weaknesses on this benchmark.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14902" title="Abstract">arXiv:2402.14902</a> [<a href="/pdf/2402.14902" title="Download PDF">pdf</a>, <a href="/format/2402.14902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BIONIB: Blockchain-based IoT using Novelty Index in Bridge Health  Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gadiraju%2C+D+S">Divija Swetha Gadiraju</a>, 
<a href="/search/cs?searchtype=author&query=McMaster%2C+R">Ryan McMaster</a>, 
<a href="/search/cs?searchtype=author&query=Azam%2C+S+E">Saeed Eftekhar Azam</a>, 
<a href="/search/cs?searchtype=author&query=Khazanchi%2C+D">Deepak Khazanchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Bridge health monitoring becomes crucial with the deployment of IoT sensors.
The challenge lies in securely storing vast amounts of data and extracting
useful information to promptly identify unhealthy bridge conditions. To address
this challenge, we propose BIONIB, wherein real-time IoT data is stored on the
blockchain for monitoring bridges. One of the emerging blockchains, EOSIO is
used because of its exceptional scaling capabilities for monitoring the health
of bridges. The approach involves collecting data from IoT sensors and using an
unsupervised machine learning-based technique called the Novelty Index (NI) to
observe meaningful patterns in the data. Smart contracts of EOSIO are used in
implementation because of their efficiency, security, and programmability,
making them well-suited for handling complex transactions and automating
processes within decentralized applications. BIONIB provides secure storage
benefits of blockchain, as well as useful predictions based on the NI.
Performance analysis uses real-time data collected from IoT sensors at the
bridge in healthy and unhealthy states. The data is collected with extensive
experimentation with different loads, climatic conditions, and the health of
the bridge. The performance of BIONIB under varying numbers of sensors and
various numbers of participating blockchain nodes is observed. We observe a
tradeoff between throughput, latency, and computational resources. Storage
efficiency can be increased by manifolds with a slight increase in latency
caused by NI calculation. As latency is not a significant concern in bridge
health applications, the results demonstrate that BIONIB has high throughput,
parallel processing, and high security while efficiently scaled.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14903" title="Abstract">arXiv:2402.14903</a> [<a href="/pdf/2402.14903" title="Download PDF">pdf</a>, <a href="/format/2402.14903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tokenization counts: the impact of tokenization on arithmetic in  frontier LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Aaditya K. Singh</a>, 
<a href="/search/cs?searchtype=author&query=Strouse%2C+D">DJ Strouse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Tokenization, the division of input text into input tokens, is an often
overlooked aspect of the large language model (LLM) pipeline and could be the
source of useful or harmful inductive biases. Historically, LLMs have relied on
byte pair encoding, without care to specific input domains. With the increased
use of LLMs for reasoning, various number-specific tokenization schemes have
been adopted, with popular models like LLaMa and PaLM opting for single-digit
tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and
3-digit numbers. In this work, we study the effect this choice has on numerical
reasoning through the use of arithmetic tasks. We consider left-to-right and
right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left
tokenization (enforced by comma separating numbers at inference time) leads to
largely improved performance. Furthermore, we find that model errors when using
standard left-to-right tokenization follow stereotyped error patterns,
suggesting that model computations are systematic rather than approximate. We
show that the model is able to convert between tokenizations easily, thus
allowing chain-of-thought-inspired approaches to recover performance on
left-to-right tokenized inputs. We also find the gap between tokenization
directions decreases when models are scaled, possibly indicating that larger
models are better able to override this tokenization-dependent inductive bias.
In summary, our work performs the first study of how number tokenization
choices lead to differences in model performance on arithmetic tasks,
accompanied by a thorough analysis of error patterns. We hope this work
inspires practitioners to more carefully ablate number tokenization-related
choices when working towards general models of numerical reasoning.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14904" title="Abstract">arXiv:2402.14904</a> [<a href="/pdf/2402.14904" title="Download PDF">pdf</a>, <a href="/format/2402.14904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Watermarking Makes Language Models Radioactive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sander%2C+T">Tom Sander</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+P">Pierre Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Durmus%2C+A">Alain Durmus</a>, 
<a href="/search/cs?searchtype=author&query=Douze%2C+M">Matthijs Douze</a>, 
<a href="/search/cs?searchtype=author&query=Furon%2C+T">Teddy Furon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper investigates the radioactivity of LLM-generated texts, i.e.
whether it is possible to detect that such input was used as training data.
Conventional methods like membership inference can carry out this detection
with some level of accuracy. We show that watermarked training data leaves
traces easier to detect and much more reliable than membership inference. We
link the contamination level to the watermark robustness, its proportion in the
training set, and the fine-tuning process. We notably demonstrate that training
on watermarked synthetic instructions can be detected with high confidence
(p-value &lt; 1e-5) even when as little as 5% of training text is watermarked.
Thus, LLM watermarking, originally designed for detecting machine-generated
text, gives the ability to easily identify if the outputs of a watermarked LLM
were used to fine-tune another LLM.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14905" title="Abstract">arXiv:2402.14905</a> [<a href="/pdf/2402.14905" title="Download PDF">pdf</a>, <a href="/format/2402.14905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MobileLLM: Optimizing Sub-billion Parameter Language Models for  On-Device Use Cases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zechun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Changsheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Iandola%2C+F">Forrest Iandola</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+C">Chen Lai</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Fedorov%2C+I">Igor Fedorov</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yunyang Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E">Ernie Chang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yangyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamoorthi%2C+R">Raghuraman Krishnamoorthi</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+L">Liangzhen Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+V">Vikas Chandra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper addresses the growing need for efficient large language models
(LLMs) on mobile devices, driven by increasing cloud costs and latency
concerns. We focus on designing top-quality LLMs with fewer than a billion
parameters, a practical choice for mobile deployment. Contrary to prevailing
belief emphasizing the pivotal role of data and parameter quantity in
determining model quality, our investigation underscores the significance of
model architecture for sub-billion scale LLMs. Leveraging deep and thin
architectures, coupled with embedding sharing and grouped-query attention
mechanisms, we establish a strong baseline network denoted as MobileLLM, which
attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M
state-of-the-art models. Additionally, we propose an immediate block-wise
weight sharing approach with no increase in model size and only marginal
latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a
further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover,
MobileLLM model family shows significant improvements compared to previous
sub-billion models on chat benchmarks, and demonstrates close correctness to
LLaMA-v2 7B in API calling tasks, highlighting the capability of small models
for common on-device use cases.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14922" title="Abstract">arXiv:2402.14922</a> [<a href="/pdf/2402.14922" title="Download PDF">pdf</a>, <a href="/format/2402.14922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Insights into Knowledge Distillation for Pre-Trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alballa%2C+N">Norah Alballa</a>, 
<a href="/search/cs?searchtype=author&query=Canini%2C+M">Marco Canini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This research investigates the enhancement of knowledge distillation (KD)
processes in pre-trained models, an emerging field in knowledge transfer with
significant implications for distributed training and federated learning
environments. These environments benefit from reduced communication demands and
accommodate various model architectures. Despite the adoption of numerous KD
approaches for transferring knowledge among pre-trained models, a comprehensive
understanding of KD's application in these scenarios is lacking. Our study
conducts an extensive comparison of multiple KD techniques, including standard
KD, tuned KD (via optimized temperature and weight parameters), deep mutual
learning, and data partitioning KD. We assess these methods across various data
distribution strategies to identify the most effective contexts for each.
Through detailed examination of hyperparameter tuning, informed by extensive
grid search evaluations, we pinpoint when adjustments are crucial to enhance
model performance. This paper sheds light on optimal hyperparameter settings
for distinct data partitioning scenarios and investigates KD's role in
improving federated learning by minimizing communication rounds and expediting
the training process. By filling a notable void in current research, our
findings serve as a practical framework for leveraging KD in pre-trained models
within collaborative and federated learning frameworks.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14925" title="Abstract">arXiv:2402.14925</a> [<a href="/pdf/2402.14925" title="Download PDF">pdf</a>, <a href="/format/2402.14925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Unbiased Sparsification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barnes%2C+L">Leighton Barnes</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+T">Timothy Chow</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+E">Emma Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Frankston%2C+K">Keith Frankston</a>, 
<a href="/search/cs?searchtype=author&query=Howard%2C+B">Benjamin Howard</a>, 
<a href="/search/cs?searchtype=author&query=Kochman%2C+F">Fred Kochman</a>, 
<a href="/search/cs?searchtype=author&query=Scheinerman%2C+D">Daniel Scheinerman</a>, 
<a href="/search/cs?searchtype=author&query=VanderKam%2C+J">Jeffrey VanderKam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a random
vector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m&lt;n$ nonzero
coordinates. Unbiased sparsification compresses the original vector without
introducing bias; it arises in various contexts, such as in federated learning
and sampling sparse probability distributions. Ideally, unbiased sparsification
should also minimize the expected value of a divergence function
$\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If
$Q$ is optimal in this sense, then we call it efficient. Our main results
describe efficient unbiased sparsifications for divergences that are either
permutation-invariant or additively separable. Surprisingly, the
characterization for permutation-invariant divergences is robust to the choice
of divergence function, in the sense that our class of optimal $Q$ for squared
Euclidean distance coincides with our class of optimal $Q$ for Kullback-Leibler
divergence, or indeed any of a wide variety of divergences.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14926" title="Abstract">arXiv:2402.14926</a> [<a href="/pdf/2402.14926" title="Download PDF">pdf</a>, <a href="/ps/2402.14926" title="Download PostScript">ps</a>, <a href="/format/2402.14926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting gets full Attention for Relational Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guillame-Bert%2C+M">Mathieu Guillame-Bert</a>, 
<a href="/search/cs?searchtype=author&query=Nock%2C+R">Richard Nock</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">More often than not in benchmark supervised ML, tabular data is flat, i.e.
consists of a single $m \times d$ (rows, columns) file, but cases abound in the
real world where observations are described by a set of tables with structural
relationships. Neural nets-based deep models are a classical fit to incorporate
general topological dependence among description features (pixels, words,
etc.), but their suboptimality to tree-based models on tabular data is still
well documented. In this paper, we introduce an attention mechanism for
structured data that blends well with tree-based models in the training context
of (gradient) boosting. Each aggregated model is a tree whose training involves
two steps: first, simple tabular models are learned descending tables in a
top-down fashion with boosting's class residuals on tables' features. Second,
what has been learned progresses back bottom-up via attention and aggregation
mechanisms, progressively crafting new features that complete at the end the
set of observation features over which a single tree is learned, boosting's
iteration clock is incremented and new class residuals are computed.
Experiments on simulated and real-world domains display the competitiveness of
our method against a state of the art containing both tree-based and neural
nets-based models.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14927" title="Abstract">arXiv:2402.14927</a> [<a href="/pdf/2402.14927" title="Download PDF">pdf</a>, <a href="/format/2402.14927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hitting Meets Packing: How Hard Can it Be?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Focke%2C+J">Jacob Focke</a>, 
<a href="/search/cs?searchtype=author&query=Frei%2C+F">Fabian Frei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaohua Li</a>, 
<a href="/search/cs?searchtype=author&query=Marx%2C+D">D&#xe1;niel Marx</a>, 
<a href="/search/cs?searchtype=author&query=Schepper%2C+P">Philipp Schepper</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Roohani Sharma</a>, 
<a href="/search/cs?searchtype=author&query=W%C4%99grzycki%2C+K">Karol W&#x119;grzycki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study a general family of problems that form a common generalization of
classic hitting (also referred to as covering or transversal) and packing
problems. An instance of X-HitPack asks: Can removing k (deletable) vertices of
a graph G prevent us from packing $\ell$ vertex-disjoint objects of type X?
This problem captures a spectrum of problems with standard hitting and packing
on opposite ends. Our main motivating question is whether the combination
X-HitPack can be significantly harder than these two base problems. Already for
a particular choice of X, this question can be posed for many different
complexity notions, leading to a large, so-far unexplored domain in the
intersection of the areas of hitting and packing problems.
<br />On a high-level, we present two case studies: (1) X being all cycles, and (2)
X being all copies of a fixed graph H. In each, we explore the classical
complexity, as well as the parameterized complexity with the natural parameters
k+l and treewidth. We observe that the combined problem can be drastically
harder than the base problems: for cycles or for H being a connected graph with
at least 3 vertices, the problem is \Sigma_2^P-complete and requires
double-exponential dependence on the treewidth of the graph (assuming the
Exponential-Time Hypothesis). In contrast, the combined problem admits
qualitatively similar running times as the base problems in some cases,
although significant novel ideas are required. For example, for X being all
cycles, we establish a 2^poly(k+l)n^O(1) algorithm using an involved branching
method. Also, for X being all edges (i.e., H = K_2; this combines Vertex Cover
and Maximum Matching) the problem can be solved in time 2^\poly(tw)n^O(1) on
graphs of treewidth tw. The key step enabling this running time relies on a
combinatorial bound obtained from an algebraic (linear delta-matroid)
representation of possible matchings.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14928" title="Abstract">arXiv:2402.14928</a> [<a href="/pdf/2402.14928" title="Download PDF">pdf</a>, <a href="/format/2402.14928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Inverse Kinodynamics for Autonomous Vehicle Drifting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suvarna%2C+M">M. Suvarna</a>, 
<a href="/search/cs?searchtype=author&query=Tehrani%2C+O">O. Tehrani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we explore a data-driven learning-based approach to learning
the kinodynamic model of a small autonomous vehicle, and observe the effect it
has on motion planning, specifically autonomous drifting. When executing a
motion plan in the real world, there are numerous causes for error, and what is
planned is often not what is executed on the actual car. Learning a kinodynamic
planner based off of inertial measurements and executed commands can help us
learn the world state. In our case, we look towards the realm of drifting; it
is a complex maneuver that requires a smooth enough surface, high enough speed,
and a drastic change in velocity. We attempt to learn the kinodynamic model for
these drifting maneuvers, and attempt to tighten the slip of the car. Our
approach is able to learn a kinodynamic model for high-speed circular
navigation, and is able to avoid obstacles on an autonomous drift at high speed
by correcting an executed curvature for loose drifts. We seek to adjust our
kinodynamic model for success in tighter drifts in future work.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14929" title="Abstract">arXiv:2402.14929</a> [<a href="/pdf/2402.14929" title="Download PDF">pdf</a>, <a href="/format/2402.14929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Fairness without Access to Sensitive Groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papadaki%2C+A">Afroditi Papadaki</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+N">Natalia Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Bertran%2C+M">Martin Bertran</a>, 
<a href="/search/cs?searchtype=author&query=Sapiro%2C+G">Guillermo Sapiro</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+M">Miguel Rodrigues</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Current approaches to group fairness in federated learning assume the
existence of predefined and labeled sensitive groups during training. However,
due to factors ranging from emerging regulations to dynamics and
location-dependency of protected groups, this assumption may be unsuitable in
many real-world scenarios. In this work, we propose a new approach to guarantee
group fairness that does not rely on any predefined definition of sensitive
groups or additional labels. Our objective allows the federation to learn a
Pareto efficient global model ensuring worst-case group fairness and it
enables, via a single hyper-parameter, trade-offs between fairness and utility,
subject only to a group size constraint. This implies that any sufficiently
large subset of the population is guaranteed to receive at least a minimum
level of utility performance from the model. The proposed objective encompasses
existing approaches as special cases, such as empirical risk minimization and
subgroup robustness objectives from centralized machine learning. We provide an
algorithm to solve this problem in federation that enjoys convergence and
excess risk guarantees. Our empirical results indicate that the proposed
approach can effectively improve the worst-performing group that may be present
without unnecessarily hurting the average performance, exhibits superior or
comparable performance to relevant baselines, and achieves a large set of
solutions with different fairness-utility trade-offs.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14932" title="Abstract">arXiv:2402.14932</a> [<a href="/pdf/2402.14932" title="Download PDF">pdf</a>, <a href="/ps/2402.14932" title="Download PostScript">ps</a>, <a href="/format/2402.14932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attractors of Parikh mapping iterations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chunikhin%2C+A">Alexander Chunikhin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Three types of the Parikh mapping are introduced, namely, alphabetic,
alphabetic-basis and basis. Explicit expressions for attractors of the k-th
order in bases n &gt;= 8, including countable ones, are found. Properties for the
alphabetic, alphabetic-basis and basis Parikh vectors are given at each step of
the Parikh mapping. The maximum number of iterations leading to attractors of
the k-th order in the basis n is determined.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14933" title="Abstract">arXiv:2402.14933</a> [<a href="/pdf/2402.14933" title="Download PDF">pdf</a>, <a href="/format/2402.14933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Path Planning based on 2D Object Bounding-box
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yanliang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Liguo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The implementation of Autonomous Driving (AD) technologies within urban
environments presents significant challenges. These challenges necessitate the
development of advanced perception systems and motion planning algorithms
capable of managing situations of considerable complexity. Although the
end-to-end AD method utilizing LiDAR sensors has achieved significant success
in this scenario, we argue that its drawbacks may hinder its practical
application. Instead, we propose the vision-centric AD as a promising
alternative offering a streamlined model without compromising performance. In
this study, we present a path planning method that utilizes 2D bounding boxes
of objects, developed through imitation learning in urban driving scenarios.
This is achieved by integrating high-definition (HD) map data with images
captured by surrounding cameras. Subsequent perception tasks involve
bounding-box detection and tracking, while the planning phase employs both
local embeddings via Graph Neural Network (GNN) and global embeddings via
Transformer for temporal-spatial feature aggregation, ultimately producing
optimal path planning information. We evaluated our model on the nuPlan
planning task and observed that it performs competitively in comparison to
existing vision-centric methods.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14936" title="Abstract">arXiv:2402.14936</a> [<a href="/pdf/2402.14936" title="Download PDF">pdf</a>, <a href="/format/2402.14936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fast Direct Solver for Elliptic PDEs on a Hierarchy of Adaptively  Refined Quadtrees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chipman%2C+D">Damyn Chipman</a>, 
<a href="/search/math?searchtype=author&query=Calhoun%2C+D">Donna Calhoun</a>, 
<a href="/search/math?searchtype=author&query=Burstedde%2C+C">Carsten Burstedde</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We describe a fast, direct solver for elliptic partial differential equations
on a two-dimensional hierarchy of adaptively refined, Cartesian meshes. Our
solver, inspired by the Hierarchical Poincar\'e-Steklov (HPS) method introduced
by Gillman and Martinsson (SIAM J. Sci. Comput., 2014) uses fast solvers on
locally uniform Cartesian patches stored in the leaves of a quadtree and is the
first such solver that works directly with the adaptive quadtree mesh managed
using the grid management library \pforest (C. Burstedde, L. Wilcox, O.
Ghattas, SIAM J. Sci. Comput. 2011). Within each Cartesian patch, stored in
leaves of the quadtree, we use a second order finite volume discretization on
cell-centered meshes. Key contributions of our algorithm include 4-to-1 merge
and split implementations for the HPS build stage and solve stage,
respectively. We demonstrate our solver on Poisson and Helmholtz problems with
a mesh adapted to the high local curvature of the right-hand side.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14937" title="Abstract">arXiv:2402.14937</a> [<a href="/pdf/2402.14937" title="Download PDF">pdf</a>, <a href="/format/2402.14937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: Analyzing Adversarial Examples: A Framework to Study Adversary  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fenaux%2C+L">Lucas Fenaux</a>, 
<a href="/search/cs?searchtype=author&query=Kerschbaum%2C+F">Florian Kerschbaum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Adversarial examples are malicious inputs to machine learning models that
trigger a misclassification. This type of attack has been studied for close to
a decade, and we find that there is a lack of study and formalization of
adversary knowledge when mounting attacks. This has yielded a complex space of
attack research with hard-to-compare threat models and attacks. We focus on the
image classification domain and provide a theoretical framework to study
adversary knowledge inspired by work in order theory. We present an adversarial
example game, inspired by cryptographic games, to standardize attacks. We
survey recent attacks in the image classification domain and classify their
adversary's knowledge in our framework. From this systematization, we compile
results that both confirm existing beliefs about adversary knowledge, such as
the potency of information about the attacked model as well as allow us to
derive new conclusions on the difficulty associated with the white-box and
transferable threat models, for example, that transferable attacks might not be
as difficult as previously thought.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14947" title="Abstract">arXiv:2402.14947</a> [<a href="/pdf/2402.14947" title="Download PDF">pdf</a>, <a href="/format/2402.14947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Avalanche of Images on Telegram Preceded Russia&#x27;s Full-Scale Invasion  of Ukraine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Theisen%2C+W">William Theisen</a>, 
<a href="/search/cs?searchtype=author&query=Yankoski%2C+M">Michael Yankoski</a>, 
<a href="/search/cs?searchtype=author&query=Hook%2C+K">Kristina Hook</a>, 
<a href="/search/cs?searchtype=author&query=Verdeja%2C+E">Ernesto Verdeja</a>, 
<a href="/search/cs?searchtype=author&query=Scheirer%2C+W">Walter Scheirer</a>, 
<a href="/search/cs?searchtype=author&query=Weninger%2C+T">Tim Weninger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Multimedia (cs.MM); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Governments use propaganda, including through visual content -- or
Politically Salient Image Patterns (PSIP) -- on social media, to influence and
manipulate public opinion. In the present work, we collected Telegram
post-history of from 989 Russian milbloggers to better understand the social
and political narratives that circulated online in the months surrounding
Russia's 2022 full-scale invasion of Ukraine. Overall, we found an 8,925%
increase (p&lt;0.001) in the number of posts and a 5,352% increase (p&lt;0.001) in
the number of images posted by these accounts in the two weeks prior to the
invasion. We also observed a similar increase in the number and intensity of
politically salient manipulated images that circulated on Telegram. Although
this paper does not evaluate malice or coordination in these activities, we do
conclude with a call for further research into the role that manipulated visual
media has in the lead-up to instability events and armed conflict.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14948" title="Abstract">arXiv:2402.14948</a> [<a href="/pdf/2402.14948" title="Download PDF">pdf</a>, <a href="/format/2402.14948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-Examine Distantly Supervised NER: A New Benchmark and a Simple  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuepei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Q">Qiao Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper delves into Named Entity Recognition (NER) under the framework of
Distant Supervision (DS-NER), where the main challenge lies in the compromised
quality of labels due to inherent errors such as false positives, false
negatives, and positive type errors. We critically assess the efficacy of
current DS-NER methodologies using a real-world benchmark dataset named QTL,
revealing that their performance often does not meet expectations. To tackle
the prevalent issue of label noise, we introduce a simple yet effective
approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which
strategically starts on "easy" and cleaner samples during the training process
to enhance model resilience to noisy samples. Our empirical results highlight
the capability of CuPUL to significantly reduce the impact of noisy labels and
outperform existing methods.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14949" title="Abstract">arXiv:2402.14949</a> [<a href="/pdf/2402.14949" title="Download PDF">pdf</a>, <a href="/format/2402.14949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Power Quality Event Classification with AI Transformer Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saber%2C+A+M">Ahmad Mohammad Saber</a>, 
<a href="/search/cs?searchtype=author&query=Youssef%2C+A">Amr Youssef</a>, 
<a href="/search/cs?searchtype=author&query=Svetinovic%2C+D">Davor Svetinovic</a>, 
<a href="/search/cs?searchtype=author&query=Zeineldin%2C+H">Hatem Zeineldin</a>, 
<a href="/search/cs?searchtype=author&query=Kundur%2C+D">Deepa Kundur</a>, 
<a href="/search/cs?searchtype=author&query=El-Saadany%2C+E">Ehab El-Saadany</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the IEEE Power and Energy Society General Meeting, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Recently, there has been a growing interest in utilizing machine learning for
accurate classification of power quality events (PQEs). However, most of these
studies are performed assuming an ideal situation, while in reality, we can
have measurement noise, DC offset, and variations in the voltage signal's
amplitude and frequency. Building on the prior PQE classification works using
deep learning, this paper proposes a deep-learning framework that leverages
attention-enabled Transformers as a tool to accurately classify PQEs under the
aforementioned considerations. The proposed framework can operate directly on
the voltage signals with no need for a separate feature extraction or
calculation phase. Our results show that the proposed framework outperforms
recently proposed learning-based techniques. It can accurately classify PQEs
under the aforementioned conditions with an accuracy varying between
99.81%$-$91.43% depending on the signal-to-noise ratio, DC offsets, and
variations in the signal amplitude and frequency.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14950" title="Abstract">arXiv:2402.14950</a> [<a href="/pdf/2402.14950" title="Download PDF">pdf</a>, <a href="/ps/2402.14950" title="Download PostScript">ps</a>, <a href="/format/2402.14950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Approximate Maximum Flows in Near-Linear Work and  Polylogarithmic Depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Arpit Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+S">Sanjeev Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huan Li</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+P">Prathamesh Patil</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+N">Nathan White</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+P">Peilin Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We present a parallel algorithm for the $(1-\epsilon)$-approximate maximum
flow problem in capacitated, undirected graphs with $n$ vertices and $m$ edges,
achieving $O(\epsilon^{-3}\text{polylog} n)$ depth and $O(m \epsilon^{-3}
\text{polylog} n)$ work in the PRAM model. Although near-linear time sequential
algorithms for this problem have been known for almost a decade, no parallel
algorithms that simultaneously achieved polylogarithmic depth and near-linear
work were known.
<br />At the heart of our result is a polylogarithmic depth, near-linear work
recursive algorithm for computing congestion approximators. Our algorithm
involves a recursive step to obtain a low-quality congestion approximator
followed by a "boosting" step to improve its quality which prevents a
multiplicative blow-up in error. Similar to Peng [SODA'16], our boosting step
builds upon the hierarchical decomposition scheme of R\"acke, Shah, and
T\"aubig [SODA'14].
<br />A direct implementation of this approach, however, leads only to an algorithm
with $n^{o(1)}$ depth and $m^{1+o(1)}$ work. To get around this, we introduce a
new hierarchical decomposition scheme, in which we only need to solve maximum
flows on subgraphs obtained by contracting vertices, as opposed to
vertex-induced subgraphs used in R\"acke, Shah, and T\"aubig [SODA'14]. In
particular, we are able to directly extract congestion approximators for the
subgraphs from a congestion approximator for the entire graph, thereby avoiding
additional recursion on those subgraphs. Along the way, we also develop a
parallel flow-decomposition algorithm that is crucial to achieving
polylogarithmic depth and may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14956" title="Abstract">arXiv:2402.14956</a> [<a href="/pdf/2402.14956" title="Download PDF">pdf</a>, <a href="/format/2402.14956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust mass lumping and outlier removal strategies in isogeometric  analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Voet%2C+Y">Yannis Voet</a>, 
<a href="/search/math?searchtype=author&query=Sande%2C+E">Espen Sande</a>, 
<a href="/search/math?searchtype=author&query=Buffa%2C+A">Annalisa Buffa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 16 figures. Submitted manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Mass lumping techniques are commonly employed in explicit time integration
schemes for problems in structural dynamics and both avoid solving costly
linear systems with the consistent mass matrix and increase the critical time
step. In isogeometric analysis, the critical time step is constrained by
so-called "outlier" frequencies, representing the inaccurate high frequency
part of the spectrum. Removing or dampening these high frequencies is paramount
for fast explicit solution techniques. In this work, we propose robust mass
lumping and outlier removal techniques for nontrivial geometries, including
multipatch and trimmed geometries. Our lumping strategies provably do not
deteriorate (and often improve) the CFL condition of the original problem and
are combined with deflation techniques to remove persistent outlier
frequencies. Numerical experiments reveal the advantages of the method,
especially for simulations covering large time spans where they may halve the
number of iterations with little or no effect on the numerical solution.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14957" title="Abstract">arXiv:2402.14957</a> [<a href="/pdf/2402.14957" title="Download PDF">pdf</a>, <a href="/format/2402.14957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Common Stability Mechanism behind most Self-Supervised Learning  Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jha%2C+A">Abhishek Jha</a>, 
<a href="/search/cs?searchtype=author&query=Blaschko%2C+M+B">Matthew B. Blaschko</a>, 
<a href="/search/cs?searchtype=author&query=Asano%2C+Y+M">Yuki M. Asano</a>, 
<a href="/search/cs?searchtype=author&query=Tuytelaars%2C+T">Tinne Tuytelaars</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Additional visualizations (.gif): <a href="https://github.com/abskjha/CenterVectorSSL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Last couple of years have witnessed a tremendous progress in self-supervised
learning (SSL), the success of which can be attributed to the introduction of
useful inductive biases in the learning process to learn meaningful visual
representations while avoiding collapse. These inductive biases and constraints
manifest themselves in the form of different optimization formulations in the
SSL techniques, e.g. by utilizing negative examples in a contrastive
formulation, or exponential moving average and predictor in BYOL and SimSiam.
In this paper, we provide a framework to explain the stability mechanism of
these different SSL techniques: i) we discuss the working mechanism of
contrastive techniques like SimCLR, non-contrastive techniques like BYOL, SWAV,
SimSiam, Barlow Twins, and DINO; ii) we provide an argument that despite
different formulations these methods implicitly optimize a similar objective
function, i.e. minimizing the magnitude of the expected representation over all
data samples, or the mean of the data distribution, while maximizing the
magnitude of the expected representation of individual samples over different
data augmentations; iii) we provide mathematical and empirical evidence to
support our framework. We formulate different hypotheses and test them using
the Imagenet100 dataset.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14958" title="Abstract">arXiv:2402.14958</a> [<a href="/pdf/2402.14958" title="Download PDF">pdf</a>, <a href="/format/2402.14958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EE3P: Event-based Estimation of Periodic Phenomena Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kol%C3%A1%C5%99%2C+J">Jakub Kol&#xe1;&#x159;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0petl%C3%ADk%2C+R">Radim &#x160;petl&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Matas%2C+J">Ji&#x159;&#xed; Matas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 55 figures, accepted and presented at CVWW24, published in Proceedings of the 27th Computer Vision Winter Workshop, 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 27th Computer Vision Winter Workshop, February
  14-16, 2024, Terme Olimia, Slovenia, pages 66-74, CIP data: COBISS.SI-ID
  185271043 ISBN 978-961-96564-0-2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce a novel method for measuring properties of periodic phenomena
with an event camera, a device asynchronously reporting brightness changes at
independently operating pixels. The approach assumes that for fast periodic
phenomena, in any spatial window where it occurs, a very similar set of events
is generated at the time difference corresponding to the frequency of the
motion. To estimate the frequency, we compute correlations of spatio-temporal
windows in the event space. The period is calculated from the time differences
between the peaks of the correlation responses. The method is contactless,
eliminating the need for markers, and does not need distinguishable landmarks.
We evaluate the proposed method on three instances of periodic phenomena: (i)
light flashes, (ii) vibration, and (iii) rotational speed. In all experiments,
our method achieves a relative error lower than 0.04%, which is within the
error margin of ground truth measurements.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14961" title="Abstract">arXiv:2402.14961</a> [<a href="/pdf/2402.14961" title="Download PDF">pdf</a>, <a href="/format/2402.14961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning with Elastic Time Steps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Beltrame%2C+G">Giovanni Beltrame</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Traditional Reinforcement Learning (RL) algorithms are usually applied in
robotics to learn controllers that act with a fixed control rate. Given the
discrete nature of RL algorithms, they are oblivious to the effects of the
choice of control rate: finding the correct control rate can be difficult and
mistakes often result in excessive use of computing resources or even lack of
convergence.
<br />We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic
algorithm to address this issue. SEAC implements elastic time steps, time steps
with a known, variable duration, which allow the agent to change its control
frequency to adapt to the situation. In practice, SEAC applies control only
when necessary, minimizing computational resources and data usage.
<br />We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze
navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the
SAC baseline in terms of energy efficiency and overall time management, and
most importantly without the need to identify a control frequency for the
learned controller. SEAC demonstrated faster and more stable training speeds
than SAC, especially at control rates where SAC struggled to converge.
<br />We also compared SEAC with a similar approach, the Continuous-Time
Continuous-Options (CTCO) model, and SEAC resulted in better task performance.
These findings highlight the potential of SEAC for practical, real-world RL
applications in robotics.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14963" title="Abstract">arXiv:2402.14963</a> [<a href="/pdf/2402.14963" title="Download PDF">pdf</a>, <a href="/format/2402.14963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hanqi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qinglin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/hanqi-qi/Mirror.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While Large language models (LLMs) have the capability to iteratively reflect
on their own outputs, recent studies have observed their struggles with
knowledge-rich problems without access to external resources. In addition to
the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle
to revisit their predictions despite receiving explicit negative feedback.
Therefore, We propose Mirror, a Multiple-perspective self-reflection method for
knowledge-rich reasoning, to avoid getting stuck at a particular reflection
iteration. Mirror enables LLMs to reflect from multiple-perspective clues,
achieved through a heuristic interaction between a Navigator and a Reasoner. It
guides agents toward diverse yet plausibly reliable reasoning trajectory
without access to ground truth by encouraging (1) diversity of directions
generated by Navigator and (2) agreement among strategically induced
perturbations in responses generated by the Reasoner. The experiments on five
reasoning datasets demonstrate that Mirror's superiority over several
contemporary self-reflection approaches. Additionally, the ablation study
studies clearly indicate that our strategies alleviate the aforementioned
challenges.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14965" title="Abstract">arXiv:2402.14965</a> [<a href="/pdf/2402.14965" title="Download PDF">pdf</a>, <a href="/ps/2402.14965" title="Download PostScript">ps</a>, <a href="/format/2402.14965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Folding polyominoes into cubes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aichholzer%2C+O">Oswin Aichholzer</a>, 
<a href="/search/cs?searchtype=author&query=Lehner%2C+F">Florian Lehner</a>, 
<a href="/search/cs?searchtype=author&query=Lindorfer%2C+C">Christian Lindorfer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Which polyominoes can be folded into a cube, using only creases along edges
of the square lattice underlying the polyomino, with fold angles of $\pm
90^\circ$ and $\pm 180^\circ$, and allowing faces of the cube to be covered
multiple times? Prior results studied tree-shaped polyominoes and polyominoes
with holes and gave partial classifications for these cases.
<br />We show that there is an algorithm deciding whether a given polyomino can be
folded into a cube. This algorithm essentially amounts to trying all possible
ways of mapping faces of the polyomino to faces of the cube, but (perhaps
surprisingly) checking whether such a mapping corresponds to a valid folding is
equivalent to the unlink recognition problem from topology.
<br />We also give further results on classes of polyominoes which can or cannot be
folded into cubes. Our results include (1) a full characterisation of all
tree-shaped polyominoes that can be folded into the cube (2) that any
rectangular polyomino which contains only one simple hole (out of five
different types) does not fold into a cube, (3) a complete characterisation
when a rectangular polyomino with two or more unit square holes (but no other
holes) can be folded into a cube, and (4) a sufficient condition when a
simply-connected polyomino can be folded to a cube.
<br />These results answer several open problems of previous work and close the
cases of tree-shaped polyominoes and rectangular polyominoes with just one
simple hole.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14968" title="Abstract">arXiv:2402.14968</a> [<a href="/pdf/2402.14968" title="Download PDF">pdf</a>, <a href="/format/2402.14968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiongxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiazhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiquan Li</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiangyu Qi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junjie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite the general capabilities of Large Language Models (LLMs) like GPT-4
and Llama-2, these models still request fine-tuning or adaptation with
customized data when it comes to meeting the specific business demands and
intricacies of tailored use cases. However, this process inevitably introduces
new safety threats, particularly against the Fine-tuning based Jailbreak Attack
(FJAttack), where incorporating just a few harmful examples into the
fine-tuning dataset can significantly compromise the model safety. Though
potential defenses have been proposed by incorporating safety examples into the
fine-tuning dataset to reduce the safety issues, such approaches require
incorporating a substantial amount of safety examples, making it inefficient.
To effectively defend against the FJAttack with limited safety examples, we
propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with
the concept of backdoor attacks. In particular, we construct prefixed safety
examples by integrating a secret prompt, acting as a "backdoor trigger", that
is prefixed to safety examples. Our comprehensive experiments demonstrate that
through the Backdoor Enhanced Safety Alignment with adding as few as 11
prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar
safety performance as the original aligned models. Furthermore, we also explore
the effectiveness of our method in a more practical setting where the
fine-tuning data consists of both FJAttack examples and the fine-tuning task
data. Our method shows great efficacy in defending against FJAttack without
harming the performance of fine-tuning tasks.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14970" title="Abstract">arXiv:2402.14970</a> [<a href="/pdf/2402.14970" title="Download PDF">pdf</a>, <a href="/ps/2402.14970" title="Download PostScript">ps</a>, <a href="/format/2402.14970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What comes after optical-bypass network? A study on  optical-computing-enabled network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hai%2C+D+T">Dao Thanh Hai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures, 4 tables; the author's version that has been accepted to Optical Fiber Technology Journal 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">A new architectural paradigm, named, optical-computing-enabled network, is
proposed as a potential evolution of the currently used optical-bypass
framework. The main idea is to leverage the optical computing capabilities
performed on transitional lightpaths at intermediate nodes and such proposal
reverses the conventional wisdom in optical-bypass network, that is, separating
in-transit lightpaths in avoidance of unwanted interference. In
optical-computing-enabled network, the optical nodes are therefore upgraded
from conventional functions of add-drop and cross-connect to include optical
computing / processing capabilities. This is enabled by exploiting the
superposition of in-transit lightpaths for computing purposes to achieve
greater capacity efficiency. While traditional network design and planning
algorithms have been well-developed for optical-bypass framework in which the
routing and resource allocation is dedicated to each optical channel
(lightpath), more complicated problems arise in optical-computing-enabled
architecture as a consequence of intricate interaction between optical channels
and hence resulting into the establishment of the so-called integrated /
computed lightpaths. This necessitates for a different framework of network
design and planning to maximize the impact of optical computing opportunities.
In highlighting this critical point, a detailed case study exploiting the
optical aggregation operation to re-design the optical core network is
investigated in this paper. Numerical results obtained from extensive
simulations on the COST239 network are presented to quantify the efficacy of
optical-computing-enabled approach versus the conventional
optical-bypass-enabled one.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14972" title="Abstract">arXiv:2402.14972</a> [<a href="/pdf/2402.14972" title="Download PDF">pdf</a>, <a href="/ps/2402.14972" title="Download PostScript">ps</a>, <a href="/format/2402.14972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiLS: A Multi-task Lexical Simplification Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=North%2C+K">Kai North</a>, 
<a href="/search/cs?searchtype=author&query=Ranasinghe%2C+T">Tharindu Ranasinghe</a>, 
<a href="/search/cs?searchtype=author&query=Shardlow%2C+M">Matthew Shardlow</a>, 
<a href="/search/cs?searchtype=author&query=Zampieri%2C+M">Marcos Zampieri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Lexical Simplification (LS) automatically replaces difficult to read words
for easier alternatives while preserving a sentence's original meaning. LS is a
precursor to Text Simplification with the aim of improving text accessibility
to various target demographics, including children, second language learners,
individuals with reading disabilities or low literacy. Several datasets exist
for LS. These LS datasets specialize on one or two sub-tasks within the LS
pipeline. However, as of this moment, no single LS dataset has been developed
that covers all LS sub-tasks. We present MultiLS, the first LS framework that
allows for the creation of a multi-task LS dataset. We also present MultiLS-PT,
the first dataset to be created using the MultiLS framework. We demonstrate the
potential of MultiLS-PT by carrying out all LS sub-tasks of (1). lexical
complexity prediction (LCP), (2). substitute generation, and (3). substitute
ranking for Portuguese. Model performances are reported, ranging from
transformer-based models to more recent large language models (LLMs).
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14973" title="Abstract">arXiv:2402.14973</a> [<a href="/pdf/2402.14973" title="Download PDF">pdf</a>, <a href="/format/2402.14973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Lele Cao</a>, 
<a href="/search/cs?searchtype=author&query=Buchner%2C+V">Valentin Buchner</a>, 
<a href="/search/cs?searchtype=author&query=Senane%2C+Z">Zineb Senane</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fangkai Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 (main paper) + 13 (appendix) pages. Source code: <a href="https://github.com/EQTPartners/GenCeption">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Multimodal Large Language Models (MLLMs) are commonly evaluated using costly
annotated multimodal benchmarks. However, these benchmarks often struggle to
keep pace with the rapidly advancing requirements of MLLM evaluation. We
propose GenCeption, a novel and annotation-free MLLM evaluation framework that
merely requires unimodal data to assess inter-modality semantic coherence and
inversely reflects the models' inclination to hallucinate. Analogous to the
popular DrawCeption game, GenCeption initiates with a non-textual sample and
undergoes a series of iterative description and generation steps. Semantic
drift across iterations is quantified using the GC@T metric. Our empirical
findings validate GenCeption's efficacy, showing strong correlations with
popular MLLM benchmarking results. GenCeption may be extended to mitigate
training data contamination by utilizing ubiquitous, previously unseen unimodal
data.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14976" title="Abstract">arXiv:2402.14976</a> [<a href="/pdf/2402.14976" title="Download PDF">pdf</a>, <a href="/format/2402.14976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Domain Adaptation within Deep Foundation Latent Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kangin%2C+D">Dmitry Kangin</a>, 
<a href="/search/cs?searchtype=author&query=Angelov%2C+P">Plamen Angelov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The vision transformer-based foundation models, such as ViT or Dino-V2, are
aimed at solving problems with little or no finetuning of features. Using a
setting of prototypical networks, we analyse to what extent such foundation
models can solve unsupervised domain adaptation without finetuning over the
source or target domain. Through quantitative analysis, as well as qualitative
interpretations of decision making, we demonstrate that the suggested method
can improve upon existing baselines, as well as showcase the limitations of
such approach yet to be solved.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14977" title="Abstract">arXiv:2402.14977</a> [<a href="/pdf/2402.14977" title="Download PDF">pdf</a>, <a href="/format/2402.14977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Reiter%2C+M+K">Michael K. Reiter</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in USENIX Security Symposium, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Foundation model has become the backbone of the AI ecosystem. In particular,
a foundation model can be used as a general-purpose feature extractor to build
various downstream classifiers. However, foundation models are vulnerable to
backdoor attacks and a backdoored foundation model is a single-point-of-failure
of the AI ecosystem, e.g., multiple downstream classifiers inherit the backdoor
vulnerabilities simultaneously. In this work, we propose Mudjacking, the first
method to patch foundation models to remove backdoors. Specifically, given a
misclassified trigger-embedded input detected after a backdoored foundation
model is deployed, Mudjacking adjusts the parameters of the foundation model to
remove the backdoor. We formulate patching a foundation model as an
optimization problem and propose a gradient descent based method to solve it.
We evaluate Mudjacking on both vision and language foundation models, eleven
benchmark datasets, five existing backdoor attacks, and thirteen adaptive
backdoor attacks. Our results show that Mudjacking can remove backdoor from a
foundation model while maintaining its utility.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14978" title="Abstract">arXiv:2402.14978</a> [<a href="/pdf/2402.14978" title="Download PDF">pdf</a>, <a href="/format/2402.14978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Augmented Brainwriting: Investigating the use of LLMs in group  ideation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaer%2C+O">Orit Shaer</a>, 
<a href="/search/cs?searchtype=author&query=Cooper%2C+A">Angelora Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Mokryn%2C+O">Osnat Mokryn</a>, 
<a href="/search/cs?searchtype=author&query=Kun%2C+A+L">Andrew L. Kun</a>, 
<a href="/search/cs?searchtype=author&query=Shoshan%2C+H+B">Hagit Ben Shoshan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">The growing availability of generative AI technologies such as large language
models (LLMs) has significant implications for creative work. This paper
explores twofold aspects of integrating LLMs into the creative process - the
divergence stage of idea generation, and the convergence stage of evaluation
and selection of ideas. We devised a collaborative group-AI Brainwriting
ideation framework, which incorporated an LLM as an enhancement into the group
ideation process, and evaluated the idea generation process and the resulted
solution space. To assess the potential of using LLMs in the idea evaluation
process, we design an evaluation engine and compared it to idea ratings
assigned by three expert and six novice evaluators. Our findings suggest that
integrating LLM in Brainwriting could enhance both the ideation process and its
outcome. We also provide evidence that LLMs can support idea evaluation. We
conclude by discussing implications for HCI education and practice.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14979" title="Abstract">arXiv:2402.14979</a> [<a href="/pdf/2402.14979" title="Download PDF">pdf</a>, <a href="/format/2402.14979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Language Models for Human Preferences is a Causal Inference  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+V">Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ben-Michael%2C+E">Eli Ben-Michael</a>, 
<a href="/search/cs?searchtype=author&query=Morency%2C+L">Louis-Philippe Morency</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Methodology (stat.ME)

</div>
<p class="mathjax">As large language models (LLMs) see greater use in academic and commercial
settings, there is increasing interest in methods that allow language models to
generate texts aligned with human preferences. In this paper, we present an
initial exploration of language model optimization for human preferences from
direct outcome datasets, where each sample consists of a text and an associated
numerical outcome measuring the reader's response. We first propose that
language model optimization should be viewed as a causal problem to ensure that
the model correctly learns the relationship between the text and the outcome.
We formalize this causal language optimization problem, and we develop a
method--causal preference optimization (CPO)--that solves an unbiased surrogate
objective for the problem. We further extend CPO with doubly robust CPO
(DR-CPO), which reduces the variance of the surrogate objective while retaining
provably strong guarantees on bias. Finally, we empirically demonstrate the
effectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human
preferences on direct outcome data, and we validate the robustness of DR-CPO
under difficult confounding conditions.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14982" title="Abstract">arXiv:2402.14982</a> [<a href="/pdf/2402.14982" title="Download PDF">pdf</a>, <a href="/format/2402.14982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Brain Exhibits Distinct Patterns When Listening to Fake Versus  Real Audio: Preliminary Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salehi%2C+M">Mahsa Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Stefanov%2C+K">Kalin Stefanov</a>, 
<a href="/search/cs?searchtype=author&query=Shareghi%2C+E">Ehsan Shareghi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">In this paper we study the variations in human brain activity when listening
to real and fake audio. Our preliminary results suggest that the
representations learned by a state-of-the-art deepfake audio detection
algorithm, do not exhibit clear distinct patterns between real and fake audio.
In contrast, human brain activity, as measured by EEG, displays distinct
patterns when individuals are exposed to fake versus real audio. This
preliminary evidence enables future research directions in areas such as
deepfake audio detection.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14983" title="Abstract">arXiv:2402.14983</a> [<a href="/pdf/2402.14983" title="Download PDF">pdf</a>, <a href="/format/2402.14983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Enhancing Collaborative Information Sharing through Federated  Learning -- A Case of the Insurance Industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+P">Panyi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+Z">Zhiyu Quan</a>, 
<a href="/search/cs?searchtype=author&query=Edwards%2C+B">Brandon Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shih-han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+R">Runhuan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Foley%2C+P">Patrick Foley</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+P">Prashant Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Risk Management (q-fin.RM)

</div>
<p class="mathjax">The report demonstrates the benefits (in terms of improved claims loss
modeling) of harnessing the value of Federated Learning (FL) to learn a single
model across multiple insurance industry datasets without requiring the
datasets themselves to be shared from one company to another. The application
of FL addresses two of the most pressing concerns: limited data volume and data
variety, which are caused by privacy concerns, the rarity of claim events, the
lack of informative rating factors, etc.. During each round of FL,
collaborators compute improvements on the model using their local private data,
and these insights are combined to update a global model. Such aggregation of
insights allows for an increase to the effectiveness in forecasting claims
losses compared to models individually trained at each collaborator.
Critically, this approach enables machine learning collaboration without the
need for raw data to leave the compute infrastructure of each respective data
owner. Additionally, the open-source framework, OpenFL, that is used in our
experiments is designed so that it can be run using confidential computing as
well as with additional algorithmic protections against leakage of information
via the shared model updates. In such a way, FL is implemented as a
privacy-enhancing collaborative learning technique that addresses the
challenges posed by the sensitivity and privacy of data in traditional machine
learning solutions. This paper's application of FL can also be expanded to
other areas including fraud detection, catastrophe modeling, etc., that have a
similar need to incorporate data privacy into machine learning collaborations.
Our framework and empirical results provide a foundation for future
collaborations among insurers, regulators, academic researchers, and InsurTech
experts.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14988" title="Abstract">arXiv:2402.14988</a> [<a href="/pdf/2402.14988" title="Download PDF">pdf</a>, <a href="/format/2402.14988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verifiable Boosted Tree Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Calzavara%2C+S">Stefano Calzavara</a>, 
<a href="/search/cs?searchtype=author&query=Cazzaro%2C+L">Lorenzo Cazzaro</a>, 
<a href="/search/cs?searchtype=author&query=Lucchese%2C+C">Claudio Lucchese</a>, 
<a href="/search/cs?searchtype=author&query=Pibiri%2C+G+E">Giulio Ermanno Pibiri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Logic in Computer Science (cs.LO); Machine Learning (stat.ML)

</div>
<p class="mathjax">Verifiable learning advocates for training machine learning models amenable
to efficient security verification. Prior research demonstrated that specific
classes of decision tree ensembles -- called large-spread ensembles -- allow
for robustness verification in polynomial time against any norm-based attacker.
This study expands prior work on verifiable learning from basic ensemble
methods (i.e., hard majority voting) to advanced boosted tree ensembles, such
as those trained using XGBoost or LightGBM. Our formal results indicate that
robustness verification is achievable in polynomial time when considering
attackers based on the $L_\infty$-norm, but remains NP-hard for other
norm-based attackers. Nevertheless, we present a pseudo-polynomial time
algorithm to verify robustness against attackers based on the $L_p$-norm for
any $p \in \mathbb{N} \cup \{0\}$, which in practice grants excellent
performance. Our experimental evaluation shows that large-spread boosted
ensembles are accurate enough for practical adoption, while being amenable to
efficient security verification.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14989" title="Abstract">arXiv:2402.14989</a> [<a href="/pdf/2402.14989" title="Download PDF">pdf</a>, <a href="/format/2402.14989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Neural Stochastic Differential Equations in Analyzing Irregular  Time Series Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+Y">YongKyung Oh</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+D">Dongyoung Lim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungil Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024, Spotlight presentation (Notable Top 5%). <a href="https://openreview.net/forum?id=4VIgNuQ1pY">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Irregular sampling intervals and missing values in real-world time series
data present challenges for conventional methods that assume consistent
intervals and complete data. Neural Ordinary Differential Equations (Neural
ODEs) offer an alternative approach, utilizing neural networks combined with
ODE solvers to learn continuous latent representations through parameterized
vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend
Neural ODEs by incorporating a diffusion term, although this addition is not
trivial, particularly when addressing irregular intervals and missing values.
Consequently, careful design of drift and diffusion functions is crucial for
maintaining stability and enhancing performance, while incautious choices can
result in adverse properties such as the absence of strong solutions,
stochastic destabilization, or unstable Euler discretizations, significantly
affecting Neural SDEs' performance. In this study, we propose three stable
classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE.
Then, we rigorously demonstrate their robustness in maintaining excellent
performance under distribution shift, while effectively preventing overfitting.
To assess the effectiveness of our approach, we conduct extensive experiments
on four benchmark datasets for interpolation, forecasting, and classification
tasks, and analyze the robustness of our methods with 30 public datasets under
different missing rates. Our results demonstrate the efficacy of the proposed
method in handling real-world irregular time series data.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14991" title="Abstract">arXiv:2402.14991</a> [<a href="/pdf/2402.14991" title="Download PDF">pdf</a>, <a href="/format/2402.14991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Theory and Application of Contextual Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mariella%2C+N">Nicola Mariella</a>, 
<a href="/search/cs?searchtype=author&query=Akhriev%2C+A">Albert Akhriev</a>, 
<a href="/search/cs?searchtype=author&query=Tacchino%2C+F">Francesco Tacchino</a>, 
<a href="/search/cs?searchtype=author&query=Zoufal%2C+C">Christa Zoufal</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez-Espitia%2C+J+C">Juan Carlos Gonzalez-Espitia</a>, 
<a href="/search/cs?searchtype=author&query=Harsanyi%2C+B">Benedek Harsanyi</a>, 
<a href="/search/cs?searchtype=author&query=Koskin%2C+E">Eugene Koskin</a>, 
<a href="/search/cs?searchtype=author&query=Tavernelli%2C+I">Ivano Tavernelli</a>, 
<a href="/search/cs?searchtype=author&query=Woerner%2C+S">Stefan Woerner</a>, 
<a href="/search/cs?searchtype=author&query=Rapsomaniki%2C+M">Marianna Rapsomaniki</a>, 
<a href="/search/cs?searchtype=author&query=Zhuk%2C+S">Sergiy Zhuk</a>, 
<a href="/search/cs?searchtype=author&query=Born%2C+J">Jannis Born</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Emerging Technologies (cs.ET); Quantum Algebra (math.QA); Quantitative Methods (q-bio.QM); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Optimal Transport (OT) has fueled machine learning (ML) applications across
many domains. In cases where paired data measurements ($\mu$, $\nu$) are
coupled to a context variable $p_i$ , one may aspire to learn a global
transportation map that can be parameterized through a potentially unseen
con-text. Existing approaches utilize Neural OT and largely rely on Brenier's
theorem. Here, we propose a first-of-its-kind quantum computing formulation for
amortized optimization of contextualized transportation plans. We exploit a
direct link between doubly stochastic matrices and unitary operators thus
finding a natural connection between OT and quantum computation. We verify our
method on synthetic and real data, by predicting variations in cell type
distributions parameterized through drug dosage as context. Our comparisons to
several baselines reveal that our method can capture dose-induced variations in
cell distributions, even to some extent when dosages are extrapolated and
sometimes with performance similar to the best classical models. In summary,
this is a first step toward learning to predict contextualized transportation
plans through quantum.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14992" title="Abstract">arXiv:2402.14992</a> [<a href="/pdf/2402.14992" title="Download PDF">pdf</a>, <a href="/format/2402.14992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> tinyBenchmarks: evaluating LLMs with fewer examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Polo%2C+F+M">Felipe Maia Polo</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+L">Lucas Weber</a>, 
<a href="/search/cs?searchtype=author&query=Choshen%2C+L">Leshem Choshen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuekai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Gongjun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yurochkin%2C+M">Mikhail Yurochkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">The versatility of large language models (LLMs) led to the creation of
diverse benchmarks that thoroughly test a variety of language models'
abilities. These benchmarks consist of tens of thousands of examples making
evaluation of LLMs very expensive. In this paper, we investigate strategies to
reduce the number of evaluations needed to assess the performance of an LLM on
several key benchmarks. For example, we show that to accurately estimate the
performance of an LLM on MMLU, a popular multiple-choice QA benchmark
consisting of 14K examples, it is sufficient to evaluate this LLM on 100
curated examples. We release evaluation tools and tiny versions of popular
benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical
analysis demonstrates that these tools and tiny benchmarks are sufficient to
reliably and efficiently reproduce the original evaluation results.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14993" title="Abstract">arXiv:2402.14993</a> [<a href="/pdf/2402.14993" title="Download PDF">pdf</a>, <a href="/format/2402.14993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Laser-to-Vehicle Extrinsic Calibration in Low-Observability Scenarios  for Subsea Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hitchcox%2C+T">Thomas Hitchcox</a>, 
<a href="/search/cs?searchtype=author&query=Forbes%2C+J+R">James Richard Forbes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures. Accepted for publication in IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Laser line scanners are increasingly being used in the subsea industry for
high-resolution mapping and infrastructure inspection. However, calibrating the
3D pose of the scanner relative to the vehicle is a perennial source of
confusion and frustration for industrial surveyors. This work describes three
novel algorithms for laser-to-vehicle extrinsic calibration using naturally
occurring features. Each algorithm makes a different assumption on the quality
of the vehicle trajectory estimate, enabling good calibration results in a wide
range of situations. A regularization technique is used to address
low-observability scenarios frequently encountered in practice with large,
rotationally stable subsea vehicles. Experimental results are provided for two
field datasets, including the recently discovered wreck of the Endurance.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14994" title="Abstract">arXiv:2402.14994</a> [<a href="/pdf/2402.14994" title="Download PDF">pdf</a>, <a href="/format/2402.14994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Element Selection for Energy Efficiency Optimization in RIS-aided  Massive MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Souza%2C+W">Wilson Souza Jr</a>, 
<a href="/search/cs?searchtype=author&query=Marinello%2C+J+C">Jos&#xe9; Carlos Marinello</a>, 
<a href="/search/cs?searchtype=author&query=Abr%C3%A3o%2C+T">Taufik Abr&#xe3;o</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 52 references, 4 tables and 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This chapter delves into the critical aspects of optimizing energy efficiency
(EE) in active reconfigurable intelligent surface (RIS)-assisted massive MIMO
(M-MIMO) wireless communication systems. We develop a comprehensive and unified
theoretical framework to analyze the boundaries of EE within M-MIMO systems
integrated with active RIS while adhering to practical constraints. Our
research focuses on a formulated EE optimization problem aiming to maximize the
EE for active RIS-assisted M-MIMO communication systems. Our goal is to
strategically find the number of active RIS elements for outperforming the EE
attainable by an entirely passive RIS. Besides, the proposed novel solution has
been tailored to the innovative problem. The formulation and solution design
consider analytical optimization techniques, such as lagrangian dual transform
(LDT) and fractional programming (FP) optimization, facilitating the effective
implementation of RIS-aided M-MIMO applications in real-world settings. In
particular, our results show that the proposed algorithm can provide up to 120%
higher EE than the entirely passive RIS. Besides, we found that the active RIS
can operate with less than half of the reflecting elements for the entirely
passive RIS. Finally, in view of active RIS achieving the complete utilization
of amplification power available, it should be equipped with a reasonable
number of reflecting elements above N = 49.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14996" title="Abstract">arXiv:2402.14996</a> [<a href="/pdf/2402.14996" title="Download PDF">pdf</a>, <a href="/ps/2402.14996" title="Download PostScript">ps</a>, <a href="/format/2402.14996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Fairness of Normalized p-Means for Allocating Goods and Chores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eckart%2C+O">Owen Eckart</a>, 
<a href="/search/cs?searchtype=author&query=Psomas%2C+A">Alexandros Psomas</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Paritosh Verma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Allocating items in a fair and economically efficient manner is a central
problem in fair division. We study this problem for agents with additive
preferences, when items are all goods or all chores, divisible or indivisible.
The celebrated notion of Nash welfare is known to produce fair and efficient
allocations for both divisible and indivisible goods; there is no known
analogue for dividing chores. The Nash welfare objective belongs to a large,
parameterized family of objectives called the p-mean welfare functions, which
includes other notable members, like social welfare and egalitarian welfare.
However, among the members of this family, only the Nash welfare produces fair
allocations for goods. Incidentally, Nash welfare is also the only member that
satisfies the axiom of scale invariance, which is crucially associated with its
fairness properties.
<br />We define the class of "normalized p-mean" objectives, which imparts the
missing key axiom of scale invariance to the p-mean family. Our results show
that optimizing the normalized p-mean objectives produces fair and efficient
allocations when the items are goods or chores, divisible or indivisible. For
instance, the normalized p-means gives us an infinite class of objectives that
produce (i) proportional and Pareto efficient allocations for divisible goods,
(ii) approximately proportional and Pareto efficient allocations for divisible
chores, (iii) EF1 and Pareto efficient allocations for indivisible goods for
two agents, and (iv) EF1 and Pareto efficient allocations for indivisible
chores for two agents.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15000" title="Abstract">arXiv:2402.15000</a> [<a href="/pdf/2402.15000" title="Download PDF">pdf</a>, <a href="/format/2402.15000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divide-or-Conquer? Which Part Should You Distill Your LLM?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhuofeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+H">He Bai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aonan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiatao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Vydiswaran%2C+V+V">VG Vinod Vydiswaran</a>, 
<a href="/search/cs?searchtype=author&query=Jaitly%2C+N">Navdeep Jaitly</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent methods have demonstrated that Large Language Models (LLMs) can solve
reasoning tasks better when they are encouraged to solve subtasks of the main
task first. In this paper we devise a similar strategy that breaks down
reasoning tasks into a problem decomposition phase and a problem solving phase
and show that the strategy is able to outperform a single stage solution.
Further, we hypothesize that the decomposition should be easier to distill into
a smaller model compared to the problem solving because the latter requires
large amounts of domain knowledge while the former only requires learning
general problem solving strategies. We propose methods to distill these two
capabilities and evaluate their impact on reasoning outcomes and inference
cost. We find that we can distill the problem decomposition phase and at the
same time achieve good generalization across tasks, datasets, and models.
However, it is harder to distill the problem solving capability without losing
performance and the resulting distilled model struggles with generalization.
These results indicate that by using smaller, distilled problem decomposition
models in combination with problem solving LLMs we can achieve reasoning with
cost-efficient inference and local adaptation.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15002" title="Abstract">arXiv:2402.15002</a> [<a href="/pdf/2402.15002" title="Download PDF">pdf</a>, <a href="/format/2402.15002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CommVQA: Situating Visual Question Answering in Communicative Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naik%2C+N+S">Nandita Shankar Naik</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Kreiss%2C+E">Elisa Kreiss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Current visual question answering (VQA) models tend to be trained and
evaluated on image-question pairs in isolation. However, the questions people
ask are dependent on their informational needs and prior knowledge about the
image content. To evaluate how situating images within naturalistic contexts
shapes visual questions, we introduce CommVQA, a VQA dataset consisting of
images, image descriptions, real-world communicative scenarios where the image
might appear (e.g., a travel website), and follow-up questions and answers
conditioned on the scenario. We show that CommVQA poses a challenge for current
models. Providing contextual information to VQA models improves performance
broadly, highlighting the relevance of situating systems within a communicative
scenario.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15005" title="Abstract">arXiv:2402.15005</a> [<a href="/pdf/2402.15005" title="Download PDF">pdf</a>, <a href="/format/2402.15005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of Machine Learning Classification Algorithms and Application  to the Framingham Heart Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kahouadji%2C+N">Nabil Kahouadji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The use of machine learning algorithms in healthcare can amplify social
injustices and health inequities. While the exacerbation of biases can occur
and compound during the problem selection, data collection, and outcome
definition, this research pertains to some generalizability impediments that
occur during the development and the post-deployment of machine learning
classification algorithms. Using the Framingham coronary heart disease data as
a case study, we show how to effectively select a probability cutoff to convert
a regression model for a dichotomous variable into a classifier. We then
compare the sampling distribution of the predictive performance of eight
machine learning classification algorithms under four training/testing
scenarios to test their generalizability and their potential to perpetuate
biases. We show that both the Extreme Gradient Boosting, and Support Vector
Machine are flawed when trained on an unbalanced dataset. We introduced and
show that the double discriminant scoring of type I is the most generalizable
as it consistently outperforms the other classification algorithms regardless
of the training/testing scenario. Finally, we introduce a methodology to
extract an optimal variable hierarchy for a classification algorithm, and
illustrate it on the overall, male and female Framingham coronary heart disease
data.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15006" title="Abstract">arXiv:2402.15006</a> [<a href="/pdf/2402.15006" title="Download PDF">pdf</a>, <a href="/ps/2402.15006" title="Download PostScript">ps</a>, <a href="/format/2402.15006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> opp/ai: Optimistic Privacy-Preserving AI on Blockchain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=So%2C+C">Cathie So</a>, 
<a href="/search/cs?searchtype=author&query=Conway%2C+K">KD Conway</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaohang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Suning Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kartin Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The convergence of Artificial Intelligence (AI) and blockchain technology is
reshaping the digital world, offering decentralized, secure, and efficient AI
services on blockchain platforms. Despite the promise, the high computational
demands of AI on blockchain raise significant privacy and efficiency concerns.
The Optimistic Privacy-Preserving AI (opp/ai) framework is introduced as a
pioneering solution to these issues, striking a balance between privacy
protection and computational efficiency. The framework integrates
Zero-Knowledge Machine Learning (zkML) for privacy with Optimistic Machine
Learning (opML) for efficiency, creating a hybrid model tailored for blockchain
AI services. This study presents the opp/ai framework, delves into the privacy
features of zkML, and assesses the framework's performance and adaptability
across different scenarios.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15008" title="Abstract">arXiv:2402.15008</a> [<a href="/pdf/2402.15008" title="Download PDF">pdf</a>, <a href="/format/2402.15008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Radiation Surveys in Active Nuclear Facilities with Heterogeneous  Collaborative Mobile Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pryor%2C+M">Mitchell Pryor</a>, 
<a href="/search/cs?searchtype=author&query=Navarro%2C+A">Alex Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Panthi%2C+J">Janak Panthi</a>, 
<a href="/search/cs?searchtype=author&query=Torres%2C+K">Kevin Torres</a>, 
<a href="/search/cs?searchtype=author&query=Tebben%2C+M">Mary Tebben</a>, 
<a href="/search/cs?searchtype=author&query=Meza%2C+D">Daniel Meza</a>, 
<a href="/search/cs?searchtype=author&query=Horan%2C+C">Caleb Horan</a>, 
<a href="/search/cs?searchtype=author&query=Macris%2C+A">Alex Macris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Nuclear facilities must routinely survey their infrastructure for radiation
contamination. Generally, this is done by trained professionals, wearing
personal protective equipment (PPE) that swipe potentially contaminated
surfaces and test the wipes under detectors. This approach leaves personnel
vulnerable to radiation exposure and is not comprehensive. Robots address these
inadequacies, offering a cost-effective solution with negligible downtime. We
present a Robot Radiation Survey System (RRSS): a heterogeneous robot team to
perform comprehensive alpha/beta/gamma radiation surveys. The RRSS system
members, core capabilities, and comprehensive survey plan are addresses in this
paper.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15010" title="Abstract">arXiv:2402.15010</a> [<a href="/pdf/2402.15010" title="Download PDF">pdf</a>, <a href="/format/2402.15010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Important Is Tokenization in French Medical Masked Language Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Labrak%2C+Y">Yanis Labrak</a>, 
<a href="/search/cs?searchtype=author&query=Bazoge%2C+A">Adrien Bazoge</a>, 
<a href="/search/cs?searchtype=author&query=Daille%2C+B">Beatrice Daille</a>, 
<a href="/search/cs?searchtype=author&query=Rouvier%2C+M">Mickael Rouvier</a>, 
<a href="/search/cs?searchtype=author&query=Dufour%2C+R">Richard Dufour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at LREC-Coling 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Subword tokenization has become the prevailing standard in the field of
natural language processing (NLP) over recent years, primarily due to the
widespread utilization of pre-trained language models. This shift began with
Byte-Pair Encoding (BPE) and was later followed by the adoption of
SentencePiece and WordPiece. While subword tokenization consistently
outperforms character and word-level tokenization, the precise factors
contributing to its success remain unclear. Key aspects such as the optimal
segmentation granularity for diverse tasks and languages, the influence of data
sources on tokenizers, and the role of morphological information in
Indo-European languages remain insufficiently explored. This is particularly
pertinent for biomedical terminology, characterized by specific rules governing
morpheme combinations. Despite the agglutinative nature of biomedical
terminology, existing language models do not explicitly incorporate this
knowledge, leading to inconsistent tokenization strategies for common terms. In
this paper, we seek to delve into the complexities of subword tokenization in
French biomedical domain across a variety of NLP tasks and pinpoint areas where
further enhancements can be made. We analyze classical tokenization algorithms,
including BPE and SentencePiece, and introduce an original tokenization
strategy that integrates morpheme-enriched word segmentation into existing
tokenization methods.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15011" title="Abstract">arXiv:2402.15011</a> [<a href="/pdf/2402.15011" title="Download PDF">pdf</a>, <a href="/format/2402.15011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Conversational Brain-Artificial Intelligence Interface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meunier%2C+A">Anja Meunier</a>, 
<a href="/search/cs?searchtype=author&query=%C5%BD%C3%A1k%2C+M+R">Michal Robert &#x17d;&#xe1;k</a>, 
<a href="/search/cs?searchtype=author&query=Munz%2C+L">Lucas Munz</a>, 
<a href="/search/cs?searchtype=author&query=Garkot%2C+S">Sofiya Garkot</a>, 
<a href="/search/cs?searchtype=author&query=Eder%2C+M">Manuel Eder</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiachen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Grosse-Wentrup%2C+M">Moritz Grosse-Wentrup</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages (39 with supplementary meterial), 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">We introduce Brain-Artificial Intelligence Interfaces (BAIs) as a new class
of Brain-Computer Interfaces (BCIs). Unlike conventional BCIs, which rely on
intact cognitive capabilities, BAIs leverage the power of artificial
intelligence to replace parts of the neuro-cognitive processing pipeline. BAIs
allow users to accomplish complex tasks by providing high-level intentions,
while a pre-trained AI agent determines low-level details. This approach
enlarges the target audience of BCIs to individuals with cognitive impairments,
a population often excluded from the benefits of conventional BCIs. We present
the general concept of BAIs and illustrate the potential of this new approach
with a Conversational BAI based on EEG. In particular, we show in an experiment
with simulated phone conversations that the Conversational BAI enables complex
communication without the need to generate language. Our work thus
demonstrates, for the first time, the ability of a speech neuroprosthesis to
enable fluent communication in realistic scenarios with non-invasive
technologies.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15012" title="Abstract">arXiv:2402.15012</a> [<a href="/pdf/2402.15012" title="Download PDF">pdf</a>, <a href="/format/2402.15012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ar-Spider: Text-to-SQL in Arabic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almohaimeed%2C+S">Saleh Almohaimeed</a>, 
<a href="/search/cs?searchtype=author&query=Almohaimeed%2C+S">Saad Almohaimeed</a>, 
<a href="/search/cs?searchtype=author&query=Ghanim%2C+M+A">Mansour Al Ghanim</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liqiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM SAC Conference (SAC 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In Natural Language Processing (NLP), one of the most important tasks is
text-to-SQL semantic parsing, which focuses on enabling users to interact with
the database in a more natural manner. In recent years, text-to-SQL has made
significant progress, but most were English-centric. In this paper, we
introduce Ar-Spider 1, the first Arabic cross-domain text-to-SQL dataset. Due
to the unique nature of the language, two major challenges have been
encountered, namely schema linguistic and SQL structural challenges. In order
to handle these issues and conduct the experiments, we adopt two baseline
models LGESQL [4] and S2SQL [12], both of which are tested with two
cross-lingual models to alleviate the effects of schema linguistic and SQL
structure linking challenges. The baselines demonstrate decent single-language
performance on our Arabic text-to-SQL dataset, Ar-Spider, achieving 62.48% for
S2SQL and 65.57% for LGESQL, only 8.79% below the highest results achieved by
the baselines when trained in English dataset. To achieve better performance on
Arabic text-to-SQL, we propose the context similarity relationship (CSR)
approach, which results in a significant increase in the overall performance of
about 1.52% for S2SQL and 1.06% for LGESQL and closes the gap between Arabic
and English languages to 7.73%.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15013" title="Abstract">arXiv:2402.15013</a> [<a href="/pdf/2402.15013" title="Download PDF">pdf</a>, <a href="/format/2402.15013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Filter Bubble or Homogenization? Disentangling the Long-Term Effects of  Recommendations on User Consumption Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anwar%2C+M+S">Md Sanzeed Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Schoenebeck%2C+G">Grant Schoenebeck</a>, 
<a href="/search/cs?searchtype=author&query=Dhillon%2C+P+S">Paramveer S. Dhillon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was accepted at the ACM Web Conference 2024 (WWW '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Recommendation algorithms play a pivotal role in shaping our media choices,
which makes it crucial to comprehend their long-term impact on user behavior.
These algorithms are often linked to two critical outcomes: homogenization,
wherein users consume similar content despite disparate underlying preferences,
and the filter bubble effect, wherein individuals with differing preferences
only consume content aligned with their preferences (without much overlap with
other users). Prior research assumes a trade-off between homogenization and
filter bubble effects and then shows that personalized recommendations mitigate
filter bubbles by fostering homogenization. However, because of this assumption
of a tradeoff between these two effects, prior work cannot develop a more
nuanced view of how recommendation systems may independently impact
homogenization and filter bubble effects. We develop a more refined definition
of homogenization and the filter bubble effect by decomposing them into two key
metrics: how different the average consumption is between users (inter-user
diversity) and how varied an individual's consumption is (intra-user
diversity). We then use a novel agent-based simulation framework that enables a
holistic view of the impact of recommendation systems on homogenization and
filter bubble effects. Our simulations show that traditional recommendation
algorithms (based on past behavior) mainly reduce filter bubbles by affecting
inter-user diversity without significantly impacting intra-user diversity.
Building on these findings, we introduce two new recommendation algorithms that
take a more nuanced approach by accounting for both types of diversity.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15017" title="Abstract">arXiv:2402.15017</a> [<a href="/pdf/2402.15017" title="Download PDF">pdf</a>, <a href="/format/2402.15017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Few-Shot Adaptation of Foundation Models via Multitask  Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhuoyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhenmei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Junyi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+F">Fangzhou Mu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingyu Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR 2024. 54 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Foundation models have emerged as a powerful tool for many AI problems.
Despite the tremendous success of foundation models, effective adaptation to
new tasks, particularly those with limited labels, remains an open question and
lacks theoretical understanding. An emerging solution with recent success in
vision and NLP involves finetuning a foundation model on a selection of
relevant tasks, before its adaptation to a target task with limited labeled
samples. In this paper, we study the theoretical justification of this
multitask finetuning approach. Our theoretical analysis reveals that with a
diverse set of related tasks, this multitask finetuning leads to reduced error
in the target task, in comparison to directly adapting the same pretrained
model. We quantify the relationship between finetuning tasks and target tasks
by diversity and consistency metrics, and further propose a practical task
selection algorithm. We substantiate our theoretical claims with extensive
empirical evidence. Further, we present results affirming our task selection
algorithm adeptly chooses related finetuning tasks, providing advantages to the
model performance on target tasks. We believe our study shed new light on the
effective adaptation of foundation models to new tasks that lack abundant
labels. Our code is available at
https://github.com/OliverXUZY/Foudation-Model_Multitask.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15018" title="Abstract">arXiv:2402.15018</a> [<a href="/pdf/2402.15018" title="Download PDF">pdf</a>, <a href="/format/2402.15018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unintended Impacts of LLM Alignment on Global Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ryan%2C+M+J">Michael J. Ryan</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+W">William Held</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Before being deployed for user-facing applications, developers align Large
Language Models (LLMs) to user preferences through a variety of procedures,
such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference
Optimization (DPO). Current evaluations of these procedures focus on benchmarks
of instruction following, reasoning, and truthfulness. However, human
preferences are not universal, and aligning to specific preference sets may
have unintended effects. We explore how alignment impacts performance along
three axes of global representation: English dialects, multilingualism, and
opinions from and about countries worldwide. Our results show that current
alignment procedures create disparities between English dialects and global
opinions. We find alignment improves capabilities in several languages. We
conclude by discussing design decisions that led to these unintended impacts
and recommendations for more equitable preference tuning.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15019" title="Abstract">arXiv:2402.15019</a> [<a href="/pdf/2402.15019" title="Download PDF">pdf</a>, <a href="/format/2402.15019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistency-Guided Temperature Scaling Using Style and Content  Information for Out-of-Domain Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+W">Wonjeong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jungwuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dong-Jun Han</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Younghyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">Jaekyun Moon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI-24 (The 38th AAAI Conference on Artificial Intelligence, February 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Research interests in the robustness of deep neural networks against domain
shifts have been rapidly increasing in recent years. Most existing works,
however, focus on improving the accuracy of the model, not the calibration
performance which is another important requirement for trustworthy AI systems.
Temperature scaling (TS), an accuracy-preserving post-hoc calibration method,
has been proven to be effective in in-domain settings, but not in out-of-domain
(OOD) due to the difficulty in obtaining a validation set for the unseen domain
beforehand. In this paper, we propose consistency-guided temperature scaling
(CTS), a new temperature scaling strategy that can significantly enhance the
OOD calibration performance by providing mutual supervision among data samples
in the source domains. Motivated by our observation that over-confidence
stemming from inconsistent sample predictions is the main obstacle to OOD
calibration, we propose to guide the scaling process by taking consistencies
into account in terms of two different aspects -- style and content -- which
are the key components that can well-represent data samples in multi-domain
settings. Experimental results demonstrate that our proposed strategy
outperforms existing works, achieving superior OOD calibration performance on
various datasets. This can be accomplished by employing only the source domains
without compromising accuracy, making our scheme directly applicable to various
trustworthy AI systems.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15020" title="Abstract">arXiv:2402.15020</a> [<a href="/pdf/2402.15020" title="Download PDF">pdf</a>, <a href="/format/2402.15020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistically-sound beam search with masked language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cowen-Breen%2C+C">Charlie Cowen-Breen</a>, 
<a href="/search/cs?searchtype=author&query=Brooks%2C+C">Creston Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Calef%2C+R">Robert Calef</a>, 
<a href="/search/cs?searchtype=author&query=Sappington%2C+A">Anna Sappington</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Beam search with masked language models (MLMs) is challenging in part because
joint probability distributions over sequences are not readily available,
unlike for autoregressive models. Nevertheless, estimating such distributions
has applications in many domains, including protein engineering and ancient
text restoration. We present probabilistically-sound methods for beam search
with MLMs. First, we clarify the conditions under which it is theoretically
sound to perform text infilling with MLMs using standard beam search. When
these conditions fail, we provide a probabilistically-sound modification with
no additional computational complexity and demonstrate that it is superior to
the aforementioned beam search in the expected conditions. We then present
empirical results comparing several infilling approaches with MLMs across
several domains.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15021" title="Abstract">arXiv:2402.15021</a> [<a href="/pdf/2402.15021" title="Download PDF">pdf</a>, <a href="/format/2402.15021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLoVe: Encoding Compositional Language in Contrastive Vision-Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castro%2C+S">Santiago Castro</a>, 
<a href="/search/cs?searchtype=author&query=Ziai%2C+A">Amir Ziai</a>, 
<a href="/search/cs?searchtype=author&query=Saluja%2C+A">Avneesh Saluja</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhuoning Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent years have witnessed a significant increase in the performance of
Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as
CLIP, have been leveraged in multiple settings and demonstrated remarkable
performance across several tasks. Such models excel at object-centric
recognition yet learn text representations that seem invariant to word order,
failing to compose known concepts in novel ways. However, no evidence exists
that any VLM, including large-scale single-stream models such as GPT-4V,
identifies compositions successfully. In this paper, we introduce a framework
to significantly improve the ability of existing models to encode compositional
language, with over 10% absolute improvement on compositionality benchmarks,
while maintaining or improving the performance on standard object-recognition
and retrieval benchmarks. Our code and pre-trained models are publicly
available at https://github.com/netflix/clove.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15025" title="Abstract">arXiv:2402.15025</a> [<a href="/pdf/2402.15025" title="Download PDF">pdf</a>, <a href="/format/2402.15025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practice Makes Perfect: Planning to Learn Skill Parameter Policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Nishanth Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Silver%2C+T">Tom Silver</a>, 
<a href="/search/cs?searchtype=author&query=McClinton%2C+W">Willie McClinton</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Linfeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Proulx%2C+S">Stephen Proulx</a>, 
<a href="/search/cs?searchtype=author&query=Lozano-P%C3%A9rez%2C+T">Tom&#xe1;s Lozano-P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Kaelbling%2C+L+P">Leslie Pack Kaelbling</a>, 
<a href="/search/cs?searchtype=author&query=Barry%2C+J">Jennifer Barry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">One promising approach towards effective robot decision making in complex,
long-horizon tasks is to sequence together parameterized skills. We consider a
setting where a robot is initially equipped with (1) a library of parameterized
skills, (2) an AI planner for sequencing together the skills given a goal, and
(3) a very general prior distribution for selecting skill parameters. Once
deployed, the robot should rapidly and autonomously learn to improve its
performance by specializing its skill parameter selection policy to the
particular objects, goals, and constraints in its environment. In this work, we
focus on the active learning problem of choosing which skills to practice to
maximize expected future task success. We propose that the robot should
estimate the competence of each skill, extrapolate the competence (asking: "how
much would the competence improve through practice?"), and situate the skill in
the task distribution through competence-aware planning. This approach is
implemented within a fully autonomous system where the robot repeatedly plans,
practices, and learns without any environment resets. Through experiments in
simulation, we find that our approach learns effective parameter policies more
sample-efficiently than several baselines. Experiments in the real-world
demonstrate our approach's ability to handle noise from perception and control
and improve the robot's ability to solve two long-horizon mobile-manipulation
tasks after a few hours of autonomous practice.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15027" title="Abstract">arXiv:2402.15027</a> [<a href="/pdf/2402.15027" title="Download PDF">pdf</a>, <a href="/ps/2402.15027" title="Download PostScript">ps</a>, <a href="/format/2402.15027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-stakeholder Perspective on Responsible Artificial Intelligence and  Acceptability in Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karran%2C+A+J">A.J. Karran</a>, 
<a href="/search/cs?searchtype=author&query=Charland%2C+P">P. Charland</a>, 
<a href="/search/cs?searchtype=author&query=Martineau%2C+J">J-T. Martineau</a>, 
<a href="/search/cs?searchtype=author&query=de+Guinea%2C+A+O">A. Ortiz de Guinea</a>, 
<a href="/search/cs?searchtype=author&query=Lesage%2C+A">AM. Lesage</a>, 
<a href="/search/cs?searchtype=author&query=Senecal%2C+S">S. Senecal</a>, 
<a href="/search/cs?searchtype=author&query=Leger%2C+P">P-M. Leger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 2 appendices, 3 figures, 5 tables, original research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study investigates the acceptability of different artificial
intelligence (AI) applications in education from a multi-stakeholder
perspective, including students, teachers, and parents. Acknowledging the
transformative potential of AI in education, it addresses concerns related to
data privacy, AI agency, transparency, explainability and the ethical
deployment of AI. Through a vignette methodology, participants were presented
with four scenarios where AI's agency, transparency, explainability, and
privacy were manipulated. After each scenario, participants completed a survey
that captured their perceptions of AI's global utility, individual usefulness,
justice, confidence, risk, and intention to use each scenario's AI if
available. The data collection comprising a final sample of 1198
multi-stakeholder participants was distributed through a partner institution
and social media campaigns and focused on individual responses to four AI use
cases. A mediation analysis of the data indicated that acceptance and trust in
AI varies significantly across stakeholder groups. We found that the key
mediators between high and low levels of AI's agency, transparency, and
explainability, as well as the intention to use the different educational AI,
included perceived global utility, justice, and confidence. The study
highlights that the acceptance of AI in education is a nuanced and multifaceted
issue that requires careful consideration of specific AI applications and their
characteristics, in addition to the diverse stakeholders' perceptions.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15033" title="Abstract">arXiv:2402.15033</a> [<a href="/pdf/2402.15033" title="Download PDF">pdf</a>, <a href="/format/2402.15033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-Stage Block Orthogonalization to Improve Performance of $s$-step  GMRES
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yamazaki%2C+I">Ichitaro Yamazaki</a>, 
<a href="/search/math?searchtype=author&query=Higgins%2C+A+J">Andrew J. Higgins</a>, 
<a href="/search/math?searchtype=author&query=Boman%2C+E+G">Erik G. Boman</a>, 
<a href="/search/math?searchtype=author&query=Szyld%2C+D+B">Daniel B. Szyld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IPDPS'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">On current computer architectures, GMRES' performance can be limited by its
communication cost to generate orthonormal basis vectors of the Krylov
subspace. To address this performance bottleneck, its $s$-step variant
orthogonalizes a block of $s$ basis vectors at a time, potentially reducing the
communication cost by a factor of $s$. Unfortunately, for a large step size
$s$, the solver can generate extremely ill-conditioned basis vectors, and to
maintain stability in practice, a conservatively small step size is used, which
limits the performance of the $s$-step solver. To enhance the performance using
a small step size, in this paper, we introduce a two-stage block
orthogonalization scheme. Similar to the original scheme, the first stage of
the proposed method operates on a block of $s$ basis vectors at a time, but its
objective is to maintain the well-conditioning of the generated basis vectors
with a lower cost. The orthogonalization of the basis vectors is delayed until
the second stage when enough basis vectors are generated to obtain higher
performance.
<br />Our analysis shows the stability of the proposed two-stage scheme. The
performance is improved because while the same amount of computation as the
original scheme is required, most of the communication is done at the second
stage of the proposed scheme, reducing the overall communication requirements.
Our performance results with up to 192 NVIDIA V100 GPUs on the Summit
supercomputer demonstrate that when solving a 2D Laplace problem, the two-stage
approach can reduce the orthogonalization time and the total time-to-solution
by the respective factors of up to $2.6\times$ and $1.6\times$ over the
original $s$-step GMRES, which had already obtained the respective speedups of
$2.1\times$ and $1.8\times$ over the standard GMRES. Similar speedups were
obtained for 3D problems and for matrices from the SuiteSparse Matrix
Collection.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15037" title="Abstract">arXiv:2402.15037</a> [<a href="/pdf/2402.15037" title="Download PDF">pdf</a>, <a href="/format/2402.15037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Games in Maker Protocol Part One: A Multi-Agent Influence  Diagram Approach Towards Coordination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nag%2C+A">Abhimanyu Nag</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Samrat Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S">Sudipan Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+A">Arka Datta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">Decentralized Finance (DeFi) ecosystems, exemplified by the Maker Protocol,
rely on intricate games to maintain stability and security. Understanding the
dynamics of these games is crucial for ensuring the robustness of the system.
This motivating research proposes a novel methodology leveraging Multi-Agent
Influence Diagrams (MAID), originally proposed by Koller and Milch, to dissect
and analyze the games within the Maker stablecoin protocol. By representing
users and governance of the Maker protocol as agents and their interactions as
edges in a graph, we capture the complex network of influences governing agent
behaviors. Furthermore in the upcoming papers, we will show a Nash Equilibrium
model to elucidate strategies that promote coordination and enhance economic
security within the ecosystem. Through this approach, we aim to motivate the
use of this method to introduce a new method of formal verification of game
theoretic security in DeFi platforms.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15038" title="Abstract">arXiv:2402.15038</a> [<a href="/pdf/2402.15038" title="Download PDF">pdf</a>, <a href="/format/2402.15038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamics-Guided Diffusion Model for Robot Manipulator Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaomeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+H">Huy Ha</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present Dynamics-Guided Diffusion Model, a data-driven framework for
generating manipulator geometry designs for a given manipulation task. Instead
of training different design models for each task, our approach employs a
learned dynamics network shared across tasks. For a new manipulation task, we
first decompose it into a collection of individual motion targets which we call
target interaction profile, where each individual motion can be modeled by the
shared dynamics network. The design objective constructed from the target and
predicted interaction profiles provides a gradient to guide the refinement of
finger geometry for the task. This refinement process is executed as a
classifier-guided diffusion process, where the design objective acts as the
classifier guidance. We evaluate our framework on various manipulation tasks,
under the sensor-less setting using only an open-loop parallel jaw motion. Our
generated designs outperform optimization-based and unguided diffusion
baselines relatively by 31.5% and 45.3% on average manipulation success rate.
With the ability to generate a design within 0.8 seconds, our framework could
facilitate rapid design iteration and enhance the adoption of data-driven
approaches for robotic mechanism design.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15039" title="Abstract">arXiv:2402.15039</a> [<a href="/pdf/2402.15039" title="Download PDF">pdf</a>, <a href="/format/2402.15039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Descripci&#xf3;n autom&#xe1;tica de secciones delgadas de rocas: una  aplicaci&#xf3;n Web
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paucar%2C+S">Stalyn Paucar</a>, 
<a href="/search/cs?searchtype=author&query=Collaguazo%2C+C+M+y+V">Christian Mej&#xed;a-Escobar y V&#xed;ctor Collaguazo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, in Spanish language, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The identification and characterization of various rock types is one of the
fundamental activities for geology and related areas such as mining, petroleum,
environment, industry and construction. Traditionally, a human specialist is
responsible for analyzing and explaining details about the type, composition,
texture, shape and other properties using rock samples collected in-situ or
prepared in a laboratory. The results become subjective based on experience, in
addition to consuming a large investment of time and effort. The present
proposal uses artificial intelligence techniques combining computer vision and
natural language processing to generate a textual and verbal description from a
thin section image of rock. We build a dataset of images and their respective
textual descriptions for the training of a model that associates the relevant
features of the image extracted by EfficientNetB7 with the textual description
generated by a Transformer network, reaching an accuracy value of 0.892 and a
BLEU value of 0.71. This model can be a useful resource for research,
professional and academic work, so it has been deployed through a Web
application for public use.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15043" title="Abstract">arXiv:2402.15043</a> [<a href="/pdf/2402.15043" title="Download PDF">pdf</a>, <a href="/format/2402.15043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhuohao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wenjin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shikun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures, our code is available at: <a href="https://github.com/zhuohaoyu/KIEval">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Automatic evaluation methods for large language models (LLMs) are hindered by
data contamination, leading to inflated assessments of their effectiveness.
Existing strategies, which aim to detect contaminated texts, focus on
quantifying contamination status instead of accurately gauging model
performance. In this paper, we introduce KIEval, a Knowledge-grounded
Interactive Evaluation framework, which incorporates an LLM-powered
"interactor" role for the first time to accomplish a dynamic
contamination-resilient evaluation. Starting with a question in a conventional
LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically
generated, multi-round, and knowledge-focused dialogues to determine whether a
model's response is merely a recall of benchmark answers or demonstrates a deep
comprehension to apply knowledge in more complex conversations. Extensive
experiments on seven leading LLMs across five datasets validate KIEval's
effectiveness and generalization. We also reveal that data contamination brings
no contribution or even negative effect to models' real-world applicability and
understanding, and existing contamination detection methods for LLMs can only
identify contamination in pre-training but not during supervised fine-tuning.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15044" title="Abstract">arXiv:2402.15044</a> [<a href="/pdf/2402.15044" title="Download PDF">pdf</a>, <a href="/format/2402.15044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fiducial Focus Augmentation for Facial Landmark Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kar%2C+P">Purbayan Kar</a>, 
<a href="/search/cs?searchtype=author&query=Chudasama%2C+V">Vishal Chudasama</a>, 
<a href="/search/cs?searchtype=author&query=Onoe%2C+N">Naoyuki Onoe</a>, 
<a href="/search/cs?searchtype=author&query=Wasnik%2C+P">Pankaj Wasnik</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+V">Vineeth Balasubramanian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to BMVC'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep learning methods have led to significant improvements in the performance
on the facial landmark detection (FLD) task. However, detecting landmarks in
challenging settings, such as head pose changes, exaggerated expressions, or
uneven illumination, continue to remain a challenge due to high variability and
insufficient samples. This inadequacy can be attributed to the model's
inability to effectively acquire appropriate facial structure information from
the input images. To address this, we propose a novel image augmentation
technique specifically designed for the FLD task to enhance the model's
understanding of facial structures. To effectively utilize the newly proposed
augmentation technique, we employ a Siamese architecture-based training
mechanism with a Deep Canonical Correlation Analysis (DCCA)-based loss to
achieve collective learning of high-level feature representations from two
different views of the input images. Furthermore, we employ a Transformer +
CNN-based network with a custom hourglass module as the robust backbone for the
Siamese framework. Extensive experiments show that our approach outperforms
multiple state-of-the-art approaches across various benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15046" title="Abstract">arXiv:2402.15046</a> [<a href="/pdf/2402.15046" title="Download PDF">pdf</a>, <a href="/format/2402.15046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for  Aspect-Level Sentiment Classification in Korean
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+D">Dongjun Jang</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Jean Seo</a>, 
<a href="/search/cs?searchtype=author&query=Byun%2C+S">Sungjoo Byun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taekyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minseok Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Hyopil Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper explores the challenges posed by aspect-based sentiment
classification (ABSC) within pretrained language models (PLMs), with a
particular focus on contextualization and hallucination issues. In order to
tackle these challenges, we introduce CARBD-Ko (a Contextually Annotated Review
Benchmark Dataset for Aspect-Based Sentiment Classification in Korean), a
benchmark dataset that incorporates aspects and dual-tagged polarities to
distinguish between aspect-specific and aspect-agnostic sentiment
classification. The dataset consists of sentences annotated with specific
aspects, aspect polarity, aspect-agnostic polarity, and the intensity of
aspects. To address the issue of dual-tagged aspect polarities, we propose a
novel approach employing a Siamese Network. Our experimental findings highlight
the inherent difficulties in accurately predicting dual-polarities and
underscore the significance of contextualized sentiment analysis models. The
CARBD-Ko dataset serves as a valuable resource for future research endeavors in
aspect-level sentiment classification.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15047" title="Abstract">arXiv:2402.15047</a> [<a href="/pdf/2402.15047" title="Download PDF">pdf</a>, <a href="/ps/2402.15047" title="Download PostScript">ps</a>, <a href="/format/2402.15047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Networked Collaborative Sensing using Multi-domain Measurements:  Architectures, Performance Limits and Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yihua Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shuqiang Xia</a>, 
<a href="/search/cs?searchtype=author&query=bai%2C+C">Chen bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongbin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Songqian Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">As a promising 6G technology, integrated sensing and communication (ISAC)
gains growing interest. ISAC provides integration gain via sharing spectrum,
hardware, and software. However, concerns exist regarding its sensing
performance when compared to dedicated radar systems. To address this issue,
the advantages of widely deployed networks should be utilized, and this paper
proposes networked collaborative sensing (NCS) using multi-domain measurements
(MM), including range, Doppler, and two-dimension angle of arrival. In the
NCS-MM architecture, this paper proposes a novel multi-domain decoupling model
and a novel guard band-based protocol. The proposed model simplifies
multi-domain derivations and algorithm designs, and the proposed protocol
conserves resources and mitigates NCS interference. To determine the
performance limits, this paper derives the Cram\'er-Rao lower bound (CRLB) of
three-dimension position and velocity in NCS-MM. An accumulated
single-dimension channel model is used to obtain the CRLB of MM, which is
proven to be equivalent to that of the multi-dimension model. The algorithms of
both MM estimation and fusion are proposed. An arbitrary-dimension Newtonized
orthogonal matched pursuit (AD-NOMP) is proposed to accurately estimate
grid-less MM. The degree-of-freedom (DoF) of MM is analyzed, and a novel
DoF-based two-stage weighted least squares (TSWLS) is proposed to reduce
equations without DoF loss. The numerical results show that the performances of
the proposed algorithms are close to their performance limits.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15048" title="Abstract">arXiv:2402.15048</a> [<a href="/pdf/2402.15048" title="Download PDF">pdf</a>, <a href="/format/2402.15048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Power of Large Language Models for Entity Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xuhui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yinghan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhichao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengjin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jian Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanzhuo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG)
data, playing a crucial role in data-driven AI applications. Traditional EA
methods primarily rely on comparing entity embeddings, but their effectiveness
is constrained by the limited input KG data and the capabilities of the
representation learning techniques. Against this backdrop, we introduce ChatEA,
an innovative framework that incorporates large language models (LLMs) to
improve EA. To address the constraints of limited input KG data, ChatEA
introduces a KG-code translation module that translates KG structures into a
format understandable by LLMs, thereby allowing LLMs to utilize their extensive
background knowledge to improve EA accuracy. To overcome the over-reliance on
entity embedding comparisons, ChatEA implements a two-stage EA strategy that
capitalizes on LLMs' capability for multi-step reasoning in a dialogue format,
thereby enhancing accuracy while preserving efficiency. Our experimental
results affirm ChatEA's superior performance, highlighting LLMs' potential in
facilitating EA tasks.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15052" title="Abstract">arXiv:2402.15052</a> [<a href="/pdf/2402.15052" title="Download PDF">pdf</a>, <a href="/format/2402.15052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToMBench: Benchmarking Theory of Mind in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jincenzi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinfeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+B">Bosi Wen</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+G">Guanqun Bi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+G">Gongyao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yaru Cao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Mengting Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yunghwei Lai</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zexuan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Theory of Mind (ToM) is the cognitive capability to perceive and ascribe
mental states to oneself and others. Recent research has sparked a debate over
whether large language models (LLMs) exhibit a form of ToM. However, existing
ToM evaluations are hindered by challenges such as constrained scope,
subjective judgment, and unintended contamination, yielding inadequate
assessments. To address this gap, we introduce ToMBench with three key
characteristics: a systematic evaluation framework encompassing 8 tasks and 31
abilities in social cognition, a multiple-choice question format to support
automated and unbiased evaluation, and a build-from-scratch bilingual inventory
to strictly avoid data leakage. Based on ToMBench, we conduct extensive
experiments to evaluate the ToM performance of 10 popular LLMs across tasks and
abilities. We find that even the most advanced LLMs like GPT-4 lag behind human
performance by over 10% points, indicating that LLMs have not achieved a
human-level theory of mind yet. Our aim with ToMBench is to enable an efficient
and effective evaluation of LLMs' ToM capabilities, thereby facilitating the
development of LLMs with inherent social intelligence.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15055" title="Abstract">arXiv:2402.15055</a> [<a href="/pdf/2402.15055" title="Download PDF">pdf</a>, <a href="/format/2402.15055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Context Look-ups in Transformers: Investigating  Attention-MLP Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neo%2C+C">Clement Neo</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we investigate the interplay between attention heads and
specialized "next-token" neurons in the Multilayer Perceptron that predict
specific tokens. By prompting an LLM like GPT-4 to explain these model
internals, we can elucidate attention mechanisms that activate certain
next-token neurons. Our analysis identifies attention heads that recognize
contexts relevant to predicting a particular token, activating the associated
neuron through the residual connection. We focus specifically on heads in
earlier layers consistently activating the same next-token neuron across
similar prompts. Exploring these differential activation patterns reveals that
heads that specialize for distinct linguistic contexts are tied to generating
certain tokens. Overall, our method combines neural explanations and probing
isolated components to illuminate how attention enables context-dependent,
specialized processing in LLMs.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15057" title="Abstract">arXiv:2402.15057</a> [<a href="/pdf/2402.15057" title="Download PDF">pdf</a>, <a href="/format/2402.15057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Multi-turn Instruction Following for Conversational Web Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Web agents powered by Large Language Models (LLMs) have demonstrated
remarkable abilities in planning and executing multi-step interactions within
complex web-based environments, fulfilling a wide range of web navigation
tasks. Despite these advancements, the potential for LLM-powered agents to
effectively engage with sequential user instructions in real-world scenarios
has not been fully explored. In this work, we introduce a new task of
Conversational Web Navigation, which necessitates sophisticated interactions
that span multiple turns with both the users and the environment, supported by
a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To
tackle the limited context length of LLMs and the context-dependency issue of
the conversational tasks, we further propose a novel framework, named
self-reflective memory-augmented planning (Self-MAP), which employs memory
utilization and self-reflection techniques. Extensive experiments are conducted
to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the
proposed method.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15059" title="Abstract">arXiv:2402.15059</a> [<a href="/pdf/2402.15059" title="Download PDF">pdf</a>, <a href="/format/2402.15059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot  Multilingual Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Louis%2C+A">Antoine Louis</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+V">Vageesh Saxena</a>, 
<a href="/search/cs?searchtype=author&query=van+Dijck%2C+G">Gijs van Dijck</a>, 
<a href="/search/cs?searchtype=author&query=Spanakis%2C+G">Gerasimos Spanakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review. Code is available at <a href="https://github.com/ant-louis/xm-retrievers">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">State-of-the-art neural retrievers predominantly focus on high-resource
languages like English, which impedes their adoption in retrieval scenarios
involving other languages. Current approaches circumvent the lack of
high-quality labeled data in non-English languages by leveraging multilingual
pretrained language models capable of cross-lingual transfer. However, these
models require substantial task-specific fine-tuning across multiple languages,
often perform poorly in languages with minimal representation in the
pretraining corpus, and struggle to incorporate new languages after the
pretraining phase. In this work, we present a novel modular dense retrieval
model that learns from the rich data of a single high-resource language and
effectively zero-shot transfers to a wide array of languages, thereby
eliminating the need for language-specific labeled data. Our model, ColBERT-XM,
demonstrates competitive performance against existing state-of-the-art
multilingual retrievers trained on more extensive datasets in various
languages. Further analysis reveals that our modular approach is highly
data-efficient, effectively adapts to out-of-distribution data, and
significantly reduces energy consumption and carbon emissions. By demonstrating
its proficiency in zero-shot scenarios, ColBERT-XM marks a shift towards more
sustainable and inclusive retrieval systems, enabling effective information
accessibility in numerous languages. We publicly release our code and models
for the community.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15061" title="Abstract">arXiv:2402.15061</a> [<a href="/pdf/2402.15061" title="Download PDF">pdf</a>, <a href="/format/2402.15061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning Large Language Models for Domain-specific Machine  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiawei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">Hanghai Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoli Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jingsong Su</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yonggui Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shikai Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 6tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have made significant progress in machine
translation (MT). However, their potential in domain-specific MT remains
under-explored. Current LLM-based MT systems still face several challenges.
First, for LLMs with in-context learning, their effectiveness is highly
sensitive to input translation examples, and processing them can increase
inference costs. They often require extra post-processing due to
over-generation. Second, LLMs with fine-tuning on domain-specific data often
require high training costs for domain adaptation, and may weaken the zero-shot
MT capabilities of LLMs due to over-specialization. The aforementioned methods
can struggle to translate rare words in domain transfer scenarios. To address
these challenges, this paper proposes a prompt-oriented fine-tuning method,
denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose
LLM for domain-specific MT tasks. First, we construct a task-specific
mix-domain dataset, which is then used to fine-tune the LLM with LoRA. This can
eliminate the need for input translation examples, post-processing, or
over-specialization. By zero-shot prompting with instructions, we adapt the MT
tasks to the target domain at inference time. To further elicit the MT
capability for rare words, we construct new prompts by incorporating
domain-specific bilingual vocabulary. We also conduct extensive experiments on
both publicly available and self-constructed datasets. The results show that
our LlamaIT can significantly enhance the domain-specific MT capabilities of
the LLM, meanwhile preserving its zero-shot MT capabilities.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15062" title="Abstract">arXiv:2402.15062</a> [<a href="/pdf/2402.15062" title="Download PDF">pdf</a>, <a href="/format/2402.15062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gotcha! Don&#x27;t trick me with unanswerable questions! Self-aligning Large  Language Models for Responding to Unknown Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Moxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the remarkable abilities of Large Language Models (LLMs) to answer
questions, they often display a considerable level of overconfidence even when
the question does not have a definitive answer. To avoid providing hallucinated
answers to these unknown questions, existing studies typically investigate
approaches to refusing to answer these questions. In this work, we propose a
novel and scalable self-alignment method to utilize the LLM itself to enhance
its response-ability to different types of unknown questions, being capable of
not only refusing to answer but also providing explanation to the
unanswerability of unknown questions. Specifically, the Self-Align method first
employ a two-stage class-aware self-augmentation approach to generate a large
amount of unknown question-response data. Then we conduct disparity-driven
self-curation to select qualified data for fine-tuning the LLM itself for
aligning the responses to unknown questions as desired. Experimental results on
two datasets across four types of unknown questions validate the superiority of
the Self-Align method over existing baselines in terms of three types of task
formulation.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15070" title="Abstract">arXiv:2402.15070</a> [<a href="/pdf/2402.15070" title="Download PDF">pdf</a>, <a href="/format/2402.15070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing One-Shot Federated Learning Through Data and Ensemble  Co-Boosting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+R">Rong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yonggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">One-shot Federated Learning (OFL) has become a promising learning paradigm,
enabling the training of a global server model via a single communication
round. In OFL, the server model is aggregated by distilling knowledge from all
client models (the ensemble), which are also responsible for synthesizing
samples for distillation. In this regard, advanced works show that the
performance of the server model is intrinsically related to the quality of the
synthesized data and the ensemble model. To promote OFL, we introduce a novel
framework, Co-Boosting, in which synthesized data and the ensemble model
mutually enhance each other progressively. Specifically, Co-Boosting leverages
the current ensemble model to synthesize higher-quality samples in an
adversarial manner. These hard samples are then employed to promote the quality
of the ensemble model by adjusting the ensembling weights for each client
model. Consequently, Co-Boosting periodically achieves high-quality data and
ensemble models. Extensive experiments demonstrate that Co-Boosting can
substantially outperform existing baselines under various settings. Moreover,
Co-Boosting eliminates the need for adjustments to the client's local training,
requires no additional data or model transmission, and allows client models to
have heterogeneous architectures.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15073" title="Abstract">arXiv:2402.15073</a> [<a href="/pdf/2402.15073" title="Download PDF">pdf</a>, <a href="/format/2402.15073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duy Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+B">Bao Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V+A">Viet Anh Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Algorithmic recourse recommends a cost-efficient action to a subject to
reverse an unfavorable machine learning classification decision. Most existing
methods in the literature generate recourse under the assumption of complete
knowledge about the cost function. In real-world practice, subjects could have
distinct preferences, leading to incomplete information about the underlying
cost function of the subject. This paper proposes a two-step approach
integrating preference learning into the recourse generation problem. In the
first step, we design a question-answering framework to refine the confidence
set of the Mahalanobis matrix cost of the subject sequentially. Then, we
generate recourse by utilizing two methods: gradient-based and graph-based
cost-adaptive recourse that ensures validity while considering the whole
confidence set of the cost matrix. The numerical evaluation demonstrates the
benefits of our approach over state-of-the-art baselines in delivering
cost-efficient recourse recommendations.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15074" title="Abstract">arXiv:2402.15074</a> [<a href="/pdf/2402.15074" title="Download PDF">pdf</a>, <a href="/ps/2402.15074" title="Download PostScript">ps</a>, <a href="/format/2402.15074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretation of Inaccessible Sets in Martin-L&#xf6;f Type Theory with  One Mahlo Universe
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+Y">Yuta Takahashi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">Martin-L\"{o}f type theory $\mathbf{MLTT}$ was extended by Setzer with the
so-called Mahlo universe types. This extension is called $\mathbf{MLM}$ and was
introduced to develop a variant of $\mathbf{MLTT}$ equipped with an analogue of
a large cardinal. Another instance of constructive systems extended with an
analogue of a large set was formulated in the context of Aczel's constructive
set theory: $\mathbf{CZF}$. Rathjen, Griffor and Palmgren extended
$\mathbf{CZF}$ with inaccessible sets of all transfinite orders. It is unknown
whether this extension of $\mathbf{CZF}$ is directly interpretable by Mahlo
universes. In particular, how to construct the transfinite hierarchy of
inaccessible sets using the reflection property of the Mahlo universe in
$\mathbf{MLM}$ is not well understood. We extend $\mathbf{MLM}$ further by
adding the accessibility predicate to it and show that the above extension of
$\mathbf{CZF}$ is directly interpretable in $\mathbf{MLM}$ using the
accessibility predicate.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15075" title="Abstract">arXiv:2402.15075</a> [<a href="/pdf/2402.15075" title="Download PDF">pdf</a>, <a href="/ps/2402.15075" title="Download PostScript">ps</a>, <a href="/format/2402.15075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stacking Factorizing Partitioned Expressions in Hybrid Bayesian Network  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+P">Peng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Neil%2C+M">Martin Neil</a>, 
<a href="/search/cs?searchtype=author&query=Fenton%2C+N">Norman Fenton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Hybrid Bayesian networks (HBN) contain complex conditional probabilistic
distributions (CPD) specified as partitioned expressions over discrete and
continuous variables. The size of these CPDs grows exponentially with the
number of parent nodes when using discrete inference, resulting in significant
inefficiency. Normally, an effective way to reduce the CPD size is to use a
binary factorization (BF) algorithm to decompose the statistical or arithmetic
functions in the CPD by factorizing the number of connected parent nodes to
sets of size two. However, the BF algorithm was not designed to handle
partitioned expressions. Hence, we propose a new algorithm called stacking
factorization (SF) to decompose the partitioned expressions. The SF algorithm
creates intermediate nodes to incrementally reconstruct the densities in the
original partitioned expression, allowing no more than two continuous parent
nodes to be connected to each child node in the resulting HBN. SF can be either
used independently or combined with the BF algorithm. We show that the SF+BF
algorithm significantly reduces the CPD size and contributes to lowering the
tree-width of a model, thus improving efficiency.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15076" title="Abstract">arXiv:2402.15076</a> [<a href="/pdf/2402.15076" title="Download PDF">pdf</a>, <a href="/format/2402.15076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Inapproximability of Target Set Reconfiguration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ohsaka%2C+N">Naoto Ohsaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Given a graph $G$ with a vertex threshold function $\tau$, consider a dynamic
process in which any inactive vertex $v$ becomes activated whenever at least
$\tau(v)$ of its neighbors are activated. A vertex set $S$ is called a target
set if all vertices of $G$ would be activated when initially activating
vertices of $S$. In the Minmax Target Set Reconfiguration problem, for a graph
$G$ and its two target sets $X$ and $Y$, we wish to transform $X$ into $Y$ by
repeatedly adding or removing a single vertex, using only target sets of $G$,
so as to minimize the maximum size of any intermediate target set. We prove
that it is NP-hard to approximate Minmax Target Set Reconfiguration within a
factor of $2-o\left(\frac{1}{\operatorname{polylog} n}\right)$, where $n$ is
the number of vertices. Our result establishes a tight lower bound on
approximability of Minmax Target Set Reconfiguration, which admits a $2$-factor
approximation algorithm. The proof is based on a gap-preserving reduction from
Target Set Selection to Minmax Target Set Reconfiguration, where NP-hardness of
approximation for the former problem is proven by Chen (SIAM J. Discrete Math.,
2009) and Charikar, Naamad, and Wirth (APPROX/RANDOM 2016).
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15078" title="Abstract">arXiv:2402.15078</a> [<a href="/pdf/2402.15078" title="Download PDF">pdf</a>, <a href="/format/2402.15078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-CompDroid: Repairing Configuration Compatibility Bugs in Android  Apps with Pre-trained Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhijie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yutian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meiyun Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yunfei Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L+F">Liang Feng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiapu Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">XML configurations are integral to the Android development framework,
particularly in the realm of UI display. However, these configurations can
introduce compatibility issues (bugs), resulting in divergent visual outcomes
and system crashes across various Android API versions (levels). In this study,
we systematically investigate LLM-based approaches for detecting and repairing
configuration compatibility bugs. Our findings highlight certain limitations of
LLMs in effectively identifying and resolving these bugs, while also revealing
their potential in addressing complex, hard-to-repair issues that traditional
tools struggle with. Leveraging these insights, we introduce the LLM-CompDroid
framework, which combines the strengths of LLMs and traditional tools for bug
resolution. Our experimental results demonstrate a significant enhancement in
bug resolution performance by LLM-CompDroid, with LLM-CompDroid-GPT-3.5 and
LLM-CompDroid-GPT-4 surpassing the state-of-the-art tool, ConfFix, by at least
9.8% and 10.4% in both Correct and Correct@k metrics, respectively. This
innovative approach holds promise for advancing the reliability and robustness
of Android applications, making a valuable contribution to the field of
software development.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15080" title="Abstract">arXiv:2402.15080</a> [<a href="/pdf/2402.15080" title="Download PDF">pdf</a>, <a href="/format/2402.15080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient  Framework for Multi-level Implicit Discourse Relation Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haodong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruifang He</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Mengnan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jing Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multi-level implicit discourse relation recognition (MIDRR) aims at
identifying hierarchical discourse relations among arguments. Previous methods
achieve the promotion through fine-tuning PLMs. However, due to the data
scarcity and the task gap, the pre-trained feature space cannot be accurately
tuned to the task-specific space, which even aggravates the collapse of the
vanilla space. Besides, the comprehension of hierarchical semantics for MIDRR
makes the conversion much harder. In this paper, we propose a prompt-based
Parameter-Efficient Multi-level IDRR (PEMI) framework to solve the above
problems. First, we leverage parameter-efficient prompt tuning to drive the
inputted arguments to match the pre-trained space and realize the approximation
with few parameters. Furthermore, we propose a hierarchical label refining
(HLR) method for the prompt verbalizer to deeply integrate hierarchical
guidance into the prompt tuning. Finally, our model achieves comparable results
on PDTB 2.0 and 3.0 using about 0.1% trainable parameters compared with
baselines and the visualization demonstrates the effectiveness of our HLR
method.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15081" title="Abstract">arXiv:2402.15081</a> [<a href="/pdf/2402.15081" title="Download PDF">pdf</a>, <a href="/format/2402.15081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Sustain a Scientific Open-Source Software Ecosystem: Learning  from the Astropy Project
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiayi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+A">Aarya Patil</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Youhai Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J+L+C">Jin L.C. Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shurui Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Scientific open-source software (OSS) has greatly benefited research
communities through its transparent and collaborative nature. Given its
critical role in scientific research, ensuring the sustainability of such
software has become vital. Earlier studies have proposed sustainability
strategies for conventional scientific software and open-source communities.
However, it remains unclear whether these solutions can be easily adapted to
the integrated framework of scientific OSS and its larger ecosystem. This study
examines the challenges and opportunities to enhance the sustainability of
scientific OSS in the context of interdisciplinary collaboration, open-source
community, and multi-project ecosystem. We conducted a case study on a
widely-used software ecosystem in the astrophysics domain, the Astropy Project,
using a mixed-methods design approach. This approach includes an interview with
core contributors regarding their participation in an interdisciplinary team, a
survey of disengaged contributors about their motivations for contribution,
reasons for disengagement, and suggestions for sustaining the communities, and
finally, an analysis of cross-referenced issues and pull requests to understand
best practices for collaboration on the ecosystem level. Our study reveals the
implications of major challenges for sustaining scientific OSS and proposes
concrete suggestions for tackling these challenges.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15082" title="Abstract">arXiv:2402.15082</a> [<a href="/pdf/2402.15082" title="Download PDF">pdf</a>, <a href="/format/2402.15082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables  Parameter-Efficient Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhisheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Han Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianling Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for
adapting pre-trained language models to various tasks efficiently. Recently,
there has been a growing interest in transferring knowledge from one or
multiple tasks to the downstream target task to achieve performance
improvements. However, current approaches typically either train adapters on
individual tasks or distill shared knowledge from source tasks, failing to
fully exploit task-specific knowledge and the correlation between source and
target tasks. To overcome these limitations, we propose PEMT, a novel
parameter-efficient fine-tuning framework based on multi-task transfer
learning. PEMT extends the mixture-of-experts (MoE) framework to capture the
transferable knowledge as a weighted combination of adapters trained on source
tasks. These weights are determined by a gated unit, measuring the correlation
between the target and each source task using task description prompt vectors.
To fully exploit the task-specific knowledge, we also propose the Task Sparsity
Loss to improve the sparsity of the gated unit. We conduct experiments on a
broad range of tasks over 17 datasets. The experimental results demonstrate our
PEMT yields stable improvements over full fine-tuning, and state-of-the-art
PEFT and knowledge transferring methods on various tasks. The results highlight
the effectiveness of our method which is capable of sufficiently exploiting the
knowledge and correlation features across multiple tasks.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15083" title="Abstract">arXiv:2402.15083</a> [<a href="/pdf/2402.15083" title="Download PDF">pdf</a>, <a href="/ps/2402.15083" title="Download PostScript">ps</a>, <a href="/format/2402.15083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hands-Free VR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+J+A+V">Jorge Askur Vazquez Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+J">Jae Joong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Vacca%2C+S+A+S">Santiago Andr&#xe9;s Serrano Vacca</a>, 
<a href="/search/cs?searchtype=author&query=Magana%2C+A">Alejandra Magana</a>, 
<a href="/search/cs?searchtype=author&query=Benes%2C+B">Bedrich Benes</a>, 
<a href="/search/cs?searchtype=author&query=Popescu%2C+V">Voicu Popescu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The paper introduces Hands-Free VR, a voice-based natural-language interface
for VR. The user gives a command using their voice, the speech audio data is
converted to text using a speech-to-text deep learning model that is fine-tuned
for robustness to word phonetic similarity and to spoken English accents, and
the text is mapped to an executable VR command using a large language model
that is robust to natural language diversity. Hands-Free VR was evaluated in a
controlled within-subjects study (N = 22) that asked participants to find
specific objects and to place them in various configurations. In the control
condition participants used a conventional VR user interface to grab, carry,
and position the objects using the handheld controllers. In the experimental
condition participants used Hands-Free VR. The results confirm that: (1)
Hands-Free VR is robust to spoken English accents, as for 20 of our
participants English was not their first language, and to word phonetic
similarity, correctly transcribing the voice command 96.71% of the time; (2)
Hands-Free VR is robust to natural language diversity, correctly mapping the
transcribed command to an executable command in 97.83% of the time; (3)
Hands-Free VR had a significant efficiency advantage over the conventional VR
interface in terms of task completion time, total viewpoint translation, total
view direction rotation, and total left and right hand translations; (4)
Hands-Free VR received high user preference ratings in terms of ease of use,
intuitiveness, ergonomics, reliability, and desirability.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15089" title="Abstract">arXiv:2402.15089</a> [<a href="/pdf/2402.15089" title="Download PDF">pdf</a>, <a href="/format/2402.15089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AttributionBench: How Hard is Automatic Attribution Evaluation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Z">Zeyi Liao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Modern generative search engines enhance the reliability of large language
model (LLM) responses by providing cited evidence. However, evaluating the
answer's attribution, i.e., whether every claim within the generated responses
is fully supported by its cited evidence, remains an open problem. This
verification, traditionally dependent on costly human evaluation, underscores
the urgent need for automatic attribution evaluation methods. To bridge the gap
in the absence of standardized benchmarks for these methods, we present
AttributionBench, a comprehensive benchmark compiled from various existing
attribution datasets. Our extensive experiments on AttributionBench reveal the
challenges of automatic attribution evaluation, even for state-of-the-art LLMs.
Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves
around 80% macro-F1 under a binary classification formulation. A detailed
analysis of more than 300 error cases indicates that a majority of failures
stem from the model's inability to process nuanced information, and the
discrepancy between the information the model has access to and that human
annotators do.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15096" title="Abstract">arXiv:2402.15096</a> [<a href="/pdf/2402.15096" title="Download PDF">pdf</a>, <a href="/format/2402.15096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Transformer With a Low-Computational-Cost Guarantee
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sungjin Park</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024 (5 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">Transformer-based models have significantly improved performance across a
range of multimodal understanding tasks, such as visual question answering and
action recognition. However, multimodal Transformers significantly suffer from
a quadratic complexity of the multi-head attention with the input sequence
length, especially as the number of modalities increases. To address this, we
introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal
attention mechanism that aims to reduce computational cost during training and
inference with minimal performance loss. Specifically, by assigning different
multimodal attention patterns to each attention head, LoCoMT can flexibly
control multimodal signals and theoretically ensures a reduced computational
cost compared to existing multimodal Transformer variants. Experimental results
on two multimodal datasets, namely Audioset and MedVidCL demonstrate that
LoCoMT not only reduces GFLOPs but also matches or even outperforms established
models.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15097" title="Abstract">arXiv:2402.15097</a> [<a href="/pdf/2402.15097" title="Download PDF">pdf</a>, <a href="/format/2402.15097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning solution operators of PDEs defined on varying domains via  MIONet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shanshan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+P">Pengzhan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yifa Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this work, we propose a method to learn the solution operators of PDEs
defined on varying domains via MIONet, and theoretically justify this method.
We first extend the approximation theory of MIONet to further deal with metric
spaces, establishing that MIONet can approximate mappings with multiple inputs
in metric spaces. Subsequently, we construct a set consisting of some
appropriate regions and provide a metric on this set thus make it a metric
space, which satisfies the approximation condition of MIONet. Building upon the
theoretical foundation, we are able to learn the solution mapping of a PDE with
all the parameters varying, including the parameters of the differential
operator, the right-hand side term, the boundary condition, as well as the
domain. Without loss of generality, we for example perform the experiments for
2-d Poisson equations, where the domains and the right-hand side terms are
varying. The results provide insights into the performance of this method
across convex polygons, polar regions with smooth boundary, and predictions for
different levels of discretization on one task. Reasonably, we point out that
this is a meshless method, hence can be flexibly used as a general solver for a
type of PDE.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15100" title="Abstract">arXiv:2402.15100</a> [<a href="/pdf/2402.15100" title="Download PDF">pdf</a>, <a href="/format/2402.15100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Studying LLM Performance on Closed- and Open-source Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+T">Toufique Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Bird%2C+C">Christian Bird</a>, 
<a href="/search/cs?searchtype=author&query=Devanbu%2C+P">Premkumar Devanbu</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Saikat Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language models (LLMs) are finding wide use in software engineering
practice. These models are extremely data-hungry, and are largely trained on
open-source (OSS) code distributed with permissive licenses. In terms of actual
use however, a great deal of software development still occurs in the
for-profit/proprietary sphere, where the code under development is not, and
never has been, in the public domain; thus, many developers, do their work, and
use LLMs, in settings where the models may not be as familiar with the code
under development. In such settings, do LLMs work as well as they do for OSS
code? If not, what are the differences? When performance differs, what are the
possible causes, and are there work-arounds? In this paper, we examine this
issue using proprietary, closed-source software data from Microsoft, where most
proprietary code is in C# and C++. We find that performance for C# changes
little from OSS --&gt; proprietary code, but does significantly reduce for C++; we
find that this difference is attributable to differences in identifiers. We
also find that some performance degradation, in some cases, can be ameliorated
efficiently by in-context learning.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15102" title="Abstract">arXiv:2402.15102</a> [<a href="/pdf/2402.15102" title="Download PDF">pdf</a>, <a href="/format/2402.15102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory-wise Iterative Reinforcement Learning Framework for  Auto-bidding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoming Li</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yusen Huo</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shuai Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhenzhe Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhilin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by The Web Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Information Retrieval (cs.IR)

</div>
<p class="mathjax">In online advertising, advertisers participate in ad auctions to acquire ad
opportunities, often by utilizing auto-bidding tools provided by demand-side
platforms (DSPs). The current auto-bidding algorithms typically employ
reinforcement learning (RL). However, due to safety concerns, most RL-based
auto-bidding policies are trained in simulation, leading to a performance
degradation when deployed in online environments. To narrow this gap, we can
deploy multiple auto-bidding agents in parallel to collect a large interaction
dataset. Offline RL algorithms can then be utilized to train a new policy. The
trained policy can subsequently be deployed for further data collection,
resulting in an iterative training framework, which we refer to as iterative
offline RL. In this work, we identify the performance bottleneck of this
iterative offline RL framework, which originates from the ineffective
exploration and exploitation caused by the inherent conservatism of offline RL
algorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration
and Exploitation (TEE), which introduces a novel data collecting and data
utilization method for iterative offline RL from a trajectory perspective.
Furthermore, to ensure the safety of online exploration while preserving the
dataset quality for TEE, we propose Safe Exploration by Adaptive Action
Selection (SEAS). Both offline experiments and real-world experiments on
Alibaba display advertising platform demonstrate the effectiveness of our
proposed method.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15105" title="Abstract">arXiv:2402.15105</a> [<a href="/pdf/2402.15105" title="Download PDF">pdf</a>, <a href="/format/2402.15105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A First Look at GPT Apps: Landscape and Vulnerability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zejun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Anlan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+F">Feng Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">With the advancement of Large Language Models (LLMs), increasingly
sophisticated and powerful GPTs are entering the market. Despite their
popularity, the LLM ecosystem still remains unexplored. Additionally, LLMs'
susceptibility to attacks raises concerns over safety and plagiarism. Thus, in
this work, we conduct a pioneering exploration of GPT stores, aiming to study
vulnerabilities and plagiarism within GPT applications. To begin with, we
conduct, to our knowledge, the first large-scale monitoring and analysis of two
stores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, we
propose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals.
To complete these two tasks efficiently, we develop two automated tools: one
for web scraping and another designed for programmatically interacting with
GPTs. Our findings reveal a significant enthusiasm among users and developers
for GPT interaction and creation, as evidenced by the rapid increase in GPTs
and their creators. However, we also uncover a widespread failure to protect
GPT internals, with nearly 90% of system prompts easily accessible, leading to
considerable plagiarism and duplication among GPTs.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15106" title="Abstract">arXiv:2402.15106</a> [<a href="/pdf/2402.15106" title="Download PDF">pdf</a>, <a href="/format/2402.15106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling-based Distributed Training with Message Passing Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kakka%2C+P">Priyesh Kakka</a>, 
<a href="/search/cs?searchtype=author&query=Nidhan%2C+S">Sheel Nidhan</a>, 
<a href="/search/cs?searchtype=author&query=Ranade%2C+R">Rishikesh Ranade</a>, 
<a href="/search/cs?searchtype=author&query=MacArt%2C+J+F">Jonathan F. MacArt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">In this study, we introduce a domain-decomposition-based distributed training
and inference approach for message-passing neural networks (MPNN). Our
objective is to address the challenge of scaling edge-based graph neural
networks as the number of nodes increases. Through our distributed training
approach, coupled with Nystr\"om-approximation sampling techniques, we present
a scalable graph neural network, referred to as DS-MPNN (D and S standing for
distributed and sampled, respectively), capable of scaling up to $O(10^5)$
nodes. We validate our sampling and distributed training approach on two cases:
(a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils,
providing comparisons with both single-GPU implementation and node-based graph
convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy
to single-GPU implementation, can accommodate a significantly larger number of
nodes compared to the single-GPU variant (S-MPNN), and significantly
outperforms the node-based GCN.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15108" title="Abstract">arXiv:2402.15108</a> [<a href="/pdf/2402.15108" title="Download PDF">pdf</a>, <a href="/format/2402.15108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Contextual Inquiry of People with Vision Impairments in Cooking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+F+M">Franklin Mingzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M+X">Michael Xieyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kane%2C+S+K">Shaun K. Kane</a>, 
<a href="/search/cs?searchtype=author&query=Carrington%2C+P">Patrick Carrington</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Individuals with vision impairments employ a variety of strategies for object
identification, such as pans or soy sauce, in the culinary process. In
addition, they often rely on contextual details about objects, such as
location, orientation, and current status, to autonomously execute cooking
activities. To understand how people with vision impairments collect and use
the contextual information of objects while cooking, we conducted a contextual
inquiry study with 12 participants in their own kitchens. This research aims to
analyze object interaction dynamics in culinary practices to enhance assistive
vision technologies for visually impaired cooks. We outline eight different
types of contextual information and the strategies that blind cooks currently
use to access the information while preparing meals. Further, we discuss
preferences for communicating contextual information about kitchen objects as
well as considerations for the deployment of AI-powered assistive technologies.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15109" title="Abstract">arXiv:2402.15109</a> [<a href="/pdf/2402.15109" title="Download PDF">pdf</a>, <a href="/format/2402.15109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Unlearning by Suppressing Sample Contribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xinwen Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhehao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaolin Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine Unlearning (MU) is to forget data from a well-trained model, which is
practically important due to the "right to be forgotten". In this paper, we
start from the fundamental distinction between training data and unseen data on
their contribution to the model: the training data contributes to the final
model while the unseen data does not. We theoretically discover that the input
sensitivity can approximately measure the contribution and practically design
an algorithm, called MU-Mis (machine unlearning via minimizing input
sensitivity), to suppress the contribution of the forgetting data. Experimental
results demonstrate that MU-Mis outperforms state-of-the-art MU methods
significantly. Additionally, MU-Mis aligns more closely with the application of
MU as it does not require the use of remaining data.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15111" title="Abstract">arXiv:2402.15111</a> [<a href="/pdf/2402.15111" title="Download PDF">pdf</a>, <a href="/format/2402.15111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled  Realization for Multi-Round Secure Aggregation in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+K">Kaiping Cui</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xia Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liangmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haiqin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%BCdder%2C+B">Boris D&#xfc;dder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Secure aggregation enables federated learning (FL) to perform collaborative
training of clients from local gradient updates without exposing raw data.
However, existing secure aggregation schemes inevitably perform an expensive
fresh setup per round because each client needs to establish fresh
input-independent secrets over different rounds. The latest research, Flamingo
(S&amp;P 2023), designed a share-transfer-based reusable secret key to support the
server continuously performing multiple rounds of aggregation. Nevertheless,
the share transfer mechanism it proposed can only be achieved with P
probability, which has limited reliability. To tackle the aforementioned
problems, we propose a more reliable and anonymously authenticated scheme
called Chu-ko-nu for multi-round secure aggregation. Specifically, in terms of
share transfer, Chu-ko-nu breaks the probability P barrier by supplementing a
redistribution process of secret key components (the sum of all components is
the secret key), thus ensuring the reusability of the secret key. Based on this
reusable secret key, Chu-ko-nu can efficiently perform consecutive aggregation
in the following rounds. Furthermore, considering the client identity
authentication and privacy protection issue most approaches ignore, Chu-ko-nu
introduces a zero-knowledge proof-based authentication mechanism. It can
support clients anonymously participating in FL training and enables the server
to authenticate clients effectively in the presence of various attacks.
Rigorous security proofs and extensive experiments demonstrated that Chu-ko-nu
can provide reliable and anonymously authenticated aggregation for FL with low
aggregation costs, at least a 21.02% reduction compared to the state-of-the-art
schemes.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15113" title="Abstract">arXiv:2402.15113</a> [<a href="/pdf/2402.15113" title="Download PDF">pdf</a>, <a href="/format/2402.15113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sheng%2C+G">Guangming Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Junwei Su</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal
graph neural networks that utilize a node memory module to capture and retain
long-term temporal dependencies, leading to superior performance compared to
memory-less counterparts. However, the iterative reading and updating process
of the memory module in MTGNNs to obtain up-to-date information needs to follow
the temporal dependencies. This introduces significant overhead and limits
training throughput. Existing optimizations for static GNNs are not directly
applicable to MTGNNs due to differences in training paradigm, model
architecture, and the absence of a memory module. Moreover, they do not
effectively address the challenges posed by temporal dependencies, making them
ineffective for MTGNN training. In this paper, we propose MSPipe, a general and
efficient framework for MTGNNs that maximizes training throughput while
maintaining model accuracy. Our design addresses the unique challenges
associated with fetching and updating node memory states in MTGNNs by
integrating staleness into the memory module. However, simply introducing a
predefined staleness bound in the memory module to break temporal dependencies
may lead to suboptimal performance and lack of generalizability across
different models and datasets. To solve this, we introduce an online pipeline
scheduling algorithm in MSPipe that strategically breaks temporal dependencies
with minimal staleness and delays memory fetching to obtain fresher memory
states. Moreover, we design a staleness mitigation mechanism to enhance
training convergence and model accuracy. We provide convergence analysis and
prove that MSPipe maintains the same convergence rate as vanilla sample-based
GNN training. Experimental results show that MSPipe achieves up to 2.45x
speed-up without sacrificing accuracy, making it a promising solution for
efficient MTGNN training.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15116" title="Abstract">arXiv:2402.15116</a> [<a href="/pdf/2402.15116" title="Download PDF">pdf</a>, <a href="/format/2402.15116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Multimodal Agents: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Junlin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhihong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have achieved superior performance in powering
text-based AI agents, endowing them with decision-making and reasoning
abilities akin to humans. Concurrently, there is an emerging research trend
focused on extending these LLM-powered AI agents into the multimodal domain.
This extension enables AI agents to interpret and respond to diverse multimodal
user queries, thereby handling more intricate and nuanced tasks. In this paper,
we conduct a systematic review of LLM-driven multimodal agents, which we refer
to as large multimodal agents ( LMAs for short). First, we introduce the
essential components involved in developing LMAs and categorize the current
body of research into four distinct types. Subsequently, we review the
collaborative frameworks integrating multiple LMAs , enhancing collective
efficacy. One of the critical challenges in this field is the diverse
evaluation methods used across existing studies, hindering effective comparison
among different LMAs . Therefore, we compile these evaluation methodologies and
establish a comprehensive framework to bridge the gaps. This framework aims to
standardize evaluations, facilitating more meaningful comparisons. Concluding
our review, we highlight the extensive applications of LMAs and propose
possible future research directions. Our discussion aims to provide valuable
insights and guidelines for future research in this rapidly evolving field. An
up-to-date resource list is available at
https://github.com/jun0wanan/awesome-large-multimodal-agents.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15119" title="Abstract">arXiv:2402.15119</a> [<a href="/pdf/2402.15119" title="Download PDF">pdf</a>, <a href="/ps/2402.15119" title="Download PostScript">ps</a>, <a href="/format/2402.15119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multidisciplinary framework for deconstructing bots&#x27; pluripotency in  dualistic antagonism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wentao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sasahara%2C+K">Kazutoshi Sasahara</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+J">Jianxun Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenlu Fan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiwen Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Anthropomorphic social bots are engineered to emulate human verbal
communication and generate toxic or inflammatory content across social
networking services (SNSs). Bot-disseminated misinformation could subtly yet
profoundly reshape societal processes by complexly interweaving factors like
repeated disinformation exposure, amplified political polarization, compromised
indicators of democratic health, shifted perceptions of national identity,
propagation of false social norms, and manipulation of collective memory over
time. However, extrapolating bots' pluripotency across hybridized,
multilingual, and heterogeneous media ecologies from isolated SNS analyses
remains largely unknown, underscoring the need for a comprehensive framework to
characterise bots' emergent risks to civic discourse. Here we propose an
interdisciplinary framework to characterise bots' pluripotency, incorporating
quantification of influence, network dynamics monitoring, and interlingual
feature analysis. When applied to the geopolitical discourse around the
Russo-Ukrainian conflict, results from interlanguage toxicity profiling and
network analysis elucidated spatiotemporal trajectories of pro-Russian and
pro-Ukrainian human and bots across hybrid SNSs. Weaponized bots predominantly
inhabited X, while human primarily populated Reddit in the social media
warfare. This rigorous framework promises to elucidate interlingual homogeneity
and heterogeneity in bots' pluripotent behaviours, revealing synergistic
human-bot mechanisms underlying regimes of information manipulation, echo
chamber formation, and collective memory manifestation in algorithmically
structured societies.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15120" title="Abstract">arXiv:2402.15120</a> [<a href="/pdf/2402.15120" title="Download PDF">pdf</a>, <a href="/format/2402.15120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning CLIP Text Encoders with Two-step Paraphrasing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunjae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Seunghyun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+T">Trung Bui</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Handong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+Q">Quan Tran</a>, 
<a href="/search/cs?searchtype=author&query=Dernoncourt%2C+F">Franck Dernoncourt</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jaewoo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 (Findings of the ACL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Contrastive language-image pre-training (CLIP) models have demonstrated
considerable success across various vision-language tasks, such as
text-to-image retrieval, where the model is required to effectively process
natural language input to produce an accurate visual output. However, current
models still face limitations in dealing with linguistic variations in input
queries, such as paraphrases, making it challenging to handle a broad range of
user queries in real-world applications. In this study, we introduce a
straightforward fine-tuning approach to enhance the representations of CLIP
models for paraphrases. Our approach involves a two-step paraphrase generation
process, where we automatically create two categories of paraphrases from
web-scale image captions by leveraging large language models. Subsequently, we
fine-tune the CLIP text encoder using these generated paraphrases while
freezing the image encoder. Our resulting model, which we call ParaCLIP,
exhibits significant improvements over baseline CLIP models across various
tasks, including paraphrased retrieval (with rank similarity scores improved by
up to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven
semantic textual similarity tasks.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15121" title="Abstract">arXiv:2402.15121</a> [<a href="/pdf/2402.15121" title="Download PDF">pdf</a>, <a href="/format/2402.15121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward High Performance, Programmable Extreme-Edge Intelligence for  Neuromorphic Vision Sensors utilizing Magnetic Domain Wall Motion-based MTJ
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaiser%2C+M+A">Md Abdullah-Al Kaiser</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+G">Gourav Datta</a>, 
<a href="/search/cs?searchtype=author&query=Beerel%2C+P+A">Peter A. Beerel</a>, 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A+R">Akhilesh R. Jaiswal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures, 2 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The desire to empower resource-limited edge devices with computer vision (CV)
must overcome the high energy consumption of collecting and processing vast
sensory data. To address the challenge, this work proposes an energy-efficient
non-von-Neumann in-pixel processing solution for neuromorphic vision sensors
employing emerging (X) magnetic domain wall magnetic tunnel junction (MDWMTJ)
for the first time, in conjunction with CMOS-based neuromorphic pixels. Our
hybrid CMOS+X approach performs in-situ massively parallel asynchronous analog
convolution, exhibiting low power consumption and high accuracy across various
CV applications by leveraging the non-volatility and programmability of the
MDWMTJ. Moreover, our developed device-circuit-algorithm co-design framework
captures device constraints (low tunnel-magnetoresistance, low dynamic range)
and circuit constraints (non-linearity, process variation, area consideration)
based on monte-carlo simulations and device parameters utilizing GF22nm FD-SOI
technology. Our experimental results suggest we can achieve an average of 45.3%
reduction in backend-processor energy, maintaining similar front-end energy
compared to the state-of-the-art and high accuracy of 79.17% and 95.99% on the
DVS-CIFAR10 and IBM DVS128-Gesture datasets, respectively.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15125" title="Abstract">arXiv:2402.15125</a> [<a href="/pdf/2402.15125" title="Download PDF">pdf</a>, <a href="/format/2402.15125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Convergence of Stein Variational Gradient Descent via Deep  Unfolding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kawamura%2C+Y">Yuya Kawamura</a>, 
<a href="/search/cs?searchtype=author&query=Takabe%2C+S">Satoshi Takabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Stein variational gradient descent (SVGD) is a prominent particle-based
variational inference method used for sampling a target distribution. SVGD has
attracted interest for application in machine-learning techniques such as
Bayesian inference. In this paper, we propose novel trainable algorithms that
incorporate a deep-learning technique called deep unfolding,into SVGD. This
approach facilitates the learning of the internal parameters of SVGD, thereby
accelerating its convergence speed. To evaluate the proposed trainable SVGD
algorithms, we conducted numerical simulations of three tasks: sampling a
one-dimensional Gaussian mixture, performing Bayesian logistic regression, and
learning Bayesian neural networks. The results show that our proposed
algorithms exhibit faster convergence than the conventional variants of SVGD.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15127" title="Abstract">arXiv:2402.15127</a> [<a href="/pdf/2402.15127" title="Download PDF">pdf</a>, <a href="/format/2402.15127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Armed Bandits with Abstention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+T">Tianyuan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+V+Y+F">Vincent Y. F. Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce a novel extension of the canonical multi-armed bandit problem
that incorporates an additional strategic element: abstention. In this enhanced
framework, the agent is not only tasked with selecting an arm at each time
step, but also has the option to abstain from accepting the stochastic
instantaneous reward before observing it. When opting for abstention, the agent
either suffers a fixed regret or gains a guaranteed reward. Given this added
layer of complexity, we ask whether we can develop efficient algorithms that
are both asymptotically and minimax optimal. We answer this question
affirmatively by designing and analyzing algorithms whose regrets meet their
corresponding information-theoretic lower bounds. Our results offer valuable
quantitative insights into the benefits of the abstention option, laying the
groundwork for further exploration in other online decision-making problems
with such an option. Numerical results further corroborate our theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15131" title="Abstract">arXiv:2402.15131</a> [<a href="/pdf/2402.15131" title="Download PDF">pdf</a>, <a href="/format/2402.15131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question  Answering with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+G">Guanming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+J">Junwei Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wen Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Codes will be released upon acceptance
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study explores the realm of knowledge-base question answering (KBQA).
KBQA is considered a challenging task, particularly in parsing intricate
questions into executable logical forms. Traditional semantic parsing
(SP)-based methods require extensive data annotations, which result in
significant costs. Recently, the advent of few-shot in-context learning,
powered by large language models (LLMs), has showcased promising capabilities.
Yet, fully leveraging LLMs to parse questions into logical forms in
low-resource scenarios poses a substantial challenge. To tackle these hurdles,
we introduce Interactive-KBQA, a framework designed to generate logical forms
through direct interaction with knowledge bases (KBs). Within this framework,
we have developed three generic APIs for KB interaction. For each category of
complex question, we devised exemplars to guide LLMs through the reasoning
processes. Our method achieves competitive results on the WebQuestionsSP,
ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of
examples (shots). Importantly, our approach supports manual intervention,
allowing for the iterative refinement of LLM outputs. By annotating a dataset
with step-wise reasoning processes, we showcase our model's adaptability and
highlight its potential for contributing significant enhancements to the field.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15132" title="Abstract">arXiv:2402.15132</a> [<a href="/pdf/2402.15132" title="Download PDF">pdf</a>, <a href="/format/2402.15132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Sentence Embeddings with an Automatically Generated NLI  Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sato%2C+S">Soma Sato</a>, 
<a href="/search/cs?searchtype=author&query=Tsukagoshi%2C+H">Hayato Tsukagoshi</a>, 
<a href="/search/cs?searchtype=author&query=Sasano%2C+R">Ryohei Sasano</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+K">Koichi Takeda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Decoder-based large language models (LLMs) have shown high performance on
many tasks in natural language processing. This is also true for sentence
embedding learning, where a decoder-based model, PromptEOL, has achieved the
best performance on semantic textual similarity (STS) tasks. However, PromptEOL
makes great use of fine-tuning with a manually annotated natural language
inference (NLI) dataset. We aim to improve sentence embeddings learned in an
unsupervised setting by automatically generating an NLI dataset with an LLM and
using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed
method achieved an average Spearman's rank correlation coefficient of 82.21
with respect to human evaluation, thus outperforming existing methods without
using large, manually annotated datasets.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15134" title="Abstract">arXiv:2402.15134</a> [<a href="/pdf/2402.15134" title="Download PDF">pdf</a>, <a href="/format/2402.15134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Coupling Network For Multivariate Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+K">Kun Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hui He</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+K">Kaize Shi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Liang Hu</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+N">Ning An</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhendong Niu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multivariate time series (MTS) forecasting is crucial in many real-world
applications. To achieve accurate MTS forecasting, it is essential to
simultaneously consider both intra- and inter-series relationships among time
series data. However, previous work has typically modeled intra- and
inter-series relationships separately and has disregarded multi-order
interactions present within and between time series data, which can seriously
degrade forecasting accuracy. In this paper, we reexamine intra- and
inter-series relationships from the perspective of mutual information and
accordingly construct a comprehensive relationship learning mechanism tailored
to simultaneously capture the intricate multi-order intra- and inter-series
couplings. Based on the mechanism, we propose a novel deep coupling network for
MTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated
to explicitly exploring the multi-order intra- and inter-series relationships
among time series data concurrently, a coupled variable representation module
aimed at encoding diverse variable patterns, and an inference module
facilitating predictions through one forward step. Extensive experiments
conducted on seven real-world datasets demonstrate that our proposed DeepCN
achieves superior performance compared with the state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15135" title="Abstract">arXiv:2402.15135</a> [<a href="/pdf/2402.15135" title="Download PDF">pdf</a>, <a href="/format/2402.15135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modified CycleGAN for the synthesization of samples for wheat head  segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Myers%2C+J">Jaden Myers</a>, 
<a href="/search/cs?searchtype=author&query=Najafian%2C+K">Keyhan Najafian</a>, 
<a href="/search/cs?searchtype=author&query=Maleki%2C+F">Farhad Maleki</a>, 
<a href="/search/cs?searchtype=author&query=Ovens%2C+K">Katie Ovens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep learning models have been used for a variety of image processing tasks.
However, most of these models are developed through supervised learning
approaches, which rely heavily on the availability of large-scale annotated
datasets. Developing such datasets is tedious and expensive. In the absence of
an annotated dataset, synthetic data can be used for model development;
however, due to the substantial differences between simulated and real data, a
phenomenon referred to as domain gap, the resulting models often underperform
when applied to real data. In this research, we aim to address this challenge
by first computationally simulating a large-scale annotated dataset and then
using a generative adversarial network (GAN) to fill the gap between simulated
and real images. This approach results in a synthetic dataset that can be
effectively utilized to train a deep-learning model. Using this approach, we
developed a realistic annotated synthetic dataset for wheat head segmentation.
This dataset was then used to develop a deep-learning model for semantic
segmentation. The resulting model achieved a Dice score of 83.4\% on an
internal dataset and Dice scores of 79.6% and 83.6% on two external Global
Wheat Head Detection datasets. While we proposed this approach in the context
of wheat head segmentation, it can be generalized to other crop types or, more
broadly, to images with dense, repeated patterns such as those found in
cellular imagery.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15140" title="Abstract">arXiv:2402.15140</a> [<a href="/pdf/2402.15140" title="Download PDF">pdf</a>, <a href="/format/2402.15140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Relation-Interactive Approach for Message Passing in Hyper-relational  Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jing%2C+Y">Yonglin Jing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Hyper-relational knowledge graphs (KGs) contain additional key-value pairs,
providing more information about the relations. In many scenarios, the same
relation can have distinct key-value pairs, making the original triple fact
more recognizable and specific. Prior studies on hyper-relational KGs have
established a solid standard method for hyper-relational graph encoding. In
this work, we propose a message-passing-based graph encoder with global
relation structure awareness ability, which we call ReSaE. Compared to the
prior state-of-the-art approach, ReSaE emphasizes the interaction of relations
during message passing process and optimizes the readout structure for link
prediction tasks. Overall, ReSaE gives a encoding solution for hyper-relational
KGs and ensures stronger performance on downstream link prediction tasks. Our
experiments demonstrate that ReSaE achieves state-of-the-art performance on
multiple link prediction benchmarks. Furthermore, we also analyze the influence
of different model structures on model performance.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15141" title="Abstract">arXiv:2402.15141</a> [<a href="/pdf/2402.15141" title="Download PDF">pdf</a>, <a href="/ps/2402.15141" title="Download PostScript">ps</a>, <a href="/format/2402.15141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on the adjoint method for neural ordinary differential equation  network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+P">Pipi Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Perturbation and operator adjoint method are used to give the right adjoint
form rigourously. From the derivation, we can have following results: 1) The
loss gradient is not an ODE, it is an integral and we shows the reason; 2) The
traditional adjoint form is not equivalent with the back propagation results.
3) The adjoint operator analysis shows that if and only if the discrete adjoint
has the same scheme with the discrete neural ODE, the adjoint form would give
the same results as BP does.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15142" title="Abstract">arXiv:2402.15142</a> [<a href="/pdf/2402.15142" title="Download PDF">pdf</a>, <a href="/format/2402.15142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher-Order Energy-Decreasing Exponential Time Differencing Runge-Kutta  methods for Gradient Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fu%2C+Z">Zhaohui Fu</a>, 
<a href="/search/math?searchtype=author&query=Shen%2C+J">Jie Shen</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+J">Jiang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we develop a general framework for constructing higher-order,
unconditionally energy-stable exponential time differencing Runge-Kutta methods
applicable to a range of gradient flows. Specifically, we identify conditions
sufficient for ETDRK schemes to maintain the original energy dissipation. Our
analysis reveals that commonly used third-order and fourth-order ETDRK schemes
fail to meet these conditions. To address this, we introduce new third-order
ETDRK schemes, designed with appropriate stabilization, which satisfy these
conditions and thus guarantee the unconditional energy decaying property. We
conduct extensive numerical experiments with these new schemes to verify their
accuracy, stability, behavior under large time steps, long-term evolution, and
adaptive time stepping strategy across various gradient flows. This study is
the first to examine the unconditional energy stability of high-order ETDRK
methods, and we are optimistic that our framework will enable the development
of ETDRK schemes beyond the third order that are unconditionally energy stable.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15143" title="Abstract">arXiv:2402.15143</a> [<a href="/pdf/2402.15143" title="Download PDF">pdf</a>, <a href="/format/2402.15143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PUAD: Frustratingly Simple Method for Robust Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sugawara%2C+S">Shota Sugawara</a>, 
<a href="/search/cs?searchtype=author&query=Imamura%2C+R">Ryuji Imamura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Developing an accurate and fast anomaly detection model is an important task
in real-time computer vision applications. There has been much research to
develop a single model that detects either structural or logical anomalies,
which are inherently distinct. The majority of the existing approaches
implicitly assume that the anomaly can be represented by identifying the
anomalous location. However, we argue that logical anomalies, such as the wrong
number of objects, can not be well-represented by the spatial feature maps and
require an alternative approach. In addition, we focused on the possibility of
detecting logical anomalies by using an out-of-distribution detection approach
on the feature space, which aggregates the spatial information of the feature
map. As a demonstration, we propose a method that incorporates a simple
out-of-distribution detection method on the feature space against
state-of-the-art reconstruction-based approaches. Despite the simplicity of our
proposal, our method PUAD (Picturable and Unpicturable Anomaly Detection)
achieves state-of-the-art performance on the MVTec LOCO AD dataset.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15145" title="Abstract">arXiv:2402.15145</a> [<a href="/pdf/2402.15145" title="Download PDF">pdf</a>, <a href="/ps/2402.15145" title="Download PostScript">ps</a>, <a href="/format/2402.15145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Cost of Parallelizing Boosting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xin Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongxun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junzhao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> appeared in SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study the cost of parallelizing weak-to-strong boosting algorithms for
learning, following the recent work of Karbasi and Larsen. Our main results are
two-fold:
<br />- First, we prove a tight lower bound, showing that even "slight"
parallelization of boosting requires an exponential blow-up in the complexity
of training.
<br />Specifically, let $\gamma$ be the weak learner's advantage over random
guessing. The famous \textsc{AdaBoost} algorithm produces an accurate
hypothesis by interacting with the weak learner for $\tilde{O}(1 / \gamma^2)$
rounds where each round runs in polynomial time.
<br />Karbasi and Larsen showed that "significant" parallelization must incur
exponential blow-up: Any boosting algorithm either interacts with the weak
learner for $\Omega(1 / \gamma)$ rounds or incurs an $\exp(d / \gamma)$ blow-up
in the complexity of training, where $d$ is the VC dimension of the hypothesis
class. We close the gap by showing that any boosting algorithm either has
$\Omega(1 / \gamma^2)$ rounds of interaction or incurs a smaller exponential
blow-up of $\exp(d)$.
<br />-Complementing our lower bound, we show that there exists a boosting
algorithm using $\tilde{O}(1/(t \gamma^2))$ rounds, and only suffer a blow-up
of $\exp(d \cdot t^2)$.
<br />Plugging in $t = \omega(1)$, this shows that the smaller blow-up in our lower
bound is tight. More interestingly, this provides the first trade-off between
the parallelism and the total work required for boosting.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15146" title="Abstract">arXiv:2402.15146</a> [<a href="/pdf/2402.15146" title="Download PDF">pdf</a>, <a href="/format/2402.15146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence Analysis of Blurring Mean Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamasaki%2C+R">Ryoya Yamasaki</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+T">Toshiyuki Tanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Blurring mean shift, mean shift, clustering, convergence, kernel. arXiv admin note: text overlap with <a href="/abs/2305.08463">arXiv:2305.08463</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Blurring mean shift (BMS) algorithm, a variant of the mean shift algorithm,
is a kernel-based iterative method for data clustering, where data points are
clustered according to their convergent points via iterative blurring. In this
paper, we analyze convergence properties of the BMS algorithm by leveraging its
interpretation as an optimization procedure, which is known but has been
underutilized in existing convergence studies. Whereas existing results on
convergence properties applicable to multi-dimensional data only cover the case
where all the blurred data point sequences converge to a single point, this
study provides a convergence guarantee even when those sequences can converge
to multiple points, yielding multiple clusters. This study also shows that the
convergence of the BMS algorithm is fast by further leveraging geometrical
characterization of the convergent points.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15147" title="Abstract">arXiv:2402.15147</a> [<a href="/pdf/2402.15147" title="Download PDF">pdf</a>, <a href="/format/2402.15147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TREC: APT Tactic / Technique Recognition via Few-Shot Provenance  Subgraph Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+M">Mingqi Lv</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">HongZhe Gao</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xuebo Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tieming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tiantian Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">APT (Advanced Persistent Threat) with the characteristics of persistence,
stealth, and diversity is one of the greatest threats against
cyber-infrastructure. As a countermeasure, existing studies leverage provenance
graphs to capture the complex relations between system entities in a host for
effective APT detection. In addition to detecting single attack events as most
existing work does, understanding the tactics / techniques (e.g., Kill-Chain,
ATT&amp;CK) applied to organize and accomplish the APT attack campaign is more
important for security operations. Existing studies try to manually design a
set of rules to map low-level system events to high-level APT tactics /
techniques. However, the rule based methods are coarse-grained and lack
generalization ability, thus they can only recognize APT tactics and cannot
identify fine-grained APT techniques and mutant APT attacks. In this paper, we
propose TREC, the first attempt to recognize APT tactics / techniques from
provenance graphs by exploiting deep learning techniques. To address the
"needle in a haystack" problem, TREC segments small and compact subgraphs
covering individual APT technique instances from a large provenance graph based
on a malicious node detection model and a subgraph sampling algorithm. To
address the "training sample scarcity" problem, TREC trains the APT tactic /
technique recognition model in a few-shot learning manner by adopting a Siamese
neural network. We evaluate TREC based on a customized dataset collected and
made public by our team. The experiment results show that TREC significantly
outperforms state-of-the-art systems in APT tactic recognition and TREC can
also effectively identify APT techniques.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15150" title="Abstract">arXiv:2402.15150</a> [<a href="/pdf/2402.15150" title="Download PDF">pdf</a>, <a href="/format/2402.15150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Runge--Kutta discontinuous Galerkin method with stage-dependent  polynomial spaces for hyperbolic conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+Q">Qifan Chen</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+Z">Zheng Sun</a>, 
<a href="/search/math?searchtype=author&query=Xing%2C+Y">Yulong Xing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we present a novel class of high-order Runge--Kutta (RK)
discontinuous Galerkin (DG) schemes for hyperbolic conservation laws. The new
method extends beyond the traditional method of lines framework and utilizes
stage-dependent polynomial spaces for the spatial discretization operators. To
be more specific, two different DG operators, associated with $\mathcal{P}^k$
and $\mathcal{P}^{k-1}$ piecewise polynomial spaces, are used at different RK
stages. The resulting method is referred to as the sdRKDG method. It features
fewer floating-point operations and may achieve larger time step sizes. For
problems without sonic points, we observe optimal convergence for all the
sdRKDG schemes; and for problems with sonic points, we observe that a subset of
the sdRKDG schemes remains optimal. We have also conducted von Neumann analysis
for the stability and error of the sdRKDG schemes for the linear advection
equation in one dimension. Numerical tests, for problems including
two-dimensional Euler equations for gas dynamics, are provided to demonstrate
the performance of the new method.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15151" title="Abstract">arXiv:2402.15151</a> [<a href="/pdf/2402.15151" title="Download PDF">pdf</a>, <a href="/format/2402.15151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and  Context-Aware Visual Speech Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeo%2C+J+H">Jeong Hun Yeo</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Seunghee Han</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ro%2C+Y+M">Yong Man Ro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In visual speech processing, context modeling capability is one of the most
important requirements due to the ambiguous nature of lip movements. For
example, homophenes, words that share identical lip movements but produce
different sounds, can be distinguished by considering the context. In this
paper, we propose a novel framework, namely Visual Speech Processing
incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by
bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to
perform multi-tasks of visual speech recognition and translation, where the
given instructions control the type of task. The input video is mapped to the
input latent space of a LLM by employing a self-supervised visual speech model.
Focused on the fact that there is redundant information in input frames, we
propose a novel deduplication method that reduces the embedded visual features
by employing visual speech units. Through the proposed deduplication and Low
Rank Adaptors (LoRA), VSP-LLM can be trained in a computationally efficient
manner. In the translation dataset, the MuAViC benchmark, we demonstrate that
VSP-LLM can more effectively recognize and translate lip movements with just 15
hours of labeled data, compared to the recent translation model trained with
433 hours of labeld data.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15152" title="Abstract">arXiv:2402.15152</a> [<a href="/pdf/2402.15152" title="Download PDF">pdf</a>, <a href="/ps/2402.15152" title="Download PostScript">ps</a>, <a href="/format/2402.15152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Duality Between Sharpness-Aware Minimization and Adversarial  Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hangzhou He</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jingyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huanran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zeming Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2305.05392">arXiv:2305.05392</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Optimization and Control (math.OC)

</div>
<p class="mathjax">Adversarial Training (AT), which adversarially perturb the input samples
during training, has been acknowledged as one of the most effective defenses
against adversarial attacks, yet suffers from a fundamental tradeoff that
inevitably decreases clean accuracy. Instead of perturbing the samples,
Sharpness-Aware Minimization (SAM) perturbs the model weights during training
to find a more flat loss landscape and improve generalization. However, as SAM
is designed for better clean accuracy, its effectiveness in enhancing
adversarial robustness remains unexplored. In this work, considering the
duality between SAM and AT, we investigate the adversarial robustness derived
from SAM. Intriguingly, we find that using SAM alone can improve adversarial
robustness. To understand this unexpected property of SAM, we first provide
empirical and theoretical insights into how SAM can implicitly learn more
robust features, and conduct comprehensive experiments to show that SAM can
improve adversarial robustness notably without sacrificing any clean accuracy,
shedding light on the potential of SAM to be a substitute for AT when accuracy
comes at a higher priority. Code is available at
https://github.com/weizeming/SAM_AT.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15153" title="Abstract">arXiv:2402.15153</a> [<a href="/pdf/2402.15153" title="Download PDF">pdf</a>, <a href="/format/2402.15153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised  Sentence Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junlong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+X">Xichen Shang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Huawen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Junhao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Unsupervised sentence embeddings task aims to convert sentences to semantic
vector representations. Most previous works directly use the sentence
representations derived from pretrained language models. However, due to the
token bias in pretrained language models, the models can not capture the
fine-grained semantics in sentences, which leads to poor predictions. To
address this issue, we propose a novel Self-Adaptive Reconstruction Contrastive
Sentence Embeddings (SARCSE) framework, which reconstructs all tokens in
sentences with an AutoEncoder to help the model to preserve more fine-grained
semantics during tokens aggregating. In addition, we proposed a self-adaptive
reconstruction loss to alleviate the token bias towards frequency. Experimental
results show that SARCSE gains significant improvements compared with the
strong baseline SimCSE on the 7 STS tasks.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15155" title="Abstract">arXiv:2402.15155</a> [<a href="/pdf/2402.15155" title="Download PDF">pdf</a>, <a href="/ps/2402.15155" title="Download PostScript">ps</a>, <a href="/format/2402.15155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmically Fair Maximization of Multiple Submodular Objective  Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amanatidis%2C+G">Georgios Amanatidis</a>, 
<a href="/search/cs?searchtype=author&query=Birmpas%2C+G">Georgios Birmpas</a>, 
<a href="/search/cs?searchtype=author&query=Lazos%2C+P">Philip Lazos</a>, 
<a href="/search/cs?searchtype=author&query=Leonardi%2C+S">Stefano Leonardi</a>, 
<a href="/search/cs?searchtype=author&query=Reiffenh%C3%A4user%2C+R">Rebecca Reiffenh&#xe4;user</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC)

</div>
<p class="mathjax">Constrained maximization of submodular functions poses a central problem in
combinatorial optimization. In many realistic scenarios, a number of agents
need to maximize multiple submodular objectives over the same ground set. We
study such a setting, where the different solutions must be disjoint, and thus,
questions of fairness arise. Inspired from the fair division literature, we
suggest a simple round-robin protocol, where agents are allowed to build their
solutions one item at a time by taking turns. Unlike what is typical in fair
division, however, the prime goal here is to provide a fair algorithmic
environment; each agent is allowed to use any algorithm for constructing their
respective solutions. We show that just by following simple greedy policies,
agents have solid guarantees for both monotone and non-monotone objectives, and
for combinatorial constraints as general as $p$-systems (which capture
cardinality and matroid intersection constraints). In the monotone case, our
results include approximate EF1-type guarantees and their implications in fair
division may be of independent interest. Further, although following a greedy
policy may not be optimal in general, we show that consistently performing
better than that is computationally hard.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15159" title="Abstract">arXiv:2402.15159</a> [<a href="/pdf/2402.15159" title="Download PDF">pdf</a>, <a href="/format/2402.15159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Unlearning of Pre-trained Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chien%2C+E">Eli Chien</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Minxin Du</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+X">Xinyao Niu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zezhou Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code will be available at <a href="https://github.com/yaojin17/Unlearning_LLM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">This study investigates the concept of the `right to be forgotten' within the
context of large language models (LLMs). We explore machine unlearning as a
pivotal solution, with a focus on pre-trained models--a notably
under-researched area. Our research delineates a comprehensive framework for
machine unlearning in pre-trained LLMs, encompassing a critical analysis of
seven diverse unlearning methods. Through rigorous evaluation using curated
datasets from arXiv, books, and GitHub, we establish a robust benchmark for
unlearning performance, demonstrating that these methods are over $10^5$ times
more computationally efficient than retraining. Our results show that
integrating gradient ascent with gradient descent on in-distribution data
improves hyperparameter robustness. We also provide detailed guidelines for
efficient hyperparameter tuning in the unlearning process. Our findings advance
the discourse on ethical AI practices, offering substantive insights into the
mechanics of machine unlearning for pre-trained LLMs and underscoring the
potential for responsible AI development.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15160" title="Abstract">arXiv:2402.15160</a> [<a href="/pdf/2402.15160" title="Download PDF">pdf</a>, <a href="/format/2402.15160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatially-Aware Transformer Memory for Embodied Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Junmo Cho</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaesik Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungjin Ahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Spotlight. First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Episodic memory plays a crucial role in various cognitive processes, such as
the ability to mentally recall past events. While cognitive science emphasizes
the significance of spatial context in the formation and retrieval of episodic
memory, the current primary approach to implementing episodic memory in AI
systems is through transformers that store temporally ordered experiences,
which overlooks the spatial dimension. As a result, it is unclear how the
underlying structure could be extended to incorporate the spatial axis beyond
temporal order alone and thereby what benefits can be obtained. To address
this, this paper explores the use of Spatially-Aware Transformer models that
incorporate spatial information. These models enable the creation of
place-centric episodic memory that considers both temporal and spatial
dimensions. Adopting this approach, we demonstrate that memory utilization
efficiency can be improved, leading to enhanced accuracy in various
place-centric downstream tasks. Additionally, we propose the Adaptive Memory
Allocator, a memory management method based on reinforcement learning that aims
to optimize efficiency of memory utilization. Our experiments demonstrate the
advantages of our proposed model in various environments and across multiple
downstream tasks, including prediction, generation, reasoning, and
reinforcement learning. The source code for our models and experiments will be
available at https://github.com/junmokane/spatially-aware-transformer.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15161" title="Abstract">arXiv:2402.15161</a> [<a href="/pdf/2402.15161" title="Download PDF">pdf</a>, <a href="/format/2402.15161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A unified constraint formulation of immersed body techniques for coupled  fluid-solid motion: continuous equations and numerical algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bhalla%2C+A+P+S">Amneet Pal Singh Bhalla</a>, 
<a href="/search/math?searchtype=author&query=Patankar%2C+N+A">Neelesh A. Patankar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Numerical simulation of moving immersed solid bodies in fluids is now
practiced routinely following pioneering work of Peskin and co-workers on
immersed boundary method (IBM), Glowinski and co-workers on fictitious domain
method (FDM), and others on related methods. A variety of variants of IBM and
FDM approaches have been published, most of which rely on using a background
mesh for the fluid equations and tracking the solid body using Lagrangian
points. The key idea that is common to these methods is to assume that the
entire fluid-solid domain is a fluid and then to constrain the fluid within the
solid domain to move in accordance with the solid governing equations. The
immersed solid body can be rigid or deforming. Thus, in all these methods the
fluid domain is extended into the solid domain. In this review, we provide a
mathemarical perspective of various immersed methods by recasting the governing
equations in an extended domain form for the fluid. The solid equations are
used to impose appropriate constraints on the fluid that is extended into the
solid domain. This leads to extended domain constrained fluid-solid governing
equations that provide a unified framework for various immersed body
techniques. The unified constrained governing equations in the strong form are
independent of the temporal or spatial discretization schemes. We show that
particular choices of time stepping and spatial discretization lead to
different techniques reported in literature ranging from freely moving rigid to
elastic self-propelling bodies. These techniques have wide ranging applications
including aquatic locomotion, underwater vehicles, car aerodynamics, and organ
physiology (e.g. cardiac flow, esophageal transport, respiratory flows), wave
energy convertors, among others. We conclude with comments on outstanding
challenges and future directions.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15162" title="Abstract">arXiv:2402.15162</a> [<a href="/pdf/2402.15162" title="Download PDF">pdf</a>, <a href="/format/2402.15162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entity-level Factual Adaptiveness of Fine-tuning based Abstractive  Summarization Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jongyoon Song</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+N">Nohil Park</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+B">Bongkyu Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+J">Jaewoong Yun</a>, 
<a href="/search/cs?searchtype=author&query=Joe%2C+S">Seongho Joe</a>, 
<a href="/search/cs?searchtype=author&query=Gwon%2C+Y+L">Youngjune L. Gwon</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sungroh Yoon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Abstractive summarization models often generate factually inconsistent
content particularly when the parametric knowledge of the model conflicts with
the knowledge in the input document. In this paper, we analyze the robustness
of fine-tuning based summarization models to the knowledge conflict, which we
call factual adaptiveness. We utilize pre-trained language models to construct
evaluation sets and find that factual adaptiveness is not strongly correlated
with factual consistency on original datasets. Furthermore, we introduce a
controllable counterfactual data augmentation method where the degree of
knowledge conflict within the augmented data can be adjustable. Our
experimental results on two pre-trained language models (PEGASUS and BART) and
two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method
enhances factual adaptiveness while achieving factual consistency on original
datasets on par with the contrastive learning baseline.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15163" title="Abstract">arXiv:2402.15163</a> [<a href="/pdf/2402.15163" title="Download PDF">pdf</a>, <a href="/format/2402.15163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Studying the Impact of Stochasticity on the Evaluation of Deep Neural  Networks for Forest-Fire Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+H">Harshit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+B">Biswadeep Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+B">Beomseok Kang</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+S">Saibal Mukhopadhyay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Initial draft submitted to KDD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents the first systematic study of the evaluation of Deep
Neural Networks (DNNs) for discrete dynamical systems under stochastic
assumptions, with a focus on wildfire prediction. We develop a framework to
study the impact of stochasticity on two classes of evaluation metrics:
classification-based metrics, which assess fidelity to observed ground truth
(GT), and proper scoring rules, which test fidelity-to-statistic. Our findings
reveal that evaluating for fidelity-to-statistic is a reliable alternative in
highly stochastic scenarios. We extend our analysis to real-world wildfire
data, highlighting limitations in traditional wildfire prediction evaluation
methods, and suggest interpretable stochasticity-compatible alternatives.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15164" title="Abstract">arXiv:2402.15164</a> [<a href="/pdf/2402.15164" title="Download PDF">pdf</a>, <a href="/format/2402.15164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning  Based Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yuanqing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chongming Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiawei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Heng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuefeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weizhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly
recognized for their ability to improve long-term user engagement. Yet, the
field grapples with challenges such as the absence of accessible frameworks,
inconsistent evaluation standards, and the complexity of replicating prior
work. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and
efficient library tailored for RL-based RSs. EasyRL4Rec features lightweight,
diverse RL environments built on five widely-used public datasets, and is
equipped with comprehensive core modules that offer rich options to ease the
development of models. It establishes consistent evaluation criteria with a
focus on long-term impacts and introduces customized solutions for state
modeling and action representation tailored to recommender systems.
Additionally, we share valuable insights gained from extensive experiments with
current methods. EasyRL4Rec aims to facilitate the model development and
experimental process in the domain of RL-based RSs. The library is openly
accessible at https://github.com/chongminggao/EasyRL4Rec.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15166" title="Abstract">arXiv:2402.15166</a> [<a href="/pdf/2402.15166" title="Download PDF">pdf</a>, <a href="/format/2402.15166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence Analysis of Split Federated Learning on Heterogeneous Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+P">Pengchao Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+G">Geng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+M">Ming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Split federated learning (SFL) is a recent distributed approach for
collaborative model training among multiple clients. In SFL, a global model is
typically split into two parts, where clients train one part in a parallel
federated manner, and a main server trains the other. Despite the recent
research on SFL algorithm development, the convergence analysis of SFL is
missing in the literature, and this paper aims to fill this gap. The analysis
of SFL can be more challenging than that of federated learning (FL), due to the
potential dual-paced updates at the clients and the main server. We provide
convergence analysis of SFL for strongly convex and general convex objectives
on heterogeneous data. The convergence rates are $O(1/T)$ and
$O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds
for SFL training. We further extend the analysis to non-convex objectives and
where some clients may be unavailable during training. Numerical experiments
validate our theoretical results and show that SFL outperforms FL and split
learning (SL) when data is highly heterogeneous across a large number of
clients.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15169" title="Abstract">arXiv:2402.15169</a> [<a href="/pdf/2402.15169" title="Download PDF">pdf</a>, <a href="/ps/2402.15169" title="Download PostScript">ps</a>, <a href="/format/2402.15169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Platforms for Efficient and Incentive-Aware Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haghtalab%2C+N">Nika Haghtalab</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+M">Mingda Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kunhe Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Collaboration is crucial for reaching collective goals. However, its
effectiveness is often undermined by the strategic behavior of individual
agents -- a fact that is captured by a high Price of Stability (PoS) in recent
literature [Blum et al., 2021]. Implicit in the traditional PoS analysis is the
assumption that agents have full knowledge of how their tasks relate to one
another. We offer a new perspective on bringing about efficient collaboration
among strategic agents using information design. Inspired by the growing
importance of collaboration in machine learning (such as platforms for
collaborative federated learning and data cooperatives), we propose a framework
where the platform has more information about how the agents' tasks relate to
each other than the agents themselves. We characterize how and to what degree
such platforms can leverage their information advantage to steer strategic
agents toward efficient collaboration.
<br />Concretely, we consider collaboration networks where each node is a task type
held by one agent, and each task benefits from contributions made in their
inclusive neighborhood of tasks. This network structure is known to the agents
and the platform, but only the platform knows each agent's real location --
from the agents' perspective, their location is determined by a random
permutation. We employ private Bayesian persuasion and design two families of
persuasive signaling schemes that the platform can use to ensure a small total
workload when agents follow the signal. The first family aims to achieve the
minmax optimal approximation ratio compared to the optimal collaboration, which
is shown to be $\Theta(\sqrt{n})$ for unit-weight graphs, $\Theta(n^{2/3})$ for
graphs with constant minimum edge weights, and $O(n^{3/4})$ for general
weighted graphs. The second family ensures per-instance strict improvement
compared to full information disclosure.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15170" title="Abstract">arXiv:2402.15170</a> [<a href="/pdf/2402.15170" title="Download PDF">pdf</a>, <a href="/format/2402.15170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiajun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+S">Shuchen Xue</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaoqiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhi-Ming Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the incorporation of the UNet architecture, diffusion probabilistic
models have become a dominant force in image generation tasks. One key design
in UNet is the skip connections between the encoder and decoder blocks.
Although skip connections have been shown to improve training stability and
model performance, we reveal that such shortcuts can be a limiting factor for
the complexity of the transformation. As the sampling steps decrease, the
generation process and the role of the UNet get closer to the push-forward
transformations from Gaussian distribution to the target, posing a challenge
for the network's complexity. To address this challenge, we propose
Skip-Tuning, a simple yet surprisingly effective training-free tuning method on
the skip connections. Our method can achieve 100% FID improvement for
pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of
ODE samplers regardless of sampling steps. Surprisingly, the improvement
persists when we increase the number of sampling steps and can even surpass the
best result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive
exploratory experiments are conducted to shed light on the surprising
effectiveness. We observe that while Skip-Tuning increases the score-matching
losses in the pixel space, the losses in the feature space are reduced,
particularly at intermediate noise levels, which coincide with the most
effective range accounting for image quality improvement.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15171" title="Abstract">arXiv:2402.15171</a> [<a href="/pdf/2402.15171" title="Download PDF">pdf</a>, <a href="/format/2402.15171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial  Semi-Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Julien Zhou</a> (Thoth, STATIFY), 
<a href="/search/cs?searchtype=author&query=Gaillard%2C+P">Pierre Gaillard</a> (Thoth), 
<a href="/search/cs?searchtype=author&query=Rahier%2C+T">Thibaud Rahier</a>, 
<a href="/search/cs?searchtype=author&query=Zenati%2C+H">Houssam Zenati</a> (SODA, PREMEDICAL), 
<a href="/search/cs?searchtype=author&query=Arbel%2C+J">Julyan Arbel</a> (STATIFY)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We address the problem of stochastic combinatorial semi-bandits, where a
player can select from P subsets of a set containing d base items. Most
existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the
reward distribution, like an upper bound on a sub-Gaussian proxy-variance,
which is hard to estimate tightly. In this work, we design a variance-adaptive
version of OLS-UCB, relying on an online estimation of the covariance
structure. Estimating the coefficients of a covariance matrix is much more
manageable in practical settings and results in improved regret upper bounds
compared to proxy variance-based algorithms. When covariance coefficients are
all non-negative, we show that our approach efficiently leverages the
semi-bandit feedback and provably outperforms bandit feedback approaches, not
only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is
not straightforward from most existing analyses.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15172" title="Abstract">arXiv:2402.15172</a> [<a href="/pdf/2402.15172" title="Download PDF">pdf</a>, <a href="/format/2402.15172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-Guided Masked Autoencoders For Learning Image Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sick%2C+L">Leon Sick</a>, 
<a href="/search/cs?searchtype=author&query=Engel%2C+D">Dominik Engel</a>, 
<a href="/search/cs?searchtype=author&query=Hermosilla%2C+P">Pedro Hermosilla</a>, 
<a href="/search/cs?searchtype=author&query=Ropinski%2C+T">Timo Ropinski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Masked autoencoders (MAEs) have established themselves as a powerful method
for unsupervised pre-training for computer vision tasks. While vanilla MAEs put
equal emphasis on reconstructing the individual parts of the image, we propose
to inform the reconstruction process through an attention-guided loss function.
By leveraging advances in unsupervised object discovery, we obtain an attention
map of the scene which we employ in the loss function to put increased emphasis
on reconstructing relevant objects, thus effectively incentivizing the model to
learn more object-focused representations without compromising the established
masking strategy. Our evaluations show that our pre-trained models learn better
latent representations than the vanilla MAE, demonstrated by improved linear
probing and k-NN classification results on several benchmarks while at the same
time making ViTs more robust against varying backgrounds.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15173" title="Abstract">arXiv:2402.15173</a> [<a href="/pdf/2402.15173" title="Download PDF">pdf</a>, <a href="/format/2402.15173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed  Zeroth-Order Optimizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanjun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+S">Sizhe Dang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haishan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+G">Guang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+I+W">Ivor W.Tsang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fine-tuning large language models (LLMs) with classic first-order optimizers
entails prohibitive GPU memory due to the backpropagation process. Recent works
have turned to zeroth-order optimizers for fine-tuning, which save substantial
memory by using two forward passes. However, these optimizers are plagued by
the heterogeneity of parameter curvatures across different dimensions. In this
work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer
which is the first work to leverage the diagonal Hessian to enhance
zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the
expensive memory cost and only increases one forward pass per step. Extensive
experiments on various models (350M~66B parameters) indicate that HiZOO
improves model convergence, significantly reducing training steps and
effectively enhancing model accuracy. Moreover, we visualize the optimization
trajectories of HiZOO on test functions, illustrating its effectiveness in
handling heterogeneous curvatures. Lastly, we provide theoretical proofs of
convergence for HiZOO. Code is publicly available at
https://anonymous.4open.science/r/HiZOO27F8.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15174" title="Abstract">arXiv:2402.15174</a> [<a href="/pdf/2402.15174" title="Download PDF">pdf</a>, <a href="/format/2402.15174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Flower Calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Donato%2C+P">Pablo Donato</a> (PARTOUT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We introduce the flower calculus, a deep inference proof system for
intuitionistic first-order logic inspired by Peirce's existential graphs. It
works as a rewriting system over inductive objects called "flowers", that enjoy
both a graphical interpretation as topological diagrams, and a textual
presentation as nested sequents akin to coherent formulas. Importantly, the
calculus dispenses completely with the traditional notion of symbolic
connective, operating solely on nested flowers containing atomic predicates. We
prove both the soundness of the full calculus and the completeness of an
analytic fragment with respect to Kripke semantics. This provides to our
knowledge the first analyticity result for a proof system based on existential
graphs, adapting semantic cut-elimination techniques to a deep inference
setting. Furthermore, the kernel of rules targetted by completeness is fully
invertible, a desirable property for both automated and interactive proof
search.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15175" title="Abstract">arXiv:2402.15175</a> [<a href="/pdf/2402.15175" title="Download PDF">pdf</a>, <a href="/format/2402.15175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified View of Grokking, Double Descent and Emergent Abilities: A  Perspective from Circuits Competition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengding Hu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent studies have uncovered intriguing phenomena in deep learning, such as
grokking, double descent, and emergent abilities in large language models,
which challenge human intuition and are crucial for a deeper understanding of
neural models. In this paper, we present a comprehensive framework that
provides a unified view of these three phenomena, focusing on the competition
between memorization and generalization circuits. This approach, initially
employed to explain grokking, is extended in our work to encompass a wider
range of model sizes and training data volumes. Our framework delineates four
distinct training dynamics, each depending on varying combinations of model
size and training data quantity. Utilizing this framework, we provide a
detailed analysis of the double descent phenomenon and propose two verifiable
predictions regarding its occurrence, both substantiated by our experimental
results. Moreover, we expand our framework to the multi-task learning paradigm,
demonstrating how algorithm tasks can be turned into emergent abilities. This
offers a novel perspective to understand emergent abilities in Large Language
Models.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15179" title="Abstract">arXiv:2402.15179</a> [<a href="/pdf/2402.15179" title="Download PDF">pdf</a>, <a href="/format/2402.15179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Parameter Efficiency in Fine-tuning via Representation Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Muling Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaohua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Changze Lv</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zixuan Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianhao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cenyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for
its ability to achieve competitive results while updating only a small subset
of trainable parameters. Despite the promising performance of current PEFT
methods, they present challenges in hyperparameter selection, such as
determining the rank of LoRA or Adapter, or specifying the length of soft
prompts. In addressing these challenges, we propose a novel approach to
fine-tuning neural models, termed Representation EDiting (RED), which scales
and biases the representation produced at each layer. RED substantially reduces
the number of trainable parameters by a factor of $25,700$ compared to full
parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably,
RED achieves comparable or superior results to full parameter fine-tuning and
other PEFT methods. Extensive experiments were conducted across models of
varying architectures and scales, including RoBERTa, GPT-2, T5, and Llama-2,
and the results demonstrate the efficiency and efficacy of RED, positioning it
as a promising PEFT approach for large neural models.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15180" title="Abstract">arXiv:2402.15180</a> [<a href="/pdf/2402.15180" title="Download PDF">pdf</a>, <a href="/format/2402.15180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks  with Self-Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heegyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yuk%2C+S">Sehyun Yuk</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hyunsouk Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Caution: This paper includes offensive words that could potentially cause
unpleasantness. Language models (LMs) are vulnerable to exploitation for
adversarial misuse. Training LMs for safety alignment is extensive and makes it
hard to respond to fast-developing attacks immediately, such as jailbreaks. We
propose self-refine with formatting that achieves outstanding safety even in
non-safety-aligned LMs and evaluate our method alongside several defense
baselines, demonstrating that it is the safest training-free method against
jailbreak attacks. Additionally, we proposed a formatting method that improves
the efficiency of the self-refine process while reducing attack success rates
in fewer iterations. We've also observed that non-safety-aligned LMs outperform
safety-aligned LMs in safety tasks by giving more helpful and safe responses.
In conclusion, our findings can achieve less safety risk with fewer
computational costs, allowing non-safety LM to be easily utilized in real-world
service.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15183" title="Abstract">arXiv:2402.15183</a> [<a href="/pdf/2402.15183" title="Download PDF">pdf</a>, <a href="/format/2402.15183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphEdit: Large Language Models for Graph Structure Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zirui Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yanhua Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zixuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies
and interactions among nodes in graph-structured data by generating novel graph
structures. Graph Neural Networks (GNNs) have emerged as promising GSL
solutions, utilizing recursive message passing to encode node-wise
inter-dependencies. However, many existing GSL methods heavily depend on
explicit graph structural information as supervision signals, leaving them
susceptible to challenges such as data noise and sparsity. In this work, we
propose GraphEdit, an approach that leverages large language models (LLMs) to
learn complex node relationships in graph-structured data. By enhancing the
reasoning capabilities of LLMs through instruction-tuning over graph
structures, we aim to overcome the limitations associated with explicit graph
structural information and enhance the reliability of graph structure learning.
Our approach not only effectively denoises noisy connections but also
identifies node-wise dependencies from a global perspective, providing a
comprehensive understanding of the graph structure. We conduct extensive
experiments on multiple benchmark datasets to demonstrate the effectiveness and
robustness of GraphEdit across various settings. We have made our model
implementation available at: https://github.com/HKUDS/GraphEdit.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15184" title="Abstract">arXiv:2402.15184</a> [<a href="/pdf/2402.15184" title="Download PDF">pdf</a>, <a href="/format/2402.15184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Linear Inverse Model for Colored-Gaussian Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lien%2C+J">Justin Lien</a>, 
<a href="/search/math?searchtype=author&query=Kuo%2C+Y">Yan-Ning Kuo</a>, 
<a href="/search/math?searchtype=author&query=Ando%2C+H">Hiroyasu Ando</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a novel data-driven linear inverse model, called Colored-LIM, to
extract the linear dynamics and diffusion matrix that define a linear
stochastic process driven by an Ornstein-Uhlenbeck colored-noise. The
Colored-LIM is a new variant of the classical linear inverse model (LIM) which
relies on the white noise assumption. Similar to LIM, the Colored-LIM
approximates the linear dynamics from a finite realization of a stochastic
process and then solves the diffusion matrix based on, for instance, a
generalized fluctuation-dissipation relation, which can be done by solving a
system of linear equations. The main difficulty is that in practice, the
colored-noise process can be hardly observed while it is correlated to the
stochastic process of interest. Nevertheless, we show that the local behavior
of the correlation function of the observable encodes the dynamics of the
stochastic process and the diffusive behavior of the colored-noise.
<br />In this article, we review the classical LIM and develop Colored-LIM with a
mathematical background and rigorous derivations. In the numerical experiments,
we examine the performance of both LIM and Colored-LIM. Finally, we discuss
some false attempts to build a linear inverse model for colored-noise driven
processes, and investigate the potential misuse and its consequence of LIM in
the appendices.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15185" title="Abstract">arXiv:2402.15185</a> [<a href="/pdf/2402.15185" title="Download PDF">pdf</a>, <a href="/format/2402.15185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-Chirp-Domain Index Modulation for Affine Frequency Division  Multiplexing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guangyao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+T">Tianqi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhenyu Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Affine frequency division multiplexing (AFDM), tailored as a novel
multicarrier technique utilizing chirp signals for high-mobility
communications, exhibits marked advantages compared to traditional orthogonal
frequency division multiplexing (OFDM). AFDM is based on the discrete affine
Fourier transform (DAFT) with two modifiable parameters of the chirp signals,
termed as the pre-chirp parameter and post-chirp parameter, respectively. These
parameters can be fine-tuned to avoid overlapping channel paths with different
delays or Doppler shifts, leading to performance enhancement especially for
doubly dispersive channel. In this paper, we propose a novel AFDM structure
with the pre-chirp index modulation (PIM) philosophy (AFDM-PIM), which can
embed additional information bits into the pre-chirp parameter design for both
spectral and energy efficiency enhancement. Specifically, we first demonstrate
that the application of distinct pre-chirp parameters to various subcarriers in
the AFDM modulation process maintains the orthogonality among these
subcarriers. Then, different pre-chirp parameters are flexibly assigned to each
AFDM subcarrier according to the incoming bits. By such arrangement, aside from
classical phase/amplitude modulation, extra binary bits can be implicitly
conveyed by the indices of selected pre-chirping parameters realizations
without additional energy consumption. At the receiver, both a maximum
likelihood (ML) detector and a reduced-complexity ML-minimum mean square error
(ML-MMSE) detector are employed to recover the information bits. It has been
shown via simulations that the proposed AFDM-PIM exhibits superior bit error
rate (BER) performance compared to classical AFDM, OFDM and IM-aided OFDM
algorithms.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15188" title="Abstract">arXiv:2402.15188</a> [<a href="/pdf/2402.15188" title="Download PDF">pdf</a>, <a href="/format/2402.15188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Free Algorithms for Performative Regret Minimization under  Decision-Dependent Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sungwoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+J">Junyeop Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Byeongnoh Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+S">Suhyun Chae</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jeeyong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dabeen Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper studies performative risk minimization, a formulation of
stochastic optimization under decision-dependent distributions. We consider the
general case where the performative risk can be non-convex, for which we
develop efficient parameter-free optimistic optimization-based methods. Our
algorithms significantly improve upon the existing Lipschitz bandit-based
method in many aspects. In particular, our framework does not require knowledge
about the sensitivity parameter of the distribution map and the Lipshitz
constant of the loss function. This makes our framework practically favorable,
together with the efficient optimistic optimization-based tree-search
mechanism. We provide experimental results that demonstrate the numerical
superiority of our algorithms over the existing method and other black-box
optimistic optimization methods.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15189" title="Abstract">arXiv:2402.15189</a> [<a href="/pdf/2402.15189" title="Download PDF">pdf</a>, <a href="/format/2402.15189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Biomedical Entity Linking as Multiple Choice Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhenxi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yefeng Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although biomedical entity linking (BioEL) has made significant progress with
pre-trained language models, challenges still exist for fine-grained and
long-tailed entities. To address these challenges, we present BioELQA, a novel
model that treats Biomedical Entity Linking as Multiple Choice Question
Answering. BioELQA first obtains candidate entities with a fast retriever,
jointly presents the mention and candidate entities to a generator, and then
outputs the predicted symbol associated with its chosen entity. This
formulation enables explicit comparison of different candidate entities, thus
capturing fine-grained interactions between mentions and entities, as well as
among entities themselves. To improve generalization for long-tailed entities,
we retrieve similar labeled training instances as clues and concatenate the
input with retrieved instances for the generator. Extensive experimental
results show that BioELQA outperforms state-of-the-art baselines on several
datasets.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15191" title="Abstract">arXiv:2402.15191</a> [<a href="/pdf/2402.15191" title="Download PDF">pdf</a>, <a href="/format/2402.15191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Digital Twinning Platform for Integrated Sensing, Communications and  Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andrei%2C+V+C">Vlad C. Andrei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Fees%2C+M">Maresa Fees</a>, 
<a href="/search/cs?searchtype=author&query=Feik%2C+A">Andreas Feik</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6nich%2C+U+J">Ullrich J. M&#xf6;nich</a>, 
<a href="/search/cs?searchtype=author&query=Boche%2C+H">Holger Boche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to the 4th IEEE Joint Communications &amp; Sensing Hybrid Symposium, 19-21 March 2024, Leuven, Belgium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Information Theory (cs.IT); Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, a digital twinning framework for indoor integrated sensing,
communications, and robotics is proposed, designed, and implemented. Besides
leveraging powerful robotics and ray-tracing technologies, the framework also
enables integration with real-world sensors and reactive updates triggered by
changes in the environment. The framework is designed with commercial,
off-the-shelf components in mind, thus facilitating experimentation in the
different areas of communication, sensing, and robotics. Experimental results
showcase the feasibility and accuracy of indoor localization using digital
twins and validate our implementation both qualitatively and quantitatively.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15194" title="Abstract">arXiv:2402.15194</a> [<a href="/pdf/2402.15194" title="Download PDF">pdf</a>, <a href="/format/2402.15194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uehara%2C+M">Masatoshi Uehara</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yulai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+K">Kevin Black</a>, 
<a href="/search/cs?searchtype=author&query=Hajiramezanali%2C+E">Ehsan Hajiramezanali</a>, 
<a href="/search/cs?searchtype=author&query=Scalia%2C+G">Gabriele Scalia</a>, 
<a href="/search/cs?searchtype=author&query=Diamant%2C+N+L">Nathaniel Lee Diamant</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+A+M">Alex M Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Biancalani%2C+T">Tommaso Biancalani</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review (codes will be released soon)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models excel at capturing complex data distributions, such as those
of natural images and proteins. While diffusion models are trained to represent
the distribution in the training dataset, we often are more concerned with
other properties, such as the aesthetic quality of the generated images or the
functional properties of generated proteins. Diffusion models can be finetuned
in a goal-directed way by maximizing the value of some reward function (e.g.,
the aesthetic quality of an image). However, these approaches may lead to
reduced sample diversity, significant deviations from the training data
distribution, and even poor sample quality due to the exploitation of an
imperfect reward function. The last issue often occurs when the reward function
is a learned model meant to approximate a ground-truth "genuine" reward, as is
the case in many practical applications. These challenges, collectively termed
"reward collapse," pose a substantial obstacle. To address this reward
collapse, we frame the finetuning problem as entropy-regularized control
against the pretrained diffusion model, i.e., directly optimizing
entropy-enhanced rewards with neural SDEs. We present theoretical and empirical
evidence that demonstrates our framework is capable of efficiently generating
diverse samples with high genuine rewards, mitigating the overoptimization of
imperfect reward models.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15195" title="Abstract">arXiv:2402.15195</a> [<a href="/pdf/2402.15195" title="Download PDF">pdf</a>, <a href="/format/2402.15195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The AffectToolbox: Affect Analysis for Everyone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mertes%2C+S">Silvan Mertes</a>, 
<a href="/search/cs?searchtype=author&query=Schiller%2C+D">Dominik Schiller</a>, 
<a href="/search/cs?searchtype=author&query=Dietz%2C+M">Michael Dietz</a>, 
<a href="/search/cs?searchtype=author&query=Andr%C3%A9%2C+E">Elisabeth Andr&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Lingenfelser%2C+F">Florian Lingenfelser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the field of affective computing, where research continually advances at a
rapid pace, the demand for user-friendly tools has become increasingly
apparent. In this paper, we present the AffectToolbox, a novel software system
that aims to support researchers in developing affect-sensitive studies and
prototypes. The proposed system addresses the challenges posed by existing
frameworks, which often require profound programming knowledge and cater
primarily to power-users or skilled developers. Aiming to facilitate ease of
use, the AffectToolbox requires no programming knowledge and offers its
functionality to reliably analyze the affective state of users through an
accessible graphical user interface. The architecture encompasses a variety of
models for emotion recognition on multiple affective channels and modalities,
as well as an elaborate fusion system to merge multi-modal assessments into a
unified result. The entire system is open-sourced and will be publicly
available to ensure easy integration into more complex applications through a
well-structured, Python-based code base - therefore marking a substantial
contribution toward advancing affective computing research and fostering a more
collaborative and inclusive environment within this interdisciplinary field.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15197" title="Abstract">arXiv:2402.15197</a> [<a href="/pdf/2402.15197" title="Download PDF">pdf</a>, <a href="/format/2402.15197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety Optimized Reinforcement Learning via Multi-Objective Policy  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Honari%2C+H">Homayoun Honari</a>, 
<a href="/search/eess?searchtype=author&query=Tamizi%2C+M+G">Mehran Ghafarian Tamizi</a>, 
<a href="/search/eess?searchtype=author&query=Najjaran%2C+H">Homayoun Najjaran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2024, 7 Pages, 3 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Safe reinforcement learning (Safe RL) refers to a class of techniques that
aim to prevent RL algorithms from violating constraints in the process of
decision-making and exploration during trial and error. In this paper, a novel
model-free Safe RL algorithm, formulated based on the multi-objective policy
optimization framework is introduced where the policy is optimized towards
optimality and safety, simultaneously. The optimality is achieved by the
environment reward function that is subsequently shaped using a safety critic.
The advantage of the Safety Optimized RL (SORL) algorithm compared to the
traditional Safe RL algorithms is that it omits the need to constrain the
policy search space. This allows SORL to find a natural tradeoff between safety
and optimality without compromising the performance in terms of either safety
or optimality due to strict search space constraints. Through our theoretical
analysis of SORL, we propose a condition for SORL's converged policy to
guarantee safety and then use it to introduce an aggressiveness parameter that
allows for fine-tuning the mentioned tradeoff. The experimental results
obtained in seven different robotic environments indicate a considerable
reduction in the number of safety violations along with higher, or competitive,
policy returns, in comparison to six different state-of-the-art Safe RL
methods. The results demonstrate the significant superiority of the proposed
SORL algorithm in safety-critical applications.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15198" title="Abstract">arXiv:2402.15198</a> [<a href="/pdf/2402.15198" title="Download PDF">pdf</a>, <a href="/format/2402.15198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bidirectional Uncertainty-Based Active Learning for Open Set Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zong%2C+C">Chen-Chen Zong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye-Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+K">Kun-Peng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haibo Ye</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sheng-Jun Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Active learning (AL) in open set scenarios presents a novel challenge of
identifying the most valuable examples in an unlabeled data pool that comprises
data from both known and unknown classes. Traditional methods prioritize
selecting informative examples with low confidence, with the risk of mistakenly
selecting unknown-class examples with similarly low confidence. Recent methods
favor the most probable known-class examples, with the risk of picking simple
already mastered examples. In this paper, we attempt to query examples that are
both likely from known classes and highly informative, and propose a
\textit{Bidirectional Uncertainty-based Active Learning} (BUAL) framework.
Specifically, we achieve this by first pushing the unknown class examples
toward regions with high-confidence predictions with our proposed
\textit{Random Label Negative Learning} method. Then, we propose a
\textit{Bidirectional Uncertainty sampling} strategy by jointly estimating
uncertainty posed by both positive and negative learning to perform consistent
and stable sampling. BUAL successfully extends existing uncertainty-based AL
methods to complex open-set scenarios. Extensive experiments on multiple
datasets with varying openness demonstrate that BUAL achieves state-of-the-art
performance.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15200" title="Abstract">arXiv:2402.15200</a> [<a href="/pdf/2402.15200" title="Download PDF">pdf</a>, <a href="/format/2402.15200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be  Better Context-aware Translators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xinglin Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanqing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Daimeng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+S">Shimin Tao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under reviewing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generally, the decoder-only large language models (LLMs) are adapted to
context-aware neural machine translation (NMT) in a concatenating way, where
LLMs take the concatenation of the source sentence (i.e., intra-sentence
context) and the inter-sentence context as the input, and then to generate the
target tokens sequentially. This adaptation strategy, i.e., concatenation mode,
considers intra-sentence and inter-sentence contexts with the same priority,
despite an apparent difference between the two kinds of contexts. In this
paper, we propose an alternative adaptation approach, named Decoding-enhanced
Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and
utilize the inter- and intra-sentence context and more effectively adapt LLMs
to context-aware NMT. First, DeMPT divides the context-aware NMT process into
three separate phases. During each phase, different continuous prompts are
introduced to make LLMs discriminately model various information. Second, DeMPT
employs a heuristic way to further discriminately enhance the utilization of
the source-side inter- and intra-sentence information at the final decoding
phase. Experiments show that our approach significantly outperforms the
concatenation method, and further improves the performance of LLMs in discourse
modeling.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15202" title="Abstract">arXiv:2402.15202</a> [<a href="/pdf/2402.15202" title="Download PDF">pdf</a>, <a href="/format/2402.15202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Grained Detoxification via Instance-Level Prefixes for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+X">Xin Yi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Linlin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoling Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Impressive results have been achieved in natural language processing (NLP)
tasks through the training of large language models (LLMs). However, these
models occasionally produce toxic content such as insults, threats, and
profanity in response to certain prompts, thereby constraining their practical
utility. To tackle this issue, various finetuning-based and decoding-based
approaches have been utilized to mitigate toxicity. However, these methods
typically necessitate additional costs such as high-quality training data or
auxiliary models. In this paper, we propose fine-grained detoxification via
instance-level prefixes (FGDILP) to mitigate toxic text without additional
cost. Specifically, FGDILP contrasts the contextualized representation in
attention space using a positive prefix-prepended prompt against multiple
negative prefix-prepended prompts at the instance level. This allows for
constructing fine-grained subtoxicity vectors, which enables collaborative
detoxification by fusing them to correct the normal generation process when
provided with a raw prompt. We validate that FGDILP enables controlled text
generation with regard to toxicity at both the utterance and context levels.
Our method surpasses prompt-based baselines in detoxification, although at a
slight cost to generation fluency and diversity.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15205" title="Abstract">arXiv:2402.15205</a> [<a href="/pdf/2402.15205" title="Download PDF">pdf</a>, <a href="/ps/2402.15205" title="Download PostScript">ps</a>, <a href="/format/2402.15205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary  Writing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freire%2C+S+K">Samuel Kernan Freire</a>, 
<a href="/search/cs?searchtype=author&query=van+Mol%2C+M+M">Margo MC van Mol</a>, 
<a href="/search/cs?searchtype=author&query=Schol%2C+C">Carola Schol</a>, 
<a href="/search/cs?searchtype=author&query=Vieira%2C+E+%C3%96">Elif &#xd6;zcan Vieira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Intensive care unit (ICU) patients often develop new health-related problems
in their long-term recovery. Health care professionals keeping a diary of a
patient's stay is a proven strategy to tackle this but faces several adoption
barriers, such as lack of time and difficulty in knowing what to write. Large
language models (LLMs), with their ability to generate human-like text and
adaptability, could solve these challenges. However, realizing this vision
involves addressing several socio-technical and practical research challenges.
This paper discusses these challenges and proposes future research directions
to utilize the potential of LLMs in ICU diary writing, ultimately improving the
long-term recovery outcomes for ICU patients.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15206" title="Abstract">arXiv:2402.15206</a> [<a href="/pdf/2402.15206" title="Download PDF">pdf</a>, <a href="/format/2402.15206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Source-Guided Similarity Preservation for Online Person  Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rami%2C+H">Hamza Rami</a>, 
<a href="/search/cs?searchtype=author&query=Giraldo%2C+J+H">Jhony H. Giraldo</a>, 
<a href="/search/cs?searchtype=author&query=Winckler%2C+N">Nicolas Winckler</a>, 
<a href="/search/cs?searchtype=author&query=Lathuili%C3%A8re%2C+S">St&#xe9;phane Lathuili&#xe8;re</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Online Unsupervised Domain Adaptation (OUDA) for person Re-Identification
(Re-ID) is the task of continuously adapting a model trained on a
well-annotated source domain dataset to a target domain observed as a data
stream. In OUDA, person Re-ID models face two main challenges: catastrophic
forgetting and domain shift. In this work, we propose a new Source-guided
Similarity Preservation (S2P) framework to alleviate these two problems. Our
framework is based on the extraction of a support set composed of source images
that maximizes the similarity with the target data. This support set is used to
identify feature similarities that must be preserved during the learning
process. S2P can incorporate multiple existing UDA methods to mitigate
catastrophic forgetting. Our experiments show that S2P outperforms previous
state-of-the-art methods on multiple real-to-real and synthetic-to-real
challenging OUDA benchmarks.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15215" title="Abstract">arXiv:2402.15215</a> [<a href="/pdf/2402.15215" title="Download PDF">pdf</a>, <a href="/format/2402.15215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Item-side Fairness of Large Language Model-based Recommendation System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+K">Keqin Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jizhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiangnan He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the Proceedings of the ACM Web Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommendation systems for Web content distribution intricately connect to
the information access and exposure opportunities for vulnerable populations.
The emergence of Large Language Models-based Recommendation System (LRS) may
introduce additional societal challenges to recommendation systems due to the
inherent biases in Large Language Models (LLMs). From the perspective of
item-side fairness, there remains a lack of comprehensive investigation into
the item-side fairness of LRS given the unique characteristics of LRS compared
to conventional recommendation systems. To bridge this gap, this study examines
the property of LRS with respect to item-side fairness and reveals the
influencing factors of both historical users' interactions and inherent
semantic biases of LLMs, shedding light on the need to extend conventional
item-side fairness methods for LRS. Towards this goal, we develop a concise and
effective framework called IFairLRS to enhance the item-side fairness of an
LRS. IFairLRS covers the main stages of building an LRS with specifically
adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS
to fine-tune LLaMA, a representative LLM, on \textit{MovieLens} and
\textit{Steam} datasets, and observe significant item-side fairness
improvements. The code can be found in
https://github.com/JiangM-C/IFairLRS.git.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15216" title="Abstract">arXiv:2402.15216</a> [<a href="/pdf/2402.15216" title="Download PDF">pdf</a>, <a href="/ps/2402.15216" title="Download PostScript">ps</a>, <a href="/format/2402.15216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-efficient Multi-organ Segmentation Method with Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yongzhi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jinxin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+H">Haseeb Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+L">Liyilei Su</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Binding Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Accurate segmentation of multiple organs in Computed Tomography (CT) images
plays a vital role in computer-aided diagnosis systems. Various
supervised-learning approaches have been proposed recently. However, these
methods heavily depend on a large amount of high-quality labeled data, which is
expensive to obtain in practice. In this study, we present a label-efficient
learning approach using a pre-trained diffusion model for multi-organ
segmentation tasks in CT images. First, a denoising diffusion model was trained
using unlabeled CT data, generating additional two-dimensional (2D) CT images.
Then the pre-trained denoising diffusion network was transferred to the
downstream multi-organ segmentation task, effectively creating a
semi-supervised learning model that requires only a small amount of labeled
data. Furthermore, linear classification and fine-tuning decoder strategies
were employed to enhance the network's segmentation performance. Our generative
model at 256x256 resolution achieves impressive performance in terms of
Fr\'echet inception distance, spatial Fr\'echet inception distance, and
F1-score, with values of 11.32, 46.93, and 73.1\%, respectively. These results
affirm the diffusion model's ability to generate diverse and realistic 2D CT
images. Additionally, our method achieves competitive multi-organ segmentation
performance compared to state-of-the-art methods on the FLARE 2022 dataset,
particularly in limited labeled data scenarios. Remarkably, even with only 1\%
and 10\% labeled data, our method achieves Dice similarity coefficients (DSCs)
of 71.56\% and 78.51\% after fine-tuning, respectively. The method achieves a
DSC score of 51.81\% using just four labeled CT scans. These results
demonstrate the efficacy of our approach in overcoming the limitations of
supervised learning heavily reliant on large-scale labeled data.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15218" title="Abstract">arXiv:2402.15218</a> [<a href="/pdf/2402.15218" title="Download PDF">pdf</a>, <a href="/format/2402.15218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BSPA: Exploring Black-box Stealthy Prompt Attacks against Image  Generators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yinpeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Heming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Extremely large image generators offer significant transformative potential
across diverse sectors. It allows users to design specific prompts to generate
realistic images through some black-box APIs. However, some studies reveal that
image generators are notably susceptible to attacks and generate Not Suitable
For Work (NSFW) contents by manually designed toxin texts, especially
imperceptible to human observers. We urgently need a multitude of universal and
transferable prompts to improve the safety of image generators, especially
black-box-released APIs. Nevertheless, they are constrained by labor-intensive
design processes and heavily reliant on the quality of the given instructions.
To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that
adopts a retriever to simulate attacks from API users. It can effectively
harness filter scores to tune the retrieval space of sensitive words for
matching the input prompts, thereby crafting stealthy prompts tailored for
image generators. Significantly, this approach is model-agnostic and requires
no internal access to the model's features, ensuring its applicability to a
wide range of image generators. Building on BSPA, we have constructed an
automated prompt tool and a comprehensive prompt attack dataset (NSFWeval).
Extensive experiments demonstrate that BSPA effectively explores the security
vulnerabilities in a variety of state-of-the-art available black-box models,
including Stable Diffusion XL, Midjourney, and DALL-E 2/3. Furthermore, we
develop a resilient text filter and offer targeted recommendations to ensure
the security of image generators against prompt attacks in the future.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15220" title="Abstract">arXiv:2402.15220</a> [<a href="/pdf/2402.15220" title="Download PDF">pdf</a>, <a href="/format/2402.15220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and  Two-Phase Partition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+L">Lu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Ze Tao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Self-attention is an essential component of large language models(LLMs) but a
significant source of inference latency for long sequences. In multi-tenant
LLMs serving scenarios, the compute and memory operation cost of self-attention
can be optimized by using the probability that multiple LLM requests have
shared system prompts in prefixes. In this paper, we introduce ChunkAttention,
a prefix-aware self-attention module that can detect matching prompt prefixes
across multiple requests and share their key/value tensors in memory at runtime
to improve the memory utilization of KV cache. This is achieved by breaking
monolithic key/value tensors into smaller chunks and structuring them into the
auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,
we design an efficient self-attention kernel, where a two-phase partition
algorithm is implemented to improve the data locality during self-attention
computation in the presence of shared system prompts. Experiments show that
ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\times$
compared to the start-of-the-art implementation, with the length of the system
prompt ranging from 1024 to 4096.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15222" title="Abstract">arXiv:2402.15222</a> [<a href="/pdf/2402.15222" title="Download PDF">pdf</a>, <a href="/format/2402.15222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Latency Upstream Scheduling in Multi-Tenant, SLA Compliant TWDM PON
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganguli%2C+A">Arijeet Ganguli</a>, 
<a href="/search/cs?searchtype=author&query=Ruffini%2C+M">Marco Ruffini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">We present a multi-tenant multi-wavelength upstream transmission scheme for
virtualised PONs, enabling compliance with latency-oriented Service Level
Agreements (SLAs). Our analysis highlights an important trade-off between
single-channel vs. multi-channel PONs, depending on ONUs tuning time.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15227" title="Abstract">arXiv:2402.15227</a> [<a href="/pdf/2402.15227" title="Download PDF">pdf</a>, <a href="/format/2402.15227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed Random Classifier Rearrangement for Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shengyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+J">Jianwen Mo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the explosive growth of data, continual learning capability is
increasingly important for neural networks. Due to catastrophic forgetting,
neural networks inevitably forget the knowledge of old tasks after learning new
ones. In visual classification scenario, a common practice of alleviating the
forgetting is to constrain the backbone. However, the impact of classifiers is
underestimated. In this paper, we analyze the variation of model predictions in
sequential binary classification tasks and find that the norm of the equivalent
one-class classifiers significantly affects the forgetting level. Based on this
conclusion, we propose a two-stage continual learning algorithm named Fixed
Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the
learnable classifiers with fixed random classifiers, constraining the norm of
the equivalent one-class classifiers without affecting the performance of the
network. In second stage, FRCR rearranges the entries of new classifiers to
implicitly reduce the drift of old latent representations. The experimental
results on multiple datasets show that FRCR significantly mitigates the model
forgetting; subsequent experimental analyses further validate the effectiveness
of the algorithm.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15230" title="Abstract">arXiv:2402.15230</a> [<a href="/pdf/2402.15230" title="Download PDF">pdf</a>, <a href="/format/2402.15230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Energy Services -- Forecasting and Optimization as a Service for  Energy Management Applications at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=W%C3%B6lfle%2C+D">David W&#xf6;lfle</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%B6rderer%2C+K">Kevin F&#xf6;rderer</a>, 
<a href="/search/cs?searchtype=author&query=Riedel%2C+T">Tobias Riedel</a>, 
<a href="/search/cs?searchtype=author&query=Landwich%2C+L">Lukas Landwich</a>, 
<a href="/search/cs?searchtype=author&query=Mikut%2C+R">Ralf Mikut</a>, 
<a href="/search/cs?searchtype=author&query=Hagenmeyer%2C+V">Veit Hagenmeyer</a>, 
<a href="/search/cs?searchtype=author&query=Schmeck%2C+H">Hartmut Schmeck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Energy management, in sense of computing optimized operation schedules for
devices, will likely play a vital role in future carbon neutral energy systems,
as it allows unlocking energy efficiency and flexibility potentials. However,
energy management systems need to be applied at large scales to realize the
desired effect, which clearly requires minimization of costs for setup and
operation of the individual applications. In order to push the latter forward,
we promote an approach to split the complex optimization algorithms employed by
energy management systems into standardized components, which can be provided
as a service with marginal costs at scale. This work is centered around the
systematic design of a framework supporting the efficient implementation and
operation of such forecasting and optimization services. Furthermore, it
describes the implementation of the design concept which we release under the
name \emph{Energy Service Generics} as a free and open source repository.
Finally, this paper marks the starting point of the \emph{Open Energy Services}
community, our effort to continuously push the development and operation of
services for energy management applications at scale, for which we invite
researchers and practitioners to participate.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15231" title="Abstract">arXiv:2402.15231</a> [<a href="/pdf/2402.15231" title="Download PDF">pdf</a>, <a href="/format/2402.15231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Which Model to Transfer? A Survey on Transferability Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuhe Ding</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+A">Aijing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+A">Aihua Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jian Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Transfer learning methods endeavor to leverage relevant knowledge from
existing source pre-trained models or datasets to solve downstream target
tasks. With the increase in the scale and quantity of available pre-trained
models nowadays, it becomes critical to assess in advance whether they are
suitable for a specific target task. Model transferability estimation is an
emerging and growing area of interest, aiming to propose a metric to quantify
this suitability without training them individually, which is computationally
prohibitive. Despite extensive recent advances already devoted to this area,
they have custom terminological definitions and experimental settings. In this
survey, we present the first review of existing advances in this area and
categorize them into two separate realms: source-free model transferability
estimation and source-dependent model transferability estimation. Each category
is systematically defined, accompanied by a comprehensive taxonomy. Besides, we
address challenges and outline future research directions, intending to provide
a comprehensive guide to aid researchers and practitioners.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15235" title="Abstract">arXiv:2402.15235</a> [<a href="/pdf/2402.15235" title="Download PDF">pdf</a>, <a href="/format/2402.15235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Collaboration Framework for Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhefan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yuanqing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wendi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weizhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">LLM-based agents have gained considerable attention for their decision-making
skills and ability to handle complex tasks. Recognizing the current gap in
leveraging agent capabilities for multi-agent collaboration in recommendation
systems, we introduce MACRec, a novel framework designed to enhance
recommendation systems through multi-agent collaboration. Unlike existing work
on using agents for user/item simulation, we aim to deploy multi-agents to
tackle recommendation tasks directly. In our framework, recommendation tasks
are addressed through the collaborative efforts of various specialized agents,
including Manager, User/Item Analyst, Reflector, Searcher, and Task
Interpreter, with different working flows. Furthermore, we provide application
examples of how developers can easily use MACRec on various recommendation
tasks, including rating prediction, sequential recommendation, conversational
recommendation, and explanation generation of recommendation results. The
framework and demonstration video are publicly available at
https://github.com/wzf2000/MACRec.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15236" title="Abstract">arXiv:2402.15236</a> [<a href="/pdf/2402.15236" title="Download PDF">pdf</a>, <a href="/format/2402.15236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Font Impression Estimation in the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kitajima%2C+K">Kazuki Kitajima</a>, 
<a href="/search/cs?searchtype=author&query=Haraguchi%2C+D">Daichi Haraguchi</a>, 
<a href="/search/cs?searchtype=author&query=Uchida%2C+S">Seiichi Uchida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper addresses the challenging task of estimating font impressions from
real font images. We use a font dataset with annotation about font impressions
and a convolutional neural network (CNN) framework for this task. However,
impressions attached to individual fonts are often missing and noisy because of
the subjective characteristic of font impression annotation. To realize stable
impression estimation even with such a dataset, we propose an exemplar-based
impression estimation approach, which relies on a strategy of ensembling
impressions of exemplar fonts that are similar to the input image. In addition,
we train CNN with synthetic font images that mimic scanned word images so that
CNN estimates impressions of font images in the wild. We evaluate the basic
performance of the proposed estimation method quantitatively and qualitatively.
Then, we conduct a correlation analysis between book genres and font
impressions on real book cover images; it is important to note that this
analysis is only possible with our impression estimation method. The analysis
reveals various trends in the correlation between them - this fact supports a
hypothesis that book cover designers carefully choose a font for a book cover
considering the impression given by the font.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15237" title="Abstract">arXiv:2402.15237</a> [<a href="/pdf/2402.15237" title="Download PDF">pdf</a>, <a href="/format/2402.15237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Domain Adaptation for Brain Vessel Segmentation through  Transwarp Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Fengming Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yan Xia</a>, 
<a href="/search/cs?searchtype=author&query=MacRaild%2C+M">Michael MacRaild</a>, 
<a href="/search/cs?searchtype=author&query=Deo%2C+Y">Yash Deo</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+H">Haoran Dou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiongyao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+N">Nishant Ravikumar</a>, 
<a href="/search/cs?searchtype=author&query=Frangi%2C+A+F">Alejandro F. Frangi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Unsupervised domain adaptation (UDA) aims to align the labelled source
distribution with the unlabelled target distribution to obtain domain-invariant
predictive models. Since cross-modality medical data exhibit significant intra
and inter-domain shifts and most are unlabelled, UDA is more important while
challenging in medical image analysis. This paper proposes a simple yet potent
contrastive learning framework for UDA to narrow the inter-domain gap between
labelled source and unlabelled target distribution. Our method is validated on
cerebral vessel datasets. Experimental results show that our approach can learn
latent features from labelled 3DRA modality data and improve vessel
segmentation performance in unlabelled MRA modality data.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15238" title="Abstract">arXiv:2402.15238</a> [<a href="/pdf/2402.15238" title="Download PDF">pdf</a>, <a href="/format/2402.15238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech  Detection?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yiping Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wanner%2C+L">Leo Wanner</a>, 
<a href="/search/cs?searchtype=author&query=Shvets%2C+A">Alexander Shvets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to LREC-COLING 2024. Content Warning: This paper contains model outputs that are offensive in nature
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Online hate detection suffers from biases incurred in data sampling,
annotation, and model pre-training. Therefore, measuring the averaged
performance over all examples in held-out test data is inadequate. Instead, we
must identify specific model weaknesses and be informed when it is more likely
to fail. A recent proposal in this direction is HateCheck, a suite for testing
fine-grained model functionalities on synthesized data generated using
templates of the kind "You are just a [slur] to me." However, despite enabling
more detailed diagnostic insights, the HateCheck test cases are often generic
and have simplistic sentence structures that do not match the real-world data.
To address this limitation, we propose GPT-HateCheck, a framework to generate
more diverse and realistic functional tests from scratch by instructing large
language models (LLMs). We employ an additional natural language inference
(NLI) model to verify the generations. Crowd-sourced annotation demonstrates
that the generated test cases are of high quality. Using the new functional
tests, we can uncover model weaknesses that would be overlooked using the
original HateCheck dataset.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15239" title="Abstract">arXiv:2402.15239</a> [<a href="/pdf/2402.15239" title="Download PDF">pdf</a>, <a href="/format/2402.15239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GS-EMA: Integrating Gradient Surgery Exponential Moving Average with  Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in  Aneurysm Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Fengming Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yan Xia</a>, 
<a href="/search/cs?searchtype=author&query=MacRaild%2C+M">Michael MacRaild</a>, 
<a href="/search/cs?searchtype=author&query=Deo%2C+Y">Yash Deo</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+H">Haoran Dou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiongyao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Nina Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+N">Nishant Ravikumar</a>, 
<a href="/search/cs?searchtype=author&query=Frangi%2C+A+F">Alejandro F. Frangi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The automated segmentation of cerebral aneurysms is pivotal for accurate
diagnosis and treatment planning. Confronted with significant domain shifts and
class imbalance in 3D Rotational Angiography (3DRA) data from various medical
institutions, the task becomes challenging. These shifts include differences in
image appearance, intensity distribution, resolution, and aneurysm size, all of
which complicate the segmentation process. To tackle these issues, we propose a
novel domain generalization strategy that employs gradient surgery exponential
moving average (GS-EMA) optimization technique coupled with boundary-aware
contrastive learning (BACL). Our approach is distinct in its ability to adapt
to new, unseen domains by learning domain-invariant features, thereby improving
the robustness and accuracy of aneurysm segmentation across diverse clinical
datasets. The results demonstrate that our proposed approach can extract more
domain-invariant features, minimizing over-segmentation and capturing more
complete aneurysm structures.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15243" title="Abstract">arXiv:2402.15243</a> [<a href="/pdf/2402.15243" title="Download PDF">pdf</a>, <a href="/format/2402.15243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety-Conscious Pushing on Diverse Oriented Surfaces with Underactuated  Aerial Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hui%2C+T">Tong Hui</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+M+J+F">Manuel J. Fernandez Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Fumagalli%2C+M">Matteo Fumagalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 2024 IEEE International Conference on Robotics and Automation (ICRA2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Pushing tasks performed by aerial manipulators can be used for contact-based
industrial inspections. Underactuated aerial vehicles are widely employed in
aerial manipulation due to their widespread availability and relatively low
cost. Industrial infrastructures often consist of diverse oriented work
surfaces. When interacting with such surfaces, the coupled gravity compensation
and interaction force generation of underactuated aerial vehicles can present
the potential challenge of near-saturation operations. The blind utilization of
these platforms for such tasks can lead to instability and accidents, creating
unsafe operating conditions and potentially damaging the platform. In order to
ensure safe pushing on these surfaces while managing platform saturation, this
work establishes a safety assessment process. This process involves the
prediction of the saturation level of each actuator during pushing across
variable surface orientations. Furthermore, the assessment results are used to
plan and execute physical experiments, ensuring safe operations and preventing
platform damage.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15247" title="Abstract">arXiv:2402.15247</a> [<a href="/pdf/2402.15247" title="Download PDF">pdf</a>, <a href="/format/2402.15247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bargaining-based Approach for Feature Trading in Vertical Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yue Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Liuyi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bolin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaofang Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Vertical Federated Learning (VFL) has emerged as a popular machine learning
paradigm, enabling model training across the data and the task parties with
different features about the same user set while preserving data privacy. In
production environment, VFL usually involves one task party and one data party.
Fair and economically efficient feature trading is crucial to the
commercialization of VFL, where the task party is considered as the data
consumer who buys the data party's features. However, current VFL feature
trading practices often price the data party's data as a whole and assume
transactions occur prior to the performing VFL. Neglecting the performance
gains resulting from traded features may lead to underpayment and overpayment
issues. In this study, we propose a bargaining-based feature trading approach
in VFL to encourage economically efficient transactions. Our model incorporates
performance gain-based pricing, taking into account the revenue-based
optimization objectives of both parties. We analyze the proposed bargaining
model under perfect and imperfect performance information settings, proving the
existence of an equilibrium that optimizes the parties' objectives. Moreover,
we develop performance gain estimation-based bargaining strategies for
imperfect performance information scenarios and discuss potential security
issues and solutions. Experiments on three real-world datasets demonstrate the
effectiveness of the proposed bargaining model.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15248" title="Abstract">arXiv:2402.15248</a> [<a href="/pdf/2402.15248" title="Download PDF">pdf</a>, <a href="/format/2402.15248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chitchat as Interference: Adding User Backstories to Task-Oriented  Dialogues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stricker%2C+A">Armand Stricker</a>, 
<a href="/search/cs?searchtype=author&query=Paroubek%2C+P">Patrick Paroubek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted @ LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">During task-oriented dialogues (TODs), human users naturally introduce
chitchat that is beyond the immediate scope of the task, interfering with the
flow of the conversation. To address this issue without the need for expensive
manual data creation, we use few-shot prompting with Llama-2-70B to enhance the
MultiWOZ dataset with user backstories, a typical example of chitchat
interference in TODs. We assess the impact of this addition by testing two
models: one trained solely on TODs and another trained on TODs with a
preliminary chitchat interaction. Our analysis reveals that our enriched
dataset poses a significant challenge to these systems. Moreover, we
demonstrate that our dataset can be effectively used for training purposes,
enabling a system to consistently acknowledge the user's backstory while also
successfully moving the task forward in the same turn, as confirmed by human
evaluation. These findings highlight the benefits of generating novel
chitchat-TOD scenarios to test TOD systems more thoroughly and improve their
resilience to natural user interferences.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15253" title="Abstract">arXiv:2402.15253</a> [<a href="/pdf/2402.15253" title="Download PDF">pdf</a>, <a href="/format/2402.15253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PICO: Accelerating All k-Core Paradigms on GPU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Ting Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhigao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Song Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiawei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Core decomposition is a well-established graph mining problem with various
applications that involves partitioning the graph into hierarchical subgraphs.
Solutions to this problem have been developed using both bottom-up and top-down
approaches from the perspective of vertex convergence dependency. However,
existing algorithms have not effectively harnessed GPU performance to expedite
core decomposition, despite the growing need for enhanced performance.
Moreover, approaching performance limitations of core decomposition from two
different directions within a parallel synchronization structure has not been
thoroughly explored. This paper introduces an efficient GPU acceleration
framework, PICO, for the Peel and Index2core paradigms of k-core decomposition.
We propose PeelOne, a Peel-based algorithm designed to simplify the parallel
logic and minimize atomic operations by eliminating vertices that are
'under-core'. We also propose an Index2core-based algorithm, named HistoCore,
which addresses the issue of extensive redundant computations across both
vertices and edges. Extensive experiments on NVIDIA RTX 3090 GPU show that
PeelOne outperforms all other Peel-based algorithms, and HistoCore outperforms
all other Index2core-based algorithms. Furthermore, HistoCore even outperforms
PeelOne by 1.1x - 3.2x speedup on six datasets, which breaks the stereotype
that the Index2core paradigm performs much worse than the Peel in a shared
memory parallel setting.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15255" title="Abstract">arXiv:2402.15255</a> [<a href="/pdf/2402.15255" title="Download PDF">pdf</a>, <a href="/format/2402.15255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport for Structure Learning Under Missing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vo%2C+V">Vy Vo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Trung Le</a>, 
<a href="/search/cs?searchtype=author&query=Bonilla%2C+E+V">Edwin V. Bonilla</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Causal discovery in the presence of missing data introduces a chicken-and-egg
dilemma. While the goal is to recover the true causal structure, robust
imputation requires considering the dependencies or preferably causal relations
among variables. Merely filling in missing values with existing imputation
methods and subsequently applying structure learning on the complete data is
empirical shown to be sub-optimal. To this end, we propose in this paper a
score-based algorithm, based on optimal transport, for learning causal
structure from missing data. This optimal transport viewpoint diverges from
existing score-based approaches that are dominantly based on EM. We project
structure learning as a density fitting problem, where the goal is to find the
causal model that induces a distribution of minimum Wasserstein distance with
the distribution over the observed data. Through extensive simulations and
real-data experiments, our framework is shown to recover the true causal graphs
more effectively than the baselines in various simulations and real-data
experiments. Empirical evidences also demonstrate the superior scalability of
our approach, along with the flexibility to incorporate any off-the-shelf
causal discovery methods for complete data.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15257" title="Abstract">arXiv:2402.15257</a> [<a href="/pdf/2402.15257" title="Download PDF">pdf</a>, <a href="/format/2402.15257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Model-Driven Dashboard Generation for Systems-of-Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rossi%2C+M+T">Maria Teresa Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Tundo%2C+A">Alessandro Tundo</a>, 
<a href="/search/cs?searchtype=author&query=Mariani%2C+L">Leonardo Mariani</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024 12th ACM/IEEE International Workshop on Software Engineering
  for Systems-of-Systems and Software Ecosystems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Configuring and evolving dashboards in complex and large-scale
Systems-of-Systems (SoS) can be an expensive and cumbersome task due to the
many Key Performance Indicators (KPIs) that are usually collected and have to
be arranged in a number of visualizations. Unfortunately, setting up dashboards
is still a largely manual and error-prone task requiring extensive human
intervention.
<br />This short paper describes emerging results about the definition of a
model-driven technology-agnostic approach that can automatically transform a
simple list of KPIs into a dashboard model, and then translate the model into
an actual dashboard for a target dashboard technology. Dashboard customization
can be efficiently obtained by solely modifying the abstract model
representation, freeing operators from expensive interactions with actual
dashboards.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15259" title="Abstract">arXiv:2402.15259</a> [<a href="/pdf/2402.15259" title="Download PDF">pdf</a>, <a href="/format/2402.15259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Ad Hoc Teamwork with Cooperative Game Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Wei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Kaski%2C+S">Samuel Kaski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Ad hoc teamwork poses a challenging problem, requiring the design of an agent
to collaborate with teammates without prior coordination or joint training.
Open ad hoc teamwork further complicates this challenge by considering
environments with a changing number of teammates, referred to as open teams.
The state-of-the-art solution to this problem is graph-based policy learning
(GPL), leveraging the generalizability of graph neural networks to handle an
unrestricted number of agents and effectively address open teams. GPL's
performance is superior to other methods, but its joint Q-value representation
presents challenges for interpretation, hindering further development of this
research line and applicability. In this paper, we establish a new theory to
give an interpretation for the joint Q-value representation employed in GPL,
from the perspective of cooperative game theory. Building on our theory, we
propose a novel algorithm based on GPL framework, to complement the critical
features that facilitate learning, but overlooked in GPL. Through experiments,
we demonstrate the correctness of our theory by comparing the performance of
the resulting algorithm with GPL in dynamic team compositions.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15262" title="Abstract">arXiv:2402.15262</a> [<a href="/pdf/2402.15262" title="Download PDF">pdf</a>, <a href="/format/2402.15262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Memory Based Adaptive Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Szegedy%2C+B">Bal&#xe1;zs Szegedy</a>, 
<a href="/search/cs?searchtype=author&query=Czifra%2C+D">Domonkos Czifra</a>, 
<a href="/search/cs?searchtype=author&query=K%C5%91r%C3%B6si-Szab%C3%B3%2C+P">P&#xe9;ter K&#x151;r&#xf6;si-Szab&#xf3;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">Define an optimizer as having memory $k$ if it stores $k$ dynamically
changing vectors in the parameter space. Classical SGD has memory $0$, momentum
SGD optimizer has $1$ and Adam optimizer has $2$. We address the following
questions: How can optimizers make use of more memory units? What information
should be stored in them? How to use them for the learning steps? As an
approach to the last question, we introduce a general method called
"Retrospective Learning Law Correction" or shortly RLLC. This method is
designed to calculate a dynamically varying linear combination (called learning
law) of memory units, which themselves may evolve arbitrarily. We demonstrate
RLLC on optimizers whose memory units have linear update rules and small memory
($\leq 4$ memory units). Our experiments show that in a variety of standard
problems, these optimizers outperform the above mentioned three classical
optimizers. We conclude that RLLC is a promising framework for boosting the
performance of known optimizers by adding more memory units and by making them
more adaptive.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15263" title="Abstract">arXiv:2402.15263</a> [<a href="/pdf/2402.15263" title="Download PDF">pdf</a>, <a href="/ps/2402.15263" title="Download PostScript">ps</a>, <a href="/format/2402.15263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Countries pushing the boundaries of knowledge: the US dominance, China  rise, and the EU stagnation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodriguez-Navarro%2C+A">Alonso Rodriguez-Navarro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 1 figure, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Knowing which countries contribute the most to pushing the boundaries of
knowledge in science and technology has social and political importance.
However, common citation metrics do not adequately measure this contribution.
This measure requires more stringent metrics appropriate for the highly
influential breakthrough papers that push the boundaries of knowledge, which
are very highly cited but very rare. Here I used the recently described Rk
index, specifically designed to address this issue. I applied this index to 25
countries and the EU across 10 key research topics, five technological and five
biomedical, studying domestic and international collaborative papers
independently. In technological topics, the Rk indices of domestic papers show
that overall, the USA, China, and the EU are leaders; other countries are
clearly behind. The USA is notably ahead of China, and the EU is far behind
China. The same approach to biomedical topics shows an overwhelming dominance
of the USA and that the EU is ahead of China. The analysis of internationally
collaborative papers further demonstrates the US dominance. These results
conflict with current country rankings based on less stringent indicators.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15264" title="Abstract">arXiv:2402.15264</a> [<a href="/pdf/2402.15264" title="Download PDF">pdf</a>, <a href="/format/2402.15264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEEM: Dynamic Experienced Expert Modeling for Stance Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sijie Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent work has made a preliminary attempt to use large language models
(LLMs) to solve the stance detection task, showing promising results. However,
considering that stance detection usually requires detailed background
knowledge, the vanilla reasoning method may neglect the domain knowledge to
make a professional and accurate analysis. Thus, there is still room for
improvement of LLMs reasoning, especially in leveraging the generation
capability of LLMs to simulate specific experts (i.e., multi-agents) to detect
the stance. In this paper, different from existing multi-agent works that
require detailed descriptions and use fixed experts, we propose a Dynamic
Experienced Expert Modeling (DEEM) method which can leverage the generated
experienced experts and let LLMs reason in a semi-parametric way, making the
experts more generalizable and reliable. Experimental results demonstrate that
DEEM consistently achieves the best results on three standard benchmarks,
outperforms methods with self-consistency reasoning, and reduces the bias of
LLMs.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15265" title="Abstract">arXiv:2402.15265</a> [<a href="/pdf/2402.15265" title="Download PDF">pdf</a>, <a href="/format/2402.15265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CloChat: Understanding How People Customize, Interact, and Experience  Personas in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ha%2C+J">Juhye Ha</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyeon Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">DaEun Han</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Jinwook Seo</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+C">Changhoon Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have facilitated significant strides in
generating conversational agents, enabling seamless, contextually relevant
dialogues across diverse topics. However, the existing LLM-driven
conversational agents have fixed personalities and functionalities, limiting
their adaptability to individual user needs. Creating personalized agent
personas with distinct expertise or traits can address this issue. Nonetheless,
we lack knowledge of how people customize and interact with agent personas. In
this research, we investigated how users customize agent personas and their
impact on interaction quality, diversity, and dynamics. To this end, we
developed CloChat, an interface supporting easy and accurate customization of
agent personas in LLMs. We conducted a study comparing how participants
interact with CloChat and ChatGPT. The results indicate that participants
formed emotional bonds with the customized agents, engaged in more dynamic
dialogues, and showed interest in sustaining interactions. These findings
contribute to design implications for future systems with conversational agents
using LLMs.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15266" title="Abstract">arXiv:2402.15266</a> [<a href="/pdf/2402.15266" title="Download PDF">pdf</a>, <a href="/format/2402.15266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibration of Deep Learning Classification Models in fNIRS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhihao Cao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zizhou Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool
for monitoring brain activity. The classification of fNIRS data in relation to
conscious activity holds significance for advancing our understanding of the
brain and facilitating the development of brain-computer interfaces (BCI). Many
researchers have turned to deep learning to tackle the classification
challenges inherent in fNIRS data due to its strong generalization and
robustness. In the application of fNIRS, reliability is really important, and
one mathematical formulation of the reliability of confidence is calibration.
However, many researchers overlook the important issue of calibration. To
address this gap, we propose integrating calibration into fNIRS field and
assess the reliability of existing models. Surprisingly, our results indicate
poor calibration performance in many proposed models. To advance calibration
development in the fNIRS field, we summarize three practical tips. Through this
letter, we hope to emphasize the critical role of calibration in fNIRS research
and argue for enhancing the reliability of deep learning-based predictions in
fNIRS classification tasks. All data from our experimental process are openly
available on GitHub.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15267" title="Abstract">arXiv:2402.15267</a> [<a href="/pdf/2402.15267" title="Download PDF">pdf</a>, <a href="/format/2402.15267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Robustness of Deep Learning-based Malware Detectors via  (De)Randomized Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gibert%2C+D">Daniel Gibert</a>, 
<a href="/search/cs?searchtype=author&query=Zizzo%2C+G">Giulio Zizzo</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+Q">Quan Le</a>, 
<a href="/search/cs?searchtype=author&query=Planes%2C+J">Jordi Planes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep learning-based malware detectors have been shown to be susceptible to
adversarial malware examples, i.e. malware examples that have been deliberately
manipulated in order to avoid detection. In light of the vulnerability of deep
learning detectors to subtle input file modifications, we propose a practical
defense against adversarial malware examples inspired by (de)randomized
smoothing. In this work, we reduce the chances of sampling adversarial content
injected by malware authors by selecting correlated subsets of bytes, rather
than using Gaussian noise to randomize inputs like in the Computer Vision (CV)
domain. During training, our ablation-based smoothing scheme trains a base
classifier to make classifications on a subset of contiguous bytes or chunk of
bytes. At test time, a large number of chunks are then classified by a base
classifier and the consensus among these classifications is then reported as
the final prediction. We propose two strategies to determine the location of
the chunks used for classification: (1) randomly selecting the locations of the
chunks and (2) selecting contiguous adjacent chunks. To showcase the
effectiveness of our approach, we have trained two classifiers with our
chunk-based ablation schemes on the BODMAS dataset. Our findings reveal that
the chunk-based smoothing classifiers exhibit greater resilience against
adversarial malware examples generated with state-of-the-are evasion attacks,
outperforming a non-smoothed classifier and a randomized smoothing-based
classifier by a great margin.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15268" title="Abstract">arXiv:2402.15268</a> [<a href="/pdf/2402.15268" title="Download PDF">pdf</a>, <a href="/format/2402.15268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rakotonirina%2C+N+C">Nathana&#xeb;l Carraz Rakotonirina</a>, 
<a href="/search/cs?searchtype=author&query=Baroni%2C+M">Marco Baroni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as conference paper at LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Transformer-based language models (LMs) track contextual information through
large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach
in which the LM is complemented by a small auxiliary recurrent network that
passes information to the LM by prefixing its regular input with a sequence of
vectors, akin to soft prompts, without requiring LM finetuning. Tested on a
task designed to probe a LM's ability to keep track of multiple fact updates, a
MemoryPrompt-augmented LM outperforms much larger LMs that have access to the
full input history. We also test MemoryPrompt on a long-distance dialogue
dataset, where its performance is comparable to that of a model conditioned on
the entire conversation history. In both experiments we also observe that,
unlike full-finetuning approaches, MemoryPrompt does not suffer from
catastrophic forgetting when adapted to new tasks, thus not disrupting the
generalist capabilities of the underlying LM.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15270" title="Abstract">arXiv:2402.15270</a> [<a href="/pdf/2402.15270" title="Download PDF">pdf</a>, <a href="/format/2402.15270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothed Graph Contrastive Learning via Seamless Proximity Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behmanesh%2C+M">Maysam Behmanesh</a>, 
<a href="/search/cs?searchtype=author&query=Ovsjanikov%2C+M">Maks Ovsjanikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph contrastive learning (GCL) aligns node representations by classifying
node pairs into positives and negatives using a selection process that
typically relies on establishing correspondences within two augmented graphs.
The conventional GCL approaches incorporate negative samples uniformly in the
contrastive loss, resulting in the equal treatment negative nodes, regardless
of their proximity to the true positive. In this paper, we present a Smoothed
Graph Contrastive Learning model (SGCL), which leverages the geometric
structure of augmented graphs to inject proximity information associated with
positive/negative pairs in the contrastive loss, thus significantly
regularizing the learning process. The proposed SGCL adjusts the penalties
associated with node pairs in the contrastive loss by incorporating three
distinct smoothing techniques that result in proximity aware positives and
negatives. To enhance scalability for large-scale graphs, the proposed
framework incorporates a graph batch-generating strategy that partitions the
given graphs into multiple subgraphs, facilitating efficient training in
separate batches. Through extensive experimentation in the unsupervised setting
on various benchmarks, particularly those of large scale, we demonstrate the
superiority of our proposed framework against recent baselines.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15272" title="Abstract">arXiv:2402.15272</a> [<a href="/pdf/2402.15272" title="Download PDF">pdf</a>, <a href="/format/2402.15272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMIFF: Enhanced Multi-scale Image Feature Fusion for  Vehicle-Infrastructure Cooperative 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Siqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+X">Xiaoliang Huo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tongda Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingjing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yilun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya-Qin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures. Accepted by ICRA 2024. arXiv admin note: text overlap with arXiv:<a href="/abs/2303.10975">arXiv:2303.10975</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In autonomous driving, cooperative perception makes use of multi-view cameras
from both vehicles and infrastructure, providing a global vantage point with
rich semantic context of road conditions beyond a single vehicle viewpoint.
Currently, two major challenges persist in vehicle-infrastructure cooperative
3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view
images, caused by time asynchrony across cameras; $2)$ information loss in
transmission process resulted from limited communication bandwidth. To address
these issues, we propose a novel camera-based 3D detection framework for VIC3D
task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit
holistic perspectives from both vehicles and infrastructure, we propose
Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM)
modules to enhance infrastructure and vehicle features at scale, spatial, and
channel levels to correct the pose error introduced by camera asynchrony. We
also introduce a Feature Compression (FC) module with channel and spatial
compression blocks for transmission efficiency. Experiments show that EMIFF
achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous
early-fusion and late-fusion methods with comparable transmission costs.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15273" title="Abstract">arXiv:2402.15273</a> [<a href="/pdf/2402.15273" title="Download PDF">pdf</a>, <a href="/format/2402.15273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Deployment of Deep Neural Networks for Visual Pose Estimation  on Nano-drones
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Risso%2C+M">Matteo Risso</a>, 
<a href="/search/cs?searchtype=author&query=Daghero%2C+F">Francesco Daghero</a>, 
<a href="/search/cs?searchtype=author&query=Motetti%2C+B+A">Beatrice Alessandra Motetti</a>, 
<a href="/search/cs?searchtype=author&query=Pagliari%2C+D+J">Daniele Jahier Pagliari</a>, 
<a href="/search/cs?searchtype=author&query=Macii%2C+E">Enrico Macii</a>, 
<a href="/search/cs?searchtype=author&query=Poncino%2C+M">Massimo Poncino</a>, 
<a href="/search/cs?searchtype=author&query=Burrello%2C+A">Alessio Burrello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in the ERF 2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Miniaturized autonomous unmanned aerial vehicles (UAVs) are gaining
popularity due to their small size, enabling new tasks such as indoor
navigation or people monitoring. Nonetheless, their size and simple electronics
pose severe challenges in implementing advanced onboard intelligence. This work
proposes a new automatic optimization pipeline for visual pose estimation tasks
using Deep Neural Networks (DNNs). The pipeline leverages two different Neural
Architecture Search (NAS) algorithms to pursue a vast complexity-driven
exploration in the DNNs' architectural space. The obtained networks are then
deployed on an off-the-shelf nano-drone equipped with a parallel ultra-low
power System-on-Chip leveraging a set of novel software kernels for the
efficient fused execution of critical DNN layer sequences. Our results improve
the state-of-the-art reducing inference latency by up to 3.22x at iso-error.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15274" title="Abstract">arXiv:2402.15274</a> [<a href="/pdf/2402.15274" title="Download PDF">pdf</a>, <a href="/format/2402.15274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification Under Strategic Self-Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Horowitz%2C+G">Guy Horowitz</a>, 
<a href="/search/cs?searchtype=author&query=Sommer%2C+Y">Yonatan Sommer</a>, 
<a href="/search/cs?searchtype=author&query=Koren%2C+M">Moran Koren</a>, 
<a href="/search/cs?searchtype=author&query=Rosenfeld%2C+N">Nir Rosenfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">When users stand to gain from certain predictions, they are prone to act
strategically to obtain favorable predictive outcomes. Whereas most works on
strategic classification consider user actions that manifest as feature
modifications, we study a novel setting in which users decide -- in response to
the learned classifier -- whether to at all participate (or not). For learning
approaches of increasing strategic awareness, we study the effects of
self-selection on learning, and the implications of learning on the composition
of the self-selected population. We then propose a differentiable framework for
learning under self-selective behavior, which can be optimized effectively. We
conclude with experiments on real data and simulated behavior that both
complement our analysis and demonstrate the utility of our approach.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15276" title="Abstract">arXiv:2402.15276</a> [<a href="/pdf/2402.15276" title="Download PDF">pdf</a>, <a href="/format/2402.15276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale  Libraries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zijun Long</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+X">Xuri Ge</a>, 
<a href="/search/cs?searchtype=author&query=Mccreadie%2C+R">Richard Mccreadie</a>, 
<a href="/search/cs?searchtype=author&query=Jose%2C+J">Joemon Jose</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Text-to-image retrieval plays a crucial role across various applications,
including digital libraries, e-commerce platforms, and multimedia databases, by
enabling the search for images using text queries. Despite the advancements in
Multimodal Large Language Models (MLLMs), which offer leading-edge performance,
their applicability in large-scale, varied, and ambiguous retrieval scenarios
is constrained by significant computational demands and the generation of
injective embeddings. This paper introduces the Text2Pic Swift framework,
tailored for efficient and robust retrieval of images corresponding to
extensive textual descriptions in sizable datasets. The framework employs a
two-tier approach: the initial Entity-based Ranking (ER) stage addresses the
ambiguity inherent in lengthy text queries through a
multiple-queries-to-multiple-targets strategy, effectively narrowing down
potential candidates for subsequent analysis. Following this, the Summary-based
Re-ranking (SR) stage further refines these selections based on concise query
summaries. Additionally, we present a novel Decoupling-BEiT-3 encoder,
specifically designed to tackle the challenges of ambiguous queries and to
facilitate both stages of the retrieval process, thereby significantly
improving computational efficiency via vector-based similarity assessments. Our
evaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift
outperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000,
alongside reductions in training and retrieval durations by 68.75% and 99.79%,
respectively.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15277" title="Abstract">arXiv:2402.15277</a> [<a href="/pdf/2402.15277" title="Download PDF">pdf</a>, <a href="/format/2402.15277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy confidential virtual machines for the masses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galanou%2C+A">Anna Galanou</a>, 
<a href="/search/cs?searchtype=author&query=Bindlish%2C+K">Khushboo Bindlish</a>, 
<a href="/search/cs?searchtype=author&query=Preibsch%2C+L">Luca Preibsch</a>, 
<a href="/search/cs?searchtype=author&query=Pignolet%2C+Y">Yvonne-Anne Pignolet</a>, 
<a href="/search/cs?searchtype=author&query=Fetzer%2C+C">Christof Fetzer</a>, 
<a href="/search/cs?searchtype=author&query=Kapitza%2C+R">R&#xfc;diger Kapitza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Confidential computing alleviates the concerns of distrustful customers by
removing the cloud provider from their trusted computing base and resolves
their disincentive to migrate their workloads to the cloud. This is facilitated
by new hardware extensions, like AMD's SEV Secure Nested Paging (SEV-SNP),
which can run a whole virtual machine with confidentiality and integrity
protection against a potentially malicious hypervisor owned by an untrusted
cloud provider. However, the assurance of such protection to either the service
providers deploying sensitive workloads or the end-users passing sensitive data
to services requires sending proof to the interested parties. Service providers
can retrieve such proof by performing remote attestation while end-users have
typically no means to acquire this proof or validate its correctness and
therefore have to rely on the trustworthiness of the service providers. In this
paper, we present Revelio, an approach that features two main contributions: i)
it allows confidential virtual machine (VM)-based workloads to be designed and
deployed in a way that disallows any tampering even by the service providers
and ii) it empowers users to easily validate their integrity. In particular, we
focus on web-facing workloads, protect them leveraging SEV-SNP, and enable
end-users to remotely attest them seamlessly each time a new web session is
established. To highlight the benefits of Revelio, we discuss how a standalone
stateful VM that hosts an open-source collaboration office suite can be secured
and present a replicated protocol proxy that enables commodity users to
securely access the Internet Computer, a decentralized blockchain
infrastructure.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15278" title="Abstract">arXiv:2402.15278</a> [<a href="/pdf/2402.15278" title="Download PDF">pdf</a>, <a href="/ps/2402.15278" title="Download PostScript">ps</a>, <a href="/format/2402.15278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Economic and Financial Learning with Artificial Intelligence: A  Mixed-Methods Study on ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arndt%2C+H">Holger Arndt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In the evolving landscape of digital education, chatbots have emerged as
potential game-changers, promising personalized and adaptive learning
experiences. This research undertook an in-depth exploration of ChatGPT's
potential as an educational tool, focusing on user perceptions, experiences and
learning outcomes. Through a mixed-methods approach, a diverse group of 102
participants engaged with ChatGPT, providing insights pre- and postinteraction.
The study reveals a notable positive shift in perceptions after exposure,
underscoring the efficacy of ChatGPT. However, challenges such as prompting
effectiveness and information accuracy emerged as pivotal concerns. Introducing
the concept of 'AI-learning-competence', this study lays the groundwork for
future research, emphasizing the need for formal training and pedagogical
integration of AI tools.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15281" title="Abstract">arXiv:2402.15281</a> [<a href="/pdf/2402.15281" title="Download PDF">pdf</a>, <a href="/format/2402.15281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Implicit Swept Volume Models for Fast Collision Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joho%2C+D">Dominik Joho</a>, 
<a href="/search/cs?searchtype=author&query=Schwinn%2C+J">Jonas Schwinn</a>, 
<a href="/search/cs?searchtype=author&query=Safronov%2C+K">Kirill Safronov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at ICRA 2024. Dominik Joho and Jonas Schwinn have equal contribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Collision detection is one of the most time-consuming operations during
motion planning. Thus, there is an increasing interest in exploring machine
learning techniques to speed up collision detection and sampling-based motion
planning. A recent line of research focuses on utilizing neural signed distance
functions of either the robot geometry or the swept volume of the robot motion.
Building on this, we present a novel neural implicit swept volume model that is
the first to continuously represent arbitrary motions parameterized by their
start and goal configurations. This allows to quickly compute signed distances
for any point in the task space to the robot motion. Further, we present an
algorithm combining the speed of the deep learning-based signed distance
computations with the strong accuracy guarantees of geometric collision
checkers. We validate our approach in simulated and real-world robotic
experiments, and demonstrate that it is able to speed up a commercial bin
picking application.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15283" title="Abstract">arXiv:2402.15283</a> [<a href="/pdf/2402.15283" title="Download PDF">pdf</a>, <a href="/format/2402.15283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benfeghoul%2C+M">Martin Benfeghoul</a>, 
<a href="/search/cs?searchtype=author&query=Zahid%2C+U">Umais Zahid</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qinghai Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fountas%2C+Z">Zafeirios Fountas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In an unfamiliar setting, a model-based reinforcement learning agent can be
limited by the accuracy of its world model. In this work, we present a novel,
training-free approach to improving the performance of such agents separately
from planning and learning. We do so by applying iterative inference at
decision-time, to fine-tune the inferred agent states based on the coherence of
future state representations. Our approach achieves a consistent improvement in
both reconstruction accuracy and task performance when applied to visual 3D
navigation tasks. We go on to show that considering more future states further
improves the performance of the agent in partially-observable environments, but
not in a fully-observable one. Finally, we demonstrate that agents with less
training pre-evaluation benefit most from our approach.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15284" title="Abstract">arXiv:2402.15284</a> [<a href="/pdf/2402.15284" title="Download PDF">pdf</a>, <a href="/format/2402.15284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatiotemporal Observer Design for Predictive Learning of  High-Dimensional Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+T">Tongyi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han-Xiong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review by IEEE Transactions on Pattern Analysis and Machine Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Although deep learning-based methods have shown great success in
spatiotemporal predictive learning, the framework of those models is designed
mainly by intuition. How to make spatiotemporal forecasting with theoretical
guarantees is still a challenging issue. In this work, we tackle this problem
by applying domain knowledge from the dynamical system to the framework design
of deep learning models. An observer theory-guided deep learning architecture,
called Spatiotemporal Observer, is designed for predictive learning of high
dimensional data. The characteristics of the proposed framework are twofold:
firstly, it provides the generalization error bound and convergence guarantee
for spatiotemporal prediction; secondly, dynamical regularization is introduced
to enable the model to learn system dynamics better during training. Further
experimental results show that this framework could capture the spatiotemporal
dynamics and make accurate predictions in both one-step-ahead and
multi-step-ahead forecasting scenarios.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15289" title="Abstract">arXiv:2402.15289</a> [<a href="/pdf/2402.15289" title="Download PDF">pdf</a>, <a href="/format/2402.15289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let&#x27;s Rectify Step by Step: Improving Aspect-based Sentiment Analysis  with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shunyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qunxi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Q">Qingchun Bai</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to LREC-COLING 2024, submission version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting
the sentiment polarity associated with identified aspects within text. However,
a notable challenge in ABSA lies in precisely determining the aspects'
boundaries (start and end indices), especially for long ones, due to users'
colloquial expressions. We propose DiffusionABSA, a novel diffusion model
tailored for ABSA, which extracts the aspects progressively step by step.
Particularly, DiffusionABSA gradually adds noise to the aspect terms in the
training process, subsequently learning a denoising process that progressively
restores these terms in a reverse manner. To estimate the boundaries, we design
a denoising neural network enhanced by a syntax-aware temporal attention
mechanism to chronologically capture the interplay between aspects and
surrounding text. Empirical evaluations conducted on eight benchmark datasets
underscore the compelling advantages offered by DiffusionABSA when compared
against robust baseline models. Our code is publicly available at
https://github.com/Qlb6x/DiffusionABSA.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15290" title="Abstract">arXiv:2402.15290</a> [<a href="/pdf/2402.15290" title="Download PDF">pdf</a>, <a href="/format/2402.15290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+T">Tongyi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han-Xiong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review by IEEE Transactions on Neural Networks and Learning Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The trade-off between performance and computational efficiency in
long-sequence modeling becomes a bottleneck for existing models. Inspired by
the continuous state space models (SSMs) with multi-input and multi-output in
control theory, we propose a new neural network called Linear Dynamics-embedded
Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties
enable LDNN to have few parameters, flexible inference, and efficient training
in long-sequence tasks. Two efficient strategies, diagonalization and
$'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to
reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to
$O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional
noncausal and multi-head settings to accommodate a broader range of
applications. Extensive experiments on the Long Range Arena (LRA) demonstrate
the effectiveness and state-of-the-art performance of LDNN.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15293" title="Abstract">arXiv:2402.15293</a> [<a href="/pdf/2402.15293" title="Download PDF">pdf</a>, <a href="/format/2402.15293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: What don&#x27;t we know? Understanding Security Vulnerabilities in  SNARKs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaliasos%2C+S">Stefanos Chaliasos</a>, 
<a href="/search/cs?searchtype=author&query=Ernstberger%2C+J">Jens Ernstberger</a>, 
<a href="/search/cs?searchtype=author&query=Theodore%2C+D">David Theodore</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+D">David Wong</a>, 
<a href="/search/cs?searchtype=author&query=Jahanara%2C+M">Mohammad Jahanara</a>, 
<a href="/search/cs?searchtype=author&query=Livshits%2C+B">Benjamin Livshits</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Zero-knowledge proofs (ZKPs) have evolved from being a theoretical concept
providing privacy and verifiability to having practical, real-world
implementations, with SNARKs (Succinct Non-Interactive Argument of Knowledge)
emerging as one of the most significant innovations. Prior work has mainly
focused on designing more efficient SNARK systems and providing security proofs
for them. Many think of SNARKs as "just math," implying that what is proven to
be correct and secure is correct in practice. In contrast, this paper focuses
on assessing end-to-end security properties of real-life SNARK implementations.
We start by building foundations with a system model and by establishing threat
models and defining adversarial roles for systems that use SNARKs. Our study
encompasses an extensive analysis of 141 actual vulnerabilities in SNARK
implementations, providing a detailed taxonomy to aid developers and security
researchers in understanding the security threats in systems employing SNARKs.
Finally, we evaluate existing defense mechanisms and offer recommendations for
enhancing the security of SNARK-based systems, paving the way for more robust
and reliable implementations in the future.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15294" title="Abstract">arXiv:2402.15294</a> [<a href="/pdf/2402.15294" title="Download PDF">pdf</a>, <a href="/format/2402.15294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Music Generation in the Context of Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agchar%2C+I">Ismael Agchar</a>, 
<a href="/search/cs?searchtype=author&query=Baumann%2C+I">Ilja Baumann</a>, 
<a href="/search/cs?searchtype=author&query=Braun%2C+F">Franziska Braun</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Toro%2C+P+A">Paula Andrea Perez-Toro</a>, 
<a href="/search/cs?searchtype=author&query=Riedhammer%2C+K">Korbinian Riedhammer</a>, 
<a href="/search/cs?searchtype=author&query=Trump%2C+S">Sebastian Trump</a>, 
<a href="/search/cs?searchtype=author&query=Ullrich%2C+M">Martin Ullrich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In recent years, machine learning, and in particular generative adversarial
neural networks (GANs) and attention-based neural networks (transformers), have
been successfully used to compose and generate music, both melodies and
polyphonic pieces. Current research focuses foremost on style replication (eg.
generating a Bach-style chorale) or style transfer (eg. classical to jazz)
based on large amounts of recorded or transcribed music, which in turn also
allows for fairly straight-forward "performance" evaluation. However, most of
these models are not suitable for human-machine co-creation through live
interaction, neither is clear, how such models and resulting creations would be
evaluated. This article presents a thorough review of music representation,
feature analysis, heuristic algorithms, statistical and parametric modelling,
and human and automatic evaluation measures, along with a discussion of which
approaches and models seem most suitable for live interaction.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15297" title="Abstract">arXiv:2402.15297</a> [<a href="/pdf/2402.15297" title="Download PDF">pdf</a>, <a href="/format/2402.15297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised Counting via Pixel-by-pixel Density Distribution  Modelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hui Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhou Su</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+X">Xiaopeng Hong</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+D">Deyu Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the technical report of a paper that was submitted to IEEE Transactions and is now under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper focuses on semi-supervised crowd counting, where only a small
portion of the training data are labeled. We formulate the pixel-wise density
value to regress as a probability distribution, instead of a single
deterministic value. On this basis, we propose a semi-supervised crowd-counting
model. Firstly, we design a pixel-wise distribution matching loss to measure
the differences in the pixel-wise density distributions between the prediction
and the ground truth; Secondly, we enhance the transformer decoder by using
density tokens to specialize the forwards of decoders w.r.t. different density
intervals; Thirdly, we design the interleaving consistency self-supervised
learning mechanism to learn from unlabeled data efficiently. Extensive
experiments on four datasets are performed to show that our method clearly
outperforms the competitors by a large margin under various labeled ratio
settings. Code will be released at
https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15300" title="Abstract">arXiv:2402.15300</a> [<a href="/pdf/2402.15300" title="Download PDF">pdf</a>, <a href="/format/2402.15300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeing is Believing: Mitigating Hallucination in Large Vision-Language  Models via CLIP-Guided Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+A">Ailin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhirui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Large Vision-Language Models (LVLMs) are susceptible to object
hallucinations, an issue in which their generated text contains non-existent
objects, greatly limiting their reliability and practicality. Current
approaches often rely on the model's token likelihoods or other internal
information, instruction tuning on additional datasets, or incorporating
complex external tools. We first perform empirical analysis on sentence-level
LVLM hallucination, finding that CLIP similarity to the image acts as a
stronger and more robust indicator of hallucination compared to token
likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)
approach, a straightforward but effective training-free approach to reduce
object hallucination at decoding time. CGD uses CLIP to guide the model's
decoding process by enhancing visual grounding of generated text with the
image. Experiments demonstrate that CGD effectively mitigates object
hallucination across multiple LVLM families while preserving the utility of
text generation.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15301" title="Abstract">arXiv:2402.15301</a> [<a href="/pdf/2402.15301" title="Download PDF">pdf</a>, <a href="/format/2402.15301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Graph Discovery with Retrieval-Augmented Generation based Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuzhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Y">Yidong Gan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Causal graph recovery is essential in the field of causal inference.
Traditional methods are typically knowledge-based or statistical
estimation-based, which are limited by data collection biases and individuals'
knowledge about factors affecting the relations between variables of interests.
The advance of large language models (LLMs) provides opportunities to address
these problems. We propose a novel method that utilizes the extensive knowledge
contained within a large corpus of scientific literature to deduce causal
relationships in general causal graph recovery tasks. This method leverages
Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and
extract pertinent information from a comprehensive collection of research
papers. Our method first retrieves relevant text chunks from the aggregated
literature. Then, the LLM is tasked with identifying and labelling potential
associations between factors. Finally, we give a method to aggregate the
associational relationships to build a causal graph. We demonstrate our method
is able to construct high quality causal graphs on the well-known SACHS dataset
solely from literature.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15302" title="Abstract">arXiv:2402.15302</a> [<a href="/pdf/2402.15302" title="Download PDF">pdf</a>, <a href="/format/2402.15302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How (un)ethical are instruction-centric responses of LLMs? Unveiling the  vulnerabilities of safety guardrails to harmful queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Somnath Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Layek%2C+S">Sayan Layek</a>, 
<a href="/search/cs?searchtype=author&query=Hazra%2C+R">Rima Hazra</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In this study, we tackle a growing concern around the safety and ethical use
of large language models (LLMs). Despite their potential, these models can be
tricked into producing harmful or unethical content through various
sophisticated methods, including 'jailbreaking' techniques and targeted
manipulation. Our work zeroes in on a specific issue: to what extent LLMs can
be led astray by asking them to generate responses that are instruction-centric
such as a pseudocode, a program or a software snippet as opposed to vanilla
text. To investigate this question, we introduce TechHazardQA, a dataset
containing complex queries which should be answered in both text and
instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers
for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b,
Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and
instruction-centric responses. For evaluation we report the harmfulness score
metric as well as judgements from GPT-4 and humans. Overall, we observe that
asking LLMs to produce instruction-centric responses enhances the unethical
response generation by ~2-38% across the models. As an additional objective, we
investigate the impact of model editing using the ROME technique, which further
increases the propensity for generating undesirable content. In particular,
asking edited LLMs to generate instruction-centric responses further increases
the unethical response generation by ~3-16% across the different models.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15307" title="Abstract">arXiv:2402.15307</a> [<a href="/pdf/2402.15307" title="Download PDF">pdf</a>, <a href="/format/2402.15307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representing Online Handwriting for Recognition in Large Vision-Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fadeeva%2C+A">Anastasiia Fadeeva</a>, 
<a href="/search/cs?searchtype=author&query=Schlattner%2C+P">Philippe Schlattner</a>, 
<a href="/search/cs?searchtype=author&query=Maksai%2C+A">Andrii Maksai</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+M">Mark Collier</a>, 
<a href="/search/cs?searchtype=author&query=Kokiopoulou%2C+E">Efi Kokiopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Berent%2C+J">Jesse Berent</a>, 
<a href="/search/cs?searchtype=author&query=Musat%2C+C">Claudiu Musat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The adoption of tablets with touchscreens and styluses is increasing, and a
key feature is converting handwriting to text, enabling search, indexing, and
AI assistance. Meanwhile, vision-language models (VLMs) are now the go-to
solution for image understanding, thanks to both their state-of-the-art
performance across a variety of tasks and the simplicity of a unified approach
to training, fine-tuning, and inference. While VLMs obtain high performance on
image-based tasks, they perform poorly on handwriting recognition when applied
naively, i.e., by rendering handwriting as an image and performing optical
character recognition (OCR). In this paper, we study online handwriting
recognition with VLMs, going beyond naive OCR. We propose a novel tokenized
representation of digital ink (online handwriting) that includes both a
time-ordered sequence of strokes as text, and as image. We show that this
representation yields results comparable to or better than state-of-the-art
online handwriting recognizers. Wide applicability is shown through results
with two different VLM families, on multiple public datasets. Our approach can
be applied to off-the-shelf VLMs, does not require any changes in their
architecture, and can be used in both fine-tuning and parameter-efficient
tuning. We perform a detailed ablation study to identify the key elements of
the proposed representation.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15309" title="Abstract">arXiv:2402.15309</a> [<a href="/pdf/2402.15309" title="Download PDF">pdf</a>, <a href="/format/2402.15309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Generation with Identifiability Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hanqi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingjing Kong</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips23. Controllable generation in causal perspective with a case study of ChatGPT, sheds light on theory-guaranteed alignment in language models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Counterfactual generation lies at the core of various machine learning tasks,
including image translation and controllable text generation. This generation
process usually requires the identification of the disentangled latent
representations, such as content and style, that underlie the observed data.
However, it becomes more challenging when faced with a scarcity of paired data
and labeling information. Existing disentangled methods crucially rely on
oversimplified assumptions, such as assuming independent content and style
variables, to identify the latent variables, even though such assumptions may
not hold for complex data distributions. For instance, food reviews tend to
involve words like tasty, whereas movie reviews commonly contain words such as
thrilling for the same positive sentiment. This problem is exacerbated when
data are sampled from multiple domains since the dependence between content and
style may vary significantly over domains. In this work, we tackle the
domain-varying dependence between the content and the style variables inherent
in the counterfactual generation task. We provide identification guarantees for
such latent-variable models by leveraging the relative sparsity of the
influences from different latent variables. Our theoretical insights enable the
development of a doMain AdapTive counTerfactual gEneration model, called
(MATTE). Our theoretically grounded framework achieves state-of-the-art
performance in unsupervised style transfer tasks, where neither paired data nor
style labels are utilized, across four large-scale datasets. Code is available
at https://github.com/hanqi-qi/Matte.git
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15313" title="Abstract">arXiv:2402.15313</a> [<a href="/pdf/2402.15313" title="Download PDF">pdf</a>, <a href="/format/2402.15313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArabianGPT: Native Arabic GPT-based Large Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koubaa%2C+A">Anis Koubaa</a>, 
<a href="/search/cs?searchtype=author&query=Ammar%2C+A">Adel Ammar</a>, 
<a href="/search/cs?searchtype=author&query=Ghouti%2C+L">Lahouari Ghouti</a>, 
<a href="/search/cs?searchtype=author&query=Najar%2C+O">Omar Najar</a>, 
<a href="/search/cs?searchtype=author&query=Sibaee%2C+S">Serry Sibaee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The predominance of English and Latin-based large language models (LLMs) has
led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated
by the prevalent inclusion of English tokens in existing Arabic models,
detracting from their efficacy in processing native Arabic's intricate
morphology and syntax. Consequently, there is a theoretical and practical
imperative for developing LLMs predominantly focused on Arabic linguistic
elements. To address this gap, this paper proposes ArabianGPT, a series of
transformer-based models within the ArabianLLM suite designed explicitly for
Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in
size and complexity, aligning with the nuanced linguistic characteristics of
Arabic. The AraNizer tokenizer, integral to these models, addresses the unique
morphological aspects of Arabic script, ensuring more accurate text processing.
Empirical results from fine-tuning the models on tasks like sentiment analysis
and summarization demonstrate significant improvements. For sentiment analysis,
the fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a
substantial increase from the base model's 56%. Similarly, in summarization
tasks, fine-tuned models showed enhanced F1 scores, indicating improved
precision and recall in generating concise summaries. Comparative analysis of
fine-tuned ArabianGPT models against their base versions across various
benchmarks reveals nuanced differences in performance, with fine-tuning
positively impacting specific tasks like question answering and summarization.
These findings underscore the efficacy of fine-tuning in aligning ArabianGPT
models more closely with specific NLP tasks, highlighting the potential of
tailored transformer architectures in advancing Arabic NLP.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15315" title="Abstract">arXiv:2402.15315</a> [<a href="/pdf/2402.15315" title="Download PDF">pdf</a>, <a href="/ps/2402.15315" title="Download PostScript">ps</a>, <a href="/format/2402.15315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Minimal Depth in Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valerdi%2C+J+L">Juan L. Valerdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">A characterization of the representability of neural networks is relevant to
comprehend their success in artificial intelligence. This study investigate two
topics on ReLU neural network expressivity and their connection with a
conjecture related to the minimum depth required for representing any
continuous piecewise linear function (CPWL). The topics are the minimal depth
representation of the sum and max operations, as well as the exploration of
polytope neural networks. For the sum operation, we establish a sufficient
condition on the minimal depth of the operands to find the minimal depth of the
operation. In contrast, regarding the max operation, a comprehensive set of
examples is presented, demonstrating that no sufficient conditions, depending
solely on the depth of the operands, would imply a minimal depth for the
operation. The study also examine the minimal depth relationship between convex
CPWL functions. On polytope neural networks, we investigate several fundamental
properties, deriving results equivalent to those of ReLU networks, such as
depth inclusions and depth computation from vertices. Notably, we compute the
minimal depth of simplices, which is strictly related to the minimal depth
conjecture in ReLU networks.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15319" title="Abstract">arXiv:2402.15319</a> [<a href="/pdf/2402.15319" title="Download PDF">pdf</a>, <a href="/format/2402.15319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPTVQ: The Blessing of Dimensionality for LLM Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Baalen%2C+M">Mart van Baalen</a>, 
<a href="/search/cs?searchtype=author&query=Kuzmin%2C+A">Andrey Kuzmin</a>, 
<a href="/search/cs?searchtype=author&query=Nagel%2C+M">Markus Nagel</a>, 
<a href="/search/cs?searchtype=author&query=Couperus%2C+P">Peter Couperus</a>, 
<a href="/search/cs?searchtype=author&query=Bastoul%2C+C">Cedric Bastoul</a>, 
<a href="/search/cs?searchtype=author&query=Mahurin%2C+E">Eric Mahurin</a>, 
<a href="/search/cs?searchtype=author&query=Blankevoort%2C+T">Tijmen Blankevoort</a>, 
<a href="/search/cs?searchtype=author&query=Whatmough%2C+P">Paul Whatmough</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In this work we show that the size versus accuracy trade-off of neural
network quantization can be significantly improved by increasing the
quantization dimensionality. We propose the GPTVQ method, a new fast method for
post-training vector quantization (VQ) that scales well to Large Language
Models (LLMs). Our method interleaves quantization of one or more columns with
updates to the remaining unquantized weights, using information from the
Hessian of the per-layer output reconstruction MSE. Quantization codebooks are
initialized using an efficient data-aware version of the EM algorithm. The
codebooks are then updated, and further compressed by using integer
quantization and SVD-based compression. GPTVQ establishes a new state-of-the
art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2
and Mistral. Furthermore, our method is efficient: on a single H100 it takes
between 3 and 11 hours to process a Llamav2-70B model, depending on
quantization setting. Lastly, with on-device timings for VQ decompression on a
mobile CPU we show that VQ leads to improved latency compared to using a 4-bit
integer format.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15321" title="Abstract">arXiv:2402.15321</a> [<a href="/pdf/2402.15321" title="Download PDF">pdf</a>, <a href="/format/2402.15321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Engelmann%2C+F">Francis Engelmann</a>, 
<a href="/search/cs?searchtype=author&query=Takmaz%2C+A">Ayca Takmaz</a>, 
<a href="/search/cs?searchtype=author&query=Schult%2C+J">Jonas Schult</a>, 
<a href="/search/cs?searchtype=author&query=Fedele%2C+E">Elisabetta Fedele</a>, 
<a href="/search/cs?searchtype=author&query=Wald%2C+J">Johanna Wald</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Songyou Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Litany%2C+O">Or Litany</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Tombari%2C+F">Federico Tombari</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>, 
<a href="/search/cs?searchtype=author&query=Guibas%2C+L">Leonidas Guibas</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Hongbo Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiaosheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bingwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuanyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+P">Phuc Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khoi Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+A">Anh Tran</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+C">Cuong Pham</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhening Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lasenby%2C+J">Joan Lasenby</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our OpenSUN3D workshop website for ICCV 2023: <a href="https://opensun3d.github.io/index_iccv23.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This report provides an overview of the challenge hosted at the OpenSUN3D
Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with
ICCV 2023. The goal of this workshop series is to provide a platform for
exploration and discussion of open-vocabulary 3D scene understanding tasks,
including but not limited to segmentation, detection and mapping. We provide an
overview of the challenge hosted at the workshop, present the challenge
dataset, the evaluation methodology, and brief descriptions of the winning
methods. For additional details, please see
https://opensun3d.github.io/index_iccv23.html.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15322" title="Abstract">arXiv:2402.15322</a> [<a href="/pdf/2402.15322" title="Download PDF">pdf</a>, <a href="/format/2402.15322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport on the Lie Group of Roto-translations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bon%2C+D">Daan Bon</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+G">Gautam Pai</a>, 
<a href="/search/cs?searchtype=author&query=Bellaard%2C+G">Gijs Bellaard</a>, 
<a href="/search/cs?searchtype=author&query=Mula%2C+O">Olga Mula</a>, 
<a href="/search/cs?searchtype=author&query=Duits%2C+R">Remco Duits</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Differential Geometry (math.DG); Optimization and Control (math.OC)

</div>
<p class="mathjax">The roto-translation group SE2 has been of active interest in image analysis
due to methods that lift the image data to multi-orientation representations
defined on this Lie group. This has led to impactful applications of
crossing-preserving flows for image de-noising, geodesic tracking, and
roto-translation equivariant deep learning. In this paper, we develop a
computational framework for optimal transportation over Lie groups, with a
special focus on SE2. We make several theoretical contributions (generalizable
to matrix Lie groups) such as the non-optimality of group actions as transport
maps, invariance and equivariance of optimal transport, and the quality of the
entropic-regularized optimal transport plan using geodesic distance
approximations. We develop a Sinkhorn like algorithm that can be efficiently
implemented using fast and accurate distance approximations of the Lie group
and GPU-friendly group convolutions. We report valuable advancements in the
experiments on 1) image barycenters, 2) interpolation of planar orientation
fields, and 3) Wasserstein gradient flows on SE2. We observe that our framework
of lifting images to SE2 and optimal transport with left-invariant anisotropic
metrics leads to equivariant transport along dominant contours and salient line
structures in the image. This yields sharper and more meaningful interpolations
compared to their counterparts on $\mathbb{R}^2$
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15324" title="Abstract">arXiv:2402.15324</a> [<a href="/pdf/2402.15324" title="Download PDF">pdf</a>, <a href="/format/2402.15324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method  and Its Application to Energy Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianhong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 206 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multi-agent reinforcement learning is an area of rapid advancement in
artificial intelligence and machine learning. One of the important questions to
be answered is how to conduct credit assignment in a multi-agent system. There
have been many schemes designed to conduct credit assignment by multi-agent
reinforcement learning algorithms. Although these credit assignment schemes
have been proved useful in improving the performance of multi-agent
reinforcement learning, most of them are designed heuristically without a
rigorous theoretic basis and therefore infeasible to understand how agents
cooperate. In this thesis, we aim at investigating the foundation of credit
assignment in multi-agent reinforcement learning via cooperative game theory.
We first extend a game model called convex game and a payoff distribution
scheme called Shapley value in cooperative game theory to Markov decision
process, named as Markov convex game and Markov Shapley value respectively. We
represent a global reward game as a Markov convex game under the grand
coalition. As a result, Markov Shapley value can be reasonably used as a credit
assignment scheme in the global reward game. Markov Shapley value possesses the
following virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii)
reflecting the contribution and (iv) symmetry, which form the fair credit
assignment. Based on Markov Shapley value, we propose three multi-agent
reinforcement learning algorithms called SHAQ, SQDDPG and SMFPPO. Furthermore,
we extend Markov convex game to partial observability to deal with the
partially observable problems, named as partially observable Markov convex
game. In application, we evaluate SQDDPG and SMFPPO on the real-world problem
in energy networks.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15326" title="Abstract">arXiv:2402.15326</a> [<a href="/pdf/2402.15326" title="Download PDF">pdf</a>, <a href="/format/2402.15326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective  of Operator Semigroup Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weichen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Congying Han</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tiande Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianshu Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper presents a novel study of the oversmoothing issue in
diffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches
grounded in random walk analysis or particle systems, we approach this problem
through operator semigroup theory. This theoretical framework allows us to
rigorously prove that oversmoothing is intrinsically linked to the ergodicity
of the diffusion operator. This finding further poses a general and mild
ergodicity-breaking condition, encompassing the various specific solutions
previously offered, thereby presenting a more universal and theoretically
grounded approach to mitigating oversmoothing in diffusion-based GNNs.
Additionally, we offer a probabilistic interpretation of our theory, forging a
link with prior works and broadening the theoretical horizon. Our experimental
results reveal that this ergodicity-breaking term effectively mitigates
oversmoothing measured by Dirichlet energy, and simultaneously enhances
performance in node classification tasks.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15328" title="Abstract">arXiv:2402.15328</a> [<a href="/pdf/2402.15328" title="Download PDF">pdf</a>, <a href="/format/2402.15328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Principled Task Grouping for Multi-Task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuanhao Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianshu Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper presents a novel approach to task grouping in Multitask Learning
(MTL), advancing beyond existing methods by addressing key theoretical and
practical limitations. Unlike prior studies, our approach offers a more
theoretically grounded method that does not rely on restrictive assumptions for
constructing transfer gains. We also propose a flexible mathematical
programming formulation which can accommodate a wide spectrum of resource
constraints, thus enhancing its versatility. Experimental results across
diverse domains, including computer vision datasets, combinatorial optimization
benchmarks and time series tasks, demonstrate the superiority of our method
over extensive baselines, validating its effectiveness and general
applicability in MTL.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15331" title="Abstract">arXiv:2402.15331</a> [<a href="/pdf/2402.15331" title="Download PDF">pdf</a>, <a href="/format/2402.15331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Blockchain-Enabled Framework of UAV Coordination for Post-Disaster  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hafeez%2C+S">Sana Hafeez</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Runze Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Mohjazi%2C+L">Lina Mohjazi</a>, 
<a href="/search/cs?searchtype=author&query=Imran%2C+M+A">Muhammad Ali Imran</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yao Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures,IEEE 99th Vehicular Technology Conference: VTC2024-Spring, Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Emergency communication is critical but challenging after natural disasters
when ground infrastructure is devastated. Unmanned aerial vehicles (UAVs) offer
enormous potential for agile relief coordination in these scenarios. However,
effectively leveraging UAV fleets poses additional challenges around security,
privacy, and efficient collaboration across response agencies. This paper
presents a robust blockchain-enabled framework to address these challenges by
integrating a consortium blockchain model, smart contracts, and cryptographic
techniques to securely coordinate UAV fleets for disaster response.
Specifically, we make two key contributions: a consortium blockchain
architecture for secure and private multi-agency coordination; and an optimized
consensus protocol balancing efficiency and fault tolerance using a delegated
proof of stake practical byzantine fault tolerance (DPoS-PBFT). Comprehensive
simulations showcase the framework's ability to enhance transparency,
automation, scalability, and cyber-attack resilience for UAV coordination in
post-disaster networks.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15332" title="Abstract">arXiv:2402.15332</a> [<a href="/pdf/2402.15332" title="Download PDF">pdf</a>, <a href="/ps/2402.15332" title="Download PostScript">ps</a>, <a href="/format/2402.15332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Categorical Deep Learning: An Algebraic Theory of Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gavranovi%C4%87%2C+B">Bruno Gavranovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Lessard%2C+P">Paul Lessard</a>, 
<a href="/search/cs?searchtype=author&query=Dudzik%2C+A">Andrew Dudzik</a>, 
<a href="/search/cs?searchtype=author&query=von+Glehn%2C+T">Tamara von Glehn</a>, 
<a href="/search/cs?searchtype=author&query=Ara%C3%BAjo%2C+J+G+M">Jo&#xe3;o G. M. Ara&#xfa;jo</a>, 
<a href="/search/cs?searchtype=author&query=Veli%C4%8Dkovi%C4%87%2C+P">Petar Veli&#x10d;kovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress -- comments welcome. More info at categoricaldeeplearning.com
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Category Theory (math.CT); Rings and Algebras (math.RA); Machine Learning (stat.ML)

</div>
<p class="mathjax">We present our position on the elusive quest for a general-purpose framework
for specifying and studying deep learning architectures. Our opinion is that
the key attempts made so far lack a coherent bridge between specifying
constraints which models must satisfy and specifying their implementations.
Focusing on building a such a bridge, we propose to apply category theory --
precisely, the universal algebra of monads valued in a 2-category of parametric
maps -- as a single theory elegantly subsuming both of these flavours of neural
network design. To defend our position, we show how this theory recovers
constraints induced by geometric deep learning, as well as implementations of
many architectures drawn from the diverse landscape of neural networks, such as
RNNs. We also illustrate how the theory naturally encodes many standard
constructs in computer science and automata theory.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15334" title="Abstract">arXiv:2402.15334</a> [<a href="/pdf/2402.15334" title="Download PDF">pdf</a>, <a href="/format/2402.15334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Inversion of (ELAA-)MIMO Channels Using Symmetric Rank-$1$  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Tafazolli%2C+R">Rahim Tafazolli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">While iterative matrix inversion methods excel in computational efficiency,
memory optimization, and support for parallel and distributed computing when
managing large matrices, their limitations are also evident in multiple-input
multiple-output (MIMO) fading channels. These methods encounter challenges
related to slow convergence and diminished accuracy, especially in
ill-conditioned scenarios, hindering their application in future MIMO networks
such as extra-large aperture array (ELAA). To address these challenges, this
paper proposes a novel matrix regularization method termed symmetric rank-$1$
regularization (SR-$1$R). The proposed method functions by augmenting the
channel matrix with a symmetric rank-$1$ matrix, with the primary goal of
minimizing the condition number of the resultant regularized matrix. This
significantly improves the matrix condition, enabling fast and accurate
iterative inversion of the regularized matrix. Then, the inverse of the
original channel matrix is obtained by applying the Sherman-Morrison transform
on the outcome of iterative inversions. Our eigenvalue analysis unveils the
best channel condition that can be achieved by an optimized SR-$1$R matrix.
Moreover, a power iteration-assisted (PIA) approach is proposed to find the
optimum SR-$1$R matrix without need of eigenvalue decomposition. The proposed
approach exhibits logarithmic algorithm-depth in parallel computing for MIMO
precoding. Finally, computer simulations demonstrate that SR-$1$R has the
potential to reduce iterative iterations by up to $33\%$, while also
significantly improve symbol error probability by approximately an order of
magnitude.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15337" title="Abstract">arXiv:2402.15337</a> [<a href="/pdf/2402.15337" title="Download PDF">pdf</a>, <a href="/format/2402.15337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ranking Entities along Conceptual Space Dimensions with LLMs: An  Analysis of Fine-Tuning Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Nitesh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+U">Usashi Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Schockaert%2C+S">Steven Schockaert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Conceptual spaces represent entities in terms of their primitive semantic
features. Such representations are highly valuable but they are notoriously
difficult to learn, especially when it comes to modelling perceptual and
subjective features. Distilling conceptual spaces from Large Language Models
(LLMs) has recently emerged as a promising strategy. However, existing work has
been limited to probing pre-trained LLMs using relatively simple zero-shot
strategies. We focus in particular on the task of ranking entities according to
a given conceptual space dimension. Unfortunately, we cannot directly fine-tune
LLMs on this task, because ground truth rankings for conceptual space
dimensions are rare. We therefore use more readily available features as
training data and analyse whether the ranking capabilities of the resulting
models transfer to perceptual and subjective features. We find that this is
indeed the case, to some extent, but having perceptual and subjective features
in the training data seems essential for achieving the best results. We
furthermore find that pointwise ranking strategies are competitive against
pairwise approaches, in defiance of common wisdom.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15340" title="Abstract">arXiv:2402.15340</a> [<a href="/pdf/2402.15340" title="Download PDF">pdf</a>, <a href="/ps/2402.15340" title="Download PostScript">ps</a>, <a href="/format/2402.15340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaStates: An Approach for Representing Human Workers  Psychophysiological States in the Industrial Metaverse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eyam%2C+A+T">Aitor Toichoa Eyam</a>, 
<a href="/search/cs?searchtype=author&query=Lastra%2C+J+L+M">Jose L. Martinez Lastra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 9 figures, 2 tables, journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Photo-realistic avatar is a modern term referring to the digital asset that
represents a human in computer graphic advance systems such as video games and
simulation tools. These avatars utilize the advances in graphic technologies on
both software and hardware aspects. While photorealistic avatars are
increasingly used in industrial simulations, representing human factors such as
human workers internal states, remains a challenge. This article addresses this
issue by introducing the concept of MetaStates which are the digitization and
representation of the psychophysiological states of a human worker in the
digital world. The MetaStates influence the physical representation and
performance of a digital human worker while performing a task. To demonstrate
this concept the study presents a development of a photorealistic avatar which
is integrated into a simulated environment and enhanced with a multi-level
graphical representation of different psychophysiological states. This approach
represents a major step forward in the use of digital humans for industrial
simulations, allowing companies to better leverage the benefits of the
Industrial Metaverse in their daily operations and simulations while keeping
human workers at the center of the system.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15343" title="Abstract">arXiv:2402.15343</a> [<a href="/pdf/2402.15343" title="Download PDF">pdf</a>, <a href="/format/2402.15343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bogdanov%2C+S">Sergei Bogdanov</a>, 
<a href="/search/cs?searchtype=author&query=Constantin%2C+A">Alexandre Constantin</a>, 
<a href="/search/cs?searchtype=author&query=Bernard%2C+T">Timoth&#xe9;e Bernard</a>, 
<a href="/search/cs?searchtype=author&query=Crabb%C3%A9%2C+B">Benoit Crabb&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Bernard%2C+E">Etienne Bernard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have shown impressive abilities in data
annotation, opening the way for new approaches to solve classic NLP problems.
In this paper, we show how to use LLMs to create NuNER, a compact language
representation model specialized in the Named Entity Recognition (NER) task.
NuNER can be fine-tuned to solve downstream NER problems in a data-efficient
way, outperforming similar-sized foundation models in the few-shot regime and
competing with much larger LLMs. We find that the size and entity-type
diversity of the pre-training dataset are key to achieving good performance. We
view NuNER as a member of the broader family of task-specific foundation
models, recently unlocked by LLMs.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15345" title="Abstract">arXiv:2402.15345</a> [<a href="/pdf/2402.15345" title="Download PDF">pdf</a>, <a href="/format/2402.15345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fourier Basis Density Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+la+Fuente%2C+A">Alfredo De la Fuente</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Saurabh Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ball%C3%A9%2C+J">Johannes Ball&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce a lightweight, flexible and end-to-end trainable probability
density model parameterized by a constrained Fourier basis. We assess its
performance at approximating a range of multi-modal 1D densities, which are
generally difficult to fit. In comparison to the deep factorized model
introduced in [1], our model achieves a lower cross entropy at a similar
computational budget. In addition, we also evaluate our method on a toy
compression task, demonstrating its utility in learned compression.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15347" title="Abstract">arXiv:2402.15347</a> [<a href="/pdf/2402.15347" title="Download PDF">pdf</a>, <a href="/format/2402.15347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-Theoretic Safe Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bottero%2C+A+G">Alessandro G. Bottero</a>, 
<a href="/search/cs?searchtype=author&query=Luis%2C+C+E">Carlos E. Luis</a>, 
<a href="/search/cs?searchtype=author&query=Vinogradska%2C+J">Julia Vinogradska</a>, 
<a href="/search/cs?searchtype=author&query=Berkenkamp%2C+F">Felix Berkenkamp</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jan Peters</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2212.04914">arXiv:2212.04914</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider a sequential decision making task, where the goal is to optimize
an unknown function without evaluating parameters that violate an a~priori
unknown (safety) constraint. A common approach is to place a Gaussian process
prior on the unknown functions and allow evaluations only in regions that are
safe with high probability. Most current methods rely on a discretization of
the domain and cannot be directly extended to the continuous case. Moreover,
the way in which they exploit regularity assumptions about the constraint
introduces an additional critical hyperparameter. In this paper, we propose an
information-theoretic safe exploration criterion that directly exploits the GP
posterior to identify the most informative safe parameters to evaluate. The
combination of this exploration criterion with a well known Bayesian
optimization acquisition function yields a novel safe Bayesian optimization
selection criterion. Our approach is naturally applicable to continuous domains
and does not require additional explicit hyperparameters. We theoretically
analyze the method and show that we do not violate the safety constraint with
high probability and that we learn about the value of the safe optimum up to
arbitrary precision. Empirical evaluations demonstrate improved data-efficiency
and scalability.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15348" title="Abstract">arXiv:2402.15348</a> [<a href="/pdf/2402.15348" title="Download PDF">pdf</a>, <a href="/ps/2402.15348" title="Download PostScript">ps</a>, <a href="/format/2402.15348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Approximation and Kernelization Bounds for Vertex-Disjoint  Shortest Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bentert%2C+M">Matthias Bentert</a>, 
<a href="/search/cs?searchtype=author&query=Fomin%2C+F+V">Fedor V. Fomin</a>, 
<a href="/search/cs?searchtype=author&query=Golovach%2C+P+A">Petr A. Golovach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We examine the possibility of approximating Maximum Vertex-Disjoint Shortest
Paths. In this problem, the input is an edge-weighted (directed or undirected)
$n$-vertex graph $G$ along with $k$ terminal pairs
$(s_1,t_1),(s_2,t_2),\ldots,(s_k,t_k)$. The task is to connect as many terminal
pairs as possible by pairwise vertex-disjoint paths such that each path is a
shortest path between the respective terminals. Our work is anchored in the
recent breakthrough by Lochet [SODA '21], which demonstrates the
polynomial-time solvability of the problem for a fixed value of $k$.
<br />Lochet's result implies the existence of a polynomial-time $ck$-approximation
for Maximum Vertex-Disjoint Shortest Paths, where $c \leq 1$ is a constant. Our
first result suggests that this approximation algorithm is, in a sense, the
best we can hope for. More precisely, assuming the gap-ETH, we exclude the
existence of an $o(k)$-approximations within $f(k) \cdot $poly($n$) time for
any function $f$ that only depends on $k$.
<br />Our second result demonstrates the infeasibility of achieving an
approximation ratio of $n^{\frac{1}{2}-\varepsilon}$ in polynomial time, unless
P = NP. It is not difficult to show that a greedy algorithm selecting a path
with the minimum number of arcs results in a
$\lceil\sqrt{\ell}\rceil$-approximation, where $\ell$ is the number of edges in
all the paths of an optimal solution. Since $\ell \leq n$, this underscores the
tightness of the $n^{\frac{1}{2}-\varepsilon}$-inapproximability bound.
<br />Additionally, we establish that Maximum Vertex-Disjoint Shortest Paths is
fixed-parameter tractable when parameterized by $\ell$ but does not admit a
polynomial kernel. Our hardness results hold for undirected graphs with unit
weights, while our positive results extend to scenarios where the input graph
is directed and features arbitrary (non-negative) edge weights.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15350" title="Abstract">arXiv:2402.15350</a> [<a href="/pdf/2402.15350" title="Download PDF">pdf</a>, <a href="/format/2402.15350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Farsight: Fostering Responsible AI Awareness During AI Application  Prototyping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z+J">Zijie J. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+C">Chinmay Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Wilcox%2C+L">Lauren Wilcox</a>, 
<a href="/search/cs?searchtype=author&query=Terry%2C+M">Michael Terry</a>, 
<a href="/search/cs?searchtype=author&query=Madaio%2C+M">Michael Madaio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CHI 2024. 40 pages, 19 figures, 5 tables. For a demo video, see <a href="https://youtu.be/BlSFbGkOlHk.">this https URL</a> For a live demo, visit <a href="https://PAIR-code.github.io/farsight.">this https URL</a> The source code is available at <a href="https://github.com/PAIR-code/farsight">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Prompt-based interfaces for Large Language Models (LLMs) have made
prototyping and building AI-powered applications easier than ever before.
However, identifying potential harms that may arise from AI applications
remains a challenge, particularly during prompt-based prototyping. To address
this, we present Farsight, a novel in situ interactive tool that helps people
identify potential harms from the AI applications they are prototyping. Based
on a user's prompt, Farsight highlights news articles about relevant AI
incidents and allows users to explore and edit LLM-generated use cases,
stakeholders, and harms. We report design insights from a co-design study with
10 AI prototypers and findings from a user study with 42 AI prototypers. After
using Farsight, AI prototypers in our user study are better able to
independently identify potential harms associated with a prompt and find our
tool more useful and usable than existing resources. Their qualitative feedback
also highlights that Farsight encourages them to focus on end-users and think
beyond immediate harms. We discuss these findings and reflect on their
implications for designing AI prototyping experiences that meaningfully engage
with AI harms. Farsight is publicly accessible at:
https://PAIR-code.github.io/farsight.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15351" title="Abstract">arXiv:2402.15351</a> [<a href="/pdf/2402.15351" title="Download PDF">pdf</a>, <a href="/format/2402.15351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoMMLab: Automatically Generating Deployable Models from Language  Instructions for Computer Vision Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zekang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Sheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wentao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Automated machine learning (AutoML) is a collection of techniques designed to
automate the machine learning development process. While traditional AutoML
approaches have been successfully applied in several critical steps of model
development (e.g. hyperparameter optimization), there lacks a AutoML system
that automates the entire end-to-end model production workflow. To fill this
blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that
follows user's language instructions to automate the whole model production
workflow for computer vision tasks. The proposed AutoMMLab system effectively
employs LLMs as the bridge to connect AutoML and OpenMMLab community,
empowering non-expert individuals to easily build task-specific models via a
user-friendly language interface. Specifically, we propose RU-LLaMA to
understand users' request and schedule the whole pipeline, and propose a novel
LLM-based hyperparameter optimizer called HPO-LLaMA to effectively search for
the optimal hyperparameters. Experiments show that our AutoMMLab system is
versatile and covers a wide range of mainstream tasks, including
classification, detection, segmentation and keypoint estimation. We further
develop a new benchmark, called LAMP, for studying key components in the
end-to-end prompt-based model training pipeline. Code, model, and data will be
released.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15352" title="Abstract">arXiv:2402.15352</a> [<a href="/pdf/2402.15352" title="Download PDF">pdf</a>, <a href="/format/2402.15352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On normalization-equivariance properties of supervised and unsupervised  denoising methods: a survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herbreteau%2C+S">S&#xe9;bastien Herbreteau</a>, 
<a href="/search/cs?searchtype=author&query=Kervrann%2C+C">Charles Kervrann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Image denoising is probably the oldest and still one of the most active
research topic in image processing. Many methodological concepts have been
introduced in the past decades and have improved performances significantly in
recent years, especially with the emergence of convolutional neural networks
and supervised deep learning. In this paper, we propose a survey of guided tour
of supervised and unsupervised learning methods for image denoising,
classifying the main principles elaborated during this evolution, with a
particular concern given to recent developments in supervised learning. It is
conceived as a tutorial organizing in a comprehensive framework current
approaches. We give insights on the rationales and limitations of the most
performant methods in the literature, and we highlight the common features
between many of them. Finally, we focus on on the normalization equivariance
properties that is surprisingly not guaranteed with most of supervised methods.
It is of paramount importance that intensity shifting or scaling applied to the
input image results in a corresponding change in the denoiser output.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15359" title="Abstract">arXiv:2402.15359</a> [<a href="/pdf/2402.15359" title="Download PDF">pdf</a>, <a href="/format/2402.15359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of  High Dimensional Categorical Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soucie%2C+J+E+S">J. E. San Soucie</a>, 
<a href="/search/cs?searchtype=author&query=Sosik%2C+H+M">H. M. Sosik</a>, 
<a href="/search/cs?searchtype=author&query=Girdhar%2C+Y">Y. Girdhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures. Published in Springer Proceedings of Advanced Robotics, ISER 2023 Conference Proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present the Streaming Gaussian Dirichlet Random Field (S-GDRF) model, a
novel approach for modeling a stream of spatiotemporally distributed, sparse,
high-dimensional categorical observations. The proposed approach efficiently
learns global and local patterns in spatiotemporal data, allowing for fast
inference and querying with a bounded time complexity. Using a high-resolution
data series of plankton images classified with a neural network, we demonstrate
the ability of the approach to make more accurate predictions compared to a
Variational Gaussian Process (VGP), and to learn a predictive distribution of
observations from streaming categorical data. S-GDRFs open the door to enabling
efficient informative path planning over high-dimensional categorical
observations, which until now has not been feasible.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15361" title="Abstract">arXiv:2402.15361</a> [<a href="/pdf/2402.15361" title="Download PDF">pdf</a>, <a href="/ps/2402.15361" title="Download PostScript">ps</a>, <a href="/format/2402.15361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A priori error estimates of Runge-Kutta discontinuous Galerkin schemes  to smooth solutions of fractional conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Leotta%2C+F">Fabio Leotta</a>, 
<a href="/search/math?searchtype=author&query=Giesselmann%2C+J">Jan Giesselmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We give a priori error estimates of second order in time fully explicit
Runge-Kutta discontinuous Galerkin schemes using upwind fluxes to smooth
solutions of scalar fractional conservation laws in one space dimension. Under
the time step restrictions $\tau\leq c h$ for piecewise linear and
$\tau\lesssim h^{4/3}$ for higher order finite elements, we prove a convergence
rate for the energy norm
$\|\cdot\|_{L^\infty_tL^2_x}+|\cdot|_{L^2_tH^{\lambda/2}_x}$ that is optimal
for solutions and flux functions that are smooth enough. Our proof relies on a
novel upwind projection of the exact solution.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15363" title="Abstract">arXiv:2402.15363</a> [<a href="/pdf/2402.15363" title="Download PDF">pdf</a>, <a href="/format/2402.15363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Follow the Footprints: Self-supervised Traversability Estimation for  Off-road Vehicle Navigation based on Geometric and Visual Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+Y">Yurim Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+E+I">E In Son</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Seung-Woo Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this study, we address the off-road traversability estimation problem,
that predicts areas where a robot can navigate in off-road environments. An
off-road environment is an unstructured environment comprising a combination of
traversable and non-traversable spaces, which presents a challenge for
estimating traversability. This study highlights three primary factors that
affect a robot's traversability in an off-road environment: surface slope,
semantic information, and robot platform. We present two strategies for
estimating traversability, using a guide filter network (GFN) and footprint
supervision module (FSM). The first strategy involves building a novel GFN
using a newly designed guide filter layer. The GFN interprets the surface and
semantic information from the input data and integrates them to extract
features optimized for traversability estimation. The second strategy involves
developing an FSM, which is a self-supervision module that utilizes the path
traversed by the robot in pre-driving, also known as a footprint. This enables
the prediction of traversability that reflects the characteristics of the robot
platform. Based on these two strategies, the proposed method overcomes the
limitations of existing methods, which require laborious human supervision and
lack scalability. Extensive experiments in diverse conditions, including
automobiles and unmanned ground vehicles, herbfields, woodlands, and farmlands,
demonstrate that the proposed method is compatible for various robot platforms
and adaptable to a range of terrains. Code is available at
https://github.com/yurimjeon1892/FtFoot.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15364" title="Abstract">arXiv:2402.15364</a> [<a href="/pdf/2402.15364" title="Download PDF">pdf</a>, <a href="/format/2402.15364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is a model equivalent to its computer implementation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hiesmayr%2C+B+C">Beatrix C. Hiesmayr</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCtt%2C+M">Marc-Thorsten H&#xfc;tt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">A recent trend in mathematical modeling is to publish the computer code
together with the research findings. Here we explore the formal question,
whether and in which sense a computer implementation is distinct from the
mathematical model. We argue that, despite the convenience of implemented
models, a set of implicit assumptions is perpetuated with the implementation to
the extent that even in widely used models the causal link between the (formal)
mathematical model and the set of results is no longer certain. Moreover, code
publication is often seen as an important contributor to reproducible research,
we suggest that in some cases the opposite may be true. A new perspective on
this topic stems from the accelerating trend that in some branches of research
only implemented models are used, e.g., in artificial intelligence (AI). With
the advent of quantum computers we argue that completely novel challenges arise
in the distinction between models and implementations.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15367" title="Abstract">arXiv:2402.15367</a> [<a href="/pdf/2402.15367" title="Download PDF">pdf</a>, <a href="/format/2402.15367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A CWENO large time-step scheme for Hamilton--Jacobi equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carlini%2C+E">E. Carlini</a>, 
<a href="/search/math?searchtype=author&query=Ferretti%2C+R">R. Ferretti</a>, 
<a href="/search/math?searchtype=author&query=Preda%2C+S">S. Preda</a>, 
<a href="/search/math?searchtype=author&query=Semplice%2C+M">M. Semplice</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose a high order numerical scheme for time-dependent first order
Hamilton--Jacobi--Bellman equations. In particular we propose to combine a
semi-Lagrangian scheme with a Central Weighted Non-Oscillatory reconstruction.
We prove a convergence result in the case of state- and time-independent
Hamiltonians.
<br />Numerical simulations are presented in space dimensions one and two, also for
more general state- and time-dependent Hamiltonians, demonstrating superior
performance in terms of CPU time gain compared with a semi-Lagrangian scheme
coupled with Weighted Non-Oscillatory reconstructions.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15368" title="Abstract">arXiv:2402.15368</a> [<a href="/pdf/2402.15368" title="Download PDF">pdf</a>, <a href="/format/2402.15368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Task Planning for Language-Instructed Multi-Robot Systems using  Conformal Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guocheng He</a>, 
<a href="/search/cs?searchtype=author&query=Kantaros%2C+Y">Yiannis Kantaros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper addresses task planning problems for language-instructed robot
teams. Tasks are expressed in natural language (NL), requiring the robots to
apply their capabilities (e.g., mobility, manipulation, and sensing) at various
locations and semantic objects. Several recent works have addressed similar
planning problems by leveraging pre-trained Large Language Models (LLMs) to
design effective multi-robot plans. However, these approaches lack mission
performance and safety guarantees. To address this challenge, we introduce a
new decentralized LLM-based planner that is capable of achieving high mission
success rates. This is accomplished by leveraging conformal prediction (CP), a
distribution-free uncertainty quantification tool in black-box models. CP
allows the proposed multi-robot planner to reason about its inherent
uncertainty in a decentralized fashion, enabling robots to make individual
decisions when they are sufficiently certain and seek help otherwise. We show,
both theoretically and empirically, that the proposed planner can achieve
user-specified task success rates while minimizing the overall number of help
requests. We demonstrate the performance of our approach on multi-robot home
service applications. We also show through comparative experiments, that our
method outperforms recent centralized and decentralized multi-robot LLM-based
planners in terms of in terms of its ability to design correct plans. The
advantage of our algorithm over baselines becomes more pronounced with
increasing mission complexity and robot team size.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15370" title="Abstract">arXiv:2402.15370</a> [<a href="/pdf/2402.15370" title="Download PDF">pdf</a>, <a href="/format/2402.15370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Encoder: Exploiting the Potential of Syntactic and Semantic for  Aspect Sentiment Triplet Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaowei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiujuan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained
sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to
model the syntax-semantic relationships inherent in triplet elements. However,
they have yet to fully tap into the vast potential of syntactic and semantic
information within the ASTE task. In this work, we propose a \emph{Dual
Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S),
which maximizes the syntactic and semantic relationships among words.
Specifically, our model utilizes a dual-channel encoder with a BERT channel to
capture semantic information, and an enhanced LSTM channel for comprehensive
syntactic information capture. Subsequently, we introduce the heterogeneous
feature interaction module to capture intricate interactions between dependency
syntax and attention semantics, and to dynamically select vital nodes. We
leverage the synergy of these modules to harness the significant potential of
syntactic and semantic information in ASTE tasks. Testing on public benchmarks,
our D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its
effectiveness.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15374" title="Abstract">arXiv:2402.15374</a> [<a href="/pdf/2402.15374" title="Download PDF">pdf</a>, <a href="/format/2402.15374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier detection by ensembling uncertainty with negative objectness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deli%C4%87%2C+A">Anja Deli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Grci%C4%87%2C+M">Matej Grci&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0egvi%C4%87%2C+S">Sini&#x161;a &#x160;egvi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Outlier detection is an essential capability in safety-critical applications
of supervised visual recognition. Most of the existing methods deliver best
results by encouraging standard closed-set models to produce low-confidence
predictions in negative training data. However, that approach conflates
prediction uncertainty with recognition of the negative class. We therefore
reconsider direct prediction of K+1 logits that correspond to K groundtruth
classes and one outlier class. This setup allows us to formulate a novel
anomaly score as an ensemble of in-distribution uncertainty and the posterior
of the outlier class which we term negative objectness. Now outliers can be
independently detected due to i) high prediction uncertainty or ii) similarity
with negative data. We embed our method into a dense prediction architecture
with mask-level recognition over K+2 classes. The training procedure encourages
the novel K+2-th class to learn negative objectness at pasted negative
instances. Our models outperform the current state-of-the art on standard
benchmarks for image-wide and pixel-level outlier detection with and without
training on real negative data.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15384" title="Abstract">arXiv:2402.15384</a> [<a href="/pdf/2402.15384" title="Download PDF">pdf</a>, <a href="/ps/2402.15384" title="Download PostScript">ps</a>, <a href="/format/2402.15384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homeostatic motion planning with innate physics knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lafratta%2C+G">Giulia Lafratta</a>, 
<a href="/search/cs?searchtype=author&query=Porr%2C+B">Bernd Porr</a>, 
<a href="/search/cs?searchtype=author&query=Chandler%2C+C">Christopher Chandler</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+A">Alice Miller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Living organisms interact with their surroundings in a closed-loop fashion,
where sensory inputs dictate the initiation and termination of behaviours. Even
simple animals are able to develop and execute complex plans, which has not yet
been replicated in robotics using pure closed-loop input control. We propose a
solution to this problem by defining a set of discrete and temporary
closed-loop controllers, called "tasks", each representing a closed-loop
behaviour. We further introduce a supervisory module which has an innate
understanding of physics and causality, through which it can simulate the
execution of task sequences over time and store the results in a model of the
environment. On the basis of this model, plans can be made by chaining
temporary closed-loop controllers. The proposed framework was implemented for a
real robot and tested in two scenarios as proof of concept.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15388" title="Abstract">arXiv:2402.15388</a> [<a href="/pdf/2402.15388" title="Download PDF">pdf</a>, <a href="/format/2402.15388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Usability of Next-Generation Authentication: A Study on Eye  Movement and Brainwave-based Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fallahi%2C+M">Matin Fallahi</a>, 
<a href="/search/cs?searchtype=author&query=Cabarcos%2C+P+A">Patricia Arias Cabarcos</a>, 
<a href="/search/cs?searchtype=author&query=Strufe%2C+T">Thorsten Strufe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Passwords remain a widely-used authentication mechanism, despite their
well-known security and usability limitations. To improve on this situation,
next-generation authentication mechanisms, based on behavioral biometric
factors such as eye movement and brainwave have emerged. However, their
usability remains relatively under-explored. To fill this gap, we conducted an
empirical user study (n=32 participants) to evaluate three brain-based and
three eye-based authentication mechanisms, using both qualitative and
quantitative methods. Our findings show good overall usability according to the
System Usability Scale for both categories of mechanisms, with average SUS
scores in the range of 78.6-79.6 and the best mechanisms rated with an
"excellent" score. Participants particularly identified brainwave
authentication as more secure yet more privacy-invasive and effort-intensive
compared to eye movement authentication. However, the significant number of
neutral responses indicates participants' need for more detailed information
about the security and privacy implications of these authentication methods.
Building on the collected evidence, we identify three key areas for
improvement: privacy, authentication interface design, and verification time.
We offer recommendations for designers and developers to improve the usability
and security of next-generation authentication mechanisms.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15390" title="Abstract">arXiv:2402.15390</a> [<a href="/pdf/2402.15390" title="Download PDF">pdf</a>, <a href="/format/2402.15390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explorations of Self-Repair in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rushing%2C+C">Cody Rushing</a>, 
<a href="/search/cs?searchtype=author&query=Nanda%2C+N">Neel Nanda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Prior interpretability research studying narrow distributions has
preliminarily identified self-repair, a phenomena where if components in large
language models are ablated, later components will change their behavior to
compensate. Our work builds off this past literature, demonstrating that
self-repair exists on a variety of models families and sizes when ablating
individual attention heads on the full training distribution. We further show
that on the full training distribution self-repair is imperfect, as the
original direct effect of the head is not fully restored, and noisy, since the
degree of self-repair varies significantly across different prompts (sometimes
overcorrecting beyond the original effect). We highlight two different
mechanisms that contribute to self-repair, including changes in the final
LayerNorm scaling factor (which can repair up to 30% of the direct effect) and
sparse sets of neurons implementing Anti-Erasure. We additionally discuss the
implications of these results for interpretability practitioners and close with
a more speculative discussion on the mystery of why self-repair occurs in these
models at all, highlighting evidence for the Iterative Inference hypothesis in
language models, a framework that predicts self-repair.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15391" title="Abstract">arXiv:2402.15391</a> [<a href="/pdf/2402.15391" title="Download PDF">pdf</a>, <a href="/format/2402.15391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Genie: Generative Interactive Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruce%2C+J">Jake Bruce</a>, 
<a href="/search/cs?searchtype=author&query=Dennis%2C+M">Michael Dennis</a>, 
<a href="/search/cs?searchtype=author&query=Edwards%2C+A">Ashley Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Parker-Holder%2C+J">Jack Parker-Holder</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuge Shi</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+E">Edward Hughes</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+M">Matthew Lai</a>, 
<a href="/search/cs?searchtype=author&query=Mavalankar%2C+A">Aditi Mavalankar</a>, 
<a href="/search/cs?searchtype=author&query=Steigerwald%2C+R">Richie Steigerwald</a>, 
<a href="/search/cs?searchtype=author&query=Apps%2C+C">Chris Apps</a>, 
<a href="/search/cs?searchtype=author&query=Aytar%2C+Y">Yusuf Aytar</a>, 
<a href="/search/cs?searchtype=author&query=Bechtle%2C+S">Sarah Bechtle</a>, 
<a href="/search/cs?searchtype=author&query=Behbahani%2C+F">Feryal Behbahani</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S">Stephanie Chan</a>, 
<a href="/search/cs?searchtype=author&query=Heess%2C+N">Nicolas Heess</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+L">Lucy Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Osindero%2C+S">Simon Osindero</a>, 
<a href="/search/cs?searchtype=author&query=Ozair%2C+S">Sherjil Ozair</a>, 
<a href="/search/cs?searchtype=author&query=Reed%2C+S">Scott Reed</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zolna%2C+K">Konrad Zolna</a>, 
<a href="/search/cs?searchtype=author&query=Clune%2C+J">Jeff Clune</a>, 
<a href="/search/cs?searchtype=author&query=de+Freitas%2C+N">Nando de Freitas</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Satinder Singh</a>, 
<a href="/search/cs?searchtype=author&query=Rockt%C3%A4schel%2C+T">Tim Rockt&#xe4;schel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://sites.google.com/corp/view/genie-2024/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We introduce Genie, the first generative interactive environment trained in
an unsupervised manner from unlabelled Internet videos. The model can be
prompted to generate an endless variety of action-controllable virtual worlds
described through text, synthetic images, photographs, and even sketches. At
11B parameters, Genie can be considered a foundation world model. It is
comprised of a spatiotemporal video tokenizer, an autoregressive dynamics
model, and a simple and scalable latent action model. Genie enables users to
act in the generated environments on a frame-by-frame basis despite training
without any ground-truth action labels or other domain-specific requirements
typically found in the world model literature. Further the resulting learned
latent action space facilitates training agents to imitate behaviors from
unseen videos, opening the path for training generalist agents of the future.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15392" title="Abstract">arXiv:2402.15392</a> [<a href="/pdf/2402.15392" title="Download PDF">pdf</a>, <a href="/ps/2402.15392" title="Download PostScript">ps</a>, <a href="/format/2402.15392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Inverse RL: New Solution Concepts and Provably Efficient  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazzati%2C+F">Filippo Lazzati</a>, 
<a href="/search/cs?searchtype=author&query=Mutti%2C+M">Mirco Mutti</a>, 
<a href="/search/cs?searchtype=author&query=Metelli%2C+A+M">Alberto Maria Metelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Inverse reinforcement learning (IRL) aims to recover the reward function of
an expert agent from demonstrations of behavior. It is well known that the IRL
problem is fundamentally ill-posed, i.e., many reward functions can explain the
demonstrations. For this reason, IRL has been recently reframed in terms of
estimating the feasible reward set, thus, postponing the selection of a single
reward. However, so far, the available formulations and algorithmic solutions
have been proposed and analyzed mainly for the online setting, where the
learner can interact with the environment and query the expert at will. This is
clearly unrealistic in most practical applications, where the availability of
an offline dataset is a much more common scenario. In this paper, we introduce
a novel notion of feasible reward set capturing the opportunities and
limitations of the offline setting and we analyze the complexity of its
estimation. This requires the introduction an original learning framework that
copes with the intrinsic difficulty of the setting, for which the data coverage
is not under control. Then, we propose two computationally and statistically
efficient algorithms, IRLO and PIRLO, for addressing the problem. In
particular, the latter adopts a specific form of pessimism to enforce the novel
desirable property of inclusion monotonicity of the delivered feasible set.
With this work, we aim to provide a panorama of the challenges of the offline
IRL problem and how they can be fruitfully addressed.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15393" title="Abstract">arXiv:2402.15393</a> [<a href="/pdf/2402.15393" title="Download PDF">pdf</a>, <a href="/format/2402.15393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esteves%2C+B">Bernardo Esteves</a>, 
<a href="/search/cs?searchtype=author&query=Vasco%2C+M">Miguel Vasco</a>, 
<a href="/search/cs?searchtype=author&query=Melo%2C+F+S">Francisco S. Melo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While machine learning methods excel at pattern recognition, they struggle
with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep
Thinking methods show promise in learning algorithms that extrapolate: learning
in smaller environments and executing the learned algorithm in larger
environments. However, these works are limited to symmetrical tasks, where the
input and output dimensionalities are the same. To address this gap, we propose
NeuralThink, a new recurrent architecture that can consistently extrapolate to
both symmetrical and asymmetrical tasks, where the dimensionality of the input
and output are different. We contribute with a novel benchmark of asymmetrical
tasks for extrapolation. We show that NeuralThink consistently outperforms the
prior state-of-the-art Deep Thinking architectures, in regards to stable
extrapolation to large observations from smaller training sizes.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15398" title="Abstract">arXiv:2402.15398</a> [<a href="/pdf/2402.15398" title="Download PDF">pdf</a>, <a href="/format/2402.15398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow  Attention for Commuting Flow Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhuoyue Wan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuzhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+G">Gengchen Mai</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+F">Fu-lai Chung</a>, 
<a href="/search/cs?searchtype=author&query=Larson%2C+K">Kent Larson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Understanding the link between urban planning and commuting flows is crucial
for guiding urban development and policymaking. This research, bridging
computer science and urban studies, addresses the challenge of integrating
these fields with their distinct focuses. Traditional urban studies methods,
like the gravity and radiation models, often underperform in complex scenarios
due to their limited handling of multiple variables and reliance on overly
simplistic and unrealistic assumptions, such as spatial isotropy. While deep
learning models offer improved accuracy, their black-box nature poses a
trade-off between performance and explainability -- both vital for analyzing
complex societal phenomena like commuting flows. To address this, we introduce
TransFlower, an explainable, transformer-based model employing flow-to-flow
attention to predict urban commuting patterns. It features a geospatial encoder
with an anisotropy-aware relative location encoder for nuanced flow
representation. Following this, the transformer-based flow predictor enhances
this by leveraging attention mechanisms to efficiently capture flow
interactions. Our model outperforms existing methods by up to 30.8% Common Part
of Commuters, offering insights into mobility dynamics crucial for urban
planning and policy decisions.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15399" title="Abstract">arXiv:2402.15399</a> [<a href="/pdf/2402.15399" title="Download PDF">pdf</a>, <a href="/format/2402.15399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Off-Dynamics Reinforcement Learning: Provable  Efficiency with Linear Function Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhishuai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 4 figures. To appear in the proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study off-dynamics Reinforcement Learning (RL), where the policy is
trained on a source domain and deployed to a distinct target domain. We aim to
solve this problem via online distributionally robust Markov decision processes
(DRMDPs), where the learning algorithm actively interacts with the source
domain while seeking the optimal performance under the worst possible dynamics
that is within an uncertainty set of the source domain's transition kernel. We
provide the first study on online DRMDPs with function approximation for
off-dynamics RL. We find that DRMDPs' dual formulation can induce nonlinearity,
even when the nominal transition kernel is linear, leading to error
propagation. By designing a $d$-rectangular uncertainty set using the total
variation distance, we remove this additional nonlinearity and bypass the error
propagation. We then introduce DR-LSVI-UCB, the first provably efficient online
DRMDP algorithm for off-dynamics RL with function approximation, and establish
a polynomial suboptimality bound that is independent of the state and action
space sizes. Our work makes the first step towards a deeper understanding of
the provable efficiency of online DRMDPs with linear function approximation.
Finally, we substantiate the performance and robustness of DR-LSVI-UCB through
different numerical experiments.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15400" title="Abstract">arXiv:2402.15400</a> [<a href="/pdf/2402.15400" title="Download PDF">pdf</a>, <a href="/format/2402.15400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithful Temporal Question Answering over Heterogeneous Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+Z">Zhen Jia</a>, 
<a href="/search/cs?searchtype=author&query=Christmann%2C+P">Philipp Christmann</a>, 
<a href="/search/cs?searchtype=author&query=Weikum%2C+G">Gerhard Weikum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Temporal question answering (QA) involves time constraints, with phrases such
as "... in 2019" or "... before COVID". In the former, time is an explicit
condition, in the latter it is implicit. State-of-the-art methods have
limitations along three dimensions. First, with neural inference, time
constraints are merely soft-matched, giving room to invalid or inexplicable
answers. Second, questions with implicit time are poorly supported. Third,
answers come from a single source: either a knowledge base (KB) or a text
corpus. We propose a temporal QA system that addresses these shortcomings.
First, it enforces temporal constraints for faithful answering with tangible
evidence. Second, it properly handles implicit questions. Third, it operates
over heterogeneous sources, covering KB, text and web tables in a unified
manner. The method has three stages: (i) understanding the question and its
temporal conditions, (ii) retrieving evidence from all sources, and (iii)
faithfully answering the question. As implicit questions are sparse in prior
benchmarks, we introduce a principled method for generating diverse questions.
Experiments show superior performance over a suite of baselines.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15402" title="Abstract">arXiv:2402.15402</a> [<a href="/pdf/2402.15402" title="Download PDF">pdf</a>, <a href="/format/2402.15402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy  Structure Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kechun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhongxiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haojian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+R">Rong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We focus on the task of unknown object rearrangement, where a robot is
supposed to re-configure the objects into a desired goal configuration
specified by an RGB-D image. Recent works explore unknown object rearrangement
systems by incorporating learning-based perception modules. However, they are
sensitive to perception error, and pay less attention to task-level
performance. In this paper, we aim to develop an effective system for unknown
object rearrangement amidst perception noise. We theoretically reveal the noisy
perception impacts grasp and place in a decoupled way, and show such a
decoupled structure is non-trivial to improve task optimality. We propose GSP,
a dual-loop system with the decoupled structure as prior. For the inner loop,
we learn an active seeing policy for self-confident object matching to improve
the perception of place. For the outer loop, we learn a grasp policy aware of
object matching and grasp capability guided by task-level rewards. We leverage
the foundation model CLIP for object matching, policy learning and
self-termination. A series of experiments indicate that GSP can conduct unknown
object rearrangement with higher completion rate and less steps.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15404" title="Abstract">arXiv:2402.15404</a> [<a href="/pdf/2402.15404" title="Download PDF">pdf</a>, <a href="/format/2402.15404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> United We Pretrain, Divided We Fail! Representation Learning for Time  Series by Pretraining on 75 Datasets at Once
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kraus%2C+M">Maurice Kraus</a>, 
<a href="/search/cs?searchtype=author&query=Divo%2C+F">Felix Divo</a>, 
<a href="/search/cs?searchtype=author&query=Steinmann%2C+D">David Steinmann</a>, 
<a href="/search/cs?searchtype=author&query=Dhami%2C+D+S">Devendra Singh Dhami</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In natural language processing and vision, pretraining is utilized to learn
effective representations. Unfortunately, the success of pretraining does not
easily carry over to time series due to potential mismatch between sources and
target. Actually, common belief is that multi-dataset pretraining does not work
for time series! Au contraire, we introduce a new self-supervised contrastive
pretraining approach to learn one encoding from many unlabeled and diverse time
series datasets, so that the single learned representation can then be reused
in several target domains for, say, classification. Specifically, we propose
the XD-MixUp interpolation method and the Soft Interpolation Contextual
Contrasting (SICC) loss. Empirically, this outperforms both supervised training
and other self-supervised pretraining methods when finetuning on low-data
regimes. This disproves the common belief: We can actually learn from multiple
time series datasets, even from 75 at once.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15406" title="Abstract">arXiv:2402.15406</a> [<a href="/pdf/2402.15406" title="Download PDF">pdf</a>, <a href="/ps/2402.15406" title="Download PostScript">ps</a>, <a href="/format/2402.15406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformalized-DeepONet: A Distribution-Free Framework for Uncertainty  Quantification in Deep Operator Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moya%2C+C">Christian Moya</a>, 
<a href="/search/cs?searchtype=author&query=Mollaali%2C+A">Amirhossein Mollaali</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zecheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+G">Guang Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper, we adopt conformal prediction, a distribution-free uncertainty
quantification (UQ) framework, to obtain confidence prediction intervals with
coverage guarantees for Deep Operator Network (DeepONet) regression. Initially,
we enhance the uncertainty quantification frameworks (B-DeepONet and
Prob-DeepONet) previously proposed by the authors by using split conformal
prediction. By combining conformal prediction with our Prob- and B-DeepONets,
we effectively quantify uncertainty by generating rigorous confidence intervals
for DeepONet prediction. Additionally, we design a novel Quantile-DeepONet that
allows for a more natural use of split conformal prediction. We refer to this
distribution-free effective uncertainty quantification framework as split
conformal Quantile-DeepONet regression. Finally, we demonstrate the
effectiveness of the proposed methods using various ordinary, partial
differential equation numerical examples, and multi-fidelity learning.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15411" title="Abstract">arXiv:2402.15411</a> [<a href="/pdf/2402.15411" title="Download PDF">pdf</a>, <a href="/ps/2402.15411" title="Download PostScript">ps</a>, <a href="/format/2402.15411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimisic Information Directed Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neu%2C+G">Gergely Neu</a>, 
<a href="/search/cs?searchtype=author&query=Papini%2C+M">Matteo Papini</a>, 
<a href="/search/cs?searchtype=author&query=Schwartz%2C+L">Ludovic Schwartz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the problem of online learning in contextual bandit problems where
the loss function is assumed to belong to a known parametric function class. We
propose a new analytic framework for this setting that bridges the Bayesian
theory of information-directed sampling due to Russo and Van Roy (2018) and the
worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the
decision-estimation coefficient. Drawing from both lines of work, we propose a
algorithmic template called Optimistic Information-Directed Sampling and show
that it can achieve instance-dependent regret guarantees similar to the ones
achievable by the classic Bayesian IDS method, but with the major advantage of
not requiring any Bayesian assumptions. The key technical innovation of our
analysis is introducing an optimistic surrogate model for the regret and using
it to define a frequentist version of the Information Ratio of Russo and Van
Roy (2018), and a less conservative version of the Decision Estimation
Coefficient of Foster et al. (2021). Keywords: Contextual bandits,
information-directed sampling, decision estimation coefficient, first-order
regret bounds.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15413" title="Abstract">arXiv:2402.15413</a> [<a href="/pdf/2402.15413" title="Download PDF">pdf</a>, <a href="/format/2402.15413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G-RepsNet: A Fast and General Construction of Equivariant Networks for  Arbitrary Matrix Groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Sourya Basu</a>, 
<a href="/search/cs?searchtype=author&query=Lohit%2C+S">Suhas Lohit</a>, 
<a href="/search/cs?searchtype=author&query=Brand%2C+M">Matthew Brand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Group equivariance is a strong inductive bias useful in a wide range of deep
learning tasks. However, constructing efficient equivariant networks for
general groups and domains is difficult. Recent work by Finzi et al. (2021)
directly solves the equivariance constraint for arbitrary matrix groups to
obtain equivariant MLPs (EMLPs). But this method does not scale well and
scaling is crucial in deep learning. Here, we introduce Group Representation
Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix
groups with features represented using tensor polynomials. The key intuition
for our design is that using tensor representations in the hidden layers of a
neural network along with simple inexpensive tensor operations can lead to
expressive universal equivariant networks. We find G-RepsNet to be competitive
to EMLP on several tasks with group symmetries such as O(5), O(1, 3), and O(3)
with scalars, vectors, and second-order tensors as data types. On image
classification tasks, we find that G-RepsNet using second-order representations
is competitive and often even outperforms sophisticated state-of-the-art
equivariant models such as GCNNs (Cohen &amp; Welling, 2016a) and E(2)-CNNs (Weiler
&amp; Cesa, 2019). To further illustrate the generality of our approach, we show
that G-RepsNet is competitive to G-FNO (Helwig et al., 2023) and EGNN (Satorras
et al., 2021) on N-body predictions and solving PDEs, respectively, while being
efficient.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15414" title="Abstract">arXiv:2402.15414</a> [<a href="/pdf/2402.15414" title="Download PDF">pdf</a>, <a href="/format/2402.15414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Combining Parameter-efficient Modules Improve Few-shot Transfer  Accuracy?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asadi%2C+N">Nader Asadi</a>, 
<a href="/search/cs?searchtype=author&query=Beitollahi%2C+M">Mahdi Beitollahi</a>, 
<a href="/search/cs?searchtype=author&query=Khalil%2C+Y">Yasser Khalil</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinchuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guojun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Parameter-efficient fine-tuning stands as the standard for efficiently
fine-tuning large language and vision models on downstream tasks. Specifically,
the efficiency of low-rank adaptation has facilitated the creation and sharing
of hundreds of custom LoRA modules, each trained on distinct data from various
downstream tasks. In this paper, we explore the composability of LoRA modules,
examining if combining these pre-trained modules enhances generalization to
unseen downstream tasks. Our investigation involves evaluating two approaches:
(a) uniform composition, involving averaging upstream LoRA modules with equal
weights, and (b) learned composition, where we learn the weights for each
upstream module and perform weighted averaging. Our experimental results on
both vision and language models reveal that in few-shot settings, where only a
limited number of samples are available for the downstream task, both uniform
and learned composition methods result in better transfer accuracy;
outperforming full fine-tuning and training a LoRA from scratch. Moreover, in
full-shot settings, learned composition performs comparably to regular LoRA
training with significantly fewer number of trainable parameters. Our research
unveils the potential of uniform composition for enhancing transferability in
low-shot settings, without introducing additional learnable parameters.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15415" title="Abstract">arXiv:2402.15415</a> [<a href="/pdf/2402.15415" title="Download PDF">pdf</a>, <a href="/format/2402.15415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of LoRA on the Emergence of Clusters in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koubbi%2C+H">Hugo Koubbi</a>, 
<a href="/search/cs?searchtype=author&query=Boussard%2C+M">Matthieu Boussard</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+L">Louis Hernandez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we employ the mathematical framework on Transformers developed
by
\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical}
to explore how variations in attention parameters and initial token values
impact the structural dynamics of token clusters. Our analysis demonstrates
that while the clusters within a modified attention matrix dynamics can exhibit
significant divergence from the original over extended periods, they maintain
close similarities over shorter intervals, depending on the parameter
differences. This work contributes to the fine-tuning field through practical
applications to the LoRA algorithm \cite{hu2021lora,peft}, enhancing our
understanding of the behavior of LoRA-enhanced Transformer models.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15420" title="Abstract">arXiv:2402.15420</a> [<a href="/pdf/2402.15420" title="Download PDF">pdf</a>, <a href="/format/2402.15420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PREDILECT: Preferences Delineated with Zero-Shot Language-based  Reasoning in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holk%2C+S">Simon Holk</a>, 
<a href="/search/cs?searchtype=author&query=Marta%2C+D">Daniel Marta</a>, 
<a href="/search/cs?searchtype=author&query=Leite%2C+I">Iolanda Leite</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 Figures, 2 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Preference-based reinforcement learning (RL) has emerged as a new field in
robot learning, where humans play a pivotal role in shaping robot behavior by
expressing preferences on different sequences of state-action pairs. However,
formulating realistic policies for robots demands responses from humans to an
extensive array of queries. In this work, we approach the sample-efficiency
challenge by expanding the information collected per query to contain both
preferences and optional text prompting. To accomplish this, we leverage the
zero-shot capabilities of a large language model (LLM) to reason from the text
provided by humans. To accommodate the additional query information, we
reformulate the reward learning objectives to contain flexible highlights --
state-action pairs that contain relatively high information and are related to
the features processed in a zero-shot fashion from a pretrained LLM. In both a
simulated scenario and a user study, we reveal the effectiveness of our work by
analyzing the feedback and its implications. Additionally, the collective
feedback collected serves to train a robot on socially compliant trajectories
in a simulated social navigation landscape. We provide video examples of the
trained policies at https://sites.google.com/view/rl-predilect
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15422" title="Abstract">arXiv:2402.15422</a> [<a href="/pdf/2402.15422" title="Download PDF">pdf</a>, <a href="/format/2402.15422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Centric Approach To Generate Faithful and High Quality Patient  Summaries with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hegselmann%2C+S">Stefan Hegselmann</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S+Z">Shannon Zejiang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Gierse%2C+F">Florian Gierse</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+M">Monica Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Sontag%2C+D">David Sontag</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoyi Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Patients often face difficulties in understanding their hospitalizations,
while healthcare workers have limited resources to provide explanations. In
this work, we investigate the potential of large language models to generate
patient summaries based on doctors' notes and study the effect of training data
on the faithfulness and quality of the generated summaries. To this end, we
develop a rigorous labeling protocol for hallucinations, and have two medical
experts annotate 100 real-world summaries and 100 generated summaries. We show
that fine-tuning on hallucination-free data effectively reduces hallucinations
from 2.60 to 1.55 per summary for Llama 2, while preserving relevant
information. Although the effect is still present, it is much smaller for GPT-4
when prompted with five examples (0.70 to 0.40). We also conduct a qualitative
evaluation using hallucination-free and improved training data. GPT-4 shows
very good results even in the zero-shot setting. We find that common
quantitative metrics do not correlate well with faithfulness and quality.
Finally, we test GPT-4 for automatic hallucination detection, which yields
promising results.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15423" title="Abstract">arXiv:2402.15423</a> [<a href="/pdf/2402.15423" title="Download PDF">pdf</a>, <a href="/format/2402.15423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis of Systems with Coupled and Decoupled RISs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Semmler%2C+D">Dominik Semmler</a>, 
<a href="/search/cs?searchtype=author&query=Nossek%2C+J+A">Josef A. Nossek</a>, 
<a href="/search/cs?searchtype=author&query=Joham%2C+M">Michael Joham</a>, 
<a href="/search/cs?searchtype=author&query=Utschick%2C+W">Wolfgang Utschick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We analyze and compare different methods for handling the mutual coupling in
RIS-aided communication systems. A new mutual coupling aware algorithm is
derived where the reactance of each element is updated successively with a
closed-form solution. In comparison to existing element-wise methods, this
approach leads to a considerably reduced computational complexity. Furthermore,
we introduce decoupling networks for the RIS array as a potential solution for
handling mutual coupling. With these networks, the system model reduces to the
same structure as when no mutual coupling were present. Including decoupling
networks, we can optimize the channel gain of a RIS-aided SISO system in
closed-form which allows to analyze the scenario under mutual coupling
analytically and to draw connections to the conventional transmit array gain.
In particular, a super-quadratic channel gain can be achieved which scales as
N^4 where N is the number of RIS elements.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15425" title="Abstract">arXiv:2402.15425</a> [<a href="/pdf/2402.15425" title="Download PDF">pdf</a>, <a href="/format/2402.15425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prime+Retouch: When Cache is Locked and Leaked
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaehyuk Lee</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+F">Fan Sang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taesoo Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Caches on the modern commodity CPUs have become one of the major sources of
side-channel leakages and been abused as a new attack vector. To thwart the
cache-based side-channel attacks, two types of countermeasures have been
proposed: detection-based ones that limit the amount of microarchitectural
traces an attacker can leave, and cache prefetching-and-locking techniques that
claim to prevent such leakage by disallowing evictions on sensitive data. In
this paper, we present the Prime+Retouch attack that completely bypasses these
defense schemes by accurately inferring the cache activities with the metadata
of the cache replacement policy. Prime+Retouch has three noticeable properties:
1) it incurs no eviction on the victim's data, allowing us to bypass the two
known mitigation schemes, 2) it requires minimal synchronization of only one
memory access to the attacker's pre-primed cache lines, and 3) it leaks data
via non-shared memory, yet because underlying eviction metadata is shared.
<br />We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and
emerging Apple M1. We elucidate how Prime+Retouch can break the T-table
implementation of AES with robust cache side-channel mitigations such as Cloak,
under both normal and SGX-protected environments. We also manifest feasibility
of the Prime+Retouch attack on the M1 platform imposing more restrictions where
the precise measurement tools such as core clock cycle timer and performance
counters are inaccessible to the attacker. Furthermore, we first demystify
undisclosed cache architecture and its eviction policy of L1 data cache on
Apple M1 architecture. We also devise a user-space noise-free cache monitoring
tool by repurposing Intel TSX.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15426" title="Abstract">arXiv:2402.15426</a> [<a href="/pdf/2402.15426" title="Download PDF">pdf</a>, <a href="/ps/2402.15426" title="Download PostScript">ps</a>, <a href="/format/2402.15426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mapping Literacies in the Tourism Labor Market: A Cross-Database  Comparison
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leyva%2C+E+S">Eddy Soria Leyva</a>, 
<a href="/search/cs?searchtype=author&query=Lara%2C+A+B+H">Ana Beatriz Hernandez Lara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">This book chapter conducts a comparative bibliometric analysis of literacies
in the tourism labor market, drawing from the Web of Science (WoS) and Scopus
databases. The objective is to assess scientific outputs and identify key
patterns of scientific collaboration. Findings suggest a statistically
significant difference between the two databases with an overlap level of
35.71%. However, there is a gradual and correlated increase in the number of
publications over time. Scopus stands out for its broader impact and enduring
citation relevance, suggesting its academic contributions have a longer-lasting
effect. Conversely, WoS is characterized by a focus on more recent influential
publications and exhibits a marginally more intense collaboration network.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15427" title="Abstract">arXiv:2402.15427</a> [<a href="/pdf/2402.15427" title="Download PDF">pdf</a>, <a href="/format/2402.15427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Entrainment in Human Groups: Optimising Human-Robot  Collaboration from Lessons Learned during Human-Human Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneiders%2C+E">Eike Schneiders</a>, 
<a href="/search/cs?searchtype=author&query=Fourie%2C+C">Christopher Fourie</a>, 
<a href="/search/cs?searchtype=author&query=Celestin%2C+S">Stanley Celestin</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+J">Julie Shah</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+M">Malte Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Successful entrainment during collaboration positively affects trust,
willingness to collaborate, and likeability towards collaborators. In this
paper, we present a mixed-method study to investigate characteristics of
successful entrainment leading to pair and group-based synchronisation. Drawing
inspiration from industrial settings, we designed a fast-paced, short-cycle
repetitive task. Using motion tracking, we investigated entrainment in both
dyadic and triadic task completion. Furthermore, we utilise audio-video
recordings and semi-structured interviews to contextualise participants'
experiences. This paper contributes to the Human-Computer/Robot Interaction
(HCI/HRI) literature using a human-centred approach to identify characteristics
of entrainment during pair- and group-based collaboration. We present five
characteristics related to successful entrainment. These are related to the
occurrence of entrainment, leader-follower patterns, interpersonal
communication, the importance of the point-of-assembly, and the value of
acoustic feedback. Finally, we present three design considerations for future
research and design on collaboration with robots.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15429" title="Abstract">arXiv:2402.15429</a> [<a href="/pdf/2402.15429" title="Download PDF">pdf</a>, <a href="/format/2402.15429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion  Models against Stochastic Perturbation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yun Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+W">Wenjie Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaowei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Khastgir%2C+S">Siddartha Khastgir</a>, 
<a href="/search/cs?searchtype=author&query=Jennings%2C+P">Paul Jennings</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xingyu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in
generating high-quality images based on simple text descriptions. However, as
is common with many Deep Learning (DL) models, DMs are subject to a lack of
robustness. While there are attempts to evaluate the robustness of T2I DMs as a
binary or worst-case problem, they cannot answer how robust in general the
model is whenever an adversarial example (AE) can be found. In this study, we
first introduce a probabilistic notion of T2I DMs' robustness; and then
establish an efficient framework, ProTIP, to evaluate it with statistical
guarantees. The main challenges stem from: i) the high computational cost of
the generation process; and ii) determining if a perturbed input is an AE
involves comparing two output distributions, which is fundamentally harder
compared to other DL tasks like classification where an AE is identified upon
misprediction of labels. To tackle the challenges, we employ sequential
analysis with efficacy and futility early stopping rules in the statistical
testing for identifying AEs, and adaptive concentration inequalities to
dynamically determine the "just-right" number of stochastic perturbations
whenever the verification target is met. Empirical experiments validate the
effectiveness and efficiency of ProTIP over common T2I DMs. Finally, we
demonstrate an application of ProTIP to rank commonly used defence methods.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15430" title="Abstract">arXiv:2402.15430</a> [<a href="/pdf/2402.15430" title="Download PDF">pdf</a>, <a href="/format/2402.15430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Invariance for Robust and Interpretable Vision Tasks at  Larger Scales
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+S">Shuren Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yushu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Z">Zhihua Xia</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+J">Jian Weng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Developing robust and interpretable vision systems is a crucial step towards
trustworthy artificial intelligence. In this regard, a promising paradigm
considers embedding task-required invariant structures, e.g., geometric
invariance, in the fundamental image representation. However, such invariant
representations typically exhibit limited discriminability, limiting their
applications in larger-scale trustworthy vision tasks. For this open problem,
we conduct a systematic investigation of hierarchical invariance, exploring
this topic from theoretical, practical, and application perspectives. At the
theoretical level, we show how to construct over-complete invariants with a
Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a
fully interpretable manner. The general blueprint, specific definitions,
invariant properties, and numerical implementations are provided. At the
practical level, we discuss how to customize this theoretical framework into a
given task. With the over-completeness, discriminative features w.r.t. the task
can be adaptively formed in a Neural Architecture Search (NAS)-like manner. We
demonstrate the above arguments with accuracy, invariance, and efficiency
results on texture, digit, and parasite classification experiments.
Furthermore, at the application level, our representations are explored in
real-world forensics tasks on adversarial perturbations and Artificial
Intelligence Generated Content (AIGC). Such applications reveal that the
proposed strategy not only realizes the theoretically promised invariance, but
also exhibits competitive discriminability even in the era of deep learning.
For robust and interpretable vision tasks at larger scales, hierarchical
invariant representation can be considered as an effective alternative to
traditional CNN and invariants.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15431" title="Abstract">arXiv:2402.15431</a> [<a href="/pdf/2402.15431" title="Download PDF">pdf</a>, <a href="/format/2402.15431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing Multispecies Worlds for Robots, Cats, and Humans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneiders%2C+E">Eike Schneiders</a>, 
<a href="/search/cs?searchtype=author&query=Benford%2C+S">Steve Benford</a>, 
<a href="/search/cs?searchtype=author&query=Chamberlain%2C+A">Alan Chamberlain</a>, 
<a href="/search/cs?searchtype=author&query=Mancini%2C+C">Clara Mancini</a>, 
<a href="/search/cs?searchtype=author&query=Castle-Green%2C+S">Simon Castle-Green</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+V">Victor Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Farr%2C+J+R">Ju Row Farr</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+M">Matt Adams</a>, 
<a href="/search/cs?searchtype=author&query=Tandavanitj%2C+N">Nick Tandavanitj</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+J">Joel Fischer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">We reflect on the design of a multispecies world centred around a bespoke
enclosure in which three cats and a robot arm coexist for six hours a day
during a twelve-day installation as part of an artist-led project. In this
paper, we present the project's design process, encompassing various
interconnected components, including the cats, the robot and its autonomous
systems, the custom end-effectors and robot attachments, the diverse roles of
the humans-in-the-loop, and the custom-designed enclosure. Subsequently, we
provide a detailed account of key moments during the deployment and discuss the
design implications for future multispecies systems. Specifically, we argue
that designing the technology and its interactions is not sufficient, but that
it is equally important to consider the design of the `world' in which the
technology operates. Finally, we highlight the necessity of human involvement
in areas such as breakdown recovery, animal welfare, and their role as
audience.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15434" title="Abstract">arXiv:2402.15434</a> [<a href="/pdf/2402.15434" title="Download PDF">pdf</a>, <a href="/ps/2402.15434" title="Download PostScript">ps</a>, <a href="/format/2402.15434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding the Pulse of Community during Disasters: Resilience Analysis  Based on Fluctuations in Latent Lifestyle Signatures within Human Visitation  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Junwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mostafavi%2C+A">Ali Mostafavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph); Applications (stat.AP)

</div>
<p class="mathjax">Examining the impact of disasters on life activities of populations is
critical for understanding community resilience dynamics, yet it remains
insufficiently studied in the existing literature. In this study, we leveraged
data from more than 1.2 million anonymized human mobility communications across
30 parishes in Louisiana to construct a temporal network that tracks visitation
to places from which we characterized human lifestyle signatures before,
during, and after Hurricane Ida in 2021. Utilizing the motif model, we
distilled complex human lifestyles into identifiable patterns and clustered
them into classes: commute, healthcare, dining out, and youth-oriented
lifestyle. We defined two metrics to evaluate disruption and recovery
fluctuations in lifestyle patterns during the perturbation period compared to
the steady period: 1) frequency (daily number of motifs), and 2) proximity
(daily average distance of motifs). The results indicate significant dynamics
in lifestyle patterns due to the hurricane, with essential facilities (e.g.,
healthcare) demonstrating a swift recovery. The study underscores the
heterogeneity of locations visited and the necessity of integrating both
essential and non-essential facilities into disaster response initiatives.
Furthermore, our study reveals sustained changes in lifestyle patterns,
highlighting the long-term impact of the hurricane on daily life. These
insights demonstrate the significance of examining lifestyle signatures and
their fluctuations in evaluating disaster resilience patterns for affected
communities. The outcomes of this study are poised to aid emergency managers
and public officials to more effectively evaluate and monitor disaster impacts
and recovery based on changes in lifestyle patterns in the community.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15437" title="Abstract">arXiv:2402.15437</a> [<a href="/pdf/2402.15437" title="Download PDF">pdf</a>, <a href="/format/2402.15437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GQL-Based Bound-Preserving and Locally Divergence-Free Central  Discontinuous Galerkin Schemes for Relativistic Magnetohydrodynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ding%2C+S">Shengrong Ding</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+K">Kailiang Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Computational Physics (physics.comp-ph); Plasma Physics (physics.plasm-ph)

</div>
<p class="mathjax">This paper develops novel and robust central discontinuous Galerkin (CDG)
schemes of arbitrarily high-order accuracy for special relativistic
magnetohydrodynamics (RMHD) with a general equation of state (EOS). These
schemes are provably bound-preserving (BP), i.e., consistently preserve the
upper bound for subluminal fluid velocity and the positivity of density and
pressure, while also (locally) maintaining the divergence-free (DF) constraint
for the magnetic field. For 1D RMHD, the standard CDG method is exactly DF, and
its BP property is proven under a condition achievable by BP limiter. For 2D
RMHD, we design provably BP and locally DF CDG schemes based on the suitable
discretization of a modified RMHD system. A key novelty in our schemes is the
discretization of additional source terms in the modified RMHD equations, so as
to precisely counteract the influence of divergence errors on the BP property
across overlapping meshes. We provide rigorous proofs of the BP property for
our CDG schemes and first establish the theoretical connection between BP and
discrete DF properties on overlapping meshes for RMHD. Owing to the absence of
explicit expressions for primitive variables in terms of conserved variables,
the constraints of physical bounds are strongly nonlinear, making the BP proofs
highly nontrivial. We overcome these challenges through technical estimates
within the geometric quasilinearization (GQL) framework, which converts the
nonlinear constraints into linear ones. Furthermore, we introduce a new 2D cell
average decomposition on overlapping meshes, which relaxes the theoretical BP
CFL constraint and reduces the number of internal nodes, thereby enhancing the
efficiency of the 2D BP CDG method. We implement the proposed CDG schemes for
extensive RMHD problems with various EOSs, demonstrating their robustness and
effectiveness in challenging scenarios.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15439" title="Abstract">arXiv:2402.15439</a> [<a href="/pdf/2402.15439" title="Download PDF">pdf</a>, <a href="/format/2402.15439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Charting Ethical Tensions in Multispecies Technology Research through  Beneficiary-Epistemology Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benford%2C+S">Steve Benford</a>, 
<a href="/search/cs?searchtype=author&query=Mancini%2C+C">Clara Mancini</a>, 
<a href="/search/cs?searchtype=author&query=Chamberlain%2C+A">Alan Chamberlain</a>, 
<a href="/search/cs?searchtype=author&query=Schneiders%2C+E">Eike Schneiders</a>, 
<a href="/search/cs?searchtype=author&query=Castle-Green%2C+S">Simon Castle-Green</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+J">Joel Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Kucukyilmaz%2C+A">Ayse Kucukyilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Salimbeni%2C+G">Guido Salimbeni</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+V">Victor Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Barnard%2C+P">Pepita Barnard</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+M">Matt Adams</a>, 
<a href="/search/cs?searchtype=author&query=Tandavanitj%2C+N">Nick Tandavanitj</a>, 
<a href="/search/cs?searchtype=author&query=Farr%2C+J+R">Ju Row Farr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">While ethical challenges are widely discussed in HCI, far less is reported
about the ethical processes that researchers routinely navigate. We reflect on
a multispecies project that negotiated an especially complex ethical approval
process. Cat Royale was an artist-led exploration of creating an artwork to
engage audiences in exploring trust in autonomous systems. The artwork took the
form of a robot that played with three cats. Gaining ethical approval required
an extensive dialogue with three Institutional Review Boards (IRBs) covering
computer science, veterinary science and animal welfare, raising tensions
around the welfare of the cats, perceived benefits and appropriate methods, and
reputational risk to the University. To reveal these tensions we introduce
beneficiary-epistemology space, that makes explicit who benefits from research
(humans or animals) and underlying epistemologies. Positioning projects and
IRBs in this space can help clarify tensions and highlight opportunities to
recruit additional expertise.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15441" title="Abstract">arXiv:2402.15441</a> [<a href="/pdf/2402.15441" title="Download PDF">pdf</a>, <a href="/format/2402.15441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Few-Shot Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%BCbotter%2C+J">Jonas H&#xfc;botter</a>, 
<a href="/search/cs?searchtype=author&query=Sukhija%2C+B">Bhavya Sukhija</a>, 
<a href="/search/cs?searchtype=author&query=Treven%2C+L">Lenart Treven</a>, 
<a href="/search/cs?searchtype=author&query=As%2C+Y">Yarden As</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We study the active few-shot fine-tuning of large neural networks to
downstream tasks. We show that few-shot fine-tuning is an instance of a
generalization of classical active learning, transductive active learning, and
we propose ITL, short for information-based transductive learning, an approach
which samples adaptively to maximize the information gained about specified
downstream tasks. Under general regularity assumptions, we prove that ITL
converges uniformly to the smallest possible uncertainty obtainable from the
accessible data. To the best of our knowledge, we are the first to derive
generalization bounds of this kind, and they may be of independent interest for
active learning. We apply ITL to the few-shot fine-tuning of large neural
networks and show that ITL substantially improves upon the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15444" title="Abstract">arXiv:2402.15444</a> [<a href="/pdf/2402.15444" title="Download PDF">pdf</a>, <a href="/format/2402.15444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Power of Imbalanced Modality Information for Multi-modal  Knowledge Graph Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Lei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by LREC-COLING 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Multi-modal knowledge graph completion (MMKGC) aims to predict the missing
triples in the multi-modal knowledge graphs by incorporating structural,
visual, and textual information of entities into the discriminant models. The
information from different modalities will work together to measure the triple
plausibility. Existing MMKGC methods overlook the imbalance problem of modality
information among entities, resulting in inadequate modal fusion and
inefficient utilization of the raw modality information. To address the
mentioned problems, we propose Adaptive Multi-modal Fusion and Modality
Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality
information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive
modality weights and further generates adversarial samples by
modality-adversarial training to enhance the imbalanced modality information.
Our approach is a co-design of the MMKGC model and training strategy which can
outperform 19 recent MMKGC methods and achieve new state-of-the-art results on
three public MMKGC benchmarks. Our code and data have been released at
https://github.com/zjukg/AdaMF-MAT.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15445" title="Abstract">arXiv:2402.15445</a> [<a href="/pdf/2402.15445" title="Download PDF">pdf</a>, <a href="/ps/2402.15445" title="Download PostScript">ps</a>, <a href="/format/2402.15445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can we forget how we learned? Doxastic redundancy in iterated belief  revision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liberatore%2C+P">Paolo Liberatore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> formerly part of <a href="/abs/2305.09200">arXiv:2305.09200</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">How information was acquired may become irrelevant. An obvious case is when
something is confirmed many times. In terms of iterated belief revision, a
specific revision may become irrelevant in presence of others. Simple
repetitions are an example, but not the only case when this happens. Sometimes,
a revision becomes redundant even in presence of none equal, or even no else
implying it. A necessary and sufficient condition for the redundancy of the
first of a sequence of lexicographic revisions is given. The problem is
coNP-complete even with two propositional revisions only. Complexity is the
same in the Horn case but only with an unbounded number of revisions: it
becomes polynomial with two revisions. Lexicographic revisions are not only
relevant by themselves, but also because sequences of them are the most compact
of the common mechanisms used to represent the state of an iterated revision
process. Shortening sequences of lexicographic revisions is shortening the most
compact representations of iterated belief revision states.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15446" title="Abstract">arXiv:2402.15446</a> [<a href="/pdf/2402.15446" title="Download PDF">pdf</a>, <a href="/format/2402.15446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-order accurate positivity-preserving and well-balanced  discontinuous Galerkin schemes for ten-moment Gaussian closure equations with  source terms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+J">Jiangfu Wang</a>, 
<a href="/search/math?searchtype=author&query=Tang%2C+H">Huazhong Tang</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+K">Kailiang Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">This paper proposes novel high-order accurate discontinuous Galerkin (DG)
schemes for the one- and two-dimensional ten-moment Gaussian closure equations
with source terms defined by a known potential function. Our DG schemes exhibit
the desirable capability of being well-balanced (WB) for a known hydrostatic
equilibrium state while simultaneously preserving positive density and
positive-definite anisotropic pressure tensor. The well-balancedness is built
on carefully modifying the solution states in the Harten-Lax-van Leer-contact
(HLLC) flux, and appropriate reformulation and discretization of the source
terms. Our novel modification technique overcomes the difficulties posed by the
anisotropic effects, maintains the high-order accuracy, and ensures that the
modified solution state remains within the physically admissible state set.
Positivity-preserving analyses of our WB DG schemes are conducted by using
several key properties of the admissible state set, the HLLC flux and the HLLC
solver, as well as the geometric quasilinearization (GQL) approach in [Wu &amp;
Shu, SIAM Review, 65: 1031-1073, 2023], which was originally applied to analyze
the admissible state set and physical-constraints-preserving schemes for the
relativistic magnetohydrodynamics in [Wu &amp; Tang, M3AS, 27: 1871-1928, 2017], to
address the difficulties arising from the nonlinear constraints on pressure
tensor. Moreover, the proposed WB DG schemes satisfy the weak positivity for
the cell averages, implying the use of a scaling limiter to enforce the
physical admissibility of the DG solution polynomials at certain points of
interest. Extensive numerical experiments are conducted to validate the
preservation of equilibrium states, accuracy in capturing small perturbations
to such states, robustness in solving problems involving low density or low
pressure, and high resolution for both smooth and discontinuous solutions.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15447" title="Abstract">arXiv:2402.15447</a> [<a href="/pdf/2402.15447" title="Download PDF">pdf</a>, <a href="/format/2402.15447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective disclosure of claims from multiple digital credentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rami%C4%87%2C+%C5%A0+B">&#x160;eila Be&#x107;irovi&#x107; Rami&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Prazina%2C+I">Irfan Prazina</a>, 
<a href="/search/cs?searchtype=author&query=Pozderac%2C+D">Damir Pozderac</a>, 
<a href="/search/cs?searchtype=author&query=Mulahasanovi%C4%87%2C+R+T">Razija Tur&#x10d;inhod&#x17e;i&#x107; Mulahasanovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Mrdovi%C4%87%2C+S">Sa&#x161;a Mrdovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the Elsevier for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Digital credentials represent a cornerstone of digital identity on the
Internet. To achieve privacy, certain functionalities in credentials should be
implemented. One is selective disclosure, which allows users to disclose only
the claims or attributes they want. This paper presents a novel approach to
selective disclosure that combines Merkle hash trees and Boneh-Lynn-Shacham
(BLS) signatures. Combining these approaches, we achieve selective disclosure
of claims in a single credential and creation of a verifiable presentation
containing selectively disclosed claims from multiple credentials signed by
different parties. Besides selective disclosure, we enable issuing credentials
signed by multiple issuers using this approach.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15448" title="Abstract">arXiv:2402.15448</a> [<a href="/pdf/2402.15448" title="Download PDF">pdf</a>, <a href="/format/2402.15448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computer Vision for Multimedia Geolocation in Human Trafficking  Investigation: A Systematic Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bamigbade%2C+O">Opeyemi Bamigbade</a>, 
<a href="/search/cs?searchtype=author&query=Sheppard%2C+J">John Sheppard</a>, 
<a href="/search/cs?searchtype=author&query=Scanlon%2C+M">Mark Scanlon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">The task of multimedia geolocation is becoming an increasingly essential
component of the digital forensics toolkit to effectively combat human
trafficking, child sexual exploitation, and other illegal acts. Typically,
metadata-based geolocation information is stripped when multimedia content is
shared via instant messaging and social media. The intricacy of geolocating,
geotagging, or finding geographical clues in this content is often overly
burdensome for investigators. Recent research has shown that contemporary
advancements in artificial intelligence, specifically computer vision and deep
learning, show significant promise towards expediting the multimedia
geolocation task. This systematic literature review thoroughly examines the
state-of-the-art leveraging computer vision techniques for multimedia
geolocation and assesses their potential to expedite human trafficking
investigation. This includes a comprehensive overview of the application of
computer vision-based approaches to multimedia geolocation, identifies their
applicability in combating human trafficking, and highlights the potential
implications of enhanced multimedia geolocation for prosecuting human
trafficking. 123 articles inform this systematic literature review. The
findings suggest numerous potential paths for future impactful research on the
subject.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15449" title="Abstract">arXiv:2402.15449</a> [<a href="/pdf/2402.15449" title="Download PDF">pdf</a>, <a href="/format/2402.15449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repetition Improves Language Model Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Springer%2C+J+M">Jacob Mitchell Springer</a>, 
<a href="/search/cs?searchtype=author&query=Kotha%2C+S">Suhas Kotha</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+D">Daniel Fried</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Raghunathan%2C+A">Aditi Raghunathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 11 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent approaches to improving the extraction of text embeddings from
autoregressive large language models (LLMs) have largely focused on
improvements to data, backbone pretrained language models, or improving
task-differentiation via instructions. In this work, we address an
architectural limitation of autoregressive models: token embeddings cannot
contain information from tokens that appear later in the input. To address this
limitation, we propose a simple approach, "echo embeddings," in which we repeat
the input twice in context and extract embeddings from the second occurrence.
We show that echo embeddings of early tokens can encode information about later
tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On
the MTEB leaderboard, echo embeddings improve over classical embeddings by over
9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a
Mistral-7B model achieve state-of-the-art compared to prior open source models
that do not leverage synthetic fine-tuning data.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15452" title="Abstract">arXiv:2402.15452</a> [<a href="/pdf/2402.15452" title="Download PDF">pdf</a>, <a href="/format/2402.15452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I see an IC: A Mixed-Methods Approach to Study Human Problem-Solving  Processes in Hardware Reverse Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Walendy%2C+R">Ren&#xe9; Walendy</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+M">Markus Weber</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+S">Steffen Becker</a>, 
<a href="/search/cs?searchtype=author&query=Wiesen%2C+C">Carina Wiesen</a>, 
<a href="/search/cs?searchtype=author&query=Elson%2C+M">Malte Elson</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Younghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Fawaz%2C+K">Kassem Fawaz</a>, 
<a href="/search/cs?searchtype=author&query=Rummel%2C+N">Nikol Rummel</a>, 
<a href="/search/cs?searchtype=author&query=Paar%2C+C">Christof Paar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Trust in digital systems depends on secure hardware, often assured through
Hardware Reverse Engineering (HRE). This work develops methods for
investigating human problem-solving processes in HRE, an underexplored yet
critical aspect. Since reverse engineers rely heavily on visual information,
eye tracking holds promise for studying their cognitive processes. To gain
further insights, we additionally employ verbal thought protocols during and
immediately after HRE tasks: Concurrent and Retrospective Think Aloud. We
evaluate the combination of eye tracking and Think Aloud with 41 participants
in an HRE simulation. Eye tracking accurately identifies fixations on
individual circuit elements and highlights critical components. Based on two
use cases, we demonstrate that eye tracking and Think Aloud can complement each
other to improve data quality. Our methodological insights can inform future
studies in HRE, a specific setting of human-computer interaction, and in other
problem-solving settings involving misleading or missing information.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15458" title="Abstract">arXiv:2402.15458</a> [<a href="/pdf/2402.15458" title="Download PDF">pdf</a>, <a href="/format/2402.15458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Optimization of Functionally-graded Triangular Lattices for  Multiple Loading Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Westermann%2C+R">R&#xfc;diger Westermann</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xifeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jun Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Aligning lattices based on local stress distribution is crucial for achieving
exceptional structural stiffness. However, this aspect has primarily been
investigated under a single load condition, where stress in 2D can be described
by two orthogonal principal stress directions. In this paper, we introduce a
novel approach for designing and optimizing triangular lattice structures to
accommodate multiple loading conditions, which means multiple stress fields.
Our method comprises two main steps: homogenization-based topology optimization
and geometry-based de-homogenization. To ensure the geometric regularity of
triangular lattices, we propose a simplified version of the general rank-$3$
laminate and parameterize the design domain using equilateral triangles with
unique thickness per edge. During optimization, the thicknesses and orientation
of each equilateral triangle are adjusted based on the homogenized properties
of triangular lattices. Our numerical findings demonstrate that this proposed
simplification results in only a slight decrease in stiffness, while achieving
triangular lattice structures with a compelling geometric regularity. In
geometry-based de-homogenization, we adopt a field-aligned triangulation
approach to generate a globally consistent triangle mesh, with each triangle
oriented according to the optimized orientation field. Our approach for
handling multiple loading conditions, akin to de-homogenization techniques for
single loading conditions, yields highly detailed, optimized, spatially varying
lattice structures. The method is computationally efficient, as simulations and
optimizations are conducted at a low-resolution discretization of the design
domain. Furthermore, since our approach is geometry-based, obtained structures
are encoded into a compact geometric format that facilitates downstream
operations such as editing and fabrication.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15464" title="Abstract">arXiv:2402.15464</a> [<a href="/pdf/2402.15464" title="Download PDF">pdf</a>, <a href="/format/2402.15464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIPPER+: A Fast Maximal Clique Algorithm for Robust Global Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fathian%2C+K">Kaveh Fathian</a>, 
<a href="/search/cs?searchtype=author&query=Summers%2C+T">Tyler Summers</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE ROBOTICS AND AUTOMATION LETTERS, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We present CLIPPER+, an algorithm for finding maximal cliques in unweighted
graphs for outlier-robust global registration. The registration problem can be
formulated as a graph and solved by finding its maximum clique. This
formulation leads to extreme robustness to outliers; however, finding the
maximum clique is an NP-hard problem, and therefore approximation is required
in practice for large-size problems. The performance of an approximation
algorithm is evaluated by its computational complexity (the lower the runtime,
the better) and solution accuracy (how close the solution is to the maximum
clique). Accordingly, the main contribution of CLIPPER+ is outperforming the
state-of-the-art in accuracy while maintaining a relatively low runtime.
CLIPPER+ builds on prior work (CLIPPER [1] and PMC [2]) and prunes the graph by
removing vertices that have a small core number and cannot be a part of the
maximum clique. This will result in a smaller graph, on which the maximum
clique can be estimated considerably faster. We evaluate the performance of
CLIPPER+ on standard graph benchmarks, as well as synthetic and real-world
point cloud registration problems. These evaluations demonstrate that CLIPPER+
has the highest accuracy and can register point clouds in scenarios where over
$99\%$ of associations are outliers. Our code and evaluation benchmarks are
released at https://github.com/ariarobotics/clipperp.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15467" title="Abstract">arXiv:2402.15467</a> [<a href="/pdf/2402.15467" title="Download PDF">pdf</a>, <a href="/format/2402.15467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human vs. Generative AI in Content Creation Competition: Symbiosis or  Conflict?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+F">Fan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chuanhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Nekipelov%2C+D">Denis Nekipelov</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The advent of generative AI (GenAI) technology produces transformative impact
on the content creation landscape, offering alternative approaches to produce
diverse, high-quality content across media, thereby reshaping online ecosystems
but also raising concerns about market over-saturation and the potential
marginalization of human creativity. Our work introduces a competition model
generalized from the Tullock contest to analyze the tension between human
creators and GenAI. Our theory and simulations suggest that despite challenges,
a stable equilibrium between human and AI-generated content is possible. Our
work contributes to understanding the competitive dynamics in the content
creation industry, offering insights into the future interplay between human
creativity and technological advancements in GenAI.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15469" title="Abstract">arXiv:2402.15469</a> [<a href="/pdf/2402.15469" title="Download PDF">pdf</a>, <a href="/format/2402.15469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking the Robustness of Panoptic Segmentation for Automated  Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haonan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gummadi%2C+D">Daniel Gummadi</a>, 
<a href="/search/cs?searchtype=author&query=Dianati%2C+M">Mehrdad Dianati</a>, 
<a href="/search/cs?searchtype=author&query=Debattista%2C+K">Kurt Debattista</a>, 
<a href="/search/cs?searchtype=author&query=Donzella%2C+V">Valentina Donzella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Precise situational awareness is required for the safe decision-making of
assisted and automated driving (AAD) functions. Panoptic segmentation is a
promising perception technique to identify and categorise objects, impending
hazards, and driveable space at a pixel level. While segmentation quality is
generally associated with the quality of the camera data, a comprehensive
understanding and modelling of this relationship are paramount for AAD system
designers. Motivated by such a need, this work proposes a unifying pipeline to
assess the robustness of panoptic segmentation models for AAD, correlating it
with traditional image quality. The first step of the proposed pipeline
involves generating degraded camera data that reflects real-world noise
factors. To this end, 19 noise factors have been identified and implemented
with 3 severity levels. Of these factors, this work proposes novel models for
unfavourable light and snow. After applying the degradation models, three
state-of-the-art CNN- and vision transformers (ViT)-based panoptic segmentation
networks are used to analyse their robustness. The variations of the
segmentation performance are then correlated to 8 selected image quality
metrics. This research reveals that: 1) certain specific noise factors produce
the highest impact on panoptic segmentation, i.e. droplets on lens and Gaussian
noise; 2) the ViT-based panoptic segmentation backbones show better robustness
to the considered noise factors; 3) some image quality metrics (i.e. LPIPS and
CW-SSIM) correlate strongly with panoptic segmentation performance and
therefore they can be used as predictive metrics for network performance.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15470" title="Abstract">arXiv:2402.15470</a> [<a href="/pdf/2402.15470" title="Download PDF">pdf</a>, <a href="/format/2402.15470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some results involving the $A_&#x3b1;$-eigenvalues for graphs and line  graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Silva+Junior%2C+J+D+G">Joao Domingos Gomes da Silva Junior</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+C+S">Carla Silva Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=da+Costa%2C+L+M+G+C">Liliana Manuela Gaspar C. da Costa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Let $G$ be a simple graph with adjacency matrix $A(G)$, signless Laplacian
matrix $Q(G)$, degree diagonal matrix $D(G)$ and let $l(G)$ be the line graph
of $G$. In 2017, Nikiforov defined the $A_\alpha$-matrix of $G$, $A_\alpha(G)$,
as a linear convex combination of $A(G)$ and $D(G)$, the following way,
$A_\alpha(G):=\alpha A(G)+(1-\alpha)D(G),$ where $\alpha\in[0,1]$. In this
paper, we present some bounds for the eigenvalues of $A_\alpha(G)$ and for the
largest and smallest eigenvalues of $A_\alpha(l(G))$. Extremal graphs attaining
some of these bounds are characterized.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15472" title="Abstract">arXiv:2402.15472</a> [<a href="/pdf/2402.15472" title="Download PDF">pdf</a>, <a href="/format/2402.15472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAIR: Filtering of Automatically Induced Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bajpai%2C+D+J">Divya Jyoti Bajpai</a>, 
<a href="/search/cs?searchtype=author&query=Maheshwari%2C+A">Ayush Maheshwari</a>, 
<a href="/search/cs?searchtype=author&query=Hanawal%2C+M+K">Manjesh Kumar Hanawal</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The availability of large annotated data can be a critical bottleneck in
training machine learning algorithms successfully, especially when applied to
diverse domains. Weak supervision offers a promising alternative by
accelerating the creation of labeled training data using domain-specific rules.
However, it requires users to write a diverse set of high-quality rules to
assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches
circumvent this problem by automatically creating rules from features on a
small labeled set and filtering a final set of rules from them. In the ARI
approach, the crucial step is to filter out a set of a high-quality useful
subset of rules from the large set of automatically created rules. In this
paper, we propose an algorithm (Filtering of Automatically Induced Rules) to
filter rules from a large number of automatically induced rules using
submodular objective functions that account for the collective precision,
coverage, and conflicts of the rule set. We experiment with three ARI
approaches and five text classification datasets to validate the superior
performance of our algorithm with respect to several semi-supervised label
aggregation approaches. Further, we show that achieves statistically
significant results in comparison to existing rule-filtering approaches.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15473" title="Abstract">arXiv:2402.15473</a> [<a href="/pdf/2402.15473" title="Download PDF">pdf</a>, <a href="/format/2402.15473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A  Case-Study in E-Commerce Opinion Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nath%2C+S">Swaroop Nath</a>, 
<a href="/search/cs?searchtype=author&query=Siledar%2C+T">Tejpalsingh Siledar</a>, 
<a href="/search/cs?searchtype=author&query=Muddu%2C+S+S+R+R">Sankara Sri Raghava Ravindra Muddu</a>, 
<a href="/search/cs?searchtype=author&query=Rangaraju%2C+R">Rupasai Rangaraju</a>, 
<a href="/search/cs?searchtype=author&query=Khadilkar%2C+H">Harshad Khadilkar</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+P">Pushpak Bhattacharyya</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Suman Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+A">Amey Patil</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S+S">Sudhanshu Shekhar Singh</a>, 
<a href="/search/cs?searchtype=author&query=Chelliah%2C+M">Muthusamy Chelliah</a>, 
<a href="/search/cs?searchtype=author&query=Garera%2C+N">Nikesh Garera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 figures, 15 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement Learning from Human Feedback (RLHF) has become a dominating
strategy in steering Language Models (LMs) towards human values/goals. The key
to the strategy is employing a reward model ({$\varphi$}) which can reflect a
latent reward model with humans. While this strategy has proven to be
effective, the training methodology requires a lot of human preference
annotation (usually of the order of tens of thousands) to train {$\varphi$}.
Such large-scale preference annotations can be achievable if the reward model
can be ubiquitously used. However, human values/goals are subjective and depend
on the nature of the task. This poses a challenge in collecting diverse
preferences for downstream applications. To address this, we propose a novel
methodology to infuse domain knowledge into {$\varphi$}, which reduces the size
of preference annotation required. We validate our approach in E-Commerce
Opinion Summarization, with a significant reduction in dataset size (just $940$
samples) while advancing the state-of-the-art. Our contributions include a
novel Reward Modelling technique, a new dataset (PromptOpinSumm) for Opinion
Summarization, and a human preference dataset (OpinPref). The proposed
methodology opens avenues for efficient RLHF, making it more adaptable to
diverse applications with varying human values. We release the artifacts for
usage under MIT License.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15477" title="Abstract">arXiv:2402.15477</a> [<a href="/pdf/2402.15477" title="Download PDF">pdf</a>, <a href="/format/2402.15477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiasing Machine Learning Models by Using Weakly Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brotto%2C+R+D+B">Renan D. B. Brotto</a>, 
<a href="/search/cs?searchtype=author&query=Loubes%2C+J">Jean-Michel Loubes</a>, 
<a href="/search/cs?searchtype=author&query=Risser%2C+L">Laurent Risser</a>, 
<a href="/search/cs?searchtype=author&query=Florens%2C+J">Jean-Pierre Florens</a>, 
<a href="/search/cs?searchtype=author&query=Nose-Filho%2C+K">Kenji Nose-Filho</a>, 
<a href="/search/cs?searchtype=author&query=Romano%2C+J+M+T">Jo&#xe3;o M. T. Romano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 25 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">We tackle the problem of bias mitigation of algorithmic decisions in a
setting where both the output of the algorithm and the sensitive variable are
continuous. Most of prior work deals with discrete sensitive variables, meaning
that the biases are measured for subgroups of persons defined by a label,
leaving out important algorithmic bias cases, where the sensitive variable is
continuous. Typical examples are unfair decisions made with respect to the age
or the financial status. In our work, we then propose a bias mitigation
strategy for continuous sensitive variables, based on the notion of endogeneity
which comes from the field of econometrics. In addition to solve this new
problem, our bias mitigation strategy is a weakly supervised learning method
which requires that a small portion of the data can be measured in a fair
manner. It is model agnostic, in the sense that it does not make any hypothesis
on the prediction model. It also makes use of a reasonably large amount of
input observations and their corresponding predictions. Only a small fraction
of the true output predictions should be known. This therefore limits the need
for expert interventions. Results obtained on synthetic data show the
effectiveness of our approach for examples as close as possible to real-life
applications in econometrics.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15478" title="Abstract">arXiv:2402.15478</a> [<a href="/pdf/2402.15478" title="Download PDF">pdf</a>, <a href="/format/2402.15478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers are Expressive, But Are They Expressive Enough for  Regression?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nath%2C+S">Swaroop Nath</a>, 
<a href="/search/cs?searchtype=author&query=Khadilkar%2C+H">Harshad Khadilkar</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+P">Pushpak Bhattacharyya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Transformers have become pivotal in Natural Language Processing,
demonstrating remarkable success in applications like Machine Translation and
Summarization. Given their widespread adoption, several works have attempted to
analyze the expressivity of Transformers. Expressivity of a neural network is
the class of functions it can approximate. A neural network is fully expressive
if it can act as a universal function approximator. We attempt to analyze the
same for Transformers. Contrary to existing claims, our findings reveal that
Transformers struggle to reliably approximate continuous functions, relying on
piecewise constant approximations with sizable intervals. The central question
emerges as: "\textit{Are Transformers truly Universal Function Approximators}?"
To address this, we conduct a thorough investigation, providing theoretical
insights and supporting evidence through experiments. Our contributions include
a theoretical analysis pinpointing the root of Transformers' limitation in
function approximation and extensive experiments to verify the limitation. By
shedding light on these challenges, we advocate a refined understanding of
Transformers' capabilities.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15480" title="Abstract">arXiv:2402.15480</a> [<a href="/pdf/2402.15480" title="Download PDF">pdf</a>, <a href="/format/2402.15480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retinotopic Mapping Enhances the Robustness of Convolutional Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=J%C3%A9r%C3%A9mie%2C+J">Jean-Nicolas J&#xe9;r&#xe9;mie</a>, 
<a href="/search/cs?searchtype=author&query=Dauc%C3%A9%2C+E">Emmanuel Dauc&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Perrinet%2C+L+U">Laurent U Perrinet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Foveated vision, a trait shared by many animals, including humans, has not
been fully utilized in machine learning applications, despite its significant
contributions to biological visual function. This study investigates whether
retinotopic mapping, a critical component of foveated vision, can enhance image
categorization and localization performance when integrated into deep
convolutional neural networks (CNNs). Retinotopic mapping was integrated into
the inputs of standard off-the-shelf convolutional neural networks (CNNs),
which were then retrained on the ImageNet task. As expected, the
logarithmic-polar mapping improved the network's ability to handle arbitrary
image zooms and rotations, particularly for isolated objects. Surprisingly, the
retinotopically mapped network achieved comparable performance in
classification. Furthermore, the network demonstrated improved classification
localization when the foveated center of the transform was shifted. This
replicates a crucial ability of the human visual system that is absent in
typical convolutional neural networks (CNNs). These findings suggest that
retinotopic mapping may be fundamental to significant preattentive visual
processes.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15481" title="Abstract">arXiv:2402.15481</a> [<a href="/pdf/2402.15481" title="Download PDF">pdf</a>, <a href="/format/2402.15481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prejudice and Caprice: A Statistical Framework for Measuring Social  Discrimination in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiran Liu</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Ke Yang</a> (1 and 3), 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zehan Qi</a> (2), 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a> (2), 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a> (2), 
<a href="/search/cs?searchtype=author&query=Zhai%2C+C">Chengxiang Zhai</a> (3) ((1) Equal contributions, (2) Tsinghua University, (3) University of Illinois Urbana-Champaign)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The growing integration of large language models (LLMs) into social
operations amplifies their impact on decisions in crucial areas such as
economics, law, education, and healthcare, raising public concerns about these
models' discrimination-related safety and reliability. However, prior
discrimination measuring frameworks solely assess the average discriminatory
behavior of LLMs, often proving inadequate due to the overlook of an additional
discrimination-leading factor, i.e., the LLMs' prediction variation across
diverse contexts. In this work, we present the Prejudice-Caprice Framework
(PCF) that comprehensively measures discrimination in LLMs by considering both
their consistently biased preference and preference variation across diverse
contexts. Specifically, we mathematically dissect the aggregated contextualized
discrimination risk of LLMs into prejudice risk, originating from LLMs'
persistent prejudice, and caprice risk, stemming from their generation
inconsistency. In addition, we utilize a data-mining approach to gather
preference-detecting probes from sentence skeletons, devoid of attribute
indications, to approximate LLMs' applied contexts. While initially intended
for assessing discrimination in LLMs, our proposed PCF facilitates the
comprehensive and flexible measurement of any inductive biases, including
knowledge alongside prejudice, across various modality models. We apply our
discrimination-measuring framework to 12 common LLMs, yielding intriguing
findings: i) modern LLMs demonstrate significant pro-male stereotypes, ii)
LLMs' exhibited discrimination correlates with several social and economic
factors, iii) prejudice risk dominates the overall discrimination risk and
follows a normal distribution, and iv) caprice risk contributes minimally to
the overall risk but follows a fat-tailed distribution, suggesting that it is
wild risk requiring enhanced surveillance.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15485" title="Abstract">arXiv:2402.15485</a> [<a href="/pdf/2402.15485" title="Download PDF">pdf</a>, <a href="/format/2402.15485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Partitioning With Limited Moves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behbahani%2C+M">Majid Behbahani</a>, 
<a href="/search/cs?searchtype=author&query=Dalirrooyfard%2C+M">Mina Dalirrooyfard</a>, 
<a href="/search/cs?searchtype=author&query=Fata%2C+E">Elaheh Fata</a>, 
<a href="/search/cs?searchtype=author&query=Nevmyvaka%2C+Y">Yuriy Nevmyvaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> shortened version accepted in AISTATS 2024 as oral
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In many real world networks, there already exists a (not necessarily optimal)
$k$-partitioning of the network. Oftentimes, one aims to find a
$k$-partitioning with a smaller cut value for such networks by moving only a
few nodes across partitions. The number of nodes that can be moved across
partitions is often a constraint forced by budgetary limitations. Motivated by
such real-world applications, we introduce and study the $r$-move
$k$-partitioning~problem, a natural variant of the Multiway cut problem. Given
a graph, a set of $k$ terminals and an initial partitioning of the graph, the
$r$-move $k$-partitioning~problem aims to find a $k$-partitioning with the
minimum-weighted cut among all the $k$-partitionings that can be obtained by
moving at most $r$ non-terminal nodes to partitions different from their
initial ones. Our main result is a polynomial time $3(r+1)$ approximation
algorithm for this problem. We further show that this problem is $W[1]$-hard,
and give an FPTAS for when $r$ is a small constant.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15487" title="Abstract">arXiv:2402.15487</a> [<a href="/pdf/2402.15487" title="Download PDF">pdf</a>, <a href="/format/2402.15487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for  Robotic Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hanxiao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Binghao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruihai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Shubham Garg</a>, 
<a href="/search/cs?searchtype=author&query=Nayyeri%2C+H">Hooshang Nayyeri</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenlong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunzhu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://jianghanxiao.github.io/roboexp-web/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Robots need to explore their surroundings to adapt to and tackle tasks in
unknown environments. Prior work has proposed building scene graphs of the
environment but typically assumes that the environment is static, omitting
regions that require active interactions. This severely limits their ability to
handle more complex tasks in household and office environments: before setting
up a table, robots must explore drawers and cabinets to locate all utensils and
condiments. In this work, we introduce the novel task of interactive scene
exploration, wherein robots autonomously explore environments and produce an
action-conditioned scene graph (ACSG) that captures the structure of the
underlying environment. The ACSG accounts for both low-level information, such
as geometry and semantics, and high-level information, such as the
action-conditioned relationships between different entities in the scene. To
this end, we present the Robotic Exploration (RoboEXP) system, which
incorporates the Large Multimodal Model (LMM) and an explicit memory design to
enhance our system's capabilities. The robot reasons about what and how to
explore an object, accumulating new information through the interaction process
and incrementally constructing the ACSG. We apply our system across various
real-world settings in a zero-shot manner, demonstrating its effectiveness in
exploring and modeling environments it has never seen before. Leveraging the
constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP
system in facilitating a wide range of real-world manipulation tasks involving
rigid, articulated objects, nested objects like Matryoshka dolls, and
deformable objects like cloth.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15490" title="Abstract">arXiv:2402.15490</a> [<a href="/pdf/2402.15490" title="Download PDF">pdf</a>, <a href="/format/2402.15490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey of Convolutions in Deep Learning: Applications,  Challenges, and Future Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Younesi%2C+A">Abolfazl Younesi</a>, 
<a href="/search/cs?searchtype=author&query=Ansari%2C+M">Mohsen Ansari</a>, 
<a href="/search/cs?searchtype=author&query=Fazli%2C+M">MohammadAmin Fazli</a>, 
<a href="/search/cs?searchtype=author&query=Ejlali%2C+A">Alireza Ejlali</a>, 
<a href="/search/cs?searchtype=author&query=Shafique%2C+M">Muhammad Shafique</a>, 
<a href="/search/cs?searchtype=author&query=Henkel%2C+J">J&#xf6;rg Henkel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In today's digital age, Convolutional Neural Networks (CNNs), a subset of
Deep Learning (DL), are widely used for various computer vision tasks such as
image classification, object detection, and image segmentation. There are
numerous types of CNNs designed to meet specific needs and requirements,
including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention,
depthwise convolutions, and NAS, among others. Each type of CNN has its unique
structure and characteristics, making it suitable for specific tasks. It's
crucial to gain a thorough understanding and perform a comparative analysis of
these different CNN types to understand their strengths and weaknesses.
Furthermore, studying the performance, limitations, and practical applications
of each type of CNN can aid in the development of new and improved
architectures in the future. We also dive into the platforms and frameworks
that researchers utilize for their research or development from various
perspectives. Additionally, we explore the main research fields of CNN like 6D
vision, generative models, and meta-learning. This survey paper provides a
comprehensive examination and comparison of various CNN architectures,
highlighting their architectural differences and emphasizing their respective
advantages, disadvantages, applications, challenges, and future trends.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15491" title="Abstract">arXiv:2402.15491</a> [<a href="/pdf/2402.15491" title="Download PDF">pdf</a>, <a href="/format/2402.15491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> API-BLEND: A Comprehensive Corpora for Training and Benchmarking API  LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basu%2C+K">Kinjal Basu</a>, 
<a href="/search/cs?searchtype=author&query=Abdelaziz%2C+I">Ibrahim Abdelaziz</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhury%2C+S">Subhajit Chaudhury</a>, 
<a href="/search/cs?searchtype=author&query=Dan%2C+S">Soham Dan</a>, 
<a href="/search/cs?searchtype=author&query=Crouse%2C+M">Maxwell Crouse</a>, 
<a href="/search/cs?searchtype=author&query=Munawar%2C+A">Asim Munawar</a>, 
<a href="/search/cs?searchtype=author&query=Kumaravel%2C+S">Sadhana Kumaravel</a>, 
<a href="/search/cs?searchtype=author&query=Muthusamy%2C+V">Vinod Muthusamy</a>, 
<a href="/search/cs?searchtype=author&query=Kapanipathi%2C+P">Pavan Kapanipathi</a>, 
<a href="/search/cs?searchtype=author&query=Lastras%2C+L+A">Luis A. Lastras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">There is a growing need for Large Language Models (LLMs) to effectively use
tools and external Application Programming Interfaces (APIs) to plan and
complete tasks. As such, there is tremendous interest in methods that can
acquire sufficient quantities of train and test data that involve calls to
tools / APIs. Two lines of research have emerged as the predominant strategies
for addressing this challenge. The first has focused on synthetic data
generation techniques, while the second has involved curating task-adjacent
datasets which can be transformed into API / Tool-based tasks. In this paper,
we focus on the task of identifying, curating, and transforming existing
datasets and, in turn, introduce API-BLEND, a large corpora for training and
systematic testing of tool-augmented LLMs. The datasets mimic real-world
scenarios involving API-tasks such as API / tool detection, slot filling, and
sequencing of the detected APIs. We demonstrate the utility of the API-BLEND
dataset for both training and benchmarking purposes.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15492" title="Abstract">arXiv:2402.15492</a> [<a href="/pdf/2402.15492" title="Download PDF">pdf</a>, <a href="/format/2402.15492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechanics-Informed Autoencoder Enables Automated Detection and  Localization of Unforeseen Structural Damage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Bolandi%2C+H">Hamed Bolandi</a>, 
<a href="/search/cs?searchtype=author&query=Masmoudi%2C+M">Mahdi Masmoudi</a>, 
<a href="/search/cs?searchtype=author&query=Salem%2C+T">Talal Salem</a>, 
<a href="/search/cs?searchtype=author&query=Lajnef%2C+N">Nizar Lajnef</a>, 
<a href="/search/cs?searchtype=author&query=Boddeti%2C+V+N">Vishnu Naresh Boddeti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Structural health monitoring (SHM) is vital for ensuring the safety and
longevity of structures like buildings and bridges. As the volume and scale of
structures and the impact of their failure continue to grow, there is a dire
need for SHM techniques that are scalable, inexpensive, operate passively
without human intervention, and customized for each mechanical structure
without the need for complex baseline models. We present a novel
"deploy-and-forget" approach for automated detection and localization of
damages in structures. It is based on a synergistic combination of fully
passive measurements from inexpensive sensors and a mechanics-informed
autoencoder. Once deployed, our solution continuously learns and adapts a
bespoke baseline model for each structure, learning from its undamaged state's
response characteristics. After learning from just 3 hours of data, it can
autonomously detect and localize different types of unforeseen damage. Results
from numerical simulations and experiments indicate that incorporating the
mechanical characteristics into the variational autoencoder allows for up to
35\% earlier detection and localization of damage over a standard autoencoder.
Our approach holds substantial promise for a significant reduction in human
intervention and inspection costs and enables proactive and preventive
maintenance strategies, thus extending the lifespan, reliability, and
sustainability of civil infrastructures.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15494" title="Abstract">arXiv:2402.15494</a> [<a href="/pdf/2402.15494" title="Download PDF">pdf</a>, <a href="/format/2402.15494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Complexity of Community-aware Network Sparsification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herrendorf%2C+E">Emanuel Herrendorf</a>, 
<a href="/search/cs?searchtype=author&query=Komusiewicz%2C+C">Christian Komusiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Morawietz%2C+N">Nils Morawietz</a>, 
<a href="/search/cs?searchtype=author&query=Sommer%2C+F">Frank Sommer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Network sparsification is the task of reducing the number of edges of a given
graph while preserving some crucial graph property. In community-aware network
sparsification, the preserved property concerns the subgraphs that are induced
by the communities of the graph which are given as vertex subsets. This is
formalized in the $\Pi$-Network Sparsification problem: given an edge-weighted
graph $G$, a collection $Z$ of $c$ subsets of $V(G)$ (communities), and two
numbers $\ell, b$, the question is whether there exists a spanning subgraph
$G'$ of $G$ with at most $\ell$ edges of total weight at most $b$ such that
$G'[C]$ fulfills $\Pi$ for each community $C$. Here, we consider two graph
properties $\Pi$: the connectivity property (Connectivity NWS) and the property
of having a spanning star (Stars NWS). Since both problems are NP-hard, we
study their parameterized and fine-grained complexity.
<br />We provide a tight $2^{\Omega(n^2+c)} poly(n+|Z|)$-time running time lower
bound based on the ETH for both problems, where $n$ is the number of vertices
in $G$. The lower bound holds even in the restricted case when all communities
have size at most 4, $G$ is a clique, and every edge has unit weight. For the
connectivity property, the unit weight case with $G$ being a clique is the
well-studied problem of computing a hypergraph support with a minimum number of
edges. We then study the complexity of both problems parameterized by the
feedback edge number $t$ of the solution graph $G'$. For Stars NWS, we present
an XP-algorithm for $t$. This answers an open question by Korach and Stern
[Disc. Appl. Math. '08] who asked for the existence of polynomial-time
algorithms for $t=0$. In contrast, we show for Connectivity NWS that known
polynomial-time algorithms for $t=0$ [Korach and Stern, Math. Program. '03;
Klemz et al., SWAT '14] cannot be extended by showing that Connectivity NWS is
NP-hard for $t=1$.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15504" title="Abstract">arXiv:2402.15504</a> [<a href="/pdf/2402.15504" title="Download PDF">pdf</a>, <a href="/format/2402.15504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gen4Gen: Generative Data Pipeline for Generative Multi-Concept  Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C">Chun-Hsiao Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T">Ta-Ying Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+H">He-Yen Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chuan-En Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Markham%2C+A">Andrew Markham</a>, 
<a href="/search/cs?searchtype=author&query=Trigoni%2C+N">Niki Trigoni</a>, 
<a href="/search/cs?searchtype=author&query=Kung%2C+H+T">H.T. Kung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yubei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint; Project Page: <a href="https://danielchyeh.github.io/Gen4Gen/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent text-to-image diffusion models are able to learn and synthesize images
containing novel, personalized concepts (e.g., their own pets or specific
items) with just a few examples for training. This paper tackles two
interconnected issues within this realm of personalizing text-to-image
diffusion models. First, current personalization techniques fail to reliably
extend to multiple concepts -- we hypothesize this to be due to the mismatch
between complex scenes and simple text descriptions in the pre-training dataset
(e.g., LAION). Second, given an image containing multiple personalized
concepts, there lacks a holistic metric that evaluates performance on not just
the degree of resemblance of personalized concepts, but also whether all
concepts are present in the image and whether the image accurately reflects the
overall text description. To address these issues, we introduce Gen4Gen, a
semi-automated dataset creation pipeline utilizing generative models to combine
personalized concepts into complex compositions along with text-descriptions.
Using this, we create a dataset called MyCanvas, that can be used to benchmark
the task of multi-concept personalization. In addition, we design a
comprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better
quantifying the performance of multi-concept, personalized text-to-image
diffusion methods. We provide a simple baseline built on top of Custom
Diffusion with empirical prompting strategies for future researchers to
evaluate on MyCanvas. We show that by improving data quality and prompting
strategies, we can significantly increase multi-concept personalized image
generation quality, without requiring any modifications to model architecture
or training algorithms.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15505" title="Abstract">arXiv:2402.15505</a> [<a href="/pdf/2402.15505" title="Download PDF">pdf</a>, <a href="/format/2402.15505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-Supervised Learning: Improving Weak-to-Strong Generalization with  Hierarchical Mixture of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuejiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Alahi%2C+A">Alexandre Alahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Steering the behavior of a strong model pre-trained on internet-scale data
can be difficult due to the scarcity of competent supervisors. Recent studies
reveal that, despite supervisory noises, a strong student model may surpass its
weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of
such weak-to-strong generalization remains limited, especially in the presence
of large capability gaps. In this paper, we propose to address this challenge
by harnessing a diverse set of specialized teachers, instead of a single
generalist one, that collectively supervises the strong student. Our approach
resembles the classical hierarchical mixture of experts, with two components
tailored for co-supervision: (i) we progressively alternate student training
and teacher assignment, leveraging the growth of the strong student to identify
plausible supervisions; (ii) we conservatively enforce teacher-student and
local-global consistency, leveraging their dependencies to reject potential
annotation noises. We validate the proposed method through visual recognition
tasks on the OpenAI weak-to-strong benchmark and additional multi-domain
datasets. Our code is available at \url{https://github.com/yuejiangliu/csl}.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15506" title="Abstract">arXiv:2402.15506</a> [<a href="/pdf/2402.15506" title="Download PDF">pdf</a>, <a href="/format/2402.15506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentOhana: Design Unified Data and Training Pipeline for Effective  Agent Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianguo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Murthy%2C+R">Rithesh Murthy</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Weiran Yao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Juntao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+T">Thai Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yihao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Awalgaonkar%2C+T">Tulika Awalgaonkar</a>, 
<a href="/search/cs?searchtype=author&query=Niebles%2C+J+C">Juan Carlos Niebles</a>, 
<a href="/search/cs?searchtype=author&query=Savarese%2C+S">Silvio Savarese</a>, 
<a href="/search/cs?searchtype=author&query=Heinecke%2C+S">Shelby Heinecke</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15509" title="Abstract">arXiv:2402.15509</a> [<a href="/pdf/2402.15509" title="Download PDF">pdf</a>, <a href="/format/2402.15509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seamless Human Motion Composition with Blended Positional Encodings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barquero%2C+G">German Barquero</a>, 
<a href="/search/cs?searchtype=author&query=Escalera%2C+S">Sergio Escalera</a>, 
<a href="/search/cs?searchtype=author&query=Palmero%2C+C">Cristina Palmero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://barquerogerman.github.io/FlowMDM/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Conditional human motion generation is an important topic with many
applications in virtual reality, gaming, and robotics. While prior works have
focused on generating motion guided by text, music, or scenes, these typically
result in isolated motions confined to short durations. Instead, we address the
generation of long, continuous sequences guided by a series of varying textual
descriptions. In this context, we introduce FlowMDM, the first diffusion-based
model that generates seamless Human Motion Compositions (HMC) without any
postprocessing or redundant denoising steps. For this, we introduce the Blended
Positional Encodings, a technique that leverages both absolute and relative
positional encodings in the denoising chain. More specifically, global motion
coherence is recovered at the absolute stage, whereas smooth and realistic
transitions are built at the relative stage. As a result, we achieve
state-of-the-art results in terms of accuracy, realism, and smoothness on the
Babel and HumanML3D datasets. FlowMDM excels when trained with only a single
description per motion sequence thanks to its Pose-Centric Cross-ATtention,
which makes it robust against varying text descriptions at inference time.
Finally, to address the limitations of existing HMC metrics, we propose two new
metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt
transitions.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Mon, 26 Feb 24</h3>
<dl>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14827" title="Abstract">arXiv:2402.14827</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.14827" title="Download PDF">pdf</a>, <a href="/ps/2402.14827" title="Download PostScript">ps</a>, <a href="/format/2402.14827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Uterine Synchronization Analysis in Pregnancy and Labor  through Window Selection and Node Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Dine%2C+K+B+E">Kamil Bader El Dine</a>, 
<a href="/search/q-bio?searchtype=author&query=Nader%2C+N">Noujoud Nader</a>, 
<a href="/search/q-bio?searchtype=author&query=Khalil%2C+M">Mohamad Khalil</a>, 
<a href="/search/q-bio?searchtype=author&query=Marque%2C+C">Catherine Marque</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Preterm labor (PL) has globally become the leading cause of death in children
under the age of 5 years. To address this problem, this paper will provide a
new approach by analyzing the EHG signals, which are recorded on the abdomen of
the mother during labor and pregnancy. The EHG signal reflects the electrical
activity that induces the mechanical contraction of the myometrium. Because
EHGs are known to be non-stationary signals, and because we anticipate
connectivity to alter during contraction, we applied the windowing approach on
real signals to help us identify the best windows and the best nodes with the
most significant data to be used for classification. The suggested pipeline
includes i) divide the 16 EHG signals that are recorded from the abdomen of
pregnant women in N windows; ii) apply the connectivity matrices on each
window; iii) apply the Graph theory-based measures on the connectivity matrices
on each window; iv) apply the consensus Matrix on each window in order to
retrieve the best windows and the best nodes. Following that, several neural
network and machine learning methods are applied to the best windows and best
nodes to categorize pregnancy and labor contractions, based on the different
input parameters (connectivity method alone, connectivity method plus graph
parameters, best nodes, all nodes, best windows, all windows). Results showed
that the best nodes are nodes 8, 9, 10, 11, and 12; while the best windows are
2, 4, and 5. The classification results obtained by using only these best nodes
are better than when using the whole nodes. The results are always better when
using the full burst, whatever the chosen nodes. Thus, the windowing approach
proved to be an innovative technique that can improve the differentiation
between labor and pregnancy EHG signals.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14844" title="Abstract">arXiv:2402.14844</a> (cross-list from math.OC) [<a href="/pdf/2402.14844" title="Download PDF">pdf</a>, <a href="/format/2402.14844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The New Era of Dynamic Pricing: Synergizing Supervised Learning and  Quadratic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bramao%2C+G">Gustavo Bramao</a>, 
<a href="/search/math?searchtype=author&query=Tarygin%2C+I">Ilia Tarygin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we explore a novel combination of supervised learning and
quadratic programming to refine dynamic pricing models in the car rental
industry. We utilize dynamic modeling of price elasticity, informed by ordinary
least squares (OLS) metrics such as p-values, homoscedasticity, error
normality. These metrics, when their underlying assumptions hold, are integral
in guiding a quadratic programming agent. The program is tasked with optimizing
margin for a given finite set target.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14847" title="Abstract">arXiv:2402.14847</a> (cross-list from math.OC) [<a href="/pdf/2402.14847" title="Download PDF">pdf</a>, <a href="/format/2402.14847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep learning-driven scheduling algorithm for a single machine problem  minimizing the total tardiness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bou%C5%A1ka%2C+M">Michal Bou&#x161;ka</a>, 
<a href="/search/math?searchtype=author&query=%C5%A0%C5%AFcha%2C+P">P&#x159;emysl &#x160;&#x16f;cha</a>, 
<a href="/search/math?searchtype=author&query=Nov%C3%A1k%2C+A">Anton&#xed;n Nov&#xe1;k</a>, 
<a href="/search/math?searchtype=author&query=Hanz%C3%A1lek%2C+Z">Zden&#x11b;k Hanz&#xe1;lek</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> European Journal of Operational Research, Volume 308, Issue 3, 1
  August 2023, Pages 990-1006
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we investigate the use of the deep learning method for solving
a well-known NP-hard single machine scheduling problem with the objective of
minimizing the total tardiness. We propose a deep neural network that acts as a
polynomial-time estimator of the criterion value used in a single-pass
scheduling algorithm based on Lawler's decomposition and symmetric
decomposition proposed by Della Croce et al. Essentially, the neural network
guides the algorithm by estimating the best splitting of the problem into
subproblems. The paper also describes a new method for generating the training
data set, which speeds up the training dataset generation and reduces the
average optimality gap of solutions. The experimental results show that our
machine learning-driven approach can efficiently generalize information from
the training phase to significantly larger instances. Even though the instances
used in the training phase have from 75 to 100 jobs, the average optimality gap
on instances with up to 800 jobs is 0.26%, which is almost five times less than
the gap of the state-of-the-art heuristic.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14877" title="Abstract">arXiv:2402.14877</a> (cross-list from physics.ao-ph) [<a href="/pdf/2402.14877" title="Download PDF">pdf</a>, <a href="/format/2402.14877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine-learning prediction of tipping and collapse of the Atlantic  Meridional Overturning Circulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Panahi%2C+S">Shirin Panahi</a>, 
<a href="/search/physics?searchtype=author&query=Kong%2C+L">Ling-Wei Kong</a>, 
<a href="/search/physics?searchtype=author&query=Moradi%2C+M">Mohammadamin Moradi</a>, 
<a href="/search/physics?searchtype=author&query=Zhai%2C+Z">Zheng-Meng Zhai</a>, 
<a href="/search/physics?searchtype=author&query=Glaz%2C+B">Bryan Glaz</a>, 
<a href="/search/physics?searchtype=author&query=Haile%2C+M">Mulugeta Haile</a>, 
<a href="/search/physics?searchtype=author&query=Lai%2C+Y">Ying-Cheng Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS); Data Analysis, Statistics and Probability (physics.data-an); Popular Physics (physics.pop-ph)

</div>
<p class="mathjax">Recent research on the Atlantic Meridional Overturning Circulation (AMOC)
raised concern about its potential collapse through a tipping point due to the
climate-change caused increase in the freshwater input into the North Atlantic.
The predicted time window of collapse is centered about the middle of the
century and the earliest possible start is approximately two years from now.
More generally, anticipating a tipping point at which the system transitions
from one stable steady state to another is relevant to a broad range of fields.
We develop a machine-learning approach to predicting tipping in noisy dynamical
systems with a time-varying parameter and test it on a number of systems
including the AMOC, ecological networks, an electrical power system, and a
climate model. For the AMOC, our prediction based on simulated fingerprint data
and real data of the sea surface temperature places the time window of a
potential collapse between the years 2040 and 2065.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14892" title="Abstract">arXiv:2402.14892</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.14892" title="Download PDF">pdf</a>, <a href="/format/2402.14892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novelty Detection on Radio Astronomy Data using Signatures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Arrubarrena%2C+P">Paola Arrubarrena</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lemercier%2C+M">Maud Lemercier</a>, 
<a href="/search/astro-ph?searchtype=author&query=Nikolic%2C+B">Bojan Nikolic</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lyons%2C+T">Terry Lyons</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cass%2C+T">Thomas Cass</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce SigNova, a new semi-supervised framework for detecting anomalies
in streamed data. While our initial examples focus on detecting radio-frequency
interference (RFI) in digitized signals within the field of radio astronomy, it
is important to note that SigNova's applicability extends to any type of
streamed data. The framework comprises three primary components. Firstly, we
use the signature transform to extract a canonical collection of summary
statistics from observational sequences. This allows us to represent
variable-length visibility samples as finite-dimensional feature vectors.
Secondly, each feature vector is assigned a novelty score, calculated as the
Mahalanobis distance to its nearest neighbor in an RFI-free training set. By
thresholding these scores we identify observation ranges that deviate from the
expected behavior of RFI-free visibility samples without relying on stringent
distributional assumptions. Thirdly, we integrate this anomaly detector with
Pysegments, a segmentation algorithm, to localize consecutive observations
contaminated with RFI, if any. This approach provides a compelling alternative
to classical windowing techniques commonly used for RFI detection. Importantly,
the complexity of our algorithm depends on the RFI pattern rather than on the
size of the observation window. We demonstrate how SigNova improves the
detection of various types of RFI (e.g., broadband and narrowband) in
time-frequency visibility data. We validate our framework on the Murchison
Widefield Array (MWA) telescope and simulated data and the Hydrogen Epoch of
Reionization Array (HERA).
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14951" title="Abstract">arXiv:2402.14951</a> (cross-list from stat.ML) [<a href="/pdf/2402.14951" title="Download PDF">pdf</a>, <a href="/ps/2402.14951" title="Download PostScript">ps</a>, <a href="/format/2402.14951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning of a Linear Transformer Block: Benefits of the MLP  Component and One-Step GD Initialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+R">Ruiqi Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+J">Jingfeng Wu</a>, 
<a href="/search/stat?searchtype=author&query=Bartlett%2C+P+L">Peter L. Bartlett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the \emph{in-context learning} (ICL) ability of a \emph{Linear
Transformer Block} (LTB) that combines a linear attention component and a
linear multi-layer perceptron (MLP) component. For ICL of linear regression
with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve
nearly Bayes optimal ICL risk. In contrast, using only linear attention must
incur an irreducible additive approximation error. Furthermore, we establish a
correspondence between LTB and one-step gradient descent estimators with
learnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sense
that every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented by
an LTB estimator and every optimal LTB estimator that minimizes the in-class
ICL risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator.
Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can be
efficiently optimized with gradient flow, despite a non-convex training
objective. Our results reveal that LTB achieves ICL by implementing
$\mathsf{GD}\text{-}\mathbf{\beta}$, and they highlight the role of MLP layers
in reducing approximation error.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14959" title="Abstract">arXiv:2402.14959</a> (cross-list from stat.AP) [<a href="/pdf/2402.14959" title="Download PDF">pdf</a>, <a href="/format/2402.14959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Christia%2C+F">Fotini Christia</a>, 
<a href="/search/stat?searchtype=author&query=Han%2C+J+X">Jessy Xinyi Han</a>, 
<a href="/search/stat?searchtype=author&query=Miller%2C+A">Andrew Miller</a>, 
<a href="/search/stat?searchtype=author&query=Shah%2C+D">Devavrat Shah</a>, 
<a href="/search/stat?searchtype=author&query=Watkins%2C+S+C">S. Craig Watkins</a>, 
<a href="/search/stat?searchtype=author&query=Winship%2C+C">Christopher Winship</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
<p class="mathjax">We are interested in developing a data-driven method to evaluate race-induced
biases in law enforcement systems. While the recent works have addressed this
question in the context of police-civilian interactions using police stop data,
they have two key limitations. First, bias can only be properly quantified if
true criminality is accounted for in addition to race, but it is absent in
prior works. Second, law enforcement systems are multi-stage and hence it is
important to isolate the true source of bias within the "causal chain of
interactions" rather than simply focusing on the end outcome; this can help
guide reforms. In this work, we address these challenges by presenting a
multi-stage causal framework incorporating criminality. We provide a
theoretical characterization and an associated data-driven method to evaluate
(a) the presence of any form of racial bias, and (b) if so, the primary source
of such a bias in terms of race and criminality. Our framework identifies three
canonical scenarios with distinct characteristics: in settings like (1) airport
security, the primary source of observed bias against a race is likely to be
bias in law enforcement against innocents of that race; (2) AI-empowered
policing, the primary source of observed bias against a race is likely to be
bias in law enforcement against criminals of that race; and (3) police-civilian
interaction, the primary source of observed bias against a race could be bias
in law enforcement against that race or bias from the general public in
reporting against the other race. Through an extensive empirical study using
police-civilian interaction data and 911 call data, we find an instance of such
a counter-intuitive phenomenon: in New Orleans, the observed bias is against
the majority race and the likely reason for it is the over-reporting (via 911
calls) of incidents involving the minority race by the general public.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14966" title="Abstract">arXiv:2402.14966</a> (cross-list from stat.ML) [<a href="/pdf/2402.14966" title="Download PDF">pdf</a>, <a href="/format/2402.14966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothness Adaptive Hypothesis Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lin%2C+H">Haotian Lin</a>, 
<a href="/search/stat?searchtype=author&query=Reimherr%2C+M">Matthew Reimherr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Many existing two-phase kernel-based hypothesis transfer learning algorithms
employ the same kernel regularization across phases and rely on the known
smoothness of functions to obtain optimality. Therefore, they fail to adapt to
the varying and unknown smoothness between the target/source and their offset
in practice. In this paper, we address these problems by proposing Smoothness
Adaptive Transfer Learning (SATL), a two-phase kernel ridge
regression(KRR)-based algorithm. We first prove that employing the misspecified
fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax
optimality and derive an adaptive procedure to the unknown Sobolev smoothness.
Leveraging these results, SATL employs Gaussian kernels in both phases so that
the estimators can adapt to the unknown smoothness of the target/source and
their offset function. We derive the minimax lower bound of the learning
problem in excess risk and show that SATL enjoys a matching upper bound up to a
logarithmic factor. The minimax convergence rate sheds light on the factors
influencing transfer dynamics and demonstrates the superiority of SATL compared
to non-transfer learning settings. While our main objective is a theoretical
analysis, we also conduct several experiments to confirm our results.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14974" title="Abstract">arXiv:2402.14974</a> (cross-list from eess.IV) [<a href="/pdf/2402.14974" title="Download PDF">pdf</a>, <a href="/format/2402.14974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An  Application for MxIF Oncology Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Farhadloo%2C+M">Majid Farhadloo</a>, 
<a href="/search/eess?searchtype=author&query=Sharma%2C+A">Arun Sharma</a>, 
<a href="/search/eess?searchtype=author&query=Gupta%2C+J">Jayant Gupta</a>, 
<a href="/search/eess?searchtype=author&query=Leontovich%2C+A">Alexey Leontovich</a>, 
<a href="/search/eess?searchtype=author&query=Markovic%2C+S+N">Svetomir N. Markovic</a>, 
<a href="/search/eess?searchtype=author&query=Shekhar%2C+S">Shashi Shekhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIAM International Conference on Data Mining (SDM24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Given multi-category point sets from different place-types, our goal is to
develop a spatially-lucid classifier that can distinguish between two classes
based on the arrangements of their points. This problem is important for many
applications, such as oncology, for analyzing immune-tumor relationships and
designing new immunotherapies. It is challenging due to spatial variability and
interpretability needs. Previously proposed techniques require dense training
data or have limited ability to handle significant spatial variability within a
single place-type. Most importantly, these deep neural network (DNN) approaches
are not designed to work in non-Euclidean space, particularly point sets.
Existing non-Euclidean DNN methods are limited to one-size-fits-all approaches.
We explore a spatial ensemble framework that explicitly uses different training
strategies, including weighted-distance learning rate and spatial domain
adaptation, on various place-types for spatially-lucid classification.
Experimental results on real-world datasets (e.g., MxIF oncology data) show
that the proposed framework provides higher prediction accuracy than baseline
methods.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14980" title="Abstract">arXiv:2402.14980</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.14980" title="Download PDF">pdf</a>, <a href="/ps/2402.14980" title="Download PostScript">ps</a>, <a href="/format/2402.14980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Analysis of Data Preprocessing Methods, Feature Selection  Techniques and Machine Learning Models for Improved Classification and  Regression Performance on Imbalanced Genetic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Kaur%2C+A">Arshmeet Kaur</a>, 
<a href="/search/q-bio?searchtype=author&query=Sarmadi%2C+M">Morteza Sarmadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Rapid advancements in genome sequencing have led to the collection of vast
amounts of genomics data. Researchers may be interested in using machine
learning models on such data to predict the pathogenicity or clinical
significance of a genetic mutation. However, many genetic datasets contain
imbalanced target variables that pose challenges to machine learning models:
observations are skewed/imbalanced in regression tasks or class-imbalanced in
classification tasks. Genetic datasets are also often high-cardinal and contain
skewed predictor variables, which poses further challenges. We aimed to
investigate the effects of data preprocessing, feature selection techniques,
and model selection on the performance of models trained on these datasets. We
measured performance with 5-fold cross-validation and compared averaged
r-squared and accuracy metrics across different combinations of techniques. We
found that outliers/skew in predictor or target variables did not pose a
challenge to regression models. We also found that class-imbalanced target
variables and skewed predictors had little to no impact on classification
performance. Random forest was the best model to use for imbalanced regression
tasks. While our study uses a genetic dataset as an example of a real-world
application, our findings can be generalized to any similar datasets.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14987" title="Abstract">arXiv:2402.14987</a> (cross-list from stat.ML) [<a href="/pdf/2402.14987" title="Download PDF">pdf</a>, <a href="/ps/2402.14987" title="Download PostScript">ps</a>, <a href="/format/2402.14987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Performance of Empirical Risk Minimization with Smoothed Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Block%2C+A">Adam Block</a>, 
<a href="/search/stat?searchtype=author&query=Rakhlin%2C+A">Alexander Rakhlin</a>, 
<a href="/search/stat?searchtype=author&query=Shetty%2C+A">Abhishek Shetty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In order to circumvent statistical and computational hardness results in
sequential decision-making, recent work has considered smoothed online
learning, where the distribution of data at each time is assumed to have
bounded likeliehood ratio with respect to a base measure when conditioned on
the history. While previous works have demonstrated the benefits of smoothness,
they have either assumed that the base measure is known to the learner or have
presented computationally inefficient algorithms applying only in special
cases. This work investigates the more general setting where the base measure
is \emph{unknown} to the learner, focusing in particular on the performance of
Empirical Risk Minimization (ERM) with square loss when the data are
well-specified and smooth. We show that in this setting, ERM is able to achieve
sublinear error whenever a class is learnable with iid data; in particular, ERM
achieves error scaling as $\tilde O( \sqrt{\mathrm{comp}(\mathcal F)\cdot T}
)$, where $\mathrm{comp}(\mathcal F)$ is the statistical complexity of learning
$\mathcal F$ with iid data. In so doing, we prove a novel norm comparison bound
for smoothed data that comprises the first sharp norm comparison for dependent
data applying to arbitrary, nonlinear function classes. We complement these
results with a lower bound indicating that our analysis of ERM is essentially
tight, establishing a separation in the performance of ERM between smoothed and
iid data.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15034" title="Abstract">arXiv:2402.15034</a> (cross-list from math.CO) [<a href="/pdf/2402.15034" title="Download PDF">pdf</a>, <a href="/ps/2402.15034" title="Download PostScript">ps</a>, <a href="/format/2402.15034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rectilinear Crossing Number of Graphs Excluding Single-Crossing Graphs  as Minors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dujmovi%C4%87%2C+V">Vida Dujmovi&#x107;</a>, 
<a href="/search/math?searchtype=author&query=La+Rose%2C+C">Camille La Rose</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">The crossing number of a graph $G$ is the minimum number of crossings in a
drawing of $G$ in the plane. A rectilinear drawing of a graph $G$ represents
vertices of $G$ by a set of points in the plane and represents each edge of $G$
by a straight-line segment connecting its two endpoints. The rectilinear
crossing number of $G$ is the minimum number of crossings in a rectilinear
drawing of $G$.
<br />By the crossing lemma, the crossing number of an $n$-vertex graph $G$ can be
$O(n)$ only if $|E(G)|\in O(n)$. Graphs of bounded genus and bounded degree
(B\"{o}r\"{o}czky, Pach and T\'{o}th, 2006) and in fact all bounded degree
proper minor-closed families (Wood and Telle, 2007) have been shown to admit
linear crossing number, with tight $\Theta(\Delta n)$ bound shown by
Dujmovi\'c, Kawarabayashi, Mohar and Wood, 2008.
<br />Much less is known about rectilinear crossing number. It is not bounded by
any function of the crossing number. We prove that graphs that exclude a
single-crossing graph as a minor have the rectilinear crossing number $O(\Delta
n)$. This dependence on $n$ and $\Delta$ is best possible. A single-crossing
graph is a graph whose crossing number is at most one. Thus the result applies
to $K_5$-minor-free graphs, for example. It also applies to bounded treewidth
graphs, since each family of bounded treewidth graphs excludes some fixed
planar graph as a minor. Prior to our work, the only bounded degree
minor-closed families known to have linear rectilinear crossing number were
bounded degree graphs of bounded treewidth (Wood and Telle, 2007), as well as,
bounded degree $K_{3,3}$-minor-free graphs (Dujmovi\'c, Kawarabayashi, Mohar
and Wood, 2008). In the case of bounded treewidth graphs, our $O(\Delta n)$
result is again tight and improves on the previous best known bound of
$O(\Delta^2 n)$ by Wood and Telle, 2007 (obtained for convex geometric
drawings).
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15053" title="Abstract">arXiv:2402.15053</a> (cross-list from stat.ML) [<a href="/pdf/2402.15053" title="Download PDF">pdf</a>, <a href="/ps/2402.15053" title="Download PostScript">ps</a>, <a href="/format/2402.15053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear Bayesian optimal experimental design using logarithmic Sobolev  inequalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+F">Fengyi Li</a>, 
<a href="/search/stat?searchtype=author&query=Belhadji%2C+A">Ayoub Belhadji</a>, 
<a href="/search/stat?searchtype=author&query=Marzouk%2C+Y">Youssef Marzouk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">We study the problem of selecting $k$ experiments from a larger candidate
pool, where the goal is to maximize mutual information (MI) between the
selected subset and the underlying parameters. Finding the exact solution is to
this combinatorial optimization problem is computationally costly, not only due
to the complexity of the combinatorial search but also the difficulty of
evaluating MI in nonlinear/non-Gaussian settings. We propose greedy approaches
based on new computationally inexpensive lower bounds for MI, constructed via
log-Sobolev inequalities. We demonstrate that our method outperforms random
selection strategies, Gaussian approximations, and nested Monte Carlo (NMC)
estimators of MI in various settings, including optimal design for nonlinear
models with non-additive noise.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15058" title="Abstract">arXiv:2402.15058</a> (cross-list from math.AT) [<a href="/pdf/2402.15058" title="Download PDF">pdf</a>, <a href="/format/2402.15058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixup Barcodes: Quantifying Geometric-Topological Interactions between  Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wagner%2C+H">Hubert Wagner</a>, 
<a href="/search/math?searchtype=author&query=Arustamyan%2C+N">Nickolas Arustamyan</a>, 
<a href="/search/math?searchtype=author&query=Wheeler%2C+M">Matthew Wheeler</a>, 
<a href="/search/math?searchtype=author&query=Bubenik%2C+P">Peter Bubenik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG); Machine Learning (cs.LG)

</div>
<p class="mathjax">We combine standard persistent homology with image persistent homology to
define a novel way of characterizing shapes and interactions between them. In
particular, we introduce: (1) a mixup barcode, which captures
geometric-topological interactions (mixup) between two point sets in arbitrary
dimension; (2) simple summary statistics, total mixup and total percentage
mixup, which quantify the complexity of the interactions as a single number;
(3) a software tool for playing with the above.
<br />As a proof of concept, we apply this tool to a problem arising from machine
learning. In particular, we study the disentanglement in embeddings of
different classes. The results suggest that topological mixup is a useful
method for characterizing interactions for low and high-dimensional data.
Compared to the typical usage of persistent homology, the new tool is sensitive
to the geometric locations of the topological features, which is often
desirable.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15095" title="Abstract">arXiv:2402.15095</a> (cross-list from math.ST) [<a href="/pdf/2402.15095" title="Download PDF">pdf</a>, <a href="/ps/2402.15095" title="Download PostScript">ps</a>, <a href="/format/2402.15095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Umeyama algorithm for matching correlated Gaussian geometric models  in the low-dimensional regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gong%2C+S">Shuyang Gong</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Z">Zhangsong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">Motivated by the problem of matching two correlated random geometric graphs,
we study the problem of matching two Gaussian geometric models correlated
through a latent node permutation. Specifically, given an unknown permutation
$\pi^*$ on $\{1,\ldots,n\}$ and given $n$ i.i.d. pairs of correlated Gaussian
vectors $\{X_{\pi^*(i)},Y_i\}$ in $\mathbb{R}^d$ with noise parameter $\sigma$,
we consider two types of (correlated) weighted complete graphs with edge
weights given by $A_{i,j}=\langle X_i,X_j \rangle$, $B_{i,j}=\langle Y_i,Y_j
\rangle$. The goal is to recover the hidden vertex correspondence $\pi^*$ based
on the observed matrices $A$ and $B$. For the low-dimensional regime where
$d=O(\log n)$, Wang, Wu, Xu, and Yolou [WWXY22+] established the information
thresholds for exact and almost exact recovery in matching correlated Gaussian
geometric models. They also conducted numerical experiments for the classical
Umeyama algorithm. In our work, we prove that this algorithm achieves exact
recovery of $\pi^*$ when the noise parameter $\sigma=o(d^{-3}n^{-2/d})$, and
almost exact recovery when $\sigma=o(d^{-3}n^{-1/d})$. Our results approach the
information thresholds up to a $\operatorname{poly}(d)$ factor in the
low-dimensional regime.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15115" title="Abstract">arXiv:2402.15115</a> (cross-list from stat.ML) [<a href="/pdf/2402.15115" title="Download PDF">pdf</a>, <a href="/format/2402.15115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-constrained polynomial chaos expansion for scientific machine  learning and uncertainty quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sharma%2C+H">Himanshu Sharma</a>, 
<a href="/search/stat?searchtype=author&query=Nov%C3%A1k%2C+L">Luk&#xe1;&#x161; Nov&#xe1;k</a>, 
<a href="/search/stat?searchtype=author&query=Shields%2C+M+D">Michael D. Shields</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">We present a novel physics-constrained polynomial chaos expansion as a
surrogate modeling method capable of performing both scientific machine
learning (SciML) and uncertainty quantification (UQ) tasks. The proposed method
possesses a unique capability: it seamlessly integrates SciML into UQ and vice
versa, which allows it to quantify the uncertainties in SciML tasks effectively
and leverage SciML for improved uncertainty assessment during UQ-related tasks.
The proposed surrogate model can effectively incorporate a variety of physical
constraints, such as governing partial differential equations (PDEs) with
associated initial and boundary conditions constraints, inequality-type
constraints (e.g., monotonicity, convexity, non-negativity, among others), and
additional a priori information in the training process to supplement limited
data. This ensures physically realistic predictions and significantly reduces
the need for expensive computational model evaluations to train the surrogate
model. Furthermore, the proposed method has a built-in uncertainty
quantification (UQ) feature to efficiently estimate output uncertainties. To
demonstrate the effectiveness of the proposed method, we apply it to a diverse
set of problems, including linear/non-linear PDEs with deterministic and
stochastic parameters, data-driven surrogate modeling of a complex physical
system, and UQ of a stochastic system with parameters modeled as random fields.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15208" title="Abstract">arXiv:2402.15208</a> (cross-list from math.OC) [<a href="/pdf/2402.15208" title="Download PDF">pdf</a>, <a href="/format/2402.15208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed-loop design for scalable performance of vehicular formations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hansson%2C+J">Jonas Hansson</a> (1), 
<a href="/search/math?searchtype=author&query=Tegling%2C+E">Emma Tegling</a> (1) ((1) Lund University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures. Submitted to IEEE Transactions on Control of Network Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper presents a novel control design for vehicular formations, which is
an alternative to the conventional second-order consensus protocol. The design
is motivated by the closed-loop system, which we construct as first-order
systems connected in series, and is therefore called serial consensus. The
serial consensus design will guarantee stability of the closed-loop system
under the minimum requirement of the underlying communication graphs each
containing a connected spanning tree -- something that is not true in general
for the conventional consensus protocols. Here, we show that the serial
consensus design also gives guarantees on the worst-case transient behavior of
the formation, which are independent of the number of vehicles and the
underlying graph structure. In particular this shows that the serial consensus
design can be used to guarantee string stability of the formation, and is
therefore suitable for directed formations. We show that it can be implemented
through message passing or measurements to neighbors at most two hops away. The
results are illustrated through numerical examples.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15213" title="Abstract">arXiv:2402.15213</a> (cross-list from stat.ML) [<a href="/pdf/2402.15213" title="Download PDF">pdf</a>, <a href="/format/2402.15213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Agnostic Regression: a machine learning method to validate  regression models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gorriz%2C+J+M">Juan M Gorriz</a>, 
<a href="/search/stat?searchtype=author&query=Ramirez%2C+J">J. Ramirez</a>, 
<a href="/search/stat?searchtype=author&query=Segovia%2C+F">F. Segovia</a>, 
<a href="/search/stat?searchtype=author&query=Martinez-Murcia%2C+F+J">F. J. Martinez-Murcia</a>, 
<a href="/search/stat?searchtype=author&query=Jim%C3%A9nez-Mesa%2C+C">C. Jim&#xe9;nez-Mesa</a>, 
<a href="/search/stat?searchtype=author&query=Suckling%2C+J">J. Suckling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Computation (stat.CO)

</div>
<p class="mathjax">Regression analysis is a central topic in statistical modeling, aiming to
estimate the relationships between a dependent variable, commonly referred to
as the response variable, and one or more independent variables, i.e.,
explanatory variables. Linear regression is by far the most popular method for
performing this task in several fields of research, such as prediction,
forecasting, or causal inference. Beyond various classical methods to solve
linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso
regressions - which are often the foundation for more advanced machine learning
(ML) techniques - the latter have been successfully applied in this scenario
without a formal definition of statistical significance. At most, permutation
or classical analyses based on empirical measures (e.g., residuals or accuracy)
have been conducted to reflect the greater ability of ML estimations for
detection. In this paper, we introduce a method, named Statistical Agnostic
Regression (SAR), for evaluating the statistical significance of an ML-based
linear regression based on concentration inequalities of the actual risk using
the analysis of the worst case. To achieve this goal, similar to the
classification problem, we define a threshold to establish that there is
sufficient evidence with a probability of at least 1-eta to conclude that there
is a linear relationship in the population between the explanatory (feature)
and the response (label) variables. Simulations in only two dimensions
demonstrate the ability of the proposed agnostic test to provide a similar
analysis of variance given by the classical $F$ test for the slope parameter.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15214" title="Abstract">arXiv:2402.15214</a> (cross-list from eess.AS) [<a href="/pdf/2402.15214" title="Download PDF">pdf</a>, <a href="/format/2402.15214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChildAugment: Data Augmentation Methods for Zero-Resource Children&#x27;s  Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Singh%2C+V+P">Vishwanath Pratap Singh</a>, 
<a href="/search/eess?searchtype=author&query=Sahidullah%2C+M">Md Sahidullah</a>, 
<a href="/search/eess?searchtype=author&query=Kinnunen%2C+T">Tomi Kinnunen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The following article has been accepted by The Journal of the Acoustical Society of America (JASA). After it is published, it will be found at <a href="https://pubs.aip.org/asa/jasa">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">The accuracy of modern automatic speaker verification (ASV) systems, when
trained exclusively on adult data, drops substantially when applied to
children's speech. The scarcity of children's speech corpora hinders
fine-tuning ASV systems for children's speech. Hence, there is a timely need to
explore more effective ways of reusing adults' speech data. One promising
approach is to align vocal-tract parameters between adults and children through
children-specific data augmentation, referred here to as ChildAugment.
Specifically, we modify the formant frequencies and formant bandwidths of adult
speech to emulate children's speech. The modified spectra are used to train
ECAPA-TDNN (emphasized channel attention, propagation, and aggregation in
time-delay neural network) recognizer for children. We compare ChildAugment
against various state-of-the-art data augmentation techniques for children's
ASV. We also extensively compare different scoring methods, including cosine
scoring, PLDA (probabilistic linear discriminant analysis), and NPLDA (neural
PLDA). We also propose a low-complexity weighted cosine score for extremely
low-resource children ASV. Our findings on the CSLU kids corpus indicate that
ChildAugment holds promise as a simple, acoustics-motivated approach, for
improving state-of-the-art deep learning based ASV for children. We achieve up
to 12.45% (boys) and 11.96% (girls) relative improvement over the baseline.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15232" title="Abstract">arXiv:2402.15232</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.15232" title="Download PDF">pdf</a>, <a href="/format/2402.15232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification of compact radio sources in the Galactic plane with  supervised machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Riggi%2C+S">S. Riggi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Umana%2C+G">G. Umana</a>, 
<a href="/search/astro-ph?searchtype=author&query=Trigilio%2C+C">C. Trigilio</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bordiu%2C+C">C. Bordiu</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bufano%2C+F">F. Bufano</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ingallinera%2C+A">A. Ingallinera</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cavallaro%2C+F">F. Cavallaro</a>, 
<a href="/search/astro-ph?searchtype=author&query=Gordon%2C+Y">Y. Gordon</a>, 
<a href="/search/astro-ph?searchtype=author&query=Norris%2C+R+P">R.P. Norris</a>, 
<a href="/search/astro-ph?searchtype=author&query=G%C3%BCrkan%2C+G">G. G&#xfc;rkan</a>, 
<a href="/search/astro-ph?searchtype=author&query=Leto%2C+P">P. Leto</a>, 
<a href="/search/astro-ph?searchtype=author&query=Buemi%2C+C">C. Buemi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Loru%2C+S">S. Loru</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hopkins%2C+A+M">A.M. Hopkins</a>, 
<a href="/search/astro-ph?searchtype=author&query=Filipovi%C4%87%2C+M+D">M.D. Filipovi&#x107;</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cecconello%2C+T">T. Cecconello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 15 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Generation of science-ready data from processed data products is one of the
major challenges in next-generation radio continuum surveys with the Square
Kilometre Array (SKA) and its precursors, due to the expected data volume and
the need to achieve a high degree of automated processing. Source extraction,
characterization, and classification are the major stages involved in this
process. In this work we focus on the classification of compact radio sources
in the Galactic plane using both radio and infrared images as inputs. To this
aim, we produced a curated dataset of ~20,000 images of compact sources of
different astronomical classes, obtained from past radio and infrared surveys,
and novel radio data from pilot surveys carried out with the Australian SKA
Pathfinder (ASKAP). Radio spectral index information was also obtained for a
subset of the data. We then trained two different classifiers on the produced
dataset. The first model uses gradient-boosted decision trees and is trained on
a set of pre-computed features derived from the data, which include
radio-infrared colour indices and the radio spectral index. The second model is
trained directly on multi-channel images, employing convolutional neural
networks. Using a completely supervised procedure, we obtained a high
classification accuracy (F1-score&gt;90%) for separating Galactic objects from the
extragalactic background. Individual class discrimination performances, ranging
from 60% to 75%, increased by 10% when adding far-infrared and spectral index
information, with extragalactic objects, PNe and HII regions identified with
higher accuracies. The implemented tools and trained models were publicly
released, and made available to the radioastronomical community for future
application on new radio data.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15233" title="Abstract">arXiv:2402.15233</a> (cross-list from quant-ph) [<a href="/pdf/2402.15233" title="Download PDF">pdf</a>, <a href="/format/2402.15233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the composable security of weak coin flipping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+J">Jiawei Wu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hu%2C+Y">Yanglin Hu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bansal%2C+A">Akshay Bansal</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tomamichel%2C+M">Marco Tomamichel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Weak coin flipping is a cryptographic primitive in which two mutually
distrustful parties generate a shared random bit to agree on a winner via
remote communication. While a stand-alone secure weak coin flipping protocol
can be constructed from noiseless communication channels, its composability has
not been explored. In this work, we demonstrate that no weak coin flipping
protocol can be abstracted into a black box resource with composable security.
Despite this, we also establish the overall stand-alone security of weak coin
flipping protocols under sequential composition.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15246" title="Abstract">arXiv:2402.15246</a> (cross-list from eess.IV) [<a href="/pdf/2402.15246" title="Download PDF">pdf</a>, <a href="/ps/2402.15246" title="Download PostScript">ps</a>, <a href="/format/2402.15246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Bee Colony optimization of Deep Convolutional Neural Networks  in the context of Biomedical Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Martin%2C+A+G">Adri Gomez Martin</a>, 
<a href="/search/eess?searchtype=author&query=del+Cerro%2C+C+F">Carlos Fernandez del Cerro</a>, 
<a href="/search/eess?searchtype=author&query=Garcia%2C+M+A">Monica Abella Garcia</a>, 
<a href="/search/eess?searchtype=author&query=Menendez%2C+M+D">Manuel Desco Menendez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Most efforts in Computer Vision focus on natural images or artwork, which
differ significantly both in size and contents from the kind of data biomedical
image processing deals with. Thus, Transfer Learning models often prove
themselves suboptimal for these tasks, even after manual finetuning. The
development of architectures from scratch is oftentimes unfeasible due to the
vastness of the hyperparameter space and a shortage of time, computational
resources and Deep Learning experts in most biomedical research laboratories.
An alternative to manually defining the models is the use of Neuroevolution,
which employs metaheuristic techniques to optimize Deep Learning architectures.
However, many algorithms proposed in the neuroevolutive literature are either
too unreliable or limited to a small, predefined region of the hyperparameter
space. To overcome these shortcomings, we propose the Chimera Algorithm, a
novel, hybrid neuroevolutive algorithm that integrates the Artificial Bee
Colony Algorithm with Evolutionary Computation tools to generate models from
scratch, as well as to refine a given previous architecture to better fit the
task at hand. The Chimera Algorithm has been validated with two datasets of
natural and medical images, producing models that surpassed the performance of
those coming from Transfer Learning.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15258" title="Abstract">arXiv:2402.15258</a> (cross-list from eess.AS) [<a href="/pdf/2402.15258" title="Download PDF">pdf</a>, <a href="/format/2402.15258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High Resolution Guitar Transcription via Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Riley%2C+X">Xavier Riley</a>, 
<a href="/search/eess?searchtype=author&query=Edwards%2C+D">Drew Edwards</a>, 
<a href="/search/eess?searchtype=author&query=Dixon%2C+S">Simon Dixon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Automatic music transcription (AMT) has achieved high accuracy for piano due
to the availability of large, high-quality datasets such as MAESTRO and MAPS,
but comparable datasets are not yet available for other instruments. In recent
work, however, it has been demonstrated that aligning scores to transcription
model activations can produce high quality AMT training data for instruments
other than piano. Focusing on the guitar, we refine this approach to training
on score data using a dataset of commercially available score-audio pairs. We
propose the use of a high-resolution piano transcription model to train a new
guitar transcription model. The resulting model obtains state-of-the-art
transcription results on GuitarSet in a zero-shot context, improving on
previously published methods.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15282" title="Abstract">arXiv:2402.15282</a> (cross-list from quant-ph) [<a href="/pdf/2402.15282" title="Download PDF">pdf</a>, <a href="/ps/2402.15282" title="Download PostScript">ps</a>, <a href="/format/2402.15282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dimension Independent Disentanglers from Unentanglement and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Jeronimo%2C+F+G">Fernando G. Jeronimo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+P">Pei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Quantum entanglement is a key enabling ingredient in diverse applications.
However, the presence of unwanted adversarial entanglement also poses
challenges in many applications.
<br />In this paper, we explore methods to "break" quantum entanglement.
Specifically, we construct a dimension-independent k-partite disentangler
(like) channel from bipartite unentangled input. We show: For every $d,\ell\ge
k$, there is an efficient channel $\Lambda: \mathbb{C}^{d\ell} \otimes
\mathbb{C}^{d\ell} \to \mathbb{C}^{dk}$ such that for every bipartite separable
state $\rho_1\otimes \rho_2$, the output $\Lambda(\rho_1\otimes\rho_2)$ is
close to a k-partite separable state. Concretely, for some distribution $\mu$
on states from $\mathbb{C}^d$, $$ \left\|\Lambda(\rho_1 \otimes \rho_2) - \int
| \psi \rangle \langle \psi |^{\otimes k} d\mu(\psi)\right\|_1 \le \tilde O
\left(\left(\frac{k^{3}}{\ell}\right)^{1/4}\right). $$ Moreover, $\Lambda(|
\psi \rangle \langle \psi |^{\otimes \ell}\otimes | \psi \rangle \langle \psi
|^{\otimes \ell}) = | \psi \rangle \langle \psi |^{\otimes k}$. Without the
bipartite unentanglement assumption, the above bound is conjectured to be
impossible.
<br />Leveraging our disentanglers, we show that unentangled quantum proofs of
almost general real amplitudes capture NEXP, greatly relaxing the nonnegative
amplitudes assumption in the recent work of QMA^+(2)=NEXP. Specifically, our
findings show that to capture NEXP, it suffices to have unentangled proofs of
the form $| \psi \rangle = \sqrt{a} | \psi_+ \rangle + \sqrt{1-a} | \psi_-
\rangle$ where $| \psi_+ \rangle$ has non-negative amplitudes, $| \psi_-
\rangle$ only has negative amplitudes and $| a-(1-a) | \ge 1/poly(n)$ with $a
\in [0,1]$. Additionally, we present a protocol achieving an almost largest
possible gap before obtaining QMA^R(k)=NEXP$, namely, a 1/poly(n) additive
improvement to the gap results in this equality.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15285" title="Abstract">arXiv:2402.15285</a> (cross-list from stat.ML) [<a href="/pdf/2402.15285" title="Download PDF">pdf</a>, <a href="/format/2402.15285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Modelling with Tensor Train approximations of  Hamilton--Jacobi--Bellman equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sommer%2C+D">David Sommer</a>, 
<a href="/search/stat?searchtype=author&query=Gruhlke%2C+R">Robert Gruhlke</a>, 
<a href="/search/stat?searchtype=author&query=Kirstein%2C+M">Max Kirstein</a>, 
<a href="/search/stat?searchtype=author&query=Eigel%2C+M">Martin Eigel</a>, 
<a href="/search/stat?searchtype=author&query=Schillings%2C+C">Claudia Schillings</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">Sampling from probability densities is a common challenge in fields such as
Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in
particular, the use of reverse-time diffusion processes depending on the
log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling
tool. In Berner et al. [2022] the authors point out that these log-densities
can be obtained by solution of a \textit{Hamilton-Jacobi-Bellman} (HJB)
equation known from stochastic optimal control. While this HJB equation is
usually treated with indirect methods such as policy iteration and unsupervised
training of black-box architectures like Neural Networks, we propose instead to
solve the HJB equation by direct time integration, using compressed polynomials
represented in the Tensor Train (TT) format for spatial discretization.
Crucially, this method is sample-free, agnostic to normalization constants and
can avoid the curse of dimensionality due to the TT compression. We provide a
complete derivation of the HJB equation's action on Tensor Train polynomials
and demonstrate the performance of the proposed time-step-, rank- and
degree-adaptive integration method on a nonlinear sampling task in 20
dimensions.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15288" title="Abstract">arXiv:2402.15288</a> (cross-list from eess.SP) [<a href="/pdf/2402.15288" title="Download PDF">pdf</a>, <a href="/format/2402.15288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time FPGA Demonstrator of ANN-Based Equalization for Optical  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ney%2C+J">Jonas Ney</a>, 
<a href="/search/eess?searchtype=author&query=Matalla%2C+P">Patrick Matalla</a>, 
<a href="/search/eess?searchtype=author&query=Lauinger%2C+V">Vincent Lauinger</a>, 
<a href="/search/eess?searchtype=author&query=Schmalen%2C+L">Laurent Schmalen</a>, 
<a href="/search/eess?searchtype=author&query=Randel%2C+S">Sebastian Randel</a>, 
<a href="/search/eess?searchtype=author&query=Wehn%2C+N">Norbert Wehn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and to be presented as demonstrator at the IEEE International Conference on Machine Learning for Communication and Networking (ICMLCN) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we present a high-throughput field programmable gate array
(FPGA) demonstrator of an artificial neural network (ANN)-based equalizer. The
equalization is performed and illustrated in real-time for a 30 GBd, two-level
pulse amplitude modulation (PAM2) optical communication system.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15333" title="Abstract">arXiv:2402.15333</a> (cross-list from quant-ph) [<a href="/pdf/2402.15333" title="Download PDF">pdf</a>, <a href="/format/2402.15333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quantum-Classical Collaborative Training Architecture Based on Quantum  State Fidelity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=L%27Abbate%2C+R">Ryan L&#x27;Abbate</a>, 
<a href="/search/quant-ph?searchtype=author&query=D%27Onofrio%2C+A">Anthony D&#x27;Onofrio Jr.</a>, 
<a href="/search/quant-ph?searchtype=author&query=Stein%2C+S">Samuel Stein</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+S+Y">Samuel Yen-Chi Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+J">Juntao Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mao%2C+Y">Ying Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Quantum Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements have highlighted the limitations of current quantum
systems, particularly the restricted number of qubits available on near-term
quantum devices. This constraint greatly inhibits the range of applications
that can leverage quantum computers. Moreover, as the available qubits
increase, the computational complexity grows exponentially, posing additional
challenges. Consequently, there is an urgent need to use qubits efficiently and
mitigate both present limitations and future complexities. To address this,
existing quantum applications attempt to integrate classical and quantum
systems in a hybrid framework. In this study, we concentrate on quantum deep
learning and introduce a collaborative classical-quantum architecture called
co-TenQu. The classical component employs a tensor network for compression and
feature extraction, enabling higher-dimensional data to be encoded onto logical
quantum circuits with limited qubits. On the quantum side, we propose a
quantum-state-fidelity-based evaluation function to iteratively train the
network through a feedback loop between the two sides. co-TenQu has been
implemented and evaluated with both simulators and the IBM-Q platform. Compared
to state-of-the-art approaches, co-TenQu enhances a classical deep neural
network by up to 41.72% in a fair setting. Additionally, it outperforms other
quantum-based methods by up to 1.9 times and achieves similar accuracy while
utilizing 70.59% fewer qubits.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15335" title="Abstract">arXiv:2402.15335</a> (cross-list from eess.IV) [<a href="/pdf/2402.15335" title="Download PDF">pdf</a>, <a href="/format/2402.15335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Rank Representations Meets Deep Unfolding: A Generalized and  Interpretable Network for Hyperspectral Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+C">Chenyu Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+B">Bing Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Hong%2C+D">Danfeng Hong</a>, 
<a href="/search/eess?searchtype=author&query=Yao%2C+J">Jing Yao</a>, 
<a href="/search/eess?searchtype=author&query=Chanussot%2C+J">Jocelyn Chanussot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Current hyperspectral anomaly detection (HAD) benchmark datasets suffer from
low resolution, simple background, and small size of the detection data. These
factors also limit the performance of the well-known low-rank representation
(LRR) models in terms of robustness on the separation of background and target
features and the reliance on manual parameter selection. To this end, we build
a new set of HAD benchmark datasets for improving the robustness of the HAD
algorithm in complex scenarios, AIR-HAD for short. Accordingly, we propose a
generalized and interpretable HAD network by deeply unfolding a
dictionary-learnable LLR model, named LRR-Net$^+$, which is capable of
spectrally decoupling the background structure and object properties in a more
generalized fashion and eliminating the bias introduced by vital interference
targets concurrently. In addition, LRR-Net$^+$ integrates the solution process
of the Alternating Direction Method of Multipliers (ADMM) optimizer with the
deep network, guiding its search process and imparting a level of
interpretability to parameter optimization. Additionally, the integration of
physical models with DL techniques eliminates the need for manual parameter
tuning. The manually tuned parameters are seamlessly transformed into trainable
parameters for deep neural networks, facilitating a more efficient and
automated optimization process. Extensive experiments conducted on the AIR-HAD
dataset show the superiority of our LRR-Net$^+$ in terms of detection
performance and generalization ability, compared to top-performing rivals.
Furthermore, the compilable codes and our AIR-HAD benchmark datasets in this
paper will be made available freely and openly at
\url{https://sites.google.com/view/danfeng-hong}.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15344" title="Abstract">arXiv:2402.15344</a> (cross-list from stat.ML) [<a href="/pdf/2402.15344" title="Download PDF">pdf</a>, <a href="/format/2402.15344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iteration and Stochastic First-order Oracle Complexities of Stochastic  Gradient Descent using Constant and Decaying Learning Rates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Imaizumi%2C+K">Kento Imaizumi</a>, 
<a href="/search/stat?searchtype=author&query=Iiduka%2C+H">Hideaki Iiduka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The latest version was updated on Feb. 23. arXiv admin note: text overlap with <a href="/abs/2307.13831">arXiv:2307.13831</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The performance of stochastic gradient descent (SGD), which is the simplest
first-order optimizer for training deep neural networks, depends on not only
the learning rate but also the batch size. They both affect the number of
iterations and the stochastic first-order oracle (SFO) complexity needed for
training. In particular, the previous numerical results indicated that, for SGD
using a constant learning rate, the number of iterations needed for training
decreases when the batch size increases, and the SFO complexity needed for
training is minimized at a critical batch size and that it increases once the
batch size exceeds that size. Here, we study the relationship between batch
size and the iteration and SFO complexities needed for nonconvex optimization
in deep learning with SGD using constant or decaying learning rates and show
that SGD using the critical batch size minimizes the SFO complexity. We also
provide numerical comparisons of SGD with the existing first-order optimizers
and show the usefulness of SGD using a critical batch size. Moreover, we show
that measured critical batch sizes are close to the sizes estimated from our
theoretical results.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15353" title="Abstract">arXiv:2402.15353</a> (cross-list from eess.IV) [<a href="/pdf/2402.15353" title="Download PDF">pdf</a>, <a href="/format/2402.15353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Background Denoising for Ptychography via Wigner Distribution  Deconvolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Melnyk%2C+O">Oleh Melnyk</a>, 
<a href="/search/eess?searchtype=author&query=R%C3%B6mer%2C+P">Patricia R&#xf6;mer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Information Theory (cs.IT); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Ptychography is a computational imaging technique that aims to reconstruct
the object of interest from a set of diffraction patterns. Each of these is
obtained by a localized illumination of the object, which is shifted after each
illumination to cover its whole domain. As in the resulting measurements the
phase information is lost, ptychography gives rise to solving a phase retrieval
problem. In this work, we consider ptychographic measurements corrupted with
background noise, a type of additive noise that is independent of the shift,
i.e., it is the same for all diffraction patterns. Two algorithms are provided,
for arbitrary objects and for so-called phase objects that do not absorb the
light but only scatter it. For the second type, a uniqueness of reconstruction
is established for almost every object. Our approach is based on the Wigner
Distribution Deconvolution, which lifts the object to a higher-dimensional
matrix space where the recovery can be reformulated as a linear problem.
Background noise only affects a few equations of the linear system that are
therefore discarded. The lost information is then restored using redundancy in
the higher-dimensional space.
<br />Keywords: phase retrieval, ptychography, background noise, Wigner
Distribution Deconvolution, uniqueness of reconstruction.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15360" title="Abstract">arXiv:2402.15360</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.15360" title="Download PDF">pdf</a>, <a href="/format/2402.15360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Navine%2C+A+K">Amanda K. Navine</a>, 
<a href="/search/q-bio?searchtype=author&query=Denton%2C+T">Tom Denton</a>, 
<a href="/search/q-bio?searchtype=author&query=Weldy%2C+M+J">Matthew J. Weldy</a>, 
<a href="/search/q-bio?searchtype=author&query=Hart%2C+P+J">Patrick J. Hart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, 3 tables; submitted to Frontiers in Bird Science; Our Hawaiian PAM dataset and classifier scores, as well as annotation information for the three study species, can be found on Zenodo at <a href="https://doi.org/10.5281/zenodo.10581530.">this https URL</a> The fully annotated Powdermill dataset assembled by Chronister et al. that was used in this study is available at <a href="https://doi.org/10.1002/ecy.3329">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Passive acoustic monitoring (PAM) studies generate thousands of hours of
audio, which may be used to monitor specific animal populations, conduct broad
biodiversity surveys, detect threats such as poachers, and more. Machine
learning classifiers for species identification are increasingly being used to
process the vast amount of audio generated by bioacoustic surveys, expediting
analysis and increasing the utility of PAM as a management tool. In common
practice, a threshold is applied to classifier output scores, and scores above
the threshold are aggregated into a detection count. The choice of threshold
produces biased counts of vocalizations, which are subject to false
positive/negative rates that may vary across subsets of the dataset. In this
work, we advocate for directly estimating call density: The proportion of
detection windows containing the target vocalization, regardless of classifier
score. Our approach targets a desirable ecological estimator and provides a
more rigorous grounding for identifying the core problems caused by
distribution shifts -- when the defining characteristics of the data
distribution change -- and designing strategies to mitigate them. We propose a
validation scheme for estimating call density in a body of data and obtain,
through Bayesian reasoning, probability distributions of confidence scores for
both the positive and negative classes. We use these distributions to predict
site-level densities, which may be subject to distribution shifts. We test our
proposed methods on a real-world study of Hawaiian birds and provide simulation
results leveraging existing fully annotated datasets, demonstrating robustness
to variations in call density and classifier model quality.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15365" title="Abstract">arXiv:2402.15365</a> (cross-list from stat.ML) [<a href="/pdf/2402.15365" title="Download PDF">pdf</a>, <a href="/format/2402.15365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient semi-supervised inference for logistic regression under  case-control studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Quan%2C+Z">Zhuojun Quan</a>, 
<a href="/search/stat?searchtype=author&query=Lin%2C+Y">Yuanyuan Lin</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+K">Kani Chen</a>, 
<a href="/search/stat?searchtype=author&query=Yu%2C+W">Wen Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Semi-supervised learning has received increasingly attention in statistics
and machine learning. In semi-supervised learning settings, a labeled data set
with both outcomes and covariates and an unlabeled data set with covariates
only are collected. We consider an inference problem in semi-supervised
settings where the outcome in the labeled data is binary and the labeled data
is collected by case-control sampling. Case-control sampling is an effective
sampling scheme for alleviating imbalance structure in binary data. Under the
logistic model assumption, case-control data can still provide consistent
estimator for the slope parameter of the regression model. However, the
intercept parameter is not identifiable. Consequently, the marginal case
proportion cannot be estimated from case-control data. We find out that with
the availability of the unlabeled data, the intercept parameter can be
identified in semi-supervised learning setting. We construct the likelihood
function of the observed labeled and unlabeled data and obtain the maximum
likelihood estimator via an iterative algorithm. The proposed estimator is
shown to be consistent, asymptotically normal, and semiparametrically
efficient. Extensive simulation studies are conducted to show the finite sample
performance of the proposed method. The results imply that the unlabeled data
not only helps to identify the intercept but also improves the estimation
efficiency of the slope parameter. Meanwhile, the marginal case proportion can
be estimated accurately by the proposed method.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15366" title="Abstract">arXiv:2402.15366</a> (cross-list from physics.ins-det) [<a href="/pdf/2402.15366" title="Download PDF">pdf</a>, <a href="/format/2402.15366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Portable acceleration of CMS computing workflows with coprocessors as a  service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=CMS+Collaboration">CMS Collaboration</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Computing and Software for Big Science. All figures and tables can be found at <a href="http://cms-results.web.cern.ch/cms-results/public-results/publications/MLG-23-001">this http URL</a> (CMS Public Pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">Computing demands for large scientific experiments, such as the CMS
experiment at the CERN LHC, will increase dramatically in the next decades. To
complement the future performance increases of software running on central
processing units (CPUs), explorations of coprocessor usage in data processing
hold great potential and interest. Coprocessors are a class of computer
processors that supplement CPUs, often improving the execution of certain
functions due to architectural design choices. We explore the approach of
Services for Optimized Network Inference on Coprocessors (SONIC) and study the
deployment of this as-a-service approach in large-scale data processing. In the
studies, we take a data processing workflow of the CMS experiment and run the
main workflow on CPUs, while offloading several machine learning (ML) inference
tasks onto either remote or local coprocessors, specifically graphics
processing units (GPUs). With experiments performed at Google Cloud, the Purdue
Tier-2 computing center, and combinations of the two, we demonstrate the
acceleration of these ML algorithms individually on coprocessors and the
corresponding throughput improvement for the entire workflow. This approach can
be easily generalized to different types of coprocessors and deployed on local
CPUs without decreasing the throughput performance. We emphasize that the SONIC
approach enables high coprocessor usage and enables the portability to run
workflows on different types of coprocessors.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15409" title="Abstract">arXiv:2402.15409</a> (cross-list from stat.ML) [<a href="/pdf/2402.15409" title="Download PDF">pdf</a>, <a href="/format/2402.15409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lasso with Latents: Efficient Estimation, Covariate Rescaling, and  Computational-Statistical Gaps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kelner%2C+J">Jonathan Kelner</a>, 
<a href="/search/stat?searchtype=author&query=Koehler%2C+F">Frederic Koehler</a>, 
<a href="/search/stat?searchtype=author&query=Meka%2C+R">Raghu Meka</a>, 
<a href="/search/stat?searchtype=author&query=Rohatgi%2C+D">Dhruv Rohatgi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">It is well-known that the statistical performance of Lasso can suffer
significantly when the covariates of interest have strong correlations. In
particular, the prediction error of Lasso becomes much worse than
computationally inefficient alternatives like Best Subset Selection. Due to a
large conjectured computational-statistical tradeoff in the problem of sparse
linear regression, it may be impossible to close this gap in general.
<br />In this work, we propose a natural sparse linear regression setting where
strong correlations between covariates arise from unobserved latent variables.
In this setting, we analyze the problem caused by strong correlations and
design a surprisingly simple fix. While Lasso with standard normalization of
covariates fails, there exists a heterogeneous scaling of the covariates with
which Lasso will suddenly obtain strong provable guarantees for estimation.
Moreover, we design a simple, efficient procedure for computing such a "smart
scaling."
<br />The sample complexity of the resulting "rescaled Lasso" algorithm incurs (in
the worst case) quadratic dependence on the sparsity of the underlying signal.
While this dependence is not information-theoretically necessary, we give
evidence that it is optimal among the class of polynomial-time algorithms, via
the method of low-degree polynomials. This argument reveals a new connection
between sparse linear regression and a special version of sparse PCA with a
near-critical negative spike. The latter problem can be thought of as a
real-valued analogue of learning a sparse parity. Using it, we also establish
the first computational-statistical gap for the closely related problem of
learning a Gaussian Graphical Model.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15418" title="Abstract">arXiv:2402.15418</a> (cross-list from econ.TH) [<a href="/pdf/2402.15418" title="Download PDF">pdf</a>, <a href="/ps/2402.15418" title="Download PostScript">ps</a>, <a href="/format/2402.15418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reputational Algorithm Aversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Weitzner%2C+G">Gregory Weitzner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">People are often reluctant to incorporate information produced by algorithms
into their decisions, a phenomenon called "algorithm aversion". This paper
shows how algorithm aversion arises when the choice to follow an algorithm
conveys information about a human's ability. I develop a model in which workers
make forecasts of a random outcome based on their own private information and
an algorithm's signal. Low-skill workers receive worse information than the
algorithm and hence should always follow the algorithm's signal, while
high-skill workers receive better information than the algorithm and should
sometimes override it. However, due to reputational concerns, low-skill workers
inefficiently override the algorithm to increase the likelihood they are
perceived as high-skill. The model provides a fully rational microfoundation
for algorithm aversion that aligns with the broad concern that AI systems will
displace many types of workers.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.15432" title="Abstract">arXiv:2402.15432</a> (cross-list from math.ST) [<a href="/pdf/2402.15432" title="Download PDF">pdf</a>, <a href="/ps/2402.15432" title="Download PostScript">ps</a>, <a href="/format/2402.15432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering  Error in Sub-Exponential Mixture Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dreveton%2C+M">Maximilien Dreveton</a>, 
<a href="/search/math?searchtype=author&query=G%C3%B6zeten%2C+A">Alperen G&#xf6;zeten</a>, 
<a href="/search/math?searchtype=author&query=Grossglauser%2C+M">Matthias Grossglauser</a>, 
<a href="/search/math?searchtype=author&query=Thiran%2C+P">Patrick Thiran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Clustering is a pivotal challenge in unsupervised machine learning and is
often investigated through the lens of mixture models. The optimal error rate
for recovering cluster labels in Gaussian and sub-Gaussian mixture models
involves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as
Lloyd's algorithm, attain this optimal error rate. In this paper, we first
establish a universal lower bound for the error rate in clustering any mixture
model, expressed through a Chernoff divergence, a more versatile measure of
model information than signal-to-noise ratios. We then demonstrate that
iterative algorithms attain this lower bound in mixture models with
sub-exponential tails, notably emphasizing location-scale mixtures featuring
Laplace-distributed errors. Additionally, for datasets better modelled by
Poisson or Negative Binomial mixtures, we study mixture models whose
distributions belong to an exponential family. In such mixtures, we establish
that Bregman hard clustering, a variant of Lloyd's algorithm employing a
Bregman divergence, is rate optimal.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Mon, 26 Feb 24</h3>
<dl>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1710.00095" title="Abstract">arXiv:1710.00095</a> (replaced) [<a href="/pdf/1710.00095" title="Download PDF">pdf</a>, <a href="/format/1710.00095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User-friendly guarantees for the Langevin Monte Carlo with inaccurate  gradient
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dalalyan%2C+A+S">Arnak S. Dalalyan</a>, 
<a href="/search/math?searchtype=author&query=Karagulyan%2C+A+G">Avetik G. Karagulyan</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Stochastic Processes and their Applications, Volume 129, Issue 12,
  December 2019, Pages 5278-5311
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Probability (math.PR); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1712.00960" title="Abstract">arXiv:1712.00960</a> (replaced) [<a href="/pdf/1712.00960" title="Download PDF">pdf</a>, <a href="/format/1712.00960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FSSD: Feature Fusion Single Shot Multibox Detector
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zuoxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fuqiang Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> update info
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1905.00517" title="Abstract">arXiv:1905.00517</a> (replaced) [<a href="/pdf/1905.00517" title="Download PDF">pdf</a>, <a href="/format/1905.00517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Abstractions to Grounded Languages for Robust Coordination of Task  Planning Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A short version of this paper appears as an extended abstract at AAMAS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.12801" title="Abstract">arXiv:2003.12801</a> (replaced) [<a href="/pdf/2003.12801" title="Download PDF">pdf</a>, <a href="/ps/2003.12801" title="Download PostScript">ps</a>, <a href="/format/2003.12801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probability error bounds for approximation of functions in reproducing  kernel Hilbert spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aydin%2C+A+D">Ata Deniz Aydin</a>, 
<a href="/search/math?searchtype=author&query=Gheondea%2C+A">Aurelian Gheondea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Function Spaces, vol. 2021, Article ID 6617774, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Information Theory (cs.IT); Functional Analysis (math.FA)

</div>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.04722" title="Abstract">arXiv:2005.04722</a> (replaced) [<a href="/pdf/2005.04722" title="Download PDF">pdf</a>, <a href="/ps/2005.04722" title="Download PostScript">ps</a>, <a href="/format/2005.04722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic IFC Theorems for Free!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Algehed%2C+M">Maximilian Algehed</a>, 
<a href="/search/cs?searchtype=author&query=Bernardy%2C+J">Jean-Philippe Bernardy</a>, 
<a href="/search/cs?searchtype=author&query=Hritcu%2C+C">Catalin Hritcu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CSF 2021 final version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Cryptography and Security (cs.CR); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.04363" title="Abstract">arXiv:2102.04363</a> (replaced) [<a href="/pdf/2102.04363" title="Download PDF">pdf</a>, <a href="/format/2102.04363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Data-Driven Optimization with Noisy Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Van+Parys%2C+B+P+G">Bart P.G. Van Parys</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.10561" title="Abstract">arXiv:2104.10561</a> (replaced) [<a href="/pdf/2104.10561" title="Download PDF">pdf</a>, <a href="/format/2104.10561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Turning Federated Learning Systems Into Covert Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Costa%2C+G">Gabriele Costa</a>, 
<a href="/search/cs?searchtype=author&query=Pinelli%2C+F">Fabio Pinelli</a>, 
<a href="/search/cs?searchtype=author&query=Soderi%2C+S">Simone Soderi</a>, 
<a href="/search/cs?searchtype=author&query=Tolomei%2C+G">Gabriele Tolomei</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, vol. 10, pp. 130642-130656, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.08620" title="Abstract">arXiv:2105.08620</a> (replaced) [<a href="/pdf/2105.08620" title="Download PDF">pdf</a>, <a href="/format/2105.08620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Examples Detection with Bayesian Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+Y">Yao Li</a>, 
<a href="/search/stat?searchtype=author&query=Tang%2C+T">Tongyi Tang</a>, 
<a href="/search/stat?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+T+C+M">Thomas C. M. Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.06257" title="Abstract">arXiv:2110.06257</a> (replaced) [<a href="/pdf/2110.06257" title="Download PDF">pdf</a>, <a href="/format/2110.06257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Discovery from Conditionally Stationary Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balsells-Rodas%2C+C">Carles Balsells-Rodas</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+R">Ruibo Tu</a>, 
<a href="/search/cs?searchtype=author&query=Kjellstrom%2C+H">Hedvig Kjellstrom</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingzhen Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.12612" title="Abstract">arXiv:2201.12612</a> (replaced) [<a href="/e-print/2201.12612" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Non-Cooperative Perfect Information Semi-Markov Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakshi%2C+K+G">K. G. Bakshi</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S">S. Sinha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The result in this paper is not correct
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.05650" title="Abstract">arXiv:2202.05650</a> (replaced) [<a href="/pdf/2202.05650" title="Download PDF">pdf</a>, <a href="/format/2202.05650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bernstein Flows for Flexible Posteriors in Variational Bayes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=D%C3%BCrr%2C+O">Oliver D&#xfc;rr</a>, 
<a href="/search/stat?searchtype=author&query=H%C3%B6rling%2C+S">Stephan H&#xf6;rling</a>, 
<a href="/search/stat?searchtype=author&query=Dold%2C+D">Daniel Dold</a>, 
<a href="/search/stat?searchtype=author&query=Kovylov%2C+I">Ivonne Kovylov</a>, 
<a href="/search/stat?searchtype=author&query=Sick%2C+B">Beate Sick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.13847" title="Abstract">arXiv:2203.13847</a> (replaced) [<a href="/pdf/2203.13847" title="Download PDF">pdf</a>, <a href="/format/2203.13847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cluster Algebras: Network Science and Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dechant%2C+P">Pierre-Philippe Dechant</a>, 
<a href="/search/math?searchtype=author&query=He%2C+Y">Yang-Hui He</a>, 
<a href="/search/math?searchtype=author&query=Heyes%2C+E">Elli Heyes</a>, 
<a href="/search/math?searchtype=author&query=Hirst%2C+E">Edward Hirst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 27 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J.Comput.Algebra 8 (2023) 100008
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Machine Learning (cs.LG); High Energy Physics - Theory (hep-th); Algebraic Geometry (math.AG)

</div>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.00994" title="Abstract">arXiv:2204.00994</a> (replaced) [<a href="/pdf/2204.00994" title="Download PDF">pdf</a>, <a href="/format/2204.00994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of the Planewave Approximations for Quantum Incommensurate  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+T">Ting Wang</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+H">Huajie Chen</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+A">Aihui Zhou</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Y">Yuzhi Zhou</a>, 
<a href="/search/math?searchtype=author&query=Massatt%2C+D">Daniel Massatt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.02201" title="Abstract">arXiv:2204.02201</a> (replaced) [<a href="/pdf/2204.02201" title="Download PDF">pdf</a>, <a href="/format/2204.02201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the size distribution of Levenshtein balls with radius one
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Geyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.13704" title="Abstract">arXiv:2204.13704</a> (replaced) [<a href="/pdf/2204.13704" title="Download PDF">pdf</a>, <a href="/format/2204.13704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperbolic Hierarchical Knowledge Graph Embeddings for Link Prediction  in Low Dimensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wenjie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+F">Fulan Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.04277" title="Abstract">arXiv:2206.04277</a> (replaced) [<a href="/pdf/2206.04277" title="Download PDF">pdf</a>, <a href="/format/2206.04277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Hypothesis Transfer Learning of Functional Linear Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lin%2C+H">Haotian Lin</a>, 
<a href="/search/stat?searchtype=author&query=Reimherr%2C+M">Matthew Reimherr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The results are extended to functional GLM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.08141" title="Abstract">arXiv:2206.08141</a> (replaced) [<a href="/pdf/2206.08141" title="Download PDF">pdf</a>, <a href="/ps/2206.08141" title="Download PostScript">ps</a>, <a href="/format/2206.08141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> i-FlatCam: A 253 FPS, 91.49 $&#x3bc;$J/Frame Ultra-Compact Intelligent  Lensless Camera for Real-Time and Efficient Eye Tracking in VR/AR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziyun Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yonggan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chaojian Li</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+C">Cheng Wan</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+H">Haoran You</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+X">Xu Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Boominathan%2C+V">Vivek Boominathan</a>, 
<a href="/search/cs?searchtype=author&query=Veeraraghavan%2C+A">Ashok Veeraraghavan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yingyan Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by VLSI 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.11143" title="Abstract">arXiv:2206.11143</a> (replaced) [<a href="/pdf/2206.11143" title="Download PDF">pdf</a>, <a href="/ps/2206.11143" title="Download PostScript">ps</a>, <a href="/format/2206.11143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair and Efficient Allocations Without Obvious Manipulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Psomas%2C+A">Alexandros Psomas</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Paritosh Verma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.06342" title="Abstract">arXiv:2207.06342</a> (replaced) [<a href="/pdf/2207.06342" title="Download PDF">pdf</a>, <a href="/format/2207.06342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient error and variance estimation for randomized matrix  computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Epperly%2C+E+N">Ethan N. Epperly</a>, 
<a href="/search/math?searchtype=author&query=Tropp%2C+J+A">Joel A. Tropp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 10 figures, 13 pages of supplementary material. v4: added additional supplementary material
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM Journal on Scientific Computing, 46(1), A508-A528 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.00541" title="Abstract">arXiv:2208.00541</a> (replaced) [<a href="/pdf/2208.00541" title="Download PDF">pdf</a>, <a href="/ps/2208.00541" title="Download PostScript">ps</a>, <a href="/format/2208.00541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the reconstruction of unknown driving forces from low-mode  observations in the 2D Navier-Stokes Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Martinez%2C+V+R">Vincent R. Martinez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, introduction and appendix expanded, main results reworded for clarity, additional remarks and references added, accepted to Proc. R. Soc. Edinb. A: Math
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09225" title="Abstract">arXiv:2208.09225</a> (replaced) [<a href="/pdf/2208.09225" title="Download PDF">pdf</a>, <a href="/format/2208.09225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FP8 Quantization: The Power of the Exponent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuzmin%2C+A">Andrey Kuzmin</a>, 
<a href="/search/cs?searchtype=author&query=Van+Baalen%2C+M">Mart Van Baalen</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuwei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Nagel%2C+M">Markus Nagel</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jorn Peters</a>, 
<a href="/search/cs?searchtype=author&query=Blankevoort%2C+T">Tijmen Blankevoort</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07924" title="Abstract">arXiv:2209.07924</a> (replaced) [<a href="/pdf/2209.07924" title="Download PDF">pdf</a>, <a href="/format/2209.07924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNNInterpreter: A Probabilistic Generative Model-Level Explanation for  Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Han-Wei Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.11924" title="Abstract">arXiv:2209.11924</a> (replaced) [<a href="/pdf/2209.11924" title="Download PDF">pdf</a>, <a href="/format/2209.11924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interventional Causal Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ahuja%2C+K">Kartik Ahuja</a>, 
<a href="/search/stat?searchtype=author&query=Mahajan%2C+D">Divyat Mahajan</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+Y">Yixin Wang</a>, 
<a href="/search/stat?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.06891" title="Abstract">arXiv:2210.06891</a> (replaced) [<a href="/pdf/2210.06891" title="Download PDF">pdf</a>, <a href="/format/2210.06891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Design for Multi-Channel Imaging via Task-Driven Feature  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blumberg%2C+S+B">Stefano B. Blumberg</a>, 
<a href="/search/cs?searchtype=author&query=Slator%2C+P+J">Paddy J. Slator</a>, 
<a href="/search/cs?searchtype=author&query=Alexander%2C+D+C">Daniel C. Alexander</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted In: International Conference of Learning Representations (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11327" title="Abstract">arXiv:2210.11327</a> (replaced) [<a href="/pdf/2210.11327" title="Download PDF">pdf</a>, <a href="/format/2210.11327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Data Quality with Training Dynamics of Gradient Boosting  Decision Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ponti%2C+M+A">Moacir Antonelli Ponti</a>, 
<a href="/search/cs?searchtype=author&query=de+Angelis+Oliveira%2C+L">Lucas de Angelis Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Esteban%2C+M">Mathias Esteban</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+V">Valentina Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Rom%C3%A1n%2C+J+M">Juan Mart&#xed;n Rom&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Argerich%2C+L">Luis Argerich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.12596" title="Abstract">arXiv:2210.12596</a> (replaced) [<a href="/pdf/2210.12596" title="Download PDF">pdf</a>, <a href="/format/2210.12596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DMODE: Differential Monocular Object Distance Estimation Module without  Class Specific Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agand%2C+P">Pedram Agand</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Michael Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.16311" title="Abstract">arXiv:2210.16311</a> (replaced) [<a href="/pdf/2210.16311" title="Download PDF">pdf</a>, <a href="/format/2210.16311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneous off-the-grid learning of mixtures issued from a continuous  dictionary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Butucea%2C+C">Cristina Butucea</a> (CREST, FAIRPLAY), 
<a href="/search/stat?searchtype=author&query=Delmas%2C+J">Jean-Fran&#xe7;ois Delmas</a> (CERMICS), 
<a href="/search/stat?searchtype=author&query=Dutfoy%2C+A">Anne Dutfoy</a> (EDF R&amp;D), 
<a href="/search/stat?searchtype=author&query=Hardy%2C+C">Cl&#xe9;ment Hardy</a> (CERMICS, EDF R&amp;D)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04175" title="Abstract">arXiv:2211.04175</a> (replaced) [<a href="/pdf/2211.04175" title="Download PDF">pdf</a>, <a href="/format/2211.04175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Centaur: Federated Learning for Constrained Edge Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+F">Fan Mo</a>, 
<a href="/search/cs?searchtype=author&query=Malekzadeh%2C+M">Mohammad Malekzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S">Soumyajit Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Kawsar%2C+F">Fahim Kawsar</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+A">Akhil Mathur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2023 Workshop on Machine Learning for IoT: Datasets, Perception, and Understanding
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14568" title="Abstract">arXiv:2211.14568</a> (replaced) [<a href="/pdf/2211.14568" title="Download PDF">pdf</a>, <a href="/format/2211.14568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BeGin: Extensive Benchmark Scenarios and An Easy-to-use Framework for  Graph Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Jihoon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Shinhwan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taehyung Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+H">Heechan Moon</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+K">Kijung Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16943" title="Abstract">arXiv:2211.16943</a> (replaced) [<a href="/pdf/2211.16943" title="Download PDF">pdf</a>, <a href="/format/2211.16943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Properties of Quantum Systems with Conditional Generative  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+H">Haoxiang Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Weber%2C+M">Maurice Weber</a>, 
<a href="/search/quant-ph?searchtype=author&query=Izaac%2C+J">Josh Izaac</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lin%2C+C+Y">Cedric Yen-Yu Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 14 figures, 5 pages appendix. Open-source code is available at <a href="https://github.com/PennyLaneAI/generative-quantum-states">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03614" title="Abstract">arXiv:2212.03614</a> (replaced) [<a href="/pdf/2212.03614" title="Download PDF">pdf</a>, <a href="/format/2212.03614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mathematical theory for mass lumping and its generalization with  applications to isogeometric analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Voet%2C+Y">Yannis Voet</a>, 
<a href="/search/math?searchtype=author&query=Sande%2C+E">Espen Sande</a>, 
<a href="/search/math?searchtype=author&query=Buffa%2C+A">Annalisa Buffa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 24 figures. Accepted manuscript
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Methods in Applied Mechanics and Engineering 410C (2023)
  116033
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09400" title="Abstract">arXiv:2212.09400</a> (replaced) [<a href="/pdf/2212.09400" title="Download PDF">pdf</a>, <a href="/format/2212.09400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Medical Knowledge Graph QA for Drug-Drug Interaction Prediction based on  Multi-hop Machine Reading Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Feng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jian-Cheng Ni</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10233" title="Abstract">arXiv:2212.10233</a> (replaced) [<a href="/pdf/2212.10233" title="Download PDF">pdf</a>, <a href="/format/2212.10233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-trained Language Models for Keyphrase Generation: A Thorough  Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+W+U">Wasi Uddin Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report. The contents are published in two separate papers in EMNLP 2023 (<a href="/abs/2310.06374">arXiv:2310.06374</a>) and LREC-COLING 2024 (<a href="/abs/2402.14052">arXiv:2402.14052</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00068" title="Abstract">arXiv:2301.00068</a> (replaced) [<a href="/pdf/2301.00068" title="Download PDF">pdf</a>, <a href="/format/2301.00068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inconsistencies in Masked Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Young%2C+T">Tom Young</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunan Chen</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00995" title="Abstract">arXiv:2301.00995</a> (replaced) [<a href="/pdf/2301.00995" title="Download PDF">pdf</a>, <a href="/ps/2301.00995" title="Download PostScript">ps</a>, <a href="/format/2301.00995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unconditional Quantum Advantage for Sampling with Shallow Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Watts%2C+A+B">Adam Bene Watts</a>, 
<a href="/search/quant-ph?searchtype=author&query=Parham%2C+N">Natalie Parham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages, 12 figures. The new version improves the result with proofs in the appendices, and includes a new proof overview in the introduction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03553" title="Abstract">arXiv:2301.03553</a> (replaced) [<a href="/pdf/2301.03553" title="Download PDF">pdf</a>, <a href="/format/2301.03553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedDebug: Systematic Debugging for Federated Learning Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gill%2C+W">Waris Gill</a>, 
<a href="/search/cs?searchtype=author&query=Anwar%2C+A">Ali Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Gulzar%2C+M+A">Muhammad Ali Gulzar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICSE 2023. Link <a href="https://ieeexplore.ieee.org/document/10172839">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In 2023 IEEE/ACM 45th International Conference on Software
  Engineering (ICSE) (pp. 456-789). IEEE (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computer Vision and Pattern Recognition (cs.CV); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11328" title="Abstract">arXiv:2301.11328</a> (replaced) [<a href="/pdf/2301.11328" title="Download PDF">pdf</a>, <a href="/format/2301.11328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cell-Free ISAC MIMO Systems: Joint Sensing and Communication Beamforming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Demirhan%2C+U">Umut Demirhan</a>, 
<a href="/search/cs?searchtype=author&query=Alkhateeb%2C+A">Ahmed Alkhateeb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12292" title="Abstract">arXiv:2301.12292</a> (replaced) [<a href="/pdf/2301.12292" title="Download PDF">pdf</a>, <a href="/format/2301.12292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot causal learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nilforoshan%2C+H">Hamed Nilforoshan</a>, 
<a href="/search/cs?searchtype=author&query=Moor%2C+M">Michael Moor</a>, 
<a href="/search/cs?searchtype=author&query=Roohani%2C+Y">Yusuf Roohani</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yining Chen</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0urina%2C+A">Anja &#x160;urina</a>, 
<a href="/search/cs?searchtype=author&query=Yasunaga%2C+M">Michihiro Yasunaga</a>, 
<a href="/search/cs?searchtype=author&query=Oblak%2C+S">Sara Oblak</a>, 
<a href="/search/cs?searchtype=author&query=Leskovec%2C+J">Jure Leskovec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13748" title="Abstract">arXiv:2301.13748</a> (replaced) [<a href="/pdf/2301.13748" title="Download PDF">pdf</a>, <a href="/format/2301.13748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Archetypal Analysis++: Rethinking the Initialization Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mair%2C+S">Sebastian Mair</a>, 
<a href="/search/cs?searchtype=author&query=Sj%C3%B6lund%2C+J">Jens Sj&#xf6;lund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 17 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00288" title="Abstract">arXiv:2302.00288</a> (replaced) [<a href="/pdf/2302.00288" title="Download PDF">pdf</a>, <a href="/format/2302.00288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoderEval: A Benchmark of Pragmatic Code Generation with Generative  Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+B">Bo Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+D">Dezhi Ran</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuchi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+G">Guangtai Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tao Xie</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICSE (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00834" title="Abstract">arXiv:2302.00834</a> (replaced) [<a href="/pdf/2302.00834" title="Download PDF">pdf</a>, <a href="/ps/2302.00834" title="Download PostScript">ps</a>, <a href="/format/2302.00834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at  Irregularly Spaced Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siegel%2C+J+W">Jonathan W. Siegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01790" title="Abstract">arXiv:2302.01790</a> (replaced) [<a href="/pdf/2302.01790" title="Download PDF">pdf</a>, <a href="/format/2302.01790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding metric-related pitfalls in image analysis validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reinke%2C+A">Annika Reinke</a>, 
<a href="/search/cs?searchtype=author&query=Tizabi%2C+M+D">Minu D. Tizabi</a>, 
<a href="/search/cs?searchtype=author&query=Baumgartner%2C+M">Michael Baumgartner</a>, 
<a href="/search/cs?searchtype=author&query=Eisenmann%2C+M">Matthias Eisenmann</a>, 
<a href="/search/cs?searchtype=author&query=Heckmann-N%C3%B6tzel%2C+D">Doreen Heckmann-N&#xf6;tzel</a>, 
<a href="/search/cs?searchtype=author&query=Kavur%2C+A+E">A. Emre Kavur</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A4dsch%2C+T">Tim R&#xe4;dsch</a>, 
<a href="/search/cs?searchtype=author&query=Sudre%2C+C+H">Carole H. Sudre</a>, 
<a href="/search/cs?searchtype=author&query=Acion%2C+L">Laura Acion</a>, 
<a href="/search/cs?searchtype=author&query=Antonelli%2C+M">Michela Antonelli</a>, 
<a href="/search/cs?searchtype=author&query=Arbel%2C+T">Tal Arbel</a>, 
<a href="/search/cs?searchtype=author&query=Bakas%2C+S">Spyridon Bakas</a>, 
<a href="/search/cs?searchtype=author&query=Benis%2C+A">Arriel Benis</a>, 
<a href="/search/cs?searchtype=author&query=Blaschko%2C+M">Matthew Blaschko</a>, 
<a href="/search/cs?searchtype=author&query=Buettner%2C+F">Florian Buettner</a>, 
<a href="/search/cs?searchtype=author&query=Cardoso%2C+M+J">M. Jorge Cardoso</a>, 
<a href="/search/cs?searchtype=author&query=Cheplygina%2C+V">Veronika Cheplygina</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianxu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Christodoulou%2C+E">Evangelia Christodoulou</a>, 
<a href="/search/cs?searchtype=author&query=Cimini%2C+B+A">Beth A. Cimini</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+G+S">Gary S. Collins</a>, 
<a href="/search/cs?searchtype=author&query=Farahani%2C+K">Keyvan Farahani</a>, 
<a href="/search/cs?searchtype=author&query=Ferrer%2C+L">Luciana Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Galdran%2C+A">Adrian Galdran</a>, 
<a href="/search/cs?searchtype=author&query=van+Ginneken%2C+B">Bram van Ginneken</a>, 
<a href="/search/cs?searchtype=author&query=Glocker%2C+B">Ben Glocker</a>, 
<a href="/search/cs?searchtype=author&query=Godau%2C+P">Patrick Godau</a>, 
<a href="/search/cs?searchtype=author&query=Haase%2C+R">Robert Haase</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+D+A">Daniel A. Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+M+M">Michael M. Hoffman</a>, 
<a href="/search/cs?searchtype=author&query=Huisman%2C+M">Merel Huisman</a>, 
<a href="/search/cs?searchtype=author&query=Isensee%2C+F">Fabian Isensee</a>, 
<a href="/search/cs?searchtype=author&query=Jannin%2C+P">Pierre Jannin</a>, 
<a href="/search/cs?searchtype=author&query=Kahn%2C+C+E">Charles E. Kahn</a>, 
<a href="/search/cs?searchtype=author&query=Kainmueller%2C+D">Dagmar Kainmueller</a>, 
<a href="/search/cs?searchtype=author&query=Kainz%2C+B">Bernhard Kainz</a>, 
<a href="/search/cs?searchtype=author&query=Karargyris%2C+A">Alexandros Karargyris</a>, 
<a href="/search/cs?searchtype=author&query=Karthikesalingam%2C+A">Alan Karthikesalingam</a>, 
<a href="/search/cs?searchtype=author&query=Kenngott%2C+H">Hannes Kenngott</a>, 
<a href="/search/cs?searchtype=author&query=Kleesiek%2C+J">Jens Kleesiek</a>, 
<a href="/search/cs?searchtype=author&query=Kofler%2C+F">Florian Kofler</a>, 
<a href="/search/cs?searchtype=author&query=Kooi%2C+T">Thijs Kooi</a>, 
<a href="/search/cs?searchtype=author&query=Kopp-Schneider%2C+A">Annette Kopp-Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Kozubek%2C+M">Michal Kozubek</a>, 
<a href="/search/cs?searchtype=author&query=Kreshuk%2C+A">Anna Kreshuk</a>, 
<a href="/search/cs?searchtype=author&query=Kurc%2C+T">Tahsin Kurc</a>, 
<a href="/search/cs?searchtype=author&query=Landman%2C+B+A">Bennett A. Landman</a>,  et al. (31 additional authors not shown)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Shared first authors: Annika Reinke and Minu D. Tizabi; shared senior authors: Lena Maier-Hein and Paul F. J\"ager. Published in Nature Methods. arXiv admin note: text overlap with <a href="/abs/2206.01653">arXiv:2206.01653</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Nature methods, 1-13 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06573" title="Abstract">arXiv:2302.06573</a> (replaced) [<a href="/pdf/2302.06573" title="Download PDF">pdf</a>, <a href="/ps/2302.06573" title="Download PostScript">ps</a>, <a href="/format/2302.06573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Propagation of weakly advantageous mutations in cancer cell population
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Polanski%2C+A">Andrzej Polanski</a>, 
<a href="/search/q-bio?searchtype=author&query=Kania%2C+M">Mateusz Kania</a>, 
<a href="/search/q-bio?searchtype=author&query=Gil%2C+J">Jaros&#x142;aw Gil</a>, 
<a href="/search/q-bio?searchtype=author&query=%C5%81abaj%2C+W">Wojciech &#x141;abaj</a>, 
<a href="/search/q-bio?searchtype=author&query=Lach%2C+E">Ewa Lach</a>, 
<a href="/search/q-bio?searchtype=author&query=Szcz%C4%99sna%2C+A">Agnieszka Szcz&#x119;sna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Removed Figure 6, corrections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07433" title="Abstract">arXiv:2302.07433</a> (replaced) [<a href="/pdf/2302.07433" title="Download PDF">pdf</a>, <a href="/format/2302.07433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Global LiDAR Localization: Challenges, Advances and Open  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Huan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuecheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Sha Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xieyuanli Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+R">Rong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Stachniss%2C+C">Cyrill Stachniss</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by International Journal of Computer Vision (IJCV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02223" title="Abstract">arXiv:2303.02223</a> (replaced) [<a href="/pdf/2303.02223" title="Download PDF">pdf</a>, <a href="/ps/2303.02223" title="Download PostScript">ps</a>, <a href="/format/2303.02223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Selection with Annealing for Forecasting Financial Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pabuccu%2C+H">Hakan Pabuccu</a>, 
<a href="/search/cs?searchtype=author&query=Barbu%2C+A">Adrian Barbu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 1 figures and 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Finance (q-fin.CP)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03870" title="Abstract">arXiv:2303.03870</a> (replaced) [<a href="/pdf/2303.03870" title="Download PDF">pdf</a>, <a href="/format/2303.03870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Aneesh Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Paranjape%2C+M">Manas Paranjape</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+U">Uttaran Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Bera%2C+A">Aniket Bera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures, 3 tables. To appear as part of the proceedings of the 38th Annual AAAI Conference on Artificial Intelligence, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Graphics (cs.GR); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04857" title="Abstract">arXiv:2303.04857</a> (replaced) [<a href="/pdf/2303.04857" title="Download PDF">pdf</a>, <a href="/format/2303.04857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking Symmetries Leads to Diverse Quadrupedal Gaits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jiayu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Z">Zhenyu Gan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05161" title="Abstract">arXiv:2303.05161</a> (replaced) [<a href="/pdf/2303.05161" title="Download PDF">pdf</a>, <a href="/format/2303.05161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inversion dynamics of class manifolds in deep learning reveals tradeoffs  underlying generalisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ciceri%2C+S">Simone Ciceri</a>, 
<a href="/search/cs?searchtype=author&query=Cassani%2C+L">Lorenzo Cassani</a>, 
<a href="/search/cs?searchtype=author&query=Osella%2C+M">Matteo Osella</a>, 
<a href="/search/cs?searchtype=author&query=Rotondo%2C+P">Pietro Rotondo</a>, 
<a href="/search/cs?searchtype=author&query=Valle%2C+F">Filippo Valle</a>, 
<a href="/search/cs?searchtype=author&query=Gherardi%2C+M">Marco Gherardi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Nature Machine Intelligence, vol 6, 40-47 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05279" title="Abstract">arXiv:2303.05279</a> (replaced) [<a href="/pdf/2303.05279" title="Download PDF">pdf</a>, <a href="/format/2303.05279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can large language models build causal graphs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+S">Stephanie Long</a>, 
<a href="/search/cs?searchtype=author&query=Schuster%2C+T">Tibor Schuster</a>, 
<a href="/search/cs?searchtype=author&query=Pich%C3%A9%2C+A">Alexandre Pich&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Peer reviewed and accepted for presentation at the Causal Machine Learning for Real-World Impact Workshop (CML4Impact) at NeuRIPs2022 Fixed author list
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06373" title="Abstract">arXiv:2303.06373</a> (replaced) [<a href="/pdf/2303.06373" title="Download PDF">pdf</a>, <a href="/format/2303.06373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recursive Generalization Transformer for Image Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Linghe Kong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024. Code is available at <a href="https://github.com/zhengchen1999/RGT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08028" title="Abstract">arXiv:2303.08028</a> (replaced) [<a href="/pdf/2303.08028" title="Download PDF">pdf</a>, <a href="/format/2303.08028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EdgeServe: A Streaming System for Decentralized Model Serving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaowang%2C+T">Ted Shaowang</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+S">Sanjay Krishnan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12243" title="Abstract">arXiv:2303.12243</a> (replaced) [<a href="/pdf/2303.12243" title="Download PDF">pdf</a>, <a href="/format/2303.12243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Sum Games between Large-Population Teams: Reachability-based  Analysis under Mean-Field Sharing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guan%2C+Y">Yue Guan</a>, 
<a href="/search/eess?searchtype=author&query=Afshari%2C+M">Mohammad Afshari</a>, 
<a href="/search/eess?searchtype=author&query=Tsiotras%2C+P">Panagiotis Tsiotras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in AAAI 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 38th Annual AAAI Conference on Artificial Intelligence 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15103" title="Abstract">arXiv:2303.15103</a> (replaced) [<a href="/pdf/2303.15103" title="Download PDF">pdf</a>, <a href="/format/2303.15103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Learning Is Spectral Clustering On Similarity Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhiquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingqin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yang Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024; We express our gratitude to the anonymous reviewers for their valuable feedback
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16749" title="Abstract">arXiv:2303.16749</a> (replaced) [<a href="/pdf/2303.16749" title="Download PDF">pdf</a>, <a href="/format/2303.16749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Code Generation by Training with Natural Language Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Scheurer%2C+J">J&#xe9;r&#xe9;my Scheurer</a>, 
<a href="/search/cs?searchtype=author&query=Korbak%2C+T">Tomasz Korbak</a>, 
<a href="/search/cs?searchtype=author&query=Campos%2C+J+A">Jon Ander Campos</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+J+S">Jun Shern Chan</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in (and superceded by) TMLR: <a href="https://openreview.net/forum?id=xo3hI5MwvU">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16755" title="Abstract">arXiv:2303.16755</a> (replaced) [<a href="/pdf/2303.16755" title="Download PDF">pdf</a>, <a href="/format/2303.16755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Language Models with Language Feedback at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scheurer%2C+J">J&#xe9;r&#xe9;my Scheurer</a>, 
<a href="/search/cs?searchtype=author&query=Campos%2C+J+A">Jon Ander Campos</a>, 
<a href="/search/cs?searchtype=author&query=Korbak%2C+T">Tomasz Korbak</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+J+S">Jun Shern Chan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in TMLR: <a href="https://openreview.net/forum?id=xo3hI5MwvU">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00468" title="Abstract">arXiv:2304.00468</a> (replaced) [<a href="/pdf/2304.00468" title="Download PDF">pdf</a>, <a href="/ps/2304.00468" title="Download PostScript">ps</a>, <a href="/format/2304.00468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Words that Matter: The Impact of Negative Words on News Sentiment and  Stock Market Index
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Wonseong Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 9 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00737" title="Abstract">arXiv:2304.00737</a> (replaced) [<a href="/pdf/2304.00737" title="Download PDF">pdf</a>, <a href="/ps/2304.00737" title="Download PostScript">ps</a>, <a href="/format/2304.00737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SparDL: Distributed Deep Learning Training with Efficient Sparse  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Minjun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yichen Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuren Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yunjun Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04558" title="Abstract">arXiv:2304.04558</a> (replaced) [<a href="/pdf/2304.04558" title="Download PDF">pdf</a>, <a href="/format/2304.04558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShakingBot: Dynamic Manipulation for Bagging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+N">Ningquan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruhan He</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lianqing Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manipulating bag through robots to bagging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05949" title="Abstract">arXiv:2304.05949</a> (replaced) [<a href="/pdf/2304.05949" title="Download PDF">pdf</a>, <a href="/format/2304.05949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic  inference and learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Singh%2C+N+S">Nihal Sanjay Singh</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kobayashi%2C+K">Keito Kobayashi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Cao%2C+Q">Qixuan Cao</a>, 
<a href="/search/cond-mat?searchtype=author&query=Selcuk%2C+K">Kemal Selcuk</a>, 
<a href="/search/cond-mat?searchtype=author&query=Hu%2C+T">Tianrui Hu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Niazi%2C+S">Shaila Niazi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Aadit%2C+N+A">Navid Anjum Aadit</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kanai%2C+S">Shun Kanai</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ohno%2C+H">Hideo Ohno</a>, 
<a href="/search/cond-mat?searchtype=author&query=Fukami%2C+S">Shunsuke Fukami</a>, 
<a href="/search/cond-mat?searchtype=author&query=Camsari%2C+K+Y">Kerem Y. Camsari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mesoscale and Nanoscale Physics (cond-mat.mes-hall)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06325" title="Abstract">arXiv:2304.06325</a> (replaced) [<a href="/pdf/2304.06325" title="Download PDF">pdf</a>, <a href="/ps/2304.06325" title="Download PostScript">ps</a>, <a href="/format/2304.06325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Sign Quantum Messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Barhoush%2C+M">Mohammed Barhoush</a>, 
<a href="/search/quant-ph?searchtype=author&query=Salvail%2C+L">Louis Salvail</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06827" title="Abstract">arXiv:2304.06827</a> (replaced) [<a href="/pdf/2304.06827" title="Download PDF">pdf</a>, <a href="/format/2304.06827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reachability Analysis Using Hybrid Zonotopes and Functional  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Siefert%2C+J+A">Jacob A. Siefert</a>, 
<a href="/search/eess?searchtype=author&query=Bird%2C+T+J">Trevor J. Bird</a>, 
<a href="/search/eess?searchtype=author&query=Thompson%2C+A+F">Andrew F. Thompson</a>, 
<a href="/search/eess?searchtype=author&query=Glunt%2C+J+J">Jonah J. Glunt</a>, 
<a href="/search/eess?searchtype=author&query=Koeln%2C+J+P">Justin P. Koeln</a>, 
<a href="/search/eess?searchtype=author&query=Jain%2C+N">Neera Jain</a>, 
<a href="/search/eess?searchtype=author&query=Pangborn%2C+H+C">Herschel C. Pangborn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08177" title="Abstract">arXiv:2304.08177</a> (replaced) [<a href="/pdf/2304.08177" title="Download PDF">pdf</a>, <a href="/format/2304.08177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yiming Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziqing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xin Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11850" title="Abstract">arXiv:2304.11850</a> (replaced) [<a href="/pdf/2304.11850" title="Download PDF">pdf</a>, <a href="/format/2304.11850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open Continuum Robotics -- One Actuation Module to Create them All
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grassmann%2C+R+M">Reinhard M. Grassmann</a>, 
<a href="/search/cs?searchtype=author&query=Shentu%2C+C">Chengnan Shentu</a>, 
<a href="/search/cs?searchtype=author&query=Hamoda%2C+T">Taqi Hamoda</a>, 
<a href="/search/cs?searchtype=author&query=Dewi%2C+P+T">Puspita Triana Dewi</a>, 
<a href="/search/cs?searchtype=author&query=Burgner-Kahrs%2C+J">Jessica Burgner-Kahrs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Frontiers in Robotics and AI; 9 pages, 8 figures; We provide the hardware alongside assembly instructions as well as software with our Open Continuum Robotics Project <a href="https://www.opencontinuumrobotics.com">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13332" title="Abstract">arXiv:2304.13332</a> (replaced) [<a href="/pdf/2304.13332" title="Download PDF">pdf</a>, <a href="/ps/2304.13332" title="Download PostScript">ps</a>, <a href="/format/2304.13332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-based convergence rates of greedy algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+Y">Yuwen Li</a>, 
<a href="/search/math?searchtype=author&query=Siegel%2C+J">Jonathan Siegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, no figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mathematical Models and Methods in Applied Sciences (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14169" title="Abstract">arXiv:2304.14169</a> (replaced) [<a href="/pdf/2304.14169" title="Download PDF">pdf</a>, <a href="/ps/2304.14169" title="Download PostScript">ps</a>, <a href="/format/2304.14169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tractability of sampling recovery on unweighted function classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Krieg%2C+D">David Krieg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03237" title="Abstract">arXiv:2305.03237</a> (replaced) [<a href="/pdf/2305.03237" title="Download PDF">pdf</a>, <a href="/format/2305.03237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Out-of-Domain Intent Detection Considering Multi-Turn Dialogue Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lang%2C+H">Hao Lang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yinhe Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+B">Binyuan Hui</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> COLING2024 Long Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03954" title="Abstract">arXiv:2305.03954</a> (replaced) [<a href="/pdf/2305.03954" title="Download PDF">pdf</a>, <a href="/format/2305.03954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Action Embeddings for Off-Policy Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cief%2C+M">Matej Cief</a>, 
<a href="/search/cs?searchtype=author&query=Golebiowski%2C+J">Jacek Golebiowski</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+P">Philipp Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Abedjan%2C+Z">Ziawasch Abedjan</a>, 
<a href="/search/cs?searchtype=author&query=Bekasov%2C+A">Artur Bekasov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04203" title="Abstract">arXiv:2305.04203</a> (replaced) [<a href="/pdf/2305.04203" title="Download PDF">pdf</a>, <a href="/format/2305.04203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Power of Open Set : A New Perspective for Open-Set Noisy  Label Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+W">Wenhai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinrui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+M">Ming-Kun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shao-Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sheng-Jun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Songcan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05448" title="Abstract">arXiv:2305.05448</a> (replaced) [<a href="/pdf/2305.05448" title="Download PDF">pdf</a>, <a href="/format/2305.05448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Implicit Regularization via Weight Normalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+H">Hung-Hsu Chou</a>, 
<a href="/search/cs?searchtype=author&query=Rauhut%2C+H">Holger Rauhut</a>, 
<a href="/search/cs?searchtype=author&query=Ward%2C+R">Rachel Ward</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07164" title="Abstract">arXiv:2305.07164</a> (replaced) [<a href="/pdf/2305.07164" title="Download PDF">pdf</a>, <a href="/format/2305.07164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-Augmented Online Packet Scheduling with Deadlines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Ya-Chun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Stein%2C+C">Clifford Stein</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hao-Ting Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09200" title="Abstract">arXiv:2305.09200</a> (replaced) [<a href="/pdf/2305.09200" title="Download PDF">pdf</a>, <a href="/ps/2305.09200" title="Download PostScript">ps</a>, <a href="/format/2305.09200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representing states in iterated belief revision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liberatore%2C+P">Paolo Liberatore</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09651" title="Abstract">arXiv:2305.09651</a> (replaced) [<a href="/pdf/2305.09651" title="Download PDF">pdf</a>, <a href="/format/2305.09651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tailoring Instructions to Student&#x27;s Learning Levels Boosts Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuxin Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zihan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xingjian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chun Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACL 2023, main conference. Code available at <a href="https://github.com/twinkle0331/LGTM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14791" title="Abstract">arXiv:2305.14791</a> (replaced) [<a href="/pdf/2305.14791" title="Download PDF">pdf</a>, <a href="/format/2305.14791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Large Language Models for Counterfactual Generation: An  Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mayi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+X">Xin Miao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+T">Tieyun Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to LREC-COLING 2024, camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14828" title="Abstract">arXiv:2305.14828</a> (replaced) [<a href="/pdf/2305.14828" title="Download PDF">pdf</a>, <a href="/format/2305.14828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Few-shot Entity Recognition in Document Images: A Graph Neural  Network Approach Robust to Image Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+P">Prashant Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yangkun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15060" title="Abstract">arXiv:2305.15060</a> (replaced) [<a href="/pdf/2305.15060" title="Download PDF">pdf</a>, <a href="/format/2305.15060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who Wrote this Code? Watermarking for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+T">Taehyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Seokhee Hong</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+J">Jaewoo Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+I">Ilgee Hong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hwaran Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jamin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gunhee Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17443" title="Abstract">arXiv:2305.17443</a> (replaced) [<a href="/pdf/2305.17443" title="Download PDF">pdf</a>, <a href="/format/2305.17443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilience in Platoons of Cooperative Heterogeneous Vehicles:  Self-organization Strategies and Provably-correct Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+D">Di Liu</a>, 
<a href="/search/eess?searchtype=author&query=Mair%2C+S">Sebastian Mair</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+K">Kang Yang</a>, 
<a href="/search/eess?searchtype=author&query=Baldi%2C+S">Simone Baldi</a>, 
<a href="/search/eess?searchtype=author&query=Frasca%2C+P">Paolo Frasca</a>, 
<a href="/search/eess?searchtype=author&query=Althoff%2C+M">Matthias Althoff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17510" title="Abstract">arXiv:2305.17510</a> (replaced) [<a href="/pdf/2305.17510" title="Download PDF">pdf</a>, <a href="/format/2305.17510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Quantum-Classical Approach based on the Hadamard Transform for  the Convolutional Layer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Hongyi Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Atici%2C+S">Salih Atici</a>, 
<a href="/search/cs?searchtype=author&query=Cetin%2C+A+E">Ahmet Enis Cetin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at International Conference on Machine Learning (ICML), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00074" title="Abstract">arXiv:2306.00074</a> (replaced) [<a href="/pdf/2306.00074" title="Download PDF">pdf</a>, <a href="/format/2306.00074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Aligned Calibration for AI-Assisted Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benz%2C+N+L+C">Nina L. Corvelo Benz</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+M+G">Manuel Gomez Rodriguez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02117" title="Abstract">arXiv:2306.02117</a> (replaced) [<a href="/pdf/2306.02117" title="Download PDF">pdf</a>, <a href="/format/2306.02117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oversmoothing: A Nightmare for Graph Contrastive Learning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jintang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wangbin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruofan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuchang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report; Code available at <a href="https://github.com/EdisonLeeeee/BlockGCL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03828" title="Abstract">arXiv:2306.03828</a> (replaced) [<a href="/pdf/2306.03828" title="Download PDF">pdf</a>, <a href="/format/2306.03828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arango%2C+S+P">Sebastian Pineda Arango</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+F">Fabio Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Kadra%2C+A">Arlind Kadra</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>, 
<a href="/search/cs?searchtype=author&query=Grabocka%2C+J">Josif Grabocka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04366" title="Abstract">arXiv:2306.04366</a> (replaced) [<a href="/pdf/2306.04366" title="Download PDF">pdf</a>, <a href="/format/2306.04366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Worker Recruitment in Collaborative Mobile Crowdsourcing: A  Graph Neural Network Trust Evaluation Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Z">Zhongwei Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yingjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+P">Peiyong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Sai%2C+A+M+V+V">Akshita Maradapu Vera Venkata Sai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaowei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+C">Chaocan Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+X">Xiangrong Tong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhipeng Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04695" title="Abstract">arXiv:2306.04695</a> (replaced) [<a href="/pdf/2306.04695" title="Download PDF">pdf</a>, <a href="/format/2306.04695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+M">Maitreya Patel</a>, 
<a href="/search/cs?searchtype=author&query=Gokhale%2C+T">Tejas Gokhale</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yezhou Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI'24 | Project page: <a href="https://conceptbed.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04778" title="Abstract">arXiv:2306.04778</a> (replaced) [<a href="/pdf/2306.04778" title="Download PDF">pdf</a>, <a href="/format/2306.04778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Evaluate Behavioral Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=d%27Eon%2C+G">Greg d&#x27;Eon</a>, 
<a href="/search/cs?searchtype=author&query=Greenwood%2C+S">Sophie Greenwood</a>, 
<a href="/search/cs?searchtype=author&query=Leyton-Brown%2C+K">Kevin Leyton-Brown</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+J+R">James R. Wright</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages (7 pages body + references and appendix). To appear at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07258" title="Abstract">arXiv:2306.07258</a> (replaced) [<a href="/pdf/2306.07258" title="Download PDF">pdf</a>, <a href="/format/2306.07258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Input Decoupling of Lagrangian Systems via Coordinate Transformation:  General Characterization and its Application to Soft Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pustina%2C+P">Pietro Pustina</a>, 
<a href="/search/cs?searchtype=author&query=Della+Santina%2C+C">Cosimo Della Santina</a>, 
<a href="/search/cs?searchtype=author&query=Boyer%2C+F">Fr&#xe9;d&#xe9;ric Boyer</a>, 
<a href="/search/cs?searchtype=author&query=De+Luca%2C+A">Alessandro De Luca</a>, 
<a href="/search/cs?searchtype=author&query=Renda%2C+F">Federico Renda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY); Classical Physics (physics.class-ph)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09541" title="Abstract">arXiv:2306.09541</a> (replaced) [<a href="/pdf/2306.09541" title="Download PDF">pdf</a>, <a href="/format/2306.09541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Validating AI-Generated Code with Live Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferdowsi%2C+K">Kasra Ferdowsi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruanqianqian Huang</a>, 
<a href="/search/cs?searchtype=author&query=James%2C+M+B">Michael B. James</a>, 
<a href="/search/cs?searchtype=author&query=Polikarpova%2C+N">Nadia Polikarpova</a>, 
<a href="/search/cs?searchtype=author&query=Lerner%2C+S">Sorin Lerner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11828" title="Abstract">arXiv:2306.11828</a> (replaced) [<a href="/pdf/2306.11828" title="Download PDF">pdf</a>, <a href="/ps/2306.11828" title="Download PostScript">ps</a>, <a href="/format/2306.11828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Dynamic Rounding of Fractional Matchings in Bipartite  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sayan Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Kiss%2C+P">Peter Kiss</a>, 
<a href="/search/cs?searchtype=author&query=Sidford%2C+A">Aaron Sidford</a>, 
<a href="/search/cs?searchtype=author&query=Wajc%2C+D">David Wajc</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version of STOC 2024 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13292" title="Abstract">arXiv:2306.13292</a> (replaced) [<a href="/pdf/2306.13292" title="Download PDF">pdf</a>, <a href="/format/2306.13292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance-Covariance Regularization Improves Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiachen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Evtimova%2C+K">Katrina Evtimova</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yubei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shwartz-Ziv%2C+R">Ravid Shwartz-Ziv</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 165 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01079" title="Abstract">arXiv:2307.01079</a> (replaced) [<a href="/pdf/2307.01079" title="Download PDF">pdf</a>, <a href="/ps/2307.01079" title="Download PostScript">ps</a>, <a href="/format/2307.01079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meaning and identity of proofs in a bilateralist setting: A two-sorted  typed lambda-calculus for proofs and refutations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ayhan%2C+S">Sara Ayhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03349" title="Abstract">arXiv:2307.03349</a> (replaced) [<a href="/pdf/2307.03349" title="Download PDF">pdf</a>, <a href="/format/2307.03349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point spread function approximation of high rank Hessians with locally  supported non-negative integral kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alger%2C+N">Nick Alger</a>, 
<a href="/search/math?searchtype=author&query=Hartland%2C+T">Tucker Hartland</a>, 
<a href="/search/math?searchtype=author&query=Petra%2C+N">Noemi Petra</a>, 
<a href="/search/math?searchtype=author&query=Ghattas%2C+O">Omar Ghattas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06175" title="Abstract">arXiv:2307.06175</a> (replaced) [<a href="/pdf/2307.06175" title="Download PDF">pdf</a>, <a href="/format/2307.06175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Decentralized Partially Observable Mean Field Control for  Artificial Collective Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+K">Kai Cui</a>, 
<a href="/search/cs?searchtype=author&query=Hauck%2C+S">Sascha Hauck</a>, 
<a href="/search/cs?searchtype=author&query=Fabian%2C+C">Christian Fabian</a>, 
<a href="/search/cs?searchtype=author&query=Koeppl%2C+H">Heinz Koeppl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06471" title="Abstract">arXiv:2307.06471</a> (replaced) [<a href="/pdf/2307.06471" title="Download PDF">pdf</a>, <a href="/format/2307.06471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Runge--Kutta discontinuous Galerkin method with compact stencils for  hyperbolic conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+Q">Qifan Chen</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+Z">Zheng Sun</a>, 
<a href="/search/math?searchtype=author&query=Xing%2C+Y">Yulong Xing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages. In the second version, we clarify the relationship between the cRKDG method and the LWDG method, with special attention to the numerical fluxes. Moreover, we provide the CFL numbers for the cRKDG method
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07333" title="Abstract">arXiv:2307.07333</a> (replaced) [<a href="/pdf/2307.07333" title="Download PDF">pdf</a>, <a href="/format/2307.07333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal  Instance Segmentation of Cluttered Tabletop Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ng%2C+Z">Zhili Ng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haozhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengshen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hock%2C+F+T+E">Francis Tay Eng Hock</a>, 
<a href="/search/cs?searchtype=author&query=Ang%2C+M+H">Marcelo H. Ang Jr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Version 2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08672" title="Abstract">arXiv:2307.08672</a> (replaced) [<a href="/pdf/2307.08672" title="Download PDF">pdf</a>, <a href="/format/2307.08672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedDefender: Backdoor Attack Defense in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gill%2C+W">Waris Gill</a> (1), 
<a href="/search/cs?searchtype=author&query=Anwar%2C+A">Ali Anwar</a> (2), 
<a href="/search/cs?searchtype=author&query=Gulzar%2C+M+A">Muhammad Ali Gulzar</a> (1) ((1) Virginia Tech, (2) University of Minnesota Twin Cities)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in SE4SafeML 2023 (co-located with FSE 2023). See <a href="https://dl.acm.org/doi/abs/10.1145/3617574.3617858">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11565" title="Abstract">arXiv:2307.11565</a> (replaced) [<a href="/pdf/2307.11565" title="Download PDF">pdf</a>, <a href="/format/2307.11565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Feature Map Pruning for Backdoor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Dong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+Q">Qingwen Bu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11792" title="Abstract">arXiv:2307.11792</a> (replaced) [<a href="/pdf/2307.11792" title="Download PDF">pdf</a>, <a href="/format/2307.11792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Convolutional Neural Networks with Interaction Layers for  Classification of Classical Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Mahmud%2C+J">Jishnu Mahmud</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mashtura%2C+R">Raisa Mashtura</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fattah%2C+S+A">Shaikh Anowarul Fattah</a>, 
<a href="/search/quant-ph?searchtype=author&query=Saquib%2C+M">Mohammad Saquib</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 13 figures, 6 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Quantum Machine Intelligence 6, 11 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01054" title="Abstract">arXiv:2308.01054</a> (replaced) [<a href="/pdf/2308.01054" title="Download PDF">pdf</a>, <a href="/format/2308.01054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation-based inference using surjective sequential neural likelihood  estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dirmeier%2C+S">Simon Dirmeier</a>, 
<a href="/search/stat?searchtype=author&query=Albert%2C+C">Carlo Albert</a>, 
<a href="/search/stat?searchtype=author&query=Perez-Cruz%2C+F">Fernando Perez-Cruz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06338" title="Abstract">arXiv:2308.06338</a> (replaced) [<a href="/pdf/2308.06338" title="Download PDF">pdf</a>, <a href="/format/2308.06338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Size Lowerbounds for Deep Operator Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Anirbit Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+A">Amartya Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 13 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published in Transactions on Machine Learning Research (TMLR) in
  February 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08784" title="Abstract">arXiv:2308.08784</a> (replaced) [<a href="/pdf/2308.08784" title="Download PDF">pdf</a>, <a href="/format/2308.08784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeCoT: Tackling Code Syntax Errors in CoT Reasoning for Code  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Dong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+Q">Qingwen Bu</a>, 
<a href="/search/cs?searchtype=author&query=Qing%2C+Y">Yuhao Qing</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Heming Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Title changed
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10001" title="Abstract">arXiv:2308.10001</a> (replaced) [<a href="/pdf/2308.10001" title="Download PDF">pdf</a>, <a href="/format/2308.10001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AltNeRF: Learning Robust Neural Radiance Field via Alternating  Depth-Pose Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiqiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Huang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10779" title="Abstract">arXiv:2308.10779</a> (replaced) [<a href="/pdf/2308.10779" title="Download PDF">pdf</a>, <a href="/format/2308.10779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spear and Shield: Adversarial Attacks and Defense Methods for  Model-Based Link Prediction on Continuous-Time Dynamic Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongjin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Juho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+K">Kijung Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12215" title="Abstract">arXiv:2308.12215</a> (replaced) [<a href="/pdf/2308.12215" title="Download PDF">pdf</a>, <a href="/format/2308.12215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Challenges of Machine Learning for Trust and Safety: A Case Study on  Misinformation Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Madelyne Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+J">Jonathan Mayer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15459" title="Abstract">arXiv:2308.15459</a> (replaced) [<a href="/pdf/2308.15459" title="Download PDF">pdf</a>, <a href="/format/2308.15459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style  Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Horvitz%2C+Z">Zachary Horvitz</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Ajay Patel</a>, 
<a href="/search/cs?searchtype=author&query=Callison-Burch%2C+C">Chris Callison-Burch</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=McKeown%2C+K">Kathleen McKeown</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16759" title="Abstract">arXiv:2308.16759</a> (replaced) [<a href="/pdf/2308.16759" title="Download PDF">pdf</a>, <a href="/format/2308.16759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructing Indoor Region-based Radio Map without Location Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+Z">Zheng Xing</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junting Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02142" title="Abstract">arXiv:2309.02142</a> (replaced) [<a href="/pdf/2309.02142" title="Download PDF">pdf</a>, <a href="/ps/2309.02142" title="Download PostScript">ps</a>, <a href="/format/2309.02142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who are the users of ChatGPT? Implications for the digital divide from  web tracking data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kacperski%2C+C">Celina Kacperski</a>, 
<a href="/search/cs?searchtype=author&query=Ulloa%2C+R">Roberto Ulloa</a>, 
<a href="/search/cs?searchtype=author&query=Bonnay%2C+D">Denis Bonnay</a>, 
<a href="/search/cs?searchtype=author&query=Kulshrestha%2C+J">Juhi Kulshrestha</a>, 
<a href="/search/cs?searchtype=author&query=Selb%2C+P">Peter Selb</a>, 
<a href="/search/cs?searchtype=author&query=Spitz%2C+A">Andreas Spitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04198" title="Abstract">arXiv:2309.04198</a> (replaced) [<a href="/pdf/2309.04198" title="Download PDF">pdf</a>, <a href="/format/2309.04198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Ignore Dual Logic Ability of LLMs while Privatizing: A  Data-Intensive Analysis in Medical Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yanrui Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sendong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Muzhen Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Ming Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Danyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiawei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08508" title="Abstract">arXiv:2309.08508</a> (replaced) [<a href="/pdf/2309.08508" title="Download PDF">pdf</a>, <a href="/format/2309.08508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOSAIC: Learning Unified Multi-Sensory Object Property Representations  for Robot Learning via Interactive Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tatiya%2C+G">Gyan Tatiya</a>, 
<a href="/search/cs?searchtype=author&query=Francis%2C+J">Jonathan Francis</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Ho-Hsiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bisk%2C+Y">Yonatan Bisk</a>, 
<a href="/search/cs?searchtype=author&query=Sinapov%2C+J">Jivko Sinapov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 2024 IEEE International Conference on Robotics and Automation (ICRA), May 13 to 17, 2024; Yokohama, Japan
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08849" title="Abstract">arXiv:2309.08849</a> (replaced) [<a href="/pdf/2309.08849" title="Download PDF">pdf</a>, <a href="/ps/2309.08849" title="Download PostScript">ps</a>, <a href="/format/2309.08849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning a Stable Dynamic System with a Lyapunov Energy Function for  Demonstratives Using Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yongxiang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xiuze Xia</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Long Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09051" title="Abstract">arXiv:2309.09051</a> (replaced) [<a href="/pdf/2309.09051" title="Download PDF">pdf</a>, <a href="/format/2309.09051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenDOM: Generalizable One-shot Deformable Object Manipulation with  Parameter-Aware Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuroki%2C+S">So Kuroki</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiaxian Guo</a>, 
<a href="/search/cs?searchtype=author&query=Matsushima%2C+T">Tatsuya Matsushima</a>, 
<a href="/search/cs?searchtype=author&query=Okubo%2C+T">Takuya Okubo</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+M">Masato Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Ikeda%2C+Y">Yuya Ikeda</a>, 
<a href="/search/cs?searchtype=author&query=Takanami%2C+R">Ryosuke Takanami</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+P">Paul Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>, 
<a href="/search/cs?searchtype=author&query=Iwasawa%2C+Y">Yusuke Iwasawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of <a href="/abs/2306.09872">arXiv:2306.09872</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09814" title="Abstract">arXiv:2309.09814</a> (replaced) [<a href="/pdf/2309.09814" title="Download PDF">pdf</a>, <a href="/ps/2309.09814" title="Download PostScript">ps</a>, <a href="/format/2309.09814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional Deep Kernel Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Milsom%2C+E">Edward Milsom</a>, 
<a href="/search/stat?searchtype=author&query=Anson%2C+B">Ben Anson</a>, 
<a href="/search/stat?searchtype=author&query=Aitchison%2C+L">Laurence Aitchison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Camera Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10092" title="Abstract">arXiv:2309.10092</a> (replaced) [<a href="/pdf/2309.10092" title="Download PDF">pdf</a>, <a href="/format/2309.10092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Temporal Logic Planning using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+J">Jiaming Tong</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K">Kaiyuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>, 
<a href="/search/cs?searchtype=author&query=Kantaros%2C+Y">Yiannis Kantaros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11038" title="Abstract">arXiv:2309.11038</a> (replaced) [<a href="/pdf/2309.11038" title="Download PDF">pdf</a>, <a href="/format/2309.11038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CaveSeg: Deep Semantic Segmentation and Scene Parsing for Autonomous  Underwater Cave Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdullah%2C+A">A. Abdullah</a>, 
<a href="/search/cs?searchtype=author&query=Barua%2C+T">T. Barua</a>, 
<a href="/search/cs?searchtype=author&query=Tibbetts%2C+R">R. Tibbetts</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Z. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+J">M. J. Islam</a>, 
<a href="/search/cs?searchtype=author&query=Rekleitis%2C+I">I. Rekleitis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted for review in ICRA 2024. 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12220" title="Abstract">arXiv:2309.12220</a> (replaced) [<a href="/pdf/2309.12220" title="Download PDF">pdf</a>, <a href="/format/2309.12220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> De-authentication using Ambient Light Sensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gangwal%2C+A">Ankit Gangwal</a>, 
<a href="/search/cs?searchtype=author&query=Paliwal%2C+A">Aashish Paliwal</a>, 
<a href="/search/cs?searchtype=author&query=Conti%2C+M">Mauro Conti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in IEEE ACCESS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00902" title="Abstract">arXiv:2310.00902</a> (replaced) [<a href="/pdf/2310.00902" title="Download PDF">pdf</a>, <a href="/format/2310.00902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+Y">Yongchan Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+E">Eric Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kevin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01362" title="Abstract">arXiv:2310.01362</a> (replaced) [<a href="/pdf/2310.01362" title="Download PDF">pdf</a>, <a href="/format/2310.01362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot Fleet Learning via Policy Merging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Allan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> See the code <a href="https://github.com/liruiw/Fleet-Tools">this https URL</a> for more details
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01552" title="Abstract">arXiv:2310.01552</a> (replaced) [<a href="/pdf/2310.01552" title="Download PDF">pdf</a>, <a href="/format/2310.01552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Ancillary Services: From Grid Codes to Transfer Function-Based  Converter Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=H%C3%A4berle%2C+V">Verena H&#xe4;berle</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+L">Linbin Huang</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+X">Xiuqiang He</a>, 
<a href="/search/eess?searchtype=author&query=Prieto-Araujo%2C+E">Eduardo Prieto-Araujo</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02861" title="Abstract">arXiv:2310.02861</a> (replaced) [<a href="/pdf/2310.02861" title="Download PDF">pdf</a>, <a href="/ps/2310.02861" title="Download PostScript">ps</a>, <a href="/format/2310.02861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiangyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sibo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03128" title="Abstract">arXiv:2310.03128</a> (replaced) [<a href="/pdf/2310.03128" title="Download PDF">pdf</a>, <a href="/format/2310.03128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaTool Benchmark for Large Language Models: Deciding Whether to Use  Tools and Which to Use
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiawen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chenrui Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qihui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04218" title="Abstract">arXiv:2310.04218</a> (replaced) [<a href="/pdf/2310.04218" title="Download PDF">pdf</a>, <a href="/format/2310.04218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence  Classes with the same Skeleton
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+V+S">Vidya Sagar Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 75 pages, 2 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05668" title="Abstract">arXiv:2310.05668</a> (replaced) [<a href="/pdf/2310.05668" title="Download PDF">pdf</a>, <a href="/format/2310.05668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised  Time Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feiyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shuiguang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Web Conference 2024 (WWW 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05963" title="Abstract">arXiv:2310.05963</a> (replaced) [<a href="/pdf/2310.05963" title="Download PDF">pdf</a>, <a href="/format/2310.05963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid  Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yining Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingfa Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 11 figures, quality-checked, typos corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06714" title="Abstract">arXiv:2310.06714</a> (replaced) [<a href="/pdf/2310.06714" title="Download PDF">pdf</a>, <a href="/format/2310.06714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Memorization in Fine-tuned Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shenglai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yue Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07446" title="Abstract">arXiv:2310.07446</a> (replaced) [<a href="/pdf/2310.07446" title="Download PDF">pdf</a>, <a href="/format/2310.07446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: An Integrated Perspective on Data, Metrics, and  Methodology for Deep Time-Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiawen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xumeng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shun Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09751" title="Abstract">arXiv:2310.09751</a> (replaced) [<a href="/pdf/2310.09751" title="Download PDF">pdf</a>, <a href="/format/2310.09751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junfeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+R">Roger Zimmermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10221" title="Abstract">arXiv:2310.10221</a> (replaced) [<a href="/pdf/2310.10221" title="Download PDF">pdf</a>, <a href="/format/2310.10221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zijun Long</a>, 
<a href="/search/cs?searchtype=author&query=Killick%2C+G">George Killick</a>, 
<a href="/search/cs?searchtype=author&query=McCreadie%2C+R">Richard McCreadie</a>, 
<a href="/search/cs?searchtype=author&query=Camarasa%2C+G+A">Gerardo Aragon Camarasa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11373" title="Abstract">arXiv:2310.11373</a> (replaced) [<a href="/pdf/2310.11373" title="Download PDF">pdf</a>, <a href="/format/2310.11373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Two-Layer Blockchain Sharding Protocol Leveraging Safety and Liveness  for Enhanced Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yibin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jingyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%BCdder%2C+B">Boris D&#xfc;dder</a>, 
<a href="/search/cs?searchtype=author&query=Slaats%2C+T">Tijs Slaats</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yongluan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted to Network and Distributed System Security (NDSS) Symposium 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Network and Distributed System Security (NDSS) Symposium 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11846" title="Abstract">arXiv:2310.11846</a> (replaced) [<a href="/pdf/2310.11846" title="Download PDF">pdf</a>, <a href="/format/2310.11846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MaskMA: Towards Zero-Shot Multi-Agent Decision Making with Mask-Based  Collaborative Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinmin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chuming Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13164" title="Abstract">arXiv:2310.13164</a> (replaced) [<a href="/pdf/2310.13164" title="Download PDF">pdf</a>, <a href="/format/2310.13164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost Equivariance via Lie Algebra Convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McNeela%2C+D">Daniel McNeela</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14058" title="Abstract">arXiv:2310.14058</a> (replaced) [<a href="/pdf/2310.14058" title="Download PDF">pdf</a>, <a href="/format/2310.14058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Caching Connections in Matchings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadeh%2C+Y">Yaniv Sadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+H">Haim Kaplan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated resource-augmentation results, related work and references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15372" title="Abstract">arXiv:2310.15372</a> (replaced) [<a href="/pdf/2310.15372" title="Download PDF">pdf</a>, <a href="/format/2310.15372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EpiK-Eval: Evaluation for Language Models as Epistemic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prato%2C+G">Gabriele Prato</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jerry Huang</a>, 
<a href="/search/cs?searchtype=author&query=Parthasarathi%2C+P">Prasannna Parthasarathi</a>, 
<a href="/search/cs?searchtype=author&query=Sodhani%2C+S">Shagun Sodhani</a>, 
<a href="/search/cs?searchtype=author&query=Chandar%2C+S">Sarath Chandar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16119" title="Abstract">arXiv:2310.16119</a> (replaced) [<a href="/pdf/2310.16119" title="Download PDF">pdf</a>, <a href="/format/2310.16119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for  Enhancing SocialBot Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobza%2C+O">Ond&#x159;ej Kobza</a>, 
<a href="/search/cs?searchtype=author&query=%C4%8Cuhel%2C+J">Jan &#x10c;uhel</a>, 
<a href="/search/cs?searchtype=author&query=Gargiani%2C+T">Tommaso Gargiani</a>, 
<a href="/search/cs?searchtype=author&query=Herel%2C+D">David Herel</a>, 
<a href="/search/cs?searchtype=author&query=Marek%2C+P">Petr Marek</a> (Faculty of Electrical Engineering, CTU in Prague)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19366" title="Abstract">arXiv:2310.19366</a> (replaced) [<a href="/pdf/2310.19366" title="Download PDF">pdf</a>, <a href="/format/2310.19366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Certificates: 6G-ready Access Control for the Service-Based  Architecture with Decentralized Identifiers and Verifiable Credentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garzon%2C+S+R">Sandro Rodriguez Garzon</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+H+D">Hai Dinh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M+M">Maria Mora Martinez</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BCpper%2C+A">Axel K&#xfc;pper</a>, 
<a href="/search/cs?searchtype=author&query=Einsiedler%2C+H+J">Hans Joachim Einsiedler</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+D">Daniela Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00847" title="Abstract">arXiv:2311.00847</a> (replaced) [<a href="/pdf/2311.00847" title="Download PDF">pdf</a>, <a href="/format/2311.00847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signatures From Pseudorandom States via $\bot$-PRFs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barhoush%2C+M">Mohammed Barhoush</a>, 
<a href="/search/cs?searchtype=author&query=Behera%2C+A">Amit Behera</a>, 
<a href="/search/cs?searchtype=author&query=Ozer%2C+L">Lior Ozer</a>, 
<a href="/search/cs?searchtype=author&query=Salvail%2C+L">Louis Salvail</a>, 
<a href="/search/cs?searchtype=author&query=Sattath%2C+O">Or Sattath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00888" title="Abstract">arXiv:2311.00888</a> (replaced) [<a href="/pdf/2311.00888" title="Download PDF">pdf</a>, <a href="/format/2311.00888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A robust shape model for blood vessels analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Romero%2C+P">Pau Romero</a>, 
<a href="/search/cs?searchtype=author&query=Pedr%C3%B3s%2C+A">Abel Pedr&#xf3;s</a>, 
<a href="/search/cs?searchtype=author&query=Sebastian%2C+R">Rafael Sebastian</a>, 
<a href="/search/cs?searchtype=author&query=Lozano%2C+M">Miguel Lozano</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Fern%C3%A1ndez%2C+I">Ignacio Garc&#xed;a-Fern&#xe1;ndez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01223" title="Abstract">arXiv:2311.01223</a> (replaced) [<a href="/pdf/2311.01223" title="Download PDF">pdf</a>, <a href="/format/2311.01223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Models for Reinforcement Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhengbang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hanye Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haoran He</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yichao Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Haoquan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tingting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02009" title="Abstract">arXiv:2311.02009</a> (replaced) [<a href="/pdf/2311.02009" title="Download PDF">pdf</a>, <a href="/format/2311.02009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust-Preserved Human-Robot Shared Autonomy enabled by Bayesian  Relational Event Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingke Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fumin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to RA-L
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02107" title="Abstract">arXiv:2311.02107</a> (replaced) [<a href="/pdf/2311.02107" title="Download PDF">pdf</a>, <a href="/ps/2311.02107" title="Download PostScript">ps</a>, <a href="/format/2311.02107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Artificial Intelligence in Healthcare: Ethical Considerations  and Assessment Checklist
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yilin Ning</a>, 
<a href="/search/cs?searchtype=author&query=Teixayavong%2C+S">Salinelat Teixayavong</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+Y">Yuqing Shang</a>, 
<a href="/search/cs?searchtype=author&query=Savulescu%2C+J">Julian Savulescu</a>, 
<a href="/search/cs?searchtype=author&query=Nagaraj%2C+V">Vaishaanth Nagaraj</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+D">Di Miao</a>, 
<a href="/search/cs?searchtype=author&query=Mertens%2C+M">Mayli Mertens</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+D+S+W">Daniel Shu Wei Ting</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+J+C+L">Jasmine Chiat Ling Ong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiuwen Cao</a>, 
<a href="/search/cs?searchtype=author&query=Dunn%2C+M">Michael Dunn</a>, 
<a href="/search/cs?searchtype=author&query=Vaughan%2C+R">Roger Vaughan</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+M+E+H">Marcus Eng Hock Ong</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+J+J">Joseph Jao-Yiu Sung</a>, 
<a href="/search/cs?searchtype=author&query=Topol%2C+E+J">Eric J Topol</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02181" title="Abstract">arXiv:2311.02181</a> (replaced) [<a href="/pdf/2311.02181" title="Download PDF">pdf</a>, <a href="/format/2311.02181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Problems in Learning Multiple Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Niu%2C+M">Mengjia Niu</a>, 
<a href="/search/math?searchtype=author&query=He%2C+X">Xiaoyu He</a>, 
<a href="/search/math?searchtype=author&query=Ry%C5%A1av%C3%BD%2C+P">Petr Ry&#x161;av&#xfd;</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Q">Quan Zhou</a>, 
<a href="/search/math?searchtype=author&query=Marecek%2C+J">Jakub Marecek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04254" title="Abstract">arXiv:2311.04254</a> (replaced) [<a href="/pdf/2311.04254" title="Download PDF">pdf</a>, <a href="/format/2311.04254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Everything of Thoughts: Defying the Law of Penrose Triangle for Thought  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+R">Ruomeng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Minghua Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+S">Si Qin</a>, 
<a href="/search/cs?searchtype=author&query=Rajmohan%2C+S">Saravan Rajmohan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04590" title="Abstract">arXiv:2311.04590</a> (replaced) [<a href="/pdf/2311.04590" title="Download PDF">pdf</a>, <a href="/format/2311.04590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Cross-Domain Sequential Recommendation under Open-World  Assumptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wujiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qitian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runzhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+M">Mingming Ha</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qiongxu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Linxun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bing Han</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05446" title="Abstract">arXiv:2311.05446</a> (replaced) [<a href="/pdf/2311.05446" title="Download PDF">pdf</a>, <a href="/ps/2311.05446" title="Download PostScript">ps</a>, <a href="/format/2311.05446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Brunn--Minkowski and functional inequalities via convexity of  entropy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aishwarya%2C+G">Gautam Aishwarya</a>, 
<a href="/search/math?searchtype=author&query=Rotem%2C+L">Liran Rotem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages; Added some historical references (Remark 3), comments on the sharpness of our log-Sobolev and Talagrand inequalities (in Examples 5.6, 5.9), and technical details. Comments welcome!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Metric Geometry (math.MG)</span>; Information Theory (cs.IT); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07215" title="Abstract">arXiv:2311.07215</a> (replaced) [<a href="/pdf/2311.07215" title="Download PDF">pdf</a>, <a href="/format/2311.07215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Seungjun Moon</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+H">Hyungjoo Chae</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yongho Song</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taeyoon Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongjin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+K+T">Kai Tzu-iunn Ong</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S">Seung-won Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Yeo%2C+J">Jinyoung Yeo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07397" title="Abstract">arXiv:2311.07397</a> (replaced) [<a href="/pdf/2311.07397" title="Download PDF">pdf</a>, <a href="/format/2311.07397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination  Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guohai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yukai Gu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+H">Haitao Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+J">Jitao Sang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08045" title="Abstract">arXiv:2311.08045</a> (replaced) [<a href="/pdf/2311.08045" title="Download PDF">pdf</a>, <a href="/format/2311.08045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Preference Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Pengyu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianhao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+P">Peixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+N">Nan Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In process
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08838" title="Abstract">arXiv:2311.08838</a> (replaced) [<a href="/pdf/2311.08838" title="Download PDF">pdf</a>, <a href="/format/2311.08838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disinformation Capabilities of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vykopal%2C+I">Ivan Vykopal</a>, 
<a href="/search/cs?searchtype=author&query=Pikuliak%2C+M">Mat&#xfa;&#x161; Pikuliak</a>, 
<a href="/search/cs?searchtype=author&query=Srba%2C+I">Ivan Srba</a>, 
<a href="/search/cs?searchtype=author&query=Moro%2C+R">Robert Moro</a>, 
<a href="/search/cs?searchtype=author&query=Macko%2C+D">Dominik Macko</a>, 
<a href="/search/cs?searchtype=author&query=Bielikova%2C+M">Maria Bielikova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10788" title="Abstract">arXiv:2311.10788</a> (replaced) [<a href="/pdf/2311.10788" title="Download PDF">pdf</a>, <a href="/format/2311.10788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Temporally-Aware DeepFake Detection using H.264 Motion Vectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gr%C3%B6nquist%2C+P">Peter Gr&#xf6;nquist</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yufan Ren</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qingyi He</a>, 
<a href="/search/cs?searchtype=author&query=Verardo%2C+A">Alessio Verardo</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%BCsstrunk%2C+S">Sabine S&#xfc;sstrunk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12943" title="Abstract">arXiv:2311.12943</a> (replaced) [<a href="/pdf/2311.12943" title="Download PDF">pdf</a>, <a href="/format/2311.12943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InteRACT: Transformer Models for Human Intent Prediction Conditioned on  Robot Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kedia%2C+K">Kushal Kedia</a>, 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+A">Atiksh Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Dan%2C+P">Prithwish Dan</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sanjiban Choudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15885" title="Abstract">arXiv:2311.15885</a> (replaced) [<a href="/pdf/2311.15885" title="Download PDF">pdf</a>, <a href="/format/2311.15885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Quantifier Depth to Quantifier Number: Separating Structures with k  Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vinall-Smeeth%2C+H">Harry Vinall-Smeeth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages, 8 figures; added new result on the relative succinctness of finite variable logic
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17852" title="Abstract">arXiv:2311.17852</a> (replaced) [<a href="/pdf/2311.17852" title="Download PDF">pdf</a>, <a href="/format/2311.17852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computing-in-Memory-based One-Class Hyperdimensional Computing Model  for Outlier Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruixuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+S+H">Sabrina Hassan Moon</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X+S">Xiaobo Sharon Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+X">Xun Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Reis%2C+D">Dayane Reis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00279" title="Abstract">arXiv:2312.00279</a> (replaced) [<a href="/pdf/2312.00279" title="Download PDF">pdf</a>, <a href="/format/2312.00279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age-Based Scheduling for Mobile Edge Computing: A Deep Reinforcement  Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xingqiu He</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Chaoqun You</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q. S. Quek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01957" title="Abstract">arXiv:2312.01957</a> (replaced) [<a href="/pdf/2312.01957" title="Download PDF">pdf</a>, <a href="/format/2312.01957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gallego%2C+V">Victor Gallego</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICLR 2024 (TinyPapers track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02677" title="Abstract">arXiv:2312.02677</a> (replaced) [<a href="/pdf/2312.02677" title="Download PDF">pdf</a>, <a href="/format/2312.02677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contact Energy Based Hindsight Experience Prioritization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayar%2C+E">Erdi Sayar</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+Z">Zhenshan Bing</a>, 
<a href="/search/cs?searchtype=author&query=D%27Eramo%2C+C">Carlo D&#x27;Eramo</a>, 
<a href="/search/cs?searchtype=author&query=Oguz%2C+O+S">Ozgur S. Oguz</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04127" title="Abstract">arXiv:2312.04127</a> (replaced) [<a href="/pdf/2312.04127" title="Download PDF">pdf</a>, <a href="/format/2312.04127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing the Inherent Response Tendency of LLMs: Real-World  Instructions-Driven Jailbreak
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yanrui Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sendong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Ming Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04772" title="Abstract">arXiv:2312.04772</a> (replaced) [<a href="/pdf/2312.04772" title="Download PDF">pdf</a>, <a href="/format/2312.04772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision  Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alamdari%2C+P+A">Parand A. Alamdari</a>, 
<a href="/search/cs?searchtype=author&query=Klassen%2C+T+Q">Toryn Q. Klassen</a>, 
<a href="/search/cs?searchtype=author&query=Creager%2C+E">Elliot Creager</a>, 
<a href="/search/cs?searchtype=author&query=McIlraith%2C+S+A">Sheila A. McIlraith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06522" title="Abstract">arXiv:2312.06522</a> (replaced) [<a href="/pdf/2312.06522" title="Download PDF">pdf</a>, <a href="/format/2312.06522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Role of Label Smoothing in Enhanced Text Sentiment  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yijie Gao</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Shijing Si</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Hua Luo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haixia Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yugui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07622" title="Abstract">arXiv:2312.07622</a> (replaced) [<a href="/pdf/2312.07622" title="Download PDF">pdf</a>, <a href="/format/2312.07622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wentao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hanglei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuyang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junsong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Jiayi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+M">Mengliang He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Aimin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1705.04146">arXiv:1705.04146</a>, <a href="/abs/2304.10977">arXiv:2304.10977</a>, <a href="/abs/2112.00114">arXiv:2112.00114</a>, <a href="/abs/1905.13319">arXiv:1905.13319</a>, <a href="/abs/2304.12244">arXiv:2304.12244</a>, <a href="/abs/2206.01347">arXiv:2206.01347</a>, <a href="/abs/2006.09265">arXiv:2006.09265</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10743" title="Abstract">arXiv:2312.10743</a> (replaced) [<a href="/pdf/2312.10743" title="Download PDF">pdf</a>, <a href="/format/2312.10743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework for Multi-Domain CTR Prediction via Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zichuan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuhan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kuicai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Mengchen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Huifeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submited to TOIS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11234" title="Abstract">arXiv:2312.11234</a> (replaced) [<a href="/pdf/2312.11234" title="Download PDF">pdf</a>, <a href="/format/2312.11234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual Musical Features for Interpretable Audio Tagging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyberatos%2C+V">Vassilis Lyberatos</a>, 
<a href="/search/cs?searchtype=author&query=Kantarelis%2C+S">Spyridon Kantarelis</a>, 
<a href="/search/cs?searchtype=author&query=Dervakos%2C+E">Edmund Dervakos</a>, 
<a href="/search/cs?searchtype=author&query=Stamou%2C+G">Giorgos Stamou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Github Repository: <a href="https://github.com/vaslyb/perceptible-music-tagging">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11434" title="Abstract">arXiv:2312.11434</a> (replaced) [<a href="/pdf/2312.11434" title="Download PDF">pdf</a>, <a href="/ps/2312.11434" title="Download PostScript">ps</a>, <a href="/format/2312.11434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factored Online Planning in Many-Agent POMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galesloot%2C+M+F+L">Maris F.L. Galesloot</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%A3o%2C+T+D">Thiago D. Sim&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Junges%2C+S">Sebastian Junges</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+N">Nils Jansen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version (includes the Appendix) of the paper accepted at AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12703" title="Abstract">arXiv:2312.12703</a> (replaced) [<a href="/pdf/2312.12703" title="Download PDF">pdf</a>, <a href="/format/2312.12703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with Extremely Noisy Clients via Negative  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yonggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+Y">Yiu-ming Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanzi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13933" title="Abstract">arXiv:2312.13933</a> (replaced) [<a href="/pdf/2312.13933" title="Download PDF">pdf</a>, <a href="/format/2312.13933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Probabilistic Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+D">Dou Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+L">Lingwei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songlin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, accepted by AAAI 2024 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15084" title="Abstract">arXiv:2312.15084</a> (replaced) [<a href="/pdf/2312.15084" title="Download PDF">pdf</a>, <a href="/format/2312.15084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated forest inventory: analysis of high-density airborne LiDAR  point clouds with 3D deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+B">Binbin Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Wielgosz%2C+M">Maciej Wielgosz</a>, 
<a href="/search/cs?searchtype=author&query=Kontogianni%2C+T">Theodora Kontogianni</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+T">Torben Peters</a>, 
<a href="/search/cs?searchtype=author&query=Puliti%2C+S">Stefano Puliti</a>, 
<a href="/search/cs?searchtype=author&query=Astrup%2C+R">Rasmus Astrup</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+K">Konrad Schindler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15304" title="Abstract">arXiv:2312.15304</a> (replaced) [<a href="/pdf/2312.15304" title="Download PDF">pdf</a>, <a href="/format/2312.15304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and  Person Name Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Shijing Si</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Siqing Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Le Tang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiaoqing Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yugui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02602" title="Abstract">arXiv:2401.02602</a> (replaced) [<a href="/pdf/2401.02602" title="Download PDF">pdf</a>, <a href="/format/2401.02602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Causal Abstractions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+K">Kevin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Bareinboim%2C+E">Elias Bareinboim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 total pages, 20 figures, short version accepted to AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03855" title="Abstract">arXiv:2401.03855</a> (replaced) [<a href="/pdf/2401.03855" title="Download PDF">pdf</a>, <a href="/format/2401.03855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yadav%2C+A">Ankit Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mayank Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04385" title="Abstract">arXiv:2401.04385</a> (replaced) [<a href="/pdf/2401.04385" title="Download PDF">pdf</a>, <a href="/format/2401.04385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine unlearning through fine-grained model parameters perturbation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zuo%2C+Z">Zhiwei Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhuo Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kenli Li</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+A">Anwitaman Datta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04829" title="Abstract">arXiv:2401.04829</a> (replaced) [<a href="/pdf/2401.04829" title="Download PDF">pdf</a>, <a href="/ps/2401.04829" title="Download PostScript">ps</a>, <a href="/format/2401.04829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNNShap: Scalable and Accurate GNN Explanation using Shapley Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akkas%2C+S">Selahattin Akkas</a>, 
<a href="/search/cs?searchtype=author&query=Azad%2C+A">Ariful Azad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06310" title="Abstract">arXiv:2401.06310</a> (replaced) [<a href="/pdf/2401.06310" title="Download PDF">pdf</a>, <a href="/format/2401.06310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViSAGe: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jha%2C+A">Akshita Jha</a>, 
<a href="/search/cs?searchtype=author&query=Prabhakaran%2C+V">Vinodkumar Prabhakaran</a>, 
<a href="/search/cs?searchtype=author&query=Denton%2C+R">Remi Denton</a>, 
<a href="/search/cs?searchtype=author&query=Laszlo%2C+S">Sarah Laszlo</a>, 
<a href="/search/cs?searchtype=author&query=Dave%2C+S">Shachi Dave</a>, 
<a href="/search/cs?searchtype=author&query=Qadri%2C+R">Rida Qadri</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+C+K">Chandan K. Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Dev%2C+S">Sunipa Dev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06477" title="Abstract">arXiv:2401.06477</a> (replaced) [<a href="/pdf/2401.06477" title="Download PDF">pdf</a>, <a href="/format/2401.06477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kun: Answer Polishment for Chinese Self-Alignment with Instruction  Back-Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shuyue Guo</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xingwei Qu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiawei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weixu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xinrun Du</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07278" title="Abstract">arXiv:2401.07278</a> (replaced) [<a href="/pdf/2401.07278" title="Download PDF">pdf</a>, <a href="/format/2401.07278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Semantic Segmentation using Redesigned Self-Training for  White Blood Cells
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luu%2C+V+Q">Vinh Quoc Luu</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D+K">Duy Khanh Le</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H+T">Huy Thanh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M+T">Minh Thanh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+T">Thinh Tien Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Dinh%2C+V+Q">Vinh Quang Dinh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10935" title="Abstract">arXiv:2401.10935</a> (replaced) [<a href="/pdf/2401.10935" title="Download PDF">pdf</a>, <a href="/format/2401.10935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kanzhi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qiushi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Y">Yougang Chu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fangzhi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yantao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianbing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12686" title="Abstract">arXiv:2401.12686</a> (replaced) [<a href="/pdf/2401.12686" title="Download PDF">pdf</a>, <a href="/format/2401.12686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fabian%2C+C">Christian Fabian</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+K">Kai Cui</a>, 
<a href="/search/cs?searchtype=author&query=Koeppl%2C+H">Heinz Koeppl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12713" title="Abstract">arXiv:2401.12713</a> (replaced) [<a href="/pdf/2401.12713" title="Download PDF">pdf</a>, <a href="/format/2401.12713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Zero-shot Abstractive Explanations for Rumour Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bilal%2C+I+M">Iman Munire Bilal</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Procter%2C+R">Rob Procter</a>, 
<a href="/search/cs?searchtype=author&query=Liakata%2C+M">Maria Liakata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised version of the original
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14340" title="Abstract">arXiv:2401.14340</a> (replaced) [<a href="/pdf/2401.14340" title="Download PDF">pdf</a>, <a href="/format/2401.14340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimation of partially known Gaussian graphical models with score-based  structural priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sevilla%2C+M">Mart&#xed;n Sevilla</a>, 
<a href="/search/stat?searchtype=author&query=Marques%2C+A+G">Antonio Garc&#xed;a Marques</a>, 
<a href="/search/stat?searchtype=author&query=Segarra%2C+S">Santiago Segarra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures, AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15236" title="Abstract">arXiv:2401.15236</a> (replaced) [<a href="/pdf/2401.15236" title="Download PDF">pdf</a>, <a href="/format/2401.15236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Deep Learning for Efficient Visual Pose Estimation aboard  Ultra-low-power Nano-drones
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Motetti%2C+B+A">Beatrice Alessandra Motetti</a>, 
<a href="/search/cs?searchtype=author&query=Crupi%2C+L">Luca Crupi</a>, 
<a href="/search/cs?searchtype=author&query=Elshaigi%2C+M+O+M+E">Mustafa Omer Mohammed Elamin Elshaigi</a>, 
<a href="/search/cs?searchtype=author&query=Risso%2C+M">Matteo Risso</a>, 
<a href="/search/cs?searchtype=author&query=Pagliari%2C+D+J">Daniele Jahier Pagliari</a>, 
<a href="/search/cs?searchtype=author&query=Palossi%2C+D">Daniele Palossi</a>, 
<a href="/search/cs?searchtype=author&query=Burrello%2C+A">Alessio Burrello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the 2024 Design, Automation and Test in Europe (DATE) conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15996" title="Abstract">arXiv:2401.15996</a> (replaced) [<a href="/pdf/2401.15996" title="Download PDF">pdf</a>, <a href="/format/2401.15996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AccessLens: Auto-detecting Inaccessibility of Everyday Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+N">Nahyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Qazi%2C+M+H">Muhammad Hasham Qazi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Joanne Liu</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+C">Changhoon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+S">Shu Kong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeeeun Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CHI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16116" title="Abstract">arXiv:2401.16116</a> (replaced) [<a href="/pdf/2401.16116" title="Download PDF">pdf</a>, <a href="/ps/2401.16116" title="Download PostScript">ps</a>, <a href="/format/2401.16116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Cheques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Barhoush%2C+M">Mohammed Barhoush</a>, 
<a href="/search/quant-ph?searchtype=author&query=Salvail%2C+L">Louis Salvail</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16509" title="Abstract">arXiv:2401.16509</a> (replaced) [<a href="/pdf/2401.16509" title="Download PDF">pdf</a>, <a href="/format/2401.16509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissecting users&#x27; needs for search result explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Juneja%2C+P">Prerna Juneja</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Smith-Renner%2C+A+M">Alison Marie Smith-Renner</a>, 
<a href="/search/cs?searchtype=author&query=Lamba%2C+H">Hemank Lamba</a>, 
<a href="/search/cs?searchtype=author&query=Tetreault%2C+J">Joel Tetreault</a>, 
<a href="/search/cs?searchtype=author&query=Jaimes%2C+A">Alex Jaimes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16979" title="Abstract">arXiv:2401.16979</a> (replaced) [<a href="/pdf/2401.16979" title="Download PDF">pdf</a>, <a href="/format/2401.16979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re3val: Reinforced and Reranked Generative Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+E">EuiYul Song</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangryul Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Haeju Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joonkee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures, Findings of the Association for Computational Linguistics: EACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17548" title="Abstract">arXiv:2401.17548</a> (replaced) [<a href="/pdf/2401.17548" title="Download PDF">pdf</a>, <a href="/format/2401.17548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Channel Dependence for Multivariate Time Series Forecasting:  Learning from Leading Indicators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lifan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yanyan Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024. Preprint version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Twelfth International Conference on Learning Representations,
  2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17791" title="Abstract">arXiv:2401.17791</a> (replaced) [<a href="/pdf/2401.17791" title="Download PDF">pdf</a>, <a href="/format/2401.17791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Transformers without Positional Encodings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+A">Ayush Garg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Independent Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17842" title="Abstract">arXiv:2401.17842</a> (replaced) [<a href="/pdf/2401.17842" title="Download PDF">pdf</a>, <a href="/format/2401.17842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Benchmarking for Iterative Optimization Heuristics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Stein%2C+N">Niki van Stein</a>, 
<a href="/search/cs?searchtype=author&query=Vermetten%2C+D">Diederick Vermetten</a>, 
<a href="/search/cs?searchtype=author&query=Kononova%2C+A+V">Anna V. Kononova</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4ck%2C+T">Thomas B&#xe4;ck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACM TELO
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00025" title="Abstract">arXiv:2402.00025</a> (replaced) [<a href="/pdf/2402.00025" title="Download PDF">pdf</a>, <a href="/format/2402.00025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with  SplitK work decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoque%2C+A">Adnan Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+L">Less Wright</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chih-Chieh Yang</a>, 
<a href="/search/cs?searchtype=author&query=Srivatsa%2C+M">Mudhakar Srivatsa</a>, 
<a href="/search/cs?searchtype=author&query=Ganti%2C+R">Raghu Ganti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00450" title="Abstract">arXiv:2402.00450</a> (replaced) [<a href="/pdf/2402.00450" title="Download PDF">pdf</a>, <a href="/format/2402.00450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPT: Competence-progressive Training Strategy for Few-shot Node  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qilong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yufeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinghao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jingpu Duan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jian Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2206.11972">arXiv:2206.11972</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00856" title="Abstract">arXiv:2402.00856</a> (replaced) [<a href="/pdf/2402.00856" title="Download PDF">pdf</a>, <a href="/format/2402.00856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Efficient and Exact Optimization of Language Model Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Haozhe Ji</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yilin Niu</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+P">Pei Ke</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01596" title="Abstract">arXiv:2402.01596</a> (replaced) [<a href="/pdf/2402.01596" title="Download PDF">pdf</a>, <a href="/format/2402.01596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Immersive Video Compression using Implicit Neural Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kwan%2C+H+M">Ho Man Kwan</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Gower%2C+A">Andrew Gower</a>, 
<a href="/search/eess?searchtype=author&query=Bull%2C+D">David Bull</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02380" title="Abstract">arXiv:2402.02380</a> (replaced) [<a href="/pdf/2402.02380" title="Download PDF">pdf</a>, <a href="/ps/2402.02380" title="Download PostScript">ps</a>, <a href="/format/2402.02380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Large Language Models in Analysing Classroom Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yun Long</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haifeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02389" title="Abstract">arXiv:2402.02389</a> (replaced) [<a href="/pdf/2402.02389" title="Download PDF">pdf</a>, <a href="/format/2402.02389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KICGPT: Large Language Model with Knowledge in Context for Knowledge  Graph Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yanbin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qiushi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03041" title="Abstract">arXiv:2402.03041</a> (replaced) [<a href="/pdf/2402.03041" title="Download PDF">pdf</a>, <a href="/format/2402.03041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Datapath Accelerator Enhanced Off-path SmartNIC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuzheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Ting Fu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yifan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lingjun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeke Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03808" title="Abstract">arXiv:2402.03808</a> (replaced) [<a href="/pdf/2402.03808" title="Download PDF">pdf</a>, <a href="/format/2402.03808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal  Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yu-Tung Liu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+K">Kuan-Chen Wang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+K">Kai-Chun Liu</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+S">Sheng-Yu Peng</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03910" title="Abstract">arXiv:2402.03910</a> (replaced) [<a href="/pdf/2402.03910" title="Download PDF">pdf</a>, <a href="/format/2402.03910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Trends, Patterns, and Dynamics in Global Acquisitions: A  Network Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalhor%2C+G">Ghazal Kalhor</a>, 
<a href="/search/cs?searchtype=author&query=Bahrak%2C+B">Behnam Bahrak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04364" title="Abstract">arXiv:2402.04364</a> (replaced) [<a href="/pdf/2402.04364" title="Download PDF">pdf</a>, <a href="/ps/2402.04364" title="Download PostScript">ps</a>, <a href="/format/2402.04364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exponential Separation Between Powers of Regular and General Resolution  Over Parities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S+K">Sreejata Kishor Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+A">Arkadev Chattopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Dvo%C5%99%C3%A1k%2C+P">Pavel Dvo&#x159;&#xe1;k</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05004" title="Abstract">arXiv:2402.05004</a> (replaced) [<a href="/pdf/2402.05004" title="Download PDF">pdf</a>, <a href="/ps/2402.05004" title="Download PostScript">ps</a>, <a href="/format/2402.05004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal Generalized Decoding of Polar-like Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+P">Peihong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Duffy%2C+K+R">Ken R. Duffy</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A9dard%2C+M">Muriel M&#xe9;dard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ISIT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05211" title="Abstract">arXiv:2402.05211</a> (replaced) [<a href="/pdf/2402.05211" title="Download PDF">pdf</a>, <a href="/ps/2402.05211" title="Download PostScript">ps</a>, <a href="/format/2402.05211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Maturity Model for Urban Dataset Meta-data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fox%2C+M+S">Mark S. Fox</a>, 
<a href="/search/cs?searchtype=author&query=Gajderowicz%2C+B">Bart Gajderowicz</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+D">Dishu Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05328" title="Abstract">arXiv:2402.05328</a> (replaced) [<a href="/pdf/2402.05328" title="Download PDF">pdf</a>, <a href="/ps/2402.05328" title="Download PostScript">ps</a>, <a href="/format/2402.05328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Simple Proofs of M&#xfc;ller&#x27;s Theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Epstein%2C+S">Samuel Epstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05629" title="Abstract">arXiv:2402.05629</a> (replaced) [<a href="/pdf/2402.05629" title="Download PDF">pdf</a>, <a href="/format/2402.05629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature  of Aggregated Factual Claims in Long-Form Generations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiang%2C+C">Cheng-Han Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05980" title="Abstract">arXiv:2402.05980</a> (replaced) [<a href="/pdf/2402.05980" title="Download PDF">pdf</a>, <a href="/format/2402.05980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Large Code Models Understand Programming Concepts? A Black-box  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hooda%2C+A">Ashish Hooda</a>, 
<a href="/search/cs?searchtype=author&query=Christodorescu%2C+M">Mihai Christodorescu</a>, 
<a href="/search/cs?searchtype=author&query=Allamanis%2C+M">Miltiadis Allamanis</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+A">Aaron Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Fawaz%2C+K">Kassem Fawaz</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06956" title="Abstract">arXiv:2402.06956</a> (replaced) [<a href="/pdf/2402.06956" title="Download PDF">pdf</a>, <a href="/format/2402.06956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform enclosures for the phase and zeros of Bessel functions and their  derivatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Filonov%2C+N">Nikolay Filonov</a>, 
<a href="/search/math?searchtype=author&query=Levitin%2C+M">Michael Levitin</a>, 
<a href="/search/math?searchtype=author&query=Polterovich%2C+I">Iosif Polterovich</a>, 
<a href="/search/math?searchtype=author&query=Sher%2C+D+A">David A. Sher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages (including Appendices), 13 figures, 3 tables, v2: minor corrections and edits. The accompanying Mathematica script and its printout are available for download at <a href="https://michaellevitin.net/bessels.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Numerical Analysis (math.NA); Spectral Theory (math.SP)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07703" title="Abstract">arXiv:2402.07703</a> (replaced) [<a href="/pdf/2402.07703" title="Download PDF">pdf</a>, <a href="/format/2402.07703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Sequential Decision-Making with Unknown Delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Ping Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengyang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07913" title="Abstract">arXiv:2402.07913</a> (replaced) [<a href="/pdf/2402.07913" title="Download PDF">pdf</a>, <a href="/format/2402.07913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QACP: An Annotated Question Answering Dataset for Assisting Chinese  Python Programming Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+R">Rui Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lu Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaoying Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+N">Na Zong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07939" title="Abstract">arXiv:2402.07939</a> (replaced) [<a href="/pdf/2402.07939" title="Download PDF">pdf</a>, <a href="/format/2402.07939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UFO: A UI-Focused Agent for Windows OS Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liqun Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shilin He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+B">Bo Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+S">Si Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Minghua Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Rajmohan%2C+S">Saravan Rajmohan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08082" title="Abstract">arXiv:2402.08082</a> (replaced) [<a href="/pdf/2402.08082" title="Download PDF">pdf</a>, <a href="/ps/2402.08082" title="Download PostScript">ps</a>, <a href="/format/2402.08082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-based generative models break the curse of dimensionality in  learning a family of sub-Gaussian probability distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cole%2C+F">Frank Cole</a>, 
<a href="/search/stat?searchtype=author&query=Lu%2C+Y">Yulong Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, to appear in the proceedings of 12th International Conference on Learning Representations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08498" title="Abstract">arXiv:2402.08498</a> (replaced) [<a href="/pdf/2402.08498" title="Download PDF">pdf</a>, <a href="/ps/2402.08498" title="Download PostScript">ps</a>, <a href="/format/2402.08498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auditing Counterfire: Evaluating Advanced Counterargument Generation  with Evidence and Style
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Preetika Verma</a>, 
<a href="/search/cs?searchtype=author&query=Jaidka%2C+K">Kokil Jaidka</a>, 
<a href="/search/cs?searchtype=author&query=Churina%2C+S">Svetlana Churina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 10 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08576" title="Abstract">arXiv:2402.08576</a> (replaced) [<a href="/pdf/2402.08576" title="Download PDF">pdf</a>, <a href="/format/2402.08576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret Minimization in Stackelberg Games with Side Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harris%2C+K">Keegan Harris</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>, 
<a href="/search/cs?searchtype=author&query=Balcan%2C+M">Maria-Florina Balcan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09015" title="Abstract">arXiv:2402.09015</a> (replaced) [<a href="/pdf/2402.09015" title="Download PDF">pdf</a>, <a href="/format/2402.09015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards better Human-Agent Alignment: Assessing Task Utility in  LLM-Powered Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arabzadeh%2C+N">Negar Arabzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kiseleva%2C+J">Julia Kiseleva</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A">Ahmed Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Dibia%2C+V">Victor Dibia</a>, 
<a href="/search/cs?searchtype=author&query=Fourney%2C+A">Adam Fourney</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+C">Charles Clarke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09679" title="Abstract">arXiv:2402.09679</a> (replaced) [<a href="/pdf/2402.09679" title="Download PDF">pdf</a>, <a href="/format/2402.09679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Visual Servoing Control of a Hybrid Dual-Segment Flexible  Neurosurgical Robot for Intraventricular Biopsy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingcong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qingxiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Ying Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+D+T+M">Danny Tat Ming Chan</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+K+T+L">Kam Tong Leo Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+D+Y+C">David Yuen Chung Chan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE International Conference on Robotics and Automation (ICRA) 2024, 7 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09727" title="Abstract">arXiv:2402.09727</a> (replaced) [<a href="/pdf/2402.09727" title="Download PDF">pdf</a>, <a href="/format/2402.09727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-Huei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Canny%2C+J">John Canny</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+I">Ian Fischer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://read-agent.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10400" title="Abstract">arXiv:2402.10400</a> (replaced) [<a href="/pdf/2402.10400" title="Download PDF">pdf</a>, <a href="/format/2402.10400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain of Logic: Rule-Based Reasoning with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Servantez%2C+S">Sergio Servantez</a>, 
<a href="/search/cs?searchtype=author&query=Barrow%2C+J">Joe Barrow</a>, 
<a href="/search/cs?searchtype=author&query=Hammond%2C+K">Kristian Hammond</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rajiv Jain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10877" title="Abstract">arXiv:2402.10877</a> (replaced) [<a href="/pdf/2402.10877" title="Download PDF">pdf</a>, <a href="/format/2402.10877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust agents learn causal world models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Richens%2C+J">Jonathan Richens</a>, 
<a href="/search/cs?searchtype=author&query=Everitt%2C+T">Tom Everitt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 (oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11621" title="Abstract">arXiv:2402.11621</a> (replaced) [<a href="/pdf/2402.11621" title="Download PDF">pdf</a>, <a href="/ps/2402.11621" title="Download PostScript">ps</a>, <a href="/format/2402.11621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding News Narratives: A Critical Analysis of Large Language Models  in Framing Bias Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pastorino%2C+V">Valeria Pastorino</a>, 
<a href="/search/cs?searchtype=author&query=Sivakumar%2C+J+A">Jasivan A. Sivakumar</a>, 
<a href="/search/cs?searchtype=author&query=Moosavi%2C+N+S">Nafise Sadat Moosavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11641" title="Abstract">arXiv:2402.11641</a> (replaced) [<a href="/pdf/2402.11641" title="Download PDF">pdf</a>, <a href="/format/2402.11641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Versatile Graph Learning Approach: from the Perspective of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+L">Lanning Wei</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Q">Quanming Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11725" title="Abstract">arXiv:2402.11725</a> (replaced) [<a href="/pdf/2402.11725" title="Download PDF">pdf</a>, <a href="/format/2402.11725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Susceptible are Large Language Models to Ideological Manipulation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zihao He</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Taiwei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+K">Kristina Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11790" title="Abstract">arXiv:2402.11790</a> (replaced) [<a href="/pdf/2402.11790" title="Download PDF">pdf</a>, <a href="/format/2402.11790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic  Swarms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Shipeng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongbo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yuhua Qi</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Dapeng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Weisong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> published in ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12269" title="Abstract">arXiv:2402.12269</a> (replaced) [<a href="/pdf/2402.12269" title="Download PDF">pdf</a>, <a href="/format/2402.12269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Supervised Prediction of Arbitrary-size Graphs with  Partially-Masked Fused Gromov-Wasserstein Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krzakala%2C+P">Paul Krzakala</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Flamary%2C+R">R&#xe9;mi Flamary</a>, 
<a href="/search/cs?searchtype=author&query=d%27Alch%C3%A9-Buc%2C+F">Florence d&#x27;Alch&#xe9;-Buc</a>, 
<a href="/search/cs?searchtype=author&query=Laclau%2C+C">Charlotte Laclau</a>, 
<a href="/search/cs?searchtype=author&query=Labeau%2C+M">Matthieu Labeau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12424" title="Abstract">arXiv:2402.12424</a> (replaced) [<a href="/pdf/2402.12424" title="Download PDF">pdf</a>, <a href="/format/2402.12424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tables as Images? Exploring the Strengths and Limitations of LLMs on  Multimodal Representations of Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhenjie Sun</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruiqi He</a>, 
<a href="/search/cs?searchtype=author&query=Sikka%2C+A">Aman Sikka</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12488" title="Abstract">arXiv:2402.12488</a> (replaced) [<a href="/pdf/2402.12488" title="Download PDF">pdf</a>, <a href="/format/2402.12488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Predictive Control Design for Unlocking the Energy Flexibility of  Heat Pump and Thermal Energy Storage Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tang%2C+W">Weihong Tang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yun Li</a>, 
<a href="/search/eess?searchtype=author&query=Walker%2C+S">Shalika Walker</a>, 
<a href="/search/eess?searchtype=author&query=Keviczky%2C+T">Tamas Keviczky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to The 8th IEEE Conference on Control Technology and Applications (CCTA) 2024, 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12531" title="Abstract">arXiv:2402.12531</a> (replaced) [<a href="/pdf/2402.12531" title="Download PDF">pdf</a>, <a href="/format/2402.12531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Deep Generative Models on Many-To-One Image-to-Image  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saxena%2C+S">Sagar Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Teli%2C+M+N">Mohammad Nayeem Teli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures; template format corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12532" title="Abstract">arXiv:2402.12532</a> (replaced) [<a href="/pdf/2402.12532" title="Download PDF">pdf</a>, <a href="/format/2402.12532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Human-Machine Point Cloud Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ulhaq%2C+M">Mateen Ulhaq</a>, 
<a href="/search/cs?searchtype=author&query=Baji%C4%87%2C+I+V">Ivan V. Baji&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, 2024 Picture Coding Symposium (PCS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12686" title="Abstract">arXiv:2402.12686</a> (replaced) [<a href="/pdf/2402.12686" title="Download PDF">pdf</a>, <a href="/format/2402.12686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Platform-Driven Collaboration Patterns: Structural Evolution Over Time  and Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maddah%2C+N">Negin Maddah</a>, 
<a href="/search/cs?searchtype=author&query=Heydari%2C+B">Babak Heydari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12734" title="Abstract">arXiv:2402.12734</a> (replaced) [<a href="/pdf/2402.12734" title="Download PDF">pdf</a>, <a href="/ps/2402.12734" title="Download PostScript">ps</a>, <a href="/format/2402.12734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lower Bound on the Competitive Ratio of the Permutation Algorithm for  Online Facility Assignment on a Line
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harada%2C+T">Tsubasa Harada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12772" title="Abstract">arXiv:2402.12772</a> (replaced) [<a href="/pdf/2402.12772" title="Download PDF">pdf</a>, <a href="/format/2402.12772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GazePrompt: Enhancing Low Vision People&#x27;s Reading Experience with  Gaze-Aware Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Potter%2C+Z">Zach Potter</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+Y">Yun Ho</a>, 
<a href="/search/cs?searchtype=author&query=Killough%2C+D">Daniel Killough</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+L">Linxiu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+S">Sanbrita Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuhang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12777" title="Abstract">arXiv:2402.12777</a> (replaced) [<a href="/pdf/2402.12777" title="Download PDF">pdf</a>, <a href="/format/2402.12777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of Quantum Extreme Learning Machines for QoS Prediction of  Elevators&#x27; Software in an Industrial Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+S">Shaukat Ali</a>, 
<a href="/search/cs?searchtype=author&query=Arrieta%2C+A">Aitor Arrieta</a>, 
<a href="/search/cs?searchtype=author&query=Arcaini%2C+P">Paolo Arcaini</a>, 
<a href="/search/cs?searchtype=author&query=Arratibel%2C+M">Maite Arratibel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12928" title="Abstract">arXiv:2402.12928</a> (replaced) [<a href="/pdf/2402.12928" title="Download PDF">pdf</a>, <a href="/format/2402.12928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Literature Review of Literature Reviews in Pattern Analysis and  Machine Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Penghai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Ming-Ming Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages,9 figures, 5 tables. [February 19, 2024] 20 pages,9 figures, 5 tables. Typos fixed. [February 23, 2024]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13005" title="Abstract">arXiv:2402.13005</a> (replaced) [<a href="/pdf/2402.13005" title="Download PDF">pdf</a>, <a href="/format/2402.13005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SzCORE: A Seizure Community Open-source Research Evaluation framework  for the validation of EEG-based automated seizure detection algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dan%2C+J">Jonathan Dan</a>, 
<a href="/search/eess?searchtype=author&query=Pale%2C+U">Una Pale</a>, 
<a href="/search/eess?searchtype=author&query=Amirshahi%2C+A">Alireza Amirshahi</a>, 
<a href="/search/eess?searchtype=author&query=Cappelletti%2C+W">William Cappelletti</a>, 
<a href="/search/eess?searchtype=author&query=Ingolfsson%2C+T+M">Thorir Mar Ingolfsson</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xiaying Wang</a>, 
<a href="/search/eess?searchtype=author&query=Cossettini%2C+A">Andrea Cossettini</a>, 
<a href="/search/eess?searchtype=author&query=Bernini%2C+A">Adriano Bernini</a>, 
<a href="/search/eess?searchtype=author&query=Benini%2C+L">Luca Benini</a>, 
<a href="/search/eess?searchtype=author&query=Beniczky%2C+S">S&#xe1;ndor Beniczky</a>, 
<a href="/search/eess?searchtype=author&query=Atienza%2C+D">David Atienza</a>, 
<a href="/search/eess?searchtype=author&query=Ryvlin%2C+P">Philippe Ryvlin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13018" title="Abstract">arXiv:2402.13018</a> (replaced) [<a href="/pdf/2402.13018" title="Download PDF">pdf</a>, <a href="/format/2402.13018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMO-SUPERB: An In-depth Look at Speech Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+H">Haibin Wu</a>, 
<a href="/search/eess?searchtype=author&query=Chou%2C+H">Huang-Cheng Chou</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Goncalves%2C+L">Lucas Goncalves</a>, 
<a href="/search/eess?searchtype=author&query=Du%2C+J">Jiawei Du</a>, 
<a href="/search/eess?searchtype=author&query=Jang%2C+J+R">Jyh-Shing Roger Jang</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+C">Chi-Chun Lee</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-Yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> webpage: <a href="https://emosuperb.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13027" title="Abstract">arXiv:2402.13027</a> (replaced) [<a href="/pdf/2402.13027" title="Download PDF">pdf</a>, <a href="/format/2402.13027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving the decision-making differential equations from eye fixation  data in Unity software by using Hermite Long-Short-Term Memory neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parand%2C+K">Kourosh Parand</a>, 
<a href="/search/cs?searchtype=author&query=Setayeshi%2C+S">Saeed Setayeshi</a>, 
<a href="/search/cs?searchtype=author&query=Pedram%2C+M+M">Mir Mohsen Pedram</a>, 
<a href="/search/cs?searchtype=author&query=Yoonesi%2C+A">Ali Yoonesi</a>, 
<a href="/search/cs?searchtype=author&query=Pakniyat%2C+A">Aida Pakniyat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13035" title="Abstract">arXiv:2402.13035</a> (replaced) [<a href="/pdf/2402.13035" title="Download PDF">pdf</a>, <a href="/format/2402.13035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Check: Unleashing Potentials for Self-Correction in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Che Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhenyang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chengcheng Han</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Y">Yixin Lian</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuejian Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13116" title="Abstract">arXiv:2402.13116</a> (replaced) [<a href="/pdf/2402.13116" title="Download PDF">pdf</a>, <a href="/format/2402.13116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Knowledge Distillation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaohan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chongyang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Reynold Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Can Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13178" title="Abstract">arXiv:2402.13178</a> (replaced) [<a href="/pdf/2402.13178" title="Download PDF">pdf</a>, <a href="/format/2402.13178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Retrieval-Augmented Generation for Medicine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+G">Guangzhi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aidong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Homepage: <a href="https://teddy-xionggz.github.io/benchmark-medical-rag/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13277" title="Abstract">arXiv:2402.13277</a> (replaced) [<a href="/pdf/2402.13277" title="Download PDF">pdf</a>, <a href="/format/2402.13277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek  in WSNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Talukder%2C+M+A">Md. Alamin Talukder</a>, 
<a href="/search/cs?searchtype=author&query=Sharmin%2C+S">Selina Sharmin</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M+A">Md Ashraf Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+M">Md Manowarul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Aryal%2C+S">Sunil Aryal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Journal of Information Security, Springer Journal - Q1, Scopus, ISI, SCIE, IF: 3.2 - Accepted on Jan 17, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13291" title="Abstract">arXiv:2402.13291</a> (replaced) [<a href="/pdf/2402.13291" title="Download PDF">pdf</a>, <a href="/format/2402.13291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berabi%2C+B">Berkay Berabi</a>, 
<a href="/search/cs?searchtype=author&query=Gronskiy%2C+A">Alexey Gronskiy</a>, 
<a href="/search/cs?searchtype=author&query=Raychev%2C+V">Veselin Raychev</a>, 
<a href="/search/cs?searchtype=author&query=Sivanrupan%2C+G">Gishor Sivanrupan</a>, 
<a href="/search/cs?searchtype=author&query=Chibotaru%2C+V">Victor Chibotaru</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 13 figures (v2, small fix in author affiliations)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13326" title="Abstract">arXiv:2402.13326</a> (replaced) [<a href="/pdf/2402.13326" title="Download PDF">pdf</a>, <a href="/format/2402.13326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Hedging with Market Impact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Neagu%2C+A">Andrei Neagu</a>, 
<a href="/search/q-fin?searchtype=author&query=Godin%2C+F">Fr&#xe9;d&#xe9;ric Godin</a>, 
<a href="/search/q-fin?searchtype=author&query=Simard%2C+C">Clarence Simard</a>, 
<a href="/search/q-fin?searchtype=author&query=Kosseim%2C+L">Leila Kosseim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13352" title="Abstract">arXiv:2402.13352</a> (replaced) [<a href="/pdf/2402.13352" title="Download PDF">pdf</a>, <a href="/format/2402.13352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Apak%2C+B">Boran Apak</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bandic%2C+M">Medina Bandic</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sarkar%2C+A">Aritra Sarkar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Feld%2C+S">Sebastian Feld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13699" title="Abstract">arXiv:2402.13699</a> (replaced) [<a href="/pdf/2402.13699" title="Download PDF">pdf</a>, <a href="/format/2402.13699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Classification Techniques for Quantum Dot Device  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schug%2C+D">Daniel Schug</a>, 
<a href="/search/cs?searchtype=author&query=Kovach%2C+T+J">Tyler J. Kovach</a>, 
<a href="/search/cs?searchtype=author&query=Wolfe%2C+M+A">M. A. Wolfe</a>, 
<a href="/search/cs?searchtype=author&query=Benson%2C+J">Jared Benson</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sanghyeok Park</a>, 
<a href="/search/cs?searchtype=author&query=Dodson%2C+J+P">J. P. Dodson</a>, 
<a href="/search/cs?searchtype=author&query=Corrigan%2C+J">J. Corrigan</a>, 
<a href="/search/cs?searchtype=author&query=Eriksson%2C+M+A">M. A. Eriksson</a>, 
<a href="/search/cs?searchtype=author&query=Zwolak%2C+J+P">Justyna P. Zwolak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the XAI4Sci: Explainable machine learning for
  sciences workshop at AAAI 2024, Vancouver, Canada
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13711" title="Abstract">arXiv:2402.13711</a> (replaced) [<a href="/pdf/2402.13711" title="Download PDF">pdf</a>, <a href="/format/2402.13711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based  Graph Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Seungyoon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Wonjoong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=In%2C+Y">Yeonjun In</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sein Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACM TheWebConf 2024 (WWW 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13764" title="Abstract">arXiv:2402.13764</a> (replaced) [<a href="/pdf/2402.13764" title="Download PDF">pdf</a>, <a href="/format/2402.13764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CriticBench: Evaluating Large Language Models as Critic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+X">Xian-ling Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13768" title="Abstract">arXiv:2402.13768</a> (replaced) [<a href="/pdf/2402.13768" title="Download PDF">pdf</a>, <a href="/format/2402.13768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Democratizing Uncertainty Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seelinger%2C+L">Linus Seelinger</a>, 
<a href="/search/cs?searchtype=author&query=Reinarz%2C+A">Anne Reinarz</a>, 
<a href="/search/cs?searchtype=author&query=Lykkegaard%2C+M+B">Mikkel B. Lykkegaard</a>, 
<a href="/search/cs?searchtype=author&query=Alghamdi%2C+A+M+A">Amal Mohammed A. Alghamdi</a>, 
<a href="/search/cs?searchtype=author&query=Aristoff%2C+D">David Aristoff</a>, 
<a href="/search/cs?searchtype=author&query=Bangerth%2C+W">Wolfgang Bangerth</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A9n%C3%A9zech%2C+J">Jean B&#xe9;n&#xe9;zech</a>, 
<a href="/search/cs?searchtype=author&query=Diez%2C+M">Matteo Diez</a>, 
<a href="/search/cs?searchtype=author&query=Frey%2C+K">Kurt Frey</a>, 
<a href="/search/cs?searchtype=author&query=Jakeman%2C+J+D">John D. Jakeman</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%B8rgensen%2C+J+S">Jakob Sauer J&#xf8;rgensen</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Ki-Tae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Martinelli%2C+M">Massimiliano Martinelli</a>, 
<a href="/search/cs?searchtype=author&query=Parno%2C+M">Matthew Parno</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrini%2C+R">Riccardo Pellegrini</a>, 
<a href="/search/cs?searchtype=author&query=Petra%2C+N">Noemi Petra</a>, 
<a href="/search/cs?searchtype=author&query=Riis%2C+N+A+B">Nicolai A. B. Riis</a>, 
<a href="/search/cs?searchtype=author&query=Rosenfeld%2C+K">Katherine Rosenfeld</a>, 
<a href="/search/cs?searchtype=author&query=Serani%2C+A">Andrea Serani</a>, 
<a href="/search/cs?searchtype=author&query=Tamellini%2C+L">Lorenzo Tamellini</a>, 
<a href="/search/cs?searchtype=author&query=Villa%2C+U">Umberto Villa</a>, 
<a href="/search/cs?searchtype=author&query=Dodwell%2C+T+J">Tim J. Dodwell</a>, 
<a href="/search/cs?searchtype=author&query=Scheichl%2C+R">Robert Scheichl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main paper and appendix have separate reference lists. For clarity, we have added a prefix to reference labels in the appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13777" title="Abstract">arXiv:2402.13777</a> (replaced) [<a href="/pdf/2402.13777" title="Download PDF">pdf</a>, <a href="/format/2402.13777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Generative Models for Offline Policy Learning: Tutorial, Survey,  and Perspectives on Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+B">Bhargav Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Y">Yongsheng Mei</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13787" title="Abstract">arXiv:2402.13787</a> (replaced) [<a href="/pdf/2402.13787" title="Download PDF">pdf</a>, <a href="/format/2402.13787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stoica%2C+A">Ana-Andreea Stoica</a>, 
<a href="/search/cs?searchtype=author&query=Litvak%2C+N">Nelly Litvak</a>, 
<a href="/search/cs?searchtype=author&query=Chaintreau%2C+A">Augustin Chaintreau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Proceedings of The Web Conference, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13876" title="Abstract">arXiv:2402.13876</a> (replaced) [<a href="/pdf/2402.13876" title="Download PDF">pdf</a>, <a href="/format/2402.13876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scene Prior Filtering for Depth Map Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengxue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiqiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jinshan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tai%2C+Y">Ying Tai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Guangwei Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.13950" title="Abstract">arXiv:2402.13950</a> (replaced) [<a href="/pdf/2402.13950" title="Download PDF">pdf</a>, <a href="/format/2402.13950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Reasoning Matter: Measuring and Improving Faithfulness of  Chain-of-Thought Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paul%2C+D">Debjit Paul</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>, 
<a href="/search/cs?searchtype=author&query=Faltings%2C+B">Boi Faltings</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14096" title="Abstract">arXiv:2402.14096</a> (replaced) [<a href="/pdf/2402.14096" title="Download PDF">pdf</a>, <a href="/format/2402.14096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EyeTrans: Merging Human and Machine Attention for Neural Code  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Karas%2C+Z">Zachary Karas</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+A">Aakash Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+J">Toby Jia-Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=McMillan%2C+C">Collin McMillan</a>, 
<a href="/search/cs?searchtype=author&query=Leach%2C+K">Kevin Leach</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14148" title="Abstract">arXiv:2402.14148</a> (replaced) [<a href="/pdf/2402.14148" title="Download PDF">pdf</a>, <a href="/format/2402.14148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Networks and Friction: Slide, Hold, Learn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Garcia-Suarez%2C+J">Joaquin Garcia-Suarez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 paged, 10 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14208" title="Abstract">arXiv:2402.14208</a> (replaced) [<a href="/pdf/2402.14208" title="Download PDF">pdf</a>, <a href="/format/2402.14208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content Conditional Debiasing for Fair Text Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Wenlong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Blair Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14221" title="Abstract">arXiv:2402.14221</a> (replaced) [<a href="/pdf/2402.14221" title="Download PDF">pdf</a>, <a href="/format/2402.14221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards singular optimality in the presence of local initial knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Hongyan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Pemmaraju%2C+S+V">Sriram V. Pemmaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14224" title="Abstract">arXiv:2402.14224</a> (replaced) [<a href="/pdf/2402.14224" title="Download PDF">pdf</a>, <a href="/format/2402.14224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Framing in the Presence of Supporting Data: A Case Study in U.S.  Economic News
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leto%2C+A">Alexandria Leto</a>, 
<a href="/search/cs?searchtype=author&query=Pickens%2C+E">Elliot Pickens</a>, 
<a href="/search/cs?searchtype=author&query=Needell%2C+C+D">Coen D. Needell</a>, 
<a href="/search/cs?searchtype=author&query=Rothschild%2C+D">David Rothschild</a>, 
<a href="/search/cs?searchtype=author&query=Pacheco%2C+M+L">Maria Leonor Pacheco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> total pages: 19; main body pages: 8; total figures: 19
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14285" title="Abstract">arXiv:2402.14285</a> (replaced) [<a href="/pdf/2402.14285" title="Download PDF">pdf</a>, <a href="/format/2402.14285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yujia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ghatare%2C+A">Adishree Ghatare</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanzhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Ziniu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinsheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+C+S">Chandramouli S Sastry</a>, 
<a href="/search/cs?searchtype=author&query=Gururani%2C+S">Siddharth Gururani</a>, 
<a href="/search/cs?searchtype=author&query=Oore%2C+S">Sageev Oore</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yisong Yue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14323" title="Abstract">arXiv:2402.14323</a> (replaced) [<a href="/pdf/2402.14323" title="Download PDF">pdf</a>, <a href="/format/2402.14323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REPOFUSE: Repository-Level Code Completion with Fused Dual Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+M">Ming Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaoheng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gehao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xunjin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+P">Peng Di</a>, 
<a href="/search/cs?searchtype=author&query=jiang%2C+w">wei jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+G">Gang Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14349" title="Abstract">arXiv:2402.14349</a> (replaced) [<a href="/pdf/2402.14349" title="Download PDF">pdf</a>, <a href="/format/2402.14349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-driven and Adversarial Calibration Learning for Epicardial  Adipose Tissue Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+K">Kai Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhiming Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jiaqi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jingbiao Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Liao%2C+B">Bihong Liao</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+H">Huifang Tang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Qiuyu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+C">Chunquan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages,7 figuers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14379" title="Abstract">arXiv:2402.14379</a> (replaced) [<a href="/pdf/2402.14379" title="Download PDF">pdf</a>, <a href="/format/2402.14379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novi jezi&#x10d;ki modeli za srpski jezik
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%A0kori%C4%87%2C+M">Mihailo &#x160;kori&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in Serbian language
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14392" title="Abstract">arXiv:2402.14392</a> (replaced) [<a href="/pdf/2402.14392" title="Download PDF">pdf</a>, <a href="/format/2402.14392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reading Relevant Feature from Global Representation Memory for Visual  Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Pinxue Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lingyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinglun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+W">Weifeng Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9pages,5 figures, accepted py the Thirty-seventh Conference on Neural Information Processing Systems(Neurips 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14434" title="Abstract">arXiv:2402.14434</a> (replaced) [<a href="/pdf/2402.14434" title="Download PDF">pdf</a>, <a href="/format/2402.14434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallelized Midpoint Randomization for Langevin Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yu%2C+L">Lu Yu</a>, 
<a href="/search/math?searchtype=author&query=Dalalyan%2C+A">Arnak Dalalyan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2306.08494">arXiv:2306.08494</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Probability (math.PR); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14484" title="Abstract">arXiv:2402.14484</a> (replaced) [<a href="/pdf/2402.14484" title="Download PDF">pdf</a>, <a href="/format/2402.14484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation  and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takayanagi%2C+T">Takehiro Takayanagi</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+M">Masahiro Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+R">Ryotaro Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Sakaji%2C+H">Hiroki Sakaji</a>, 
<a href="/search/cs?searchtype=author&query=Izumi%2C+K">Kiyoshi Izumi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14485" title="Abstract">arXiv:2402.14485</a> (replaced) [<a href="/pdf/2402.14485" title="Download PDF">pdf</a>, <a href="/format/2402.14485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine-Checked Categorical Diagrammatic Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guillemet%2C+B">Beno&#xee;t Guillemet</a>, 
<a href="/search/cs?searchtype=author&query=Mahboubi%2C+A">Assia Mahboubi</a>, 
<a href="/search/cs?searchtype=author&query=Piquerez%2C+M">Matthieu Piquerez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14547" title="Abstract">arXiv:2402.14547</a> (replaced) [<a href="/pdf/2402.14547" title="Download PDF">pdf</a>, <a href="/format/2402.14547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmniPred: Language Models as Universal Regressors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xingyou Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+O">Oscar Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chansoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bangding Yang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Daiyi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Perel%2C+S">Sagi Perel</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yutian Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14601" title="Abstract">arXiv:2402.14601</a> (replaced) [<a href="/pdf/2402.14601" title="Download PDF">pdf</a>, <a href="/format/2402.14601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bringing Generative AI to Adaptive Learning in Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianlong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xing Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14615" title="Abstract">arXiv:2402.14615</a> (replaced) [<a href="/pdf/2402.14615" title="Download PDF">pdf</a>, <a href="/format/2402.14615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Entropy-Stable Discontinuous Galerkin Discretization of the Ideal  Multi-Ion Magnetohydrodynamics System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rueda-Ram%C3%ADrez%2C+A+M">Andr&#xe9;s M Rueda-Ram&#xed;rez</a>, 
<a href="/search/math?searchtype=author&query=Sikstel%2C+A">Aleksey Sikstel</a>, 
<a href="/search/math?searchtype=author&query=Gassner%2C+G+J">Gregor J Gassner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14660" title="Abstract">arXiv:2402.14660</a> (replaced) [<a href="/pdf/2402.14660" title="Download PDF">pdf</a>, <a href="/format/2402.14660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConceptMath: A Bilingual Concept-wise Benchmark for Measuring  Mathematical Reasoning of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yanan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+X">Xingyuan Bu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zhiqi Bai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haibin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+T">Tiezheng Ge</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Wenbo Su</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Bo Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The benchmark dataset will be released soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14691" title="Abstract">arXiv:2402.14691</a> (replaced) [<a href="/pdf/2402.14691" title="Download PDF">pdf</a>, <a href="/format/2402.14691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Estimates for First- and Second-Order Lagrange-Galerkin Moving  Mesh Schemes for the One-Dimensional Convection-Diffusion Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Putri%2C+K+S">Kharisma Surya Putri</a>, 
<a href="/search/math?searchtype=author&query=Mizuochi%2C+T">Tatsuki Mizuochi</a>, 
<a href="/search/math?searchtype=author&query=Kolbe%2C+N">Niklas Kolbe</a>, 
<a href="/search/math?searchtype=author&query=Notsu%2C+H">Hirofumi Notsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.14704" title="Abstract">arXiv:2402.14704</a> (replaced) [<a href="/pdf/2402.14704" title="Download PDF">pdf</a>, <a href="/format/2402.14704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An LLM-Enhanced Adversarial Editing System for Lexical Simplification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+K">Keren Tan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kangyang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+J">Jinlong Shu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by COLING 2024 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item335">Cross-lists</a></li>
<li><a href="#item371">Replacements</a></li>
</ul>
<small>[ total of 629 entries:  <b>1-629</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
