<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Tue 13 Feb 24  to  Wed 14 Feb 24, announced Thu, 15 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item327">Cross-lists</a></li>
<li><a href="#item390">Replacements</a></li>
</ul>
<small>[ total of 619 entries:  <b>1-619</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Thu, 15 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08690" title="Abstract">arXiv:2402.08690</a> [<a href="/pdf/2402.08690" title="Download PDF">pdf</a>, <a href="/format/2402.08690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> If Turing played piano with an artificial partner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dotov%2C+D">Dobromir Dotov</a>, 
<a href="/search/cs?searchtype=author&query=Camarena%2C+D">Dante Camarena</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+Z">Zack Harris</a>, 
<a href="/search/cs?searchtype=author&query=Spyra%2C+J">Joanna Spyra</a>, 
<a href="/search/cs?searchtype=author&query=Gagliano%2C+P">Pietro Gagliano</a>, 
<a href="/search/cs?searchtype=author&query=Trainor%2C+L">Laurel Trainor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Music is an inherently social activity that allows people to share
experiences and feel connected with one another. There has been little progress
in designing artificial partners exhibiting a similar social experience as
playing with another person. Neural network architectures that implement
generative models, such as large language models, are suited for producing
musical scores. Playing music socially, however, involves more than playing a
score; it must complement the other musicians' ideas and keep time correctly.
We addressed the question of whether a convincing social experience is made
possible by a generative model trained to produce musical scores, not
necessarily optimized for synchronization and continuation. The network, a
variational autoencoder trained on a large corpus of digital scores, was
adapted for a timed call-and-response task with a human partner. Participants
played piano with a human or artificial partner-in various configurations-and
rated the performance quality and first-person experience of self-other
integration. Overall, the artificial partners held promise but were rated lower
than human partners. The artificial partner with simplest design and highest
similarity parameter was not rated differently from the human partners on some
measures, suggesting that interactive rather than generative sophistication is
important in enabling social AI.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08695" title="Abstract">arXiv:2402.08695</a> [<a href="/pdf/2402.08695" title="Download PDF">pdf</a>, <a href="/format/2402.08695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game of Trojans: Adaptive Adversaries Against Output-based  Trojaned-Model Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahabandu%2C+D">Dinuka Sahabandu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaojun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Rajabi%2C+A">Arezoo Rajabi</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Luyao Niu</a>, 
<a href="/search/cs?searchtype=author&query=Ramasubramanian%2C+B">Bhaskar Ramasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Poovendran%2C+R">Radha Poovendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose and analyze an adaptive adversary that can retrain a Trojaned DNN
and is also aware of SOTA output-based Trojaned model detectors. We show that
such an adversary can ensure (1) high accuracy on both trigger-embedded and
clean samples and (2) bypass detection. Our approach is based on an observation
that the high dimensionality of the DNN parameters provides sufficient degrees
of freedom to simultaneously achieve these objectives. We also enable SOTA
detectors to be adaptive by allowing retraining to recalibrate their
parameters, thus modeling a co-evolution of parameters of a Trojaned model and
detectors. We then show that this co-evolution can be modeled as an iterative
game, and prove that the resulting (optimal) solution of this interactive game
leads to the adversary successfully achieving the above objectives. In
addition, we provide a greedy algorithm for the adversary to select a minimum
number of input samples for embedding triggers. We show that for cross-entropy
or log-likelihood loss functions used by the DNNs, the greedy algorithm
provides provable guarantees on the needed number of trigger-embedded input
samples. Extensive experiments on four diverse datasets -- MNIST, CIFAR-10,
CIFAR-100, and SpeechCommand -- reveal that the adversary effectively evades
four SOTA output-based Trojaned model detectors: MNTD, NeuralCleanse, STRIP,
and TABOR.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08698" title="Abstract">arXiv:2402.08698</a> [<a href="/pdf/2402.08698" title="Download PDF">pdf</a>, <a href="/format/2402.08698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMEND: A Mixture of Experts Framework for Long-tailed Trajectory  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mercurius%2C+R+C">Ray Coden Mercurius</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+E">Ehsan Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Shabestary%2C+S+M+A">Soheil Mohamad Alizadeh Shabestary</a>, 
<a href="/search/cs?searchtype=author&query=Rasouli%2C+A">Amir Rasouli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Accurate prediction of pedestrians' future motions is critical for
intelligent driving systems. Developing models for this task requires rich
datasets containing diverse sets of samples. However, the existing naturalistic
trajectory prediction datasets are generally imbalanced in favor of simpler
samples and lack challenging scenarios. Such a long-tail effect causes
prediction models to underperform on the tail portion of the data distribution
containing safety-critical scenarios. Previous methods tackle the long-tail
problem using methods such as contrastive learning and class-conditioned
hypernetworks. These approaches, however, are not modular and cannot be applied
to many machine learning architectures. In this work, we propose a modular
model-agnostic framework for trajectory prediction that leverages a specialized
mixture of experts. In our approach, each expert is trained with a specialized
skill with respect to a particular part of the data. To produce predictions, we
utilise a router network that selects the best expert by generating relative
confidence scores. We conduct experimentation on common pedestrian trajectory
prediction datasets and show that besides achieving state-of-the-art
performance, our method significantly performs better on long-tail scenarios.
We further conduct ablation studies to highlight the contribution of different
proposed components.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08699" title="Abstract">arXiv:2402.08699</a> [<a href="/pdf/2402.08699" title="Download PDF">pdf</a>, <a href="/format/2402.08699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Evaluation of Code LLMs with Round-Trip Correctness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allamanis%2C+M">Miltiadis Allamanis</a>, 
<a href="/search/cs?searchtype=author&query=Panthaplackel%2C+S">Sheena Panthaplackel</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+P">Pengcheng Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">To evaluate code large language models (LLMs), research has relied on a few
small manually curated benchmarks, such as HumanEval and MBPP, which represent
a narrow part of the real-world software domains. In this work, we introduce
round-trip correctness (RTC) as an alternative evaluation method. RTC allows
Code LLM evaluation on a broader spectrum of real-world software domains
without the need for costly human curation. RTC rests on the idea that we can
ask a model to make a prediction (e.g., describe some code using natural
language), feed that prediction back (e.g., synthesize code from the predicted
description), and check if this round-trip leads to code that is semantically
equivalent to the original input. We show how to employ RTC to evaluate code
synthesis and editing. We find that RTC strongly correlates with model
performance on existing narrow-domain code synthesis benchmarks while allowing
us to expand to a much broader set of domains and tasks which was not
previously possible without costly human annotations.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08701" title="Abstract">arXiv:2402.08701</a> [<a href="/pdf/2402.08701" title="Download PDF">pdf</a>, <a href="/format/2402.08701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Primal-Dual Algorithms with Predictions for Online Bounded Allocation  and Ad-Auctions Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kevi%2C+E">Eniko Kevi</a>, 
<a href="/search/cs?searchtype=author&query=Thang%2C+N+K">Nguyen Kim Thang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
<p class="mathjax">Matching problems have been widely studied in the research community,
especially Ad-Auctions with many applications ranging from network design to
advertising. Following the various advancements in machine learning, one
natural question is whether classical algorithms can benefit from machine
learning and obtain better-quality solutions. Even a small percentage of
performance improvement in matching problems could result in significant gains
for the studied use cases. For example, the network throughput or the revenue
of Ad-Auctions can increase remarkably. This paper presents algorithms with
machine learning predictions for the Online Bounded Allocation and the Online
Ad-Auctions problems. We constructed primal-dual algorithms that achieve
competitive performance depending on the quality of the predictions. When the
predictions are accurate, the algorithms' performance surpasses previous
performance bounds, while when the predictions are misleading, the algorithms
maintain standard worst-case performance guarantees. We provide supporting
experiments on generated data for our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08702" title="Abstract">arXiv:2402.08702</a> [<a href="/pdf/2402.08702" title="Download PDF">pdf</a>, <a href="/format/2402.08702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human  Feedback and Preference Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongchao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Arkin%2C+J">Jacob Arkin</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yilun Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+N">Nicholas Roy</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chuchu Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Robotics (cs.RO)

</div>
<p class="mathjax">Prompt optimization aims to find the best prompt to a large language model
(LLM) for a given task. LLMs have been successfully used to help find and
improve prompt candidates for single-step tasks. However, realistic tasks for
agents are multi-step and introduce new challenges: (1) Prompt content is
likely to be more extensive and complex, making it more difficult for LLMs to
analyze errors, (2) the impact of an individual step is difficult to evaluate,
and (3) different people may have varied preferences about task execution.
While humans struggle to optimize prompts, they are good at providing feedback
about LLM outputs; we therefore introduce a new LLM-driven discrete prompt
optimization framework that incorporates human-designed feedback rules about
potential errors to automatically offer direct suggestions for improvement. Our
framework is stylized as a genetic algorithm in which an LLM generates new
candidate prompts from a parent prompt and its associated feedback; we use a
learned heuristic function that predicts prompt performance to efficiently
sample from these candidates. This approach significantly outperforms both
human-engineered prompts and several other prompt optimization methods across
eight representative multi-step tasks (an average 27.7% and 28.2% improvement
to current best methods on GPT-3.5 and GPT-4, respectively). We further show
that the score function for tasks can be modified to better align with
individual preferences. We believe our work can serve as a benchmark for
automatic prompt optimization for LLM-driven multi-step tasks. Datasets and
Codes are available at https://github.com/yongchao98/PROMST. Project Page is
available at https://yongchao98.github.io/MIT-REALM-PROMST.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08706" title="Abstract">arXiv:2402.08706</a> [<a href="/pdf/2402.08706" title="Download PDF">pdf</a>, <a href="/ps/2402.08706" title="Download PostScript">ps</a>, <a href="/format/2402.08706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Apples, Oranges, and Software Engineering: Study Selection Challenges  for Secondary Research on Latent Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wyrich%2C+M">Marvin Wyrich</a>, 
<a href="/search/cs?searchtype=author&query=Bar%C3%B3n%2C+M+M">Marvin Mu&#xf1;oz Bar&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Bogner%2C+J">Justus Bogner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WSESE 2024, an ICSE co-located workshop on methodological issues with empirical studies in software engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software engineering (SE) is full of abstract concepts that are crucial for
both researchers and practitioners, such as programming experience, team
productivity, code comprehension, and system security. Secondary studies aimed
at summarizing research on the influences and consequences of such concepts
would therefore be of great value.
<br />However, the inability to measure abstract concepts directly poses a
challenge for secondary studies: primary studies in SE can operationalize such
concepts in many ways. Standardized measurement instruments are rarely
available, and even if they are, many researchers do not use them or do not
even provide a definition for the studied concept. SE researchers conducting
secondary studies therefore have to decide a) which primary studies intended to
measure the same construct, and b) how to compare and aggregate vastly
different measurements for the same construct.
<br />In this experience report, we discuss the challenge of study selection in SE
secondary research on latent variables. We report on two instances where we
found it particularly challenging to decide which primary studies should be
included for comparison and synthesis, so as not to end up comparing apples
with oranges. Our report aims to spark a conversation about developing
strategies to address this issue systematically and pave the way for more
efficient and rigorous secondary studies in software engineering.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08712" title="Abstract">arXiv:2402.08712</a> [<a href="/pdf/2402.08712" title="Download PDF">pdf</a>, <a href="/format/2402.08712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BECoTTA: Input-dependent Online Blending of Experts for Continual  Test-time Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Daeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Continual Test Time Adaptation (CTTA) is required to adapt efficiently to
continuous unseen domains while retaining previously learned knowledge.
However, despite the progress of CTTA, forgetting-adaptation trade-offs and
efficiency are still unexplored. Moreover, current CTTA scenarios assume only
the disjoint situation, even though real-world domains are seamlessly changed.
To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet
efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts
(MoDE) that contains two core components: i) Domain-Adaptive Routing, which
aids in selectively capturing the domain-adaptive knowledge with multiple
domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency
between each domain and expert. We validate our method outperforms multiple
CTTA scenarios including disjoint and gradual domain shits, while only
requiring ~98% fewer trainable parameters. We also provide analyses of our
method, including the construction of experts, the effect of domain-adaptive
experts, and visualizations.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08714" title="Abstract">arXiv:2402.08714</a> [<a href="/pdf/2402.08714" title="Download PDF">pdf</a>, <a href="/format/2402.08714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRDP: Proximal Reward Difference Prediction for Large-Scale Reward  Finetuning of Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+F">Fei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Grundmann%2C+M">Matthias Grundmann</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+T">Tingbo Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project webpage: <a href="https://fdeng18.github.io/prdp">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reward finetuning has emerged as a promising approach to aligning foundation
models with downstream objectives. Remarkable success has been achieved in the
language domain by using reinforcement learning (RL) to maximize rewards that
reflect human preference. However, in the vision domain, existing RL-based
reward finetuning methods are limited by their instability in large-scale
training, rendering them incapable of generalizing to complex, unseen prompts.
In this paper, we propose Proximal Reward Difference Prediction (PRDP),
enabling stable black-box reward finetuning for diffusion models for the first
time on large-scale prompt datasets with over 100K prompts. Our key innovation
is the Reward Difference Prediction (RDP) objective that has the same optimal
solution as the RL objective while enjoying better training stability.
Specifically, the RDP objective is a supervised regression objective that tasks
the diffusion model with predicting the reward difference of generated image
pairs from their denoising trajectories. We theoretically prove that the
diffusion model that obtains perfect reward difference prediction is exactly
the maximizer of the RL objective. We further develop an online algorithm with
proximal updates to stably optimize the RDP objective. In experiments, we
demonstrate that PRDP can match the reward maximization ability of
well-established RL-based methods in small-scale training. Furthermore, through
large-scale training on text prompts from the Human Preference Dataset v2 and
the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a
diverse set of complex, unseen prompts whereas RL-based methods completely
fail.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08733" title="Abstract">arXiv:2402.08733</a> [<a href="/pdf/2402.08733" title="Download PDF">pdf</a>, <a href="/format/2402.08733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experts Don&#x27;t Cheat: Learning What You Don&#x27;t Know By Predicting Pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+D+D">Daniel D. Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Tarlow%2C+D">Daniel Tarlow</a>, 
<a href="/search/cs?searchtype=author&query=Duvenaud%2C+D">David Duvenaud</a>, 
<a href="/search/cs?searchtype=author&query=Maddison%2C+C+J">Chris J. Maddison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the
stochastic real-world process $p(Y|X)$ it was trained on is important to ensure
it avoids producing incorrect or "hallucinated" answers or taking unsafe
actions. But this is difficult for generative models because probabilistic
predictions do not distinguish between per-response noise (aleatoric
uncertainty) and lack of knowledge about the process (epistemic uncertainty),
and existing epistemic uncertainty quantification techniques tend to be
overconfident when the model underfits. We propose a general strategy for
teaching a model to both approximate $p(Y|X)$ and also estimate the remaining
gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict
pairs of independent responses drawn from the true conditional distribution,
allow it to "cheat" by observing one response while predicting the other, then
measure how much it cheats. Remarkably, we prove that being good at cheating
(i.e. cheating whenever it improves your prediction) is equivalent to being
second-order calibrated, a principled extension of ordinary calibration that
allows us to construct provably-correct frequentist confidence intervals for
$p(Y|X)$ and detect incorrect responses with high probability. We demonstrate
empirically that our approach accurately estimates how much models don't know
across ambiguous image classification, (synthetic) language modeling, and
partially-observable navigation tasks, outperforming existing techniques.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08739" title="Abstract">arXiv:2402.08739</a> [<a href="/pdf/2402.08739" title="Download PDF">pdf</a>, <a href="/format/2402.08739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEASONS: Signal and Energy Aware Sensing on iNtermittent Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gholami%2C+P+M">Pouya Mahdi Gholami</a>, 
<a href="/search/eess?searchtype=author&query=Hoffmann%2C+H">Henry Hoffmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Both energy-aware, batteryless intermittent systems and signal-aware adaptive
sampling algorithms (ASA) aim to maximize sensor data accuracy under energy
constraints in edge devices. Intuitively, combining both into a signal- &amp;
energy-aware solution would yield even better accuracy. Unfortunately, ASAs and
intermittent systems rely on conflicting energy availability assumptions. So, a
straightforward combination cannot achieve their combined benefits. Therefore,
we propose SEASONS, the first framework for signal- and energy-aware
intermittent systems. SEASONS buffers signal data in time, monitoring queue
dynamics to ensure the data is reported within a user-specified latency
constraint. SEASONS improves sensor data accuracy by 31% without increasing
energy.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08742" title="Abstract">arXiv:2402.08742</a> [<a href="/pdf/2402.08742" title="Download PDF">pdf</a>, <a href="/format/2402.08742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize  Energy Management in Sports Facilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fadli%2C+F">Fodil Fadli</a>, 
<a href="/search/cs?searchtype=author&query=Himeur%2C+Y">Yassine Himeur</a>, 
<a href="/search/cs?searchtype=author&query=Elnour%2C+M">Mariam Elnour</a>, 
<a href="/search/cs?searchtype=author&query=Amira%2C+A">Abbes Amira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures and 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Anomaly detection in sport facilities has gained significant attention due to
its potential to promote energy saving and optimizing operational efficiency.
In this research article, we investigate the role of machine learning,
particularly deep learning, in anomaly detection for sport facilities. We
explore the challenges and perspectives of utilizing deep learning methods for
this task, aiming to address the drawbacks and limitations of conventional
approaches. Our proposed approach involves feature extraction from the data
collected in sport facilities. We present a problem formulation using Deep
Feedforward Neural Networks (DFNN) and introduce threshold estimation
techniques to identify anomalies effectively. Furthermore, we propose methods
to reduce false alarms, ensuring the reliability and accuracy of anomaly
detection. To evaluate the effectiveness of our approach, we conduct
experiments on aquatic center dataset at Qatar University. The results
demonstrate the superiority of our deep learning-based method over conventional
techniques, highlighting its potential in real-world applications. Typically,
94.33% accuracy and 92.92% F1-score have been achieved using the proposed
scheme.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08743" title="Abstract">arXiv:2402.08743</a> [<a href="/pdf/2402.08743" title="Download PDF">pdf</a>, <a href="/format/2402.08743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ADS: Approximate Densest Subgraph for Novel Image Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shanfeng Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The volume of image repositories continues to grow. Despite the availability
of content-based addressing, we still lack a lightweight tool that allows us to
discover images of distinct characteristics from a large collection. In this
paper, we propose a fast and training-free algorithm for novel image discovery.
The key of our algorithm is formulating a collection of images as a perceptual
distance-weighted graph, within which our task is to locate the K-densest
subgraph that corresponds to a subset of the most unique images. While solving
this problem is not just NP-hard but also requires a full computation of the
potentially huge distance matrix, we propose to relax it into a K-sparse
eigenvector problem that we can efficiently solve using stochastic gradient
descent (SGD) without explicitly computing the distance matrix. We compare our
algorithm against state-of-the-arts on both synthetic and real datasets,
showing that it is considerably faster to run with a smaller memory footprint
while able to mine novel images more accurately.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08746" title="Abstract">arXiv:2402.08746</a> [<a href="/pdf/2402.08746" title="Download PDF">pdf</a>, <a href="/ps/2402.08746" title="Download PostScript">ps</a>, <a href="/format/2402.08746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An impossibility result for strongly group-strategyproof multi-winner  approval-based voting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caragiannis%2C+I">Ioannis Caragiannis</a>, 
<a href="/search/cs?searchtype=author&query=LeGrand%2C+R">Rob LeGrand</a>, 
<a href="/search/cs?searchtype=author&query=Markakis%2C+E">Evangelos Markakis</a>, 
<a href="/search/cs?searchtype=author&query=Pountourakis%2C+E">Emmanouil Pountourakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Multi-winner approval-based voting has received considerable attention
recently. A voting rule in this setting takes as input ballots in which each
agent approves a subset of the available alternatives and outputs a committee
of alternatives of given size $k$. We consider the scenario when a coalition of
agents can act strategically and alter their ballots so that the new outcome is
strictly better for a coalition member and at least as good for anyone else in
the coalition. Voting rules that are robust against this strategic behaviour
are called strongly group-strategyproof. We prove that, for $k\in \{1,2, ...,
m-2\}$, strongly group-strategyproof multi-winner approval-based voting rules
which furthermore satisfy the minimum efficiency requirement of unanimity do
not exist, where $m$ is the number of available alternatives. Our proof builds
a connection to single-winner voting with ranking-based ballots and exploits
the infamous Gibbard-Satterthwaite theorem to reach the desired impossibility
result. Our result has implications for paradigmatic problems from the area of
approximate mechanism design without money and indicates that strongly
group-strategyproof mechanisms for minimax approval voting, variants of
facility location, and classification can only have an unbounded approximation
ratio.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08747" title="Abstract">arXiv:2402.08747</a> [<a href="/pdf/2402.08747" title="Download PDF">pdf</a>, <a href="/format/2402.08747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rationality of Learning Algorithms in Repeated Normal-Form Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bajaj%2C+S">Shivam Bajaj</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+P">Pranoy Das</a>, 
<a href="/search/cs?searchtype=author&query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vijay Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Many learning algorithms are known to converge to an equilibrium for specific
classes of games if the same learning algorithm is adopted by all agents.
However, when the agents are self-interested, a natural question is whether
agents have a strong incentive to adopt an alternative learning algorithm that
yields them greater individual utility. We capture such incentives as an
algorithm's rationality ratio, which is the ratio of the highest payoff an
agent can obtain by deviating from a learning algorithm to its payoff from
following it. We define a learning algorithm to be $c$-rational if its
rationality ratio is at most $c$ irrespective of the game. We first establish
that popular learning algorithms such as fictitious play and regret matching
are not $c$-rational for any constant $c\geq 1$. We then propose and analyze
two algorithms that are provably $1$-rational under mild assumptions, and have
the same properties as (a generalized version of) fictitious play and regret
matching, respectively, if all agents follow them. Finally, we show that if an
assumption of perfect monitoring is not satisfied, there are games for which
$c$-rational algorithms do not exist, and illustrate our results with numerical
case studies.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08748" title="Abstract">arXiv:2402.08748</a> [<a href="/pdf/2402.08748" title="Download PDF">pdf</a>, <a href="/ps/2402.08748" title="Download PostScript">ps</a>, <a href="/format/2402.08748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearest Neighbor Representations of Neurons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kilic%2C+K+M">Kordag Mehmet Kilic</a>, 
<a href="/search/cs?searchtype=author&query=Sima%2C+J">Jin Sima</a>, 
<a href="/search/cs?searchtype=author&query=Bruck%2C+J">Jehoshua Bruck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is submitted to ISIT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">The Nearest Neighbor (NN) Representation is an emerging computational model
that is inspired by the brain. We study the complexity of representing a neuron
(threshold function) using the NN representations. It is known that two anchors
(the points to which NN is computed) are sufficient for a NN representation of
a threshold function, however, the resolution (the maximum number of bits
required for the entries of an anchor) is $O(n\log{n})$. In this work, the
trade-off between the number of anchors and the resolution of a NN
representation of threshold functions is investigated. We prove that the
well-known threshold functions EQUALITY, COMPARISON, and ODD-MAX-BIT, which
require 2 or 3 anchors and resolution of $O(n)$, can be represented by
polynomially large number of anchors in $n$ and $O(\log{n})$ resolution. We
conjecture that for all threshold functions, there are NN representations with
polynomially large size and logarithmic resolution in $n$.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08749" title="Abstract">arXiv:2402.08749</a> [<a href="/pdf/2402.08749" title="Download PDF">pdf</a>, <a href="/ps/2402.08749" title="Download PostScript">ps</a>, <a href="/format/2402.08749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated detection of motion artifacts in brain MR images using deep  learning and explainable artificial intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jimeno%2C+M+M">Marina Manso Jimeno</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+K+S">Keerthi Sravan Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+M">Maggie Fung</a>, 
<a href="/search/cs?searchtype=author&query=Vaughan%2C+J+T">John Thomas Vaughan, Jr.</a>, 
<a href="/search/cs?searchtype=author&query=Geethanath%2C+S">Sairam Geethanath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 9 figures, 1 table. Submitted to NMR in Biomedicine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Quality assessment, including inspecting the images for artifacts, is a
critical step during MRI data acquisition to ensure data quality and downstream
analysis or interpretation success. This study demonstrates a deep learning
model to detect rigid motion in T1-weighted brain images. We leveraged a 2D CNN
for three-class classification and tested it on publicly available
retrospective and prospective datasets. Grad-CAM heatmaps enabled the
identification of failure modes and provided an interpretation of the model's
results. The model achieved average precision and recall metrics of 85% and 80%
on six motion-simulated retrospective datasets. Additionally, the model's
classifications on the prospective dataset showed a strong inverse correlation
(-0.84) compared to average edge strength, an image quality metric indicative
of motion. This model is part of the ArtifactID tool, aimed at inline automatic
detection of Gibbs ringing, wrap-around, and motion artifacts. This tool
automates part of the time-consuming QA process and augments expertise on-site,
particularly relevant in low-resource settings where local MR knowledge is
scarce.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08750" title="Abstract">arXiv:2402.08750</a> [<a href="/pdf/2402.08750" title="Download PDF">pdf</a>, <a href="/format/2402.08750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the Detection of AI-Synthesized Human Face Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuhang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimi%2C+T">Touradj Ebrahimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Over the past years, image generation and manipulation have achieved
remarkable progress due to the rapid development of generative AI based on deep
learning. Recent studies have devoted significant efforts to address the
problem of face image manipulation caused by deepfake techniques. However, the
problem of detecting purely synthesized face images has been explored to a
lesser extent. In particular, the recent popular Diffusion Models (DMs) have
shown remarkable success in image synthesis. Existing detectors struggle to
generalize between synthesized images created by different generative models.
In this work, a comprehensive benchmark including human face images produced by
Generative Adversarial Networks (GANs) and a variety of DMs has been
established to evaluate both the generalization ability and robustness of
state-of-the-art detectors. Then, the forgery traces introduced by different
generative models have been analyzed in the frequency domain to draw various
insights. The paper further demonstrates that a detector trained with frequency
representation can generalize well to other unseen generative models.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08751" title="Abstract">arXiv:2402.08751</a> [<a href="/pdf/2402.08751" title="Download PDF">pdf</a>, <a href="/ps/2402.08751" title="Download PostScript">ps</a>, <a href="/format/2402.08751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearest Neighbor Representations of Neural Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kilic%2C+K+M">Kordag Mehmet Kilic</a>, 
<a href="/search/cs?searchtype=author&query=Sima%2C+J">Jin Sima</a>, 
<a href="/search/cs?searchtype=author&query=Bruck%2C+J">Jehoshua Bruck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is submitted to ISIT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Neural networks successfully capture the computational power of the human
brain for many tasks. Similarly inspired by the brain architecture, Nearest
Neighbor (NN) representations is a novel approach of computation. We establish
a firmer correspondence between NN representations and neural networks.
Although it was known how to represent a single neuron using NN
representations, there were no results even for small depth neural networks.
Specifically, for depth-2 threshold circuits, we provide explicit constructions
for their NN representation with an explicit bound on the number of bits to
represent it. Example functions include NN representations of convex polytopes
(AND of threshold gates), IP2, OR of threshold gates, and linear or exact
decision lists.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08753" title="Abstract">arXiv:2402.08753</a> [<a href="/pdf/2402.08753" title="Download PDF">pdf</a>, <a href="/ps/2402.08753" title="Download PostScript">ps</a>, <a href="/format/2402.08753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting for Swap Regret for All Downstream Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Aaron Roth</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Mirah Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the problem of making predictions so that downstream agents who best
respond to them will be guaranteed diminishing swap regret, no matter what
their utility functions are. It has been known since Foster and Vohra (1997)
that agents who best-respond to calibrated forecasts have no swap regret.
Unfortunately, the best known algorithms for guaranteeing calibrated forecasts
in sequential adversarial environments do so at rates that degrade
exponentially with the dimension of the prediction space. In this work, we show
that by making predictions that are not calibrated, but are unbiased subject to
a carefully selected collection of events, we can guarantee arbitrary
downstream agents diminishing swap regret at rates that substantially improve
over the rates that result from calibrated forecasts -- while maintaining the
appealing property that our forecasts give guarantees for any downstream agent,
without our forecasting algorithm needing to know their utility function.
<br />We give separate results in the ``low'' (1 or 2) dimensional setting and the
``high'' ($&gt; 2$) dimensional setting. In the low dimensional setting, we show
how to make predictions such that all agents who best respond to our
predictions have diminishing swap regret -- in 1 dimension, at the optimal
$O(\sqrt{T})$ rate. In the high dimensional setting we show how to make
forecasts that guarantee regret scaling at a rate of $O(T^{2/3})$ (crucially, a
dimension independent exponent), under the assumption that downstream agents
smoothly best respond. Our results stand in contrast to rates that derive from
agents who best respond to calibrated forecasts, which have an exponential
dependence on the dimension of the prediction space.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08754" title="Abstract">arXiv:2402.08754</a> [<a href="/pdf/2402.08754" title="Download PDF">pdf</a>, <a href="/format/2402.08754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-the-Fly Syntax Highlighting: Generalisation and Speed-ups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palma%2C+M+E">Marco Edoardo Palma</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+A">Alex Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Salza%2C+P">Pasquale Salza</a>, 
<a href="/search/cs?searchtype=author&query=Gall%2C+H+C">Harald C. Gall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">On-the-fly syntax highlighting is the task of rapidly associating visual
secondary notation values with each character of a language derivation.
Research in this domain is driven by the prevalence of online software
development tools, which frequently display source code on screen and heavily
rely on syntax highlighting mechanisms. In this context, three contrasting
demands confront resolvers in this space: speed, accuracy, and development
costs. Speed constraints are essential to ensure tool usability, manifesting as
responsiveness for end users accessing online source code and minimising system
overhead. Simultaneously, achieving precise highlighting is critical for
enhancing code comprehensibility. Nevertheless, obtaining accurate results
necessitates the capacity to perform grammatical analysis on the code under
consideration, even in cases of varying grammatical correctness. Furthermore,
addressing the development costs of such resolvers is imperative, given the
multitude of programming language versions. The current state-of-the-art
approach in this field leverages the original lexer and parser of programming
languages to create syntax highlighting oracles, subsequently used for training
base Recurrent Neural Network models. As the question of the generalisation of
such a solution persists, this paper addresses this aspect by extending the
original work to three additional mainstream programming languages and
conducting a comprehensive review of the outcomes. Moreover, the original
limitations in evaluation performance and training costs are mitigated through
the introduction of a novel Convolutional based Neural Network model. This
study examines the performance gains of running models on GPUs, finding that
the new CNN implementation is much faster than previous methods while
maintaining high accuracy.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08755" title="Abstract">arXiv:2402.08755</a> [<a href="/pdf/2402.08755" title="Download PDF">pdf</a>, <a href="/format/2402.08755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-driven Imitation of Subrational Behavior : Illusion or Reality?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coletta%2C+A">Andrea Coletta</a>, 
<a href="/search/cs?searchtype=author&query=Dwarakanath%2C+K">Kshama Dwarakanath</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Penghang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Vyetrenko%2C+S">Svitlana Vyetrenko</a>, 
<a href="/search/cs?searchtype=author&query=Balch%2C+T">Tucker Balch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">Modeling subrational agents, such as humans or economic households, is
inherently challenging due to the difficulty in calibrating reinforcement
learning models or collecting data that involves human subjects. Existing work
highlights the ability of Large Language Models (LLMs) to address complex
reasoning tasks and mimic human communication, while simulation using LLMs as
agents shows emergent social behaviors, potentially improving our comprehension
of human conduct. In this paper, we propose to investigate the use of LLMs to
generate synthetic human demonstrations, which are then used to learn
subrational agent policies though Imitation Learning. We make an assumption
that LLMs can be used as implicit computational models of humans, and propose a
framework to use synthetic demonstrations derived from LLMs to model
subrational behaviors that are characteristic of humans (e.g., myopic behavior
or preference for risk aversion). We experimentally evaluate the ability of our
framework to model sub-rationality through four simple scenarios, including the
well-researched ultimatum game and marshmallow experiment. To gain confidence
in our framework, we are able to replicate well-established findings from prior
human studies associated with the above scenarios. We conclude by discussing
the potential benefits, challenges and limitations of our framework.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08756" title="Abstract">arXiv:2402.08756</a> [<a href="/pdf/2402.08756" title="Download PDF">pdf</a>, <a href="/format/2402.08756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal  Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diesendruck%2C+M">Maurice Diesendruck</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianzhe Lin</a>, 
<a href="/search/cs?searchtype=author&query=Imani%2C+S">Shima Imani</a>, 
<a href="/search/cs?searchtype=author&query=Mahalingam%2C+G">Gayathri Mahalingam</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jie Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">When LLMs perform zero-shot inference, they typically use a prompt with a
task specification, and generate a completion. However, there is no work to
explore the possibility of the reverse - going from completion to task
specification. In this paper, we employ both directions to perform
cycle-supervised learning entirely in-context. Our goal is to create a forward
map f : X -&gt; Y (e.g. image -&gt; generated caption), coupled with a backward map g
: Y -&gt; X (e.g. caption -&gt; generated image) to construct a cycle-consistency
"loss" (formulated as an update to the prompt) to enforce g(f(X)) ~= X. The
technique, called CyclePrompt, uses cycle-consistency as a free supervisory
signal to iteratively craft the prompt. Importantly, CyclePrompt reinforces
model performance without expensive fine-tuning, without training data, and
without the complexity of external environments (e.g. compilers, APIs). We
demonstrate CyclePrompt in two domains: code generation and image captioning.
Our results on the HumanEval coding benchmark put us in first place on the
leaderboard among models that do not rely on extra training data or usage of
external environments, and third overall. Compared to the GPT4 baseline, we
improve accuracy from 80.5% to 87.2%. In the vision-language space, we generate
detailed image captions which outperform baseline zero-shot GPT4V captions,
when tested against natural (VQAv2) and diagrammatic (FigureQA) visual
question-answering benchmarks. To the best of our knowledge, this is the first
use of self-supervised learning for prompting.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08758" title="Abstract">arXiv:2402.08758</a> [<a href="/pdf/2402.08758" title="Download PDF">pdf</a>, <a href="/ps/2402.08758" title="Download PostScript">ps</a>, <a href="/format/2402.08758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Strategic Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+L">Lee Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Sharifi-Malvajerdi%2C+S">Saeed Sharifi-Malvajerdi</a>, 
<a href="/search/cs?searchtype=author&query=Stangl%2C+K">Kevin Stangl</a>, 
<a href="/search/cs?searchtype=author&query=Vakilian%2C+A">Ali Vakilian</a>, 
<a href="/search/cs?searchtype=author&query=Ziani%2C+J">Juba Ziani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">In strategic classification, agents modify their features, at a cost, to
ideally obtain a positive classification from the learner's classifier. The
typical response of the learner is to carefully modify their classifier to be
robust to such strategic behavior. When reasoning about agent manipulations,
most papers that study strategic classification rely on the following strong
assumption: agents fully know the exact parameters of the deployed classifier
by the learner. This often is an unrealistic assumption when using complex or
proprietary machine learning techniques in real-world prediction tasks.
<br />We initiate the study of partial information release by the learner in
strategic classification. We move away from the traditional assumption that
agents have full knowledge of the classifier. Instead, we consider agents that
have a common distributional prior on which classifier the learner is using.
The learner in our model can reveal truthful, yet not necessarily complete,
information about the deployed classifier to the agents. The learner's goal is
to release just enough information about the classifier to maximize accuracy.
We show how such partial information release can, counter-intuitively, benefit
the learner's accuracy, despite increasing agents' abilities to manipulate.
<br />We show that while it is intractable to compute the best response of an agent
in the general case, there exist oracle-efficient algorithms that can solve the
best response of the agents when the learner's hypothesis class is the class of
linear classifiers, or when the agents' cost function satisfies a natural
notion of submodularity as we define. We then turn our attention to the
learner's optimization problem and provide both positive and negative results
on the algorithmic problem of how much information the learner should release
about the classifier to maximize their expected accuracy.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08761" title="Abstract">arXiv:2402.08761</a> [<a href="/pdf/2402.08761" title="Download PDF">pdf</a>, <a href="/format/2402.08761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding  over Small Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fisher%2C+J">Jillian Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Ximing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+J">Jaehun Jung</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Harchaoui%2C+Z">Zaid Harchaoui</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/jfisher52/JAMDecoding">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The permanence of online content combined with the enhanced authorship
identification techniques calls for stronger computational methods to protect
the identity and privacy of online authorship when needed, e.g., blind reviews
for scientific papers, anonymous online reviews, or anonymous interactions in
the mental health forums. In this paper, we propose an unsupervised
inference-time approach to authorship obfuscation to address the unique
challenges of authorship obfuscation: lack of supervision data for diverse
authorship and domains, and the need for a sufficient level of revision beyond
simple paraphrasing to obfuscate the authorship, all the while preserving the
original content and fluency.
<br />We introduce JAMDEC, a user-controlled, inference-time algorithm for
authorship obfuscation that can be in principle applied to any text and
authorship. Our approach builds on small language models such as GPT2-XL in
order to help avoid disclosing the original content to proprietary LLM's APIs,
while also reducing the performance gap between small and large language models
via algorithmic enhancement. The key idea behind our approach is to boost the
creative power of smaller language models through constrained decoding, while
also allowing for user-specified controls and flexibility. Experimental results
demonstrate that our approach based on GPT2-XL outperforms previous
state-of-the-art methods based on comparably small models, while performing
competitively against GPT3.5 175B, a propriety model that is two orders of
magnitudes larger.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08763" title="Abstract">arXiv:2402.08763</a> [<a href="/pdf/2402.08763" title="Download PDF">pdf</a>, <a href="/format/2402.08763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Robustness of Indoor Robotic Navigation with Free-Space  Segmentation Models Against Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+Q">Qiyuan An</a>, 
<a href="/search/cs?searchtype=author&query=Sevastopoulos%2C+C">Christos Sevastopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Makedon%2C+F">Fillia Makedon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2023 IEEE International Conference on Robotic Computing (IRC). arXiv admin note: text overlap with <a href="/abs/2311.01966">arXiv:2311.01966</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Endeavors in indoor robotic navigation rely on the accuracy of segmentation
models to identify free space in RGB images. However, deep learning models are
vulnerable to adversarial attacks, posing a significant challenge to their
real-world deployment. In this study, we identify vulnerabilities within the
hidden layers of neural networks and introduce a practical approach to
reinforce traditional adversarial training. Our method incorporates a novel
distance loss function, minimizing the gap between hidden layers in clean and
adversarial images. Experiments demonstrate satisfactory performance in
improving the model's robustness against adversarial perturbations.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08764" title="Abstract">arXiv:2402.08764</a> [<a href="/pdf/2402.08764" title="Download PDF">pdf</a>, <a href="/format/2402.08764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dataset for the Detection of Dehumanizing Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Engelmann%2C+P">Paul Engelmann</a>, 
<a href="/search/cs?searchtype=author&query=Trolle%2C+P+B">Peter Brunsgaard Trolle</a>, 
<a href="/search/cs?searchtype=author&query=Hardmeier%2C+C">Christian Hardmeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dehumanization is a mental process that enables the exclusion and ill
treatment of a group of people. In this paper, we present two data sets of
dehumanizing text, a large, automatically collected corpus and a smaller,
manually annotated data set. Both data sets include a combination of political
discourse and dialogue from movie subtitles. Our methods give us a broad and
varied amount of dehumanization data to work with, enabling further exploratory
analysis and automatic classification of dehumanization patterns. Both data
sets will be publicly released.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08765" title="Abstract">arXiv:2402.08765</a> [<a href="/pdf/2402.08765" title="Download PDF">pdf</a>, <a href="/format/2402.08765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who is driving the conversation? Analysing the nodality of British MPs  and journalists on Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castro-Gonzalez%2C+L">Leonardo Castro-Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Sukankana Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Margetts%2C+H">Helen Margetts</a>, 
<a href="/search/cs?searchtype=author&query=Rajpal%2C+H">Hardik Rajpal</a>, 
<a href="/search/cs?searchtype=author&query=Guariso%2C+D">Daniele Guariso</a>, 
<a href="/search/cs?searchtype=author&query=Bright%2C+J">Jonathan Bright</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 6 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Who sets the policy agenda? Much political science research has been
dedicated to investigating the influences at work in policy processes. This
paper looks at the actors influencing the policy agenda through the lens of
"nodality", the capacity to share information and to be at the centre of social
and information networks. Nodality has traditionally been seen as a capacity or
policy tool relating to government as a set of institutions. But in a digital
world, there is greater possibility for individual actors using social media to
develop their own nodality and to shape the policy conversation. We analyse
information networks on Twitter to quantify the nodality of two sets of actors
in the UK political system, Members of Parliament (MPs) and accredited
journalists, in conversations on four policy topics: The Russia Ukraine War,
the Cost of Living Crisis, Brexit and COVID 19. We perform a Transfer Entropy
analysis and a Principle Component Analysis (PCA) using different network
centralities while ensuring reproducibility and interpretability. We define and
measure two dimensions of nodality; inherent nodality, associated with an
actor's institutional position, transferable between topics, and individual
nodality, measuring active engagement in a policy conversation on a specific
topic. These two dimensions are validated through a linear model showing how
the influence on a particular topic exercised by an actor stems from the sum of
nodality scores across these two dimensions. We thereby show the relative
influence of different types of actor on policy conversations. More generally,
we provide a methodology for measuring individual and institutional drivers of
actor influence, and understanding where influence relates to individual or
institutional characteristics.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08769" title="Abstract">arXiv:2402.08769</a> [<a href="/pdf/2402.08769" title="Download PDF">pdf</a>, <a href="/format/2402.08769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLASH: Federated Learning Across Simultaneous Heterogeneities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiangyu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+S+M">Sk Miraj Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+S+V">Srikanth V. Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Guler%2C+B">Basak Guler</a>, 
<a href="/search/cs?searchtype=author&query=Swami%2C+A">Ananthram Swami</a>, 
<a href="/search/cs?searchtype=author&query=Oymak%2C+S">Samet Oymak</a>, 
<a href="/search/cs?searchtype=author&query=Roy-Chowdhury%2C+A+K">Amit K. Roy-Chowdhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The key premise of federated learning (FL) is to train ML models across a
diverse set of data-owners (clients), without exchanging local data. An
overarching challenge to this date is client heterogeneity, which may arise not
only from variations in data distribution, but also in data quality, as well as
compute/communication latency. An integrated view of these diverse and
concurrent sources of heterogeneity is critical; for instance, low-latency
clients may have poor data quality, and vice versa. In this work, we propose
FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight
and flexible client selection algorithm that outperforms state-of-the-art FL
frameworks under extensive sources of heterogeneity, by trading-off the
statistical information associated with the client's data quality, data
distribution, and latency. FLASH is the first method, to our knowledge, for
handling all these heterogeneities in a unified manner. To do so, FLASH models
the learning dynamics through contextual multi-armed bandits (CMAB) and
dynamically selects the most promising clients. Through extensive experiments,
we demonstrate that FLASH achieves substantial and consistent improvements over
state-of-the-art baselines -- as much as 10% in absolute accuracy -- thanks to
its unified approach. Importantly, FLASH also outperforms federated aggregation
methods that are designed to handle highly heterogeneous settings and even
enjoys a performance boost when integrated with them.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08771" title="Abstract">arXiv:2402.08771</a> [<a href="/pdf/2402.08771" title="Download PDF">pdf</a>, <a href="/ps/2402.08771" title="Download PostScript">ps</a>, <a href="/format/2402.08771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introducing RSESS: An Open Source Enumerative Sphere Shaping  Implementation Coded in Rust
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ritter%2C+F">Frederik Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Rode%2C+A">Andrej Rode</a>, 
<a href="/search/cs?searchtype=author&query=Schmalen%2C+L">Laurent Schmalen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the 13th GNU Radio conference (GRCon)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this work, we present an open-source implementation of the enumerative
sphere shaping (ESS) algorithm used for probabilistic constellation shaping
(PCS). PCS aims at closing the shaping gap caused by using uniformly
distributed modulation symbols in channels for which information theory shows
non-uniformly distributed signaling to be optimal. ESS is one such PCS
algorithm that sets itself apart as it operates on a trellis representation of
a subset of the possible symbol sequences. ESS leads to an empirical
distribution of the symbols that closely approximates the optimal distribution
for the additive white Gaussian noise (AWGN) channel. We provide an open-source
implementation of this algorithm in the compiled language Rust, as well as
Python bindings with which our Rust code can be called in a regular Python
script. We also compare simulation results on the AWGN channel using our
implementation with previous works on this topic.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08772" title="Abstract">arXiv:2402.08772</a> [<a href="/pdf/2402.08772" title="Download PDF">pdf</a>, <a href="/format/2402.08772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Task Assignment and Path Planning using Conflict-Based Search  with Precedence and Temporal Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chong%2C+Y+Q">Yu Quan Chong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sycara%2C+K">Katia Sycara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">The Multi-Agent Path Finding (MAPF) problem entails finding collision-free
paths for a set of agents, guiding them from their start to goal locations.
However, MAPF does not account for several practical task-related constraints.
For example, agents may need to perform actions at goal locations with specific
execution times, adhering to predetermined orders and timeframes. Moreover,
goal assignments may not be predefined for agents, and the optimization
objective may lack an explicit definition. To incorporate task assignment, path
planning, and a user-defined objective into a coherent framework, this paper
examines the Task Assignment and Path Finding with Precedence and Temporal
Constraints (TAPF-PTC) problem. We augment Conflict-Based Search (CBS) to
simultaneously generate task assignments and collision-free paths that adhere
to precedence and temporal constraints, maximizing an objective quantified by
the return from a user-defined reward function in reinforcement learning (RL).
Experimentally, we demonstrate that our algorithm, CBS-TA-PTC, can solve highly
challenging bomb-defusing tasks with precedence and temporal constraints
efficiently relative to MARL and adapted Target Assignment and Path Finding
(TAPF) methods.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08774" title="Abstract">arXiv:2402.08774</a> [<a href="/pdf/2402.08774" title="Download PDF">pdf</a>, <a href="/ps/2402.08774" title="Download PostScript">ps</a>, <a href="/format/2402.08774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LDTrack: Dynamic People Tracking by Service Robots using Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fung%2C+A">Angus Fung</a>, 
<a href="/search/cs?searchtype=author&query=Benhabib%2C+B">Beno Benhabib</a>, 
<a href="/search/cs?searchtype=author&query=Nejat%2C+G">Goldie Nejat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Tracking of dynamic people in cluttered and crowded human-centered
environments is a challenging robotics problem due to the presence of
intraclass variations including occlusions, pose deformations, and lighting
variations. This paper introduces a novel deep learning architecture, using
conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for
tracking multiple dynamic people under intraclass variations. By uniquely
utilizing conditional latent diffusion models to capture temporal person
embeddings, our architecture can adapt to appearance changes of people over
time. We incorporated a latent feature encoder network which enables the
diffusion process to operate within a high-dimensional latent space to allow
for the extraction and spatial-temporal refinement of such rich features as
person appearance, motion, location, identity, and contextual information.
Extensive experiments demonstrate the effectiveness of LDTrack over other
state-of-the-art tracking methods in cluttered and crowded human-centered
environments under intraclass variations. Namely, the results show our method
outperforms existing deep learning robotic people tracking methods in both
tracking accuracy and tracking precision with statistical significance.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08775" title="Abstract">arXiv:2402.08775</a> [<a href="/pdf/2402.08775" title="Download PDF">pdf</a>, <a href="/ps/2402.08775" title="Download PostScript">ps</a>, <a href="/format/2402.08775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost Tight Bounds for Online Hypergraph Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tr%C3%B6bst%2C+T">Thorben Tr&#xf6;bst</a>, 
<a href="/search/cs?searchtype=author&query=Udwani%2C+R">Rajan Udwani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In the online hypergraph matching problem, hyperedges of size $k$ over a
common ground set arrive online in adversarial order. The goal is to obtain a
maximum matching (disjoint set of hyperedges). A na\"ive greedy algorithm for
this problem achieves a competitive ratio of $\frac{1}{k}$. We show that no
(randomized) online algorithm has competitive ratio better than
$\frac{2+o(1)}{k}$. If edges are allowed to be assigned fractionally, we give a
deterministic online algorithm with competitive ratio $\frac{1-o(1)}{\ln(k)}$
and show that no online algorithm can have competitive ratio strictly better
than $\frac{1+o(1)}{\ln(k)}$. Lastly, we give a $\frac{1-o(1)}{\ln(k)}$
competitive algorithm for the fractional edge-weighted version of the problem
under a free disposal assumption.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08780" title="Abstract">arXiv:2402.08780</a> [<a href="/pdf/2402.08780" title="Download PDF">pdf</a>, <a href="/format/2402.08780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Deep Q-Learning for 2D Self-Driving Cars: Implementation and  Evaluation on a Custom Track Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pathak%2C+S">Sagar Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+B">Bidhya Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Pahi%2C+K">Kritish Pahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This research project presents the implementation of a Deep Q-Learning
Network (DQN) for a self-driving car on a 2-dimensional (2D) custom track, with
the objective of enhancing the DQN network's performance. It encompasses the
development of a custom driving environment using Pygame on a track surrounding
the University of Memphis map, as well as the design and implementation of the
DQN model. The algorithm utilizes data from 7 sensors installed in the car,
which measure the distance between the car and the track. These sensors are
positioned in front of the vehicle, spaced 20 degrees apart, enabling them to
sense a wide area ahead. We successfully implemented the DQN and also a
modified version of the DQN with a priority-based action selection mechanism,
which we refer to as modified DQN. The model was trained over 1000 episodes,
and the average reward received by the agent was found to be around 40, which
is approximately 60% higher than the original DQN and around 50% higher than
the vanilla neural network.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08784" title="Abstract">arXiv:2402.08784</a> [<a href="/pdf/2402.08784" title="Download PDF">pdf</a>, <a href="/format/2402.08784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preconditioners for the Stochastic Training of Implicit Neural  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chng%2C+S">Shin-Fang Chng</a>, 
<a href="/search/cs?searchtype=author&query=Saratchandran%2C+H">Hemanth Saratchandran</a>, 
<a href="/search/cs?searchtype=author&query=Lucey%2C+S">Simon Lucey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Implicit neural representations have emerged as a powerful technique for
encoding complex continuous multidimensional signals as neural networks,
enabling a wide range of applications in computer vision, robotics, and
geometry. While Adam is commonly used for training due to its stochastic
proficiency, it entails lengthy training durations. To address this, we explore
alternative optimization techniques for accelerated training without
sacrificing accuracy. Traditional second-order optimizers like L-BFGS are
suboptimal in stochastic settings, making them unsuitable for large-scale data
sets. Instead, we propose stochastic training using curvature-aware diagonal
preconditioners, showcasing their effectiveness across various signal
modalities such as images, shape reconstruction, and Neural Radiance Fields
(NeRF).
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08785" title="Abstract">arXiv:2402.08785</a> [<a href="/pdf/2402.08785" title="Download PDF">pdf</a>, <a href="/format/2402.08785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructGraph: Boosting Large Language Models via Graph-centric  Instruction Tuning and Preference Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junda Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yupeng Hou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Ming Gao</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Do current large language models (LLMs) better solve graph reasoning and
generation tasks with parameter updates? In this paper, we propose
InstructGraph, a framework that empowers LLMs with the abilities of graph
reasoning and generation by instruction tuning and preference alignment.
Specifically, we first propose a structured format verbalizer to unify all
graph data into a universal code-like format, which can simply represent the
graph without any external graph-specific encoders. Furthermore, a graph
instruction tuning stage is introduced to guide LLMs in solving graph reasoning
and generation tasks. Finally, we identify potential hallucination problems in
graph tasks and sample negative instances for preference alignment, the target
of which is to enhance the output's reliability of the model. Extensive
experiments across multiple graph-centric tasks exhibit that InstructGraph can
achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\%
and 38\%, respectively.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08787" title="Abstract">arXiv:2402.08787</a> [<a href="/pdf/2402.08787" title="Download PDF">pdf</a>, <a href="/format/2402.08787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Machine Unlearning for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuanshun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinghan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Casper%2C+S">Stephen Casper</a>, 
<a href="/search/cs?searchtype=author&query=Baracaldo%2C+N">Nathalie Baracaldo</a>, 
<a href="/search/cs?searchtype=author&query=Hase%2C+P">Peter Hase</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaojun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuguang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+K+R">Kush R. Varshney</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We explore machine unlearning (MU) in the domain of large language models
(LLMs), referred to as LLM unlearning. This initiative aims to eliminate
undesirable data influence (e.g., sensitive or illegal information) and the
associated model capabilities, while maintaining the integrity of essential
knowledge generation and not affecting causally unrelated information. We
envision LLM unlearning becoming a pivotal element in the life-cycle management
of LLMs, potentially standing as an essential foundation for developing
generative AI that is not only safe, secure, and trustworthy, but also
resource-efficient without the need of full retraining. We navigate the
unlearning landscape in LLMs from conceptual formulation, methodologies,
metrics, and applications. In particular, we highlight the often-overlooked
aspects of existing LLM unlearning research, e.g., unlearning scope, data-model
interaction, and multifaceted efficacy assessment. We also draw connections
between LLM unlearning and related areas such as model editing, influence
functions, model explanation, adversarial training, and reinforcement learning.
Furthermore, we outline an effective assessment framework for LLM unlearning
and explore its applications in copyright and privacy safeguards and
sociotechnical harm reduction.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08788" title="Abstract">arXiv:2402.08788</a> [<a href="/pdf/2402.08788" title="Download PDF">pdf</a>, <a href="/ps/2402.08788" title="Download PostScript">ps</a>, <a href="/format/2402.08788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Syllable based DNN-HMM Cantonese Speech to Text System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wong%2C+T">Timothy Wong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Claire Li</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+S">Sam Lam</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+B">Billy Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minglei Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Dan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R+S">Roy Shing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+V+T+Y">Vincent T.Y. Ng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, LREC 2016
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper reports our work on building up a Cantonese Speech-to-Text (STT)
system with a syllable based acoustic model. This is a part of an effort in
building a STT system to aid dyslexic students who have cognitive deficiency in
writing skills but have no problem expressing their ideas through speech. For
Cantonese speech recognition, the basic unit of acoustic models can either be
the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC)
syllables where finals are further split into nucleus and coda to reflect the
intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system
is trained using the stochastic gradient descent optimization model with the
aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model
(DNN-HMM) with and without I-vector based speaker adaptive training technique.
The input features of the same Gaussian Mixture Model with speaker adaptive
training (GMM-SAT) to DNN are used in all cases. Experiments show that the
ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the
best performance with the word error rate (WER) of 9.66% and the real time
factor (RTF) of 1.38812.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08790" title="Abstract">arXiv:2402.08790</a> [<a href="/pdf/2402.08790" title="Download PDF">pdf</a>, <a href="/format/2402.08790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Molecule Generation and Drug Discovery with a  Knowledge-enhanced Generative Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malusare%2C+A">Aditya Malusare</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Recent advancements in generative models have established state-of-the-art
benchmarks in generating molecules and novel drug candidates. Despite these
successes, a significant gap persists between generative models and the
utilization of extensive biomedical knowledge, often systematized within
knowledge graphs, whose potential to inform and enhance generative processes
has not been realized. In this paper, we present a novel approach that bridges
this divide by developing a framework for knowledge-enhanced generative models
called K-DReAM. We develop a scalable methodology to extend the functionality
of knowledge graphs while preserving semantic integrity and incorporate this
contextual information into a generative framework to guide a diffusion-based
model. The integration of knowledge graph embeddings with our generative model
furnishes a robust mechanism for producing novel drug candidates possessing
specific characteristics while ensuring validity and synthesizability. K-DReAM
outperforms state-of-the-art generative models on both unconditional and
targeted generation tasks.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08793" title="Abstract">arXiv:2402.08793</a> [<a href="/pdf/2402.08793" title="Download PDF">pdf</a>, <a href="/format/2402.08793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manzari%2C+O+N">Omid Nejati Manzari</a>, 
<a href="/search/cs?searchtype=author&query=Kaleybar%2C+J+M">Javad Mirzapour Kaleybar</a>, 
<a href="/search/cs?searchtype=author&query=Saadat%2C+H">Hooman Saadat</a>, 
<a href="/search/cs?searchtype=author&query=Maleki%2C+S">Shahin Maleki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The accurate segmentation of medical images is critical for various
healthcare applications. Convolutional neural networks (CNNs), especially Fully
Convolutional Networks (FCNs) like U-Net, have shown remarkable success in
medical image segmentation tasks. However, they have limitations in capturing
global context and long-range relations, especially for objects with
significant variations in shape, scale, and texture. While transformers have
achieved state-of-the-art results in natural language processing and image
recognition, they face challenges in medical image segmentation due to image
locality and translational invariance issues. To address these challenges, this
paper proposes an innovative U-shaped network called BEFUnet, which enhances
the fusion of body and edge information for precise medical image segmentation.
The BEFUnet comprises three main modules, including a novel Local
Cross-Attention Feature (LCAF) fusion module, a novel Double-Level Fusion (DLF)
module, and dual-branch encoder. The dual-branch encoder consists of an edge
encoder and a body encoder. The edge encoder employs PDC blocks for effective
edge information extraction, while the body encoder uses the Swin Transformer
to capture semantic information with global attention. The LCAF module
efficiently fuses edge and body features by selectively performing local
cross-attention on features that are spatially close between the two
modalities. This local approach significantly reduces computational complexity
compared to global cross-attention while ensuring accurate feature matching.
BEFUnet demonstrates superior performance over existing methods across various
evaluation metrics on medical image segmentation datasets.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08794" title="Abstract">arXiv:2402.08794</a> [<a href="/pdf/2402.08794" title="Download PDF">pdf</a>, <a href="/ps/2402.08794" title="Download PostScript">ps</a>, <a href="/format/2402.08794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An information-theoretic lower bound in time-uniform estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duchi%2C+J+C">John C. Duchi</a>, 
<a href="/search/cs?searchtype=author&query=Haque%2C+S">Saminul Haque</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">We present an information-theoretic lower bound for the problem of parameter
estimation with time-uniform coverage guarantees. Via a new a reduction to
sequential testing, we obtain stronger lower bounds that capture the hardness
of the time-uniform setting. In the case of location model estimation, logistic
regression, and exponential family models, our $\Omega(\sqrt{n^{-1}\log \log
n})$ lower bound is sharp to within constant factors in typical settings.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08796" title="Abstract">arXiv:2402.08796</a> [<a href="/pdf/2402.08796" title="Download PDF">pdf</a>, <a href="/format/2402.08796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reproducibility, Replicability, and Transparency in Research: What 430  Professors Think in Universities across the USA and India
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakravorti%2C+T">Tatiana Chakravorti</a>, 
<a href="/search/cs?searchtype=author&query=Koneru%2C+S+D">Sai Dileep Koneru</a>, 
<a href="/search/cs?searchtype=author&query=Rajtmajer%2C+S">Sarah Rajtmajer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In the past decade, open science and science of science communities have
initiated innovative efforts to address concerns about the reproducibility and
replicability of published scientific research. In some respects, these efforts
have been successful, yet there are still many pockets of researchers with
little to no familiarity with these concerns, subsequent responses, or best
practices for engaging in reproducible, replicable, and reliable scholarship.
In this work, we survey 430 professors from Universities across the USA and
India to understand perspectives on scientific processes and identify key
points for intervention. Our findings reveal both national and disciplinary
gaps in attention to reproducibility and replicability, aggravated by incentive
misalignment and resource constraints. We suggest that solutions addressing
scientific integrity should be culturally-centered, where definitions of
culture should include both regional and domain-specific elements.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08797" title="Abstract">arXiv:2402.08797</a> [<a href="/pdf/2402.08797" title="Download PDF">pdf</a>, <a href="/format/2402.08797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Power and the Governance of Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sastry%2C+G">Girish Sastry</a>, 
<a href="/search/cs?searchtype=author&query=Heim%2C+L">Lennart Heim</a>, 
<a href="/search/cs?searchtype=author&query=Belfield%2C+H">Haydn Belfield</a>, 
<a href="/search/cs?searchtype=author&query=Anderljung%2C+M">Markus Anderljung</a>, 
<a href="/search/cs?searchtype=author&query=Brundage%2C+M">Miles Brundage</a>, 
<a href="/search/cs?searchtype=author&query=Hazell%2C+J">Julian Hazell</a>, 
<a href="/search/cs?searchtype=author&query=O%27Keefe%2C+C">Cullen O&#x27;Keefe</a>, 
<a href="/search/cs?searchtype=author&query=Hadfield%2C+G+K">Gillian K. Hadfield</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+R">Richard Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Pilz%2C+K">Konstantin Pilz</a>, 
<a href="/search/cs?searchtype=author&query=Gor%2C+G">George Gor</a>, 
<a href="/search/cs?searchtype=author&query=Bluemke%2C+E">Emma Bluemke</a>, 
<a href="/search/cs?searchtype=author&query=Shoker%2C+S">Sarah Shoker</a>, 
<a href="/search/cs?searchtype=author&query=Egan%2C+J">Janet Egan</a>, 
<a href="/search/cs?searchtype=author&query=Trager%2C+R+F">Robert F. Trager</a>, 
<a href="/search/cs?searchtype=author&query=Avin%2C+S">Shahar Avin</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+A">Adrian Weller</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Coyle%2C+D">Diane Coyle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Figures can be accessed at: <a href="https://github.com/lheim/CPGAI-Figures">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Computing power, or "compute," is crucial for the development and deployment
of artificial intelligence (AI) capabilities. As a result, governments and
companies have started to leverage compute as a means to govern AI. For
example, governments are investing in domestic compute capacity, controlling
the flow of compute to competing countries, and subsidizing compute access to
certain sectors. However, these efforts only scratch the surface of how compute
can be used to govern AI development and deployment. Relative to other key
inputs to AI (data and algorithms), AI-relevant compute is a particularly
effective point of intervention: it is detectable, excludable, and
quantifiable, and is produced via an extremely concentrated supply chain. These
characteristics, alongside the singular importance of compute for cutting-edge
AI models, suggest that governing compute can contribute to achieving common
policy objectives, such as ensuring the safety and beneficial use of AI. More
precisely, policymakers could use compute to facilitate regulatory visibility
of AI, allocate resources to promote beneficial outcomes, and enforce
restrictions against irresponsible or malicious AI development and usage.
However, while compute-based policies and technologies have the potential to
assist in these areas, there is significant variation in their readiness for
implementation. Some ideas are currently being piloted, while others are
hindered by the need for fundamental research. Furthermore, naive or poorly
scoped approaches to compute governance carry significant risks in areas like
privacy, economic impacts, and centralization of power. We end by suggesting
guardrails to minimize these risks from compute governance.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08799" title="Abstract">arXiv:2402.08799</a> [<a href="/pdf/2402.08799" title="Download PDF">pdf</a>, <a href="/ps/2402.08799" title="Download PostScript">ps</a>, <a href="/format/2402.08799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Projection-Free Online Convex Optimization with Time-Varying Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garber%2C+D">Dan Garber</a>, 
<a href="/search/cs?searchtype=author&query=Kretzu%2C+B">Ben Kretzu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the setting of online convex optimization with adversarial
time-varying constraints in which actions must be feasible w.r.t. a fixed
constraint set, and are also required on average to approximately satisfy
additional time-varying constraints. Motivated by scenarios in which the fixed
feasible set (hard constraint) is difficult to project on, we consider
projection-free algorithms that access this set only through a linear
optimization oracle (LOO). We present an algorithm that, on a sequence of
length $T$ and using overall $T$ calls to the LOO, guarantees
$\tilde{O}(T^{3/4})$ regret w.r.t. the losses and $O(T^{7/8})$ constraints
violation (ignoring all quantities except for $T$) . In particular, these
bounds hold w.r.t. any interval of the sequence. We also present a more
efficient algorithm that requires only first-order oracle access to the soft
constraints and achieves similar bounds w.r.t. the entire sequence. We extend
the latter to the setting of bandit feedback and obtain similar bounds (as a
function of $T$) in expectation.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08801" title="Abstract">arXiv:2402.08801</a> [<a href="/pdf/2402.08801" title="Download PDF">pdf</a>, <a href="/ps/2402.08801" title="Download PostScript">ps</a>, <a href="/format/2402.08801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow  Discussions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Da+Silva%2C+L">Leuson Da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Samhi%2C+J">Jordan Samhi</a>, 
<a href="/search/cs?searchtype=author&query=Khomh%2C+F">Foutse Khomh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the
premier platform for developers' queries on programming and software
development. Demonstrating an ability to generate instant, human-like responses
to technical questions, ChatGPT has ignited debates within the developer
community about the evolving role of human-driven platforms in the age of
generative AI. Two months after ChatGPT's release, Meta released its answer
with its own Large Language Model (LLM) called LLaMA: the race was on. We
conducted an empirical study analyzing questions from Stack Overflow and using
these LLMs to address them. This way, we aim to (ii) measure user engagement
evolution with Stack Overflow over time; (ii) quantify the reliability of LLMs'
answers and their potential to replace Stack Overflow in the long term; (iii)
identify and understand why LLMs fails; and (iv) compare LLMs together. Our
empirical results are unequivocal: ChatGPT and LLaMA challenge human expertise,
yet do not outperform it for some domains, while a significant decline in user
posting activity has been observed. Furthermore, we also discuss the impact of
our findings regarding the usage and development of new LLMs.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08802" title="Abstract">arXiv:2402.08802</a> [<a href="/pdf/2402.08802" title="Download PDF">pdf</a>, <a href="/format/2402.08802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Label Zero-Shot Product Attribute-Value Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jiaying Gong</a>, 
<a href="/search/cs?searchtype=author&query=Eldardiry%2C+H">Hoda Eldardiry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures, WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">E-commerce platforms should provide detailed product descriptions (attribute
values) for effective product search and recommendation. However, attribute
value information is typically not available for new products. To predict
unseen attribute values, large quantities of labeled training data are needed
to train a traditional supervised learning model. Typically, it is difficult,
time-consuming, and costly to manually label large quantities of new product
profiles. In this paper, we propose a novel method to efficiently and
effectively extract unseen attribute values from new products in the absence of
labeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot
attribute value extraction model that leverages inductive inference in
heterogeneous hypergraphs. In particular, our proposed technique constructs
heterogeneous hypergraphs to capture complex higher-order relations (i.e. user
behavior information) to learn more accurate feature representations for graph
nodes. Furthermore, our proposed HyperPAVE model uses an inductive link
prediction mechanism to infer future connections between unseen nodes. This
enables HyperPAVE to identify new attribute values without the need for labeled
training data. We conduct extensive experiments with ablation studies on
different categories of the MAVE dataset. The results demonstrate that our
proposed HyperPAVE model significantly outperforms existing
classification-based, generation-based large language models for attribute
value extraction in the zero-shot setting.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08806" title="Abstract">arXiv:2402.08806</a> [<a href="/pdf/2402.08806" title="Download PDF">pdf</a>, <a href="/format/2402.08806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Insights From Multiple Large Language Models Improves  Diagnostic Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barabucci%2C+G">Gioele Barabucci</a>, 
<a href="/search/cs?searchtype=author&query=Shia%2C+V">Victor Shia</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+E">Eugene Chu</a>, 
<a href="/search/cs?searchtype=author&query=Harack%2C+B">Benjamin Harack</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+N">Nathan Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Background: Large language models (LLMs) such as OpenAI's GPT-4 or Google's
PaLM 2 are proposed as viable diagnostic support tools or even spoken of as
replacements for "curbside consults". However, even LLMs specifically trained
on medical topics may lack sufficient diagnostic accuracy for real-life
applications.
<br />Methods: Using collective intelligence methods and a dataset of 200 clinical
vignettes of real-life cases, we assessed and compared the accuracy of
differential diagnoses obtained by asking individual commercial LLMs (OpenAI
GPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of
differential diagnoses synthesized by aggregating responses from combinations
of the same LLMs.
<br />Results: We find that aggregating responses from multiple, various LLMs leads
to more accurate differential diagnoses (average accuracy for 3 LLMs:
$75.3\%\pm 1.6pp$) compared to the differential diagnoses produced by single
LLMs (average accuracy for single LLMs: $59.0\%\pm 6.1pp$).
<br />Discussion: The use of collective intelligence methods to synthesize
differential diagnoses combining the responses of different LLMs achieves two
of the necessary steps towards advancing acceptance of LLMs as a diagnostic
support tool: (1) demonstrate high diagnostic accuracy and (2) eliminate
dependence on a single commercial vendor.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08808" title="Abstract">arXiv:2402.08808</a> [<a href="/pdf/2402.08808" title="Download PDF">pdf</a>, <a href="/format/2402.08808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Depth Separation in Norm-Bounded Infinite-Width Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parkinson%2C+S">Suzanna Parkinson</a>, 
<a href="/search/cs?searchtype=author&query=Ongie%2C+G">Greg Ongie</a>, 
<a href="/search/cs?searchtype=author&query=Willett%2C+R">Rebecca Willett</a>, 
<a href="/search/cs?searchtype=author&query=Shamir%2C+O">Ohad Shamir</a>, 
<a href="/search/cs?searchtype=author&query=Srebro%2C+N">Nathan Srebro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study depth separation in infinite-width neural networks, where complexity
is controlled by the overall squared $\ell_2$-norm of the weights (sum of
squares of all weights in the network). Whereas previous depth separation
results focused on separation in terms of width, such results do not give
insight into whether depth determines if it is possible to learn a network that
generalizes well even when the network width is unbounded. Here, we study
separation in terms of the sample complexity required for learnability.
Specifically, we show that there are functions that are learnable with sample
complexity polynomial in the input dimension by norm-controlled depth-3 ReLU
networks, yet are not learnable with sub-exponential sample complexity by
norm-controlled depth-2 ReLU networks (with any value for the norm). We also
show that a similar statement in the reverse direction is not possible: any
function learnable with polynomial sample complexity by a norm-controlled
depth-2 ReLU network with infinite width is also learnable with polynomial
sample complexity by a norm-controlled depth-3 ReLU network.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08809" title="Abstract">arXiv:2402.08809</a> [<a href="/pdf/2402.08809" title="Download PDF">pdf</a>, <a href="/ps/2402.08809" title="Download PostScript">ps</a>, <a href="/format/2402.08809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Byzantine fault-tolerant distributed set intersection with redundancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+N+H">Nitin H. Vaidya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In this report, we study the problem of Byzantine fault-tolerant distributed
set intersection and the importance of redundancy in solving this problem.
Specifically, consider a distributed system with $n$ agents, each of which has
a local set. There are up to $f$ agents that are Byzantine faulty. The goal is
to find the intersection of the sets of the non-faulty agents.
<br />We derive the Byzantine set intersection problem from the Byzantine
optimization problem. We present the definition of $2f$-redundancy, and
identify the necessary and sufficient condition if the Byzantine set
intersection problem can be solved if a certain redundancy property is
satisfied, and then present an equivalent condition. We further extend our
results to arbitrary communication graphs in a decentralized setting. Finally,
we present solvability results for the Byzantine optimization problem, inspired
by our findings on Byzantine set intersection. The results we provide are for
synchronous and asynchronous systems both.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08812" title="Abstract">arXiv:2402.08812</a> [<a href="/pdf/2402.08812" title="Download PDF">pdf</a>, <a href="/format/2402.08812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Canvas: Enabling Design-Like Exploratory Visual Data  Analysis through Rapid Prototyping, Iteration and Curation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zijian Ding</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+J">Joel Chan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Complex data analysis inherently seeks unexpected insights through
exploratory \re{visual analysis} methods, transcending logical, step-by-step
processing. However, \re{existing interfaces such as notebooks and dashboards
have limitations in exploration and comparison for visual data analysis}.
Addressing these limitations, we introduce a "design-like" intelligent canvas
environment integrating generative AI into data analysis, offering rapid
prototyping, iteration, and comparative visualization management. Our dual
contributions include the integration of generative AI components into a canvas
interface, and empirical findings from a user study (N=10) evaluating the
effectiveness of the canvas interface.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08816" title="Abstract">arXiv:2402.08816</a> [<a href="/pdf/2402.08816" title="Download PDF">pdf</a>, <a href="/format/2402.08816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized dynamic data structure for Split Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majewski%2C+K">Konrad Majewski</a>, 
<a href="/search/cs?searchtype=author&query=Pilipczuk%2C+M">Micha&#x142; Pilipczuk</a>, 
<a href="/search/cs?searchtype=author&query=Zych-Pawlewicz%2C+A">Anna Zych-Pawlewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We design a randomized data structure that, for a fully dynamic graph $G$
updated by edge insertions and deletions and integers $k, d$ fixed upon
initialization, maintains the answer to the Split Completion problem: whether
one can add $k$ edges to $G$ to obtain a split graph. The data structure can be
initialized on an edgeless $n$-vertex graph in time $n \cdot (k d \cdot \log
n)^{\mathcal{O}(1)}$, and the amortized time complexity of an update is $5^k
\cdot (k d \cdot \log n)^{\mathcal{O}(1)}$. The answer provided by the data
structure is correct with probability $1-\mathcal{O}(n^{-d})$.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08817" title="Abstract">arXiv:2402.08817</a> [<a href="/pdf/2402.08817" title="Download PDF">pdf</a>, <a href="/format/2402.08817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adaptive System Architecture for Multimodal Intelligent  Transportation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Farooq%2C+M">Muhammad Farooq</a>, 
<a href="/search/eess?searchtype=author&query=Afraz%2C+N">Nima Afraz</a>, 
<a href="/search/eess?searchtype=author&query=Golpayegani%2C+F">Fatemeh Golpayegani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Multimodal intelligent transportation systems (M-ITS) encompass a range of
transportation services that utilise various modes of transport and incorporate
intelligent technologies for enhanced efficiency and user experience. There are
several challenges in M-ITS including data integration, Interoperability,
scalability, user experience, etc. To address these challenges, such a system
requires an adaptive system architecture that enables M-ITS to operate as an
integrated ecosystem. In this paper, we provide an adaptive, user-centric, and
layered architecture for multimodal transportation systems. The proposed
architecture ensures scalability for seamless interactions of various
subcomponents, that are often managed by different stakeholders. Concurrently,
the data architecture is detailed, covering diverse data sources, advanced
analytics, and stringent governance, providing a robust basis for intelligent
decision-making. We provide two example use cases of the proposed architecture,
showing how the data architecture and the system architecture can be fused and
serve multimodal intelligent transport services.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08819" title="Abstract">arXiv:2402.08819</a> [<a href="/pdf/2402.08819" title="Download PDF">pdf</a>, <a href="/ps/2402.08819" title="Download PostScript">ps</a>, <a href="/format/2402.08819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infinite-horizon optimal scheduling for feedback control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Siyi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Hirche%2C+S">Sandra Hirche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Emerging cyber-physical systems impel the development of communication
protocols to efficiently utilize resources. This paper investigates the optimal
co-design of control and scheduling in networked control systems. The objective
is to co-design the control law and the scheduling mechanism that jointly
optimize the tradeoff between regulation performance and communication resource
consumption in the long run. The concept of the value of information (VoI) is
employed to evaluate the importance of data being transmitted. The optimal
solution includes a certainty equivalent control law and a stationary
scheduling policy based on the VoI function. The closed-loop system under the
designed scheduling policy is shown to be stochastically stable. By analyzing
the property of the VoI function, we show that the optimal scheduling policy is
symmetric and is a monotone function when the system matrix is diagonal.
Moreover, by the diagonal system matrix assumption, the optimal scheduling
policy is shown to be of threshold type. Then we provide a simplified yet
equivalent form of the threshold-based optimal scheduling policy. The threshold
value searching region is also given. Finally, the numerical simulation
illustrates the theoretical result of the VoI-based scheduling.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08823" title="Abstract">arXiv:2402.08823</a> [<a href="/pdf/2402.08823" title="Download PDF">pdf</a>, <a href="/format/2402.08823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RanDumb: A Simple Approach that Questions the Efficacy of Continual  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+A">Ameya Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S">Shiven Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Sener%2C+O">Ozan Sener</a>, 
<a href="/search/cs?searchtype=author&query=Dokania%2C+P+K">Puneet K. Dokania</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose RanDumb to examine the efficacy of continual representation
learning. RanDumb embeds raw pixels using a fixed random transform which
approximates an RBF-Kernel, initialized before seeing any data, and learns a
simple linear classifier on top. We present a surprising and consistent
finding: RanDumb significantly outperforms the continually learned
representations using deep networks across numerous continual learning
benchmarks, demonstrating the poor performance of representation learning in
these scenarios. RanDumb stores no exemplars and performs a single pass over
the data, processing one sample at a time. It complements GDumb, operating in a
low-exemplar regime where GDumb has especially poor performance. We reach the
same consistent conclusions when RanDumb is extended to scenarios with
pretrained models replacing the random transform with pretrained feature
extractor. Our investigation is both surprising and alarming as it questions
our understanding of how to effectively design and train models that require
efficient continual representation learning, and necessitates a principled
reinvestigation of the widely explored problem formulation itself. Our code is
available at https://github.com/drimpossible/RanDumb.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08824" title="Abstract">arXiv:2402.08824</a> [<a href="/pdf/2402.08824" title="Download PDF">pdf</a>, <a href="/format/2402.08824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disambiguated Node Classification with Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tianxiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WebConf (WWW) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data across various domains. Despite their great
successful, one critical challenge is often overlooked by existing works, i.e.,
the learning of message propagation that can generalize effectively to
underrepresented graph regions. These minority regions often exhibit irregular
homophily/heterophily patterns and diverse neighborhood class distributions,
resulting in ambiguity. In this work, we investigate the ambiguity problem
within GNNs, its impact on representation learning, and the development of
richer supervision signals to fight against this problem. We conduct a
fine-grained evaluation of GNN, analyzing the existence of ambiguity in
different graph regions and its relation with node positions. To disambiguate
node embeddings, we propose a novel method, {\method}, which exploits
additional optimization guidance to enhance representation learning,
particularly for nodes in ambiguous regions. {\method} identifies ambiguous
nodes based on temporal inconsistency of predictions and introduces a
disambiguation regularization by employing contrastive learning in a
topology-aware manner. {\method} promotes discriminativity of node
representations and can alleviating semantic mixing caused by message
propagation, effectively addressing the ambiguity problem. Empirical results
validate the efficiency of {\method} and highlight its potential to improve GNN
performance in underrepresented graph regions.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08825" title="Abstract">arXiv:2402.08825</a> [<a href="/pdf/2402.08825" title="Download PDF">pdf</a>, <a href="/format/2402.08825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximizing Throughput with Routing Interference Avoidance in  RIS-Assisted Relay Mesh Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phung%2C+C+V">Cao Vien Phung</a>, 
<a href="/search/cs?searchtype=author&query=Drummond%2C+A">Andre Drummond</a>, 
<a href="/search/cs?searchtype=author&query=Jukan%2C+A">Admela Jukan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is uploaded here for research community, thus it is for non-commercial purposes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In the modern landscape of wireless communications, multi-hop,
high-bandwidth, indoor Terahertz (THz) wireless communications are gaining
significant attention. These systems couple Reconfigurable Intelligent Surface
(RIS) and relay devices within the emerging 6G network framework, offering
promising solutions for creating cell-less, indoor, and on-demand mesh
networks. RIS devices are especially attractive, constructed by an array of
reflecting elements that can phase shifts, such that the reflecting signals can
be focused, steered, and the power of the signal enhanced towards the
destination. This paper presents an in-depth, analytical examination of how
path allocation impacts interference within such networks. We develop the first
model which analyzes interference based on the geometric parameters of beams
(conic, cylindrical) as they interact with RIS, User Equipment (UE), and relay
devices. We introduce a transmission scheduling heuristic designed to mitigate
interference, alongside an efficient optimization method to maximize
throughput. Our performance results elucidate the interference's effect on
communication path quality and highlight effective path selection strategies
with throughput maximization.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08826" title="Abstract">arXiv:2402.08826</a> [<a href="/pdf/2402.08826" title="Download PDF">pdf</a>, <a href="/format/2402.08826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equilibria of Data Marketplaces with Privacy-Aware Sellers under  Endogenous Privacy Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sen%2C+D">Diptangshu Sen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ziani%2C+J">Juba Ziani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study a two-sided online data ecosystem comprised of an online platform,
users on the platform, and downstream learners or data buyers. The learners can
buy user data on the platform (to run a statistic or machine learning task).
Potential users decide whether to join by looking at the trade-off between i)
their benefit from joining the platform and interacting with other users and
ii) the privacy costs they incur from sharing their data.
<br />First, we introduce a novel modeling element for two-sided data platforms:
the privacy costs of the users are endogenous and depend on how much of their
data is purchased by the downstream learners. Then, we characterize marketplace
equilibria in certain simple settings. In particular, we provide a full
characterization in two variants of our model that correspond to different
utility functions for the users: i) when each user gets a constant benefit for
participating in the platform and ii) when each user's benefit is linearly
increasing in the number of other users that participate. In both variants,
equilibria in our setting are significantly different from equilibria when
privacy costs are exogenous and fixed, highlighting the importance of taking
endogeneity in the privacy costs into account. Finally, we provide simulations
and semi-synthetic experiments to extend our results to more general
assumptions. We experiment with different distributions of users' privacy costs
and different functional forms of the users' utilities for joining the
platform.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08830" title="Abstract">arXiv:2402.08830</a> [<a href="/pdf/2402.08830" title="Download PDF">pdf</a>, <a href="/ps/2402.08830" title="Download PostScript">ps</a>, <a href="/format/2402.08830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequence graphs realizations and ambiguity in language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalife%2C+S">Sammy Khalife</a>, 
<a href="/search/cs?searchtype=author&query=Ponty%2C+Y">Yann Ponty</a>, 
<a href="/search/cs?searchtype=author&query=Bulteau%2C+L">Laurent Bulteau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Computation and Language (cs.CL)

</div>
<p class="mathjax">Several popular language models represent local contexts in an input text as
bags of words. Such representations are naturally encoded by a sequence graph
whose vertices are the distinct words occurring in x, with edges representing
the (ordered) co-occurrence of two words within a sliding window of size w.
However, this compressed representation is not generally bijective, and may
introduce some degree of ambiguity. Some sequence graphs may admit several
realizations as a sequence, while others may not admit any realization. In this
paper, we study the realizability and ambiguity of sequence graphs from a
combinatorial and computational point of view. We consider the existence and
enumeration of realizations of a sequence graph under multiple settings: window
size w, presence/absence of graph orientation, and presence/absence of weights
(multiplicities). When w = 2, we provide polynomial time algorithms for
realizability and enumeration in all cases except the undirected/weighted
setting, where we show the #P-hardness of enumeration. For a window of size at
least 3, we prove hardness of all variants, even when w is considered as a
constant, with the notable exception of the undirected/unweighted case for
which we propose an XP algorithms for both (realizability and enumeration)
problems, tight due to a corresponding W[1]-hardness result. We conclude with
an integer program formulation to solve the realizability problem, and with
dynamic programming to solve the enumeration problem. This work leaves open the
membership to NP for both problems, a non-trivial question due to the existence
of minimum realizations having exponential size on the instance encoding.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08831" title="Abstract">arXiv:2402.08831</a> [<a href="/pdf/2402.08831" title="Download PDF">pdf</a>, <a href="/format/2402.08831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> eCeLLM: Generalizing Large Language Models for E-commerce from  Large-scale, High-quality Instruction Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+X">Xinyi Ling</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziru Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xia Ning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Bo Peng and Xinyi Ling contributed equally to this paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">With tremendous efforts on developing effective e-commerce models,
conventional e-commerce models show limited success in generalist e-commerce
modeling, and suffer from unsatisfactory performance on new users and new
products - a typical out-of-domain generalization challenge. Meanwhile, large
language models (LLMs) demonstrate outstanding performance in generalist
modeling and out-of-domain generalizability in many fields. Toward fully
unleashing their power for e-commerce, in this paper, we construct ECInstruct,
the first open-sourced, large-scale, and high-quality benchmark instruction
dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of
e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive
experiments and evaluation demonstrate that eCeLLM models substantially
outperform baseline models, including the most advanced GPT-4, and the
state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM
exhibits excellent generalizability to out-of-domain settings, including unseen
products and unseen instructions, highlighting its superiority as a generalist
e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great
potential in empowering versatile and effective LLMs for e-commerce. ECInstruct
and eCeLLM models are publicly accessible through
https://ninglab.github.io/eCeLLM.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08832" title="Abstract">arXiv:2402.08832</a> [<a href="/pdf/2402.08832" title="Download PDF">pdf</a>, <a href="/format/2402.08832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Agricultural Management Considering N$_2$O Emission and  Climate Variability with Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shaoping Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Parab%2C+A">Ashwin Parab</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+S">Shivam Patel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">This study examines how artificial intelligence (AI), especially
Reinforcement Learning (RL), can be used in farming to boost crop yields,
fine-tune nitrogen use and watering, and reduce nitrate runoff and greenhouse
gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil. Facing climate
change and limited agricultural knowledge, we use Partially Observable Markov
Decision Processes (POMDPs) with a crop simulator to model AI agents'
interactions with farming environments. We apply deep Q-learning with Recurrent
Neural Network (RNN)-based Q networks for training agents on optimal actions.
Also, we develop Machine Learning (ML) models to predict N$_2$O emissions,
integrating these predictions into the simulator. Our research tackles
uncertainties in N$_2$O emission estimates with a probabilistic ML approach and
climate variability through a stochastic weather model, offering a range of
emission outcomes to improve forecast reliability and decision-making. By
incorporating climate change effects, we enhance agents' climate adaptability,
aiming for resilient agricultural practices. Results show these agents can
align crop productivity with environmental concerns by penalizing N$_2$O
emissions, adapting effectively to climate shifts like warmer temperatures and
less rain. This strategy improves farm management under climate change,
highlighting AI's role in sustainable agriculture.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08837" title="Abstract">arXiv:2402.08837</a> [<a href="/pdf/2402.08837" title="Download PDF">pdf</a>, <a href="/format/2402.08837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Generate Context-Sensitive Backchannel Smiles for Embodied  AI Agents with Applications in Mental Health Dialogues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bilalpur%2C+M">Maneesh Bilalpur</a>, 
<a href="/search/cs?searchtype=author&query=Inan%2C+M">Mert Inan</a>, 
<a href="/search/cs?searchtype=author&query=Zeinali%2C+D">Dorsa Zeinali</a>, 
<a href="/search/cs?searchtype=author&query=Cohn%2C+J+F">Jeffrey F. Cohn</a>, 
<a href="/search/cs?searchtype=author&query=Alikhani%2C+M">Malihe Alikhani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Machine Learning for Cognitive and Mental Health Workshop at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Addressing the critical shortage of mental health resources for effective
screening, diagnosis, and treatment remains a significant challenge. This
scarcity underscores the need for innovative solutions, particularly in
enhancing the accessibility and efficacy of therapeutic support. Embodied
agents with advanced interactive capabilities emerge as a promising and
cost-effective supplement to traditional caregiving methods. Crucial to these
agents' effectiveness is their ability to simulate non-verbal behaviors, like
backchannels, that are pivotal in establishing rapport and understanding in
therapeutic contexts but remain under-explored. To improve the rapport-building
capabilities of embodied agents we annotated backchannel smiles in videos of
intimate face-to-face conversations over topics such as mental health, illness,
and relationships. We hypothesized that both speaker and listener behaviors
affect the duration and intensity of backchannel smiles. Using cues from speech
prosody and language along with the demographics of the speaker and listener,
we found them to contain significant predictors of the intensity of backchannel
smiles. Based on our findings, we introduce backchannel smile production in
embodied agents as a generation problem. Our attention-based generative model
suggests that listener information offers performance improvements over the
baseline speaker-centric generation approach. Conditioned generation using the
significant predictors of smile intensity provides statistically significant
improvements in empirical measures of generation quality. Our user study by
transferring generated smiles to an embodied agent suggests that agent with
backchannel smiles is perceived to be more human-like and is an attractive
alternative for non-personal conversations over agent without backchannel
smiles.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08841" title="Abstract">arXiv:2402.08841</a> [<a href="/pdf/2402.08841" title="Download PDF">pdf</a>, <a href="/format/2402.08841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Sequential Optimization for Informative Path Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ott%2C+J">Joshua Ott</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>, 
<a href="/search/cs?searchtype=author&query=Boyd%2C+S">Stephen Boyd</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We consider the problem of finding an informative path through a graph, given
initial and terminal nodes and a given maximum path length. We assume that a
linear noise corrupted measurement is taken at each node of an underlying
unknown vector that we wish to estimate. The informativeness is measured by the
reduction in uncertainty in our estimate, evaluated using several metrics. We
present a convex relaxation for this informative path planning problem, which
we can readily solve to obtain a bound on the possible performance. We develop
an approximate sequential method where the path is constructed segment by
segment through dynamic programming. This involves solving an orienteering
problem, with the node reward acting as a surrogate for informativeness, taking
the first step, and then repeating the process. The method scales to very large
problem instances and achieves performance not too far from the bound produced
by the convex relaxation. We also demonstrate our method's ability to handle
adaptive objectives, multimodal sensing, and multi-agent variations of the
informative path planning problem.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08845" title="Abstract">arXiv:2402.08845</a> [<a href="/pdf/2402.08845" title="Download PDF">pdf</a>, <a href="/format/2402.08845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Attribution with Necessity and Sufficiency via Dual-stage  Perturbation Test for Causal Explanation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuexin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Ruichu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhengting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuxuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Horwood%2C+J">Julien Horwood</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhifeng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijian Li</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez-Lobato%2C+J+M">Jose Miguel Hernandez-Lobato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">We investigate the problem of explainability in machine learning.To address
this problem, Feature Attribution Methods (FAMs) measure the contribution of
each feature through a perturbation test, where the difference in prediction is
compared under different perturbations.However, such perturbation tests may not
accurately distinguish the contributions of different features, when their
change in prediction is the same after perturbation.In order to enhance the
ability of FAMs to distinguish different features' contributions in this
challenging setting, we propose to utilize the probability (PNS) that
perturbing a feature is a necessary and sufficient cause for the prediction to
change as a measure of feature importance.Our approach, Feature Attribution
with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test
involving two stages (factual and interventional).In practice, to generate
counterfactual samples, we use a resampling-based approach on the observed
samples to approximate the required conditional distribution.Finally, we
combine FANS and gradient-based optimization to extract the subset with the
largest PNS.We demonstrate that FANS outperforms existing feature attribution
methods on six benchmarks.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08846" title="Abstract">arXiv:2402.08846</a> [<a href="/pdf/2402.08846" title="Download PDF">pdf</a>, <a href="/format/2402.08846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Embarrassingly Simple Approach for LLM with Strong ASR Capacity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guanrou Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhifu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhihao Du</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Siqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xie Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in progress and will open-source soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we focus on solving one of the most important tasks in the
field of speech processing, i.e., automatic speech recognition (ASR), with
speech foundation encoders and large language models (LLM). Recent works have
complex designs such as compressing the output temporally for the speech
encoder, tackling modal alignment for the projector, and utilizing
parameter-efficient fine-tuning for the LLM. We found that delicate designs are
not necessary, while an embarrassingly simple composition of off-the-shelf
speech encoder, LLM, and the only trainable linear projector is competent for
the ASR task. To be more specific, we benchmark and explore various
combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR
system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup
and little task-specific design, where only the linear projector is trained. To
the best of our knowledge, SLAM-ASR achieves the best performance on the
Librispeech benchmark among LLM-based ASR models and even outperforms the
latest LLM-based audio-universal model trained on massive pair data. Finally,
we explore the capability emergence of LLM-based ASR in the process of modal
alignment. We hope that our study can facilitate the research on extending LLM
with cross-modality capacity and shed light on the LLM-based ASR community.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08848" title="Abstract">arXiv:2402.08848</a> [<a href="/pdf/2402.08848" title="Download PDF">pdf</a>, <a href="/format/2402.08848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Inverse Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Juntao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Swamy%2C+G">Gokul Swamy</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bagnell%2C+J+A">J. Andrew Bagnell</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sanjiban Choudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The inverse reinforcement learning approach to imitation learning is a
double-edged sword. On the one hand, it can enable learning from a smaller
number of expert demonstrations with more robustness to error compounding than
behavioral cloning approaches. On the other hand, it requires that the learner
repeatedly solve a computationally expensive reinforcement learning (RL)
problem. Often, much of this computation is wasted searching over policies very
dissimilar to the expert's. In this work, we propose using hybrid RL --
training on a mixture of online and expert data -- to curtail unnecessary
exploration. Intuitively, the expert data focuses the learner on good states
during training, which reduces the amount of exploration required to compute a
strong policy. Notably, such an approach doesn't need the ability to reset the
learner to arbitrary states in the environment, a requirement of prior work in
efficient inverse RL. More formally, we derive a reduction from inverse RL to
expert-competitive RL (rather than globally optimal RL) that allows us to
dramatically reduce interaction during the inner policy search loop while
maintaining the benefits of the IRL approach. This allows us to derive both
model-free and model-based hybrid inverse RL algorithms with strong policy
performance guarantees. Empirically, we find that our approaches are
significantly more sample efficient than standard inverse RL and several other
baselines on a suite of continuous control tasks.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08851" title="Abstract">arXiv:2402.08851</a> [<a href="/pdf/2402.08851" title="Download PDF">pdf</a>, <a href="/ps/2402.08851" title="Download PostScript">ps</a>, <a href="/format/2402.08851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cardinal-Utility Matching Markets: The Quest for Envy-Freeness,  Pareto-Optimality, and Efficient Computability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Troebst%2C+T">Thorben Troebst</a>, 
<a href="/search/cs?searchtype=author&query=Vazirani%2C+V+V">Vijay V. Vazirani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Unlike ordinal-utility matching markets, which are well-developed from the
viewpoint of both theory and practice, recent insights from a computer science
perspective have left cardinal-utility matching markets in a quandary. The
celebrated pricing-based mechanism for one-sided cardinal-utility matching
markets due to Hylland and Zeckhauser, which had long eluded efficient
algorithms, was finally shown to be PPAD-complete.
<br />This led us to ask the question: is there an alternative, polynomial time,
mechanism for one-sided cardinal-utility matching markets which achieves the
desirable properties of HZ, i.e.\ (ex-ante) envy-freeness (EF) and
Pareto-optimality (PO)? In this paper we show:
<br />1. The problem of finding an EF+PO lottery in a one-sided cardinal-utility
matching market is PPAD-complete.
<br />2. A $(2 + \epsilon)$-approximately envy-free and (exactly) Pareto-optimal
lottery can be found in polynomial time using Nash bargaining.
<br />We also present several results on two-sided cardinal-utility matching
markets, including non-existence of EF+PO lotteries as well as existence of
justified-envy-free and weak Pareto-optimal lotteries.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08855" title="Abstract">arXiv:2402.08855</a> [<a href="/pdf/2402.08855" title="Download PDF">pdf</a>, <a href="/format/2402.08855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GhostWriter: Augmenting Collaborative Human-AI Writing Experiences  Through Personalization and Agency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C">Catherine Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+G">Gonzalo Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+R">Rachel Ng</a>, 
<a href="/search/cs?searchtype=author&query=Huntington%2C+A">Andy Huntington</a>, 
<a href="/search/cs?searchtype=author&query=Banks%2C+R">Richard Banks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are becoming more prevalent and have found a
ubiquitous use in providing different forms of writing assistance. However,
LLM-powered writing systems can frustrate users due to their limited
personalization and control, which can be exacerbated when users lack
experience with prompt engineering. We see design as one way to address these
challenges and introduce GhostWriter, an AI-enhanced writing design probe where
users can exercise enhanced agency and personalization. GhostWriter leverages
LLMs to learn the user's intended writing style implicitly as they write, while
allowing explicit teaching moments through manual style edits and annotations.
We study 18 participants who use GhostWriter on two different writing tasks,
observing that it helps users craft personalized text generations and empowers
them by providing multiple ways to control the system's writing style. From
this study, we present insights regarding people's relationship with
AI-assisted writing and offer design recommendations for future work.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08856" title="Abstract">arXiv:2402.08856</a> [<a href="/pdf/2402.08856" title="Download PDF">pdf</a>, <a href="/format/2402.08856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation of relation functions and attention mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altabaa%2C+A">Awni Altabaa</a>, 
<a href="/search/cs?searchtype=author&query=Lafferty%2C+J">John Lafferty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Inner products of neural network feature maps arises in a wide variety of
machine learning frameworks as a method of modeling relations between inputs.
This work studies the approximation properties of inner products of neural
networks. It is shown that the inner product of a multi-layer perceptron with
itself is a universal approximator for symmetric positive-definite relation
functions. In the case of asymmetric relation functions, it is shown that the
inner product of two different multi-layer perceptrons is a universal
approximator. In both cases, a bound is obtained on the number of neurons
required to achieve a given accuracy of approximation. In the symmetric case,
the function class can be identified with kernels of reproducing kernel Hilbert
spaces, whereas in the asymmetric case the function class can be identified
with kernels of reproducing kernel Banach spaces. Finally, these approximation
results are applied to analyzing the attention mechanism underlying
Transformers, showing that any retrieval mechanism defined by an abstract
preorder can be approximated by attention through its inner product relations.
This result uses the Debreu representation theorem in economics to represent
preference relations in terms of utility functions.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08857" title="Abstract">arXiv:2402.08857</a> [<a href="/pdf/2402.08857" title="Download PDF">pdf</a>, <a href="/format/2402.08857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Planning for Articulated Robots Using Reachability-based Obstacle  Avoidance With Spheres
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michaux%2C+J">Jonathan Michaux</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Adam Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Che Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bohao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Vasudevan%2C+R">Ram Vasudevan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Generating safe motion plans in real-time is necessary for the wide-scale
deployment of robots in unstructured and human-centric environments. These
motion plans must be safe to ensure humans are not harmed and nearby objects
are not damaged. However, they must also be generated in real-time to ensure
the robot can quickly adapt to changes in the environment. Many trajectory
optimization methods introduce heuristics that trade-off safety and real-time
performance, which can lead to potentially unsafe plans. This paper addresses
this challenge by proposing Safe Planning for Articulated Robots Using
Reachability-based Obstacle Avoidance With Spheres (SPARROWS). SPARROWS is a
receding-horizon trajectory planner that utilizes the combination of a novel
reachable set representation and an exact signed distance function to generate
provably-safe motion plans. At runtime, SPARROWS uses parameterized
trajectories to compute reachable sets composed entirely of spheres that
overapproximate the swept volume of the robot's motion. SPARROWS then performs
trajectory optimization to select a safe trajectory that is guaranteed to be
collision-free. We demonstrate that SPARROWS' novel reachable set is
significantly less conservative than previous approaches. We also demonstrate
that SPARROWS outperforms a variety of state-of-the-art methods in solving
challenging motion planning tasks in cluttered environments. Code, data, and
video demonstrations can be found at
\url{https://roahmlab.github.io/sparrows/}.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08859" title="Abstract">arXiv:2402.08859</a> [<a href="/pdf/2402.08859" title="Download PDF">pdf</a>, <a href="/format/2402.08859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model with Graph Convolution for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yingpeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+H">Haoyan Chua</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongzhi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhonghai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yining Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Youchen Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In recent years, efforts have been made to use text information for better
user profiling and item characterization in recommendations. However, text
information can sometimes be of low quality, hindering its effectiveness for
real-world applications. With knowledge and reasoning capabilities capsuled in
Large Language Models (LLMs), utilizing LLMs emerges as a promising way for
description improvement. However, existing ways of prompting LLMs with raw
texts ignore structured knowledge of user-item interactions, which may lead to
hallucination problems like inconsistent description generation. To this end,
we propose a Graph-aware Convolutional LLM method to elicit LLMs to capture
high-order relations in the user-item graph. To adapt text-based LLMs with
structured graphs, We use the LLM as an aggregator in graph processing,
allowing it to understand graph-based information step by step. Specifically,
the LLM is required for description enhancement by exploring multi-hop
neighbors layer by layer, thereby propagating information progressively in the
graph. To enable LLMs to capture large-scale graph information, we break down
the description task into smaller parts, which drastically reduces the context
length of the token input with each step. Extensive experiments on three
real-world datasets show that our method consistently outperforms
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08863" title="Abstract">arXiv:2402.08863</a> [<a href="/pdf/2402.08863" title="Download PDF">pdf</a>, <a href="/format/2402.08863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale graph neural networks with adaptive mesh refinement for  accelerating mesh-based simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perera%2C+R">Roberto Perera</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+V">Vinamra Agrawal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Mesh-based Graph Neural Networks (GNNs) have recently shown capabilities to
simulate complex multiphysics problems with accelerated performance times.
However, mesh-based GNNs require a large number of message-passing (MP) steps
and suffer from over-smoothing for problems involving very fine mesh. In this
work, we develop a multiscale mesh-based GNN framework mimicking a conventional
iterative multigrid solver, coupled with adaptive mesh refinement (AMR), to
mitigate challenges with conventional mesh-based GNNs. We use the framework to
accelerate phase field (PF) fracture problems involving coupled partial
differential equations with a near-singular operator due to near-zero modulus
inside the crack. We define the initial graph representation using all mesh
resolution levels. We perform a series of downsampling steps using Transformer
MP GNNs to reach the coarsest graph followed by upsampling steps to reach the
original graph. We use skip connectors from the generated embedding during
coarsening to prevent over-smoothing. We use Transfer Learning (TL) to
significantly reduce the size of training datasets needed to simulate different
crack configurations and loading conditions. The trained framework showed
accelerated simulation times, while maintaining high accuracy for all cases
compared to physics-based PF fracture model. Finally, this work provides a new
approach to accelerate a variety of mesh-based engineering multiphysics
problems
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08864" title="Abstract">arXiv:2402.08864</a> [<a href="/pdf/2402.08864" title="Download PDF">pdf</a>, <a href="/format/2402.08864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hebbar%2C+S+A">S Ashwin Hebbar</a>, 
<a href="/search/cs?searchtype=author&query=Ankireddy%2C+S+K">Sravan Kumar Ankireddy</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeji Kim</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sewoong Oh</a>, 
<a href="/search/cs?searchtype=author&query=Viswanath%2C+P">Pramod Viswanath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Polar codes, developed on the foundation of Arikan's polarization kernel,
represent a breakthrough in coding theory and have emerged as the
state-of-the-art error-correction-code in short-to-medium block length regimes.
Importantly, recent research has indicated that the reliability of polar codes
can be further enhanced by substituting Arikan's kernel with a larger one,
leading to a faster polarization. However, for short-to-medium block length
regimes, the development of polar codes that effectively employ large kernel
sizes has not yet been realized. In this paper, we explore a novel, non-linear
generalization of polar codes with an expanded kernel size, which we call
DeepPolar codes. Our results show that DeepPolar codes effectively utilize the
benefits of larger kernel size, resulting in enhanced reliability compared to
both the existing neural codes and conventional polar codes.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08867" title="Abstract">arXiv:2402.08867</a> [<a href="/pdf/2402.08867" title="Download PDF">pdf</a>, <a href="/format/2402.08867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Optimization with Consensus Constraint for Multi-Robot  Semantic Octree Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asgharivaskasi%2C+A">Arash Asgharivaskasi</a>, 
<a href="/search/cs?searchtype=author&query=Atanasov%2C+N">Nikolay Atanasov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This work develops a distributed optimization algorithm for multi-robot 3-D
semantic mapping using streaming range and visual observations and single-hop
communication. Our approach relies on gradient-based optimization of the
observation log-likelihood of each robot subject to a map consensus constraint
to build a common multi-class map of the environment. This formulation leads to
closed-form updates which resemble Bayes rule with one-hop prior averaging. To
reduce the amount of information exchanged among the robots, we utilize an
octree data structure that compresses the multi-class map distribution using
adaptive-resolution.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08869" title="Abstract">arXiv:2402.08869</a> [<a href="/pdf/2402.08869" title="Download PDF">pdf</a>, <a href="/ps/2402.08869" title="Download PostScript">ps</a>, <a href="/format/2402.08869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScamSpot: Fighting Financial Fraud in Instagram Comments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erben%2C+S">Stefan Erben</a>, 
<a href="/search/cs?searchtype=author&query=Waldis%2C+A">Andreas Waldis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 Demo Paper, 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The long-standing problem of spam and fraudulent messages in the comment
sections of Instagram pages in the financial sector claims new victims every
day. Instagram's current spam filter proves inadequate, and existing research
approaches are primarily confined to theoretical concepts. Practical
implementations with evaluated results are missing. To solve this problem, we
propose ScamSpot, a comprehensive system that includes a browser extension, a
fine-tuned BERT model and a REST API. This approach ensures public
accessibility of our results for Instagram users using the Chrome browser.
Furthermore, we conduct a data annotation study, shedding light on the reasons
and causes of the problem and evaluate the system through user feedback and
comparison with existing models. ScamSpot is an open-source project and is
publicly available at https://scamspot.github.io/.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08871" title="Abstract">arXiv:2402.08871</a> [<a href="/pdf/2402.08871" title="Download PDF">pdf</a>, <a href="/format/2402.08871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: Challenges and Opportunities in Topological Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papamarkou%2C+T">Theodore Papamarkou</a>, 
<a href="/search/cs?searchtype=author&query=Birdal%2C+T">Tolga Birdal</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Carlsson%2C+G">Gunnar Carlsson</a>, 
<a href="/search/cs?searchtype=author&query=Curry%2C+J">Justin Curry</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yue Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hajij%2C+M">Mustafa Hajij</a>, 
<a href="/search/cs?searchtype=author&query=Kwitt%2C+R">Roland Kwitt</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Di+Lorenzo%2C+P">Paolo Di Lorenzo</a>, 
<a href="/search/cs?searchtype=author&query=Maroulas%2C+V">Vasileios Maroulas</a>, 
<a href="/search/cs?searchtype=author&query=Miolane%2C+N">Nina Miolane</a>, 
<a href="/search/cs?searchtype=author&query=Nasrin%2C+F">Farzana Nasrin</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+B">Bastian Rieck</a>, 
<a href="/search/cs?searchtype=author&query=Scardapane%2C+S">Simone Scardapane</a>, 
<a href="/search/cs?searchtype=author&query=Schaub%2C+M+T">Michael T. Schaub</a>, 
<a href="/search/cs?searchtype=author&query=Veli%C4%8Dkovi%C4%87%2C+P">Petar Veli&#x10d;kovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yusu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Guo-Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zamzmi%2C+G">Ghada Zamzmi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Topological deep learning (TDL) is a rapidly evolving field that uses
topological features to understand and design deep learning models. This paper
posits that TDL may complement graph representation learning and geometric deep
learning by incorporating topological concepts, and can thus provide a natural
choice for various machine learning settings. To this end, this paper discusses
open problems in TDL, ranging from practical benefits to theoretical
foundations. For each problem, it outlines potential solutions and future
research opportunities. At the same time, this paper serves as an invitation to
the scientific community to actively participate in TDL research to unlock the
potential of this emerging field.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08874" title="Abstract">arXiv:2402.08874</a> [<a href="/pdf/2402.08874" title="Download PDF">pdf</a>, <a href="/format/2402.08874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree-Based Hard Attention with Self-Motivation for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenxi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiayu Ren</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guoxiu He</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhuoren Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaomin Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large language models (LLMs) excel at understanding and generating
plain text, they are not specifically tailored to handle hierarchical text
structures. Extracting the task-desired property from their natural language
responses typically necessitates additional processing steps. In fact,
selectively comprehending the hierarchical structure of large-scale text is
pivotal to understanding its substance. Aligning LLMs more closely with the
classification or regression values of specific task through prompting also
remains challenging. To this end, we propose a novel framework called
Tree-Based Hard Attention with Self-Motivation for Large Language Models
(TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs
to process hierarchically structured text inputs. By leveraging prompting, it
enables a frozen LLM to selectively focus on relevant leaves in relation to the
root, generating a tailored symbolic representation of their relationship.
Moreover, TEAROOM comprises a self-motivation strategy for another LLM equipped
with a trainable adapter and a linear layer. The selected symbolic outcomes are
integrated into another prompt, along with the predictive value of the task. We
iteratively feed output values back into the prompt, enabling the trainable LLM
to progressively approximate the golden truth. TEAROOM outperforms existing
state-of-the-art methods in experimental evaluations across three benchmark
datasets, showing its effectiveness in estimating task-specific properties.
Through comprehensive experiments and analysis, we have validated the ability
of TEAROOM to gradually approach the underlying golden truth through multiple
inferences.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08875" title="Abstract">arXiv:2402.08875</a> [<a href="/pdf/2402.08875" title="Download PDF">pdf</a>, <a href="/format/2402.08875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TikTokActions: A TikTok-Derived Video Dataset for Human Action  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yang Qian</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yinan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kargarandehkordi%2C+A">Ali Kargarandehkordi</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O+C">Onur Cezmi Mutlu</a>, 
<a href="/search/cs?searchtype=author&query=Surabhi%2C+S">Saimourya Surabhi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pingyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jabbar%2C+Z">Zain Jabbar</a>, 
<a href="/search/cs?searchtype=author&query=Wall%2C+D+P">Dennis Paul Wall</a>, 
<a href="/search/cs?searchtype=author&query=Washington%2C+P">Peter Washington</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The increasing variety and quantity of tagged multimedia content on platforms
such as TikTok provides an opportunity to advance computer vision modeling. We
have curated a distinctive dataset of 283,582 unique video clips categorized
under 386 hashtags relating to modern human actions. We release this dataset as
a valuable resource for building domain-specific foundation models for human
movement modeling tasks such as action recognition. To validate this dataset,
which we name TikTokActions, we perform two sets of experiments. First, we
pretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone on
TikTokActions subset, and then fine-tune and evaluate on popular datasets such
as UCF101 and the HMDB51. We find that the performance of the model pre-trained
using our Tik-Tok dataset is comparable to models trained on larger action
recognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, our
investigation into the relationship between pre-training dataset size and
fine-tuning performance reveals that beyond a certain threshold, the
incremental benefit of larger training sets diminishes. This work introduces a
useful TikTok video dataset that is available for public use and provides
insights into the marginal benefit of increasing pre-training dataset sizes for
video-based foundation models.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08876" title="Abstract">arXiv:2402.08876</a> [<a href="/pdf/2402.08876" title="Download PDF">pdf</a>, <a href="/format/2402.08876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fainstein%2C+M">Miguel Fainstein</a>, 
<a href="/search/cs?searchtype=author&query=Siless%2C+V">Viviana Siless</a>, 
<a href="/search/cs?searchtype=author&query=Iarussi%2C+E">Emmanuel Iarussi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
<p class="mathjax">In recent years, there has been a growing interest in training Neural
Networks to approximate Unsigned Distance Fields (UDFs) for representing open
surfaces in the context of 3D reconstruction. However, UDFs are
non-differentiable at the zero level set which leads to significant errors in
distances and gradients, generally resulting in fragmented and discontinuous
surfaces. In this paper, we propose to learn a hyperbolic scaling of the
unsigned distance field, which defines a new Eikonal problem with distinct
boundary conditions. This allows our formulation to integrate seamlessly with
state-of-the-art continuously differentiable implicit neural representation
networks, largely applied in the literature to represent signed distance
fields. Our approach not only addresses the challenge of open surface
representation but also demonstrates significant improvement in reconstruction
quality and training performance. Moreover, the unlocked field's
differentiability allows the accurate computation of essential topological
properties such as normal directions and curvatures, pervasive in downstream
tasks such as rendering. Through extensive experiments, we validate our
approach across various data sets and against competitive baselines. The
results demonstrate enhanced accuracy and up to an order of magnitude increase
in speed compared to previous methods.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08878" title="Abstract">arXiv:2402.08878</a> [<a href="/pdf/2402.08878" title="Download PDF">pdf</a>, <a href="/ps/2402.08878" title="Download PostScript">ps</a>, <a href="/format/2402.08878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Secret Securing in Discrete-Event Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Matsui%2C+S">Shoma Matsui</a>, 
<a href="/search/eess?searchtype=author&query=Cai%2C+K">Kai Cai</a>, 
<a href="/search/eess?searchtype=author&query=Rudie%2C+K">Karen Rudie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we study a security problem of protecting secrets in
distributed systems. Specifically, we employ discrete-event systems to describe
the structure and behaviour of distributed systems, in which global secret
information is separated into pieces and stored in local component agents. The
goal is to prevent such secrets from being exposed to intruders by imposing
appropriate protection measures. This problem is formulated as to ensure that
at least one piece of every distributed global secret is secured by a required
number of protections, while the overall cost to apply protections is minimum.
We first characterize the solvability of this security problem by providing a
necessary and sufficient condition, and then develop an algorithm to compute a
solution based on the supervisory control theory of discrete-event systems.
Finally, we illustrate the effectiveness of our solution with an example system
comprising distributed databases.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08882" title="Abstract">arXiv:2402.08882</a> [<a href="/pdf/2402.08882" title="Download PDF">pdf</a>, <a href="/format/2402.08882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moving Object Proposals with Deep Learned Optical Flow for Video Object  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Ge Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhili Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Dynamic scene understanding is one of the most conspicuous field of interest
among computer vision community. In order to enhance dynamic scene
understanding, pixel-wise segmentation with neural networks is widely accepted.
The latest researches on pixel-wise segmentation combined semantic and motion
information and produced good performance. In this work, we propose a state of
art architecture of neural networks to accurately and efficiently get the
moving object proposals (MOP). We first train an unsupervised convolutional
neural network (UnFlow) to generate optical flow estimation. Then we render the
output of optical flow net to a fully convolutional SegNet model. The main
contribution of our work is (1) Fine-tuning the pretrained optical flow model
on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural
networks with Encoder-Decoder architecture to segment objects. We developed the
codes with TensorFlow, and executed the training and evaluation processes on an
AWS EC2 instance.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08892" title="Abstract">arXiv:2402.08892</a> [<a href="/pdf/2402.08892" title="Download PDF">pdf</a>, <a href="/format/2402.08892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Segmentation of Vertebral Bodies with Iterative  Slice-propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Shiqi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+B">Bolin Lai</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+G">Guangyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan-Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hui Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1412.7062">arXiv:1412.7062</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Vertebral body (VB) segmentation is an important preliminary step towards
medical visual diagnosis for spinal diseases. However, most previous works
require pixel/voxel-wise strong supervisions, which is expensive, tedious and
time-consuming for experts to annotate. In this paper, we propose a Weakly
supervised Iterative Spinal Segmentation (WISS) method leveraging only four
corner landmark weak labels on a single sagittal slice to achieve automatic
volumetric segmentation from CT images for VBs. WISS first segments VBs on an
annotated sagittal slice in an iterative self-training manner. This
self-training method alternates between training and refining labels in the
training set. Then WISS proceeds to segment the whole VBs slice by slice with a
slice-propagation method to obtain volumetric segmentations. We evaluate the
performance of WISS on a private spinal metastases CT dataset and the public
lumbar CT dataset. On the first dataset, WISS achieves distinct improvements
with regard to two different backbones. For the second dataset, WISS achieves
dice coefficients of $91.7\%$ and $83.7\%$ for mid-sagittal slices and 3D CT
volumes, respectively, saving a lot of labeling costs and only sacrificing a
little segmentation performance.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08893" title="Abstract">arXiv:2402.08893</a> [<a href="/pdf/2402.08893" title="Download PDF">pdf</a>, <a href="/format/2402.08893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inconsistency of evaluation metrics in link prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Y">Yilin Bi</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+X">Xinshan Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yan-Li Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tao Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Link prediction is a paradigmatic and challenging problem in network science,
which predicts missing links, future links and temporal links based on known
topology. Along with the increasing number of link prediction algorithms, a
critical yet previously ignored risk is that the evaluation metrics for
algorithm performance are usually chosen at will. This paper implements
extensive experiments on hundreds of real networks and 25 well-known
algorithms, revealing statistically significant inconsistency of evaluation
metrics, namely different metrics probably produce remarkably different
rankings of algorithms. Therefore, we conclude that any single metric cannot
comprehensively or credibly evaluate algorithm performance. Further analysis
suggests the usage of at least two metrics: one is the area under the receiver
operating characteristic curve (AUC), and the other is one of the following
three candidates metrics, say the area under the precision-recall curve (AUPR),
the area under the precision curve (AUC-Precision), and the normalized
discounted cumulative gain (NDCG). In addition, as we have proved the essential
equivalence of threshold-dependent metrics, if in a link prediction task, some
specific thresholds are meaningful, we can consider any one threshold-dependent
metric with those thresholds. This work completes a missing part in the
landscape of link prediction, and provides a starting point toward a
well-accepted criterion or standard to select proper evaluation metrics for
link prediction.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08897" title="Abstract">arXiv:2402.08897</a> [<a href="/pdf/2402.08897" title="Download PDF">pdf</a>, <a href="/format/2402.08897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RB5 Low-Cost Explorer: Implementing Autonomous Long-Term Exploration on  Low-Cost Robotic Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seewald%2C+A">Adam Seewald</a>, 
<a href="/search/cs?searchtype=author&query=Chanc%C3%A1n%2C+M">Marvin Chanc&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=McCann%2C+C+M">Connor M. McCann</a>, 
<a href="/search/cs?searchtype=author&query=Noh%2C+S">Seonghoon Noh</a>, 
<a href="/search/cs?searchtype=author&query=Fallahi%2C+O">Omeed Fallahi</a>, 
<a href="/search/cs?searchtype=author&query=Castillo%2C+H">Hector Castillo</a>, 
<a href="/search/cs?searchtype=author&query=Abraham%2C+I">Ian Abraham</a>, 
<a href="/search/cs?searchtype=author&query=Dollar%2C+A+M">Aaron M. Dollar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, ICRA'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This systems paper presents the implementation and design of RB5, a wheeled
robot for autonomous long-term exploration with fewer and cheaper sensors.
Requiring just an RGB-D camera and low-power computing hardware, the system
consists of an experimental platform with rocker-bogie suspension. It operates
in unknown and GPS-denied environments and on indoor and outdoor terrains. The
exploration consists of a methodology that extends frontier- and sampling-based
exploration with a path-following vector field and a state-of-the-art SLAM
algorithm. The methodology allows the robot to explore its surroundings at
lower update frequencies, enabling the use of lower-performing and lower-cost
hardware while still retaining good autonomous performance. The approach
further consists of a methodology to interact with a remotely located human
operator based on an inexpensive long-range and low-power communication
technology from the internet-of-things domain (i.e., LoRa) and a customized
communication protocol. The results and the feasibility analysis show the
possible applications and limitations of the approach.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08902" title="Abstract">arXiv:2402.08902</a> [<a href="/pdf/2402.08902" title="Download PDF">pdf</a>, <a href="/format/2402.08902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-Encoding Bayesian Inverse Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+L">Lasse Peters</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>, 
<a href="/search/cs?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>, 
<a href="/search/cs?searchtype=author&query=Fridovich-Keil%2C+D">David Fridovich-Keil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
<p class="mathjax">When multiple agents interact in a common environment, each agent's actions
impact others' future decisions, and noncooperative dynamic games naturally
capture this coupling. In interactive motion planning, however, agents
typically do not have access to a complete model of the game, e.g., due to
unknown objectives of other players. Therefore, we consider the inverse game
problem, in which some properties of the game are unknown a priori and must be
inferred from observations. Existing maximum likelihood estimation (MLE)
approaches to solve inverse games provide only point estimates of unknown
parameters without quantifying uncertainty, and perform poorly when many
parameter values explain the observed behavior. To address these limitations,
we take a Bayesian perspective and construct posterior distributions of game
parameters. To render inference tractable, we employ a variational autoencoder
(VAE) with an embedded differentiable game solver. This structured VAE can be
trained from an unlabeled dataset of observed interactions, naturally handles
continuous, multi-modal distributions, and supports efficient sampling from the
inferred posteriors without computing game solutions at runtime. Extensive
evaluations in simulated driving scenarios demonstrate that the proposed
approach successfully learns the prior and posterior objective distributions,
provides more accurate objective estimates than MLE baselines, and facilitates
safer and more efficient game-theoretic motion planning.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08907" title="Abstract">arXiv:2402.08907</a> [<a href="/pdf/2402.08907" title="Download PDF">pdf</a>, <a href="/format/2402.08907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Negative Transfer on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zehong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuxu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yanfang Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Transfer learning aims to boost the learning on the target task leveraging
knowledge learned from other relevant tasks. However, when the source and
target are not closely related, the learning performance may be adversely
affected, a phenomenon known as negative transfer. In this paper, we
investigate the negative transfer in graph transfer learning, which is
important yet underexplored. We reveal that, unlike image or text, negative
transfer commonly occurs in graph-structured data, even when source and target
graphs share semantic similarities. Specifically, we identify that structural
differences significantly amplify the dissimilarities in the node embeddings
across graphs. To mitigate this, we bring a new insight: for semantically
similar graphs, although structural differences lead to significant
distribution shift in node embeddings, their impact on subgraph embeddings
could be marginal. Building on this insight, we introduce two effective yet
elegant methods, Subgraph Pooling (SP) and Subgraph Pooling++ (SP++), that
transfer subgraph-level knowledge across graphs. We theoretically analyze the
role of SP in reducing graph discrepancy and conduct extensive experiments to
evaluate its superiority under various settings. Our code and datasets are
available at: https://github.com/Zehong-Wang/Subgraph-Pooling.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08908" title="Abstract">arXiv:2402.08908</a> [<a href="/pdf/2402.08908" title="Download PDF">pdf</a>, <a href="/format/2402.08908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teamwork Makes TEE Work: Open and Resilient Remote Attestation on  Decentralized Trust
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaolin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+K">Kailun Qin</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+S">Shipei Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tengfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+D">Dawu Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Remote Attestation (RA) enables the integrity and authenticity of
applications in Trusted Execution Environment (TEE) to be verified. Existing
TEE RA designs employ a centralized trust model where they rely on a single
provisioned secret key and a centralized verifier to establish trust for remote
parties. This model is however brittle and can be untrusted under advanced
attacks nowadays. Besides, most designs only provide fixed functionalities once
deployed, making them hard to adapt to different needs on availability, Quality
of Service (QoS), etc.
<br />Therefore, we propose JANUS, an open and resilient TEE RA scheme. To
decentralize trust, we, on one hand, introduce Physically Unclonable Function
(PUF) as an intrinsic root of trust (RoT) in TEE to provide additional
measurements and cryptographic enhancements. On the other hand, we use
blockchain and smart contract to realize decentralized verification and result
audit. Furthermore, we design an automated turnout mechanism that allows JANUS
to remain resilient and offer flexible RA services under various situations. We
provide a UC-based security proof and demonstrate the scalability and
generality of JANUS by implementing an open-sourced prototype.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08909" title="Abstract">arXiv:2402.08909</a> [<a href="/pdf/2402.08909" title="Download PDF">pdf</a>, <a href="/format/2402.08909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A locally mass-conservative enriched Petrov-Galerkin method without  penalty for the Darcy flow in porous media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+H">Huangxin Chen</a>, 
<a href="/search/math?searchtype=author&query=Dong%2C+P">Piaopiao Dong</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+S">Shuyu Sun</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Z">Zixuan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work we present an enriched Petrov-Galerkin (EPG) method for the
simulation of the Darcy flow in porous media. The new method enriches the
approximation trial space of the conforming continuous Galerkin (CG) method
with bubble functions and enriches the approximation test space of the CG
method with piecewise constant functions, and it does not require any penalty
term in the weak formulation. Moreover, we propose a framework for constructing
the bubble functions and consider a decoupled algorithm for the EPG method
based on this framework, which enables the process of solving pressure to be
decoupled into two steps. The first step is to solve the pressure by the
standard CG method, and the second step is a post-processing correction of the
first step. Compared with the CG method, the proposed EPG method is locally
mass-conservative, while keeping fewer degrees of freedom than the
discontinuous Galerkin (DG) method. In addition, this method is more concise in
the error analysis than the enriched Galerkin (EG) method. The coupled flow and
transport in porous media is considered to illustrate the advantages of locally
mass-conservative properties of the EPG method. We establish the optimal
convergence of numerical solutions and present several numerical examples to
illustrate the performance of the proposed method.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08910" title="Abstract">arXiv:2402.08910</a> [<a href="/pdf/2402.08910" title="Download PDF">pdf</a>, <a href="/format/2402.08910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-based Bone Quality Classification Method for Spinal Metastasis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Shiqi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+B">Bolin Lai</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+G">Guangyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan-Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hui Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Spinal metastasis is the most common disease in bone metastasis and may cause
pain, instability and neurological injuries. Early detection of spinal
metastasis is critical for accurate staging and optimal treatment. The
diagnosis is usually facilitated with Computed Tomography (CT) scans, which
requires considerable efforts from well-trained radiologists. In this paper, we
explore a learning-based automatic bone quality classification method for
spinal metastasis based on CT images. We simultaneously take the posterolateral
spine involvement classification task into account, and employ multi-task
learning (MTL) technique to improve the performance. MTL acts as a form of
inductive bias which helps the model generalize better on each task by sharing
representations between related tasks. Based on the prior knowledge that the
mixed type can be viewed as both blastic and lytic, we model the task of bone
quality classification as two binary classification sub-tasks, i.e., whether
blastic and whether lytic, and leverage a multiple layer perceptron to combine
their predictions. In order to make the model more robust and generalize
better, self-paced learning is adopted to gradually involve from easy to more
complex samples into the training process. The proposed learning-based method
is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our
method significantly outperforms an 121-layer DenseNet classifier in
sensitivities by $+12.54\%$, $+7.23\%$ and $+29.06\%$ for blastic, mixed and
lytic lesions, respectively, meanwhile $+12.33\%$, $+23.21\%$ and $+34.25\%$ at
vertebrae level.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08912" title="Abstract">arXiv:2402.08912</a> [<a href="/pdf/2402.08912" title="Download PDF">pdf</a>, <a href="/ps/2402.08912" title="Download PostScript">ps</a>, <a href="/format/2402.08912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supercloseness of the DDG method for a singularly perturbed convection  diffusion problem on Shishkin mesh
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ma%2C+X">Xiaoqi Ma</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jin Zhang</a>, 
<a href="/search/math?searchtype=author&query=Feng%2C+X">Xinyi Feng</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Chunxiao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper investigates the supercloseness of a singularly perturbed
convection diffusion problem using the direct discontinuous Galerkin (DDG)
method on a Shishkin mesh. The main technical difficulties lie in controlling
the diffusion term inside the layer, the convection term outside the layer, and
the inter element jump term caused by the discontinuity of the numerical
solution. The main idea is to design a new composite interpolation, in which a
global projection is used outside the layer to satisfy the interface conditions
determined by the selection of numerical flux, thereby eliminating or
controlling the troublesome terms on the unit interface; and inside the layer,
Gau{\ss} Lobatto projection is used to improve the convergence order of the
diffusion term. On the basis of that, by selecting appropriate parameters in
the numerical flux, we obtain the supercloseness result of almost $k+1$ order
under an energy norm. Numerical experiments support our main theoretical
conclusion.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08917" title="Abstract">arXiv:2402.08917</a> [<a href="/pdf/2402.08917" title="Download PDF">pdf</a>, <a href="/format/2402.08917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interference-aware Approach for Co-located Container Orchestration  with Novel Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Linfeng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minxian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+K">Kejiang Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In the Proceedings of IEEE SmartData 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Container orchestration technologies are widely employed in cloud computing,
facilitating the co-location of online and offline services on the same
infrastructure. Online services demand rapid responsiveness and high
availability, whereas offline services require extensive computational
resources. However, this mixed deployment can lead to resource contention,
adversely affecting the performance of online services, yet the metrics used by
existing methods cannot accurately reflect the extent of interference.
<br />In this paper, we introduce scheduling latency as a novel metric for
quantifying interference and compare it with existing metrics. Empirical
evidence demonstrates that scheduling latency more accurately reflects the
performance degradation of online services. We also utilize various machine
learning techniques to predict potential interference on specific hosts for
online services, providing reference information for subsequent scheduling
decisions. Simultaneously, we propose a method for quantifying node
interference based on scheduling latency. To enhance resource utilization, we
train a model for online services that predicts CPU and MEM (memory) resource
allocation based on workload type and QPS. Finally, we present a scheduling
algorithm based on predictive modeling, aiming to reduce interference in online
services while balancing node resource utilization. Through experiments and
comparisons with three other baseline methods, we demonstrate the effectiveness
of our approach. Compared with three baselines, our approach can reduce the
average response time, 90th percentile response time, and 99th percentile
response time of online services by 29.4%, 31.4%, and 14.5%, respectively.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08918" title="Abstract">arXiv:2402.08918</a> [<a href="/pdf/2402.08918" title="Download PDF">pdf</a>, <a href="/format/2402.08918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Inference Acceleration by Learning MLPs on Graphs without  Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zehong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuxu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yanfang Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph
learning tasks, yet their reliance on message-passing constraints their
deployment in latency-sensitive applications such as financial fraud detection.
Recent works have explored distilling knowledge from GNNs to Multi-Layer
Perceptrons (MLPs) to accelerate inference. However, this task-specific
supervised distillation limits generalization to unseen nodes, which are
prevalent in latency-sensitive applications. To this end, we present
\textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework
for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance
generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs
and MLPs to capture the fine-grained and generalizable correlation between node
features and graph structures, and proposes two strategies to alleviate the
risk of trivial solutions. Theoretically, we comprehensively analyze
\textsc{SimMLP} to demonstrate its equivalence to GNNs in the optimal case and
its generalization capability. Empirically, \textsc{SimMLP} outperforms
state-of-the-art baselines, especially in settings with unseen nodes. In
particular, it obtains significant performance gains {\bf (7$\sim$26\%)} over
MLPs and inference acceleration over GNNs {\bf (90$\sim$126$\times$)} on
large-scale graph datasets. Our codes are available at:
\url{https://github.com/Zehong-Wang/SimMLP}.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08919" title="Abstract">arXiv:2402.08919</a> [<a href="/pdf/2402.08919" title="Download PDF">pdf</a>, <a href="/format/2402.08919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Measures of Conceptual Similarity by  Complexity-Constrained Descriptive Auto-Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Achille%2C+A">Alessandro Achille</a>, 
<a href="/search/cs?searchtype=author&query=Steeg%2C+G+V">Greg Ver Steeg</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T+Y">Tian Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Trager%2C+M">Matthew Trager</a>, 
<a href="/search/cs?searchtype=author&query=Klingenberg%2C+C">Carson Klingenberg</a>, 
<a href="/search/cs?searchtype=author&query=Soatto%2C+S">Stefano Soatto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantifying the degree of similarity between images is a key copyright issue
for image-based machine learning. In legal doctrine however, determining the
degree of similarity between works requires subjective analysis, and
fact-finders (judges and juries) can demonstrate considerable variability in
these subjective judgement calls. Images that are structurally similar can be
deemed dissimilar, whereas images of completely different scenes can be deemed
similar enough to support a claim of copying. We seek to define and compute a
notion of "conceptual similarity" among images that captures high-level
relations even among images that do not share repeated elements or visually
similar components. The idea is to use a base multi-modal model to generate
"explanations" (captions) of visual data at increasing levels of complexity.
Then, similarity can be measured by the length of the caption needed to
discriminate between the two images: Two highly dissimilar images can be
discriminated early in their description, whereas conceptually dissimilar ones
will need more detail to be distinguished. We operationalize this definition
and show that it correlates with subjective (averaged human evaluation)
assessment, and beats existing baselines on both image-to-image and
text-to-text similarity benchmarks. Beyond just providing a number, our method
also offers interpretability by pointing to the specific level of granularity
of the description where the source data are differentiated.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08920" title="Abstract">arXiv:2402.08920</a> [<a href="/pdf/2402.08920" title="Download PDF">pdf</a>, <a href="/format/2402.08920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying and Characterizing Clones of Self-Admitted Technical Debt in  Build Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhili Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hata%2C+H">Hideaki Hata</a>, 
<a href="/search/cs?searchtype=author&query=McIntosh%2C+S">Shane McIntosh</a>, 
<a href="/search/cs?searchtype=author&query=Matsumoto%2C+K">Kenichi Matsumoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Self-Admitted Technical Debt (SATD) annotates development decisions that
intentionally exchange long-term software artifact quality for short-term
goals. Recent work explores the existence of SATD clones (duplicate or near
duplicate SATD comments) in source code. Cloning of SATD in build systems
(e.g., CMake and Maven) may propagate suboptimal design choices, threatening
qualities of the build system that stakeholders rely upon (e.g.,
maintainability, reliability, repeatability). Hence, we conduct a large-scale
study on 50,608 SATD comments extracted from Autotools, CMake, Maven, and Ant
build systems to investigate the prevalence of SATD clones and to characterize
their incidences. We observe that: (i) prior work suggests that 41-65% of SATD
comments in source code are clones, but in our studied build system context,
the rates range from 62% to 95%, suggesting that SATD clones are a more
prevalent phenomenon in build systems than in source code; (ii) statements
surrounding SATD clones are highly similar, with 76% of occurrences having
similarity scores greater than 0.8; (iii) a quarter of SATD clones are
introduced by the author of the original SATD statements; and (iv) among the
most commonly cloned SATD comments, external factors (e.g., platform and tool
configuration) are the most frequent locations, limitations in tools and
libraries are the most frequent causes, and developers often copy SATD comments
that describe issues to be fixed later. Our work presents the first step toward
systematically understanding SATD clones in build systems and opens up avenues
for future work, such as distinguishing different SATD clone behavior, as well
as designing an automated recommendation system for repaying SATD effectively
based on resolved clones.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08921" title="Abstract">arXiv:2402.08921</a> [<a href="/pdf/2402.08921" title="Download PDF">pdf</a>, <a href="/format/2402.08921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing ID and Text Fusion via Alternative Training in Session-based  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Haoyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhikai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shomer%2C+H">Harry Shomer</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Javari%2C+A">Amin Javari</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Session-based recommendation has gained increasing attention in recent years,
with its aim to offer tailored suggestions based on users' historical behaviors
within sessions.
<br />To advance this field, a variety of methods have been developed, with
ID-based approaches typically demonstrating promising performance. However,
these methods often face challenges with long-tail items and overlook other
rich forms of information, notably valuable textual semantic information. To
integrate text information, various methods have been introduced, mostly
following a naive fusion framework. Surprisingly, we observe that fusing these
two modalities does not consistently outperform the best single modality by
following the naive fusion framework. Further investigation reveals an
potential imbalance issue in naive fusion, where the ID dominates and text
modality is undertrained. This suggests that the unexpected observation may
stem from naive fusion's failure to effectively balance the two modalities,
often over-relying on the stronger ID modality. This insight suggests that
naive fusion might not be as effective in combining ID and text as previously
expected. To address this, we propose a novel alternative training strategy
AlterRec. It separates the training of ID and text, thereby avoiding the
imbalance issue seen in naive fusion. Additionally, AlterRec designs a novel
strategy to facilitate the interaction between the two modalities, enabling
them to mutually learn from each other and integrate the text more effectively.
Comprehensive experiments demonstrate the effectiveness of AlterRec in
session-based recommendation. The implementation is available at
https://github.com/Juanhui28/AlterRec.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08922" title="Abstract">arXiv:2402.08922</a> [<a href="/pdf/2402.08922" title="Download PDF">pdf</a>, <a href="/format/2402.08922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Mirrored Influence Hypothesis: Efficient Data Influence Estimation  by Harnessing Forward Passes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+M">Myeongseob Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+F">Feiyang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weiyan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Ming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Ruoxi Jia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Large-scale black-box models have become ubiquitous across numerous
applications. Understanding the influence of individual training data sources
on predictions made by these models is crucial for improving their
trustworthiness. Current influence estimation techniques involve computing
gradients for every training point or repeated training on different subsets.
These approaches face obvious computational challenges when scaled up to large
datasets and models.
<br />In this paper, we introduce and explore the Mirrored Influence Hypothesis,
highlighting a reciprocal nature of influence between training and test data.
Specifically, it suggests that evaluating the influence of training data on
test predictions can be reformulated as an equivalent, yet inverse problem:
assessing how the predictions for training samples would be altered if the
model were trained on specific test samples. Through both empirical and
theoretical validations, we demonstrate the wide applicability of our
hypothesis. Inspired by this, we introduce a new method for estimating the
influence of training data, which requires calculating gradients for specific
test samples, paired with a forward pass for each training point. This approach
can capitalize on the common asymmetry in scenarios where the number of test
samples under concurrent examination is much smaller than the scale of the
training dataset, thus gaining a significant improvement in efficiency compared
to existing approaches.
<br />We demonstrate the applicability of our method across a range of scenarios,
including data attribution in diffusion models, data leakage detection,
analysis of memorization, mislabeled data detection, and tracing behavior in
language models. Our code will be made available at
https://github.com/ruoxi-jia-group/Forward-INF.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08923" title="Abstract">arXiv:2402.08923</a> [<a href="/pdf/2402.08923" title="Download PDF">pdf</a>, <a href="/format/2402.08923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human  Pose Estimation with Transformer Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramani%2C+V">Varun Ramani</a>, 
<a href="/search/cs?searchtype=author&query=Khayemi%2C+H">Hossein Khayemi</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+N">Nakul Garg</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+N">Nirupam Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper presents a novel approach for predicting human poses using IMU
data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose,
which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two
main innovations: a data-driven strategy for optimal IMU placement and a
transformer-based model architecture for time series analysis. Our findings
indicate that our approach not only outperforms traditional 6 IMU-based biRNN
models but also that the transformer architecture significantly enhances pose
reconstruction from data obtained from 24 IMU locations, with equivalent
performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by
our optimally chosen locations, when coupled with the parallelizability and
performance of transformers, provides significant improvements to the field of
IMU-based pose estimation.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08925" title="Abstract">arXiv:2402.08925</a> [<a href="/pdf/2402.08925" title="Download PDF">pdf</a>, <a href="/format/2402.08925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with  Diverse Human Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Jiahao Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hui Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Koppel%2C+A">Alec Koppel</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Reinforcement Learning from Human Feedback (RLHF) aligns language models to
human preferences by employing a singular reward model derived from preference
data. However, such an approach overlooks the rich diversity of human
preferences inherent in data collected from multiple users. In this work, we
first derive an impossibility result of alignment with single reward RLHF,
thereby highlighting its insufficiency in representing diverse human
preferences. To provide an equitable solution to the problem, we learn a
mixture of preference distributions via an expectation-maximization algorithm
and propose a MaxMin alignment objective for policy learning inspired by the
Egalitarian principle in social choice theory to better represent diverse human
preferences. We elucidate the connection of our proposed approach to
distributionally robust optimization and general utility RL, thereby
highlighting the generality and robustness of our proposed solution. We present
comprehensive experimental results on small-scale (GPT-2) and large-scale
language models (with Tulu2-7B) and show the efficacy of the proposed approach
in the presence of diversity among human preferences. Our algorithm achieves an
average improvement of more than 16% in win-rates over conventional RLHF
algorithms and improves the win-rate (accuracy) for minority groups by over 33%
without compromising the performance of majority groups, showcasing the
robustness and fairness of our approach. We remark that our findings in this
work are not only limited to language models but also extend to reinforcement
learning in general.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08926" title="Abstract">arXiv:2402.08926</a> [<a href="/pdf/2402.08926" title="Download PDF">pdf</a>, <a href="/format/2402.08926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Finite Element Methods for Nonlinear  Rosenau-Burgers-Biharmonic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ankur">Ankur</a>, 
<a href="/search/math?searchtype=author&query=Jiwari%2C+R">Ram Jiwari</a>, 
<a href="/search/math?searchtype=author&query=Narayan%2C+A">Akil Narayan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a novel and comparative analysis of finite element discretizations
for a nonlinear Rosenau-Burgers model including a biharmonic term. We analyze
both continuous and mixed finite element approaches, providing stability,
existence, and uniqueness statements of the corresponding variational methods.
We also obtain optimal error estimates of the semidiscrete scheme in
corresponding B\^ochner spaces. Finally, we construct a fully discrete scheme
through a backward Euler discretization of the time derivative, and prove
well-posedness statements for this fully discrete scheme. Our findings show
that the mixed approach removes some theoretical impediments to analysis and is
numerically easier to implement. We provide numerical simulations for the mixed
formulation approach using $C^0$ Taylor-Hood finite elements on several
domains. Our numerical results confirm that the algorithm has optimal
convergence in accordance with the observed theoretical results.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08929" title="Abstract">arXiv:2402.08929</a> [<a href="/pdf/2402.08929" title="Download PDF">pdf</a>, <a href="/format/2402.08929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Second Order Methods for Bandit Optimization and Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suggala%2C+A">Arun Suggala</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y+J">Y. Jennifer Sun</a>, 
<a href="/search/cs?searchtype=author&query=Netrapalli%2C+P">Praneeth Netrapalli</a>, 
<a href="/search/cs?searchtype=author&query=Hazan%2C+E">Elad Hazan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Bandit convex optimization (BCO) is a general framework for online decision
making under uncertainty. While tight regret bounds for general convex losses
have been established, existing algorithms achieving these bounds have
prohibitive computational costs for high dimensional data.
<br />In this paper, we propose a simple and practical BCO algorithm inspired by
the online Newton step algorithm. We show that our algorithm achieves optimal
(in terms of horizon) regret bounds for a large class of convex functions that
we call $\kappa$-convex. This class contains a wide range of practically
relevant loss functions including linear, quadratic, and generalized linear
models. In addition to optimal regret, this method is the most efficient known
algorithm for several well-studied applications including bandit logistic
regression.
<br />Furthermore, we investigate the adaptation of our second-order bandit
algorithm to online convex optimization with memory. We show that for loss
functions with a certain affine structure, the extended algorithm attains
optimal regret. This leads to an algorithm with optimal regret for bandit
LQR/LQG problems under a fully adversarial noise model, thereby resolving an
open question posed in \citep{gradu2020non} and \citep{sun2023optimal}.
<br />Finally, we show that the more general problem of BCO with (non-affine)
memory is harder. We derive a $\tilde{\Omega}(T^{2/3})$ regret lower bound,
even under the assumption of smooth and quadratic losses.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08931" title="Abstract">arXiv:2402.08931</a> [<a href="/pdf/2402.08931" title="Download PDF">pdf</a>, <a href="/format/2402.08931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Depth-aware Volume Attention for Texture-less Stereo Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yintao Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Stereo matching plays a crucial role in 3D perception and scenario
understanding. Despite the proliferation of promising methods, addressing
texture-less and texture-repetitive conditions remains challenging due to the
insufficient availability of rich geometric and semantic information. In this
paper, we propose a lightweight volume refinement scheme to tackle the texture
deterioration in practical outdoor scenarios. Specifically, we introduce a
depth volume supervised by the ground-truth depth map, capturing the relative
hierarchy of image texture. Subsequently, the disparity discrepancy volume
undergoes hierarchical filtering through the incorporation of depth-aware
hierarchy attention and target-aware disparity attention modules. Local fine
structure and context are emphasized to mitigate ambiguity and redundancy
during volume aggregation. Furthermore, we propose a more rigorous evaluation
metric that considers depth-wise relative error, providing comprehensive
evaluations for universal stereo matching and depth estimation models. We
extensively validate the superiority of our proposed methods on public
datasets. Results demonstrate that our model achieves state-of-the-art
performance, particularly excelling in scenarios with texture-less images. The
code is available at https://github.com/ztsrxh/DVANet.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08936" title="Abstract">arXiv:2402.08936</a> [<a href="/pdf/2402.08936" title="Download PDF">pdf</a>, <a href="/format/2402.08936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive Temporal Attention on Event-based Video Stream for  Energy-efficient Situation Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bu%2C+Y">Yiming Bu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiayang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Q">Qinru Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The Dynamic Vision Sensor (DVS) is an innovative technology that efficiently
captures and encodes visual information in an event-driven manner. By combining
it with event-driven neuromorphic processing, the sparsity in DVS camera output
can result in high energy efficiency. However, similar to many embedded
systems, the off-chip communication between the camera and processor presents a
bottleneck in terms of power consumption. Inspired by the predictive coding
model and expectation suppression phenomenon found in human brain, we propose a
temporal attention mechanism to throttle the camera output and pay attention to
it only when the visual events cannot be well predicted. The predictive
attention not only reduces power consumption in the sensor-processor interface
but also effectively decreases the computational workload by filtering out
noisy events. We demonstrate that the predictive attention can reduce 46.7% of
data communication between the camera and the processor and reduce 43.8%
computation activities in the processor.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08938" title="Abstract">arXiv:2402.08938</a> [<a href="/pdf/2402.08938" title="Download PDF">pdf</a>, <a href="/format/2402.08938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AINeedsPlanner: AWorkbook to Support Effective Collaboration Between AI  Experts and Clients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D+H">Dae Hyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Hyungyu Shin</a>, 
<a href="/search/cs?searchtype=author&query=Yadgarova%2C+S">Shakhnozakhon Yadgarova</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+J">Jinho Son</a>, 
<a href="/search/cs?searchtype=author&query=Subramonyam%2C+H">Hariharan Subramonyam</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Clients often partner with AI experts to develop AI applications tailored to
their needs. In these partnerships, careful planning and clear communication
are critical, as inaccurate or incomplete specifications can result in
misaligned model characteristics, expensive reworks, and potential friction
between collaborators. Unfortunately, given the complexity of requirements
ranging from functionality, data, and governance, effective guidelines for
collaborative specification of requirements in client-AI expert collaborations
are missing. In this work, we introduce AINeedsPlanner, a workbook that AI
experts and clients can use to facilitate effective interchange and clear
specifications. The workbook is based on (1) an interview of 10 completed AI
application project teams, which identifies and characterizes steps in AI
application planning and (2) a study with 12 AI experts, which defines a
taxonomy of AI experts' information needs and dimensions that affect the
information needs. Finally, we demonstrate the workbook's utility with two case
studies in real-world settings.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08939" title="Abstract">arXiv:2402.08939</a> [<a href="/pdf/2402.08939" title="Download PDF">pdf</a>, <a href="/format/2402.08939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Premise Order Matters in Reasoning with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+R+A">Ryan A. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuezhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Xinyun and Ryan contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have accomplished remarkable reasoning
performance in various domains. However, in the domain of reasoning tasks, we
discover a frailty: LLMs are surprisingly brittle to the ordering of the
premises, despite the fact that such ordering does not alter the underlying
task. In particular, we observe that LLMs achieve the best performance when the
premise order aligns with the context required in intermediate reasoning steps.
For example, in deductive reasoning tasks, presenting the premises in the same
order as the ground truth proof in the prompt (as opposed to random ordering)
drastically increases the model's accuracy. We first examine the effect of
premise ordering on deductive reasoning on a variety of LLMs, and our
evaluation shows that permuting the premise order can cause a performance drop
of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to
examine the ordering effect for mathematical problem-solving, and we again
observe a significant drop in accuracy, relative to the original GSM8K
benchmark.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08943" title="Abstract">arXiv:2402.08943</a> [<a href="/pdf/2402.08943" title="Download PDF">pdf</a>, <a href="/format/2402.08943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating DTW Measures via a Synthesis Framework for Time-Series Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajput%2C+K">Kishansingh Rajput</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+B">Duong Binh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guoning Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Time-series data originate from various applications that describe specific
observations or quantities of interest over time. Their analysis often involves
the comparison across different time-series data sequences, which in turn
requires the alignment of these sequences. Dynamic Time Warping (DTW) is the
standard approach to achieve an optimal alignment between two temporal signals.
Different variations of DTW have been proposed to address various needs for
signal alignment or classifications. However, a comprehensive evaluation of
their performance in these time-series data processing tasks is lacking. Most
DTW measures perform well on certain types of time-series data without a clear
explanation of the reason. To address that, we propose a synthesis framework to
model the variation between two time-series data sequences for comparison. Our
synthesis framework can produce a realistic initial signal and deform it with
controllable variations that mimic real-world scenarios. With this synthesis
framework, we produce a large number of time-series sequence pairs with
different but known variations, which are used to assess the performance of a
number of well-known DTW measures for the tasks of alignment and
classification. We report their performance on different variations and suggest
the proper DTW measure to use based on the type of variations between two
time-series sequences. This is the first time such a guideline is presented for
selecting a proper DTW measure. To validate our conclusion, we apply our
findings to real-world applications, i.e., the detection of the formation top
for the oil and gas industry and the pattern search in streamlines for flow
visualization.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08946" title="Abstract">arXiv:2402.08946</a> [<a href="/pdf/2402.08946" title="Download PDF">pdf</a>, <a href="/format/2402.08946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Sharpness in Grokking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miller%2C+J">Jack Miller</a>, 
<a href="/search/cs?searchtype=author&query=Gleeson%2C+P">Patrick Gleeson</a>, 
<a href="/search/cs?searchtype=author&query=O%27Neill%2C+C">Charles O&#x27;Neill</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+T">Thang Bui</a>, 
<a href="/search/cs?searchtype=author&query=Levi%2C+N">Noam Levi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural networks sometimes exhibit grokking, a phenomenon where perfect or
near-perfect performance is achieved on a validation set well after the same
performance has been obtained on the corresponding training set. In this
workshop paper, we introduce a robust technique for measuring grokking, based
on fitting an appropriate functional form. We then use this to investigate the
sharpness of transitions in training and validation accuracy under two
settings. The first setting is the theoretical framework developed by Levi et
al. (2023) where closed form expressions are readily accessible. The second
setting is a two-layer MLP trained to predict the parity of bits, with grokking
induced by the concealment strategy of Miller et al. (2023). We find that
trends between relative grokking gap and grokking sharpness are similar in both
settings when using absolute and relative measures of sharpness. Reflecting on
this, we make progress toward explaining some trends and identify the need for
further study to untangle the various mechanisms which influence the sharpness
of grokking.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08948" title="Abstract">arXiv:2402.08948</a> [<a href="/pdf/2402.08948" title="Download PDF">pdf</a>, <a href="/ps/2402.08948" title="Download PostScript">ps</a>, <a href="/format/2402.08948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean-Field Analysis for Learning Subspace-Sparse Polynomials with  Gaussian Input
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+R">Rong Ge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we study the mean-field flow for learning subspace-sparse
polynomials using stochastic gradient descent and two-layer neural networks,
where the input distribution is standard Gaussian and the output only depends
on the projection of the input onto a low-dimensional subspace. We propose a
basis-free generalization of the merged-staircase property in Abbe et al.
(2022) and establish a necessary condition for the SGD-learnability. In
addition, we prove that the condition is almost sufficient, in the sense that a
condition slightly stronger than the necessary condition can guarantee the
exponential decay of the loss functional to zero.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08950" title="Abstract">arXiv:2402.08950</a> [<a href="/pdf/2402.08950" title="Download PDF">pdf</a>, <a href="/format/2402.08950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Evaluative Comparison of Performance Portability across GPU  Programming Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+H">Joshua H. Davis</a>, 
<a href="/search/cs?searchtype=author&query=Sivaraman%2C+P">Pranav Sivaraman</a>, 
<a href="/search/cs?searchtype=author&query=Minn%2C+I">Isaac Minn</a>, 
<a href="/search/cs?searchtype=author&query=Parasyris%2C+K">Konstantinos Parasyris</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+H">Harshitha Menon</a>, 
<a href="/search/cs?searchtype=author&query=Georgakoudis%2C+G">Giorgis Georgakoudis</a>, 
<a href="/search/cs?searchtype=author&query=Bhatele%2C+A">Abhinav Bhatele</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF)

</div>
<p class="mathjax">Ensuring high productivity in scientific software development necessitates
developing and maintaining a single codebase that can run efficiently on a
range of accelerator-based supercomputing platforms. While prior work has
investigated the performance portability of a few selected proxy applications
or programming models, this paper provides a comprehensive study of a range of
proxy applications implemented in the major programming models suitable for
GPU-based platforms. We present and analyze performance results across NVIDIA
and AMD GPU hardware currently deployed in leadership-class computing
facilities using a representative range of scientific codes and several
programming models -- CUDA, HIP, Kokkos, RAJA, OpenMP, OpenACC, and SYCL. Based
on the specific characteristics of applications tested, we include
recommendations to developers on how to choose the right programming model for
their code. We find that Kokkos and RAJA in particular offer the most promise
empirically as performance portable programming models. These results provide a
comprehensive evaluation of the extent to which each programming model for
heterogeneous systems provides true performance portability in real-world
usage.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08954" title="Abstract">arXiv:2402.08954</a> [<a href="/pdf/2402.08954" title="Download PDF">pdf</a>, <a href="/format/2402.08954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HTML papers on arXiv -- why it is important, and how we made it happen
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frankston%2C+C">Charles Frankston</a>, 
<a href="/search/cs?searchtype=author&query=Godfrey%2C+J">Jonathan Godfrey</a>, 
<a href="/search/cs?searchtype=author&query=Brinn%2C+S">Shamsi Brinn</a>, 
<a href="/search/cs?searchtype=author&query=Hofer%2C+A">Alison Hofer</a>, 
<a href="/search/cs?searchtype=author&query=Nazzaro%2C+M">Mark Nazzaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 5th International Workshop on "Digitization and E-Inclusion in Mathematics and Science 2024" (DEIMS2024). <a href="https://workshop.sciaccess.net/deims2024/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">In October 2023, arXiv made HTML formatted papers available to readers. This
was the exciting outcome of over a year of accessibility research and
development with the scientific community. Currently, only 2.4% of research
outputs meet accessibility guidelines. Informed by scientists who rely on
assistive technology, our analysis demonstrates that offering HTML is the most
impactful step arXiv can take. Scientists need HTML now, and emphasize to not
let perfect be the enemy of good enough. In this paper we share with you how
arXiv is achieving HTML conversions from LaTeX now, and our plans for future
improvements.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08955" title="Abstract">arXiv:2402.08955</a> [<a href="/pdf/2402.08955" title="Download PDF">pdf</a>, <a href="/format/2402.08955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Counterfactual Tasks to Evaluate the Generality of Analogical  Reasoning in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Martha Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+M">Melanie Mitchell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have performed well on several reasoning
benchmarks, including ones that test analogical reasoning abilities. However,
it has been debated whether they are actually performing humanlike abstract
reasoning or instead employing less general processes that rely on similarity
to what has been seen in their training data. Here we investigate the
generality of analogy-making abilities previously claimed for LLMs (Webb,
Holyoak, &amp; Lu, 2023). We take one set of analogy problems used to evaluate LLMs
and create a set of "counterfactual" variants-versions that test the same
abstract reasoning abilities but that are likely dissimilar from any
pre-training data. We test humans and three GPT models on both the original and
counterfactual problems, and show that, while the performance of humans remains
high for all the problems, the GPT models' performance declines sharply on the
counterfactual set. This work provides evidence that, despite previously
reported successes of LLMs on analogical reasoning, these models lack the
robustness and generality of human analogy-making.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08956" title="Abstract">arXiv:2402.08956</a> [<a href="/pdf/2402.08956" title="Download PDF">pdf</a>, <a href="/format/2402.08956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seagull: Privacy preserving network verification system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daneshamooz%2C+J">Jaber Daneshamooz</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Melody Yu</a>, 
<a href="/search/cs?searchtype=author&query=Maddury%2C+S">Sucheer Maddury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The current routing protocol used in the internet backbone is based on manual
configuration, making it susceptible to errors. To mitigate these
configuration-related issues, it becomes imperative to validate the accuracy
and convergence of the algorithm, ensuring a seamless operation devoid of
problems. However, the process of network verification faces challenges related
to privacy and scalability. This paper addresses these challenges by
introducing a novel approach: leveraging privacy-preserving computation,
specifically multiparty computation (MPC), to verify the correctness of
configurations in the internet backbone, governed by the BGP protocol. Not only
does our proposed solution effectively address scalability concerns, but it
also establishes a robust privacy framework. Through rigorous analysis, we
demonstrate that our approach maintains privacy by not disclosing any
information beyond the query result, thus providing a comprehensive and secure
solution to the intricacies associated with routing protocol verification in
large-scale networks.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08957" title="Abstract">arXiv:2402.08957</a> [<a href="/pdf/2402.08957" title="Download PDF">pdf</a>, <a href="/format/2402.08957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiaohan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+H">Huajian Xin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2024 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Formal Languages and Automata Theory (cs.FL); Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
<p class="mathjax">Recent large language models (LLMs) have witnessed significant advancement in
various tasks, including mathematical reasoning and theorem proving. As these
two tasks require strict and formal multi-step inference, they are appealing
domains for exploring the reasoning ability of LLMs but still face important
challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the
effectiveness of intermediate steps guidance. However, such step-wise
annotation requires heavy labor, leading to insufficient training steps for
current benchmarks. To fill this gap, this work introduces MUSTARD, a data
generation framework that masters uniform synthesis of theorem and proof data
of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It
samples a few mathematical concept seeds as the problem category. (2) Then, it
prompts a generative language model with the sampled concepts to obtain both
the problems and their step-wise formal solutions. (3) Lastly, the framework
utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With
the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE
with 5,866 valid data points. Each data point contains an informal statement,
an informal proof, and a translated formal proof that passes the prover
validation. We perform extensive analysis and demonstrate that MUSTARD
generates validated high-quality step-by-step data. We further apply the
MUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B
achieves a 15.41% average relative performance gain in automated theorem
proving, and 8.18% in math word problems. Codes and data are available at
https://github.com/Eleanor-H/MUSTARD.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08958" title="Abstract">arXiv:2402.08958</a> [<a href="/pdf/2402.08958" title="Download PDF">pdf</a>, <a href="/format/2402.08958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Next-Level Post-Training Quantization of Hyper-Scale  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junhan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Kyungphil Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chungman Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Ho-young Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joonyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+Y">Yongkweon Jeon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the increasing complexity of generative AI models, post-training
quantization (PTQ) has emerged as a promising solution for deploying
hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ
schemes, however, consume considerable time and resources, which could be a
bottleneck in real situations where frequent model updates and multiple
hyper-parameter tunings are required. As a cost-effective alternative, one-shot
PTQ schemes have been proposed. Still, the performance is somewhat limited
because they cannot consider the inter-layer dependency within the attention
module, which is a very important feature of Transformers. In this paper, we
thus propose a novel PTQ algorithm that balances accuracy and efficiency. The
key idea of the proposed algorithm called aespa is to perform quantization
layer-wise for efficiency while considering cross-layer dependency to preserve
the attention score. Through extensive experiments on various language models
and complexity analysis, we demonstrate that aespa is accurate and efficient in
quantizing Transformer models.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08960" title="Abstract">arXiv:2402.08960</a> [<a href="/pdf/2402.08960" title="Download PDF">pdf</a>, <a href="/format/2402.08960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xiaobo Xia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziye Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiao He</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yandong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Mingming Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 17 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Contemporary cutting-edge open-vocabulary segmentation approaches commonly
rely on image-mask-text triplets, yet this restricted annotation is
labour-intensive and encounters scalability hurdles in complex real-world
scenarios. Although some methods are proposed to reduce the annotation cost
with only text supervision, the incompleteness of supervision severely limits
the versatility and performance. In this paper, we liberate the strict
correspondence between masks and texts by using independent image-mask and
image-text pairs, which can be easily collected respectively. With this
unpaired mask-text supervision, we propose a new weakly-supervised
open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident
pairs of mask predictions and entities in text descriptions. Using the
independent image-mask and image-text pairs, we predict a set of binary masks
and associate them with entities by resorting to the CLIP embedding space.
However, the inherent noise in the correspondence between masks and entities
poses a significant challenge when obtaining reliable pairs. In light of this,
we advocate using the large vision-language model (LVLM) to refine text
descriptions and devise a multi-scale ensemble to stablise the matching between
masks and entities. Compared to text-only weakly-supervised methods, our
Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K
datasets, and even surpasses fully-supervised methods on the challenging PASCAL
Context-459 dataset.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08961" title="Abstract">arXiv:2402.08961</a> [<a href="/pdf/2402.08961" title="Download PDF">pdf</a>, <a href="/format/2402.08961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional  Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenbin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Existing knowledge hypergraph embedding methods mainly focused on improving
model performance, but their model structures are becoming more complex and
redundant. Furthermore, due to the inherent complex semantic knowledge, the
computation of knowledge hypergraph embedding models is often very expensive,
leading to low efficiency. In this paper, we propose a feature interaction and
extraction-enhanced 3D circular convolutional embedding model, HyCubE, which
designs a novel 3D circular convolutional neural network and introduces the
alternate mask stack strategy to achieve efficient n-ary knowledge hypergraph
embedding. By adaptively adjusting the 3D circular convolution kernel size and
uniformly embedding the entity position information, HyCubE improves the model
performance with fewer parameters and reaches a better trade-off between model
performance and efficiency. In addition, we use 1-N multilinear scoring based
on the entity mask mechanism to further accelerate the model training
efficiency. Finally, extensive experimental results on all datasets demonstrate
that HyCubE consistently outperforms state-of-the-art baselines, with an
average improvement of 4.08%-10.77% and a maximum improvement of 21.16% across
all metrics. Commendably, HyCubE speeds up by an average of 7.55x and reduces
memory usage by an average of 77.02% compared to the latest state-of-the-art
baselines.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08963" title="Abstract">arXiv:2402.08963</a> [<a href="/pdf/2402.08963" title="Download PDF">pdf</a>, <a href="/format/2402.08963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DUEL: Duplicate Elimination on Active Memory for Self-Supervised  Class-Imbalanced Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+W">Won-Seok Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyundo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dong-Sig Han</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Junseok Park</a>, 
<a href="/search/cs?searchtype=author&query=Koo%2C+H">Heeyeon Koo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Byoung-Tak Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a full paper at AAAI 2024: The 38th Annual AAAI Conference on Artificial Intelligence (Main Tech Track). 7 pages (main paper), 2 pages (references), 11 pages (appendix) each
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent machine learning algorithms have been developed using well-curated
datasets, which often require substantial cost and resources. On the other
hand, the direct use of raw data often leads to overfitting towards frequently
occurring class information. To address class imbalances cost-efficiently, we
propose an active data filtering process during self-supervised pre-training in
our novel framework, Duplicate Elimination (DUEL). This framework integrates an
active memory inspired by human working memory and introduces distinctiveness
information, which measures the diversity of the data in the memory, to
optimize both the feature extractor and the memory. The DUEL policy, which
replaces the most duplicated data with new samples, aims to enhance the
distinctiveness information in the memory and thereby mitigate class
imbalances. We validate the effectiveness of the DUEL framework in
class-imbalanced environments, demonstrating its robustness and providing
reliable results in downstream tasks. We also analyze the role of the DUEL
policy in the training process through various metrics and visualizations.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08964" title="Abstract">arXiv:2402.08964</a> [<a href="/pdf/2402.08964" title="Download PDF">pdf</a>, <a href="/format/2402.08964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting User Experience on Laptops from Hardware Specifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padhi%2C+S">Saswat Padhi</a>, 
<a href="/search/cs?searchtype=author&query=Bhasin%2C+S+K">Sunil K. Bhasin</a>, 
<a href="/search/cs?searchtype=author&query=Ammu%2C+U+K">Udaya K. Ammu</a>, 
<a href="/search/cs?searchtype=author&query=Bergman%2C+A">Alex Bergman</a>, 
<a href="/search/cs?searchtype=author&query=Knies%2C+A">Allan Knies</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Spotlight presentation at the ML for Systems workshop at NeurIPS 2023 ; 9 pages with appendix ; <a href="https://openreview.net/forum?id=mHShSE7MSU">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Estimating the overall user experience (UX) on a device is a common challenge
faced by manufacturers. Today, device makers primarily rely on microbenchmark
scores, such as Geekbench, that stress test specific hardware components, such
as CPU or RAM, but do not satisfactorily capture consumer workloads. System
designers often rely on domain-specific heuristics and extensive testing of
prototypes to reach a desired UX goal, and yet there is often a mismatch
between the manufacturers' performance claims and the consumers' experience.
<br />We present our initial results on predicting real-life experience on laptops
from their hardware specifications. We target web applications that run on
Chromebooks (ChromeOS laptops) for a simple and fair aggregation of experience
across applications and workloads. On 54 laptops, we track 9 UX metrics on
common end-user workloads: web browsing, video playback and audio/video calls.
We focus on a subset of high-level metrics exposed by the Chrome browser, that
are part of the Web Vitals initiative for judging the UX on web applications.
<br />With a dataset of 100K UX data points, we train gradient boosted regression
trees that predict the metric values from device specifications. Across our 9
metrics, we note a mean $R^2$ score (goodness-of-fit on our dataset) of 97.8%
and a mean MAAPE (percentage error in prediction on unseen data) of 10.1%.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08966" title="Abstract">arXiv:2402.08966</a> [<a href="/pdf/2402.08966" title="Download PDF">pdf</a>, <a href="/format/2402.08966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretraining Vision-Language Model for Difference Visual Question  Answering in Longitudinal Chest X-rays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y">Yeongjae Cho</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+H">Heejun Shin</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sungzoon Cho</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+D">Dongmyung Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Difference visual question answering (diff-VQA) is a challenging task that
requires answering complex questions based on differences between a pair of
images. This task is particularly important in reading chest X-ray images
because radiologists often compare multiple images of the same patient taken at
different times to track disease progression and changes in its severity in
their clinical practice. However, previous works focused on designing specific
network architectures for the diff-VQA task, missing opportunities to enhance
the model's performance using a pretrained vision-language model (VLM). Here,
we introduce a novel VLM called PLURAL, which is pretrained on natural and
longitudinal chest X-ray data for the diff-VQA task. The model is developed
using a step-by-step approach, starting with being pretrained on natural images
and texts, followed by being trained using longitudinal chest X-ray data. The
longitudinal data consist of pairs of X-ray images, along with question-answer
sets and radiologist's reports that describe the changes in lung abnormalities
and diseases over time. Our experimental results show that the PLURAL model
outperforms state-of-the-art methods not only in diff-VQA for longitudinal
X-rays but also in conventional VQA for a single X-ray image. Through extensive
experiments, we demonstrate the effectiveness of the proposed VLM architecture
and pretraining method in improving the model's performance.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08967" title="Abstract">arXiv:2402.08967</a> [<a href="/pdf/2402.08967" title="Download PDF">pdf</a>, <a href="/format/2402.08967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI for Pull Request Descriptions: Adoption, Impact, and  Developer Interventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hata%2C+H">Hideaki Hata</a>, 
<a href="/search/cs?searchtype=author&query=Treude%2C+C">Christoph Treude</a>, 
<a href="/search/cs?searchtype=author&query=Matsumoto%2C+K">Kenichi Matsumoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">GitHub's Copilot for Pull Requests (PRs) is a promising service aiming to
automate various developer tasks related to PRs, such as generating summaries
of changes or providing complete walkthroughs with links to the relevant code.
As this innovative technology gains traction in the Open Source Software (OSS)
community, it is crucial to examine its early adoption and its impact on the
development process. Additionally, it offers a unique opportunity to observe
how developers respond when they disagree with the generated content. In our
study, we employ a mixed-methods approach, blending quantitative analysis with
qualitative insights, to examine 18,256 PRs in which parts of the descriptions
were crafted by generative AI. Our findings indicate that: (1) Copilot for PRs,
though in its infancy, is seeing a marked uptick in adoption. (2) PRs enhanced
by Copilot for PRs require less review time and have a higher likelihood of
being merged. (3) Developers using Copilot for PRs often complement the
automated descriptions with their manual input. These results offer valuable
insights into the growing integration of generative AI in software development.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08968" title="Abstract">arXiv:2402.08968</a> [<a href="/pdf/2402.08968" title="Download PDF">pdf</a>, <a href="/format/2402.08968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GrounDial: Human-norm Grounded Safe Dialog Response Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Siwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+S">Shuyang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Kachuee%2C+M">Mohammad Kachuee</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+S">Shayan Ray</a>, 
<a href="/search/cs?searchtype=author&query=Taghavi%2C+T">Tara Taghavi</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sungroh Yoon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to findings of EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Current conversational AI systems based on large language models (LLMs) are
known to generate unsafe responses, agreeing to offensive user input or
including toxic content. Previous research aimed to alleviate the toxicity, by
fine-tuning LLM with manually annotated safe dialogue histories. However, the
dependency on additional tuning requires substantial costs. To remove the
dependency, we propose GrounDial, where response safety is achieved by
grounding responses to commonsense social rules without requiring fine-tuning.
A hybrid approach of in-context learning and human-norm-guided decoding of
GrounDial enables the response to be quantitatively and qualitatively safer
even without additional data or tuning.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08971" title="Abstract">arXiv:2402.08971</a> [<a href="/pdf/2402.08971" title="Download PDF">pdf</a>, <a href="/format/2402.08971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Language Generation Model for Robust Structure Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+J">Junghyun Min</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+W">Woochul Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yeonsoo Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 5 tables, 7 pages of appendix with 8 additional tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose Structured Language Generation Model (SLGM), a mixture of new loss
function and inference method for better generalization of structured outputs.
Previous studies on structure prediction (e.g. NER, RE) make use of explicit
dataset information, which would boost performance, yet it might pose
challenges to robust generalization in real-world situations. Instead, our
model gives generalized format information about data indirectly. With format
information, we could reduce sequence-to-sequence problem into classification
problem via loss calibration and formatted decoding. Our experimental results
showed SLGM successfully maintain performance without dataset information, and
showed much less format errors. We also showed our model can work like adapters
on individual dataset, with no additional training.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08973" title="Abstract">arXiv:2402.08973</a> [<a href="/pdf/2402.08973" title="Download PDF">pdf</a>, <a href="/ps/2402.08973" title="Download PostScript">ps</a>, <a href="/format/2402.08973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence rate and exponential stability of backward Euler method for  neutral stochastic delay differential equations under generalized  monotonicity conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cai%2C+J">Jingjing Cai</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+Z">Ziheng Chen</a>, 
<a href="/search/math?searchtype=author&query=Niu%2C+Y">Yuanling Niu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work focuses on the numerical approximations of neutral stochastic delay
differential equations with their drift and diffusion coefficients growing
super-linearly with respect to both delay variables and state variables. Under
generalized monotonicity conditions, we prove that the backward Euler method
not only converges strongly in the mean square sense with order $1/2$, but also
inherit the mean square exponential stability of the original equations. As a
byproduct, we obtain the same results on convergence rate and exponential
stability of the backward Euler method for stochastic delay differential
equations with generalized monotonicity conditions. These theoretical results
are finally supported by several numerical experiments.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08974" title="Abstract">arXiv:2402.08974</a> [<a href="/pdf/2402.08974" title="Download PDF">pdf</a>, <a href="/format/2402.08974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining the Unique Online Risk Experiences and Mental Health Outcomes  of LGBTQ+ versus Heterosexual Youth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanni%2C+T">Tangila Tanni</a>, 
<a href="/search/cs?searchtype=author&query=Akter%2C+M">Mamtaj Akter</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+J">Joshua Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Amon%2C+M">Mary Amon</a>, 
<a href="/search/cs?searchtype=author&query=Wisniewski%2C+P">Pamela Wisniewski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">We collected and analyzed Instagram direct messages (DMs) from 173 youth aged
13-21 (including 86 LGBTQ+ youth). We examined youth's risk-flagged social
media trace data with their self-reported mental health outcomes to examine how
the differing online experiences of LGBTQ+ youth compare with their
heterosexual counterparts. We found that LGBTQ+ youth experienced significantly
more high-risk online interactions compared to heterosexual youth. LGBTQ+ youth
reported overall poorer mental health, with online harassment specifically
amplifying Self-Harm and Injury. LGBTQ+ youth's mental well-being linked
positively to sexual messages, unlike heterosexual youth. Qualitatively, we
found that most of the risk-flagged messages of LGBTQ+ youth were sexually
motivated; however, a silver lining was that they sought support for their
sexual identity from peers on the platform. The study highlights the importance
of tailored online safety and inclusive design for LGBTQ+ youth, with
implications for CHI community advancements in fostering a supportive online
environments.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08975" title="Abstract">arXiv:2402.08975</a> [<a href="/pdf/2402.08975" title="Download PDF">pdf</a>, <a href="/format/2402.08975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Research and application of Transformer based anomaly detection model: A  literature review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Mingrui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lansheng Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunjie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 77 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformer, as one of the most advanced neural network models in Natural
Language Processing (NLP), exhibits diverse applications in the field of
anomaly detection. To inspire research on Transformer-based anomaly detection,
this review offers a fresh perspective on the concept of anomaly detection. We
explore the current challenges of anomaly detection and provide detailed
insights into the operating principles of Transformer and its variants in
anomaly detection tasks. Additionally, we delineate various application
scenarios for Transformer-based anomaly detection models and discuss the
datasets and evaluation metrics employed. Furthermore, this review highlights
the key challenges in Transformer-based anomaly detection research and conducts
a comprehensive analysis of future research trends in this domain. The review
includes an extensive compilation of over 100 core references related to
Transformer-based anomaly detection. To the best of our knowledge, this is the
first comprehensive review that focuses on the research related to Transformer
in the context of anomaly detection. We hope that this paper can provide
detailed technical information to researchers interested in Transformer-based
anomaly detection tasks.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08976" title="Abstract">arXiv:2402.08976</a> [<a href="/pdf/2402.08976" title="Download PDF">pdf</a>, <a href="/format/2402.08976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidence-aware Fine-tuning of Sequential Recommendation Systems via  Conformal Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fangxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Ruocheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yueqing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kay Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">In Sequential Recommendation Systems, Cross-Entropy (CE) loss is commonly
used but fails to harness item confidence scores during training. Recognizing
the critical role of confidence in aligning training objectives with evaluation
metrics, we propose CPFT, a versatile framework that enhances recommendation
confidence by integrating Conformal Prediction (CP)-based losses with CE loss
during fine-tuning. CPFT dynamically generates a set of items with a high
probability of containing the ground truth, enriching the training process by
incorporating validation data without compromising its role in model selection.
This innovative approach, coupled with CP-based losses, sharpens the focus on
refining recommendation sets, thereby elevating the confidence in potential
item predictions. By fine-tuning item confidence through CP-based losses, CPFT
significantly enhances model performance, leading to more precise and
trustworthy recommendations that increase user trust and satisfaction. Our
extensive evaluation across five diverse datasets and four distinct sequential
models confirms CPFT's substantial impact on improving recommendation quality
through strategic confidence optimization. Access to the framework's code will
be provided following the acceptance of the paper.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08978" title="Abstract">arXiv:2402.08978</a> [<a href="/pdf/2402.08978" title="Download PDF">pdf</a>, <a href="/format/2402.08978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kam-Kwai%2C+W">Wong Kam-Kwai</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xuanwu Yue</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Huamin Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages. A preprint version submitted to IEEE Transactions on Visualization and Computer Graphics (TVCG), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">Financial cluster analysis allows investors to discover investment
alternatives and avoid undertaking excessive risks. However, this analytical
task faces substantial challenges arising from many pairwise comparisons, the
dynamic correlations across time spans, and the ambiguity in deriving
implications from business relational knowledge. We propose Prismatic, a visual
analytics system that integrates quantitative analysis of historical
performance and qualitative analysis of business relational knowledge to
cluster correlated businesses interactively. Prismatic features three
clustering processes: dynamic cluster generation, knowledge-based cluster
exploration, and correlation-based cluster validation. Utilizing a multi-view
clustering approach, it enriches data-driven clusters with knowledge-driven
similarity, providing a nuanced understanding of business correlations. Through
well-coordinated visual views, Prismatic facilitates a comprehensive
interpretation of intertwined quantitative and qualitative features,
demonstrating its usefulness and effectiveness via case studies on formulating
concept stocks and extensive interviews with domain experts.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08979" title="Abstract">arXiv:2402.08979</a> [<a href="/pdf/2402.08979" title="Download PDF">pdf</a>, <a href="/ps/2402.08979" title="Download PostScript">ps</a>, <a href="/format/2402.08979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-enabled Flexible Job-shop Scheduling for Scalable Smart  Manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Moon%2C+S">Sihoon Moon</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+S">Sanghoon Lee</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+K">Kyung-Joon Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In smart manufacturing systems (SMSs), flexible job-shop scheduling with
transportation constraints (FJSPT) is essential to optimize solutions for
maximizing productivity, considering production flexibility based on automated
guided vehicles (AGVs). Recent developments in deep reinforcement learning
(DRL)-based methods for FJSPT have encountered a scale generalization
challenge. These methods underperform when applied to environment at scales
different from their training set, resulting in low-quality solutions. To
address this, we introduce a novel graph-based DRL method, named the
Heterogeneous Graph Scheduler (HGS). Our method leverages locally extracted
relational knowledge among operations, machines, and vehicle nodes for
scheduling, with a graph-structured decision-making framework that reduces
encoding complexity and enhances scale generalization. Our performance
evaluation, conducted with benchmark datasets, reveals that the proposed method
outperforms traditional dispatching rules, meta-heuristics, and existing
DRL-based approaches in terms of makespan performance, even on large-scale
instances that have not been experienced during training.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08980" title="Abstract">arXiv:2402.08980</a> [<a href="/pdf/2402.08980" title="Download PDF">pdf</a>, <a href="/ps/2402.08980" title="Download PostScript">ps</a>, <a href="/format/2402.08980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmniBOR: A System for Automatic, Verifiable Artifact Resolution across  Software Supply Chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seshadri%2C+B">Bharathi Seshadri</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yongkui Han</a>, 
<a href="/search/cs?searchtype=author&query=Olson%2C+C">Chris Olson</a>, 
<a href="/search/cs?searchtype=author&query=Pollak%2C+D">David Pollak</a>, 
<a href="/search/cs?searchtype=author&query=Tomasevic%2C+V">Vojislav Tomasevic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Software supply chain attacks, which exploit the build process or artifacts
used in the process of building a software product, are increasingly of
concern. To combat these attacks, one must be able to check that every artifact
that a software product depends on does not contain vulnerabilities. In this
paper, we introduce OmniBOR, (Universal Bill of Receipts) a minimalistic scheme
for build tools to create an artifact dependency graph which can be used to
track every software artifact incorporated into a built software product. We
present the architecture of OmniBOR, the underlying data representations, and
two implementations that produce OmniBOR data and embed an OmniBOR Identifier
into built software, including a compiler-based approach and one based on
tracing the build process. We demonstrate the efficacy of this approach on
benchmarks including a Linux distribution for applications such as Common
Vulnerabilities and Exposures (CVE) detection and software bill of materials
(SBOM) computation.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08982" title="Abstract">arXiv:2402.08982</a> [<a href="/pdf/2402.08982" title="Download PDF">pdf</a>, <a href="/format/2402.08982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEL: Efficient Multi-Task Evolutionary Learning for High-Dimensional  Feature Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xubin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shangguan%2C+H">Haojiong Shangguan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fengyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shangrui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+W">Weijia Jia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Feature selection is a crucial step in data mining to enhance model
performance by reducing data dimensionality. However, the increasing
dimensionality of collected data exacerbates the challenge known as the "curse
of dimensionality", where computation grows exponentially with the number of
dimensions. To tackle this issue, evolutionary computational (EC) approaches
have gained popularity due to their simplicity and applicability.
Unfortunately, the diverse designs of EC methods result in varying abilities to
handle different data, often underutilizing and not sharing information
effectively. In this paper, we propose a novel approach called PSO-based
Multi-task Evolutionary Learning (MEL) that leverages multi-task learning to
address these challenges. By incorporating information sharing between
different feature selection tasks, MEL achieves enhanced learning ability and
efficiency. We evaluate the effectiveness of MEL through extensive experiments
on 22 high-dimensional datasets. Comparing against 24 EC approaches, our method
exhibits strong competitiveness. Additionally, we have open-sourced our code on
GitHub at https://github.com/wangxb96/MEL.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08983" title="Abstract">arXiv:2402.08983</a> [<a href="/pdf/2402.08983" title="Download PDF">pdf</a>, <a href="/format/2402.08983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware  Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhangchen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Fengqing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Luyao Niu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinyuan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B+Y">Bill Yuchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Poovendran%2C+R">Radha Poovendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As large language models (LLMs) become increasingly integrated into
real-world applications such as code generation and chatbot assistance,
extensive efforts have been made to align LLM behavior with human values,
including safety. Jailbreak attacks, aiming to provoke unintended and unsafe
behaviors from LLMs, remain a significant/leading LLM safety threat. In this
paper, we aim to defend LLMs against jailbreak attacks by introducing
SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and
harmless responses to user queries. Our insight in developing SafeDecoding is
based on the observation that, even though probabilities of tokens representing
harmful contents outweigh those representing harmless responses, safety
disclaimers still appear among the top tokens after sorting tokens by
probability in descending order. This allows us to mitigate jailbreak attacks
by identifying safety disclaimers and amplifying their token probabilities,
while simultaneously attenuating the probabilities of token sequences that are
aligned with the objectives of jailbreak attacks. We perform extensive
experiments on five LLMs using six state-of-the-art jailbreak attacks and four
benchmark datasets. Our results show that SafeDecoding significantly reduces
the attack success rate and harmfulness of jailbreak attacks without
compromising the helpfulness of responses to benign user queries. SafeDecoding
outperforms six defense methods.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08986" title="Abstract">arXiv:2402.08986</a> [<a href="/pdf/2402.08986" title="Download PDF">pdf</a>, <a href="/ps/2402.08986" title="Download PostScript">ps</a>, <a href="/format/2402.08986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Adversarial Spectrum Attacks via Distance to Decision Boundary  Statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenwei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shangqing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhuo Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Machine learning has been adopted for efficient cooperative spectrum sensing.
However, it incurs an additional security risk due to attacks leveraging
adversarial machine learning to create malicious spectrum sensing values to
deceive the fusion center, called adversarial spectrum attacks. In this paper,
we propose an efficient framework for detecting adversarial spectrum attacks.
Our design leverages the concept of the distance to the decision boundary (DDB)
observed at the fusion center and compares the training and testing DDB
distributions to identify adversarial spectrum attacks. We create a
computationally efficient way to compute the DDB for machine learning based
spectrum sensing systems. Experimental results based on realistic spectrum data
show that our method, under typical settings, achieves a high detection rate of
up to 99\% and maintains a low false alarm rate of less than 1\%. In addition,
our method to compute the DDB based on spectrum data achieves 54\%--64\%
improvements in computational efficiency over existing distance calculation
methods. The proposed DDB-based detection framework offers a practical and
efficient solution for identifying malicious sensing values created by
adversarial spectrum attacks.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08988" title="Abstract">arXiv:2402.08988</a> [<a href="/pdf/2402.08988" title="Download PDF">pdf</a>, <a href="/format/2402.08988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An In-Depth Investigation of LEO Satellite Topology Design Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zihan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jyothi%2C+S+A">Sangeetha Abdu Jyothi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Performance (cs.PF)

</div>
<p class="mathjax">Low Earth Orbit (LEO) satellite networks are rapidly gaining traction today.
Although several real-world deployments exist, our preliminary analysis of LEO
topology performance with the soon-to-be operational Inter-Satellite Links
(ISLs) reveals several interesting characteristics that are difficult to
explain based on our current understanding of topologies. For example, a
real-world satellite shell with a low density of satellites offers better
latency performance than another shell with nearly double the number of
satellites. In this work, we conduct an in-depth investigation of LEO satellite
topology design parameters and their impact on network performance while using
the ISLs. In particular, we focus on three design parameters: the number of
orbits in a shell, the inclination of orbits, and the number of satellites per
orbit. Through an extensive analysis of real-world and synthetic satellite
configurations, we uncover several interesting properties of satellite
topologies. Notably, there exist thresholds for the number of satellites per
orbit and the number of orbits below which the latency performance degrades
significantly. Moreover, network delay between a pair of traffic endpoints
depends on the alignment of the satellite's orbit (Inclination) with the
geographic locations of endpoints.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08989" title="Abstract">arXiv:2402.08989</a> [<a href="/pdf/2402.08989" title="Download PDF">pdf</a>, <a href="/ps/2402.08989" title="Download PostScript">ps</a>, <a href="/format/2402.08989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Algorithmic Meta Theorem for Homomorphism Indistinguishability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seppelt%2C+T">Tim Seppelt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">Two graphs $G$ and $H$ are homomorphism indistinguishable over a family of
graphs $\mathcal{F}$ if for all graphs $F \in \mathcal{F}$ the number of
homomorphisms from $F$ to $G$ is equal to the number of homomorphism from $F$
to $H$. Many natural equivalence relations comparing graphs such as (quantum)
isomorphism, cospectrality, and logical equivalences can be characterised as
homomorphism indistinguishability relations over various graph classes.
<br />For a fixed graph class $\mathcal{F}$, the decision problem
HomInd($\mathcal{F}$) asks to determine whether two input graphs $G$ and $H$
are homomorphism indistinguishable over $\mathcal{F}$. The problem
HomInd($\mathcal{F}$) is known to be decidable only for few graph classes
$\mathcal{F}$. We show that HomInd($\mathcal{F}$) admits a randomised
polynomial-time algorithm for every graph class $\mathcal{F}$ of bounded
treewidth which is definable in counting monadic second-order logic CMSO2.
Thereby, we give the first general algorithm for deciding homomorphism
indistinguishability.
<br />This result extends to a version of HomInd where the graph class
$\mathcal{F}$ is specified by a CMSO2-sentence and a bound $k$ on the
treewidth, which are given as input. For fixed $k$, this problem is randomised
fixed-parameter tractable. If $k$ is part of the input then it is coNP- and
coW[1]-hard. Addressing a problem posed by Berkholz (2012), we show
coNP-hardness by establishing that deciding indistinguishability under the
$k$-dimensional Weisfeiler--Leman algorithm is coNP-hard when $k$ is part of
the input.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08994" title="Abstract">arXiv:2402.08994</a> [<a href="/pdf/2402.08994" title="Download PDF">pdf</a>, <a href="/format/2402.08994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic  Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qiongyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Changde Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengpei Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Huiguang He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The study of decoding visual neural information faces challenges in
generalizing single-subject decoding models to multiple subjects, due to
individual differences. Moreover, the limited availability of data from a
single subject has a constraining impact on model performance. Although prior
multi-subject decoding methods have made significant progress, they still
suffer from several limitations, including difficulty in extracting global
neural response features, linear scaling of model parameters with the number of
subjects, and inadequate characterization of the relationship between neural
responses of different subjects to various stimuli. To overcome these
limitations, we propose a CLIP-guided Multi-sUbject visual neural information
SEmantic Decoding (CLIP-MUSED) method. Our method consists of a
Transformer-based feature extractor to effectively model global neural
representations. It also incorporates learnable subject-specific tokens that
facilitates the aggregation of multi-subject data without a linear increase of
parameters. Additionally, we employ representational similarity analysis (RSA)
to guide token representation learning based on the topological relationship of
visual stimuli in the representation space of CLIP, enabling full
characterization of the relationship between neural responses of different
subjects under different stimuli. Finally, token representations are used for
multi-subject semantic decoding. Our proposed method outperforms single-subject
decoding methods and achieves state-of-the-art performance among the existing
multi-subject methods on two fMRI datasets. Visualization results provide
insights into the effectiveness of our proposed method. Code is available at
https://github.com/CLIP-MUSED/CLIP-MUSED.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08995" title="Abstract">arXiv:2402.08995</a> [<a href="/pdf/2402.08995" title="Download PDF">pdf</a>, <a href="/format/2402.08995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiaying Lu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bo Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jieyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yingchaojie Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jingyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yuchen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, Large Language Model based Autonomous system(LLMAS) has gained
great popularity for its potential to simulate complicated behaviors of human
societies. One of its main challenges is to present and analyze the dynamic
events evolution of LLMAS. In this work, we present a visualization approach to
explore detailed statuses and agents' behavior within LLMAS. We propose a
general pipeline that establishes a behavior structure from raw LLMAS execution
events, leverages a behavior summarization algorithm to construct a
hierarchical summary of the entire structure in terms of time sequence, and a
cause trace method to mine the causal relationship between agent behaviors. We
then develop AgentLens, a visual analysis system that leverages a hierarchical
temporal visualization for illustrating the evolution of LLMAS, and supports
users to interactively investigate details and causes of agents' behaviors. Two
usage scenarios and a user study demonstrate the effectiveness and usability of
our AgentLens.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08996" title="Abstract">arXiv:2402.08996</a> [<a href="/pdf/2402.08996" title="Download PDF">pdf</a>, <a href="/format/2402.08996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Task Learning of Active Fault-Tolerant Controller for Leg Failures  in Quadruped robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+T">Taixian Hou</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+J">Jiaxin Tu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiaofei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhiyan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+P">Peng Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lihua Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 9 figures, ICRA2024 Accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Electric quadruped robots used in outdoor exploration are susceptible to
leg-related electrical or mechanical failures. Unexpected joint power loss and
joint locking can immediately pose a falling threat. Typically, controllers
lack the capability to actively sense the condition of their own joints and
take proactive actions. Maintaining the original motion patterns could lead to
disastrous consequences, as the controller may produce irrational output within
a short period of time, further creating the risk of serious physical injuries.
This paper presents a hierarchical fault-tolerant control scheme employing a
multi-task training architecture capable of actively perceiving and overcoming
two types of leg joint faults. The architecture simultaneously trains three
joint task policies for health, power loss, and locking scenarios in parallel,
introducing a symmetric reflection initialization technique to ensure rapid and
stable gait skill transformations. Experiments demonstrate that the control
scheme is robust in unexpected scenarios where a single leg experiences
concurrent joint faults in two joints. Furthermore, the policy retains the
robot's planar mobility, enabling rough velocity tracking. Finally, zero-shot
Sim2Real transfer is achieved on the real-world SOLO8 robot, countering both
electrical and mechanical failures.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08998" title="Abstract">arXiv:2402.08998</a> [<a href="/pdf/2402.08998" title="Download PDF">pdf</a>, <a href="/format/2402.08998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearly Minimax Optimal Regret for Learning Linear Mixture Stochastic  Shortest Path
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di%2C+Q">Qiwei Di</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiafan He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dongruo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 1 figure, In ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the Stochastic Shortest Path (SSP) problem with a linear mixture
transition kernel, where an agent repeatedly interacts with a stochastic
environment and seeks to reach certain goal state while minimizing the
cumulative cost. Existing works often assume a strictly positive lower bound of
the cost function or an upper bound of the expected length for the optimal
policy. In this paper, we propose a new algorithm to eliminate these
restrictive assumptions. Our algorithm is based on extended value iteration
with a fine-grained variance-aware confidence set, where the variance is
estimated recursively from high-order moments. Our algorithm achieves an
$\tilde{\mathcal O}(dB_*\sqrt{K})$ regret bound, where $d$ is the dimension of
the feature mapping in the linear transition kernel, $B_*$ is the upper bound
of the total cumulative cost for the optimal policy, and $K$ is the number of
episodes. Our regret upper bound matches the $\Omega(dB_*\sqrt{K})$ lower bound
of linear mixture SSPs in Min et al. (2022), which suggests that our algorithm
is nearly minimax optimal.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08999" title="Abstract">arXiv:2402.08999</a> [<a href="/pdf/2402.08999" title="Download PDF">pdf</a>, <a href="/ps/2402.08999" title="Download PostScript">ps</a>, <a href="/format/2402.08999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Federated Deep Learning for Standardising Naming Conventions  in Radiotherapy Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haidar%2C+A">Ali Haidar</a>, 
<a href="/search/cs?searchtype=author&query=Mouiee%2C+D+A">Daniel Al Mouiee</a>, 
<a href="/search/cs?searchtype=author&query=Aly%2C+F">Farhannah Aly</a>, 
<a href="/search/cs?searchtype=author&query=Thwaites%2C+D">David Thwaites</a>, 
<a href="/search/cs?searchtype=author&query=Holloway%2C+L">Lois Holloway</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Standardising structure volume names in radiotherapy (RT) data is necessary
to enable data mining and analyses, especially across multi-institutional
centres. This process is time and resource intensive, which highlights the need
for new automated and efficient approaches to handle the task. Several machine
learning-based methods have been proposed and evaluated to standardise
nomenclature. However, no studies have considered that RT patient records are
distributed across multiple data centres. This paper introduces a method that
emulates real-world environments to establish standardised nomenclature. This
is achieved by integrating decentralised real-time data and federated learning
(FL). A multimodal deep artificial neural network was proposed to standardise
RT data in federated settings. Three types of possible attributes were
extracted from the structures to train the deep learning models: tabular,
visual, and volumetric. Simulated experiments were carried out to train the
models across several scenarios including multiple data centres, input
modalities, and aggregation strategies. The models were compared against models
developed with single modalities in federated settings, in addition to models
trained in centralised settings. Categorical classification accuracy was
calculated on hold-out samples to inform the models performance. Our results
highlight the need for fusing multiple modalities when training such models,
with better performance reported with tabular-volumetric models. In addition,
we report comparable accuracy compared to models built in centralised settings.
This demonstrates the suitability of FL for handling the standardization task.
Additional ablation analyses showed that the total number of samples in the
data centres and the number of data centres highly affects the training process
and should be carefully considered when building standardisation models.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09004" title="Abstract">arXiv:2402.09004</a> [<a href="/pdf/2402.09004" title="Download PDF">pdf</a>, <a href="/format/2402.09004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient Alignment with Prototype Feature for Fully Test-time Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Juhyeon Shin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jonghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Saehyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+M">Minjun Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongjun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+U">Uiwon Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sungroh Yoon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In context of Test-time Adaptation(TTA), we propose a regularizer, dubbed
Gradient Alignment with Prototype feature (GAP), which alleviates the
inappropriate guidance from entropy minimization loss from misclassified pseudo
label. We developed a gradient alignment loss to precisely manage the
adaptation process, ensuring that changes made for some data don't negatively
impact the model's performance on other data. We introduce a prototype feature
of a class as a proxy measure of the negative impact. To make GAP regularizer
feasible under the TTA constraints, where model can only access test data
without labels, we tailored its formula in two ways: approximating prototype
features with weight vectors of the classifier, calculating gradient without
back-propagation. We demonstrate GAP significantly improves TTA methods across
various datasets, which proves its versatility and effectiveness.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09008" title="Abstract">arXiv:2402.09008</a> [<a href="/pdf/2402.09008" title="Download PDF">pdf</a>, <a href="/format/2402.09008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Query Focused Disaster Summarization via Instruction-Based  Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seeberger%2C+P">Philipp Seeberger</a>, 
<a href="/search/cs?searchtype=author&query=Riedhammer%2C+K">Korbinian Riedhammer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CrisisFACTS (TREC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic summarization of mass-emergency events plays a critical role in
disaster management. The second edition of CrisisFACTS aims to advance disaster
summarization based on multi-stream fact-finding with a focus on web sources
such as Twitter, Reddit, Facebook, and Webnews. Here, participants are asked to
develop systems that can extract key facts from several disaster-related
events, which ultimately serve as a summary. This paper describes our method to
tackle this challenging task. We follow previous work and propose to use a
combination of retrieval, reranking, and an embarrassingly simple
instruction-following summarization. The two-stage retrieval pipeline relies on
BM25 and MonoT5, while the summarizer module is based on the open-source Large
Language Model (LLM) LLaMA-13b. For summarization, we explore a Question
Answering (QA)-motivated prompting approach and find the evidence useful for
extracting query-relevant facts. The automatic metrics and human evaluation
show strong results but also highlight the gap between open-source and
proprietary systems.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09009" title="Abstract">arXiv:2402.09009</a> [<a href="/pdf/2402.09009" title="Download PDF">pdf</a>, <a href="/format/2402.09009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Practical and Online Trajectory Planner for Autonomous Ships&#x27;  Berthing, Incorporating Speed Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mwange%2C+A+N">Agnes Ngina Mwange</a>, 
<a href="/search/eess?searchtype=author&query=Rachman%2C+D+M">Dimas Maulana Rachman</a>, 
<a href="/search/eess?searchtype=author&query=Suyama%2C+R">Rin Suyama</a>, 
<a href="/search/eess?searchtype=author&query=Maki%2C+A">Atsuo Maki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Autonomous ships are essentially designed and equipped to perceive their
internal and external environment and subsequently perform appropriate actions
depending on the predetermined objective(s) without human intervention.
Consequently, trajectory planning algorithms for autonomous berthing must
consider factors such as system dynamics, ship actuators, environmental
disturbances, and the safety of the ship, other ships, and port structures,
among others. In this study, basing the ship dynamics on the low-speed MMG
model, trajectory planning for an autonomous ship is modeled as an optimal
control problem (OCP) that is transcribed into a nonlinear programming problem
(NLP) using the direct multiple shooting technique. To enhance berthing safety,
besides considering wind disturbances, speed control, actuators' limitations,
and collision avoidance features are incorporated as constraints in the NLP,
which is then solved using the Sequential Quadratic Programming (SQP) algorithm
in MATLAB. Finally, the performance of the proposed planner is evaluated
through (i) comparison with solutions obtained using CMA-ES for two different
model ships, (ii) trajectory planning for different harbor entry and berth
approach scenarios, and (iii) feasibility study using stochastically generated
initial conditions and positions within the port boundaries. Simulation results
indicate enhanced berthing safety as well as practical and computational
feasibility making the planner suitable for real-time applications.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09011" title="Abstract">arXiv:2402.09011</a> [<a href="/pdf/2402.09011" title="Download PDF">pdf</a>, <a href="/ps/2402.09011" title="Download PostScript">ps</a>, <a href="/format/2402.09011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Deterministic Distributed Maximum Weight Independent Set  Approximation in Sparse Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gil%2C+Y">Yuval Gil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We design new deterministic CONGEST approximation algorithms for
\emph{maximum weight independent set (MWIS)} in \emph{sparse graphs}. As our
main results, we obtain new $\Delta(1+\epsilon)$-approximation algorithms as
well as algorithms whose approximation ratio depend strictly on $\alpha$, in
graphs with maximum degree $\Delta$ and arboricity $\alpha$. For
(deterministic) $\Delta(1+\epsilon)$-approximation, the current
state-of-the-art is due to a recent breakthrough by Faour et al.\ [SODA 2023]
that showed an $O(\log^{2} (\Delta W)\cdot \log (1/\epsilon)+\log ^{*}n)$-round
algorithm, where $W$ is the largest node-weight (this bound translates to
$O(\log^{2} n\cdot\log (1/\epsilon))$ under the common assumption that
$W=\text{poly}(n)$). As for $\alpha$-dependent approximations, a deterministic
CONGEST $(8(1+\epsilon)\cdot\alpha)$-approximation algorithm with runtime
$O(\log^{3} n\cdot\log (1/\epsilon))$ can be derived by combining the
aforementioned algorithm of Faour et al.\ with a method presented by
Kawarabayashi et al.\ [DISC 2020].
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09015" title="Abstract">arXiv:2402.09015</a> [<a href="/pdf/2402.09015" title="Download PDF">pdf</a>, <a href="/format/2402.09015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards better Human-Agent Alignment: Assessing Task Utility in  LLM-Powered Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arabzadeh%2C+N">Negar Arabzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kiseleva%2C+J">Julia Kiseleva</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A">Ahmed Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Dibia%2C+V">Victor Dibia</a>, 
<a href="/search/cs?searchtype=author&query=Fourney%2C+A">Adam Fourney</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+C">Charles Clarke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid development in the field of Large Language Models (LLMs) has led to
a surge in applications that facilitate collaboration among multiple agents to
assist humans in their daily tasks. However, a significant gap remains in
assessing whether LLM-powered applications genuinely enhance user experience
and task execution efficiency. This highlights the pressing need for methods to
verify utility of LLM-powered applications, particularly by ensuring alignment
between the application's functionality and end-user needs. We introduce
AgentEval provides an implementation for the math problems}, a novel framework
designed to simplify the utility verification process by automatically
proposing a set of criteria tailored to the unique purpose of any given
application. This allows for a comprehensive assessment, quantifying the
utility of an application against the suggested criteria. We present a
comprehensive analysis of the robustness of quantifier's work.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09016" title="Abstract">arXiv:2402.09016</a> [<a href="/pdf/2402.09016" title="Download PDF">pdf</a>, <a href="/format/2402.09016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pyramid Attention Network for Medical Image Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhuoyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiqiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, published to ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The advent of deep-learning-based registration networks has addressed the
time-consuming challenge in traditional iterative methods.However, the
potential of current registration networks for comprehensively capturing
spatial relationships has not been fully explored, leading to inadequate
performance in large-deformation image registration.The pure convolutional
neural networks (CNNs) neglect feature enhancement, while current
Transformer-based networks are susceptible to information redundancy.To
alleviate these issues, we propose a pyramid attention network (PAN) for
deformable medical image registration.Specifically, the proposed PAN
incorporates a dual-stream pyramid encoder with channel-wise attention to boost
the feature representation.Moreover, a multi-head local attention Transformer
is introduced as decoder to analyze motion patterns and generate deformation
fields.Extensive experiments on two public brain magnetic resonance imaging
(MRI) datasets and one abdominal MRI dataset demonstrate that our method
achieves favorable registration performance, while outperforming several
CNN-based and Transformer-based registration networks.Our code is publicly
available at https://github.com/JuliusWang-7/PAN.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09021" title="Abstract">arXiv:2402.09021</a> [<a href="/pdf/2402.09021" title="Download PDF">pdf</a>, <a href="/format/2402.09021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Opinion Dynamic Modeling as Concurrent Set Relations in  Rewriting Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olarte%2C+C">Carlos Olarte</a>, 
<a href="/search/cs?searchtype=author&query=Ram%C3%ADrez%2C+C">Carlos Ram&#xed;rez</a>, 
<a href="/search/cs?searchtype=author&query=Rocha%2C+C">Camilo Rocha</a>, 
<a href="/search/cs?searchtype=author&query=Valencia%2C+F">Frank Valencia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Social media platforms have played a key role in weaponizing the polarization
of social, political, and democratic processes. This is, mainly, because they
are a medium for opinion formation. Opinion dynamic models are a tool for
understanding the role of specific social factors on the acceptance/rejection
of opinions because they can be used to analyze certain assumptions on human
behaviors. This work presents a framework that uses concurrent set relations as
the formal basis to specify, simulate, and analyze social interaction systems
with dynamic opinion models. Standard models for social learning are obtained
as particular instances of the proposed framework. It has been implemented in
the Maude system as a fully executable rewrite theory that can be used to
better understand how opinions of a system of agents can be shaped. This paper
also reports an initial exploration in Maude on the use of reachability
analysis, probabilistic simulation, and statistical model checking of important
properties related to opinion dynamic models.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09022" title="Abstract">arXiv:2402.09022</a> [<a href="/pdf/2402.09022" title="Download PDF">pdf</a>, <a href="/ps/2402.09022" title="Download PostScript">ps</a>, <a href="/format/2402.09022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing AI-Based Code Assistants in Method Generation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corso%2C+V">Vincenzo Corso</a>, 
<a href="/search/cs?searchtype=author&query=Mariani%2C+L">Leonardo Mariani</a>, 
<a href="/search/cs?searchtype=author&query=Micucci%2C+D">Daniela Micucci</a>, 
<a href="/search/cs?searchtype=author&query=Riganelli%2C+O">Oliviero Riganelli</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the 46th International Conference on Software
  Engineering (ICSE 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">AI-based code assistants are increasingly popular as a means to enhance
productivity and improve code quality. This study compares four AI-based code
assistants, GitHub Copilot, Tabnine, ChatGPT, and Google Bard, in method
generation tasks, assessing their ability to produce accurate, correct, and
efficient code. Results show that code assistants are useful, with
complementary capabilities, although they rarely generate ready-to-use correct
code.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09023" title="Abstract">arXiv:2402.09023</a> [<a href="/pdf/2402.09023" title="Download PDF">pdf</a>, <a href="/format/2402.09023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review-Incorporated Model-Agnostic Profile Injection Attacks on  Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shiyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liming Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent studies have shown that recommender systems (RSs) are highly
vulnerable to data poisoning attacks. Understanding attack tactics helps
improve the robustness of RSs. We intend to develop efficient attack methods
that use limited resources to generate high-quality fake user profiles to
achieve 1) transferability among black-box RSs 2) and imperceptibility among
detectors. In order to achieve these goals, we introduce textual reviews of
products to enhance the generation quality of the profiles. Specifically, we
propose a novel attack framework named R-Trojan, which formulates the attack
objectives as an optimization problem and adopts a tailored transformer-based
generative adversarial network (GAN) to solve it so that high-quality attack
profiles can be produced. Comprehensive experiments on real-world datasets
demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods
on various victim RSs under black-box settings and show its good
imperceptibility.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09024" title="Abstract">arXiv:2402.09024</a> [<a href="/pdf/2402.09024" title="Download PDF">pdf</a>, <a href="/ps/2402.09024" title="Download PostScript">ps</a>, <a href="/format/2402.09024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Rewrite Rules to Axioms in the $&#x3bb;$$&#x3a0;$-Calculus Modulo  Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blot%2C+V">Valentin Blot</a> (DEDUCTEAM, LMF, ENS Paris Saclay), 
<a href="/search/cs?searchtype=author&query=Dowek%2C+G">Gilles Dowek</a> (DEDUCTEAM, LMF, ENS Paris Saclay), 
<a href="/search/cs?searchtype=author&query=Traversi%C3%A9%2C+T">Thomas Traversi&#xe9;</a> (DEDUCTEAM, LMF, ENS Paris Saclay, MICS), 
<a href="/search/cs?searchtype=author&query=Winterhalter%2C+T">Th&#xe9;o Winterhalter</a> (DEDUCTEAM, LMF, ENS Paris Saclay)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">The $\lambda$$\Pi$-calculus modulo theory is an extension of simply typed
$\lambda$-calculus with dependent types and user-defined rewrite rules. We show
that it is possible to replace the rewrite rules of a theory of the
$\lambda$$\Pi$-calculus modulo theory by equational axioms, when this theory
features the notions of proposition and proof, while maintaining the same
expressiveness. To do so, we introduce in the target theory a heterogeneous
equality, and we build a translation that replaces each use of the conversion
rule by the insertion of a transport. At the end, the theory with rewrite rules
is a conservative extension of the theory with axioms.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09025" title="Abstract">arXiv:2402.09025</a> [<a href="/pdf/2402.09025" title="Download PDF">pdf</a>, <a href="/format/2402.09025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLEB: Streamlining LLMs through Redundancy Verification and Elimination  of Transformer Blocks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiwon Song</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+K">Kyungseok Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taesu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyungjun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yulhwa Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jae-Joon Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have proven to be highly effective across
various natural language processing tasks. However, their large number of
parameters poses significant challenges for practical deployment. Pruning, a
technique aimed at reducing the size and complexity of LLMs, offers a potential
solution by removing redundant components from the network. Despite the promise
of pruning, existing methods often struggle to achieve substantial end-to-end
LLM inference speedup. In this paper, we introduce SLEB, a novel approach
designed to streamline LLMs by eliminating redundant transformer blocks. We
choose the transformer block as the fundamental unit for pruning, because LLMs
exhibit block-level redundancy with high similarity between the outputs of
neighboring blocks. This choice allows us to effectively enhance the processing
speed of LLMs. Our experimental results demonstrate that SLEB successfully
accelerates LLM inference without compromising the linguistic capabilities of
these models, making it a promising technique for optimizing the efficiency of
LLMs. The code is available at: https://github.com/leapingjagg-dev/SLEB
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09028" title="Abstract">arXiv:2402.09028</a> [<a href="/pdf/2402.09028" title="Download PDF">pdf</a>, <a href="/format/2402.09028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Stress, Burnout, and Behavioral Patterns in Medical  Residents Using Large-scale Longitudinal Wearable Recordings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tiantian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+S">Shrikanth Narayanan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Medical residency training is often associated with physically intense and
emotionally demanding tasks, requiring them to engage in extended working hours
providing complex clinical care. Residents are hence susceptible to negative
psychological effects, including stress and anxiety, that can lead to decreased
well-being, affecting them achieving desired training outcomes. Understanding
the daily behavioral patterns of residents can guide the researchers to
identify the source of stress in residency training, offering unique
opportunities to improve residency programs. In this study, we investigate the
workplace behavioral patterns of 43 medical residents across different stages
of their training, using longitudinal wearable recordings collected over a
3-week rotation. Specifically, we explore their ambulatory patterns, the
computer access, and the interactions with mentors of residents. Our analysis
reveals that residents showed distinct working behaviors in walking movement
patterns and computer usage compared to different years in the program.
Moreover, we identify that interaction patterns with mentoring doctors indicate
stress, burnout, and job satisfaction.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09030" title="Abstract">arXiv:2402.09030</a> [<a href="/pdf/2402.09030" title="Download PDF">pdf</a>, <a href="/format/2402.09030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Awareness in robotics: An early perspective from the viewpoint of the  EIC Pathfinder Challenge &quot;Awareness Inside&#x27;&#x27;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Della+Santina%2C+C">Cosimo Della Santina</a>, 
<a href="/search/cs?searchtype=author&query=Corbato%2C+C+H">Carlos Hernandez Corbato</a>, 
<a href="/search/cs?searchtype=author&query=Sisman%2C+B">Burak Sisman</a>, 
<a href="/search/cs?searchtype=author&query=Leiva%2C+L+A">Luis A. Leiva</a>, 
<a href="/search/cs?searchtype=author&query=Arapakis%2C+I">Ioannis Arapakis</a>, 
<a href="/search/cs?searchtype=author&query=Vakalellis%2C+M">Michalis Vakalellis</a>, 
<a href="/search/cs?searchtype=author&query=Vanderdonckt%2C+J">Jean Vanderdonckt</a>, 
<a href="/search/cs?searchtype=author&query=D%27Haro%2C+L+F">Luis Fernando D&#x27;Haro</a>, 
<a href="/search/cs?searchtype=author&query=Manzi%2C+G">Guido Manzi</a>, 
<a href="/search/cs?searchtype=author&query=Becchio%2C+C">Cristina Becchio</a>, 
<a href="/search/cs?searchtype=author&query=Elamrani%2C+A">A&#xef;da Elamrani</a>, 
<a href="/search/cs?searchtype=author&query=Alirezaei%2C+M">Mohsen Alirezaei</a>, 
<a href="/search/cs?searchtype=author&query=Castellano%2C+G">Ginevra Castellano</a>, 
<a href="/search/cs?searchtype=author&query=Dimarogonas%2C+D+V">Dimos V. Dimarogonas</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Arabinda Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Haesaert%2C+S">Sofie Haesaert</a>, 
<a href="/search/cs?searchtype=author&query=Soudjani%2C+S">Sadegh Soudjani</a>, 
<a href="/search/cs?searchtype=author&query=Stroeve%2C+S">Sybert Stroeve</a>, 
<a href="/search/cs?searchtype=author&query=Verschure%2C+P">Paul Verschure</a>, 
<a href="/search/cs?searchtype=author&query=Bacciu%2C+D">Davide Bacciu</a>, 
<a href="/search/cs?searchtype=author&query=Deroy%2C+O">Ophelia Deroy</a>, 
<a href="/search/cs?searchtype=author&query=Bahrami%2C+B">Bahador Bahrami</a>, 
<a href="/search/cs?searchtype=author&query=Gallicchio%2C+C">Claudio Gallicchio</a>, 
<a href="/search/cs?searchtype=author&query=Hauert%2C+S">Sabine Hauert</a>, 
<a href="/search/cs?searchtype=author&query=Sanz%2C+R">Ricardo Sanz</a>, 
<a href="/search/cs?searchtype=author&query=Lanillos%2C+P">Pablo Lanillos</a>, 
<a href="/search/cs?searchtype=author&query=Iacca%2C+G">Giovanni Iacca</a>, 
<a href="/search/cs?searchtype=author&query=Sigg%2C+S">Stephan Sigg</a>, 
<a href="/search/cs?searchtype=author&query=Gasulla%2C+M">Manel Gasulla</a>, 
<a href="/search/cs?searchtype=author&query=Steels%2C+L">Luc Steels</a>, 
<a href="/search/cs?searchtype=author&query=Sierra%2C+C">Carles Sierra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Consciousness has been historically a heavily debated topic in engineering,
science, and philosophy. On the contrary, awareness had less success in raising
the interest of scholars in the past. However, things are changing as more and
more researchers are getting interested in answering questions concerning what
awareness is and how it can be artificially generated. The landscape is rapidly
evolving, with multiple voices and interpretations of the concept being
conceived and techniques being developed. The goal of this paper is to
summarize and discuss the ones among these voices connected with projects
funded by the EIC Pathfinder Challenge called ``Awareness Inside'', a
nonrecurring call for proposals within Horizon Europe designed specifically for
fostering research on natural and synthetic awareness. In this perspective, we
dedicate special attention to challenges and promises of applying synthetic
awareness in robotics, as the development of mature techniques in this new
field is expected to have a special impact on generating more capable and
trustworthy embodied systems.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09034" title="Abstract">arXiv:2402.09034</a> [<a href="/pdf/2402.09034" title="Download PDF">pdf</a>, <a href="/ps/2402.09034" title="Download PostScript">ps</a>, <a href="/format/2402.09034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST)  Activation Under Data Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+B">Barathi Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Jeyaraj%2C+R">Rathinaraja Jeyaraj</a>, 
<a href="/search/cs?searchtype=author&query=Ugli%2C+R+A+A">Rakhmonov Akhrorjon Akhmadjon Ugli</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeonghong Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages,9 figures, Submitted to IJCAI 2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Activation functions enable neural networks to learn complex representations
by introducing non-linearities. While feedforward models commonly use rectified
linear units, sequential models like recurrent neural networks, long short-term
memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH
activation functions. However, these classical activation functions often
struggle to model sparse patterns when trained on small sequential datasets to
effectively capture temporal dependencies. To address this limitation, we
propose squared Sigmoid TanH (SST) activation specifically tailored to enhance
the learning capability of sequential models under data constraints. SST
applies mathematical squaring to amplify differences between strong and weak
activations as signals propagate over time, facilitating improved gradient flow
and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse
applications, such as sign language recognition, regression, and time-series
classification tasks, where the dataset is limited. Our experiments demonstrate
that SST models consistently outperform RNN-based models with baseline
activations, exhibiting improved test accuracy.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09036" title="Abstract">arXiv:2402.09036</a> [<a href="/pdf/2402.09036" title="Download PDF">pdf</a>, <a href="/format/2402.09036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Text-to-image Model Assist Multi-modal Learning for Visual  Recognition with Visual Modality Missing?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tiantian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Daniel Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+D">Digbalay Bose</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+S">Shrikanth Narayanan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-modal learning has emerged as an increasingly promising avenue in
vision recognition, driving innovations across diverse domains ranging from
media and education to healthcare and transportation. Despite its success, the
robustness of multi-modal learning for visual recognition is often challenged
by the unavailability of a subset of modalities, especially the visual
modality. Conventional approaches to mitigate missing modalities in multi-modal
learning rely heavily on algorithms and modality fusion schemes. In contrast,
this paper explores the use of text-to-image models to assist multi-modal
learning. Specifically, we propose a simple but effective multi-modal learning
framework GTI-MM to enhance the data efficiency and model robustness against
missing visual modality by imputing the missing data with generative
transformers. Using multiple multi-modal datasets with visual recognition
tasks, we present a comprehensive analysis of diverse conditions involving
missing visual modality in data, including model training. Our findings reveal
that synthetic images benefit training data efficiency with visual data missing
in training and improve model robustness with visual data missing involving
training and testing. Moreover, we demonstrate GTI-MM is effective with lower
generation quantity and simple prompt techniques.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09042" title="Abstract">arXiv:2402.09042</a> [<a href="/pdf/2402.09042" title="Download PDF">pdf</a>, <a href="/format/2402.09042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Application Admission Control and Network Slicing in Virtual  Sensor Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delgado%2C+C">Carmen Delgado</a>, 
<a href="/search/cs?searchtype=author&query=Canales%2C+M">Mar&#xed;a Canales</a>, 
<a href="/search/cs?searchtype=author&query=Ort%C3%ADn%2C+J">Jorge Ort&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%A1llego%2C+J+R">Jos&#xe9; Ram&#xf3;n G&#xe1;llego</a>, 
<a href="/search/cs?searchtype=author&query=Redondi%2C+A">Alessandro Redondi</a>, 
<a href="/search/cs?searchtype=author&query=Bousnina%2C+S">Sonda Bousnina</a>, 
<a href="/search/cs?searchtype=author&query=Cesana%2C+M">Matteo Cesana</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Internet of Things Journal 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">We focus on the problem of managing a shared physical wireless sensor network
where a single network infrastructure provider leases the physical resources of
the networks to application providers to run/deploy specific
applications/services. In this scenario, we solve jointly the problems of
Application Admission Control (AAC), that is, whether to admit the
application/service to the physical network, and wireless Sensor Network
Slicing (SNS), that is, to allocate the required physical resources to the
admitted applications in a transparent and effective way. We propose a
mathematical programming framework to model the joint AAC-SNS problem which is
then leveraged to design effective solution algorithms. The proposed framework
is thoroughly evaluated on realistic wireless sensor networks infrastructures.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09043" title="Abstract">arXiv:2402.09043</a> [<a href="/pdf/2402.09043" title="Download PDF">pdf</a>, <a href="/ps/2402.09043" title="Download PostScript">ps</a>, <a href="/format/2402.09043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Under manipulations, are some AI models harder to audit?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Godinot%2C+A">Augustin Godinot</a>, 
<a href="/search/cs?searchtype=author&query=Tredan%2C+G">Gilles Tredan</a>, 
<a href="/search/cs?searchtype=author&query=Merrer%2C+E+L">Erwan Le Merrer</a>, 
<a href="/search/cs?searchtype=author&query=Penzo%2C+C">Camilla Penzo</a>, 
<a href="/search/cs?searchtype=author&query=Ta%C3%AFani%2C+F">Francois Ta&#xef;ani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the IEEE Conference on Secure and Trustworthy Machine Learning, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Auditors need robust methods to assess the compliance of web platforms with
the law. However, since they hardly ever have access to the algorithm,
implementation, or training data used by a platform, the problem is harder than
a simple metric estimation. Within the recent framework of manipulation-proof
auditing, we study in this paper the feasibility of robust audits in realistic
settings, in which models exhibit large capacities. We first prove a
constraining result: if a web platform uses models that may fit any data, no
audit strategy -- whether active or not -- can outperform random sampling when
estimating properties such as demographic parity. To better understand the
conditions under which state-of-the-art auditing techniques may remain
competitive, we then relate the manipulability of audits to the capacity of the
targeted models, using the Rademacher complexity. We empirically validate these
results on popular models of increasing capacities, thus confirming
experimentally that large-capacity models, which are commonly used in practice,
are particularly hard to audit robustly. These results refine the limits of the
auditing problem, and open up enticing questions on the connection between
model capacity and the ability of platforms to manipulate audit attempts.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09046" title="Abstract">arXiv:2402.09046</a> [<a href="/pdf/2402.09046" title="Download PDF">pdf</a>, <a href="/format/2402.09046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inference of Abstraction for a Unified Account of Reasoning and Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kido%2C+H">Hiroyuki Kido</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2402.08646">arXiv:2402.08646</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Inspired by Bayesian approaches to brain function in neuroscience, we give a
simple theory of probabilistic inference for a unified account of reasoning and
learning. We simply model how data cause symbolic knowledge in terms of its
satisfiability in formal logic. The underlying idea is that reasoning is a
process of deriving symbolic knowledge from data via abstraction, i.e.,
selective ignorance. The logical consequence relation is discussed for its
proof-based theoretical correctness. The MNIST dataset is discussed for its
experiment-based empirical correctness.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09047" title="Abstract">arXiv:2402.09047</a> [<a href="/pdf/2402.09047" title="Download PDF">pdf</a>, <a href="/format/2402.09047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yiming He</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jia Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaokai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+N">Na Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+T">Tuo Leng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The application of contemporary artificial intelligence techniques to address
geometric problems and automated deductive proof has always been a grand
challenge to the interdiscipline field of mathematics and artificial
Intelligence. This is the fourth article in a series of our works, in our
previous work, we established of a geometric formalized system known as
FormalGeo. Moreover we annotated approximately 7000 geometric problems, forming
the FormalGeo7k dataset. Despite the FGPS (Formal Geometry Problem Solver) can
achieve interpretable algebraic equation solving and human-like deductive
reasoning, it often experiences timeouts due to the complexity of the search
strategy. In this paper, we introduced FGeo-TP (Theorem Predictor), which
utilizes the language model to predict theorem sequences for solving geometry
problems. We compared the effectiveness of various Transformer architectures,
such as BART or T5, in theorem prediction, implementing pruning in the search
process of FGPS, thereby improving its performance in solving geometry
problems. Our results demonstrate a significant increase in the problem-solving
rate of the language model-enhanced FGeo-TP on the FormalGeo7k dataset, rising
from 39.7% to 80.86%. Furthermore, FGeo-TP exhibits notable reductions in
solving time and search steps across problems of varying difficulty levels.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09050" title="Abstract">arXiv:2402.09050</a> [<a href="/pdf/2402.09050" title="Download PDF">pdf</a>, <a href="/format/2402.09050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Training Induces Information Bottleneck through Layer-Role  Differentiation: A Comparative Analysis with Layer-wise Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sakamoto%2C+K">Keitaro Sakamoto</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+I">Issei Sato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">End-to-end (E2E) training, optimizing the entire model through error
backpropagation, fundamentally supports the advancements of deep learning.
Despite its high performance, E2E training faces the problems of memory
consumption, parallel computing, and discrepancy with the functionalities of
the actual brain. Various alternative methods have been proposed to overcome
these difficulties; however, no one can yet match the performance of E2E
training, thereby falling short in practicality. Furthermore, there is no deep
understanding regarding differences in the trained model properties beyond the
performance gap. In this paper, we reconsider why E2E training demonstrates a
superior performance through a comparison with layer-wise training, a non-E2E
method that locally sets errors. On the basis of the observation that E2E
training has an advantage in propagating input information, we analyze the
information plane dynamics of intermediate representations based on the
Hilbert-Schmidt independence criterion (HSIC). The results of our normalized
HSIC value analysis reveal the E2E training ability to exhibit different
information dynamics across layers, in addition to efficient information
propagation. Furthermore, we show that this layer-role differentiation leads to
the final representation following the information bottleneck principle. It
suggests the need to consider the cooperative interactions between layers, not
just the final layer when analyzing the information bottleneck of deep
learning.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09051" title="Abstract">arXiv:2402.09051</a> [<a href="/pdf/2402.09051" title="Download PDF">pdf</a>, <a href="/format/2402.09051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jia Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaokai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yiming He</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+N">Na Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+T">Tuo Leng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The human-like automatic deductive reasoning has always been one of the most
challenging open problems in the interdiscipline of mathematics and artificial
intelligence. This paper is the third in a series of our works. We built a
neural-symbolic system, called FGeoDRL, to automatically perform human-like
geometric deductive reasoning. The neural part is an AI agent based on
reinforcement learning, capable of autonomously learning problem-solving
methods from the feedback of a formalized environment, without the need for
human supervision. It leverages a pre-trained natural language model to
establish a policy network for theorem selection and employ Monte Carlo Tree
Search for heuristic exploration. The symbolic part is a reinforcement learning
environment based on geometry formalization theory and
FormalGeo\cite{FormalGeo}, which models GPS as a Markov Decision
Process\cite{MDP}. In this formal symbolic system, the known conditions and
objectives of the problem form the state space, while the set of theorems forms
the action space. Leveraging FGeoDRL, we have achieved readable and verifiable
automated solutions to geometric problems. Experiments conducted on the
formalgeo7k dataset have achieved a problem-solving success rate of 86.40\%.
The project is available at https://github.com/PersonNoName/FGeoDRL.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09052" title="Abstract">arXiv:2402.09052</a> [<a href="/pdf/2402.09052" title="Download PDF">pdf</a>, <a href="/format/2402.09052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L3GO: Language Agents with Chain-of-3D-Thoughts for Generating  Unconventional Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamada%2C+Y">Yutaro Yamada</a>, 
<a href="/search/cs?searchtype=author&query=Chandu%2C+K">Khyathi Chandu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Yildirim%2C+I">Ilker Yildirim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Diffusion-based image generation models such as DALL-E 3 and Stable
Diffusion-XL demonstrate remarkable capabilities in generating images with
realistic and unique compositions. Yet, these models are not robust in
precisely reasoning about physical and spatial configurations of objects,
especially when instructed with unconventional, thereby out-of-distribution
descriptions, such as "a chair with five legs". In this paper, we propose a
language agent with chain-of-3D-thoughts (L3GO), an inference-time approach
that can reason about part-based 3D mesh generation of unconventional objects
that current data-driven diffusion models struggle with. More concretely, we
use large language models as agents to compose a desired object via
trial-and-error within the 3D simulation environment. To facilitate our
investigation, we develop a new benchmark, Unconventionally Feasible Objects
(UFO), as well as SimpleBlenv, a wrapper environment built on top of Blender
where language agents can build and compose atomic building blocks via API
calls. Human and automatic GPT-4V evaluations show that our approach surpasses
the standard GPT-4 and other language agents (e.g., ReAct and Reflexion) for 3D
mesh generation on ShapeNet. Moreover, when tested on our UFO benchmark, our
approach outperforms other state-of-the-art text-to-2D image and text-to-3D
models based on human evaluation.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09055" title="Abstract">arXiv:2402.09055</a> [<a href="/pdf/2402.09055" title="Download PDF">pdf</a>, <a href="/format/2402.09055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comment-aided Video-Language Alignment via Contrastive Pre-training for  Short-form Video Humor Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tongfei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qingying Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shoushan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guodong Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The growing importance of multi-modal humor detection within affective
computing correlates with the expanding influence of short-form video sharing
on social media platforms. In this paper, we propose a novel two-branch
hierarchical model for short-form video humor detection (SVHD), named
Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal
contrastive pre-training. Notably, our CVLA not only operates on raw signals
across various modal channels but also yields an appropriate multi-modal
representation by aligning the video and language components within a
consistent semantic space. The experimental results on two humor detection
datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically
outperforms state-of-the-art and several competitive baseline approaches. Our
dataset, code and model release at https://github.com/yliu-cs/CVLA.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09056" title="Abstract">arXiv:2402.09056</a> [<a href="/pdf/2402.09056" title="Download PDF">pdf</a>, <a href="/format/2402.09056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Epistemic Uncertainty Faithfully Represented by Evidential Deep  Learning Methods?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=J%C3%BCrgens%2C+M">Mira J&#xfc;rgens</a>, 
<a href="/search/cs?searchtype=author&query=Meinert%2C+N">Nis Meinert</a>, 
<a href="/search/cs?searchtype=author&query=Bengs%2C+V">Viktor Bengs</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCllermeier%2C+E">Eyke H&#xfc;llermeier</a>, 
<a href="/search/cs?searchtype=author&query=Waegeman%2C+W">Willem Waegeman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Trustworthy ML systems should not only return accurate predictions, but also
a reliable representation of their uncertainty. Bayesian methods are commonly
used to quantify both aleatoric and epistemic uncertainty, but alternative
approaches, such as evidential deep learning methods, have become popular in
recent years. The latter group of methods in essence extends empirical risk
minimization (ERM) for predicting second-order probability distributions over
outcomes, from which measures of epistemic (and aleatoric) uncertainty can be
extracted. This paper presents novel theoretical insights of evidential deep
learning, highlighting the difficulties in optimizing second-order loss
functions and interpreting the resulting epistemic uncertainty measures. With a
systematic setup that covers a wide range of approaches for classification,
regression and counts, it provides novel insights into issues of
identifiability and convergence in second-order loss minimization, and the
relative (rather than absolute) nature of epistemic uncertainty measures.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09059" title="Abstract">arXiv:2402.09059</a> [<a href="/pdf/2402.09059" title="Download PDF">pdf</a>, <a href="/format/2402.09059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I can&#x27;t see it but I can Fine-tune it: On Encrypted Fine-tuning of  Transformers using Fully Homomorphic Encryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panzade%2C+P">Prajwal Panzade</a>, 
<a href="/search/cs?searchtype=author&query=Takabi%2C+D">Daniel Takabi</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhipeng Cai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the presentation at PPAI @The 38th Annual AAAI Conference on Artificial Intelligence 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In today's machine learning landscape, fine-tuning pretrained transformer
models has emerged as an essential technique, particularly in scenarios where
access to task-aligned training data is limited. However, challenges surface
when data sharing encounters obstacles due to stringent privacy regulations or
user apprehension regarding personal information disclosure. Earlier works
based on secure multiparty computation (SMC) and fully homomorphic encryption
(FHE) for privacy-preserving machine learning (PPML) focused more on
privacy-preserving inference than privacy-preserving training. In response, we
introduce BlindTuner, a privacy-preserving fine-tuning system that enables
transformer training exclusively on homomorphically encrypted data for image
classification. Our extensive experimentation validates BlindTuner's
effectiveness by demonstrating comparable accuracy to non-encrypted models.
Notably, our findings highlight a substantial speed enhancement of 1.5x to 600x
over previous work in this domain.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09062" title="Abstract">arXiv:2402.09062</a> [<a href="/pdf/2402.09062" title="Download PDF">pdf</a>, <a href="/format/2402.09062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blind Deep-Learning-Based Image Watermarking Robust Against Geometric  Transformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mareen%2C+H">Hannes Mareen</a>, 
<a href="/search/cs?searchtype=author&query=Antchougov%2C+L">Lucas Antchougov</a>, 
<a href="/search/cs?searchtype=author&query=Van+Wallendael%2C+G">Glenn Van Wallendael</a>, 
<a href="/search/cs?searchtype=author&query=Lambert%2C+P">Peter Lambert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and presented at IEEE International Conference on Consumer Electronics (ICCE) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Digital watermarking enables protection against copyright infringement of
images. Although existing methods embed watermarks imperceptibly and
demonstrate robustness against attacks, they typically lack resilience against
geometric transformations. Therefore, this paper proposes a new watermarking
method that is robust against geometric attacks. The proposed method is based
on the existing HiDDeN architecture that uses deep learning for watermark
encoding and decoding. We add new noise layers to this architecture, namely for
a differentiable JPEG estimation, rotation, rescaling, translation, shearing
and mirroring. We demonstrate that our method outperforms the state of the art
when it comes to geometric robustness. In conclusion, the proposed method can
be used to protect images when viewed on consumers' devices.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09063" title="Abstract">arXiv:2402.09063</a> [<a href="/pdf/2402.09063" title="Download PDF">pdf</a>, <a href="/format/2402.09063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft Prompt Threats: Attacking Safety Alignment and Unlearning in  Open-Source LLMs through the Embedding Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schwinn%2C+L">Leo Schwinn</a>, 
<a href="/search/cs?searchtype=author&query=Dobre%2C+D">David Dobre</a>, 
<a href="/search/cs?searchtype=author&query=Xhonneux%2C+S">Sophie Xhonneux</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>, 
<a href="/search/cs?searchtype=author&query=Gunnemann%2C+S">Stephan Gunnemann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Trigger Warning: the appendix contains LLM-generated text with violence and harassment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Current research in adversarial robustness of LLMs focuses on discrete input
manipulations in the natural language space, which can be directly transferred
to closed-source models. However, this approach neglects the steady progression
of open-source models. As open-source models advance in capability, ensuring
their safety also becomes increasingly imperative. Yet, attacks tailored to
open-source LLMs that exploit full model access remain largely unexplored. We
address this research gap and propose the embedding space attack, which
directly attacks the continuous embedding representation of input tokens. We
find that embedding space attacks circumvent model alignments and trigger
harmful behaviors more efficiently than discrete attacks or model fine-tuning.
Furthermore, we present a novel threat model in the context of unlearning and
show that embedding space attacks can extract supposedly deleted information
from unlearned LLMs across multiple datasets and models. Our findings highlight
embedding space attacks as an important threat model in open-source LLMs.
Trigger Warning: the appendix contains LLM-generated text with violence and
harassment.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09066" title="Abstract">arXiv:2402.09066</a> [<a href="/pdf/2402.09066" title="Download PDF">pdf</a>, <a href="/format/2402.09066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solid Waste Detection in Remote Sensing Images: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fraternali%2C+P">Piero Fraternali</a>, 
<a href="/search/cs?searchtype=author&query=Morandini%2C+L">Luca Morandini</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez%2C+S+L+H">Sergio Luis Herrera Gonz&#xe1;lez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The detection and characterization of illegal solid waste disposal sites are
essential for environmental protection, particularly for mitigating pollution
and health hazards. Improperly managed landfills contaminate soil and
groundwater via rainwater infiltration, posing threats to both animals and
humans. Traditional landfill identification approaches, such as on-site
inspections, are time-consuming and expensive. Remote sensing is a
cost-effective solution for the identification and monitoring of solid waste
disposal sites that enables broad coverage and repeated acquisitions over time.
Earth Observation (EO) satellites, equipped with an array of sensors and
imaging capabilities, have been providing high-resolution data for several
decades. Researchers proposed specialized techniques that leverage remote
sensing imagery to perform a range of tasks such as waste site detection,
dumping site monitoring, and assessment of suitable locations for new
landfills. This review aims to provide a detailed illustration of the most
relevant proposals for the detection and monitoring of solid waste sites by
describing and comparing the approaches, the implemented techniques, and the
employed data. Furthermore, since the data sources are of the utmost importance
for developing an effective solid waste detection model, a comprehensive
overview of the satellites and publicly available data sets is presented.
Finally, this paper identifies the open issues in the state-of-the-art and
discusses the relevant research directions for reducing the costs and improving
the effectiveness of novel solid waste detection methods.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09071" title="Abstract">arXiv:2402.09071</a> [<a href="/pdf/2402.09071" title="Download PDF">pdf</a>, <a href="/format/2402.09071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Affine transformation estimation improves visual self-supervised  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Torpey%2C+D">David Torpey</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+R">Richard Klein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under consideration at Pattern Recognition Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The standard approach to modern self-supervised learning is to generate
random views through data augmentations and minimise a loss computed from the
representations of these views. This inherently encourages invariance to the
transformations that comprise the data augmentation function. In this work, we
show that adding a module to constrain the representations to be predictive of
an affine transformation improves the performance and efficiency of the
learning process. The module is agnostic to the base self-supervised model and
manifests in the form of an additional loss term that encourages an aggregation
of the encoder representations to be predictive of an affine transformation
applied to the input images. We perform experiments in various modern
self-supervised models and see a performance improvement in all cases. Further,
we perform an ablation study on the components of the affine transformation to
understand which of them is affecting performance the most, as well as on key
architectural design decisions.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09072" title="Abstract">arXiv:2402.09072</a> [<a href="/pdf/2402.09072" title="Download PDF">pdf</a>, <a href="/ps/2402.09072" title="Download PostScript">ps</a>, <a href="/format/2402.09072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trace Ratio Based Manifold Learning with Tensor Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bouallala%2C+M">Mohammed Bouallala</a>, 
<a href="/search/math?searchtype=author&query=Dufrenois%2C+F">Franck Dufrenois</a>, 
<a href="/search/math?searchtype=author&query=jbilou%2C+k">khalide jbilou</a>, 
<a href="/search/math?searchtype=author&query=Ratnani%2C+A">Ahmed Ratnani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we propose an extension of trace ratio based Manifold learning
methods to deal with multidimensional data sets. Based on recent progress on
the tensor-tensor product, we present a generalization of the trace ratio
criterion by using the properties of the t-product. This will conduct us to
introduce some new concepts such as Laplacian tensor and we will study formally
the trace ratio problem by discuting the conditions for the exitence of
solutions and optimality. Next, we will present a tensor Newton QR
decomposition algorithm for solving the trace ratio problem. Manifold learning
methods such as Laplacian eigenmaps, linear discriminant analysis and locally
linear embedding will be formulated in a tensor representation and optimized by
the proposed algorithm. Lastly, we will evaluate the performance of the
different studied dimension reduction methods on several synthetic and real
world data sets.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09075" title="Abstract">arXiv:2402.09075</a> [<a href="/pdf/2402.09075" title="Download PDF">pdf</a>, <a href="/format/2402.09075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steady-State Error Compensation for Reinforcement Learning with  Quadratic Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Liyao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+Z">Zishun Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yuan Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The selection of a reward function in Reinforcement Learning (RL) has
garnered significant attention because of its impact on system performance.
Issues of steady-state error often manifest when quadratic reward functions are
employed. Although existing solutions using absolute-value-type reward
functions partially address this problem, they tend to induce substantial
fluctuations in specific system states, leading to abrupt changes. In response
to this challenge, this study proposes an approach that introduces an integral
term. By integrating this term into quadratic-type reward functions, the RL
algorithm is adeptly tuned, augmenting the system's consideration of long-term
rewards and, consequently, alleviating concerns related to steady-state errors.
Through experiments and performance evaluations on the Adaptive Cruise Control
(ACC) model and lane change models, we validate that the proposed method not
only effectively diminishes steady-state errors but also results in smoother
variations in system states.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09077" title="Abstract">arXiv:2402.09077</a> [<a href="/pdf/2402.09077" title="Download PDF">pdf</a>, <a href="/format/2402.09077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning  of Gough-Stewart Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huizhi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenxia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we propose a graph neural network, DisGNet, for learning the
graph distance matrix to address the forward kinematics problem of the
Gough-Stewart platform. DisGNet employs the k-FWL algorithm for
message-passing, providing high expressiveness with a small parameter count,
making it suitable for practical deployment. Additionally, we introduce the
GPU-friendly Newton-Raphson method, an efficient parallelized optimization
method executed on the GPU to refine DisGNet's output poses, achieving
ultra-high-precision pose. This novel two-stage approach delivers ultra-high
precision output while meeting real-time requirements. Our results indicate
that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at
79.8\% and 98.2\%, respectively. As executed on a GPU, our two-stage method can
ensure the requirement for real-time computation. Codes are released at
https://github.com/FLAMEZZ5201/DisGNet.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09078" title="Abstract">arXiv:2402.09078</a> [<a href="/pdf/2402.09078" title="Download PDF">pdf</a>, <a href="/format/2402.09078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic  Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinigaglia%2C+A">Alberto Sinigaglia</a>, 
<a href="/search/cs?searchtype=author&query=Turcato%2C+N">Niccol&#xf2; Turcato</a>, 
<a href="/search/cs?searchtype=author&query=Libera%2C+A+D">Alberto Dalla Libera</a>, 
<a href="/search/cs?searchtype=author&query=Carli%2C+R">Ruggero Carli</a>, 
<a href="/search/cs?searchtype=author&query=Susto%2C+G+A">Gian Antonio Susto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces innovative methods in Reinforcement Learning (RL),
focusing on addressing and exploiting estimation biases in Actor-Critic methods
for continuous control tasks, using Deep Double Q-Learning. We propose two
novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3)
and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3).
ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a
balance between computational efficiency and performance, while BE-TD3 is
designed to dynamically select the most advantageous estimation bias during
training. Our extensive experiments across various continuous control tasks
demonstrate the effectiveness of our approaches. We show that these algorithms
can either match or surpass existing methods like TD3, particularly in
environments where estimation biases significantly impact learning. The results
underline the importance of bias exploitation in improving policy learning in
RL.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09082" title="Abstract">arXiv:2402.09082</a> [<a href="/pdf/2402.09082" title="Download PDF">pdf</a>, <a href="/ps/2402.09082" title="Download PostScript">ps</a>, <a href="/format/2402.09082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puccetti%2C+T">Tommaso Puccetti</a>, 
<a href="/search/cs?searchtype=author&query=Ceccarelli%2C+A">Andrea Ceccarelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The ever-evolving landscape of attacks, coupled with the growing complexity
of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error
detectors (ED) a difficult task: they must accurately detect attacks, and they
should promptly perform detections. Although improving and comparing the
detection capability is the focus of most research works, the timeliness of the
detection is less considered and often insufficiently evaluated or discussed.
In this paper, we argue the relevance of measuring the temporal latency of
attacks and errors, and we propose an evaluation approach for detectors to
ensure a pragmatic trade-off between correct and in-time detection. Briefly,
the approach relates the false positive rate with the temporal latency of
attacks and errors, and this ultimately leads to guidelines for configuring a
detector. We apply our approach by evaluating different ED and ID solutions in
two industrial cases: i) an embedded railway on-board system that optimizes
public mobility, and ii) an edge device for the Industrial Internet of Things.
Our results show that considering latency in addition to traditional metrics
like the false positive rate, precision, and coverage gives an additional
fundamental perspective on the actual performance of the detector and should be
considered when assessing and configuring anomaly detectors.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09084" title="Abstract">arXiv:2402.09084</a> [<a href="/pdf/2402.09084" title="Download PDF">pdf</a>, <a href="/format/2402.09084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sobolev Training for Operator Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+N">Namkyeong Cho</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+J">Junseung Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+H+J">Hyung Ju Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study investigates the impact of Sobolev Training on operator learning
frameworks for improving model performance. Our research reveals that
integrating derivative information into the loss function enhances the training
process, and we propose a novel framework to approximate derivatives on
irregular meshes in operator learning. Our findings are supported by both
experimental evidence and theoretical analysis. This demonstrates the
effectiveness of Sobolev Training in approximating the solution operators
between infinite-dimensional spaces.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09085" title="Abstract">arXiv:2402.09085</a> [<a href="/pdf/2402.09085" title="Download PDF">pdf</a>, <a href="/format/2402.09085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polynomial Semantics of Tractable Probabilistic Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Broadrick%2C+O">Oliver Broadrick</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Honghua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Broeck%2C+G">Guy Van den Broeck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Probabilistic circuits compute multilinear polynomials that represent
probability distributions. They are tractable models that support efficient
marginal inference. However, various polynomial semantics have been considered
in the literature (e.g., network polynomials, likelihood polynomials,
generating functions, Fourier transforms, and characteristic polynomials). The
relationships between these polynomial encodings of distributions is largely
unknown. In this paper, we prove that for binary distributions, each of these
probabilistic circuit models is equivalent in the sense that any circuit for
one of them can be transformed into a circuit for any of the others with only a
polynomial increase in size. They are therefore all tractable for marginal
inference on the same class of distributions. Finally, we explore the natural
extension of one such polynomial semantics, called probabilistic generating
circuits, to categorical random variables, and establish that marginal
inference becomes #P-hard.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09087" title="Abstract">arXiv:2402.09087</a> [<a href="/pdf/2402.09087" title="Download PDF">pdf</a>, <a href="/format/2402.09087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Vienna Architecture Description Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Himmelbauer%2C+S">Simon Himmelbauer</a>, 
<a href="/search/cs?searchtype=author&query=Hochrainer%2C+C">Christoph Hochrainer</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+B">Benedikt Huber</a>, 
<a href="/search/cs?searchtype=author&query=Mischkulnig%2C+N">Niklas Mischkulnig</a>, 
<a href="/search/cs?searchtype=author&query=Paulweber%2C+P">Philipp Paulweber</a>, 
<a href="/search/cs?searchtype=author&query=Schwarzinger%2C+T">Tobias Schwarzinger</a>, 
<a href="/search/cs?searchtype=author&query=Krall%2C+A">Andreas Krall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">The Vienna Architecture Description Language (VADL) is a powerful processor
description language (PDL) that enables the concise formal specification of
processor architectures. By utilizing a single VADL processor specification,
the VADL system exhibits the capability to automatically generate a range of
artifacts necessary for rapid design space exploration. These include
assemblers, compilers, linkers, functional instruction set simulators,
cycle-accurate instruction set simulators, synthesizable specifications in a
hardware description language, as well as test cases and documentation. One
distinctive feature of VADL lies in its separation of the instruction set
architecture (ISA) specification and the microarchitecture (MiA) specification.
This segregation allows users the flexibility to combine various ISAs with
different MiAs, providing a versatile approach to processor design. In contrast
to existing PDLs, VADL's MiA specification operates at a higher level of
abstraction, enhancing the clarity and simplicity of the design process.
Notably, with a single ISA specification, VADL streamlines compiler generation
and maintenance by eliminating the need for intricate compiler-specific
knowledge. This article introduces VADL, describes the generator techniques in
detail and demonstrates the power of the language and the performance of the
generators in an empirical evaluation. The evaluation shows the expressiveness
and conciseness of VADL and the efficiency of the generated artifacts.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09091" title="Abstract">arXiv:2402.09091</a> [<a href="/pdf/2402.09091" title="Download PDF">pdf</a>, <a href="/format/2402.09091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit  Clues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Z">Zhiyuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">With the development of LLMs, the security threats of LLMs are getting more
and more attention. Numerous jailbreak attacks have been proposed to assess the
security defense of LLMs. Current jailbreak attacks primarily utilize scenario
camouflage techniques. However their explicitly mention of malicious intent
will be easily recognized and defended by LLMs. In this paper, we propose an
indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense
strategy and obtain malicious response by implicitly providing LLMs with some
clues about the original malicious query. In addition, inspired by the wisdom
of ''When unable to attack, defend'' from Sun Tzu's Art of War, we adopt a
defensive stance to gather clues about the original malicious query through
LLMs. Extensive experimental results show that Puzzler achieves a query success
rate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than
baselines. Furthermore, when tested against the state-of-the-art jailbreak
detection approaches, Puzzler proves to be more effective at evading detection
compared to baselines.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09092" title="Abstract">arXiv:2402.09092</a> [<a href="/pdf/2402.09092" title="Download PDF">pdf</a>, <a href="/format/2402.09092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three Decades of Activations: A Comprehensive Survey of 400 Activation  Functions for Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kunc%2C+V">Vladim&#xed;r Kunc</a>, 
<a href="/search/cs?searchtype=author&query=Kl%C3%A9ma%2C+J">Ji&#x159;&#xed; Kl&#xe9;ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Neural networks have proven to be a highly effective tool for solving complex
problems in many areas of life. Recently, their importance and practical
usability have further been reinforced with the advent of deep learning. One of
the important conditions for the success of neural networks is the choice of an
appropriate activation function introducing non-linearity into the model. Many
types of these functions have been proposed in the literature in the past, but
there is no single comprehensive source containing their exhaustive overview.
The absence of this overview, even in our experience, leads to redundancy and
the unintentional rediscovery of already existing activation functions. To
bridge this gap, our paper presents an extensive survey involving 400
activation functions, which is several times larger in scale than previous
surveys. Our comprehensive compilation also references these surveys; however,
its main goal is to provide the most comprehensive overview and systematization
of previously published activation functions with links to their original
sources. The secondary aim is to update the current understanding of this
family of functions.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09094" title="Abstract">arXiv:2402.09094</a> [<a href="/pdf/2402.09094" title="Download PDF">pdf</a>, <a href="/format/2402.09094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unity is Strength: Enhancing Precision in Reentrancy Vulnerability  Detection of Smart Contract Analysis Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zexu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiachi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+P">Peilin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weizhe Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Reentrancy is one of the most notorious vulnerabilities in smart contracts,
resulting in significant digital asset losses. However, many previous works
indicate that current Reentrancy detection tools suffer from high false
positive rates. Even worse, recent years have witnessed the emergence of new
Reentrancy attack patterns fueled by intricate and diverse vulnerability
exploit mechanisms. Unfortunately, current tools face a significant limitation
in their capacity to adapt and detect these evolving Reentrancy patterns.
Consequently, ensuring precise and highly extensible Reentrancy vulnerability
detection remains critical challenges for existing tools. To address this
issue, we propose a tool named ReEP, designed to reduce the false positives for
Reentrancy vulnerability detection. Additionally, ReEP can integrate multiple
tools, expanding its capacity for vulnerability detection. It evaluates results
from existing tools to verify vulnerability likelihood and reduce false
positives. ReEP also offers excellent extensibility, enabling the integration
of different detection tools to enhance precision and cover different
vulnerability attack patterns. We perform ReEP to eight existing
state-of-the-art Reentrancy detection tools. The average precision of these
eight tools increased from the original 0.5% to 73% without sacrificing recall.
Furthermore, ReEP exhibits robust extensibility. By integrating multiple tools,
the precision further improved to a maximum of 83.6%. These results demonstrate
that ReEP effectively unites the strengths of existing works, enhances the
precision of Reentrancy vulnerability detection tools.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09095" title="Abstract">arXiv:2402.09095</a> [<a href="/pdf/2402.09095" title="Download PDF">pdf</a>, <a href="/format/2402.09095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedSiKD: Clients Similarity and Knowledge Distillation: Addressing  Non-i.i.d. and Constraints in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alsenani%2C+Y">Yousef Alsenani</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+R">Rahul Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+K+R">Khaled R. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+A+U">Atta Ur Rahman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 10 figures Under Review - IEEE Transactions on Information Forensics &amp; Security
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In recent years, federated learning (FL) has emerged as a promising technique
for training machine learning models in a decentralized manner while also
preserving data privacy. The non-independent and identically distributed
(non-i.i.d.) nature of client data, coupled with constraints on client or edge
devices, presents significant challenges in FL. Furthermore, learning across a
high number of communication rounds can be risky and potentially unsafe for
model exploitation. Traditional FL approaches may suffer from these challenges.
Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD)
within a similarity-based federated learning framework. As clients join the
system, they securely share relevant statistics about their data distribution,
promoting intra-cluster homogeneity. This enhances optimization efficiency and
accelerates the learning process, effectively transferring knowledge between
teacher and student models and addressing device constraints. FedSiKD
outperforms state-of-the-art algorithms by achieving higher accuracy, exceeding
by 25\% and 18\% for highly skewed data at $\alpha = {0.1,0.5}$ on the HAR and
MNIST datasets, respectively. Its faster convergence is illustrated by a 17\%
and 20\% increase in accuracy within the first five rounds on the HAR and MNIST
datasets, respectively, highlighting its early-stage learning proficiency. Code
is publicly available and hosted on GitHub (https://github.com/SimuEnv/FedSiKD)
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09097" title="Abstract">arXiv:2402.09097</a> [<a href="/pdf/2402.09097" title="Download PDF">pdf</a>, <a href="/format/2402.09097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Digital Twin prototype for traffic sign recognition of a  learning-enabled autonomous vehicle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AbdElSalam%2C+M">Mohamed AbdElSalam</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+L">Loai Ali</a>, 
<a href="/search/cs?searchtype=author&query=Bensalem%2C+S">Saddek Bensalem</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Weicheng He</a>, 
<a href="/search/cs?searchtype=author&query=Katsaros%2C+P">Panagiotis Katsaros</a>, 
<a href="/search/cs?searchtype=author&query=Kekatos%2C+N">Nikolaos Kekatos</a>, 
<a href="/search/cs?searchtype=author&query=Peled%2C+D">Doron Peled</a>, 
<a href="/search/cs?searchtype=author&query=Temperekidis%2C+A">Anastasios Temperekidis</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Changshun Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we present a novel digital twin prototype for a
learning-enabled self-driving vehicle. The primary objective of this digital
twin is to perform traffic sign recognition and lane keeping. The digital twin
architecture relies on co-simulation and uses the Functional Mock-up Interface
and SystemC Transaction Level Modeling standards. The digital twin consists of
four clients, i) a vehicle model that is designed in Amesim tool, ii) an
environment model developed in Prescan, iii) a lane-keeping controller designed
in Robot Operating System, and iv) a perception and speed control module
developed in the formal modeling language of BIP (Behavior, Interaction,
Priority). These clients interface with the digital twin platform,
PAVE360-Veloce System Interconnect (PAVE360-VSI). PAVE360-VSI acts as the
co-simulation orchestrator and is responsible for synchronization,
interconnection, and data exchange through a server. The server establishes
connections among the different clients and also ensures adherence to the
Ethernet protocol. We conclude with illustrative digital twin simulations and
recommendations for future work.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09099" title="Abstract">arXiv:2402.09099</a> [<a href="/pdf/2402.09099" title="Download PDF">pdf</a>, <a href="/format/2402.09099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Neuron Interactions and Emergence in LLMs: From the  Multifractal Analysis Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiongye Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chenyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ping%2C+H">Heng Ping</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+D">Defu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaxing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yizhuo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Bogdan%2C+P">Paul Bogdan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Prior studies on the emergence in large models have primarily focused on how
the functional capabilities of large language models (LLMs) scale with model
size. Our research, however, transcends this traditional paradigm, aiming to
deepen our understanding of the emergence within LLMs by placing a special
emphasis not just on the model size but more significantly on the complex
behavior of neuron interactions during the training process. By introducing the
concepts of "self-organization" and "multifractal analysis," we explore how
neuron interactions dynamically evolve during training, leading to "emergence,"
mirroring the phenomenon in natural systems where simple micro-level
interactions give rise to complex macro-level behaviors. To quantitatively
analyze the continuously evolving interactions among neurons in large models
during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA).
Utilizing NeuroMFA, we conduct a comprehensive examination of the emergent
behavior in LLMs through the lens of both model size and training process,
paving new avenues for research into the emergence in large models.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09100" title="Abstract">arXiv:2402.09100</a> [<a href="/pdf/2402.09100" title="Download PDF">pdf</a>, <a href="/format/2402.09100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lohesara%2C+F+G">Fatemeh Ghorbani Lohesara</a>, 
<a href="/search/cs?searchtype=author&query=Egiazarian%2C+K">Karen Egiazarian</a>, 
<a href="/search/cs?searchtype=author&query=Knorr%2C+S">Sebastian Knorr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Electronic Imaging 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Facial video inpainting plays a crucial role in a wide range of applications,
including but not limited to the removal of obstructions in video conferencing
and telemedicine, enhancement of facial expression analysis, privacy
protection, integration of graphical overlays, and virtual makeup. This domain
presents serious challenges due to the intricate nature of facial features and
the inherent human familiarity with faces, heightening the need for accurate
and persuasive completions. In addressing challenges specifically related to
occlusion removal in this context, our focus is on the progressive task of
generating complete images from facial data covered by masks, ensuring both
spatial and temporal coherence. Our study introduces a network designed for
expression-based video inpainting, employing generative adversarial networks
(GANs) to handle static and moving occlusions across all frames. By utilizing
facial landmarks and an occlusion-free reference image, our model maintains the
user's identity consistently across frames. We further enhance emotional
preservation through a customized facial expression recognition (FER) loss
function, ensuring detailed inpainted outputs. Our proposed framework exhibits
proficiency in eliminating occlusions from facial videos in an adaptive form,
whether appearing static or dynamic on the frames, while providing realistic
and coherent results.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09105" title="Abstract">arXiv:2402.09105</a> [<a href="/pdf/2402.09105" title="Download PDF">pdf</a>, <a href="/format/2402.09105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scheduling for On-Board Federated Learning with Satellite Clusters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razmi%2C+N">Nasrin Razmi</a>, 
<a href="/search/cs?searchtype=author&query=Matthiesen%2C+B">Bho Matthiesen</a>, 
<a href="/search/cs?searchtype=author&query=Dekorsy%2C+A">Armin Dekorsy</a>, 
<a href="/search/cs?searchtype=author&query=Popovski%2C+P">Petar Popovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE GLOBECOM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Mega-constellations of small satellites have evolved into a source of massive
amount of valuable data. To manage this data efficiently, on-board federated
learning (FL) enables satellites to train a machine learning (ML) model
collaboratively without having to share the raw data. This paper introduces a
scheme for scheduling on-board FL for constellations connected with intra-orbit
inter-satellite links. The proposed scheme utilizes the predictable visibility
pattern between satellites and ground station (GS), both at the individual
satellite level and cumulatively within the entire orbit, to mitigate
intermittent connectivity and best use of available time. To this end, two
distinct schedulers are employed: one for coordinating the FL procedures among
orbits, and the other for controlling those within each orbit. These two
schedulers cooperatively determine the appropriate time to perform global
updates in GS and then allocate suitable duration to satellites within each
orbit for local training, proportional to usable time until next global update.
This scheme leads to improved test accuracy within a shorter time.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09107" title="Abstract">arXiv:2402.09107</a> [<a href="/pdf/2402.09107" title="Download PDF">pdf</a>, <a href="/format/2402.09107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Headset: Human emotion awareness under partial occlusions multimodal  dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lohesara%2C+F+G">Fatemeh Ghorbani Lohesara</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+D+R">Davi Rabbouni Freitas</a>, 
<a href="/search/cs?searchtype=author&query=Guillemot%2C+C">Christine Guillemot</a>, 
<a href="/search/cs?searchtype=author&query=Eguiazarian%2C+K">Karen Eguiazarian</a>, 
<a href="/search/cs?searchtype=author&query=Knorr%2C+S">Sebastian Knorr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ISMAR 2023 and published in IEEE Transactions on Visualization and Computer Graphics Dataset: <a href="https://webpages.tuni.fi/headset">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The volumetric representation of human interactions is one of the fundamental
domains in the development of immersive media productions and telecommunication
applications. Particularly in the context of the rapid advancement of Extended
Reality (XR) applications, this volumetric data has proven to be an essential
technology for future XR elaboration. In this work, we present a new multimodal
database to help advance the development of immersive technologies. Our
proposed database provides ethically compliant and diverse volumetric data, in
particular 27 participants displaying posed facial expressions and subtle body
movements while speaking, plus 11 participants wearing head-mounted displays
(HMDs). The recording system consists of a volumetric capture (VoCap) studio,
including 31 synchronized modules with 62 RGB cameras and 31 depth cameras. In
addition to textured meshes, point clouds, and multi-view RGB-D data, we use
one Lytro Illum camera for providing light field (LF) data simultaneously.
Finally, we also provide an evaluation of our dataset employment with regard to
the tasks of facial expression classification, HMDs removal, and point cloud
reconstruction. The dataset can be helpful in the evaluation and performance
testing of various XR algorithms, including but not limited to facial
expression recognition and reconstruction, facial reenactment, and volumetric
video. HEADSET and its all associated raw data and license agreement will be
publicly available for research purposes.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09109" title="Abstract">arXiv:2402.09109</a> [<a href="/pdf/2402.09109" title="Download PDF">pdf</a>, <a href="/format/2402.09109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Spiking Attention: Accelerating Attention with Stochastic  Computing in Spiking Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zihang Song</a>, 
<a href="/search/cs?searchtype=author&query=Katti%2C+P">Prabodh Katti</a>, 
<a href="/search/cs?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+B">Bipin Rajendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Signal Processing (eess.SP)

</div>
<p class="mathjax">Spiking Neural Networks (SNNs) have been recently integrated into Transformer
architectures due to their potential to reduce computational demands and to
improve power efficiency. Yet, the implementation of the attention mechanism
using spiking signals on general-purpose computing platforms remains
inefficient. In this paper, we propose a novel framework leveraging stochastic
computing (SC) to effectively execute the dot-product attention for SNN-based
Transformers. We demonstrate that our approach can achieve high classification
accuracy ($83.53\%$) on CIFAR-10 within 10 time steps, which is comparable to
the performance of a baseline artificial neural network implementation
($83.66\%$). We estimate that the proposed SC approach can lead to over
$6.3\times$ reduction in computing energy and $1.7\times$ reduction in memory
access costs for a digital CMOS-based ASIC design. We experimentally validate
our stochastic attention block design through an FPGA implementation, which is
shown to achieve $48\times$ lower latency as compared to a GPU implementation,
while consuming $15\times$ less power.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09113" title="Abstract">arXiv:2402.09113</a> [<a href="/pdf/2402.09113" title="Download PDF">pdf</a>, <a href="/format/2402.09113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Exploration in Reinforcement Learning via Optimal Transport in  Policy Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nkhumise%2C+R+M">Reabetswe M. Nkhumise</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+D">Debabrota Basu</a>, 
<a href="/search/cs?searchtype=author&query=Prescott%2C+T+J">Tony J. Prescott</a>, 
<a href="/search/cs?searchtype=author&query=Gilra%2C+A">Aditya Gilra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Exploration is the key ingredient of reinforcement learning (RL) that
determines the speed and success of learning. Here, we quantify and compare the
amount of exploration and learning accomplished by a Reinforcement Learning
(RL) algorithm. Specifically, we propose a novel measure, named Exploration
Index, that quantifies the relative effort of knowledge transfer
(transferability) by an RL algorithm in comparison to supervised learning (SL)
that transforms the initial data distribution of RL to the corresponding final
data distribution. The comparison is established by formulating learning in RL
as a sequence of SL tasks, and using optimal transport based metrics to compare
the total path traversed by the RL and SL algorithms in the data distribution
space. We perform extensive empirical analysis on various environments and with
multiple algorithms to demonstrate that the exploration index yields insights
about the exploration behaviour of any RL algorithm, and also allows us to
compare the exploratory behaviours of different RL algorithms.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09115" title="Abstract">arXiv:2402.09115</a> [<a href="/pdf/2402.09115" title="Download PDF">pdf</a>, <a href="/format/2402.09115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Topology and Traffic Engineering for Reconfigurable  Datacenter Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Griner%2C+C">Chen Griner</a>, 
<a href="/search/cs?searchtype=author&query=Avin%2C+C">Chen Avin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The state-of-the-art topologies of datacenter networks are fixed, based on
electrical switching technology, and by now, we understand their throughput and
cost well. For the past years, researchers have been developing novel optical
switching technologies that enable the emergence of reconfigurable datacenter
networks (RDCNs) that support dynamic psychical topologies. The art of network
design of dynamic topologies, i.e., 'Topology Engineering,' is still in its
infancy. Different designs offer distinct advantages, such as faster switch
reconfiguration times or demand-aware topologies, and to date, it is yet
unclear what design maximizes the throughput.
<br />This paper aims to improve our analytical understanding and formally studies
the throughput of reconfigurable networks by presenting a general and unifying
model for dynamic networks and their topology and traffic engineering. We use
our model to study demand-oblivious and demand-aware systems and prove new
upper bounds for the throughput of a system as a function of its topology and
traffic schedules.
<br />Next, we offer a novel system design that combines both demand-oblivious and
demand-aware schedules, and we prove its throughput supremacy under a large
family of demand matrices. We evaluate our design numerically for sparse and
dense traffic and show that our approach can outperform other designs by up to
25% using common network parameters.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09117" title="Abstract">arXiv:2402.09117</a> [<a href="/pdf/2402.09117" title="Download PDF">pdf</a>, <a href="/format/2402.09117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic identification over channels with finite output: a  dimensional perspective on superlinear rates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colomer%2C+P">Pau Colomer</a>, 
<a href="/search/cs?searchtype=author&query=Deppe%2C+C">Christian Deppe</a>, 
<a href="/search/cs?searchtype=author&query=Boche%2C+H">Holger Boche</a>, 
<a href="/search/cs?searchtype=author&query=Winter%2C+A">Andreas Winter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Following initial work by JaJa and Ahlswede/Cai, and inspired by a recent
renewed surge in interest in deterministic identification via noisy channels,
we consider the problem in its generality for memoryless channels with finite
output, but arbitrary input alphabets.
<br />Such a channel is essentially given by (the closure of) the subset of its
output distributions in the probability simplex. Our main findings are that the
maximum number of messages thus identifiable scales super-exponentially as
$2^{R\,n\log n}$ with the block length $n$, and that the optimal rate $R$ is
upper and lower bounded in terms of the covering (aka Minkowski, or Kolmogorov,
or entropy) dimension $d$ of the output set: $\frac14 d \leq R \leq d$. Leading
up to the general case, we treat the important special case of the so-called
Bernoulli channel with input alphabet $[0;1]$ and binary output, which has
$d=1$, to gain intuition. Along the way, we show a certain Hypothesis Testing
Lemma (generalising an earlier insight of Ahlswede regarding the intersection
of typical sets) that implies that for the construction of a deterministic
identification code, it is sufficient to ensure pairwise reliable
distinguishability of the output distributions.
<br />These results are then shown to generalise directly to classical-quantum
channels with finite-dimensional output quantum system (but arbitrary input
alphabet), and in particular to quantum channels on finite-dimensional quantum
systems under the constraint that the identification code can only use tensor
product inputs.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09121" title="Abstract">arXiv:2402.09121</a> [<a href="/pdf/2402.09121" title="Download PDF">pdf</a>, <a href="/ps/2402.09121" title="Download PostScript">ps</a>, <a href="/format/2402.09121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inform: From Compartmental Models to Stochastic Bounded Counter Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leys%2C+T">Tim Leys</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+G+A">Guillermo A. Perez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">Compartmental models are used in epidemiology to capture the evolution of
infectious diseases such as COVID-19 in a population by assigning members of it
to compartments with labels such as susceptible, infected, and recovered. In a
stochastic compartmental model the flow of individuals between compartments is
determined probabilistically. We establish that certain stochastic compartment
models can be encoded as probabilistic counter machines where the
configurations are bounded. Based on the latter, we obtain simple descriptions
of the models in the PRISM language. This enables the analysis of such
compartmental models via probabilistic model checkers. Finally, we report on
experimental results where we analyze results from a Belgian COVID-19 model
using a probabilistic model checkers.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09124" title="Abstract">arXiv:2402.09124</a> [<a href="/pdf/2402.09124" title="Download PDF">pdf</a>, <a href="/format/2402.09124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Densest Subgraphs with Edge-Color Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oettershagen%2C+L">Lutz Oettershagen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Honglian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gionis%2C+A">Aristides Gionis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">We consider a variant of the densest subgraph problem in networks with single
or multiple edge attributes. For example, in a social network, the edge
attributes may describe the type of relationship between users, such as
friends, family, or acquaintances, or different types of communication. For
conceptual simplicity, we view the attributes as edge colors. The new problem
we address is to find a diverse densest subgraph that fulfills given
requirements on the numbers of edges of specific colors. When searching for a
dense social network community, our problem will enforce the requirement that
the community is diverse according to criteria specified by the edge
attributes. We show that the decision versions for finding exactly, at most,
and at least $\textbf{h}$ colored edges densest subgraph, where $\textbf{h}$ is
a vector of color requirements, are NP-complete, for already two colors. For
the problem of finding a densest subgraph with at least $\textbf{h}$ colored
edges, we provide a linear-time constant-factor approximation algorithm when
the input graph is sparse. On the way, we introduce the related at least $h$
(non-colored) edges densest subgraph problem, show its hardness, and also
provide a linear-time constant-factor approximation. In our experiments, we
demonstrate the efficacy and efficiency of our new algorithms.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09126" title="Abstract">arXiv:2402.09126</a> [<a href="/pdf/2402.09126" title="Download PDF">pdf</a>, <a href="/format/2402.09126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MPIrigen: MPI Code Generation through Domain-Specific Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneider%2C+N">Nadav Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Hasabnis%2C+N">Niranjan Hasabnis</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+V+A">Vy A. Vo</a>, 
<a href="/search/cs?searchtype=author&query=Kadosh%2C+T">Tal Kadosh</a>, 
<a href="/search/cs?searchtype=author&query=Krien%2C+N">Neva Krien</a>, 
<a href="/search/cs?searchtype=author&query=Capot%C4%83%2C+M">Mihai Capot&#x103;</a>, 
<a href="/search/cs?searchtype=author&query=Wasay%2C+A">Abdul Wasay</a>, 
<a href="/search/cs?searchtype=author&query=Tamir%2C+G">Guy Tamir</a>, 
<a href="/search/cs?searchtype=author&query=Willke%2C+T">Ted Willke</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N">Nesreen Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Pinter%2C+Y">Yuval Pinter</a>, 
<a href="/search/cs?searchtype=author&query=Mattson%2C+T">Timothy Mattson</a>, 
<a href="/search/cs?searchtype=author&query=Oren%2C+G">Gal Oren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">The imperative need to scale computation across numerous nodes highlights the
significance of efficient parallel computing, particularly in the realm of
Message Passing Interface (MPI) integration. The challenging parallel
programming task of generating MPI-based parallel programs has remained
unexplored. This study first investigates the performance of state-of-the-art
language models in generating MPI-based parallel programs. Findings reveal that
widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual
code models) exhibit notable performance degradation, when generating MPI-based
programs compared to general-purpose programs. In contrast, domain-specific
models such as MonoCoder, which are pretrained on MPI-related programming
languages of C and C++, outperform larger models. Subsequently, we introduce a
dedicated downstream task of MPI-based program generation by fine-tuning
MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose
an innovative preprocessing for completion only after observing the whole code,
thus enabling better completion with a wider context. Comparative analysis
against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation
method, demonstrates that MPIrigen excels in generating accurate MPI functions
up to 0.8 accuracy in location and function predictions, and with more than 0.9
accuracy for argument predictions. The success of this tailored solution
underscores the importance of domain-specific fine-tuning in optimizing
language models for parallel computing code generation, paving the way for a
new generation of automatic parallelization tools. The sources of this work are
available at our GitHub MPIrigen repository:
https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09129" title="Abstract">arXiv:2402.09129</a> [<a href="/pdf/2402.09129" title="Download PDF">pdf</a>, <a href="/format/2402.09129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Automated Market Makers: Differentiable Economics and Strong  Duality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Curry%2C+M+J">Michael J. Curry</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhou Fan</a>, 
<a href="/search/cs?searchtype=author&query=Parkes%2C+D+C">David C. Parkes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Theoretical Economics (econ.TH); Trading and Market Microstructure (q-fin.TR)

</div>
<p class="mathjax">The role of a market maker is to simultaneously offer to buy and sell
quantities of goods, often a financial asset such as a share, at specified
prices. An automated market maker (AMM) is a mechanism that offers to trade
according to some predetermined schedule; the best choice of this schedule
depends on the market maker's goals. The literature on the design of AMMs has
mainly focused on prediction markets with the goal of information elicitation.
More recent work motivated by DeFi has focused instead on the goal of profit
maximization, but considering only a single type of good (traded with a
numeraire), including under adverse selection (Milionis et al. 2022). Optimal
market making in the presence of multiple goods, including the possibility of
complex bundling behavior, is not well understood. In this paper, we show that
finding an optimal market maker is dual to an optimal transport problem, with
specific geometric constraints on the transport plan in the dual. We show that
optimal mechanisms for multiple goods and under adverse selection can take
advantage of bundling, both improved prices for bundled purchases and sales as
well as sometimes accepting payment "in kind." We present conjectures of
optimal mechanisms in additional settings which show further complex behavior.
From a methodological perspective, we make essential use of the tools of
differentiable economics to generate conjectures of optimal mechanisms, and
give a proof-of-concept for the use of such tools in guiding theoretical
investigations.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09130" title="Abstract">arXiv:2402.09130</a> [<a href="/pdf/2402.09130" title="Download PDF">pdf</a>, <a href="/ps/2402.09130" title="Download PostScript">ps</a>, <a href="/format/2402.09130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recommendation Algorithm Based on Recommendation Sessions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malinowski%2C+M">Micha&#x142; Malinowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2402.08275">arXiv:2402.08275</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 36th International Business Information
  Management Association (IBIMA) (2020)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The enormous development of the Internet, both in the geographical scale and
in the area of using its possibilities in everyday life, determines the
creation and collection of huge amounts of data. Due to the scale, it is not
possible to analyse them using traditional methods, therefore it makes a
necessary to use modern methods and techniques. Such methods are provided,
among others, by the area of recommendations. The aim of this study is to
present a new algorithm in the area of recommendation systems, the algorithm
based on data from various sets of information, both static (categories of
objects, features of objects) and dynamic (user behaviour).
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09132" title="Abstract">arXiv:2402.09132</a> [<a href="/pdf/2402.09132" title="Download PDF">pdf</a>, <a href="/format/2402.09132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Adversarial Capabilities of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Struppek%2C+L">Lukas Struppek</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+M+H">Minh Hieu Le</a>, 
<a href="/search/cs?searchtype=author&query=Hintersdorf%2C+D">Dominik Hintersdorf</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The proliferation of large language models (LLMs) has sparked widespread and
general interest due to their strong language generation capabilities, offering
great potential for both industry and research. While previous research delved
into the security and privacy issues of LLMs, the extent to which these models
can exhibit adversarial behavior remains largely unexplored. Addressing this
gap, we investigate whether common publicly available LLMs have inherent
capabilities to perturb text samples to fool safety measures, so-called
adversarial examples resp.~attacks. More specifically, we investigate whether
LLMs are inherently able to craft adversarial examples out of benign samples to
fool existing safe rails. Our experiments, which focus on hate speech
detection, reveal that LLMs succeed in finding adversarial perturbations,
effectively undermining hate speech detection systems. Our findings carry
significant implications for (semi-)autonomous systems relying on LLMs,
highlighting potential challenges in their interaction with existing systems
and safety measures.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09134" title="Abstract">arXiv:2402.09134</a> [<a href="/pdf/2402.09134" title="Download PDF">pdf</a>, <a href="/format/2402.09134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing the Norwegian regulated power market anno 2016 to 2022
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Austnes%2C+P+F">P&#xe5;l Forr Austnes</a>, 
<a href="/search/eess?searchtype=author&query=Riemer-S%C3%B8rensen%2C+S">Signe Riemer-S&#xf8;rensen</a>, 
<a href="/search/eess?searchtype=author&query=Bordvik%2C+D+A">David Andreas Bordvik</a>, 
<a href="/search/eess?searchtype=author&query=Andresen%2C+C+A">Christian Andre Andresen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Energy Strategy Reviews Volume 52, March 2024, 101331
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The balancing market for power is designed to account for the difference
between predicted supply/demand of electricity and the realised supply/demand.
However, increased electrification of society changes the consumption patterns,
and increased production from renewable sources leads to larger un-predicted
fluctuations in production, both effects potentially leading to increased
balancing. We analyse public market data for the balancing market (manual
Frequency Restoration Reserve) for Norway from 2016 to 2022 to investigate and
document these effects. The data is newer than for similar analyses and the
eight years of data is more than double the time span previously covered. The
main findings are: a) The balancing volumes are dominated by hours of zero
regulation but for non-zero hours, the balancing volumes are increasing during
the eight-year period. b) The balancing prices are primarily correlated with
day-ahead prices and secondary with balancing volumes. The latter correlation
is found to be increasingly non-linear with time. c) The balancing volumes and
the price difference between balancing price and day-ahead price are strongly
correlated with the previous hour. d) The increasing share of wind power has
not impacted the frequency of balancing, which has remained stable during the 8
years studied. However, the volumes and share of balancing power compared to
overall production have increased, suggesting that the hours which are
inherently difficult to predict remain the same. e) Market data alone cannot
predict balancing volumes. If attempting, the auto-correlation becomes the main
source of information.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09136" title="Abstract">arXiv:2402.09136</a> [<a href="/pdf/2402.09136" title="Download PDF">pdf</a>, <a href="/format/2402.09136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DolphCoder: Echo-Locating Code Large Language Models with Diverse and  Multi-Objective Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yejie Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Weihao Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+M">Muxi Diao</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+Y">Yutao Mou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengdi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xunliang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Code Large Language Models (Code LLMs) have demonstrated outstanding
performance in code-related tasks. Several instruction tuning approaches have
been proposed to boost the code generation performance of pre-trained Code
LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with
self-evaluating for code generation. It learns diverse instruction targets and
combines a code evaluation objective to enhance its code generation ability.
Our model achieves superior performance on the HumanEval and MBPP benchmarks,
demonstrating new insights for future code instruction tuning work. Our key
findings are: (1) Augmenting more diverse responses with distinct reasoning
paths increases the code capability of LLMs. (2) Improving one's ability to
evaluate the correctness of code solutions also enhances their ability to
create it.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09138" title="Abstract">arXiv:2402.09138</a> [<a href="/pdf/2402.09138" title="Download PDF">pdf</a>, <a href="/format/2402.09138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Graded Linear Logic and Differential Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Breuvart%2C+F">Flavien Breuvart</a> (1), 
<a href="/search/cs?searchtype=author&query=Kerjean%2C+M">Marie Kerjean</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Mirwasser%2C+S">Simon Mirwasser</a> (1) ((1) USPN, (2) CNRS)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Logical Methods in Computer Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Linear Logic refines Intuitionnistic Logic by taking into account the
resources used during the proof and program computation. In the past decades,
it has been extended to various frameworks. The most famous are indexed linear
logics which can describe the resource management or the complexity analysis of
a program. From an other perspective, Differential Linear Logic is an extension
which allows the linearization of proofs. In this article, we merge these two
directions by first defining a differential version of Graded linear logic:
this is made by indexing exponential connectives with a monoid of differential
operators. We prove that it is equivalent to a graded version of previously
defined extension of finitary differential linear logic. We give a denotational
model of our logic, based on distribution theory and linear partial
differential operators with constant coefficients.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09139" title="Abstract">arXiv:2402.09139</a> [<a href="/pdf/2402.09139" title="Download PDF">pdf</a>, <a href="/format/2402.09139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monotonicity of the cops and robber game for bounded depth treewidth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adler%2C+I">Isolde Adler</a>, 
<a href="/search/cs?searchtype=author&query=Fluck%2C+E">Eva Fluck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">We study a variation of the cops and robber game characterising treewidth,
where in each play at most q cops can be placed in order to catch the robber,
where q is a parameter of the game. We prove that if k cops have a winning
strategy in this game, then k cops have a monotone winning strategy. As a
corollary we obtain a new characterisation of bounded depth treewidth, and we
give a positive answer to an open question by Fluck, Seppelt and Spitzer
(2024), thus showing that graph classes of bounded depth treewidth are
homomorphism distinguishing closed. Our proof of monotonicity substantially
reorganises a winning strategy by first transforming it into a
pre-decomposition, which is inspired by decompositions of matroids, and then
applying an intricate breadth-first "cleaning up" procedure along the
pre-decomposition (which may temporarily lose the property of representing a
strategy), in order to achieve monotonicity while controlling the number of cop
placements simultaneously across all branches of the decomposition via a vertex
exchange argument. We believe this can be useful in future research.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09141" title="Abstract">arXiv:2402.09141</a> [<a href="/pdf/2402.09141" title="Download PDF">pdf</a>, <a href="/format/2402.09141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing NLP Models with Strategic Text Augmentation: A Comprehensive  Study of Augmentation Methods and Curriculum Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kesgin%2C+H+T">Himmet Toprak Kesgin</a>, 
<a href="/search/cs?searchtype=author&query=Amasyali%2C+M+F">Mehmet Fatih Amasyali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study conducts a thorough evaluation of text augmentation techniques
across a variety of datasets and natural language processing (NLP) tasks to
address the lack of reliable, generalized evidence for these methods. It
examines the effectiveness of these techniques in augmenting training sets to
improve performance in tasks such as topic classification, sentiment analysis,
and offensive language detection. The research emphasizes not only the
augmentation methods, but also the strategic order in which real and augmented
instances are introduced during training. A major contribution is the
development and evaluation of Modified Cyclical Curriculum Learning (MCCL) for
augmented datasets, which represents a novel approach in the field. Results
show that specific augmentation methods, especially when integrated with MCCL,
significantly outperform traditional training approaches in NLP model
performance. These results underscore the need for careful selection of
augmentation techniques and sequencing strategies to optimize the balance
between speed and quality improvement in various NLP tasks. The study concludes
that the use of augmentation methods, especially in conjunction with MCCL,
leads to improved results in various classification tasks, providing a
foundation for future advances in text augmentation strategies in NLP.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09142" title="Abstract">arXiv:2402.09142</a> [<a href="/pdf/2402.09142" title="Download PDF">pdf</a>, <a href="/format/2402.09142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Representations Align: Universality in Representation Learning  Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Rossem%2C+L">Loek van Rossem</a>, 
<a href="/search/cs?searchtype=author&query=Saxe%2C+A+M">Andrew M. Saxe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Deep neural networks come in many sizes and architectures. The choice of
architecture, in conjunction with the dataset and learning algorithm, is
commonly understood to affect the learned neural representations. Yet, recent
results have shown that different architectures learn representations with
striking qualitative similarities. Here we derive an effective theory of
representation learning under the assumption that the encoding map from input
to hidden representation and the decoding map from representation to output are
arbitrary smooth functions. This theory schematizes representation learning
dynamics in the regime of complex, large architectures, where hidden
representations are not strongly constrained by the parametrization. We show
through experiments that the effective theory describes aspects of
representation learning dynamics across a range of deep networks with different
activation functions and architectures, and exhibits phenomena similar to the
"rich" and "lazy" regime. While many network behaviors depend quantitatively on
architecture, our findings point to certain behaviors that are widely conserved
once models are sufficiently flexible.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09146" title="Abstract">arXiv:2402.09146</a> [<a href="/pdf/2402.09146" title="Download PDF">pdf</a>, <a href="/format/2402.09146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kashif%2C+M">Muhammad Kashif</a>, 
<a href="/search/cs?searchtype=author&query=Shafique%2C+M">Muhammad Shafique</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09147" title="Abstract">arXiv:2402.09147</a> [<a href="/pdf/2402.09147" title="Download PDF">pdf</a>, <a href="/format/2402.09147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Into the Unknown: Self-Learning Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferdinan%2C+T">Teddy Ferdinan</a>, 
<a href="/search/cs?searchtype=author&query=Koco%C5%84%2C+J">Jan Koco&#x144;</a>, 
<a href="/search/cs?searchtype=author&query=Kazienko%2C+P">Przemys&#x142;aw Kazienko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 13 figures, to be submitted to ACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We address the main problem of self-learning LLM: the question of what to
learn. We propose a self-learning LLM framework that enables an LLM to
independently learn previously unknown knowledge through self-assessment of
their own hallucinations. Using the hallucination score, we introduce a new
concept of Points in The Unknown (PiUs), along with one extrinsic and three
intrinsic methods for automatic PiUs identification. It facilitates the
creation of a self-learning loop that focuses exclusively on the knowledge gap
in Points in The Unknown, resulting in a reduced hallucination score. We also
developed evaluation metrics for gauging an LLM's self-learning capability. Our
experiments revealed that 7B-Mistral models that have been finetuned or aligned
are capable of self-learning considerably well. Our self-learning concept
allows more efficient LLM updates and opens new perspectives for knowledge
exchange. It may also increase public trust in AI.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09148" title="Abstract">arXiv:2402.09148</a> [<a href="/pdf/2402.09148" title="Download PDF">pdf</a>, <a href="/format/2402.09148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiasEye: A Bias-Aware Real-time Interactive Material Screening System  for Impartial Candidate Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haoran Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zihao Pan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Q">Qiushi Han</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Zhenhui Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Quan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IUI' 24, March 18-21, 2024, Greenville, SC, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In the process of evaluating competencies for job or student recruitment
through material screening, decision-makers can be influenced by inherent
cognitive biases, such as the screening order or anchoring information, leading
to inconsistent outcomes. To tackle this challenge, we conducted interviews
with seven experts to understand their challenges and needs for support in the
screening process. Building on their insights, we introduce BiasEye, a
bias-aware real-time interactive material screening visualization system.
BiasEye enhances awareness of cognitive biases by improving information
accessibility and transparency. It also aids users in identifying and
mitigating biases through a machine learning (ML) approach that models
individual screening preferences. Findings from a mixed-design user study with
20 participants demonstrate that, compared to a baseline system lacking our
bias-aware features, BiasEye increases participants' bias awareness and boosts
their confidence in making final decisions. At last, we discuss the potential
of ML and visualization in mitigating biases during human decision-making
tasks.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09150" title="Abstract">arXiv:2402.09150</a> [<a href="/pdf/2402.09150" title="Download PDF">pdf</a>, <a href="/ps/2402.09150" title="Download PostScript">ps</a>, <a href="/format/2402.09150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better Decremental and Fully Dynamic Sensitivity Oracles for Subgraph  Connectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yaowei Long</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunfan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study the \emph{sensitivity oracles problem for subgraph connectivity} in
the \emph{decremental} and \emph{fully dynamic} settings. In the fully dynamic
setting, we preprocess an $n$-vertices $m$-edges undirected graph $G$ with
$n_{\rm off}$ deactivated vertices initially and the others are activated. Then
we receive a single update $D\subseteq V(G)$ of size $|D| = d \leq d_{\star}$,
representing vertices whose states will be switched. Finally, we get a sequence
of queries, each of which asks the connectivity of two given vertices $u$ and
$v$ in the activated subgraph. The decremental setting is a special case when
there is no deactivated vertex initially, and it is also known as the
\emph{vertex-failure connectivity oracles} problem.
<br />We present a better deterministic vertex-failure connectivity oracle with
$\widehat{O}(d_{\star}m)$ preprocessing time, $\widetilde{O}(m)$ space,
$\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which improves the
update time of the previous almost-optimal oracle [Long-Saranurak, FOCS 2022]
from $\widehat{O}(d^{2})$ to $\widetilde{O}(d^{2})$.
<br />We also present a better deterministic fully dynamic sensitivity oracle for
subgraph connectivity with $\widehat{O}(\min\{m(n_{\rm off} +
d_{\star}),n^{\omega}\})$ preprocessing time, $\widetilde{O}(\min\{m(n_{\rm
off} + d_{\star}),n^{2}\})$ space, $\widetilde{O}(d^{2})$ update time and
$O(d)$ query time, which significantly improves the update time of the state of
the art [Hu-Kosinas-Polak, 2023] from $\widetilde{O}(d^{4})$ to
$\widetilde{O}(d^{2})$. Furthermore, our solution is even almost-optimal
assuming popular fine-grained complexity conjectures.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09151" title="Abstract">arXiv:2402.09151</a> [<a href="/pdf/2402.09151" title="Download PDF">pdf</a>, <a href="/format/2402.09151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for  Chinese Mental Health Text Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+W">Wei Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+H">Hongzhi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B+X">Bing Xiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+G">Guanghui Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the current environment, psychological issues are prevalent and
widespread, with social media serving as a key outlet for individuals to share
their feelings. This results in the generation of vast quantities of data
daily, where negative emotions have the potential to precipitate crisis
situations. There is a recognized need for models capable of efficient
analysis. While pre-trained language models have demonstrated their
effectiveness broadly, there's a noticeable gap in pre-trained models tailored
for specialized domains like psychology. To address this, we have collected a
huge dataset from Chinese social media platforms and enriched it with publicly
available datasets to create a comprehensive database encompassing 3.36 million
text entries. To enhance the model's applicability to psychological text
analysis, we integrated psychological lexicons into the pre-training masking
mechanism. Building on an existing Chinese language model, we performed
adaptive training to develop a model specialized for the psychological domain.
We assessed our model's effectiveness across four public benchmarks, where it
not only surpassed the performance of standard pre-trained models but also
showed a inclination for making psychologically relevant predictions. Due to
concerns regarding data privacy, the dataset will not be made publicly
available. However, we have made the pre-trained models and codes publicly
accessible to the community via:
https://github.com/zwzzzQAQ/Chinese-MentalBERT.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09152" title="Abstract">arXiv:2402.09152</a> [<a href="/pdf/2402.09152" title="Download PDF">pdf</a>, <a href="/format/2402.09152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Regret for Bandit Convex Optimization with Delayed Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yuanyu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+C">Chang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingli Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We investigate bandit convex optimization (BCO) with delayed feedback, where
only the loss value of the action is revealed under an arbitrary delay.
Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$
for this problem, where $d$ is the maximum delay, by simply feeding delayed
loss values to the classical bandit gradient descent (BGD) algorithm. In this
paper, we develop a novel algorithm to enhance the regret, which carefully
exploits the delayed bandit feedback via a blocking update mechanism. Our
analysis first reveals that the proposed algorithm can decouple the joint
effect of the delays and bandit feedback on the regret, and improve the regret
bound to $O(T^{3/4}+\sqrt{dT})$ for convex functions. Compared with the
previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the
non-delayed setting for a larger amount of delay, i.e., $d=O(\sqrt{T})$,
instead of $d=O(T^{1/4})$. Furthermore, we consider the case with strongly
convex functions, and prove that the proposed algorithm can enjoy a better
regret bound of $O(T^{2/3}\log^{1/3}T+d\log T)$. Finally, we show that in a
special case with unconstrained action sets, it can be simply extended to
achieve a regret bound of $O(\sqrt{T\log T}+d\log T)$ for strongly convex and
smooth functions.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09154" title="Abstract">arXiv:2402.09154</a> [<a href="/pdf/2402.09154" title="Download PDF">pdf</a>, <a href="/format/2402.09154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attacking Large Language Models with Projected Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geisler%2C+S">Simon Geisler</a>, 
<a href="/search/cs?searchtype=author&query=Wollschl%C3%A4ger%2C+T">Tom Wollschl&#xe4;ger</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+M+H+I">M. H. I. Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Gasteiger%2C+J">Johannes Gasteiger</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnnemann%2C+S">Stephan G&#xfc;nnemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Current LLM alignment methods are readily broken through specifically crafted
adversarial prompts. While crafting adversarial prompts using discrete
optimization is highly effective, such attacks typically use more than 100,000
LLM calls. This high computational cost makes them unsuitable for, e.g.,
quantitative analyses and adversarial training. To remedy this, we revisit
Projected Gradient Descent (PGD) on the continuously relaxed input prompt.
Although previous attempts with ordinary gradient-based attacks largely failed,
we show that carefully controlling the error introduced by the continuous
relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one
order of magnitude faster than state-of-the-art discrete optimization to
achieve the same devastating attack results.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09158" title="Abstract">arXiv:2402.09158</a> [<a href="/pdf/2402.09158" title="Download PDF">pdf</a>, <a href="/format/2402.09158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wireless Crowd Detection for Smart Overtourism Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+T+M">Tom&#xe1;s Mestre Santos</a>, 
<a href="/search/cs?searchtype=author&query=Marinheiro%2C+R+N">Rui Neto Marinheiro</a>, 
<a href="/search/cs?searchtype=author&query=Abreu%2C+F+B+e">Fernando Brito e Abreu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Overtourism occurs when the number of tourists exceeds the carrying capacity
of a destination, leading to negative impacts on the environment, culture, and
quality of life for residents. By monitoring overtourism, destination managers
can identify areas of concern and implement measures to mitigate the negative
impacts of tourism while promoting smarter tourism practices. This can help
ensure that tourism benefits both visitors and residents while preserving the
natural and cultural resources that make these destinations so appealing.
<br />This chapter describes a low-cost approach to monitoring overtourism based on
mobile devices' wireless activity. A flexible architecture was designed for a
smart tourism toolkit to be used by Small and Medium-sized Enterprises (SMEs)
in crowding management solutions, to build better tourism services, improve
efficiency and sustainability, and reduce the overwhelming feeling of pressure
in critical hotspots.
<br />The crowding sensors count the number of surrounding mobile devices, by
detecting trace elements of wireless technologies, mitigating the effect of MAC
address randomization. They run detection programs for several technologies,
and fingerprinting analysis results are only stored locally in an anonymized
database, without infringing privacy rights. After that edge computing, sensors
communicate the crowding information to a cloud server, by using a variety of
uplink techniques to mitigate local connectivity limitations, something that
has been often disregarded in alternative approaches.
<br />Field validation of sensors has been performed on Iscte's campus. Preliminary
results show that these sensors can be deployed in multiple scenarios and
provide a diversity of spatio-temporal crowding data that can scaffold tourism
overcrowding management strategies.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09161" title="Abstract">arXiv:2402.09161</a> [<a href="/pdf/2402.09161" title="Download PDF">pdf</a>, <a href="/ps/2402.09161" title="Download PostScript">ps</a>, <a href="/format/2402.09161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Role-Playing Simulation Games using ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stampfl%2C+R">Rita Stampfl</a>, 
<a href="/search/cs?searchtype=author&query=Ivki%C4%87%2C+I">Igor Ivki&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Geyer%2C+B">Barbara Geyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Link to online article: <a href="https://ercim-news.ercim.eu/en136/special/role-playing-simulation-games-using-chatgpt">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ERCIM News Special Theme: Large Language Models 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Since the COVID-19 pandemic, educational institutions have embarked on
digital transformation projects. The success of these projects depends on
integrating new technologies and understanding the needs of digitally literate
students. The "learning by doing" approach suggests that real success in
learning new skills is achieved when students can try out and practise these
skills. In this article, we demonstrate how Large Language Models (LLMs) can
enhance the quality of teaching by using ChatGPT in a role-playing simulation
game scenario to promote active learning. Moreover, we discuss how LLMs can
boost students' interest in learning by allowing them to practice real-life
scenarios using ChatGPT.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09164" title="Abstract">arXiv:2402.09164</a> [<a href="/pdf/2402.09164" title="Download PDF">pdf</a>, <a href="/format/2402.09164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less is More: Fewer Interpretable Region via Submodular Subset Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruoyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Siyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Image attribution algorithms aim to identify important regions that are
highly relevant to model decisions. Although existing attribution solutions can
effectively assign importance to target elements, they still face the following
challenges: 1) existing attribution methods generate inaccurate small regions
thus misleading the direction of correct attribution, and 2) the model cannot
produce good attribution results for samples with wrong predictions. To address
the above challenges, this paper re-models the above image attribution problem
as a submodular subset selection problem, aiming to enhance model
interpretability using fewer regions. To address the lack of attention to local
regions, we construct a novel submodular function to discover more accurate
fine-grained interpretation regions. To enhance the attribution effect for all
samples, we also impose four different constraints on the selection of
sub-regions, i.e., confidence, effectiveness, consistency, and collaboration
scores, to assess the importance of various subsets. Moreover, our theoretical
analysis substantiates that the proposed function is in fact submodular.
Extensive experiments show that the proposed method outperforms SOTA methods on
two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset
(CUB-200-2011). For correctly predicted samples, the proposed method improves
the Deletion and Insertion scores with an average of 4.9% and 2.5% gain
relative to HSIC-Attribution. For incorrectly predicted samples, our method
achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in
the average highest confidence and Insertion score respectively. The code is
released at https://github.com/RuoyuChen10/SMDL-Attribution.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09165" title="Abstract">arXiv:2402.09165</a> [<a href="/pdf/2402.09165" title="Download PDF">pdf</a>, <a href="/format/2402.09165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Invariance and Spuriousity for Graph Out-of-Distribution via  Probability of Necessity and Sufficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuexin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Ruichu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaitao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhifan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhengting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhifeng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijian Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Out-of-Distribution (OOD), requiring that models trained on biased data
generalize to the unseen test data, has a massive of real-world applications.
One of the most mainstream methods is to extract the invariant subgraph by
aligning the original and augmented data with the help of environment
augmentation. However, these solutions might lead to the loss or redundancy of
semantic subgraph and further result in suboptimal generalization. To address
this challenge, we propose a unified framework to exploit the Probability of
Necessity and Sufficiency to extract the Invariant Substructure (PNSIS). Beyond
that, this framework further leverages the spurious subgraph to boost the
generalization performance in an ensemble manner to enhance the robustness on
the noise data. Specificially, we first consider the data generation process
for graph data. Under mild conditions, we show that the invariant subgraph can
be extracted by minimizing an upper bound, which is built on the theoretical
advance of probability of necessity and sufficiency. To further bridge the
theory and algorithm, we devise the PNSIS model, which involves an invariant
subgraph extractor for invariant graph learning as well invariant and spurious
subgraph classifiers for generalization enhancement. Experimental results
demonstrate that our \textbf{PNSIS} model outperforms the state-of-the-art
techniques on graph OOD on several benchmarks, highlighting the effectiveness
in real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09166" title="Abstract">arXiv:2402.09166</a> [<a href="/pdf/2402.09166" title="Download PDF">pdf</a>, <a href="/format/2402.09166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deinterleaving of Discrete Renewal Process Mixtures with Application to  Electronic Support Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pinsolle%2C+J">Jean Pinsolle</a>, 
<a href="/search/cs?searchtype=author&query=Goudet%2C+O">Olivier Goudet</a>, 
<a href="/search/cs?searchtype=author&query=Enderli%2C+C">Cyrille Enderli</a>, 
<a href="/search/cs?searchtype=author&query=Lamprier%2C+S">Sylvain Lamprier</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jin-Kao Hao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we propose a new deinterleaving method for mixtures of
discrete renewal Markov chains. This method relies on the maximization of a
penalized likelihood score. It exploits all available information about both
the sequence of the different symbols and their arrival times. A theoretical
analysis is carried out to prove that minimizing this score allows to recover
the true partition of symbols in the large sample limit, under mild conditions
on the component processes. This theoretical analysis is then validated by
experiments on synthetic data. Finally, the method is applied to deinterleave
pulse trains received from different emitters in a RESM (Radar Electronic
Support Measurements) context and we show that the proposed method competes
favorably with state-of-the-art methods on simulated warfare datasets.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09167" title="Abstract">arXiv:2402.09167</a> [<a href="/pdf/2402.09167" title="Download PDF">pdf</a>, <a href="/format/2402.09167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolving Restricted Boltzmann Machine-Kohonen Network for Online  Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Senthilnath%2C+J">J. Senthilnath</a>, 
<a href="/search/cs?searchtype=author&query=Bhattiprolu%2C+A">Adithya Bhattiprolu</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Ankur Singh</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bangjian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>, 
<a href="/search/cs?searchtype=author&query=Benediktsson%2C+J+A">J&#xf3;n Atli Benediktsson</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoli Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 11 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A novel online clustering algorithm is presented where an Evolving Restricted
Boltzmann Machine (ERBM) is embedded with a Kohonen Network called ERBM-KNet.
The proposed ERBM-KNet efficiently handles streaming data in a single-pass mode
using the ERBM, employing a bias-variance strategy for neuron growing and
pruning, as well as online clustering based on a cluster update strategy for
cluster prediction and cluster center update using KNet. Initially, ERBM
evolves its architecture while processing unlabeled image data, effectively
disentangling the data distribution in the latent space. Subsequently, the KNet
utilizes the feature extracted from ERBM to predict the number of clusters and
updates the cluster centers. By overcoming the common challenges associated
with clustering algorithms, such as prior initialization of the number of
clusters and subpar clustering accuracy, the proposed ERBM-KNet offers
significant improvements. Extensive experimental evaluations on four benchmarks
and one industry dataset demonstrate the superiority of ERBM-KNet compared to
state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09168" title="Abstract">arXiv:2402.09168</a> [<a href="/pdf/2402.09168" title="Download PDF">pdf</a>, <a href="/ps/2402.09168" title="Download PostScript">ps</a>, <a href="/format/2402.09168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilizing Agreement is Impossible in Delayed Message Passing Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Felber%2C+S">Stephan Felber</a>, 
<a href="/search/cs?searchtype=author&query=Galeana%2C+H+R">Hugo Rincon Galeana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Most distributed computing research has focused on terminating problems like
consensus and similar agreement problems. Non-terminating problems have been
studied exhaustively in the context of self-stabilizing distributed algorithms,
however, which may start from arbitrary initial states and can tolerate
arbitrary transient faults. Somehow in-between is the stabilizing consensus
problem, where the processes start from a well-defined initial state but do not
need to decide irrevocably and need to agree on a common value only eventually.
Charron-Bost and Moran studied stabilizing consensus in synchronous dynamic
networks controlled by a message adversary. They introduced the simple and
elegant class of min-max algorithms, which allow to solve stabilizing consensus
under every message adversary that (i) allows at least one process to reach all
other processes infinitely often, and (ii) does so within a bounded (but
unknown) number of rounds. Moreover, the authors proved that (i) is a necessary
condition. The question whether (i) is also sufficient, i.e., whether (ii) is
also necessary, was left open. We answer this question by proving that
stabilizing consensus is impossible if (ii) is dropped, i.e., even if some
process reaches all other processes infinitely often but only within finite
time. We accomplish this by introducing a novel class of arbitrarily delayed
message adversaries, which also allows us to establish a connection between
terminating task solvability under some message adversary to stabilizing task
solvability under the corresponding arbitrarily delayed message adversary.
Finally, we outline how to extend this relation to terminating task solvability
in asynchronous message passing with guaranteed broadcasts, which highlights
the asynchronous characteristics induced by arbitrary delays.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09171" title="Abstract">arXiv:2402.09171</a> [<a href="/pdf/2402.09171" title="Download PDF">pdf</a>, <a href="/format/2402.09171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Unit Test Improvement using Large Language Models at Meta
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alshahwan%2C+N">Nadia Alshahwan</a>, 
<a href="/search/cs?searchtype=author&query=Chheda%2C+J">Jubin Chheda</a>, 
<a href="/search/cs?searchtype=author&query=Finegenova%2C+A">Anastasia Finegenova</a>, 
<a href="/search/cs?searchtype=author&query=Gokkaya%2C+B">Beliz Gokkaya</a>, 
<a href="/search/cs?searchtype=author&query=Harman%2C+M">Mark Harman</a>, 
<a href="/search/cs?searchtype=author&query=Harper%2C+I">Inna Harper</a>, 
<a href="/search/cs?searchtype=author&query=Marginean%2C+A">Alexandru Marginean</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+S">Shubho Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+E">Eddy Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, 32nd ACM Symposium on the Foundations of Software Engineering (FSE 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">This paper describes Meta's TestGen-LLM tool, which uses LLMs to
automatically improve existing human-written tests. TestGen-LLM verifies that
its generated test classes successfully clear a set of filters that assure
measurable improvement over the original test suite, thereby eliminating
problems due to LLM hallucination. We describe the deployment of TestGen-LLM at
Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on
Reels and Stories products for Instagram, 75% of TestGen-LLM's test cases built
correctly, 57% passed reliably, and 25% increased coverage. During Meta's
Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which
it was applied, with 73% of its recommendations being accepted for production
deployment by Meta software engineers. We believe this is the first report on
industrial scale deployment of LLM-generated code backed by such assurances of
code improvement.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09173" title="Abstract">arXiv:2402.09173</a> [<a href="/pdf/2402.09173" title="Download PDF">pdf</a>, <a href="/format/2402.09173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearly Optimal Regret for Decentralized Online Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yuanyu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tong Wei</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingli Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We investigate decentralized online convex optimization (D-OCO), in which a
set of local learners are required to minimize a sequence of global loss
functions using only local computations and communications. Previous studies
have established $O(n^{5/4}\rho^{-1/2}\sqrt{T})$ and ${O}(n^{3/2}\rho^{-1}\log
T)$ regret bounds for convex and strongly convex functions respectively, where
$n$ is the number of local learners, $\rho&lt;1$ is the spectral gap of the
communication matrix, and $T$ is the time horizon. However, there exist large
gaps from the existing lower bounds, i.e., $\Omega(n\sqrt{T})$ for convex
functions and $\Omega(n)$ for strongly convex functions. To fill these gaps, in
this paper, we first develop novel D-OCO algorithms that can respectively
reduce the regret bounds for convex and strongly convex functions to
$\tilde{O}(n\rho^{-1/4}\sqrt{T})$ and $\tilde{O}(n\rho^{-1/2}\log T)$. The
primary technique is to design an online accelerated gossip strategy that
enjoys a faster average consensus among local learners. Furthermore, by
carefully exploiting the spectral properties of a specific network topology, we
enhance the lower bounds for convex and strongly convex functions to
$\Omega(n\rho^{-1/4}\sqrt{T})$ and $\Omega(n\rho^{-1/2})$, respectively. These
lower bounds suggest that our algorithms are nearly optimal in terms of $T$,
$n$, and $\rho$.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09176" title="Abstract">arXiv:2402.09176</a> [<a href="/pdf/2402.09176" title="Download PDF">pdf</a>, <a href="/format/2402.09176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Interaction Simulator for Cold-Start Item  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Feiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhenghang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junyi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Bei%2C+Y">Yuanchen Bei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yijie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommending cold items is a long-standing challenge for collaborative
filtering models because these cold items lack historical user interactions to
model their collaborative features. The gap between the content of cold items
and their behavior patterns makes it difficult to generate accurate behavioral
embeddings for cold items. Existing cold-start models use mapping functions to
generate fake behavioral embeddings based on the content feature of cold items.
However, these generated embeddings have significant differences from the real
behavioral embeddings, leading to a negative impact on cold recommendation
performance. To address this challenge, we propose an LLM Interaction Simulator
(LLM-InS) to model users' behavior patterns based on the content aspect. This
simulator allows recommender systems to simulate vivid interactions for each
cold item and transform them from cold to warm items directly. Specifically, we
outline the designing and training process of a tailored LLM-simulator that can
simulate the behavioral patterns of users and items. Additionally, we introduce
an efficient "filtering-and-refining" approach to take full advantage of the
simulation power of the LLMs. Finally, we propose an updating method to update
the embeddings of the items. we unified trains for both cold and warm items
within a recommender model based on the simulated and real interactions.
Extensive experiments using real behavioral embeddings demonstrate that our
proposed model, LLM-InS, outperforms nine state-of-the-art cold-start methods
and three LLM models in cold-start item recommendations.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09177" title="Abstract">arXiv:2402.09177</a> [<a href="/pdf/2402.09177" title="Download PDF">pdf</a>, <a href="/format/2402.09177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging the Context through Multi-Round Interactions for Jailbreaking  Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yixin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Georgopoulos%2C+M">Markos Georgopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>, 
<a href="/search/cs?searchtype=author&query=Chrysos%2C+G+G">Grigorios G. Chrysos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which
aim to extract harmful information by subtly modifying the attack query. As
defense mechanisms evolve, directly obtaining harmful information becomes
increasingly challenging for Jailbreaking attacks. In this work, inspired by
human practices of indirect context to elicit harmful information, we focus on
a new attack form called Contextual Interaction Attack. The idea relies on the
autoregressive nature of the generation process in LLMs. We contend that the
prior context--the information preceding the attack query--plays a pivotal role
in enabling potent Jailbreaking attacks. Specifically, we propose an approach
that leverages preliminary question-answer pairs to interact with the LLM. By
doing so, we guide the responses of the model toward revealing the 'desired'
harmful information. We conduct experiments on four different LLMs and
demonstrate the efficacy of this attack, which is black-box and can also
transfer across LLMs. We believe this can lead to further developments and
understanding of the context vector in LLMs.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09178" title="Abstract">arXiv:2402.09178</a> [<a href="/pdf/2402.09178" title="Download PDF">pdf</a>, <a href="/format/2402.09178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Portrait Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chahine%2C+N">Nicolas Chahine</a>, 
<a href="/search/cs?searchtype=author&query=Ferradans%2C+S">Sira Ferradans</a>, 
<a href="/search/cs?searchtype=author&query=Vazquez-Corral%2C+J">Javier Vazquez-Corral</a>, 
<a href="/search/cs?searchtype=author&query=Ponce%2C+J">Jean Ponce</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Automated and robust portrait quality assessment (PQA) is of paramount
importance in high-impact applications such as smartphone photography. This
paper presents FHIQA, a learning-based approach to PQA that introduces a simple
but effective quality score rescaling method based on image semantics, to
enhance the precision of fine-grained image quality metrics while ensuring
robust generalization to various scene settings beyond the training dataset.
The proposed approach is validated by extensive experiments on the PIQ23
benchmark and comparisons with the current state of the art. The source code of
FHIQA will be made publicly available on the PIQ23 GitHub repository at
https://github.com/DXOMARK-Research/PIQ2023.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09179" title="Abstract">arXiv:2402.09179</a> [<a href="/pdf/2402.09179" title="Download PDF">pdf</a>, <a href="/format/2402.09179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model  Customization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+R">Rui Wen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenbo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The increasing demand for customized Large Language Models (LLMs) has led to
the development of solutions like GPTs. These solutions facilitate tailored LLM
creation via natural language prompts without coding. However, the
trustworthiness of third-party custom versions of LLMs remains an essential
concern. In this paper, we propose the first instruction backdoor attacks
against applications integrated with untrusted customized LLMs (e.g., GPTs).
Specifically, these attacks embed the backdoor into the custom version of LLMs
by designing prompts with backdoor instructions, outputting the attacker's
desired result when inputs contain the pre-defined triggers. Our attack
includes 3 levels of attacks: word-level, syntax-level, and semantic-level,
which adopt different types of triggers with progressive stealthiness. We
stress that our attacks do not require fine-tuning or any modification to the
backend LLMs, adhering strictly to GPTs development guidelines. We conduct
extensive experiments on 4 prominent LLMs and 5 benchmark text classification
datasets. The results show that our instruction backdoor attacks achieve the
desired attack performance without compromising utility. Additionally, we
propose an instruction-ignoring defense mechanism and demonstrate its partial
effectiveness in mitigating such attacks. Our findings highlight the
vulnerability and the potential risks of LLM customization such as GPTs.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09182" title="Abstract">arXiv:2402.09182</a> [<a href="/pdf/2402.09182" title="Download PDF">pdf</a>, <a href="/format/2402.09182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Space of Visual Feedforward And Corrective Feedback in XR-Based  Motion Guidance Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xingyao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Benjamin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Sedlmair%2C+M">Michael Sedlmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ACM CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Extended reality (XR) technologies are highly suited in assisting individuals
in learning motor skills and movements -- referred to as motion guidance. In
motion guidance, the "feedforward" provides instructional cues of the motions
that are to be performed, whereas the "feedback" provides cues which help
correct mistakes and minimize errors. Designing synergistic feedforward and
feedback is vital to providing an effective learning experience, but this
interplay between the two has not yet been adequately explored. Based on a
survey of the literature, we propose design space for both motion feedforward
and corrective feedback in XR, and describe the interaction effects between
them. We identify common design approaches of XR-based motion guidance found in
our literature corpus, and discuss them through the lens of our design
dimensions. We then discuss additional contextual factors and considerations
that influence this design, together with future research opportunities for
motion guidance in XR.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09185" title="Abstract">arXiv:2402.09185</a> [<a href="/pdf/2402.09185" title="Download PDF">pdf</a>, <a href="/format/2402.09185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flattability of Priority Vector Addition Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guttenberg%2C+R">Roland Guttenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 2 figures, submitted to ICALP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">Vector addition systems (VAS), also known as Petri nets, are a popular model
of concurrent systems. Many problems from many areas reduce to the reachability
problem for VAS, which consists of deciding whether a target configuration of a
VAS is reachable from a given initial configuration. One of the main approaches
to solve the problem on practical instances is called flattening, intuitively
removing nested loops. This technique is known to terminate for semilinear VAS.
In this paper, we prove that also for VAS with nested zero tests, called
Priority VAS, flattening does in fact terminate for all semilinear reachability
relations. Furthermore, we prove that Priority VAS admit semilinear inductive
invariants. Both of these results are obtained by defining a well-quasi-order
on runs of Priority VAS which has good pumping properties.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09187" title="Abstract">arXiv:2402.09187</a> [<a href="/pdf/2402.09187" title="Download PDF">pdf</a>, <a href="/format/2402.09187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Tractable Quantified Temporal Constraints within Ord-Horn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rydval%2C+J">Jakub Rydval</a>, 
<a href="/search/cs?searchtype=author&query=Semani%C5%A1inov%C3%A1%2C+%C5%BD">&#x17d;aneta Semani&#x161;inov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Wrona%2C+M">Micha&#x142; Wrona</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">The constraint satisfaction problem, parameterized by a relational structure,
provides a general framework for expressing computational decision problems.
Already the restriction to the class of all finite structures forms an
interesting microcosm on its own, but to express decision problems in temporal
reasoning one has to take a step beyond the finite-domain realm. An important
class of templates used in this context are temporal structures, i.e.,
structures over $\mathbb{Q}$ whose relations are first-order definable using
the usual countable dense linear order without endpoints. In the standard
setting, which allows only existential quantification over input variables, the
complexity of finite and temporal constraints has been fully classified. In the
quantified setting, i.e., when one also allows universal quantifiers, there is
only a handful of partial classification results and many concrete cases of
unknown complexity. This paper presents a significant progress towards
understanding the complexity of the quantified constraint satisfaction problem
for temporal structures. We provide a complexity dichotomy for quantified
constraints over the Ord-Horn fragment, which played an important role in
understanding the complexity of constraints both over temporal structures and
in Allen's interval algebra. We show that all problems under consideration are
in P or coNP-hard. In particular, we determine the complexity of the quantified
constraint satisfaction problem for $(\mathbb{Q};x=y\Rightarrow x\geq z)$,
hereby settling a question open for more than ten years.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09189" title="Abstract">arXiv:2402.09189</a> [<a href="/pdf/2402.09189" title="Download PDF">pdf</a>, <a href="/format/2402.09189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Traj-LIO: A Resilient Multi-LiDAR Multi-IMU State Estimator Through  Sparse Gaussian Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianke Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Nowadays, sensor suits have been equipped with redundant LiDARs and IMUs to
mitigate the risks associated with sensor failure. It is challenging for the
previous discrete-time and IMU-driven kinematic systems to incorporate multiple
asynchronized sensors, which are susceptible to abnormal IMU data. To address
these limitations, we introduce a multi-LiDAR multi-IMU state estimator by
taking advantage of Gaussian Process (GP) that predicts a non-parametric
continuous-time trajectory to capture sensors' spatial-temporal movement with
limited control states. Since the kinematic model driven by three types of
linear time-invariant stochastic differential equations are independent of
external sensor measurements, our proposed approach is capable of handling
different sensor configurations and resilient to sensor failures. Moreover, we
replace the conventional $\mathrm{SE}(3)$ state representation with the
combination of $\mathrm{SO}(3)$ and vector space, which enables GP-based
LiDAR-inertial system to fulfill the real-time requirement. Extensive
experiments on the public datasets demonstrate the versatility and resilience
of our proposed multi-LiDAR multi-IMU state estimator. To contribute to the
community, we will make our source code publicly available.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09191" title="Abstract">arXiv:2402.09191</a> [<a href="/pdf/2402.09191" title="Download PDF">pdf</a>, <a href="/format/2402.09191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyber Deception Reactive: TCP Stealth Redirection to On-Demand Honeypots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lopez%2C+P+B">Pedro Beltran Lopez</a>, 
<a href="/search/cs?searchtype=author&query=Nespoli%2C+P">Pantaleone Nespoli</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+M+G">Manuel Gil Perez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI); Performance (cs.PF); Systems and Control (eess.SY)

</div>
<p class="mathjax">Cybersecurity is developing rapidly, and new methods of defence against
attackers are appearing, such as Cyber Deception (CYDEC). CYDEC consists of
deceiving the enemy who performs actions without realising that he/she is being
deceived. This article proposes designing, implementing, and evaluating a
deception mechanism based on the stealthy redirection of TCP communications to
an on-demand honey server with the same characteristics as the victim asset,
i.e., it is a clone. Such a mechanism ensures that the defender fools the
attacker, thanks to stealth redirection. In this situation, the attacker will
focus on attacking the honey server while enabling the recollection of relevant
information to generate threat intelligence. The experiments in different
scenarios show how the proposed solution can effectively redirect an attacker
to a copied asset on demand, thus protecting the real asset. Finally, the
results obtained by evaluating the latency times ensure that the redirection is
undetectable by humans and very difficult to detect by a machine.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09193" title="Abstract">arXiv:2402.09193</a> [<a href="/pdf/2402.09193" title="Download PDF">pdf</a>, <a href="/format/2402.09193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Ir)rationality and Cognitive Biases in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Macmillan-Scott%2C+O">Olivia Macmillan-Scott</a>, 
<a href="/search/cs?searchtype=author&query=Musolesi%2C+M">Mirco Musolesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Do large language models (LLMs) display rational reasoning? LLMs have been
shown to contain human biases due to the data they have been trained on;
whether this is reflected in rational reasoning remains less clear. In this
paper, we answer this question by evaluating seven language models using tasks
from the cognitive psychology literature. We find that, like humans, LLMs
display irrationality in these tasks. However, the way this irrationality is
displayed does not reflect that shown by humans. When incorrect answers are
given by LLMs to these tasks, they are often incorrect in ways that differ from
human-like biases. On top of this, the LLMs reveal an additional layer of
irrationality in the significant inconsistency of the responses. Aside from the
experimental results, this paper seeks to make a methodological contribution by
showing how we can assess and compare different capabilities of these types of
models, in this case with respect to rational reasoning.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09196" title="Abstract">arXiv:2402.09196</a> [<a href="/pdf/2402.09196" title="Download PDF">pdf</a>, <a href="/ps/2402.09196" title="Download PostScript">ps</a>, <a href="/format/2402.09196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> comparison of two models to predict vertebral failure loads on the same  experimental dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Allard%2C+V">V. Allard</a> (1, 2), 
<a href="/search/math?searchtype=author&query=Heidsieck%2C+C">C. Heidsieck</a> (3), 
<a href="/search/math?searchtype=author&query=Bermond%2C+F">F. Bermond</a> (2), 
<a href="/search/math?searchtype=author&query=Confavreux%2C+C">C. Confavreux</a> (1, 4), 
<a href="/search/math?searchtype=author&query=Travert%2C+C">C. Travert</a> (3,5), 
<a href="/search/math?searchtype=author&query=Gajny%2C+L">L. Gajny</a>, 3, 
<a href="/search/math?searchtype=author&query=Skalli%2C+W">W. Skalli</a> (3), 
<a href="/search/math?searchtype=author&query=Mitton%2C+D">D. Mitton</a> (2), 
<a href="/search/math?searchtype=author&query=Follet%2C+H">H. Follet</a> (1)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, 2 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CMBBE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Clinical use of finite element analysis requires validation and
reproducibility studies. The current study compared two models of vertebral
bodies including endplates, on the same experimental dataset and evaluated the
influence of the operator on the failure load. Models used were strongly
correlated (R2=0.91). The intra-operator reproducibility was 6.4% and 3.5 % for
each model. Both simulated results were close to experimental results. The
differences in performance could be associated to the differences in
segmentation process, mesh (hexahedral vs tetrahedral), material representation
and failure criteria. Linear analysis did not decrease model accuracy.
Comparison with literature for accuracy and precision shows a wide range of
values partly related to the different experimental datasets and the different
modelling approaches. Models benchmark using the same experimental dataset are
needed to go towards clinical applications.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09197" title="Abstract">arXiv:2402.09197</a> [<a href="/pdf/2402.09197" title="Download PDF">pdf</a>, <a href="/format/2402.09197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implementing local-explainability in Gradient Boosting Trees: Feature  Contribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delgado-Panadero%2C+%C3%81">&#xc1;ngel Delgado-Panadero</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lorca%2C+B">Beatriz Hern&#xe1;ndez-Lorca</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information Sciences, Volume 589, 2022, Pages 199-212
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Gradient Boost Decision Trees (GBDT) is a powerful additive model based on
tree ensembles. Its nature makes GBDT a black-box model even though there are
multiple explainable artificial intelligence (XAI) models obtaining information
by reinterpreting the model globally and locally. Each tree of the ensemble is
a transparent model itself but the final outcome is the result of a sum of
these trees and it is not easy to clarify.
<br />In this paper, a feature contribution method for GBDT is developed. The
proposed method takes advantage of the GBDT architecture to calculate the
contribution of each feature using the residue of each node. This algorithm
allows to calculate the sequence of node decisions given a prediction.
<br />Theoretical proofs and multiple experiments have been carried out to
demonstrate the performance of our method which is not only a local
explicability model for the GBDT algorithm but also a unique option that
reflects GBDTs internal behavior. The proposal is aligned to the contribution
of characteristics having impact in some artificial intelligence problems such
as ethical analysis of Artificial Intelligence (AI) and comply with the new
European laws such as the General Data Protection Regulation (GDPR) about the
right to explain and nondiscrimination.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09199" title="Abstract">arXiv:2402.09199</a> [<a href="/pdf/2402.09199" title="Download PDF">pdf</a>, <a href="/format/2402.09199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ten Words Only Still Help: Improving Black-Box AI-Generated Text  Detection via Proxy-Guided Efficient Re-Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Q">Qiang Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Juan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+H">Hao Mi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Beizhe Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Danding Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the rapidly increasing application of large language models (LLMs),
their abuse has caused many undesirable societal problems such as fake news,
academic dishonesty, and information pollution. This makes AI-generated text
(AIGT) detection of great importance. Among existing methods, white-box methods
are generally superior to black-box methods in terms of performance and
generalizability, but they require access to LLMs' internal states and are not
applicable to black-box settings. In this paper, we propose to estimate word
generation probabilities as pseudo white-box features via multiple re-sampling
to help improve AIGT detection under the black-box setting. Specifically, we
design POGER, a proxy-guided efficient re-sampling method, which selects a
small subset of representative words (e.g., 10 words) for performing multiple
re-sampling in black-box AIGT detection. Experiments on datasets containing
texts from humans and seven LLMs show that POGER outperforms all baselines in
macro F1 under black-box, partial white-box, and out-of-distribution settings
and maintains lower re-sampling costs than its existing counterparts.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09200" title="Abstract">arXiv:2402.09200</a> [<a href="/pdf/2402.09200" title="Download PDF">pdf</a>, <a href="/format/2402.09200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Command and Control (C2) Channels on Tor and Public Networks  Using Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Redino%2C+C">Christopher Redino</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+A">Abdul Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+R">Ryan Clark</a>, 
<a href="/search/cs?searchtype=author&query=Radke%2C+D">Daniel Radke</a>, 
<a href="/search/cs?searchtype=author&query=Cody%2C+T">Tyler Cody</a>, 
<a href="/search/cs?searchtype=author&query=Nandakumar%2C+D">Dhruv Nandakumar</a>, 
<a href="/search/cs?searchtype=author&query=Bowen%2C+E">Edward Bowen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Command and control (C2) channels are an essential component of many types of
cyber attacks, as they enable attackers to remotely control their
malware-infected machines and execute harmful actions, such as propagating
malicious code across networks, exfiltrating confidential data, or initiating
distributed denial of service (DDoS) attacks. Identifying these C2 channels is
therefore crucial in helping to mitigate and prevent cyber attacks. However,
identifying C2 channels typically involves a manual process, requiring deep
knowledge and expertise in cyber operations. In this paper, we propose a
reinforcement learning (RL) based approach to automatically emulate C2 attack
campaigns using both the normal (public) and the Tor networks. In addition,
payload size and network firewalls are configured to simulate real-world attack
scenarios. Results on a typical network configuration show that the RL agent
can automatically discover resilient C2 attack paths utilizing both Tor-based
and conventional communication channels, while also bypassing network
firewalls.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09201" title="Abstract">arXiv:2402.09201</a> [<a href="/pdf/2402.09201" title="Download PDF">pdf</a>, <a href="/ps/2402.09201" title="Download PostScript">ps</a>, <a href="/format/2402.09201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better-than-KL PAC-Bayes Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuzborskij%2C+I">Ilja Kuzborskij</a>, 
<a href="/search/cs?searchtype=author&query=Jun%2C+K">Kwang-Sung Jun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yulian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+K">Kyoungseok Jang</a>, 
<a href="/search/cs?searchtype=author&query=Orabona%2C+F">Francesco Orabona</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Let $f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$ be a sequence of random
elements, where $f$ is a fixed scalar function, $X_1, \dots, X_n$ are
independent random variables (data), and $\theta$ is a random parameter
distributed according to some data-dependent posterior distribution $P_n$. In
this paper, we consider the problem of proving concentration inequalities to
estimate the mean of the sequence. An example of such a problem is the
estimation of the generalization error of some predictor trained by a
stochastic algorithm, such as a neural network where $f$ is a loss function.
Classically, this problem is approached through a PAC-Bayes analysis where, in
addition to the posterior, we choose a prior distribution which captures our
belief about the inductive bias of the learning problem. Then, the key quantity
in PAC-Bayes concentration bounds is a divergence that captures the complexity
of the learning problem where the de facto standard choice is the KL
divergence. However, the tightness of this choice has rarely been questioned.
<br />In this paper, we challenge the tightness of the KL-divergence-based bounds
by showing that it is possible to achieve a strictly tighter bound. In
particular, we demonstrate new high-probability PAC-Bayes bounds with a novel
and better-than-KL divergence that is inspired by Zhang et al. (2022). Our
proof is inspired by recent advances in regret analysis of gambling algorithms,
and its use to derive concentration inequalities. Our result is
first-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are
not known to be strictly better than KL. Thus, we believe our work marks the
first step towards identifying optimal rates of PAC-Bayes bounds.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09204" title="Abstract">arXiv:2402.09204</a> [<a href="/pdf/2402.09204" title="Download PDF">pdf</a>, <a href="/format/2402.09204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-adaptive and Subgroup-specific Cascaded Temperature Regression  for Out-of-distribution Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiexin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiahao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+B">Bing Su</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024 IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP 2024), Seoul, Korea
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Although deep neural networks yield high classification accuracy given
sufficient training data, their predictions are typically overconfident or
under-confident, i.e., the prediction confidences cannot truly reflect the
accuracy. Post-hoc calibration tackles this problem by calibrating the
prediction confidences without re-training the classification model. However,
current approaches assume congruence between test and validation data
distributions, limiting their applicability to out-of-distribution scenarios.
To this end, we propose a novel meta-set-based cascaded temperature regression
method for post-hoc calibration. Our method tailors fine-grained scaling
functions to distinct test sets by simulating various domain shifts through
data augmentation on the validation set. We partition each meta-set into
subgroups based on predicted category and confidence level, capturing diverse
uncertainties. A regression network is then trained to derive category-specific
and confidence-level-specific scaling, achieving calibration across meta-sets.
Extensive experimental results on MNIST, CIFAR-10, and TinyImageNet demonstrate
the effectiveness of the proposed method.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09205" title="Abstract">arXiv:2402.09205</a> [<a href="/pdf/2402.09205" title="Download PDF">pdf</a>, <a href="/format/2402.09205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tell Me More! Towards Implicit User Intention Understanding of Language  Model Driven Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Cheng Qian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingxiang He</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zhong Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jia Deng</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xin Cong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 tables, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Current language model-driven agents often lack mechanisms for effective user
participation, which is crucial given the vagueness commonly found in user
instructions. Although adept at devising strategies and performing tasks, these
agents struggle with seeking clarification and grasping precise user
intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a
novel benchmark designed to inspect users' implicit intentions through explicit
queries. Next, we propose the incorporation of model experts as the upstream in
agent designs to enhance user-agent interaction. Employing IN3, we empirically
train Mistral-Interact, a powerful model that proactively assesses task
vagueness, inquires user intentions, and refines them into actionable goals
before starting downstream agent task execution. Integrating it into the XAgent
framework, we comprehensively evaluate the enhanced agent system regarding user
instruction understanding and execution, revealing that our approach notably
excels at identifying vague user tasks, recovering and summarizing critical
missing information, setting precise and necessary agent execution goals, and
minimizing redundant tool usage, thus boosting overall efficiency. All the data
and codes are released.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09210" title="Abstract">arXiv:2402.09210</a> [<a href="/pdf/2402.09210" title="Download PDF">pdf</a>, <a href="/ps/2402.09210" title="Download PostScript">ps</a>, <a href="/format/2402.09210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inter-laboratory replicability and sensitivity study of a finite element  model to quantify human femur failure load: case of metastases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gardegaront%2C+M">Marc Gardegaront</a>, 
<a href="/search/math?searchtype=author&query=Sas%2C+A">Amelie Sas</a>, 
<a href="/search/math?searchtype=author&query=Brizard%2C+D">Denis Brizard</a>, 
<a href="/search/math?searchtype=author&query=Levillain%2C+A">Aurelie Levillain</a>, 
<a href="/search/math?searchtype=author&query=Bermond%2C+F">Francois Bermond</a>, 
<a href="/search/math?searchtype=author&query=Confavreux%2C+C+B">Cyrille B. Confavreux</a>, 
<a href="/search/math?searchtype=author&query=Pialat%2C+J">Jean-Baptiste Pialat</a>, 
<a href="/search/math?searchtype=author&query=van+Lenthe%2C+G+H">G. Harry van Lenthe</a>, 
<a href="/search/math?searchtype=author&query=Follet%2C+H">Helene Follet</a>, 
<a href="/search/math?searchtype=author&query=Mitton%2C+D">David Mitton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computation (stat.CO)

</div>
<p class="mathjax">Metastases increase the risk of fracture when affecting the femur.
Consequently, clinicians need to know if the patients femur can withstand the
stress of daily activities. The current tools used in clinics are not
sufficiently precise. A new method, the CT-scan-based finite element analysis,
gives good predictive results. However, none of the existing models were tested
for reproducibility. This is a critical issue to address in order to apply the
technique on a large cohort around the world to help evaluate bone metastatic
fracture risk in patients. Please see pdf file
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09211" title="Abstract">arXiv:2402.09211</a> [<a href="/pdf/2402.09211" title="Download PDF">pdf</a>, <a href="/format/2402.09211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced  Three-Point Trackers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dongseok Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiho Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lingni Ma</a>, 
<a href="/search/cs?searchtype=author&query=Greer%2C+J">Joseph Greer</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yuting Ye</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sung-Hee Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to Eurographics 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Full-body avatar presence is crucial for immersive social and environmental
interactions in digital reality. However, current devices only provide three
six degrees of freedom (DOF) poses from the headset and two controllers (i.e.
three-point trackers). Because it is a highly under-constrained problem,
inferring full-body pose from these inputs is challenging, especially when
supporting the full range of body proportions and use cases represented by the
general population. In this paper, we propose a deep learning framework,
DivaTrack, which outperforms existing methods when applied to diverse body
sizes and activities. We augment the sparse three-point inputs with linear
accelerations from Inertial Measurement Units (IMU) to improve foot contact
prediction. We then condition the otherwise ambiguous lower-body pose with the
predictions of foot contact and upper-body pose in a two-stage model. We
further stabilize the inferred full-body pose in a wide range of configurations
by learning to blend predictions that are computed in two reference frames,
each of which is designed for different types of motions. We demonstrate the
effectiveness of our design on a large dataset that captures 22 subjects
performing challenging locomotion for three-point tracking, including lunges,
hula-hooping, and sitting. As shown in a live demo using the Meta VR headset
and Xsens IMUs, our method runs in real-time while accurately tracking a user's
motion when they perform a diverse set of movements.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09213" title="Abstract">arXiv:2402.09213</a> [<a href="/pdf/2402.09213" title="Download PDF">pdf</a>, <a href="/ps/2402.09213" title="Download PostScript">ps</a>, <a href="/format/2402.09213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification of cohesive subgroups in a university hall of residence  during the COVID-19 pandemic using a social network analysis approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marqu%C3%A9s-S%C3%A1nchez%2C+P">Pilar Marqu&#xe9;s-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Pinto-Carral%2C+A">Arrate Pinto-Carral</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Villa%2C+T">Tania Fern&#xe1;ndez-Villa</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%A1zquez-Casares%2C+A">Ana V&#xe1;zquez-Casares</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%A9bana-Presa%2C+C">Cristina Li&#xe9;bana-Presa</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Scientific Reports, Volume 11, Article ID 22055, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">The aims: (i) analyze connectivity between subgroups of university students,
(ii) assess which bridges of relational contacts are essential for connecting
or disconnecting subgroups and (iii) to explore the similarities between the
attributes of the subgroup nodes in relation to the pandemic context. During
the COVID-19 pandemic, young university students have experienced significant
changes in their relationships, especially in the halls of residence. Previous
research has shown the importance of relationship structure in contagion
processes. However, there is a lack of studies in the university setting, where
students live closely together. The case study methodology was applied to carry
out a descriptive study. The participation consisted of 43 university students
living in the same hall of residence. Social network analysis has been applied
for data analysis. Factions and Girvan Newman algorithms have been applied to
detect the existing cohesive subgroups. The UCINET tool was used for the
calculation of the SNA measure. A visualization of the global network will be
carried out using Gephi software. After applying the Girvan-Newman and
Factions, in both cases it was found that the best division into subgroups was
the one that divided the network into 4 subgroups. There is high degree of
cohesion within the subgroups and a low cohesion between them. The relationship
between subgroup membership and gender was significant. The degree of COVID-19
infection is related to the degree of clustering between the students. College
students form subgroups in their residence. Social network analysis facilitates
an understanding of structural behavior during the pandemic. The study provides
evidence on the importance of gender, race and the building where they live in
creating network structures that favor, or not, contagion during a pandemic.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09216" title="Abstract">arXiv:2402.09216</a> [<a href="/pdf/2402.09216" title="Download PDF">pdf</a>, <a href="/format/2402.09216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling the Authoring of AutoTutors with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+P">Sankalan Pal Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Zouhar%2C+V">Vil&#xe9;m Zouhar</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Large Language Models (LLMs) have found several use cases in education,
ranging from automatic question generation to essay evaluation. In this paper,
we explore the potential of using Large Language Models (LLMs) to author
Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from
desired pedagogical strategies such as leaking the answer to the student, and
in general, providing no guarantees. We posit that while LLMs with certain
guardrails can take the place of subject experts, the overall pedagogical
design still needs to be handcrafted for the best learning results. Based on
this principle, we create a sample end-to-end tutoring system named MWPTutor,
which uses LLMs to fill in the state space of a pre-defined finite state
transducer. This approach retains the structure and the pedagogy of traditional
tutoring systems that has been developed over the years by learning scientists
but brings in additional flexibility of LLM-based approaches. Through a human
evaluation study on two datasets based on math word problems, we show that our
hybrid approach achieves a better overall tutoring score than an instructed,
but otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the
scope for the community to improve its performance by improving individual
modules or using different teaching strategies that it can follow
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09217" title="Abstract">arXiv:2402.09217</a> [<a href="/pdf/2402.09217" title="Download PDF">pdf</a>, <a href="/format/2402.09217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferentialist Resource Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gheorghiu%2C+A+V">Alexander V. Gheorghiu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+T">Tao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Pym%2C+D+J">David J. Pym</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Cryptography and Security (cs.CR); Systems and Control (eess.SY)

</div>
<p class="mathjax">In systems modelling, a system typically comprises located resources relative
to which processes execute. One important use of logic in informatics is in
modelling such systems for the purpose of reasoning (perhaps automated) about
their behaviour and properties. To this end, one requires an interpretation of
logical formulae in terms of the resources and states of the system; such an
interpretation is called a resource semantics of the logic. This paper shows
how inferentialism -- the view that meaning is given in terms of inferential
behaviour -- enables a versatile and expressive framework for resource
semantics. Specifically, how inferentialism seamlessly incorporates the
assertion-based approach of the logic of Bunched Implications, foundational in
program verification (e.g., as the basis of Separation Logic), and the renowned
number-of-uses reading of Linear Logic. This integration enables reasoning
about shared and separated resources in intuitive and familiar ways, as well as
about the composition and interfacing of system components.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09219" title="Abstract">arXiv:2402.09219</a> [<a href="/pdf/2402.09219" title="Download PDF">pdf</a>, <a href="/ps/2402.09219" title="Download PostScript">ps</a>, <a href="/format/2402.09219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A case study of university student networks and the COVID-19 pandemic  using a social network analysis approach in halls of residence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Villa%2C+T">Tania Fern&#xe1;ndez-Villa</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=Gayubo-Serrenes%2C+A">Andrea Gayubo-Serrenes</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn%2C+V">Vicente Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Marqu%C3%A9s-S%C3%A1nchez%2C+P">Pilar Marqu&#xe9;s-S&#xe1;nchez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Scientific Reports volume 11, Article number: 14877 (2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">The COVID-19 pandemic has meant that young university students have had to
adapt their learning and have a reduced relational context. Adversity contexts
build models of human behaviour based on relationships. However, there is a
lack of studies that analyse the behaviour of university students based on
their social structure in the context of a pandemic. This information could be
useful in making decisions on how to plan collective responses to adversities.
The Social Network Analysis (SNA) method has been chosen to address this
structural perspective. The aim of our research is to describe the structural
behaviour of students in university residences during the COVID-19 pandemic
with a more in-depth analysis of student leaders. A descriptive cross-sectional
study was carried out at one Spanish Public University, Le\'on, from 23th
October 2020 to 20th November 2020. The participation was of 93 students, from
four halls of residence. The data were collected from a database created
specifically at the university to "track" contacts in the COVID-19 pandemic,
SiVeUle. We applied the SNA for the analysis of the data. The leadership on the
university residence was measured using centrality measures. The top leaders
were analyzed using the Egonetwork and an assessment of the key players.
Students with higher social reputations experience higher levels of pandemic
contagion in relation to COVID-19 infection. The results were statistically
significant between the centrality in the network and the results of the
COVID-19 infection. The most leading students showed a high degree of
Betweenness, and three students had the key player structure in the network.
Networking behaviour of university students in halls of residence could be
related to contagion in the COVID-19 pandemic.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09221" title="Abstract">arXiv:2402.09221</a> [<a href="/pdf/2402.09221" title="Download PDF">pdf</a>, <a href="/format/2402.09221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral Filters, Dark Signals, and Attention Sinks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cancedda%2C+N">Nicola Cancedda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Projecting intermediate representations onto the vocabulary is an
increasingly popular interpretation tool for transformer-based LLMs, also known
as the logit lens. We propose a quantitative extension to this approach and
define spectral filters on intermediate representations based on partitioning
the singular vectors of the vocabulary embedding and unembedding matrices into
bands. We find that the signals exchanged in the tail end of the spectrum are
responsible for attention sinking (Xiao et al. 2023), of which we provide an
explanation. We find that the loss of pretrained models can be kept low despite
suppressing sizable parts of the embedding spectrum in a layer-dependent way,
as long as attention sinking is preserved. Finally, we discover that the
representation of tokens that draw attention from many tokens have large
projections on the tail end of the spectrum.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09222" title="Abstract">arXiv:2402.09222</a> [<a href="/pdf/2402.09222" title="Download PDF">pdf</a>, <a href="/format/2402.09222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating ytopt and libEnsemble to Autotune OpenMC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xingfu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tramm%2C+J+R">John R. Tramm</a>, 
<a href="/search/cs?searchtype=author&query=Larson%2C+J">Jeffrey Larson</a>, 
<a href="/search/cs?searchtype=author&query=Navarro%2C+J">John-Luke Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Balaprakash%2C+P">Prasanna Balaprakash</a>, 
<a href="/search/cs?searchtype=author&query=Videau%2C+B">Brice Videau</a>, 
<a href="/search/cs?searchtype=author&query=Kruse%2C+M">Michael Kruse</a>, 
<a href="/search/cs?searchtype=author&query=Hovland%2C+P">Paul Hovland</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+V">Valerie Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+M">Mary Hall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">ytopt is a Python machine-learning-based autotuning software package
developed within the ECP PROTEAS-TUNE project. The ytopt software adopts an
asynchronous search framework that consists of sampling a small number of input
parameter configurations and progressively fitting a surrogate model over the
input-output space until exhausting the user-defined maximum number of
evaluations or the wall-clock time. libEnsemble is a Python toolkit for
coordinating workflows of asynchronous and dynamic ensembles of calculations
across massively parallel resources developed within the ECP PETSc/TAO project.
libEnsemble helps users take advantage of massively parallel resources to solve
design, decision, and inference problems and expands the class of problems that
can benefit from increased parallelism. In this paper we present our
methodology and framework to integrate ytopt and libEnsemble to take advantage
of massively parallel resources to accelerate the autotuning process.
Specifically, we focus on using the proposed framework to autotune the ECP
ExaSMR application OpenMC, an open source Monte Carlo particle transport code.
OpenMC has seven tunable parameters some of which have large ranges such as the
number of particles in-flight, which is in the range of 100,000 to 8 million,
with its default setting of 1 million. Setting the proper combination of these
parameter values to achieve the best performance is extremely time-consuming.
Therefore, we apply the proposed framework to autotune the MPI/OpenMP offload
version of OpenMC based on a user-defined metric such as the figure of merit
(FoM) (particles/s) or energy efficiency energy-delay product (EDF) on the OLCF
Frontier TDS system Crusher. The experimental results show that we achieve
improvement up to 29.49% in FoM and up to 30.44% in EDP.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09225" title="Abstract">arXiv:2402.09225</a> [<a href="/pdf/2402.09225" title="Download PDF">pdf</a>, <a href="/format/2402.09225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is my Data in your AI Model? Membership Inference Test with Application  to Face Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DeAlcala%2C+D">Daniel DeAlcala</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+A">Aythami Morales</a>, 
<a href="/search/cs?searchtype=author&query=Mancera%2C+G">Gonzalo Mancera</a>, 
<a href="/search/cs?searchtype=author&query=Fierrez%2C+J">Julian Fierrez</a>, 
<a href="/search/cs?searchtype=author&query=Tolosana%2C+R">Ruben Tolosana</a>, 
<a href="/search/cs?searchtype=author&query=Ortega-Garcia%2C+J">Javier Ortega-Garcia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces the Membership Inference Test (MINT), a novel approach
that aims to empirically assess if specific data was used during the training
of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT
architectures designed to learn the distinct activation patterns that emerge
when an audited model is exposed to data used during its training process. The
first architecture is based on a Multilayer Perceptron (MLP) network and the
second one is based on Convolutional Neural Networks (CNNs). The proposed MINT
architectures are evaluated on a challenging face recognition task, considering
three state-of-the-art face recognition models. Experiments are carried out
using six publicly available databases, comprising over 22 million face images
in total. Also, different experimental scenarios are considered depending on
the context available of the AI model to test. Promising results, up to 90%
accuracy, are achieved using our proposed MINT approach, suggesting that it is
possible to recognize if an AI model has been trained with specific data.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09226" title="Abstract">arXiv:2402.09226</a> [<a href="/pdf/2402.09226" title="Download PDF">pdf</a>, <a href="/format/2402.09226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Directional Convergence Near Small Initializations and Saddles in  Two-Homogeneous Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Akshay Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Haupt%2C+J">Jarvis Haupt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper examines gradient flow dynamics of two-homogeneous neural networks
for small initializations, where all weights are initialized near the origin.
For both square and logistic losses, it is shown that for sufficiently small
initializations, the gradient flow dynamics spend sufficient time in the
neighborhood of the origin to allow the weights of the neural network to
approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a
neural correlation function that quantifies the correlation between the output
of the neural network and corresponding labels in the training data set. For
square loss, it has been observed that neural networks undergo saddle-to-saddle
dynamics when initialized close to the origin. Motivated by this, this paper
also shows a similar directional convergence among weights of small magnitude
in the neighborhood of certain saddle points.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09230" title="Abstract">arXiv:2402.09230</a> [<a href="/pdf/2402.09230" title="Download PDF">pdf</a>, <a href="/ps/2402.09230" title="Download PostScript">ps</a>, <a href="/format/2402.09230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context Composing for Full Line Code Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Semenkin%2C+A">Anton Semenkin</a>, 
<a href="/search/cs?searchtype=author&query=Sokolov%2C+Y">Yaroslav Sokolov</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+E">Evgeniia Vu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages. Accepted for publication in the proceedings of ICSE 2024 IDE workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Code Completion is one of the most used Integrated Development Environment
(IDE) features, which affects the everyday life of a software developer. Modern
code completion approaches moved from the composition of several static
analysis-based contributors to pipelines that involve neural networks. This
change allows the proposal of longer code suggestions while maintaining the
relatively short time spent on generation itself. At JetBrains, we put a lot of
effort into perfecting the code completion workflow so it can be both helpful
and non-distracting for a programmer. We managed to ship the Full Line Code
Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing
on hundreds of real Python users. The paper describes our approach to context
composing for the Transformer model that is a core of the feature's
implementation. In addition to that, we share our next steps to improve the
feature and emphasize the importance of several research aspects in the area.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09231" title="Abstract">arXiv:2402.09231</a> [<a href="/pdf/2402.09231" title="Download PDF">pdf</a>, <a href="/format/2402.09231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Premature Convergence in Co-optimization of Morphology and  Control in Evolved Virtual Soft Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mertan%2C+A">Alican Mertan</a>, 
<a href="/search/cs?searchtype=author&query=Cheney%2C+N">Nick Cheney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This preprint is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). The Version of Record of this contribution is published in Proceedings of Genetic Programming - 27th European Conference, EuroGP 2024, Part of EvoStar
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Evolving virtual creatures is a field with a rich history and recently it has
been getting more attention, especially in the soft robotics domain. The
compliance of soft materials endows soft robots with complex behavior, but it
also makes their design process unintuitive and in need of automated design.
Despite the great interest, evolved virtual soft robots lack the complexity,
and co-optimization of morphology and control remains a challenging problem.
Prior work identifies and investigates a major issue with the co-optimization
process -- fragile co-adaptation of brain and body resulting in premature
convergence of morphology. In this work, we expand the investigation of this
phenomenon by comparing learnable controllers with proprioceptive observations
and fixed controllers without any observations, whereas in the latter case, we
only have the optimization of the morphology. Our experiments in two morphology
spaces and two environments that vary in complexity show, concrete examples of
the existence of high-performing regions in the morphology space that are not
able to be discovered during the co-optimization of the morphology and control,
yet exist and are easily findable when optimizing morphologies alone. Thus this
work clearly demonstrates and characterizes the challenges of optimizing
morphology during co-optimization. Based on these results, we propose a new
body-centric framework to think about the co-optimization problem which helps
us understand the issue from a search perspective. We hope the insights we
share with this work attract more attention to the problem and help us to
enable efficient brain-body co-optimization.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09232" title="Abstract">arXiv:2402.09232</a> [<a href="/pdf/2402.09232" title="Download PDF">pdf</a>, <a href="/ps/2402.09232" title="Download PostScript">ps</a>, <a href="/format/2402.09232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterated Straight-Line Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navarro%2C+G">Gonzalo Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Urbina%2C+C">Cristian Urbina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version of the article includes the proofs omitted from LATIN24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We explore an extension to straight-line programs (SLPs) that outperforms,
for some text families, the measure $\delta$ based on substring complexity, a
lower bound for most measures and compressors exploiting repetitiveness (which
are crucial in areas like Bioinformatics). The extension, called iterated SLPs
(ISLPs), allows rules of the form $A \rightarrow \Pi_{i=k_1}^{k_2}
B_1^{i^{c_1}}\cdots B_t^{i^{c_t}}$, for which we show how to extract any
substring of length $\lambda$, from the represented text $T[1.. n]$, in time
$O(\lambda + \log^2 n\log\log n)$. This is the first compressed representation
for repetitive texts breaking $\delta$ while, at the same time, supporting
direct access to arbitrary text symbols in polylogarithmic time. As a
byproduct, we extend Ganardi et al.'s technique to balance any SLP (so it has a
derivation tree of logarithmic height) to a wide generalization of SLPs,
including ISLPs.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09233" title="Abstract">arXiv:2402.09233</a> [<a href="/pdf/2402.09233" title="Download PDF">pdf</a>, <a href="/format/2402.09233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Realization of a Benchmarking Testbed for Evaluating  Autonomous Platooning Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaham%2C+M">Michael Shaham</a>, 
<a href="/search/cs?searchtype=author&query=Ranjan%2C+R">Risha Ranjan</a>, 
<a href="/search/cs?searchtype=author&query=Kirda%2C+E">Engin Kirda</a>, 
<a href="/search/cs?searchtype=author&query=Padir%2C+T">Taskin Padir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in International Symposium on Experimental Robotics, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">Autonomous vehicle platoons present near- and long-term opportunities to
enhance operational efficiencies and save lives. The past 30 years have seen
rapid development in the autonomous driving space, enabling new technologies
that will alleviate the strain placed on human drivers and reduce vehicle
emissions. This paper introduces a testbed for evaluating and benchmarking
platooning algorithms on 1/10th scale vehicles with onboard sensors. To
demonstrate the testbed's utility, we evaluate three algorithms, linear
feedback and two variations of distributed model predictive control, and
compare their results on a typical platooning scenario where the lead vehicle
tracks a reference trajectory that changes speed multiple times. We validate
our algorithms in simulation to analyze the performance as the platoon size
increases, and find that the distributed model predictive control algorithms
outperform linear feedback on hardware and in simulation.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09234" title="Abstract">arXiv:2402.09234</a> [<a href="/pdf/2402.09234" title="Download PDF">pdf</a>, <a href="/format/2402.09234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Hierarchical Surrogate Learning for Structural Dynamics of  Automotive Crashworthiness Using Graph Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kneifl%2C+J">Jonas Kneifl</a>, 
<a href="/search/cs?searchtype=author&query=Fehr%2C+J">J&#xf6;rg Fehr</a>, 
<a href="/search/cs?searchtype=author&query=Brunton%2C+S+L">Steven L. Brunton</a>, 
<a href="/search/cs?searchtype=author&query=Kutz%2C+J+N">J. Nathan Kutz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">Crash simulations play an essential role in improving vehicle safety, design
optimization, and injury risk estimation. Unfortunately, numerical solutions of
such problems using state-of-the-art high-fidelity models require significant
computational effort. Conventional data-driven surrogate modeling approaches
create low-dimensional embeddings for evolving the dynamics in order to
circumvent this computational effort. Most approaches directly operate on
high-resolution data obtained from numerical discretization, which is both
costly and complicated for mapping the flow of information over large spatial
distances. Furthermore, working with a fixed resolution prevents the adaptation
of surrogate models to environments with variable computing capacities,
different visualization resolutions, and different accuracy requirements. We
thus propose a multi-hierarchical framework for structurally creating a series
of surrogate models for a kart frame, which is a good proxy for
industrial-relevant crash simulations, at different levels of resolution. For
multiscale phenomena, macroscale features are captured on a coarse surrogate,
whereas microscale effects are resolved by finer ones. The learned behavior of
the individual surrogates is passed from coarse to finer levels through
transfer learning. In detail, we perform a mesh simplification on the kart
model to obtain multi-resolution representations of it. We then train a
graph-convolutional neural network-based surrogate that learns
parameter-dependent low-dimensional latent dynamics on the coarsest
representation. Subsequently, another, similarly structured surrogate is
trained on the residual of the first surrogate using a finer resolution. This
step can be repeated multiple times. By doing so, we construct multiple
surrogates for the same system with varying hardware requirements and
increasing accuracy.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09236" title="Abstract">arXiv:2402.09236</a> [<a href="/pdf/2402.09236" title="Download PDF">pdf</a>, <a href="/format/2402.09236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Interpretable Concepts: Unifying Causal Representation Learning  and Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+G">Goutham Rajendran</a>, 
<a href="/search/cs?searchtype=author&query=Buchholz%2C+S">Simon Buchholz</a>, 
<a href="/search/cs?searchtype=author&query=Aragam%2C+B">Bryon Aragam</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+P">Pradeep Ravikumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">To build intelligent machine learning systems, there are two broad
approaches. One approach is to build inherently interpretable models, as
endeavored by the growing field of causal representation learning. The other
approach is to build highly-performant foundation models and then invest
efforts into understanding how they work. In this work, we relate these two
approaches and study how to learn human-interpretable concepts from data.
Weaving together ideas from both fields, we formally define a notion of
concepts and show that they can be provably recovered from diverse data.
Experiments on synthetic data and large language models show the utility of our
unified approach.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09237" title="Abstract">arXiv:2402.09237</a> [<a href="/pdf/2402.09237" title="Download PDF">pdf</a>, <a href="/format/2402.09237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weatherproofing Retrieval for Localization with Generative AI and  Geometric Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalantidis%2C+Y">Yannis Kalantidis</a>, 
<a href="/search/cs?searchtype=author&query=Sar%C4%B1y%C4%B1ld%C4%B1z%2C+M+B">Mert B&#xfc;lent Sar&#x131;y&#x131;ld&#x131;z</a>, 
<a href="/search/cs?searchtype=author&query=Rezende%2C+R+S">Rafael S. Rezende</a>, 
<a href="/search/cs?searchtype=author&query=Weinzaepfel%2C+P">Philippe Weinzaepfel</a>, 
<a href="/search/cs?searchtype=author&query=Larlus%2C+D">Diane Larlus</a>, 
<a href="/search/cs?searchtype=author&query=Csurka%2C+G">Gabriela Csurka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024. Project Page: <a href="https://europe.naverlabs.com/ret4loc">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">State-of-the-art visual localization approaches generally rely on a first
image retrieval step whose role is crucial. Yet, retrieval often struggles when
facing varying conditions, due to e.g. weather or time of day, with dramatic
consequences on the visual localization accuracy. In this paper, we improve
this retrieval step and tailor it to the final localization task. Among the
several changes we advocate for, we propose to synthesize variants of the
training set images, obtained from generative text-to-image models, in order to
automatically expand the training set towards a number of nameable variations
that particularly hurt visual localization. After expanding the training set,
we propose a training approach that leverages the specificities and the
underlying geometry of this mix of real and synthetic images. We experimentally
show that those changes translate into large improvements for the most
challenging visual localization datasets. Project page:
https://europe.naverlabs.com/ret4loc
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09239" title="Abstract">arXiv:2402.09239</a> [<a href="/pdf/2402.09239" title="Download PDF">pdf</a>, <a href="/format/2402.09239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Training of Temporal GNNs using Nearest Neighbours based Hard  Negatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shubham Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Bedathur%2C+S">Srikanta Bedathur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Temporal graph neural networks Tgnn have exhibited state-of-art performance
in future-link prediction tasks. Training of these TGNNs is enumerated by
uniform random sampling based unsupervised loss. During training, in the
context of a positive example, the loss is computed over uninformative
negatives, which introduces redundancy and sub-optimal performance. In this
paper, we propose modified unsupervised learning of Tgnn, by replacing the
uniform negative sampling with importance-based negative sampling. We
theoretically motivate and define the dynamically computed distribution for a
sampling of negative examples. Finally, using empirical evaluations over three
real-world datasets, we show that Tgnn trained using loss based on proposed
negative sampling provides consistent superior performance.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09240" title="Abstract">arXiv:2402.09240</a> [<a href="/pdf/2402.09240" title="Download PDF">pdf</a>, <a href="/format/2402.09240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Switch EMA: A Free Lunch for Better Flatness and Sharpness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Juanxi Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Ge Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zedong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Weiyang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Baigui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint V1. Source code and models at <a href="https://github.com/Westlake-AI/SEMA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Exponential Moving Average (EMA) is a widely used weight averaging (WA)
regularization to learn flat optima for better generalizations without extra
cost in deep neural network (DNN) optimization. Despite achieving better
flatness, existing WA methods might fall into worse final performances or
require extra test-time computations. This work unveils the full potential of
EMA with a single line of modification, i.e., switching the EMA parameters to
the original model after each epoch, dubbed as Switch EMA (SEMA). From both
theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to
reach generalization optima that better trade-off between flatness and
sharpness. To verify the effectiveness of SEMA, we conduct comparison
experiments with discriminative, generative, and regression tasks on vision and
language datasets, including image classification, self-supervised learning,
object detection and segmentation, image generation, video prediction,
attribute regression, and language modeling. Comprehensive results with popular
optimizers and networks show that SEMA is a free lunch for DNN training by
improving performances and boosting convergence speeds.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09241" title="Abstract">arXiv:2402.09241</a> [<a href="/pdf/2402.09241" title="Download PDF">pdf</a>, <a href="/format/2402.09241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient One-stage Video Object Detection by Exploiting Temporal  Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guanxiong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yang Hua</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+G">Guosheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Robertson%2C+N">Neil Robertson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, one-stage detectors have achieved competitive accuracy and faster
speed compared with traditional two-stage detectors on image data. However, in
the field of video object detection (VOD), most existing VOD methods are still
based on two-stage detectors. Moreover, directly adapting existing VOD methods
to one-stage detectors introduces unaffordable computational costs. In this
paper, we first analyse the computational bottlenecks of using one-stage
detectors for VOD. Based on the analysis, we present a simple yet efficient
framework to address the computational bottlenecks and achieve efficient
one-stage VOD by exploiting the temporal consistency in video frames.
Specifically, our method consists of a location-prior network to filter out
background regions and a size-prior network to skip unnecessary computations on
low-level feature maps for specific frames. We test our method on various
modern one-stage detectors and conduct extensive experiments on the ImageNet
VID dataset. Excellent experimental results demonstrate the superior
effectiveness, efficiency, and compatibility of our method. The code is
available at https://github.com/guanxiongsun/vfe.pytorch.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09242" title="Abstract">arXiv:2402.09242</a> [<a href="/pdf/2402.09242" title="Download PDF">pdf</a>, <a href="/format/2402.09242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pengfei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+W">Weiqing Min</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiajun Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shuqiang Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, accepted by IEEE Transactions on Image Processing (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Food computing brings various perspectives to computer vision like
vision-based food analysis for nutrition and health. As a fundamental task in
food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen
food objects to support real-world scenarios, such as intelligent kitchens and
smart restaurants. Therefore, we first benchmark the task of Zero-Shot Food
Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations.
Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make
synthesized features inseparable. The complexity of food semantic attributes
further makes it more difficult for current ZSD methods to distinguish various
food categories. To address these problems, we propose a novel framework ZSFDet
to tackle fine-grained problems by exploiting the interaction between complex
attributes. Specifically, we model the correlation between food categories and
attributes in ZSFDet by multi-source graphs to provide prior knowledge for
distinguishing fine-grained features. Within ZSFDet, Knowledge-Enhanced Feature
Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g.,
ingredients correlation from knowledge graph) via the multi-source graph
fusion. Conditioned on the fusion of semantic knowledge representation, the
region feature diffusion model in KEFS can generate fine-grained features for
training the effective zero-shot detector. Extensive evaluations demonstrate
the superior performance of our method ZSFDet on FOWA and the widely-used food
dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP
compared with the strong baseline RRFS. Further experiments on PASCAL VOC and
MS COCO prove that enhancement of the semantic knowledge can also improve the
performance on general ZSD. Code and dataset are available at
https://github.com/LanceZPF/KEFS.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09246" title="Abstract">arXiv:2402.09246</a> [<a href="/pdf/2402.09246" title="Download PDF">pdf</a>, <a href="/format/2402.09246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who Plays First? Optimizing the Order of Play in Stackelberg Games with  Many Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haimin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Dragotto%2C+G">Gabriele Dragotto</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kaiqu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Stellato%2C+B">Bartolomeo Stellato</a>, 
<a href="/search/cs?searchtype=author&query=Fisac%2C+J+F">Jaime F. Fisac</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">We consider the multi-agent spatial navigation problem of computing the
socially optimal order of play, i.e., the sequence in which the agents commit
to their decisions, and its associated equilibrium in an N-player Stackelberg
trajectory game. We model this problem as a mixed-integer optimization problem
over the space of all possible Stackelberg games associated with the order of
play's permutations. To solve the problem, we introduce Branch and Play (B&amp;P),
an efficient and exact algorithm that provably converges to a socially optimal
order of play and its Stackelberg equilibrium. As a subroutine for B&amp;P, we
employ and extend sequential trajectory planning, i.e., a popular multi-agent
control approach, to scalably compute valid local Stackelberg equilibria for
any given order of play. We demonstrate the practical utility of B&amp;P to
coordinate air traffic control, swarm formation, and delivery vehicle fleets.
We find that B&amp;P consistently outperforms various baselines, and computes the
socially optimal equilibrium.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09247" title="Abstract">arXiv:2402.09247</a> [<a href="/pdf/2402.09247" title="Download PDF">pdf</a>, <a href="/format/2402.09247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Momentum Approximation in Asynchronous Private Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Congzheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chitnis%2C+M">Mona Chitnis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Asynchronous protocols have been shown to improve the scalability of
federated learning (FL) with a massive number of clients. Meanwhile,
momentum-based methods can achieve the best model quality in synchronous FL.
However, naively applying momentum in asynchronous FL algorithms leads to
slower convergence and degraded model performance. It is still unclear how to
effective combinie these two techniques together to achieve a win-win. In this
paper, we find that asynchrony introduces implicit bias to momentum updates. In
order to address this problem, we propose momentum approximation that minimizes
the bias by finding an optimal weighted average of all historical model
updates. Momentum approximation is compatible with secure aggregation as well
as differential privacy, and can be easily integrated in production FL systems
with a minor communication and storage cost. We empirically demonstrate that on
benchmark FL datasets, momentum approximation can achieve $1.15
\textrm{--}4\times$ speed up in convergence compared to existing asynchronous
FL optimizers with momentum.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09249" title="Abstract">arXiv:2402.09249</a> [<a href="/pdf/2402.09249" title="Download PDF">pdf</a>, <a href="/format/2402.09249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Relationship: Transformative Adaptive Activation Functions  in Comparison to Other Activation Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kunc%2C+V">Vladim&#xed;r Kunc</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Neural networks are the state-of-the-art approach for many tasks and the
activation function is one of the main building blocks that allow such
performance. Recently, a novel transformative adaptive activation function
(TAAF) allowing for any vertical and horizontal translation and scaling was
proposed. This work sets the TAAF into the context of other activation
functions. It shows that the TAAFs generalize over 50 existing activation
functions and utilize similar concepts as over 70 other activation functions,
underscoring the versatility of TAAFs. This comprehensive exploration positions
TAAFs as a promising and adaptable addition to neural networks.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09252" title="Abstract">arXiv:2402.09252</a> [<a href="/pdf/2402.09252" title="Download PDF">pdf</a>, <a href="/format/2402.09252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An asymptotic-preserving scheme for Euler equations I: non-ideal gases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Orlando%2C+G">Giuseppe Orlando</a>, 
<a href="/search/math?searchtype=author&query=Bonaventura%2C+L">Luca Bonaventura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">{We analyze a general Implicit-Explicit (IMEX) time discretization for the
compressible Euler equations of gas dynamics, showing that they are
asymptotic-preserving (AP) in the low Mach number limit. The analysis is
carried out for a general equation of state (EOS). We consider both a single
asymptotic length scale and two length scales. We then show that, when coupling
these time discretizations with a Discontinuous Galerkin (DG) space
discretization with appropriate fluxes, an all Mach number numerical method is
obtained. A number of relevant benchmarks for ideal gases and their non-trivial
extension to non-ideal EOS validate the performed analysis.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09257" title="Abstract">arXiv:2402.09257</a> [<a href="/pdf/2402.09257" title="Download PDF">pdf</a>, <a href="/format/2402.09257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TDViT: Temporal Dilated Video Transformer for Dense Video Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guanxiong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yang Hua</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+G">Guosheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Robertson%2C+N">Neil Robertson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep video models, for example, 3D CNNs or video transformers, have achieved
promising performance on sparse video tasks, i.e., predicting one result per
video. However, challenges arise when adapting existing deep video models to
dense video tasks, i.e., predicting one result per frame. Specifically, these
models are expensive for deployment, less effective when handling redundant
frames, and difficult to capture long-range temporal correlations. To overcome
these issues, we propose a Temporal Dilated Video Transformer (TDViT) that
consists of carefully designed temporal dilated transformer blocks (TDTB). TDTB
can efficiently extract spatiotemporal representations and effectively
alleviate the negative effect of temporal redundancy. Furthermore, by using
hierarchical TDTBs, our approach obtains an exponentially expanded temporal
receptive field and therefore can model long-range dynamics. Extensive
experiments are conducted on two different dense video benchmarks, i.e.,
ImageNet VID for video object detection and YouTube VIS for video instance
segmentation. Excellent experimental results demonstrate the superior
efficiency, effectiveness, and compatibility of our method. The code is
available at https://github.com/guanxiongsun/vfe.pytorch.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09259" title="Abstract">arXiv:2402.09259</a> [<a href="/pdf/2402.09259" title="Download PDF">pdf</a>, <a href="/format/2402.09259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SyntaxShap: Syntax-aware Explainability Method for Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amara%2C+K">Kenza Amara</a>, 
<a href="/search/cs?searchtype=author&query=Sevastjanova%2C+R">Rita Sevastjanova</a>, 
<a href="/search/cs?searchtype=author&query=El-Assady%2C+M">Mennatallah El-Assady</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">To harness the power of large language models in safety-critical domains we
need to ensure the explainability of their predictions. However, despite the
significant attention to model interpretability, there remains an unexplored
domain in explaining sequence-to-sequence tasks using methods tailored for
textual data. This paper introduces SyntaxShap, a local, model-agnostic
explainability method for text generation that takes into consideration the
syntax in the text data. The presented work extends Shapley values to account
for parsing-based syntactic dependencies. Taking a game theoric approach,
SyntaxShap only considers coalitions constraint by the dependency tree. We
adopt a model-based evaluation to compare SyntaxShap and its weighted form to
state-of-the-art explainability methods adapted to text generation tasks, using
diverse metrics including faithfulness, complexity, coherency, and semantic
alignment of the explanations to the model. We show that our syntax-aware
method produces explanations that help build more faithful, coherent, and
interpretable explanations for predictions by autoregressive models.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09260" title="Abstract">arXiv:2402.09260</a> [<a href="/pdf/2402.09260" title="Download PDF">pdf</a>, <a href="/format/2402.09260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Experience of LGBTQ+ People Using Large Language Model  Based Chatbots for Mental Health Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zilin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Y">Yiyang Mei</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yinru Long</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhaoyuan Su</a>, 
<a href="/search/cs?searchtype=author&query=Gajos%2C+K+Z">Krzysztof Z. Gajos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">LGBTQ+ individuals are increasingly turning to chatbots powered by large
language models (LLMs) to meet their mental health needs. However, little
research has explored whether these chatbots can adequately and safely provide
tailored support for this demographic. We interviewed 18 LGBTQ+ and 13
non-LGBTQ+ participants about their experiences with LLM-based chatbots for
mental health needs. LGBTQ+ participants relied on these chatbots for mental
health support, likely due to an absence of support in real life. Notably,
while LLMs offer prompt support, they frequently fall short in grasping the
nuances of LGBTQ-specific challenges. Although fine-tuning LLMs to address
LGBTQ+ needs can be a step in the right direction, it isn't the panacea. The
deeper issue is entrenched in societal discrimination. Consequently, we call on
future researchers and designers to look beyond mere technical refinements and
advocate for holistic strategies that confront and counteract the societal
biases burdening the LGBTQ+ community.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09262" title="Abstract">arXiv:2402.09262</a> [<a href="/pdf/2402.09262" title="Download PDF">pdf</a>, <a href="/format/2402.09262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Royer%2C+C">Corentin Royer</a>, 
<a href="/search/cs?searchtype=author&query=Menze%2C+B">Bjoern Menze</a>, 
<a href="/search/cs?searchtype=author&query=Sekuboyina%2C+A">Anjany Sekuboyina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce MultiMedEval, an open-source toolkit for fair and reproducible
evaluation of large, medical vision-language models (VLM). MultiMedEval
comprehensively assesses the models' performance on a broad array of six
multi-modal tasks, conducted over 23 datasets, and spanning over 11 medical
domains. The chosen tasks and performance metrics are based on their widespread
adoption in the community and their diversity, ensuring a thorough evaluation
of the model's overall generalizability. We open-source a Python toolkit
(github.com/corentin-ryr/MultiMedEval) with a simple interface and setup
process, enabling the evaluation of any VLM in just a few lines of code. Our
goal is to simplify the intricate landscape of VLM evaluation, thus promoting
fair and uniform benchmarking of future models.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09263" title="Abstract">arXiv:2402.09263</a> [<a href="/pdf/2402.09263" title="Download PDF">pdf</a>, <a href="/ps/2402.09263" title="Download PostScript">ps</a>, <a href="/format/2402.09263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-Aware Transient Stability-Constrained Preventive Redispatch:  A Distributional Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhengcheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Teng%2C+F">Fei Teng</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yanzhen Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Q">Qinglai Guo</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+H">Hongbin Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages,10 figures,submittd to IEEE Transactions on Power Systems on 01/08/2023, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Transient stability-constrained preventive redispatch plays a crucial role in
ensuring power system security and stability. Since redispatch strategies need
to simultaneously satisfy complex transient constraints and the economic need,
model-based formulation and optimization become extremely challenging. In
addition, the increasing uncertainty and variability introduced by renewable
sources start to drive the system stability consideration from deterministic to
probabilistic, which further exaggerates the complexity. In this paper, a Graph
neural network guided Distributional Deep Reinforcement Learning (GD2RL) method
is proposed, for the first time, to solve the uncertainty-aware transient
stability-constrained preventive redispatch problem. First, a graph neural
network-based transient simulator is trained by supervised learning to
efficiently generate post-contingency rotor angle curves with the steady-state
and contingency as inputs, which serves as a feature extractor for operating
states and a surrogate time-domain simulator during the environment interaction
for reinforcement learning. Distributional deep reinforcement learning with
explicit uncertainty distribution of system operational conditions is then
applied to generate the redispatch strategy to balance the user-specified
probabilistic stability performance and economy preferences. The full
distribution of the post-control transient stability index is directly provided
as the output. Case studies on the modified New England 39-bus system validate
the proposed method.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09264" title="Abstract">arXiv:2402.09264</a> [<a href="/pdf/2402.09264" title="Download PDF">pdf</a>, <a href="/format/2402.09264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+H">Hong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+Y+D">Young D. Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Dong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+N">Nhat Pham</a>, 
<a href="/search/cs?searchtype=author&query=Qendro%2C+L">Lorena Qendro</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Tam Vu</a>, 
<a href="/search/cs?searchtype=author&query=Mascolo%2C+C">Cecilia Mascolo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Traditional machine learning techniques are prone to generating inaccurate
predictions when confronted with shifts in the distribution of data between the
training and testing phases. This vulnerability can lead to severe
consequences, especially in applications such as mobile healthcare. Uncertainty
estimation has the potential to mitigate this issue by assessing the
reliability of a model's output. However, existing uncertainty estimation
techniques often require substantial computational resources and memory, making
them impractical for implementation on microcontrollers (MCUs). This limitation
hinders the feasibility of many important on-device wearable event detection
(WED) applications, such as heart attack detection.
<br />In this paper, we present UR2M, a novel Uncertainty and Resource-aware event
detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware
WED based on evidential theory for accurate event detection and reliable
uncertainty estimation; (ii) introduce a cascade ML framework to achieve
efficient model inference via early exits, by sharing shallower model layers
among different event models; (iii) optimize the deployment of the model and
MCU library for system efficiency. We conducted extensive experiments and
compared UR2M to traditional uncertainty baselines using three wearable
datasets. Our results demonstrate that UR2M achieves up to 864% faster
inference speed, 857% energy-saving for uncertainty estimation, 55% memory
saving on two popular MCUs, and a 22% improvement in uncertainty quantification
performance.
<br />UR2M can be deployed on a wide range of MCUs, significantly expanding
real-time and reliable WED applications.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09265" title="Abstract">arXiv:2402.09265</a> [<a href="/pdf/2402.09265" title="Download PDF">pdf</a>, <a href="/format/2402.09265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational Complexity of Preferred Subset Repairs on Data-Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pardal%2C+N">Nina Pardal</a>, 
<a href="/search/cs?searchtype=author&query=Cifuentes%2C+S">Santiago Cifuentes</a>, 
<a href="/search/cs?searchtype=author&query=Pin%2C+E">Edwin Pin</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M+V">Maria Vanina Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Abriola%2C+S">Sergio Abriola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 2 figures, Appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">The problem of repairing inconsistent knowledge bases has a long history
within the communities of database theory and knowledge representation and
reasoning, especially from the perspective of structured data. However, as the
data available in real-world domains becomes more complex and interconnected,
the need naturally arises for developing new types of repositories,
representation languages, and semantics, to allow for more suitable ways to
query and reason about it. Graph databases provide an effective way to
represent relationships among semi-structured data, and allow processing and
querying these connections efficiently. In this work, we focus on the problem
of computing prioritized repairs over graph databases with data values, using a
notion of consistency based on Reg-GXPath expressions as integrity constraints.
We present several preference criteria based on the standard subset repair
semantics, incorporating weights, multisets, and set-based priority levels. We
study the most common repairing tasks, showing that it is possible to maintain
the same computational complexity as in the case where no preference criterion
is available for exploitation. To complete the picture, we explore the
complexity of consistent query answering in this setting and obtain tight lower
and upper bounds for all the preference criteria introduced.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09266" title="Abstract">arXiv:2402.09266</a> [<a href="/pdf/2402.09266" title="Download PDF">pdf</a>, <a href="/format/2402.09266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning in management of precautionary closures caused by  lipophilic biotoxins
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Molares-Ulloa%2C+A">Andres Molares-Ulloa</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Blanco%2C+E">Enrique Fernandez-Blanco</a>, 
<a href="/search/cs?searchtype=author&query=Pazos%2C+A">Alejandro Pazos</a>, 
<a href="/search/cs?searchtype=author&query=Rivero%2C+D">Daniel Rivero</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computers and Electronics in Agriculture, 197, 106956. (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Mussel farming is one of the most important aquaculture industries. The main
risk to mussel farming is harmful algal blooms (HABs), which pose a risk to
human consumption. In Galicia, the Spanish main producer of cultivated mussels,
the opening and closing of the production areas is controlled by a monitoring
program. In addition to the closures resulting from the presence of toxicity
exceeding the legal threshold, in the absence of a confirmatory sampling and
the existence of risk factors, precautionary closures may be applied. These
decisions are made by experts without the support or formalisation of the
experience on which they are based. Therefore, this work proposes a predictive
model capable of supporting the application of precautionary closures.
Achieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and
0.75 respectively, the kNN algorithm has provided the best results. This allows
the creation of a system capable of helping in complex situations where
forecast errors are more common.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09267" title="Abstract">arXiv:2402.09267</a> [<a href="/pdf/2402.09267" title="Download PDF">pdf</a>, <a href="/format/2402.09267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via  Self-Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Baolin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Ye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingyan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lifeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linfeng Song</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+H">Haitao Mi</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H">Helen Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite showing increasingly human-like abilities, large language models
(LLMs) often struggle with factual inaccuracies, i.e. "hallucinations", even
when they hold relevant knowledge. To address these hallucinations, current
approaches typically necessitate high-quality human factuality annotations. In
this work, we explore Self-Alignment for Factuality, where we leverage the
self-evaluation capability of an LLM to provide training signals that steer the
model towards factuality. Specifically, we incorporate Self-Eval, a
self-evaluation component, to prompt an LLM to validate the factuality of its
own generated responses solely based on its internal knowledge. Additionally,
we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's
self-evaluation ability by improving the model's confidence estimation and
calibration. We then utilize these self-annotated responses to fine-tune the
model via Direct Preference Optimization algorithm. We show that the proposed
self-alignment approach substantially enhances factual accuracy over Llama
family models across three key knowledge-intensive tasks on TruthfulQA and
BioGEN.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09268" title="Abstract">arXiv:2402.09268</a> [<a href="/pdf/2402.09268" title="Download PDF">pdf</a>, <a href="/format/2402.09268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers, parallel computation, and logarithmic depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanford%2C+C">Clayton Sanford</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+D">Daniel Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Telgarsky%2C+M">Matus Telgarsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages, 19 figures, code available at <a href="https://github.com/chsanford/hop-induction-heads">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We show that a constant number of self-attention layers can efficiently
simulate, and be simulated by, a constant number of communication rounds of
Massively Parallel Computation. As a consequence, we show that logarithmic
depth is sufficient for transformers to solve basic computational tasks that
cannot be efficiently solved by several other neural sequence models and
sub-quadratic transformer approximations. We thus establish parallelism as a
key distinguishing property of transformers.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09269" title="Abstract">arXiv:2402.09269</a> [<a href="/pdf/2402.09269" title="Download PDF">pdf</a>, <a href="/format/2402.09269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wo%C5%BAniak%2C+S">Stanis&#x142;aw Wo&#x17a;niak</a>, 
<a href="/search/cs?searchtype=author&query=Koptyra%2C+B">Bart&#x142;omiej Koptyra</a>, 
<a href="/search/cs?searchtype=author&query=Janz%2C+A">Arkadiusz Janz</a>, 
<a href="/search/cs?searchtype=author&query=Kazienko%2C+P">Przemys&#x142;aw Kazienko</a>, 
<a href="/search/cs?searchtype=author&query=Koco%C5%84%2C+J">Jan Koco&#x144;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have significantly advanced Natural Language
Processing (NLP) tasks in recent years. However, their universal nature poses
limitations in scenarios requiring personalized responses, such as
recommendation systems and chatbots. This paper investigates methods to
personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on
subjective tasks. Results demonstrate that personalized fine-tuning improves
model reasoning compared to non-personalized models. Experiments on datasets
for emotion recognition and hate speech detection show consistent performance
gains with personalized methods across different LLM architectures. These
findings underscore the importance of personalization for enhancing LLM
capabilities in subjective text perception tasks.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09270" title="Abstract">arXiv:2402.09270</a> [<a href="/pdf/2402.09270" title="Download PDF">pdf</a>, <a href="/format/2402.09270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Window-Based Event Denoising with Spatiotemporal Correlation  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Huachen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinjian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Q">Qibin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Weisheng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guangming Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Previous deep learning-based event denoising methods mostly suffer from poor
interpretability and difficulty in real-time processing due to their complex
architecture designs. In this paper, we propose window-based event denoising,
which simultaneously deals with a stack of events while existing element-based
denoising focuses on one event each time. Besides, we give the theoretical
analysis based on probability distributions in both temporal and spatial
domains to improve interpretability. In temporal domain, we use timestamp
deviations between processing events and central event to judge the temporal
correlation and filter out temporal-irrelevant events. In spatial domain, we
choose maximum a posteriori (MAP) to discriminate real-world event and noise,
and use the learned convolutional sparse coding to optimize the objective
function. Based on the theoretical analysis, we build Temporal Window (TW)
module and Soft Spatial Feature Embedding (SSFE) module to process temporal and
spatial information separately, and construct a novel multi-scale window-based
event denoising network, named MSDNet. The high denoising accuracy and fast
running speed of our MSDNet enables us to achieve real-time denoising in
complex scenes. Extensive experimental results verify the effectiveness and
robustness of our MSDNet. Our algorithm can remove event noise effectively and
efficiently and improve the performance of downstream tasks.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09271" title="Abstract">arXiv:2402.09271</a> [<a href="/pdf/2402.09271" title="Download PDF">pdf</a>, <a href="/format/2402.09271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Machine Learning techniques in the management of harmful algal  blooms impact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Molares-Ulloa%2C+A">Andres Molares-Ulloa</a>, 
<a href="/search/cs?searchtype=author&query=Rivero%2C+D">Daniel Rivero</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz%2C+J+G">Jesus Gil Ruiz</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Blanco%2C+E">Enrique Fernandez-Blanco</a>, 
<a href="/search/cs?searchtype=author&query=de-la-Fuente-Valent%C3%ADn%2C+L">Luis de-la-Fuente-Valent&#xed;n</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computers and Electronics in Agriculture, 211, 107988. (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Harmful algal blooms (HABs) are episodes of high concentrations of algae that
are potentially toxic for human consumption. Mollusc farming can be affected by
HABs because, as filter feeders, they can accumulate high concentrations of
marine biotoxins in their tissues. To avoid the risk to human consumption,
harvesting is prohibited when toxicity is detected. At present, the closure of
production areas is based on expert knowledge and the existence of a predictive
model would help when conditions are complex and sampling is not possible.
Although the concentration of toxin in meat is the method most commonly used by
experts in the control of shellfish production areas, it is rarely used as a
target by automatic prediction models. This is largely due to the irregularity
of the data due to the established sampling programs. As an alternative, the
activity status of production areas has been proposed as a target variable
based on whether mollusc meat has a toxicity level below or above the legal
limit. This new option is the most similar to the actual functioning of the
control of shellfish production areas. For this purpose, we have made a
comparison between hybrid machine learning models like Neural-Network-Adding
Bootstrap (BAGNET) and Discriminative Nearest Neighbor Classification (SVM-KNN)
when estimating the state of production areas. The study has been carried out
in several estuaries with different levels of complexity in the episodes of
algal blooms to demonstrate the generalization capacity of the models in bloom
detection. As a result, we could observe that, with an average recall value of
93.41% and without dropping below 90% in any of the estuaries, BAGNET
outperforms the other models both in terms of results and robustness.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09272" title="Abstract">arXiv:2402.09272</a> [<a href="/pdf/2402.09272" title="Download PDF">pdf</a>, <a href="/format/2402.09272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Insights and caveats from mining local and global temporal motifs in  cryptocurrency transaction networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arnold%2C+N+A">Naomi A. Arnold</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+P">Peijie Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ba%2C+C+T">Cheick Tidiane Ba</a>, 
<a href="/search/cs?searchtype=author&query=Steer%2C+B">Ben Steer</a>, 
<a href="/search/cs?searchtype=author&query=Mondragon%2C+R">Raul Mondragon</a>, 
<a href="/search/cs?searchtype=author&query=Cuadrado%2C+F">Felix Cuadrado</a>, 
<a href="/search/cs?searchtype=author&query=Lambiotte%2C+R">Renaud Lambiotte</a>, 
<a href="/search/cs?searchtype=author&query=Clegg%2C+R+G">Richard G. Clegg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Distributed ledger technologies have opened up a wealth of fine-grained
transaction data from cryptocurrencies like Bitcoin and Ethereum. This allows
research into problems like anomaly detection, anti-money laundering, pattern
mining and activity clustering (where data from traditional currencies is
rarely available). The formalism of temporal networks offers a natural way of
representing this data and offers access to a wealth of metrics and models.
However, the large scale of the data presents a challenge using standard graph
analysis techniques. We use temporal motifs to analyse two Bitcoin datasets and
one NFT dataset, using sequences of three transactions and up to three users.
We show that the commonly used technique of simply counting temporal motifs
over all users and all time can give misleading conclusions. Here we also study
the motifs contributed by each user and discover that the motif distribution is
heavy-tailed and that the key players have diverse motif signatures. We study
the motifs that occur in different time periods and find events and anomalous
activity that cannot be seen just by a count on the whole dataset. Studying
motif completion time reveals dynamics driven by human behaviour as well as
algorithmic behaviour.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09275" title="Abstract">arXiv:2402.09275</a> [<a href="/pdf/2402.09275" title="Download PDF">pdf</a>, <a href="/ps/2402.09275" title="Download PostScript">ps</a>, <a href="/format/2402.09275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The socialisation of the adolescent who carries out team sports: a  transversal study of centrality with a social network analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marqu%C3%A9s-S%C3%A1nchez%2C+P">Pilar Marqu&#xe9;s-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez%2C+M+D+C">Mar&#xed;a Dolores Calvo S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Arias%2C+N">Natalia Arias</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> BMJ Open, 2021, Volume 11, ID e042773
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Objectives: This study analyzed adolescent physical activity, its link to
overweight, and the social network structure in group sports participants,
focusing on centrality measures.
<br />Setting: Conducted in 11 classrooms across 5 schools in Ponferrada, Spain.
<br />Participants: Included 235 adolescents (49.4% female), categorized as normal
weight or overweight.
<br />Methods: The Physical Activity Questionnaire for Adolescents (PAQ-A) assessed
physical activity levels. Social network analysis evaluated centrality in
varying contact degrees.
<br />Results: 30.2% were overweight. Males scored higher in PAQ-A and were more
likely to engage in group sports. No significant correlation was found between
physical activity and weight in the total sample. However, overweight females
reported higher exercise levels. Centrality analysis showed gender differences;
women in group sports had lower centrality, whereas men had higher.
<br />Conclusions: The study highlights the importance of gender and social network
centrality in designing future strategies, considering peer interaction
intensity
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09277" title="Abstract">arXiv:2402.09277</a> [<a href="/pdf/2402.09277" title="Download PDF">pdf</a>, <a href="/format/2402.09277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Modular Deep Learning-based Approach for Diffuse Optical Tomography  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Benfenati%2C+A">Alessandro Benfenati</a>, 
<a href="/search/math?searchtype=author&query=Causin%2C+P">Paola Causin</a>, 
<a href="/search/math?searchtype=author&query=Quinteri%2C+M">Martina Quinteri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Medical imaging is nowadays a pillar in diagnostics and therapeutic
follow-up. Current research tries to integrate established - but ionizing -
tomographic techniques with technologies offering reduced radiation exposure.
Diffuse Optical Tomography (DOT) uses non-ionizing light in the Near-Infrared
(NIR) window to reconstruct optical coefficients in living beings, providing
functional indications about the composition of the investigated organ/tissue.
Due to predominant light scattering at NIR wavelengths, DOT reconstruction is,
however, a severely ill-conditioned inverse problem. Conventional
reconstruction approaches show severe weaknesses when dealing also with mildly
complex cases and/or are computationally very intensive. In this work we
explore deep learning techniques for DOT inversion. Namely, we propose a fully
data-driven approach based on a modularity concept: first data and originating
signal are separately processed via autoencoders, then the corresponding
low-dimensional latent spaces are connected via a bridging network which acts
at the same time as a learned regularizer.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09281" title="Abstract">arXiv:2402.09281</a> [<a href="/pdf/2402.09281" title="Download PDF">pdf</a>, <a href="/format/2402.09281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synergistic eigenanalysis of covariance and Hessian matrices for  enhanced binary classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hartoyo%2C+A">Agus Hartoyo</a>, 
<a href="/search/cs?searchtype=author&query=Argasi%C5%84ski%2C+J">Jan Argasi&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Trenk%2C+A">Aleksandra Trenk</a>, 
<a href="/search/cs?searchtype=author&query=Przybylska%2C+K">Kinga Przybylska</a>, 
<a href="/search/cs?searchtype=author&query=B%C5%82asiak%2C+A">Anna B&#x142;asiak</a>, 
<a href="/search/cs?searchtype=author&query=Crimi%2C+A">Alessandro Crimi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Covariance and Hessian matrices have been analyzed separately in the
literature for classification problems. However, integrating these matrices has
the potential to enhance their combined power in improving classification
performance. We present a novel approach that combines the eigenanalysis of a
covariance matrix evaluated on a training set with a Hessian matrix evaluated
on a deep learning model to achieve optimal class separability in binary
classification tasks. Our approach is substantiated by formal proofs that
establish its capability to maximize between-class mean distance and minimize
within-class variances. By projecting data into the combined space of the most
relevant eigendirections from both matrices, we achieve optimal class
separability as per the linear discriminant analysis (LDA) criteria. Empirical
validation across neural and health datasets consistently supports our
theoretical framework and demonstrates that our method outperforms established
methods. Our method stands out by addressing both LDA criteria, unlike PCA and
the Hessian method, which predominantly emphasize one criterion each. This
comprehensive approach captures intricate patterns and relationships, enhancing
classification performance. Furthermore, through the utilization of both LDA
criteria, our method outperforms LDA itself by leveraging higher-dimensional
feature spaces, in accordance with Cover's theorem, which favors linear
separability in higher dimensions. Our method also surpasses kernel-based
methods and manifold learning techniques in performance. Additionally, our
approach sheds light on complex DNN decision-making, rendering them
comprehensible within a 2D space.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09282" title="Abstract">arXiv:2402.09282</a> [<a href="/pdf/2402.09282" title="Download PDF">pdf</a>, <a href="/format/2402.09282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Large Language Models for Enhanced NLP Task Performance  through Knowledge Distillation and Optimized Training Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yining Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The integration of Large Language Models (LLMs) like GPT-4 into traditional
Natural Language Processing (NLP) tasks has opened new avenues for enhancing
model performance while reducing the reliance on extensive human annotations.
This paper presents a novel approach that leverages the Chain of Thought (CoT)
prompting technique to distill knowledge from GPT-4, subsequently applying it
to improve the efficiency and effectiveness of a smaller model, BERT, on Named
Entity Recognition (NER) tasks. Our method involves a two-phase training
process: initially employing GPT-4 annotated data for pre-training and then
refining the model with a combination of distilled and original human-annotated
data. The results demonstrate that our mixed-training strategy significantly
outperforms models trained solely on human annotations, achieving superior
F1-scores and showcasing a cost-effective solution for resource-limited or
closed-network settings. The study also discusses the challenges encountered,
such as LLM output variability and the tendency towards hallucinations,
proposing future work directions to enhance prompt design and annotation
selection. Our findings indicate a promising synergy between LLM insights and
traditional NLP techniques, paving the way for more accessible and robust NLP
applications.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09283" title="Abstract">arXiv:2402.09283</a> [<a href="/pdf/2402.09283" title="Download PDF">pdf</a>, <a href="/format/2402.09283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhichen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jing Shao</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09284" title="Abstract">arXiv:2402.09284</a> [<a href="/pdf/2402.09284" title="Download PDF">pdf</a>, <a href="/ps/2402.09284" title="Download PostScript">ps</a>, <a href="/format/2402.09284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Cities and Villages: Concept Review and Implementation  Perspectives in Developing Cities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kabuya%2C+K+I">Kamiba I. Kabuya</a>, 
<a href="/search/cs?searchtype=author&query=Ajayi%2C+O+O">Olasupo O. Ajayi</a>, 
<a href="/search/cs?searchtype=author&query=Bagula%2C+A+B">Anotine B. Bagula</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 Pages, 4 figures, 4 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The "Smart City" (SC) concept has been around for decades with deployment
scenarios revealed in major cities of developed countries. However, while SC
has enhanced the living conditions of city dwellers in the developed world, the
concept is still either missing or poorly deployed in the developing world.
This paper presents a review of the SC concept from the perspective of its
application to cities in developing nations, the opportunities it avails, and
challenges related to its applicability to these cities. Building upon a
systematic review of literature, this paper shows that there are neither
canonical definitions, models or frameworks of references for the SC concept.
This paper also aims to bridge the gap between the "smart city" and "smart
village" concepts, with the expectation of providing a holistic approach to
solving common issues in cities around the world. Drawing inspiration from
other authors, we propose a conceptual model for a SC initiative in Africa and
demonstrate the need to prioritize research and capacity development. We also
discuss the potential opportunities for such SC implementations in sub-Saharan
Africa. As a case study, we consider the city of Lubumbashi in the Democratic
Republic of Congo and discuss ways of making it a smart city by building around
successful smart city initiatives. It is our belief that for Lubumbashi, as
with any other city in Sub-Saharan Africa, the first step to developing a smart
city is to build knowledge and create an intellectual capital.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09286" title="Abstract">arXiv:2402.09286</a> [<a href="/pdf/2402.09286" title="Download PDF">pdf</a>, <a href="/ps/2402.09286" title="Download PostScript">ps</a>, <a href="/format/2402.09286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into  Practice in Gun Violence Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jessica Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cukier%2C+D+M">Dr. Michel Cukier</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+D+J">Dr. Joseph Richardson Jr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Objective: Firearm injury research necessitates using data from
often-exploited vulnerable populations of Black and Brown Americans. In order
to minimize distrust, this study provides a framework for establishing AI trust
and transparency with the general population. Methods: We propose a Model Facts
template that is easily extendable and decomposes accuracy and demographics
into standardized and minimally complex values. This framework allows general
users to assess the validity and biases of a model without diving into
technical model documentation. Examples: We apply the Model Facts template on
two previously published models, a violence risk identification model and a
suicide risk prediction model. We demonstrate the ease of accessing the
appropriate information when the data is structured appropriately. Discussion:
The Model Facts template is limited in its current form to human based data and
biases. Like nutrition facts, it also will require some educational resources
for users to grasp its full utility. Human computer interaction experiments
should be conducted to ensure that the interaction between user interface and
model interface is as desired. Conclusion: The Model Facts label is the first
framework dedicated to establishing trust with end users and general population
consumers. Implementation of Model Facts into firearm injury research will
provide public health practitioners and those impacted by firearm injury
greater faith in the tools the research provides.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09288" title="Abstract">arXiv:2402.09288</a> [<a href="/pdf/2402.09288" title="Download PDF">pdf</a>, <a href="/format/2402.09288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EcoVal: An Efficient Data Valuation Framework for Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarun%2C+A+K">Ayush K Tarun</a>, 
<a href="/search/cs?searchtype=author&query=Chundawat%2C+V+S">Vikram S Chundawat</a>, 
<a href="/search/cs?searchtype=author&query=Mandal%2C+M">Murari Mandal</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H+M">Hong Ming Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bowei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Quantifying the value of data within a machine learning workflow can play a
pivotal role in making more strategic decisions in machine learning
initiatives. The existing Shapley value based frameworks for data valuation in
machine learning are computationally expensive as they require considerable
amount of repeated training of the model to obtain the Shapley value. In this
paper, we introduce an efficient data valuation framework EcoVal, to estimate
the value of data for machine learning models in a fast and practical manner.
Instead of directly working with individual data sample, we determine the value
of a cluster of similar data points. This value is further propagated amongst
all the member cluster points. We show that the overall data value can be
determined by estimating the intrinsic and extrinsic value of each data. This
is enabled by formulating the performance of a model as a \textit{production
function}, a concept which is popularly used to estimate the amount of output
based on factors like labor and capital in a traditional free economic market.
We provide a formal proof of our valuation technique and elucidate the
principles and mechanisms that enable its accelerated performance. We
demonstrate the real-world applicability of our method by showcasing its
effectiveness for both in-distribution and out-of-sample data. This work
addresses one of the core challenges of efficient data valuation at scale in
machine learning models.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09290" title="Abstract">arXiv:2402.09290</a> [<a href="/pdf/2402.09290" title="Download PDF">pdf</a>, <a href="/format/2402.09290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Interpretable Policies in Hindsight-Observable POMDPs through  Partially Supervised Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanier%2C+M">Michael Lanier</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Ying Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+N">Nathan Jacobs</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongjie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep reinforcement learning has demonstrated remarkable achievements across
diverse domains such as video games, robotic control, autonomous driving, and
drug discovery. Common methodologies in partially-observable domains largely
lean on end-to-end learning from high-dimensional observations, such as images,
without explicitly reasoning about true state. We suggest an alternative
direction, introducing the Partially Supervised Reinforcement Learning (PSRL)
framework. At the heart of PSRL is the fusion of both supervised and
unsupervised learning. The approach leverages a state estimator to distill
supervised semantic state information from high-dimensional observations which
are often fully observable at training time. This yields more interpretable
policies that compose state predictions with control. In parallel, it captures
an unsupervised latent representation. These two-the semantic state and the
latent state-are then fused and utilized as inputs to a policy network. This
juxtaposition offers practitioners a flexible and dynamic spectrum: from
emphasizing supervised state information to integrating richer, latent
insights. Extensive experimental results indicate that by merging these dual
representations, PSRL offers a potent balance, enhancing model interpretability
while preserving, and often significantly outperforming, the performance
benchmarks set by traditional methods in terms of reward and convergence speed.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09294" title="Abstract">arXiv:2402.09294</a> [<a href="/pdf/2402.09294" title="Download PDF">pdf</a>, <a href="/format/2402.09294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The impact of load placement on grid resonances during grid restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Anta%2C+A">Adolfo Anta</a>, 
<a href="/search/eess?searchtype=author&query=Cifelli%2C+D">Diego Cifelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">As inverter-based generation is being massively deployed in the grid, these
type of units have to take over the current roles of conventional generation,
including the capability of restoring the grid. In this context, the resonances
of the grid during the first steps of a black start can be concerning, given
that the grid is lightly loaded. Especially relevant are the low frequency
resonances, that may be excited by the harmonic components of the inverter. A
typical strategy to avoid or minimize the effect of such resonances relies on
connecting load banks. This was fairly feasible with conventional generation,
but given the limited ratings of inverters, the amount of load that can be
connected at the beginning is very limited. In this paper we consider the
energization of a transmission line, and investigate the optimal location of a
load along a line in order to maximize the damping in the system. By analysing
the spectral properties as a function of the load location, we formally prove
that placing the load in the middle of the transmission line maximizes the
damping ratio of the first resonance of the system.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09295" title="Abstract">arXiv:2402.09295</a> [<a href="/pdf/2402.09295" title="Download PDF">pdf</a>, <a href="/format/2402.09295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of an Adaptive Safeguarded Newton-Anderson Algorithm with  Applications to Fluid Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dallas%2C+M">Matt Dallas</a>, 
<a href="/search/math?searchtype=author&query=Pollock%2C+S">Sara Pollock</a>, 
<a href="/search/math?searchtype=author&query=Rebholz%2C+L">Leo Rebholz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The purpose of this paper is to develop a practical strategy to accelerate
Newton's method in the vicinity of singular points. We do this by developing an
adaptive safeguarding scheme, which we call gamma-safeguarding, that one can
apply to Anderson accelerated Newton's method when solving problems near
singular points. The key features of adaptive gamma-safeguarding are that it
converges locally for singular problems, and it can detect nonsingular
problems, in which case the Newton-Anderson iterates are scaled towards a
standard Newton step. This leads to faster local convergence compared to both
Newton's method and Newton-Anderson without safeguarding, at no additional
computational cost. We demonstrate three strategies one can use when
implementing Newton-Anderson and gamma-safeguarded Newton-Anderson to solve
parameter-dependent problems near singular points. For our benchmark problems,
we take two parameter-dependent incompressible flow systems: flow in a channel
and Rayleigh-Benard convection.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09299" title="Abstract">arXiv:2402.09299</a> [<a href="/pdf/2402.09299" title="Download PDF">pdf</a>, <a href="/format/2402.09299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trained Without My Consent: Detecting Code Inclusion In Language Models  Trained on Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majdinasab%2C+V">Vahid Majdinasab</a>, 
<a href="/search/cs?searchtype=author&query=Nikanjam%2C+A">Amin Nikanjam</a>, 
<a href="/search/cs?searchtype=author&query=Khomh%2C+F">Foutse Khomh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to TOSEM (ACM Transactions on Software Engineering and Methodology)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Code auditing ensures that the developed code adheres to standards,
regulations, and copyright protection by verifying that it does not contain
code from protected sources. The recent advent of Large Language Models (LLMs)
as coding assistants in the software development process poses new challenges
for code auditing. The dataset for training these models is mainly collected
from publicly available sources. This raises the issue of intellectual property
infringement as developers' codes are already included in the dataset.
Therefore, auditing code developed using LLMs is challenging, as it is
difficult to reliably assert if an LLM used during development has been trained
on specific copyrighted codes, given that we do not have access to the training
datasets of these models. Given the non-disclosure of the training datasets,
traditional approaches such as code clone detection are insufficient for
asserting copyright infringement. To address this challenge, we propose a new
approach, TraWiC; a model-agnostic and interpretable method based on membership
inference for detecting code inclusion in an LLM's training dataset. We extract
syntactic and semantic identifiers unique to each program to train a classifier
for detecting code inclusion. In our experiments, we observe that TraWiC is
capable of detecting 83.87% of codes that were used to train an LLM. In
comparison, the prevalent clone detection tool NiCad is only capable of
detecting 47.64%. In addition to its remarkable performance, TraWiC has low
resource overhead in contrast to pair-wise clone detection that is conducted
during the auditing process of tools like CodeWhisperer reference tracker,
across thousands of code snippets.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09303" title="Abstract">arXiv:2402.09303</a> [<a href="/pdf/2402.09303" title="Download PDF">pdf</a>, <a href="/format/2402.09303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Immediate generalisation in humans but a generalisation lag in deep  neural networks$\unicode{x2014}$evidence for representational divergence?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huber%2C+L+S">Lukas S. Huber</a>, 
<a href="/search/cs?searchtype=author&query=Mast%2C+F+W">Fred W. Mast</a>, 
<a href="/search/cs?searchtype=author&query=Wichmann%2C+F+A">Felix A. Wichmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at the ICLR 2024 Workshop on Representational Alignment (Re-Align)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Recent research has seen many behavioral comparisons between humans and deep
neural networks (DNNs) in the domain of image classification. Often, comparison
studies focus on the end-result of the learning process by measuring and
comparing the similarities in the representations of object categories once
they have been formed. However, the process of how these representations
emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages
observed during the acquisition$\unicode{x2014}$is less often directly and
empirically compared.
<br />Here we report a detailed investigation of how transferable representations
are acquired in human observers and various classic and state-of-the-art DNNs.
We develop a constrained supervised learning environment in which we align
learning-relevant parameters such as starting point, input modality, available
input data and the feedback provided. Across the whole learning process we
evaluate and compare how well learned representations can be generalized to
previously unseen test data.
<br />Our findings indicate that in terms of absolute classification performance
DNNs demonstrate a level of data efficiency comparable to$\unicode{x2014}$and
sometimes even exceeding that$\unicode{x2014}$of human learners, challenging
some prevailing assumptions in the field. However, comparisons across the
entire learning process reveal significant representational differences: while
DNNs' learning is characterized by a pronounced generalisation lag, humans
appear to immediately acquire generalizable representations without a
preliminary phase of learning training set-specific information that is only
later transferred to novel data.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09305" title="Abstract">arXiv:2402.09305</a> [<a href="/pdf/2402.09305" title="Download PDF">pdf</a>, <a href="/format/2402.09305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embracing the black box: Heading towards foundation models for causal  discovery from time series data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stein%2C+G">Gideon Stein</a>, 
<a href="/search/cs?searchtype=author&query=Shadaydeh%2C+M">Maha Shadaydeh</a>, 
<a href="/search/cs?searchtype=author&query=Denzler%2C+J">Joachim Denzler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI Workshop (AI4TS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Causal discovery from time series data encompasses many existing solutions,
including those based on deep learning techniques. However, these methods
typically do not endorse one of the most prevalent paradigms in deep learning:
End-to-end learning. To address this gap, we explore what we call Causal
Pretraining. A methodology that aims to learn a direct mapping from
multivariate time series to the underlying causal graphs in a supervised
manner. Our empirical findings suggest that causal discovery in a supervised
manner is possible, assuming that the training and test time series samples
share most of their dynamics. More importantly, we found evidence that the
performance of Causal Pretraining can increase with data and model size, even
if the additional data do not share the same dynamics. Further, we provide
examples where causal discovery for real-world data with causally pretrained
neural networks is possible within limits. We argue that this hints at the
possibility of a foundation model for causal discovery.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09315" title="Abstract">arXiv:2402.09315</a> [<a href="/pdf/2402.09315" title="Download PDF">pdf</a>, <a href="/format/2402.09315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Object Detection with Sparse Context Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jie Mei</a>, 
<a href="/search/cs?searchtype=author&query=Jiu%2C+M">Mingyuan Jiu</a>, 
<a href="/search/cs?searchtype=author&query=Sahbi%2C+H">Hichem Sahbi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingliang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Few-shot detection is a major task in pattern recognition which seeks to
localize objects using models trained with few labeled data. One of the
mainstream few-shot methods is transfer learning which consists in pretraining
a detection model in a source domain prior to its fine-tuning in a target
domain. However, it is challenging for fine-tuned models to effectively
identify new classes in the target domain, particularly when the underlying
labeled training data are scarce. In this paper, we devise a novel sparse
context transformer (SCT) that effectively leverages object knowledge in the
source domain, and automatically learns a sparse context from only few training
images in the target domain. As a result, it combines different relevant clues
in order to enhance the discrimination power of the learned detectors and
reduce class confusion. We evaluate the proposed method on two challenging
few-shot object detection benchmarks, and empirical results show that the
proposed method obtains competitive performance compared to the related
state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09316" title="Abstract">arXiv:2402.09316</a> [<a href="/pdf/2402.09316" title="Download PDF">pdf</a>, <a href="/format/2402.09316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Only My Model On My Data: A Privacy Preserving Approach Protecting one  Model and Deceiving Unauthorized Black-Box Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chai%2C+W">Weiheng Chai</a>, 
<a href="/search/cs?searchtype=author&query=Testa%2C+B">Brian Testa</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Huantao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Salekin%2C+A">Asif Salekin</a>, 
<a href="/search/cs?searchtype=author&query=Velipasalar%2C+S">Senem Velipasalar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep neural networks are extensively applied to real-world tasks, such as
face recognition and medical image classification, where privacy and data
protection are critical. Image data, if not protected, can be exploited to
infer personal or contextual information. Existing privacy preservation
methods, like encryption, generate perturbed images that are unrecognizable to
even humans. Adversarial attack approaches prohibit automated inference even
for authorized stakeholders, limiting practical incentives for commercial and
widespread adaptation. This pioneering study tackles an unexplored practical
privacy preservation use case by generating human-perceivable images that
maintain accurate inference by an authorized model while evading other
unauthorized black-box models of similar or dissimilar objectives, and
addresses the previous research gaps. The datasets employed are ImageNet, for
image classification, Celeba-HQ dataset, for identity classification, and
AffectNet, for emotion classification. Our results show that the generated
images can successfully maintain the accuracy of a protected model and degrade
the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and
55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09318" title="Abstract">arXiv:2402.09318</a> [<a href="/pdf/2402.09318" title="Download PDF">pdf</a>, <a href="/format/2402.09318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning  of Music Audio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alonso-Jim%C3%A9nez%2C+P">Pablo Alonso-Jim&#xe9;nez</a>, 
<a href="/search/cs?searchtype=author&query=Pepino%2C+L">Leonardo Pepino</a>, 
<a href="/search/cs?searchtype=author&query=Batlle-Roca%2C+R">Roser Batlle-Roca</a>, 
<a href="/search/cs?searchtype=author&query=Zinemanas%2C+P">Pablo Zinemanas</a>, 
<a href="/search/cs?searchtype=author&query=Bogdanov%2C+D">Dmitry Bogdanov</a>, 
<a href="/search/cs?searchtype=author&query=Serra%2C+X">Xavier Serra</a>, 
<a href="/search/cs?searchtype=author&query=Rocamora%2C+M">Mart&#xed;n Rocamora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We present PECMAE, an interpretable model for music audio classification
based on prototype learning. Our model is based on a previous method, APNet,
which jointly learns an autoencoder and a prototypical network. Instead, we
propose to decouple both training processes. This enables us to leverage
existing self-supervised autoencoders pre-trained on much larger data
(EnCodecMAE), providing representations with better generalization. APNet
allows prototypes' reconstruction to waveforms for interpretability relying on
the nearest training data samples. In contrast, we explore using a diffusion
decoder that allows reconstruction without such dependency. We evaluate our
method on datasets for music instrument classification (Medley-Solos-DB) and
genre recognition (GTZAN and a larger in-house dataset), the latter being a
more challenging task not addressed with prototypical networks before. We find
that the prototype-based models preserve most of the performance achieved with
the autoencoder embeddings, while the sonification of prototypes benefits
understanding the behavior of the classifier.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09320" title="Abstract">arXiv:2402.09320</a> [<a href="/pdf/2402.09320" title="Download PDF">pdf</a>, <a href="/format/2402.09320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICDPO: Effectively Borrowing Alignment Capability of Others via  In-context Direct Preference Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+F">Feifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yuxuan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Houfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to
ensure the generation of safe content. Due to the heavy cost associated with
fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM
decoding with external auxiliary methods. However, these methods do not
essentially enhance the LLM itself. In this paper, we rethink the derivation
procedures of DPO, based on which we conversely build an instant scorer using
the states of the LLM before and after In-context Learning (ICL). Accordingly,
we propose a novel approach called In-Context Direct Preference Optimization
(ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with
ICL, generating well-aligned responses as estimated by the aforementioned
instant scorer, thereby enhancing the final performance. ICDPO can be further
enhanced with a two-stage retriever and an upgraded scorer, both offering
benefits. Extensive experiments show its effectiveness, particularly in
outperforming two fine-tuning-free baselines, and it exhibits competitiveness
with SFT + LoRA. We also conduct detailed analyses to offer comprehensive
insights into ICDPO.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09321" title="Abstract">arXiv:2402.09321</a> [<a href="/pdf/2402.09321" title="Download PDF">pdf</a>, <a href="/ps/2402.09321" title="Download PostScript">ps</a>, <a href="/format/2402.09321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collusion-Resilience in Transaction Fee Mechanism Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+H">Hao Chung</a>, 
<a href="/search/cs?searchtype=author&query=Roughgarden%2C+T">Tim Roughgarden</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+E">Elaine Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
<p class="mathjax">Users bid in a transaction fee mechanism (TFM) to get their transactions
included and confirmed by a blockchain protocol. Roughgarden (EC'21) initiated
the formal treatment of TFMs and proposed three requirements: user incentive
compatibility (UIC), miner incentive compatibility (MIC), and a form of
collusion-resilience called OCA-proofness. Ethereum's EIP-1559 mechanism
satisfies all three properties simultaneously when there is no contention
between transactions, but loses the UIC property when there are too many
eligible transactions to fit in a single block. Chung and Shi (SODA'23)
considered an alternative notion of collusion-resilience, called
c-side-constract-proofness (c-SCP), and showed that, when there is contention
between transactions, no TFM can satisfy UIC, MIC, and c-SCP for any c at least
1. OCA-proofness asserts that the users and a miner should not be able to
"steal from the protocol" and is intuitively weaker than the c-SCP condition,
which stipulates that a coalition of a miner and a subset of users should not
be able to profit through strategic deviations (whether at the expense of the
protocol or of the users outside the coalition).
<br />Our main result is the first proof that, when there is contention between
transactions, no (possibly randomized) direct-revelation TFM satisfies UIC,
MIC, and OCA-proofness. This result resolves the main open question in
Roughgarden(EC'21). We also suggest several relaxations of the basic model that
allow our impossibility result to be circumvented.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09325" title="Abstract">arXiv:2402.09325</a> [<a href="/pdf/2402.09325" title="Download PDF">pdf</a>, <a href="/format/2402.09325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames  in Autonomous Driving Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiuzhong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+G">Guangming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zang%2C+Z">Zheng Zang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Peng Jia</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yuxuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Junyi Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.00874">arXiv:2310.00874</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Large-scale 3D scene reconstruction and novel view synthesis are vital for
autonomous vehicles, especially utilizing temporally sparse LiDAR frames.
However, conventional explicit representations remain a significant bottleneck
towards representing the reconstructed and synthetic scenes at unlimited
resolution. Although the recently developed neural radiance fields (NeRF) have
shown compelling results in implicit representations, the problem of
large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR
frames remains unexplored. To bridge this gap, we propose a 3D scene
reconstruction and novel view synthesis framework called parent-child neural
radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF,
the framework implements hierarchical spatial partitioning and multi-level
scene representation, including scene, segment, and point levels. The
multi-level scene representation enhances the efficient utilization of sparse
LiDAR point cloud data and enables the rapid acquisition of an approximate
volumetric scene representation. With extensive experiments, PC-NeRF is proven
to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in
large-scale scenes. Moreover, PC-NeRF can effectively handle situations with
sparse LiDAR frames and demonstrate high deployment efficiency with limited
training epochs. Our approach implementation and the pre-trained models are
available at https://github.com/biter0088/pc-nerf.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09326" title="Abstract">arXiv:2402.09326</a> [<a href="/pdf/2402.09326" title="Download PDF">pdf</a>, <a href="/format/2402.09326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability and Multigroup Fairness in Ranking with Uncertain Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Devic%2C+S">Siddartha Devic</a>, 
<a href="/search/cs?searchtype=author&query=Korolova%2C+A">Aleksandra Korolova</a>, 
<a href="/search/cs?searchtype=author&query=Kempe%2C+D">David Kempe</a>, 
<a href="/search/cs?searchtype=author&query=Sharan%2C+V">Vatsal Sharan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Rankings are ubiquitous across many applications, from search engines to
hiring committees. In practice, many rankings are derived from the output of
predictors. However, when predictors trained for classification tasks have
intrinsic uncertainty, it is not obvious how this uncertainty should be
represented in the derived rankings. Our work considers ranking functions: maps
from individual predictions for a classification task to distributions over
rankings. We focus on two aspects of ranking functions: stability to
perturbations in predictions and fairness towards both individuals and
subgroups. Not only is stability an important requirement for its own sake, but
-- as we show -- it composes harmoniously with individual fairness in the sense
of Dwork et al. (2012). While deterministic ranking functions cannot be stable
aside from trivial scenarios, we show that the recently proposed uncertainty
aware (UA) ranking functions of Singh et al. (2021) are stable. Our main result
is that UA rankings also achieve multigroup fairness through successful
composition with multiaccurate or multicalibrated predictors. Our work
demonstrates that UA rankings naturally interpolate between group and
individual level fairness guarantees, while simultaneously satisfying stability
guarantees important whenever machine-learned predictions are used.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09327" title="Abstract">arXiv:2402.09327</a> [<a href="/pdf/2402.09327" title="Download PDF">pdf</a>, <a href="/format/2402.09327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Complexity of Stochastic Convex Optimization: Applications  to Generalization and Memorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attias%2C+I">Idan Attias</a>, 
<a href="/search/cs?searchtype=author&query=Dziugaite%2C+G+K">Gintare Karolina Dziugaite</a>, 
<a href="/search/cs?searchtype=author&query=Haghifam%2C+M">Mahdi Haghifam</a>, 
<a href="/search/cs?searchtype=author&query=Livni%2C+R">Roi Livni</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+D+M">Daniel M. Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we investigate the interplay between memorization and learning
in the context of \emph{stochastic convex optimization} (SCO). We define
memorization via the information a learning algorithm reveals about its
training data points. We then quantify this information using the framework of
conditional mutual information (CMI) proposed by Steinke and Zakynthinou
(2020). Our main result is a precise characterization of the tradeoff between
the accuracy of a learning algorithm and its CMI, answering an open question
posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting
and under strong convexity, every learner with an excess error $\varepsilon$
has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$,
respectively. We further demonstrate the essential role of memorization in
learning problems in SCO by designing an adversary capable of accurately
identifying a significant fraction of the training samples in specific SCO
problems. Finally, we enumerate several implications of our results, such as a
limitation of generalization bounds based on CMI and the incompressibility of
samples in SCO problems.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09329" title="Abstract">arXiv:2402.09329</a> [<a href="/pdf/2402.09329" title="Download PDF">pdf</a>, <a href="/ps/2402.09329" title="Download PostScript">ps</a>, <a href="/format/2402.09329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chien%2C+C">Chun-Tse Chien</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+R">Rui-Yang Ju</a>, 
<a href="/search/cs?searchtype=author&query=Chou%2C+K">Kuang-Yi Chou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chien-Sheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+J">Jen-Shiun Chiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Wrist trauma and even fractures occur frequently in daily life, particularly
among children who account for a significant proportion of fracture cases.
Before performing surgery, surgeons often request patients to undergo X-ray
imaging first and prepare for it based on the analysis of the radiologist. With
the development of neural networks, You Only Look Once (YOLO) series models
have been widely used in fracture detection as computer-assisted diagnosis
(CAD). In 2023, Ultralytics presented the latest version of the YOLO models,
which has been employed for detecting fractures across various parts of the
body. Attention mechanism is one of the hottest methods to improve the model
performance. This research work proposes YOLOv8-AM, which incorporates the
attention mechanism into the original YOLOv8 architecture. Specifically, we
respectively employ four attention modules, Convolutional Block Attention
Module (CBAM), Global Attention Mechanism (GAM), Efficient Channel Attention
(ECA), and Shuffle Attention (SA), to design the improved models and train them
on GRAZPEDWRI-DX dataset. Experimental results demonstrate that the mean
Average Precision at IoU 50 (mAP 50) of the YOLOv8-AM model based on ResBlock +
CBAM (ResCBAM) increased from 63.6% to 65.8%, which achieves the
state-of-the-art (SOTA) performance. Conversely, YOLOv8-AM model incorporating
GAM obtains the mAP 50 value of 64.2%, which is not a satisfactory enhancement.
Therefore, we combine ResBlock and GAM, introducing ResGAM to design another
new YOLOv8-AM model, whose mAP 50 value is increased to 65.0%.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09334" title="Abstract">arXiv:2402.09334</a> [<a href="/pdf/2402.09334" title="Download PDF">pdf</a>, <a href="/format/2402.09334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amirizaniani%2C+M">Maryam Amirizaniani</a>, 
<a href="/search/cs?searchtype=author&query=Roosta%2C+T">Tanya Roosta</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+C">Chirag Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">As Large Language Models (LLMs) gain wider adoption in various contexts, it
becomes crucial to ensure they are reasonably safe, consistent, and reliable
for an application at hand. This may require probing or auditing them. Probing
LLMs with varied iterations of a single question could reveal potential
inconsistencies in their knowledge or functionality. However, a tool for
performing such audits with simple workflow and low technical threshold is
lacking. In this demo, we introduce "AuditLLM," a novel tool designed to
evaluate the performance of various LLMs in a methodical way. AuditLLM's core
functionality lies in its ability to test a given LLM by auditing it using
multiple probes generated from a single question, thereby identifying any
inconsistencies in the model's understanding or operation. A reasonably robust,
reliable, and consistent LLM should output semantically similar responses for a
question asked differently or by different people. Based on this assumption,
AuditLLM produces easily interpretable results regarding the LLM's
consistencies from a single question that the user enters. A certain level of
inconsistency has been shown to be an indicator of potential bias,
hallucinations, and other issues. One could then use the output of AuditLLM to
further investigate issues with the aforementioned LLM. To facilitate
demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode
which allows instant auditing of LLMs by analyzing responses to real-time
queries; (2) Batch mode which facilitates comprehensive LLM auditing by
processing multiple queries at once for in-depth analysis. This tool is
beneficial for both researchers and general users, as it enhances our
understanding of LLMs' capabilities in generating responses, using a
standardized auditing platform.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09336" title="Abstract">arXiv:2402.09336</a> [<a href="/pdf/2402.09336" title="Download PDF">pdf</a>, <a href="/ps/2402.09336" title="Download PostScript">ps</a>, <a href="/format/2402.09336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Modern Approach to Electoral Delimitation using the Quadtree Data  Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kale%2C+S">Sahil Kale</a>, 
<a href="/search/cs?searchtype=author&query=Khaire%2C+G">Gautam Khaire</a>, 
<a href="/search/cs?searchtype=author&query=Patankar%2C+J">Jay Patankar</a>, 
<a href="/search/cs?searchtype=author&query=Vidap%2C+P">Pujashree Vidap</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, Accepted in 1st IEEE International Conference on Cognitive Computing &amp; Engineering Education (ICCCEE) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The boundaries of electoral constituencies for assembly and parliamentary
seats are drafted using a process referred to as delimitation, which ensures
fair and equal representation of all citizens. The current delimitation
exercise suffers from a number of drawbacks viz. inefficiency, gerrymandering
and an uneven seat-to-population ratio, owing to existing legal and
constitutional dictates. The existing methods allocate seats to every state but
remain silent about their actual shape and location within the state. The main
purpose of this research is to study and analyse the performance of existing
delimitation algorithms and further propose a potential solution, along with
its merits, that involves using a computational model based on the quadtree
data structure to automate the districting process by optimizing objective
population criteria. The paper presents an approach to electoral delimitation
using the quadtree data structure, which is used to partition a two-dimensional
geographical space by recursively subdividing it into four quadrants or regions
on the basis of population as a parameter value associated with the node. The
quadtree makes use of a quadrant schema of the geographical space for
representing constituencies, which not only keeps count of the allocated
constituencies but also holds their location-specific information. The
performance of the proposed algorithm is analysed and evaluated against
existing techniques and proves to be an efficient solution in terms of
algorithmic complexity and boundary visualisation to the process of political
districting.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09344" title="Abstract">arXiv:2402.09344</a> [<a href="/pdf/2402.09344" title="Download PDF">pdf</a>, <a href="/format/2402.09344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Diverse Translation with Perturbed kNN-MT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nishida%2C+Y">Yuto Nishida</a>, 
<a href="/search/cs?searchtype=author&query=Morishita%2C+M">Makoto Morishita</a>, 
<a href="/search/cs?searchtype=author&query=Kamigaito%2C+H">Hidetaka Kamigaito</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+T">Taro Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 SRW
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generating multiple translation candidates would enable users to choose the
one that satisfies their needs. Although there has been work on diversified
generation, there exists room for improving the diversity mainly because the
previous methods do not address the overcorrection problem -- the model
underestimates a prediction that is largely different from the training data,
even if that prediction is likely. This paper proposes methods that generate
more diverse translations by introducing perturbed k-nearest neighbor machine
translation (kNN-MT). Our methods expand the search space of kNN-MT and help
incorporate diverse words into candidates by addressing the overcorrection
problem. Our experiments show that the proposed methods drastically improve
candidate diversity and control the degree of diversity by tuning the
perturbation's magnitude.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09345" title="Abstract">arXiv:2402.09345</a> [<a href="/pdf/2402.09345" title="Download PDF">pdf</a>, <a href="/format/2402.09345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Reward Hacking via Information-Theoretic Reward Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+Y">Yuchun Miao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+R">Rong Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lefei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 28 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the success of reinforcement learning from human feedback (RLHF) in
aligning language models with human values, reward hacking, also termed reward
overoptimization, remains a critical challenge, which primarily stems from
limitations in reward modeling, i.e., generalizability of the reward model and
inconsistency in the preference dataset. In this work, we tackle this problem
from an information theoretic-perspective, and propose a generalizable and
robust framework for reward modeling, namely InfoRM, by introducing a
variational information bottleneck objective to filter out irrelevant
information and developing a mechanism for model complexity modulation.
Notably, we further identify a correlation between overoptimization and
outliers in the latent space, establishing InfoRM as a promising tool for
detecting reward overoptimization. Inspired by this finding, we propose the
Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the
latent space, as an indicator of reward overoptimization to facilitate the
development of online mitigation strategies. Extensive experiments on a wide
range of settings and model scales (70M, 440M, 1.4B, and 7B) support the
effectiveness of InfoRM. Further analyses reveal that InfoRM's overoptimization
detection mechanism is effective, potentially signifying a notable advancement
in the field of RLHF. Code will be released upon acceptance.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09346" title="Abstract">arXiv:2402.09346</a> [<a href="/pdf/2402.09346" title="Download PDF">pdf</a>, <a href="/format/2402.09346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing a Framework for Auditing Large Language Models Using  Human-in-the-Loop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amirizaniani%2C+M">Maryam Amirizaniani</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jihan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Lavergne%2C+A">Adrian Lavergne</a>, 
<a href="/search/cs?searchtype=author&query=Okada%2C+E+S">Elizabeth Snell Okada</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Roosta%2C+T">Tanya Roosta</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+C">Chirag Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">As LLMs become more pervasive across various users and scenarios, identifying
potential issues when using these models becomes essential. Examples include
bias, inconsistencies, and hallucination. Although auditing the LLM for these
problems is desirable, it is far from being easy or solved. An effective method
is to probe the LLM using different versions of the same question. This could
expose inconsistencies in its knowledge or operation, indicating potential for
bias or hallucination. However, to operationalize this auditing method at
scale, we need an approach to create those probes reliably and automatically.
In this paper we propose an automatic and scalable solution, where one uses a
different LLM along with human-in-the-loop. This approach offers verifiability
and transparency, while avoiding circular reliance on the same LLMs, and
increasing scientific rigor and generalizability. Specifically, we present a
novel methodology with two phases of verification using humans: standardized
evaluation criteria to verify responses, and a structured prompt template to
generate desired probes. Experiments on a set of questions from TruthfulQA
dataset show that we can generate a reliable set of probes from one LLM that
can be used to audit inconsistencies in a different LLM. The criteria for
generating and applying auditing probes is generalizable to various LLMs
regardless of the underlying structure or training mechanism.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09353" title="Abstract">arXiv:2402.09353</a> [<a href="/pdf/2402.09353" title="Download PDF">pdf</a>, <a href="/format/2402.09353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoRA: Weight-Decomposed Low-Rank Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shih-Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chien-Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongxu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Molchanov%2C+P">Pavlo Molchanov</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y+F">Yu-Chiang Frank Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kwang-Ting Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min-Hung Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and
its variants have gained considerable popularity because of avoiding additional
inference costs. However, there still often exists an accuracy gap between
these methods and full fine-tuning (FT). In this work, we first introduce a
novel weight decomposition analysis to investigate the inherent differences
between FT and LoRA. Aiming to resemble the learning capacity of FT from the
findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA
decomposes the pre-trained weight into two components, magnitude and direction,
for fine-tuning, specifically employing LoRA for directional updates to
efficiently minimize the number of trainable parameters. By employing DoRA, we
enhance both the learning capacity and training stability of LoRA while
avoiding any additional inference overhead. DoRA consistently outperforms LoRA
on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as
commonsense reasoning, visual instruction tuning, and image/video-text
understanding.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09355" title="Abstract">arXiv:2402.09355</a> [<a href="/pdf/2402.09355" title="Download PDF">pdf</a>, <a href="/format/2402.09355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-Reset Divide &amp; Conquer Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chenu%2C+A">Alexandre Chenu</a>, 
<a href="/search/cs?searchtype=author&query=Serris%2C+O">Olivier Serris</a>, 
<a href="/search/cs?searchtype=author&query=Sigaud%2C+O">Olivier Sigaud</a>, 
<a href="/search/cs?searchtype=author&query=Perrin-Gilbert%2C+N">Nicolas Perrin-Gilbert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Demonstrations are commonly used to speed up the learning process of Deep
Reinforcement Learning algorithms. To cope with the difficulty of accessing
multiple demonstrations, some algorithms have been developed to learn from a
single demonstration. In particular, the Divide &amp; Conquer Imitation Learning
algorithms leverage a sequential bias to learn a control policy for complex
robotic tasks using a single state-based demonstration. The latest version,
DCIL-II demonstrates remarkable sample efficiency. This novel method operates
within an extended Goal-Conditioned Reinforcement Learning framework, ensuring
compatibility between intermediate and subsequent goals extracted from the
demonstration. However, a fundamental limitation arises from the assumption
that the system can be reset to specific states along the demonstrated
trajectory, confining the application to simulated systems. In response, we
introduce an extension called Single-Reset DCIL (SR-DCIL), designed to overcome
this constraint by relying on a single initial state reset rather than
sequential resets. To address this more challenging setting, we integrate two
mechanisms inspired by the Learning from Demonstrations literature, including a
Demo-Buffer and Value Cloning, to guide the agent toward compatible success
states. In addition, we introduce Approximate Goal Switching to facilitate
training to reach goals distant from the reset state. Our paper makes several
contributions, highlighting the importance of the reset assumption in DCIL-II,
presenting the mechanisms of SR-DCIL variants and evaluating their performance
in challenging robotic tasks compared to DCIL-II. In summary, this work offers
insights into the significance of reset assumptions in the framework of DCIL
and proposes SR-DCIL, a first step toward a versatile algorithm capable of
learning control policies under a weaker reset assumption.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09357" title="Abstract">arXiv:2402.09357</a> [<a href="/pdf/2402.09357" title="Download PDF">pdf</a>, <a href="/ps/2402.09357" title="Download PostScript">ps</a>, <a href="/format/2402.09357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechanism Design for Automated Market Makers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+H">T-H. Hubert Chan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Ke Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+E">Elaine Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 title page and 23 pages for the main body
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">Blockchains have popularized automated market makers (AMMs). An AMM exchange
is an application running on a blockchain which maintains a pool of
crypto-assets and automatically trades assets with users governed by some
pricing function that prices the assets based on their relative demand/supply.
AMMs have created an important challenge commonly known as the Miner
Extractable Value (MEV). In particular, the miners who control the contents and
ordering of transactions in a block can extract value by front-running and
back-running users' transactions, leading to arbitrage opportunities that
guarantee them risk-free returns.
<br />In this paper, we consider how to design AMM mechanisms that eliminate MEV
opportunities. Specifically, we propose a new AMM mechanism that processes all
transactions contained within a block in a batch. We show that our new
mechanism satisfies two tiers of guarantees. First, for legacy blockchains
where each block is proposed by a single (possibly rotating) miner, we prove
that our mechanism satisfies arbitrage resilience, i.e., a miner cannot gain
risk-free profit. Moreover, we also guarantee fair treatment among all
transactions within the same block, such that the miner is unable to sell off
favorable positions in the block to users or arbitragers. Second, for
blockchains where the block proposal process is decentralized and offers
sequencing-fairness, we prove a stronger notion called incentive compatibility
-- roughly speaking, we guarantee that any individual user's best response is
to follow the honest strategy.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09358" title="Abstract">arXiv:2402.09358</a> [<a href="/pdf/2402.09358" title="Download PDF">pdf</a>, <a href="/format/2402.09358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating ChatGPT into Secure Hospital Networks: A Case Study on  Improving Radiology Report Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kyungsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Junhyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Langarica%2C+S">Saul Langarica</a>, 
<a href="/search/cs?searchtype=author&query=Alkhadrawi%2C+A+M">Adham Mahmoud Alkhadrawi</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+S">Synho Do</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This study demonstrates the first in-hospital adaptation of a cloud-based AI,
similar to ChatGPT, into a secure model for analyzing radiology reports,
prioritizing patient data privacy. By employing a unique sentence-level
knowledge distillation method through contrastive learning, we achieve over 95%
accuracy in detecting anomalies. The model also accurately flags uncertainties
in its predictions, enhancing its reliability and interpretability for
physicians with certainty indicators. These advancements represent significant
progress in developing secure and efficient AI tools for healthcare, suggesting
a promising future for in-hospital AI applications with minimal supervision.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09360" title="Abstract">arXiv:2402.09360</a> [<a href="/pdf/2402.09360" title="Download PDF">pdf</a>, <a href="/format/2402.09360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%2C+Y+S+B">Yashas Samaga B L</a>, 
<a href="/search/cs?searchtype=author&query=Yerram%2C+V">Varun Yerram</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Chong You</a>, 
<a href="/search/cs?searchtype=author&query=Bhojanapalli%2C+S">Srinadh Bhojanapalli</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sanjiv Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Prateek Jain</a>, 
<a href="/search/cs?searchtype=author&query=Netrapalli%2C+P">Praneeth Netrapalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Autoregressive decoding with generative Large Language Models (LLMs) on
accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent
on transferring model parameters from high bandwidth memory (HBM) to cache. On
the other hand, recent works show that LLMs can maintain quality with
significant sparsity/redundancy in the feedforward (FFN) layers by
appropriately training the model to operate on a top-$k$ fraction of
rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the
transfer of model parameters, and hence latency. However, exploiting this
sparsity for improving latency is hindered by the fact that identifying top
rows/columns is data-dependent and is usually performed using full matrix
operations, severely limiting potential gains. To address these issues, we
introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of
two novel components: (i) a compression scheme to cheaply predict top-$k$
rows/columns with high recall, followed by full computation restricted to the
predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate
top-$k$ operator. We demonstrate that on a one billion parameter model, HiRE
applied to both the softmax as well as feedforward layers, achieves almost
matching pretraining and downstream accuracy, and speeds up inference latency
by $1.47\times$ on a single TPUv5e device.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09363" title="Abstract">arXiv:2402.09363</a> [<a href="/pdf/2402.09363" title="Download PDF">pdf</a>, <a href="/format/2402.09363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Copyright Traps for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meeus%2C+M">Matthieu Meeus</a>, 
<a href="/search/cs?searchtype=author&query=Shilov%2C+I">Igor Shilov</a>, 
<a href="/search/cs?searchtype=author&query=Faysse%2C+M">Manuel Faysse</a>, 
<a href="/search/cs?searchtype=author&query=de+Montjoye%2C+Y">Yves-Alexandre de Montjoye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Questions of fair use of copyright-protected content to train Large Language
Models (LLMs) are being very actively debated. Document-level inference has
been proposed as a new task: inferring from black-box access to the trained
model whether a piece of content has been seen during training. SOTA methods
however rely on naturally occurring memorization of (part of) the content.
While very effective against models that memorize a lot, we hypothesize--and
later confirm--that they will not work against models that do not naturally
memorize, e.g. medium-size 1B models. We here propose to use copyright traps,
the inclusion of fictitious entries in original content, to detect the use of
copyrighted materials in LLMs with a focus on models where memorization does
not naturally occur. We carefully design an experimental setup, randomly
inserting traps into original content (books) and train a 1.3B LLM. We first
validate that the use of content in our target model would be undetectable
using existing methods. We then show, contrary to intuition, that even
medium-length trap sentences repeated a significant number of times (100) are
not detectable using existing methods. However, we show that longer sequences
repeated a large number of times can be reliably detected (AUC=0.75) and used
as copyright traps. We further improve these results by studying how the number
of times a sequence is seen improves detectability, how sequences with higher
perplexity tend to be memorized more, and how taking context into account
further improves detectability.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09364" title="Abstract">arXiv:2402.09364</a> [<a href="/pdf/2402.09364" title="Download PDF">pdf</a>, <a href="/format/2402.09364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance-Complexity-Latency Trade-offs of Concatenated RS-BCH Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukmadji%2C+A+Y">Alvin Y. Sukmadji</a>, 
<a href="/search/cs?searchtype=author&query=Kschischang%2C+F+R">Frank R. Kschischang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Using a generating function approach, a computationally tractable expression
is derived to predict the frame error rate arising at the output of the binary
symmetric channel when a number of outer Reed-Solomon codes are concatenated
with a number of inner Bose-Ray-Chaudhuri-Hocquenghem codes, thereby obviating
the need for time-consuming Monte Carlo simulations. Measuring (a) code
performance via the gap to the Shannon limit, (b) decoding complexity via an
estimate of the number of operations per decoded bit, and (c) decoding latency
by the overall frame length, a code search is performed to determine the Pareto
frontier for performance-complexity-latency trade-offs.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09367" title="Abstract">arXiv:2402.09367</a> [<a href="/pdf/2402.09367" title="Download PDF">pdf</a>, <a href="/ps/2402.09367" title="Download PostScript">ps</a>, <a href="/format/2402.09367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction of Activated Sludge Settling Characteristics from Microscopy  Images with Deep Convolutional Neural Networks and Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borzooei%2C+S">Sina Borzooei</a>, 
<a href="/search/cs?searchtype=author&query=Scabini%2C+L">Leonardo Scabini</a>, 
<a href="/search/cs?searchtype=author&query=Miranda%2C+G">Gisele Miranda</a>, 
<a href="/search/cs?searchtype=author&query=Daneshgar%2C+S">Saba Daneshgar</a>, 
<a href="/search/cs?searchtype=author&query=Deblieck%2C+L">Lukas Deblieck</a>, 
<a href="/search/cs?searchtype=author&query=De+Langhe%2C+P">Piet De Langhe</a>, 
<a href="/search/cs?searchtype=author&query=Bruno%2C+O">Odemir Bruno</a>, 
<a href="/search/cs?searchtype=author&query=De+Baets%2C+B">Bernard De Baets</a>, 
<a href="/search/cs?searchtype=author&query=Nopens%2C+I">Ingmar Nopens</a>, 
<a href="/search/cs?searchtype=author&query=Torfs%2C+E">Elena Torfs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 Pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Microbial communities play a key role in biological wastewater treatment
processes. Activated sludge settling characteristics, for example, are affected
by microbial community composition, varying by changes in operating conditions
and influent characteristics of wastewater treatment plants (WWTPs). Timely
assessment and prediction of changes in microbial composition leading to
settling problems, such as filamentous bulking (FB), can prevent operational
challenges, reductions in treatment efficiency, and adverse environmental
impacts. This study presents an innovative computer vision-based approach to
assess activated sludge-settling characteristics based on the morphological
properties of flocs and filaments in microscopy images. Implementing the
transfer learning of deep convolutional neural network (CNN) models, this
approach aims to overcome the limitations of existing quantitative image
analysis techniques. The offline microscopy image dataset was collected over
two years, with weekly sampling at a full-scale industrial WWTP in Belgium.
Multiple data augmentation techniques were employed to enhance the
generalizability of the CNN models. Various CNN architectures, including
Inception v3, ResNet18, ResNet152, ConvNeXt-nano, and ConvNeXt-S, were tested
to evaluate their performance in predicting sludge settling characteristics.
The sludge volume index was used as the final prediction variable, but the
method can easily be adjusted to predict any other settling metric of choice.
The results showed that the suggested CNN-based approach provides less
labour-intensive, objective, and consistent assessments, while transfer
learning notably minimises the training phase, resulting in a generalizable
system that can be employed in real-time applications.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09368" title="Abstract">arXiv:2402.09368</a> [<a href="/pdf/2402.09368" title="Download PDF">pdf</a>, <a href="/format/2402.09368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magic-Me: Identity-Specific Video Customized Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ze Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Daquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C">Chun-Hsiao Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xue-She Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiuyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huanrui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiashi Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Creating content for a specific identity (ID) has shown significant interest
in the field of generative models. In the field of text-to-image generation
(T2I), subject-driven content generation has achieved great progress with the
ID in the images controllable. However, extending it to video generation is not
well explored. In this work, we propose a simple yet effective subject identity
controllable video generation framework, termed Video Custom Diffusion (VCD).
With a specified subject ID defined by a few images, VCD reinforces the
identity information extraction and injects frame-wise correlation at the
initialization stage for stable video outputs with identity preserved to a
large extent. To achieve this, we propose three novel components that are
essential for high-quality ID preservation: 1) an ID module trained with the
cropped identity by prompt-to-segmentation to disentangle the ID information
and the background noise for more accurate ID token learning; 2) a
text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better
inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD
modules to deblur the face and upscale the video for higher resolution.
<br />Despite its simplicity, we conducted extensive experiments to verify that VCD
is able to generate stable and high-quality videos with better ID over the
selected strong baselines. Besides, due to the transferability of the ID
module, VCD is also working well with finetuned text-to-image models available
publically, further improving its usability. The codes are available at
https://github.com/Zhen-Dong/Magic-Me.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09369" title="Abstract">arXiv:2402.09369</a> [<a href="/pdf/2402.09369" title="Download PDF">pdf</a>, <a href="/format/2402.09369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Massively Multi-Cultural Knowledge Acquisition &amp; LM Benchmarking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fung%2C+Y">Yi Fung</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ruining Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Doo%2C+J">Jae Doo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chenkai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pretrained large language models have revolutionized many applications but
still face challenges related to cultural bias and a lack of cultural
commonsense knowledge crucial for guiding cross-culture communication and
interactions. Recognizing the shortcomings of existing methods in capturing the
diverse and rich cultures across the world, this paper introduces a novel
approach for massively multicultural knowledge acquisition. Specifically, our
method strategically navigates from densely informative Wikipedia documents on
cultural topics to an extensive network of linked pages. Leveraging this
valuable source of data collection, we construct the CultureAtlas dataset,
which covers a wide range of sub-country level geographical regions and
ethnolinguistic groups, with data cleaning and preprocessing to ensure textual
assertion sentence self-containment, as well as fine-grained cultural profile
information extraction. Our dataset not only facilitates the evaluation of
language model performance in culturally diverse contexts but also serves as a
foundational tool for the development of culturally sensitive and aware
language models. Our work marks an important step towards deeper understanding
and bridging the gaps of cultural disparities in AI, to promote a more
inclusive and balanced representation of global cultures in the digital domain.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09370" title="Abstract">arXiv:2402.09370</a> [<a href="/pdf/2402.09370" title="Download PDF">pdf</a>, <a href="/format/2402.09370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pseudorandom Error-Correcting Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christ%2C+M">Miranda Christ</a>, 
<a href="/search/cs?searchtype=author&query=Gunn%2C+S">Sam Gunn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We construct pseudorandom error-correcting codes (or simply pseudorandom
codes), which are error-correcting codes with the property that any polynomial
number of codewords are pseudorandom to any computationally-bounded adversary.
Efficient decoding of corrupted codewords is possible with the help of a
decoding key.
<br />We build pseudorandom codes that are robust to substitution and deletion
errors, where pseudorandomness rests on standard cryptographic assumptions.
Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of
LPN, or polynomial hardness of LPN and the planted XOR problem at low density.
<br />As our primary application of pseudorandom codes, we present an undetectable
watermarking scheme for outputs of language models that is robust to cropping
and a constant rate of random substitutions and deletions. The watermark is
undetectable in the sense that any number of samples of watermarked text are
computationally indistinguishable from text output by the original model. This
is the first undetectable watermarking scheme that can tolerate a constant rate
of errors.
<br />Our second application is to steganography, where a secret message is hidden
in innocent-looking content. We present a constant-rate stateless steganography
scheme with robustness to a constant rate of substitutions. Ours is the first
stateless steganography scheme with provable steganographic security and any
robustness to errors.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09371" title="Abstract">arXiv:2402.09371</a> [<a href="/pdf/2402.09371" title="Download PDF">pdf</a>, <a href="/format/2402.09371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers Can Achieve Length Generalization But Not Robustly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yongchao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+U">Uri Alon</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuezhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+R">Rishabh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Length generalization, defined as the ability to extrapolate from shorter
training sequences to longer test ones, is a significant challenge for language
models. This issue persists even with large-scale Transformers handling
relatively straightforward tasks. In this paper, we test the Transformer's
ability of length generalization using the task of addition of two integers. We
show that the success of length generalization is intricately linked to the
data format and the type of position encoding. Using the right combination of
data format and position encodings, we show for the first time that standard
Transformers can extrapolate to a sequence length that is 2.5x the input
length. Nevertheless, unlike in-distribution generalization, length
generalization remains fragile, significantly influenced by factors like random
weight initialization and training data order, leading to large variances
across different random seeds.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09373" title="Abstract">arXiv:2402.09373</a> [<a href="/pdf/2402.09373" title="Download PDF">pdf</a>, <a href="/format/2402.09373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loss Shaping Constraints for Long-Term Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hounie%2C+I">Ignacio Hounie</a>, 
<a href="/search/cs?searchtype=author&query=Porras-Valenzuela%2C+J">Javier Porras-Valenzuela</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Several applications in time series forecasting require predicting multiple
steps ahead. Despite the vast amount of literature in the topic, both classical
and recent deep learning based approaches have mostly focused on minimising
performance averaged over the predicted window. We observe that this can lead
to disparate distributions of errors across forecasting steps, especially for
recent transformer architectures trained on popular forecasting benchmarks.
That is, optimising performance on average can lead to undesirably large errors
at specific time-steps. In this work, we present a Constrained Learning
approach for long-term time series forecasting that aims to find the best model
in terms of average performance that respects a user-defined upper bound on the
loss at each time-step. We call our approach loss shaping constraints because
it imposes constraints on the loss at each time step, and leverage recent
duality results to show that despite its non-convexity, the resulting problem
has a bounded duality gap. We propose a practical Primal-Dual algorithm to
tackle it, and demonstrate that the proposed approach exhibits competitive
average performance in time series forecasting benchmarks, while shaping the
distribution of errors across the predicted window.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09377" title="Abstract">arXiv:2402.09377</a> [<a href="/pdf/2402.09377" title="Download PDF">pdf</a>, <a href="/ps/2402.09377" title="Download PostScript">ps</a>, <a href="/format/2402.09377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limitless FaaS: Overcoming serverless functions execution time limits  with invoke driven architecture and memory checkpoints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andraca%2C+R+L">Rodrigo Landa Andraca</a>, 
<a href="/search/cs?searchtype=author&query=Zareei%2C+M">Mahdi Zareei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Function-as-a-Service (FaaS) allows to directly submit function code to a
cloud provider without the burden of managing infrastructure resources. Each
cloud provider establishes execution time limits to their FaaS offerings, which
impose the risk of spending computation time without achieving partial results.
In this work, a framework that enables limitless execution time in FaaS, with
little to no modifications to the user-provided function code, is presented.
After a thorough literature and theoretical framework review, Apache OpenWhisk
Actions and the DMCTP checkpoint-and-restore (CR) tool were selected. With
these, dependent successive serverless same-function invocations that exploit
the persistence of partial results were implemented. The solution was submitted
to the FaaSDom benchmark and time metrics were collected. Additionally, the
solution was characterized in terms of the Serverless Trilemma. The resultant
system, even at this proof-of-concept state, offers a lot of value to companies
that rely heavily on serverless architecture.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09379" title="Abstract">arXiv:2402.09379</a> [<a href="/pdf/2402.09379" title="Download PDF">pdf</a>, <a href="/format/2402.09379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed-sparsity matrix approximation from matrix-vector products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amsel%2C+N">Noah Amsel</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tyler Chen</a>, 
<a href="/search/cs?searchtype=author&query=Halikias%2C+D">Diana Halikias</a>, 
<a href="/search/cs?searchtype=author&query=Keles%2C+F+D">Feyza Duman Keles</a>, 
<a href="/search/cs?searchtype=author&query=Musco%2C+C">Cameron Musco</a>, 
<a href="/search/cs?searchtype=author&query=Musco%2C+C">Christopher Musco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We study the problem of approximating a matrix $\mathbf{A}$ with a matrix
that has a fixed sparsity pattern (e.g., diagonal, banded, etc.), when
$\mathbf{A}$ is accessed only by matrix-vector products. We describe a simple
randomized algorithm that returns an approximation with the given sparsity
pattern with Frobenius-norm error at most $(1+\varepsilon)$ times the best
possible error. When each row of the desired sparsity pattern has at most $s$
nonzero entries, this algorithm requires $O(s/\varepsilon)$ non-adaptive
matrix-vector products with $\mathbf{A}$. We proceed to prove a matching
lower-bound. Specifically, we show that for any $s\geq 1$, there are matrices
$\mathbf{A}$ such that, for any sparsity pattern with $\Theta(s)$ nonzeros per
row and column, any algorithm which obtains a $(1+\varepsilon)$ accurate
approximation of the given sparsity from matrix-vector products requires at
least $\Omega(s/\varepsilon)$ matrix-vector products. Our bounds therefore
resolve the matrix-vector product query complexity of the problem up to
constant factors, even for the well-studied case of diagonal approximation, for
which no previous lower bounds were known.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09381" title="Abstract">arXiv:2402.09381</a> [<a href="/pdf/2402.09381" title="Download PDF">pdf</a>, <a href="/format/2402.09381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in  Metagenomic Assembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azizpour%2C+A">Ali Azizpour</a>, 
<a href="/search/cs?searchtype=author&query=Balaji%2C+A">Advait Balaji</a>, 
<a href="/search/cs?searchtype=author&query=Treangen%2C+T+J">Todd J. Treangen</a>, 
<a href="/search/cs?searchtype=author&query=Segarra%2C+S">Santiago Segarra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Repetitive DNA (repeats) poses significant challenges for accurate and
efficient genome assembly and sequence alignment. This is particularly true for
metagenomic data, where genome dynamics such as horizontal gene transfer, gene
duplication, and gene loss/gain complicate accurate genome assembly from
metagenomic communities. Detecting repeats is a crucial first step in
overcoming these challenges. To address this issue, we propose GraSSRep, a
novel approach that leverages the assembly graph's structure through graph
neural networks (GNNs) within a self-supervised learning framework to classify
DNA sequences into repetitive and non-repetitive categories. Specifically, we
frame this problem as a node classification task within a metagenomic assembly
graph. In a self-supervised fashion, we rely on a high-precision (but
low-recall) heuristic to generate pseudo-labels for a small proportion of the
nodes. We then use those pseudo-labels to train a GNN embedding and a random
forest classifier to propagate the labels to the remaining nodes. In this way,
GraSSRep combines sequencing features with pre-defined and learned graph
features to achieve state-of-the-art performance in repeat detection. We
evaluate our method using simulated and synthetic metagenomic datasets. The
results on the simulated data highlight our GraSSRep's robustness to repeat
attributes, demonstrating its effectiveness in handling the complexity of
repeated sequences. Additionally, our experiments with synthetic metagenomic
datasets reveal that incorporating the graph structure and the GNN enhances our
detection performance. Finally, in comparative analyses, GraSSRep outperforms
existing repeat detection tools with respect to precision and recall.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09382" title="Abstract">arXiv:2402.09382</a> [<a href="/pdf/2402.09382" title="Download PDF">pdf</a>, <a href="/format/2402.09382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Distributed Control of Multi-Robot Systems with Communication  Delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ballotta%2C+L">Luca Ballotta</a>, 
<a href="/search/cs?searchtype=author&query=Talak%2C+R">Rajat Talak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 10 figures. This work has been submitted to IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Safe operation of multi-robot systems is critical, especially in
communication-degraded environments such as underwater for seabed mapping,
underground caves for navigation, and in extraterrestrial missions for assembly
and construction. We address safety of networked autonomous systems where the
information exchanged between robots incurs communication delays. We formalize
a notion of distributed control barrier function (CBF) for multi-robot systems,
a safety certificate amenable to a distributed implementation, which provides
formal ground to using graph neural networks to learn safe distributed
controllers. Further, we observe that learning a distributed controller
ignoring delays can severely degrade safety. Our main contribution is a
predictor-based framework to train a safe distributed controller under
communication delays, where the current state of nearby robots is predicted
from received data and age-of-information. Numerical experiments on multi-robot
collision avoidance show that our predictor-based approach can significantly
improve the safety of a learned distributed controller under communication
delays
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09386" title="Abstract">arXiv:2402.09386</a> [<a href="/pdf/2402.09386" title="Download PDF">pdf</a>, <a href="/ps/2402.09386" title="Download PostScript">ps</a>, <a href="/format/2402.09386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introduction to Physically Unclonable Fuctions: Properties and  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia-Bosque%2C+M">M. Garcia-Bosque</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%ADez-Se%C3%B1orans%2C+G">G. D&#xed;ez-Se&#xf1;orans</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez-Azqueta%2C+C">C. S&#xe1;nchez-Azqueta</a>, 
<a href="/search/cs?searchtype=author&query=Celma%2C+S">S. Celma</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of 2020 European Conference on Circuit Theory and
  Design (ECCTD)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">During the last years, Physically Unclonable Functions (PUFs) have become a
very important research area in the field of hardware security due to their
capability of generating volatile secret keys as well as providing a low-cost
authentication. In this paper, an introduction to Physically Unclonable
Functions is given, including their definition, properties and applications.
Finally, as an example of how to design a PUF, the general structure of a ring
oscillator PUF is presented.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09388" title="Abstract">arXiv:2402.09388</a> [<a href="/pdf/2402.09388" title="Download PDF">pdf</a>, <a href="/format/2402.09388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-regularized Point-based Value Iteration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delecki%2C+H">Harrison Delecki</a>, 
<a href="/search/cs?searchtype=author&query=Vazquez-Chanlatte%2C+M">Marcell Vazquez-Chanlatte</a>, 
<a href="/search/cs?searchtype=author&query=Yel%2C+E">Esen Yel</a>, 
<a href="/search/cs?searchtype=author&query=Wray%2C+K">Kyle Wray</a>, 
<a href="/search/cs?searchtype=author&query=Arnon%2C+T">Tomer Arnon</a>, 
<a href="/search/cs?searchtype=author&query=Witwicki%2C+S">Stefan Witwicki</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Model-based planners for partially observable problems must accommodate both
model uncertainty during planning and goal uncertainty during objective
inference. However, model-based planners may be brittle under these types of
uncertainty because they rely on an exact model and tend to commit to a single
optimal behavior. Inspired by results in the model-free setting, we propose an
entropy-regularized model-based planner for partially observable problems.
Entropy regularization promotes policy robustness for planning and objective
inference by encouraging policies to be no more committed to a single action
than necessary. We evaluate the robustness and objective inference performance
of entropy-regularized policies in three problem domains. Our results show that
entropy-regularized policies outperform non-entropy-regularized baselines in
terms of higher expected returns under modeling errors and higher accuracy
during objective inference.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09390" title="Abstract">arXiv:2402.09390</a> [<a href="/pdf/2402.09390" title="Download PDF">pdf</a>, <a href="/format/2402.09390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context  Learning in Factuality Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yihao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+S+W">Stephen W. Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaodan Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">With the widespread adoption of large language models (LLMs) in numerous
applications, the challenge of factuality and the propensity for hallucinations
raises significant concerns. To address this issue, particularly in
retrieval-augmented in-context learning, we introduce the hierarchical graph of
thoughts (HGOT), a structured, multi-layered graph approach designed to enhance
the retrieval of pertinent passages during in-context learning. The framework
utilizes the emergent planning capabilities of LLMs, employing the
divide-and-conquer strategy to break down complex queries into manageable
sub-queries. It refines self-consistency majority voting for answer selection,
which incorporates the recently proposed citation recall and precision metrics
to assess the quality of thoughts, linking an answer's credibility
intrinsically to the thought's quality. This methodology introduces a weighted
system in majority voting, prioritizing answers based on the citation quality
of their thoughts. Additionally, we propose a scoring mechanism for evaluating
retrieved passages, considering factors such as citation frequency and quality,
self-consistency confidence, and the retrieval module's ranking. Experiments
reveal that HGOT outperforms other retrieval-augmented in-context learning
methods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and
Retrieve-then-Read on different datasets by as much as $7\%$, demonstrating its
efficacy in enhancing the factuality of LLMs.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09391" title="Abstract">arXiv:2402.09391</a> [<a href="/pdf/2402.09391" title="Download PDF">pdf</a>, <a href="/format/2402.09391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LlaSMol: Advancing Large Language Models for Chemistry with a  Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Botao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Baker%2C+F+N">Frazier N. Baker</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xia Ning</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL)

</div>
<p class="mathjax">Chemistry plays a crucial role in many domains, such as drug discovery and
material science. While large language models (LLMs) such as GPT-4 exhibit
remarkable capabilities on natural language processing tasks, existing work
shows their performance on chemistry tasks is discouragingly low. In this
paper, however, we demonstrate that our developed LLMs can achieve very strong
results on a comprehensive set of chemistry tasks, outperforming the most
advanced GPT-4 across all the tasks by a substantial margin and approaching the
SoTA task-specific models. The key to our success is a large-scale,
comprehensive, high-quality dataset for instruction tuning named SMolInstruct.
It contains 14 meticulously selected chemistry tasks and over three million
high-quality samples, laying a solid foundation for training and evaluating
LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source
LLMs, among which, we find that Mistral serves as the best base model for
chemistry tasks. We further conduct analysis on the impact of trainable
parameters, providing insights for future research.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09392" title="Abstract">arXiv:2402.09392</a> [<a href="/pdf/2402.09392" title="Download PDF">pdf</a>, <a href="/format/2402.09392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raman%2C+A">Adithya Raman</a>, 
<a href="/search/cs?searchtype=author&query=Turkkan%2C+B">Bekir Turkkan</a>, 
<a href="/search/cs?searchtype=author&query=Kosar%2C+T">Tevfik Kosar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, 3 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Over the recent years, research and development in adaptive bitrate (ABR)
algorithms for live video streaming have been successful in improving users'
quality of experience (QoE) by reducing latency to near real-time levels while
delivering higher bitrate videos with minimal rebuffering time. However, the
QoE models used by these ABR algorithms do not take into account that a large
portion of live video streaming clients use mobile devices where a higher
bitrate does not necessarily translate into higher perceived quality. Ignoring
perceived quality results in playing videos at higher bitrates without a
significant increase in perceptual video quality and becomes a burden for
battery-constrained mobile devices due to higher energy consumption. In this
paper, we propose LL-GABR, a deep reinforcement learning approach that models
the QoE using perceived video quality instead of bitrate and uses energy
consumption along with other metrics like latency, rebuffering events, and
smoothness. LL-GABR makes no assumptions about the underlying video,
environment, or network settings and can operate flexibly on different video
titles, each having a different bitrate encoding ladder without additional
re-training, unlike existing learning-based ABRs. Trace-driven experimental
results show that LL-GABR outperforms the state-of-the-art approaches by up to
44% in terms of perceptual QoE and a 73% increase in energy efficiency as a
result of reducing net energy consumption by 11%.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09394" title="Abstract">arXiv:2402.09394</a> [<a href="/pdf/2402.09394" title="Download PDF">pdf</a>, <a href="/format/2402.09394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-form evaluation of model editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosati%2C+D">Domenic Rosati</a>, 
<a href="/search/cs?searchtype=author&query=Gonzales%2C+R">Robie Gonzales</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinkun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xuemin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Erkan%2C+M">Melis Erkan</a>, 
<a href="/search/cs?searchtype=author&query=Kayani%2C+Y">Yahya Kayani</a>, 
<a href="/search/cs?searchtype=author&query=Chavatapalli%2C+S+D">Satya Deepika Chavatapalli</a>, 
<a href="/search/cs?searchtype=author&query=Rudzicz%2C+F">Frank Rudzicz</a>, 
<a href="/search/cs?searchtype=author&query=Sajjad%2C+H">Hassan Sajjad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Evaluations of model editing currently only use the `next few token'
completions after a prompt. As a result, the impact of these methods on longer
natural language generation is largely unknown. We introduce long-form
evaluation of model editing (\textbf{\textit{LEME}}) a novel evaluation
protocol that measures the efficacy and impact of model editing in long-form
generative settings. Our protocol consists of a machine-rated survey and a
classifier which correlates well with human ratings. Importantly, we find that
our protocol has very little relationship with previous short-form metrics
(despite being designed to extend efficacy, generalization, locality, and
portability into a long-form setting), indicating that our method introduces a
novel set of dimensions for understanding model editing methods. Using this
protocol, we benchmark a number of model editing techniques and present several
findings including that, while some methods (ROME and MEMIT) perform well in
making consistent edits within a limited scope, they suffer much more from
factual drift than other methods. Finally, we present a qualitative analysis
that illustrates common failure modes in long-form generative settings
including internal consistency, lexical cohesion, and locality issues.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09398" title="Abstract">arXiv:2402.09398</a> [<a href="/pdf/2402.09398" title="Download PDF">pdf</a>, <a href="/format/2402.09398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Get More with LESS: Synthesizing Recurrence with KV Cache Compression  for Efficient LLM Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Harry Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Many computational factors limit broader deployment of large language models.
In this paper, we focus on a memory bottleneck imposed by the key-value (KV)
cache, a computational shortcut that requires storing previous KV pairs during
decoding. While existing KV cache methods approach this problem by pruning or
evicting large swaths of relatively less important KV pairs to dramatically
reduce the memory footprint of the cache, they can have limited success in
tasks that require recollecting a majority of previous tokens. To alleviate
this issue, we propose LESS, a simple integration of a (nearly free) constant
sized cache with eviction-based cache methods, such that all tokens can be
queried at later decoding steps. Its ability to retain information throughout
time shows merit on a variety of tasks where we demonstrate LESS can help
reduce the performance gap from caching everything, sometimes even matching it,
all while being efficient.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09401" title="Abstract">arXiv:2402.09401</a> [<a href="/pdf/2402.09401" title="Download PDF">pdf</a>, <a href="/format/2402.09401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning from Human Feedback with Active Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kaixuan Ji</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiafan He</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 1 figure, 4 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Aligning large language models (LLM) with human preference plays a key role
in building modern generative models and can be achieved by reinforcement
learning from human feedback (RLHF). Despite their superior performance,
current RLHF approaches often require a large amount of human-labelled
preference data, which is expensive to collect. In this paper, inspired by the
success of active learning, we address this problem by proposing
query-efficient RLHF methods. We first formalize the alignment problem as a
contextual dueling bandit problem and design an active-query-based proximal
policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret
bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the
dimension of feature space and $\Delta$ is the sub-optimality gap over all the
contexts. We then propose ADPO, a practical version of our algorithm based on
direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our
experiments show that ADPO, while only making about half of queries for human
preference, matches the performance of the state-of-the-art DPO method.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09403" title="Abstract">arXiv:2402.09403</a> [<a href="/pdf/2402.09403" title="Download PDF">pdf</a>, <a href="/format/2402.09403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auditing Private Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chadha%2C+K">Karan Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Jagielski%2C+M">Matthew Jagielski</a>, 
<a href="/search/cs?searchtype=author&query=Papernot%2C+N">Nicolas Papernot</a>, 
<a href="/search/cs?searchtype=author&query=Choquette-Choo%2C+C">Christopher Choquette-Choo</a>, 
<a href="/search/cs?searchtype=author&query=Nasr%2C+M">Milad Nasr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Differential privacy (DP) offers a theoretical upper bound on the potential
privacy leakage of analgorithm, while empirical auditing establishes a
practical lower bound. Auditing techniques exist forDP training algorithms.
However machine learning can also be made private at inference. We propose
thefirst framework for auditing private prediction where we instantiate
adversaries with varying poisoningand query capabilities. This enables us to
study the privacy leakage of four private prediction algorithms:PATE [Papernot
et al., 2016], CaPC [Choquette-Choo et al., 2020], PromptPATE [Duan et al.,
2023],and Private-kNN [Zhu et al., 2020]. To conduct our audit, we introduce
novel techniques to empiricallyevaluate privacy leakage in terms of Renyi DP.
Our experiments show that (i) the privacy analysis ofprivate prediction can be
improved, (ii) algorithms which are easier to poison lead to much higher
privacyleakage, and (iii) the privacy leakage is significantly lower for
adversaries without query control than thosewith full control.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09404" title="Abstract">arXiv:2402.09404</a> [<a href="/pdf/2402.09404" title="Download PDF">pdf</a>, <a href="/format/2402.09404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AQA-Bench: An Interactive Benchmark for Evaluating LLMs&#x27; Sequential  Reasoning Ability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Siwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bingchen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Cihang Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces AQA-Bench, a novel benchmark to assess the sequential
reasoning capabilities of large language models (LLMs) in algorithmic contexts,
such as depth-first search (DFS). The key feature of our evaluation benchmark
lies in its interactive evaluation protocol -- for example, in DFS, the
availability of each node's connected edge is contingent upon the model's
traversal to that node, thereby necessitating the LLM's ability to effectively
remember visited nodes and strategize subsequent moves. We comprehensively
build AQA-Bench with three different algorithms, namely binary search,
depth-first search, and breadth-first search, and to evaluate the sequential
reasoning ability of 12 different LLMs. Our investigations reveal several
interesting findings: (1) Closed-source models like GPT-4 and Gemini generally
show strong sequential reasoning ability, significantly outperforming
open-source LLMs. (2) Naively providing interactive examples may inadvertently
hurt few-shot performance. (3) A very limited number of predecessor steps
following the optimal policy can substantially boost small models' performance.
(4) The scaling correlation between performance and model size is not always
significant, sometimes even showcasing an inverse trend. We hope our study can
catalyze future work on advancing the understanding and enhancement of LLMs'
capabilities in sequential reasoning. The code is available at
https://github.com/UCSC-VLAA/AQA-Bench.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Thu, 15 Feb 24</h3>
<dl>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00478" title="Abstract">arXiv:2402.00478</a> (cross-list from quant-ph) [<a href="/pdf/2402.00478" title="Download PDF">pdf</a>, <a href="/format/2402.00478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource Bounds for Quantum Circuit Mapping via Quantum Circuit  Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Steinberg%2C+M">Matthew Steinberg</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bandic%2C+M">Medina Bandic</a>, 
<a href="/search/quant-ph?searchtype=author&query=Szkudlarek%2C+S">Sacha Szkudlarek</a>, 
<a href="/search/quant-ph?searchtype=author&query=Almudever%2C+C+G">Carmen G. Almudever</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sarkar%2C+A">Aritra Sarkar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Feld%2C+S">Sebastian Feld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Efficiently mapping quantum circuits onto hardware is an integral part of the
quantum compilation process, wherein a quantum circuit is modified in
accordance with the stringent architectural demands of a quantum processor.
Many techniques exist for solving the quantum circuit mapping problem, many of
which relate quantum circuit mapping to classical computer science. This work
considers a novel perspective on quantum circuit mapping, in which the routing
process of a simplified circuit is viewed as a composition of quantum
operations acting on density matrices representing the quantum circuit and
processor. Drawing on insight from recent advances in quantum information
theory and information geometry, we show that a minimal SWAP gate count for
executing a quantum circuit on a device emerges via the minimization of the
distance between quantum states using the quantum Jensen-Shannon divergence.
Additionally, we develop a novel initial placement algorithm based on a graph
similarity search that selects the partition nearest to a graph isomorphism
between interaction and coupling graphs. From these two ingredients, we then
construct a polynomial-time algorithm for calculating the SWAP gate lower
bound, which is directly compared alongside the IBM Qiskit compiler for over
600 realistic benchmark experiments, as well as against a brute-force method
for smaller benchmarks. In our simulations, we unambiguously find that neither
the brute-force method nor the Qiskit compiler surpass our bound, implying
utility as a precise estimation of minimal overhead when realizing quantum
algorithms on constrained quantum hardware. This work constitutes the first use
of quantum circuit uncomplexity to practically-relevant quantum computing. We
anticipate that this method may have diverse applicability outside of the scope
of quantum information science, and we discuss several of these possibilities.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08143" title="Abstract">arXiv:2402.08143</a> (cross-list from econ.GN) [<a href="/pdf/2402.08143" title="Download PDF">pdf</a>, <a href="/ps/2402.08143" title="Download PostScript">ps</a>, <a href="/format/2402.08143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial intelligence and the transformation of higher education  institutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Katsamakas%2C+E">Evangelos Katsamakas</a>, 
<a href="/search/econ?searchtype=author&query=Pavlov%2C+O+V">Oleg V. Pavlov</a>, 
<a href="/search/econ?searchtype=author&query=Saklad%2C+R">Ryan Saklad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Artificial intelligence (AI) advances and the rapid adoption of generative AI
tools like ChatGPT present new opportunities and challenges for higher
education. While substantial literature discusses AI in higher education, there
is a lack of a systemic approach that captures a holistic view of the AI
transformation of higher education institutions (HEIs). To fill this gap, this
article, taking a complex systems approach, develops a causal loop diagram
(CLD) to map the causal feedback mechanisms of AI transformation in a typical
HEI. Our model accounts for the forces that drive the AI transformation and the
consequences of the AI transformation on value creation in a typical HEI. The
article identifies and analyzes several reinforcing and balancing feedback
loops, showing how, motivated by AI technology advances, the HEI invests in AI
to improve student learning, research, and administration. The HEI must take
measures to deal with academic integrity problems and adapt to changes in
available jobs due to AI, emphasizing AI-complementary skills for its students.
However, HEIs face a competitive threat and several policy traps that may lead
to decline. HEI leaders need to become systems thinkers to manage the
complexity of the AI transformation and benefit from the AI feedback loops
while avoiding the associated pitfalls. We also discuss long-term scenarios,
the notion of HEIs influencing the direction of AI, and directions for future
research on AI transformation.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08687" title="Abstract">arXiv:2402.08687</a> (cross-list from stat.AP) [<a href="/pdf/2402.08687" title="Download PDF">pdf</a>, <a href="/format/2402.08687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuzzy clustering of circular time series based on a new dependence  measure with applications to wind data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=L%C3%B3pez-Oriona%2C+%C3%81">&#xc1;ngel L&#xf3;pez-Oriona</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+Y">Ying Sun</a>, 
<a href="/search/stat?searchtype=author&query=Crujeiras%2C+R+M">Rosa M. Crujeiras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2304.12249">arXiv:2304.12249</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Time series clustering is an essential machine learning task with
applications in many disciplines. While the majority of the methods focus on
time series taking values on the real line, very few works consider time series
defined on the unit circle, although the latter objects frequently arise in
many applications. In this paper, the problem of clustering circular time
series is addressed. To this aim, a distance between circular series is
introduced and used to construct a clustering procedure. The metric relies on a
new measure of serial dependence considering circular arcs, thus taking
advantage of the directional character inherent to the series range. Since the
dynamics of the series may vary over the time, we adopt a fuzzy approach, which
enables the procedure to locate each series into several clusters with
different membership degrees. The resulting clustering algorithm is able to
group series generated from similar stochastic processes, reaching accurate
results with series coming from a broad variety of models. An extensive
simulation study shows that the proposed method outperforms several alternative
techniques, besides being computationally efficient. Two interesting
applications involving time series of wind direction in Saudi Arabia highlight
the potential of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08688" title="Abstract">arXiv:2402.08688</a> (cross-list from stat.AP) [<a href="/pdf/2402.08688" title="Download PDF">pdf</a>, <a href="/format/2402.08688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-Aware Automated Passenger Counting Data Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cherrier%2C+N">No&#xeb;lie Cherrier</a>, 
<a href="/search/stat?searchtype=author&query=R%C3%A9rolle%2C+B">Baptiste R&#xe9;rolle</a>, 
<a href="/search/stat?searchtype=author&query=Graive%2C+M">Martin Graive</a>, 
<a href="/search/stat?searchtype=author&query=Dib%2C+A">Amir Dib</a>, 
<a href="/search/stat?searchtype=author&query=Schmitt%2C+E">Eglantine Schmitt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in this version to ITSC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A reliable and accurate knowledge of the ridership in public transportation
networks is crucial for public transport operators and public authorities to be
aware of their network's use and optimize transport offering. Several
techniques to estimate ridership exist nowadays, some of them in an automated
manner. Among them, Automatic Passenger Counting (APC) systems detect
passengers entering and leaving the vehicle at each station of its course.
However, data resulting from these systems are often noisy or even biased,
resulting in under or overestimation of onboard occupancy. In this work, we
propose a denoising algorithm for APC data to improve their robustness and ease
their analyzes. The proposed approach consists in a constrained integer linear
optimization, taking advantage of ticketing data and historical ridership data
to further constrain and guide the optimization. The performances are assessed
and compared to other denoising methods on several public transportation
networks in France, to manual counts available on one of these networks, and on
simulated data.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08692" title="Abstract">arXiv:2402.08692</a> (cross-list from eess.IV) [<a href="/pdf/2402.08692" title="Download PDF">pdf</a>, <a href="/format/2402.08692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inference Stage Denoising for Undersampled MRI Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xue%2C+Y">Yuyang Xue</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+C">Chen Qin</a>, 
<a href="/search/eess?searchtype=author&query=Tsaftaris%2C+S+A">Sotirios A. Tsaftaris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reconstruction of magnetic resonance imaging (MRI) data has been positively
affected by deep learning. A key challenge remains: to improve generalisation
to distribution shifts between the training and testing data. Most approaches
aim to address this via inductive design or data augmentation. However, they
can be affected by misleading data, e.g. random noise, and cases where the
inference stage data do not match assumptions in the modelled shifts. In this
work, by employing a conditional hyperparameter network, we eliminate the need
of augmentation, yet maintain robust performance under various levels of
Gaussian noise. We demonstrate that our model withstands various input noise
levels while producing high-definition reconstructions during the test stage.
Moreover, we present a hyperparameter sampling strategy that accelerates the
convergence of training. Our proposed method achieves the highest accuracy and
image quality in all settings compared to baseline methods.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08697" title="Abstract">arXiv:2402.08697</a> (cross-list from eess.IV) [<a href="/pdf/2402.08697" title="Download PDF">pdf</a>, <a href="/format/2402.08697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Detection of Pheochromocytomas and Paragangliomas in  CT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Oluigboa%2C+D+C">David C. Oluigboa</a>, 
<a href="/search/eess?searchtype=author&query=Santra%2C+B">Bikash Santra</a>, 
<a href="/search/eess?searchtype=author&query=Mathai%2C+T+S">Tejas Sudharshan Mathai</a>, 
<a href="/search/eess?searchtype=author&query=Mukherjee%2C+P">Pritam Mukherjee</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jianfei Liu</a>, 
<a href="/search/eess?searchtype=author&query=Jha%2C+A">Abhishek Jha</a>, 
<a href="/search/eess?searchtype=author&query=Patel%2C+M">Mayank Patel</a>, 
<a href="/search/eess?searchtype=author&query=Pacak%2C+K">Karel Pacak</a>, 
<a href="/search/eess?searchtype=author&query=Summers%2C+R+M">Ronald M. Summers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SPIE 2024. arXiv admin note: text overlap with <a href="/abs/2402.00175">arXiv:2402.00175</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Pheochromocytomas and Paragangliomas (PPGLs) are rare adrenal and
extra-adrenal tumors which have the potential to metastasize. For the
management of patients with PPGLs, CT is the preferred modality of choice for
precise localization and estimation of their progression. However, due to the
myriad variations in size, morphology, and appearance of the tumors in
different anatomical regions, radiologists are posed with the challenge of
accurate detection of PPGLs. Since clinicians also need to routinely measure
their size and track their changes over time across patient visits, manual
demarcation of PPGLs is quite a time-consuming and cumbersome process. To
ameliorate the manual effort spent for this task, we propose an automated
method to detect PPGLs in CT studies via a proxy segmentation task. As only
weak annotations for PPGLs in the form of prospectively marked 2D bounding
boxes on an axial slice were available, we extended these 2D boxes into weak 3D
annotations and trained a 3D full-resolution nnUNet model to directly segment
PPGLs. We evaluated our approach on a dataset consisting of
chest-abdomen-pelvis CTs of 255 patients with confirmed PPGLs. We obtained a
precision of 70% and sensitivity of 64.1% with our proposed approach when
tested on 53 CT studies. Our findings highlight the promising nature of
detecting PPGLs via segmentation, and furthers the state-of-the-art in this
exciting yet challenging area of rare cancer management.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08703" title="Abstract">arXiv:2402.08703</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.08703" title="Download PDF">pdf</a>, <a href="/format/2402.08703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Generative AI for De Novo Drug Design: New Frontiers in  Molecule and Protein Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/q-bio?searchtype=author&query=Dai%2C+H">Howard Dai</a>, 
<a href="/search/q-bio?searchtype=author&query=Knight%2C+E">Elizabeth Knight</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+F">Fang Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+Y">Yunyang Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+T">Tianxiao Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Artificial intelligence (AI)-driven methods can vastly improve the
historically costly drug design process, with various generative models already
in widespread use. Generative models for de novo drug design, in particular,
focus on the creation of novel biological compounds entirely from scratch,
representing a promising future direction. Rapid development in the field,
combined with the inherent complexity of the drug design process, creates a
difficult landscape for new researchers to enter. In this survey, we organize
de novo drug design into two overarching themes: small molecule and protein
generation. Within each theme, we identify a variety of subtasks and
applications, highlighting important datasets, benchmarks, and model
architectures and comparing the performance of top models. We take a broad
approach to AI-driven drug design, allowing for both micro-level comparisons of
various methods within each subtask and macro-level observations across
different fields. We discuss parallel challenges and approaches between the two
applications and highlight future directions for AI-driven de novo drug design
as a whole. An organized repository of all covered sources is available at
https://github.com/gersteinlab/GenAI4Drug.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08708" title="Abstract">arXiv:2402.08708</a> (cross-list from physics.chem-ph) [<a href="/pdf/2402.08708" title="Download PDF">pdf</a>, <a href="/format/2402.08708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero Shot Molecular Generation via Similarity Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Elijo%C5%A1ius%2C+R">Rokas Elijo&#x161;ius</a>, 
<a href="/search/physics?searchtype=author&query=Zills%2C+F">Fabian Zills</a>, 
<a href="/search/physics?searchtype=author&query=Batatia%2C+I">Ilyes Batatia</a>, 
<a href="/search/physics?searchtype=author&query=Norwood%2C+S+W">Sam Walton Norwood</a>, 
<a href="/search/physics?searchtype=author&query=Kov%C3%A1cs%2C+D+P">D&#xe1;vid P&#xe9;ter Kov&#xe1;cs</a>, 
<a href="/search/physics?searchtype=author&query=Holm%2C+C">Christian Holm</a>, 
<a href="/search/physics?searchtype=author&query=Cs%C3%A1nyi%2C+G">G&#xe1;bor Cs&#xe1;nyi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative modelling aims to accelerate the discovery of novel chemicals by
directly proposing structures with desirable properties. Recently, score-based,
or diffusion, generative models have significantly outperformed previous
approaches. Key to their success is the close relationship between the score
and physical force, allowing the use of powerful equivariant neural networks.
However, the behaviour of the learnt score is not yet well understood. Here, we
analyse the score by training an energy-based diffusion model for molecular
generation. We find that during the generation the score resembles a
restorative potential initially and a quantum-mechanical force at the end. In
between the two endpoints, it exhibits special properties that enable the
building of large molecules. Using insights from the trained model, we present
Similarity-based Molecular Generation (SiMGen), a new method for zero shot
molecular generation. SiMGen combines a time-dependent similarity kernel with
descriptors from a pretrained machine learning force field to generate
molecules without any further training. Our approach allows full control over
the molecular shape through point cloud priors and supports conditional
generation. We also release an interactive web tool that allows users to
generate structures with SiMGen online (https://zndraw.icp.uni-stuttgart.de).
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08711" title="Abstract">arXiv:2402.08711</a> (cross-list from stat.ML) [<a href="/pdf/2402.08711" title="Download PDF">pdf</a>, <a href="/ps/2402.08711" title="Download PostScript">ps</a>, <a href="/format/2402.08711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correction to &quot;Wasserstein distance estimates for the distributions of  numerical approximations to ergodic stochastic differential equations&quot;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Paulin%2C+D">Daniel Paulin</a>, 
<a href="/search/stat?searchtype=author&query=Whalley%2C+P+A">Peter A. Whalley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Comment on <a href="https://doi.org/10.1080/14685248.2020.1855352">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA); Probability (math.PR)

</div>
<p class="mathjax">A method for analyzing non-asymptotic guarantees of numerical discretizations
of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and
Zygalakis in ``Wasserstein distance estimates for the distributions of
numerical approximations to ergodic stochastic differential equations". They
analyze the UBU integrator which is strong order two and only requires one
gradient evaluation per step, resulting in desirable non-asymptotic guarantees,
in particular $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ steps to reach a distance
of $\epsilon &gt; 0$ in Wasserstein-2 distance away from the target distribution.
However, there is a mistake in the local error estimates in Sanz-Serna and
Zygalakis (2021), in particular, a stronger assumption is needed to achieve
these complexity estimates. This note reconciles the theory with the dimension
dependence observed in practice in many applications of interest.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08726" title="Abstract">arXiv:2402.08726</a> (cross-list from quant-ph) [<a href="/pdf/2402.08726" title="Download PDF">pdf</a>, <a href="/format/2402.08726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trained quantum neural networks are Gaussian processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Girardi%2C+F">Filippo Girardi</a>, 
<a href="/search/quant-ph?searchtype=author&query=De+Palma%2C+G">Giacomo De Palma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 151 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Mathematical Physics (math-ph); Probability (math.PR)

</div>
<p class="mathjax">We study quantum neural networks made by parametric one-qubit gates and fixed
two-qubit gates in the limit of infinite width, where the generated function is
the expectation value of the sum of single-qubit observables over all the
qubits. First, we prove that the probability distribution of the function
generated by the untrained network with randomly initialized parameters
converges in distribution to a Gaussian process whenever each measured qubit is
correlated only with few other measured qubits. Then, we analytically
characterize the training of the network via gradient descent with square loss
on supervised learning problems. We prove that, as long as the network is not
affected by barren plateaus, the trained network can perfectly fit the training
set and that the probability distribution of the function generated after
training still converges in distribution to a Gaussian process. Finally, we
consider the statistical noise of the measurement at the output of the network
and prove that a polynomial number of measurements is sufficient for all the
previous results to hold and that the network can always be trained in
polynomial time.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08752" title="Abstract">arXiv:2402.08752</a> (cross-list from quant-ph) [<a href="/pdf/2402.08752" title="Download PDF">pdf</a>, <a href="/format/2402.08752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge coloring lattice graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kattem%C3%B6lle%2C+J">Joris Kattem&#xf6;lle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">We develop the theory of the edge coloring of infinite lattice graphs,
proving a necessary and sufficient condition for a proper edge coloring of a
patch of a lattice graph to induce a proper edge coloring of the entire lattice
graph by translation. This condition forms the cornerstone of a method that
finds nearly minimal or minimal edge colorings of infinite lattice graphs. In
case a nearly minimal edge coloring is requested, the running time is $O(\mu^2
D^4)$, where $\mu$ is the number of edges in one cell (or `basis graph') of the
lattice graph and $D$ is the maximum distance between two cells so that there
is an edge from within one cell to the other. In case a minimal edge coloring
is requested, we lack an upper bound on the running time, which we find need
not pose a limitation in practice; we use the method to minimal edge color the
meshes of all $k$-uniform tilings of the plane for $k\leq 6$, while utilizing
modest computational resources. We find that all these lattice graphs are
Vizing class~I. Relating edge colorings to quantum circuits, our work finds
direct application by offering minimal-depth quantum circuits in the areas of
quantum simulation, quantum optimization, and quantum state verification.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08768" title="Abstract">arXiv:2402.08768</a> (cross-list from eess.IV) [<a href="/pdf/2402.08768" title="Download PDF">pdf</a>, <a href="/format/2402.08768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarially Robust Feature Learning for Breast Cancer Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hao%2C+D">Degan Hao</a>, 
<a href="/search/eess?searchtype=author&query=Arefan%2C+D">Dooman Arefan</a>, 
<a href="/search/eess?searchtype=author&query=Zuley%2C+M">Margarita Zuley</a>, 
<a href="/search/eess?searchtype=author&query=Berg%2C+W">Wendie Berg</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+S">Shandong Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Adversarial data can lead to malfunction of deep learning applications. It is
essential to develop deep learning models that are robust to adversarial data
while accurate on standard, clean data. In this study, we proposed a novel
adversarially robust feature learning (ARFL) method for a real-world
application of breast cancer diagnosis. ARFL facilitates adversarial training
using both standard data and adversarial data, where a feature correlation
measure is incorporated as an objective function to encourage learning of
robust features and restrain spurious features. To show the effects of ARFL in
breast cancer diagnosis, we built and evaluated diagnosis models using two
independent clinically collected breast imaging datasets, comprising a total of
9,548 mammogram images. We performed extensive experiments showing that our
method outperformed several state-of-the-art methods and that our method can
enhance safer breast cancer diagnosis against adversarial attacks in clinical
settings.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08777" title="Abstract">arXiv:2402.08777</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.08777" title="Download PDF">pdf</a>, <a href="/format/2402.08777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhou%2C+Z">Zhihan Zhou</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+W">Winmin Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Ho%2C+H">Harrison Ho</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+J">Jiayi Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Shi%2C+L">Lizhen Shi</a>, 
<a href="/search/q-bio?searchtype=author&query=Davuluri%2C+R+V">Ramana V Davuluri</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Z">Zhong Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+H">Han Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL)

</div>
<p class="mathjax">Effective DNA embedding remains crucial in genomic analysis, particularly in
scenarios lacking labeled data for model fine-tuning, despite the significant
advancements in genome foundation models. A prime example is metagenomics
binning, a critical process in microbiome research that aims to group DNA
sequences by their species from a complex mixture of DNA sequences derived from
potentially thousands of distinct, often uncharacterized species. To fill the
lack of effective DNA embedding models, we introduce DNABERT-S, a genome
foundation model that specializes in creating species-aware DNA embeddings. To
encourage effective embeddings to error-prone long-read DNA sequences, we
introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes
the hidden representations of DNA sequences at randomly selected layers and
trains the model to recognize and differentiate these mixed proportions at the
output layer. We further enhance it with the proposed Curriculum Contrastive
Learning (C$^2$LR) strategy. Empirical results on 18 diverse datasets showed
DNABERT-S's remarkable performance. It outperforms the top baseline's
performance in 10-shot species classification with just a 2-shot training while
doubling the Adjusted Rand Index (ARI) in species clustering and substantially
increasing the number of correctly identified species in metagenomics binning.
The code, data, and pre-trained model are publicly available at
https://github.com/Zhihan1996/DNABERT_S.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08779" title="Abstract">arXiv:2402.08779</a> (cross-list from math.OC) [<a href="/pdf/2402.08779" title="Download PDF">pdf</a>, <a href="/format/2402.08779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Contract Negotiation in Financial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jalan%2C+A">Akhil Jalan</a>, 
<a href="/search/math?searchtype=author&query=Chakrabarti%2C+D">Deepayan Chakrabarti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">How can firms optimally negotiate bilateral contracts with each other in a
financial network? Every firm seeks to maximize the utility it gains from its
portfolio of contracts. We focus on mean-variance utilities, where each firm
has its own beliefs about the expected returns of the contracts and the
covariances between them (Markowitz, J. Finance 7(11), 1952). Instead of
revealing these beliefs, a firm may adopt a different negotiating position,
seeking better contract terms. We formulate a contract negotiation process by
which such strategic behavior leads to a network of contracts. In our
formulation, any subset of firms can be strategic. The negotiating positions of
these firms can form Nash equilibria, where each firm's position is optimal
given the others' positions.
<br />We give a polynomial-time algorithm to find the Nash equilibria, if they
exist, and certify their nonexistence otherwise. We explore the implications of
such equilibria on several model networks. These illustrate that firms'
utilities can be sensitive to their negotiating position. We then propose trade
deadlines as a mechanism to reduce the need for strategic behavior. At the
deadline, each firm can unilaterally cancel some or all of its contracts, for a
penalty. In our model networks, we show that trade deadlines can reduce the
loss of utility from being honest. We empirically verify our insights using
data on international trade between 46 large economies.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08789" title="Abstract">arXiv:2402.08789</a> (cross-list from eess.AS) [<a href="/pdf/2402.08789" title="Download PDF">pdf</a>, <a href="/format/2402.08789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging cough sounds to optimize chest x-ray usage in low-resource  settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Philip%2C+A">Alexander Philip</a>, 
<a href="/search/eess?searchtype=author&query=Chawla%2C+S">Sanya Chawla</a>, 
<a href="/search/eess?searchtype=author&query=Jover%2C+L">Lola Jover</a>, 
<a href="/search/eess?searchtype=author&query=Kafentzis%2C+G+P">George P. Kafentzis</a>, 
<a href="/search/eess?searchtype=author&query=Brew%2C+J">Joe Brew</a>, 
<a href="/search/eess?searchtype=author&query=Saraf%2C+V">Vishakh Saraf</a>, 
<a href="/search/eess?searchtype=author&query=Vijayan%2C+S">Shibu Vijayan</a>, 
<a href="/search/eess?searchtype=author&query=Small%2C+P">Peter Small</a>, 
<a href="/search/eess?searchtype=author&query=Chaccour%2C+C">Carlos Chaccour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Chest X-ray is a commonly used tool during triage, diagnosis and management
of respiratory diseases. In resource-constricted settings, optimizing this
resource can lead to valuable cost savings for the health care system and the
patients as well as to and improvement in consult time. We used
prospectively-collected data from 137 patients referred for chest X-ray at the
Christian Medical Center and Hospital (CMCH) in Purnia, Bihar, India. Each
patient provided at least five coughs while awaiting radiography. Collected
cough sounds were analyzed using acoustic AI methods. Cross-validation was done
on temporal and spectral features on the cough sounds of each patient. Features
were summarized using standard statistical approaches. Three models were
developed, tested and compared in their capacity to predict an abnormal result
in the chest X-ray. All three methods yielded models that could discriminate to
some extent between normal and abnormal with the logistic regression performing
best with an area under the receiver operating characteristic curves ranging
from 0.7 to 0.78. Despite limitations and its relatively small sample size,
this study shows that AI-enabled algorithms can use cough sounds to predict
which individuals presenting for chest radiographic examination will have a
normal or abnormal results. These results call for expanding this research
given the potential optimization of limited health care resources in low- and
middle-income countries.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08811" title="Abstract">arXiv:2402.08811</a> (cross-list from eess.IV) [<a href="/pdf/2402.08811" title="Download PDF">pdf</a>, <a href="/ps/2402.08811" title="Download PostScript">ps</a>, <a href="/format/2402.08811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep and shallow data science for multi-scale optical neuroscience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mishne%2C+G">Gal Mishne</a>, 
<a href="/search/eess?searchtype=author&query=Charles%2C+A">Adam Charles</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Optical imaging of the brain has expanded dramatically in the past two
decades. New optics, indicators, and experimental paradigms are now enabling
in-vivo imaging from the synaptic to the cortex-wide scales. To match the
resulting flood of data across scales, computational methods are continuously
being developed to meet the need of extracting biologically relevant
information. In this pursuit, challenges arise in some domains (e.g., SNR and
resolution limits in micron-scale data) that require specialized algorithms.
These algorithms can, for example, make use of state-of-the-art machine
learning to maximally learn the details of a given scale to optimize the
processing pipeline. In contrast, other methods, however, such as graph signal
processing, seek to abstract away from some of the details that are
scale-specific to provide solutions to specific sub-problems common across
scales of neuroimaging. Here we discuss limitations and tradeoffs in
algorithmic design with the goal of identifying how data quality and
variability can hamper algorithm use and dissemination.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08813" title="Abstract">arXiv:2402.08813</a> (cross-list from math.OC) [<a href="/pdf/2402.08813" title="Download PDF">pdf</a>, <a href="/format/2402.08813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model approximation in MDPs with unbounded per-step cost
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bozkurt%2C+B">Berk Bozkurt</a>, 
<a href="/search/math?searchtype=author&query=Mahajan%2C+A">Aditya Mahajan</a>, 
<a href="/search/math?searchtype=author&query=Nayyar%2C+A">Ashutosh Nayyar</a>, 
<a href="/search/math?searchtype=author&query=Ouyang%2C+Y">Yi Ouyang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">We consider the problem of designing a control policy for an infinite-horizon
discounted cost Markov decision process $\mathcal{M}$ when we only have access
to an approximate model $\hat{\mathcal{M}}$. How well does an optimal policy
$\hat{\pi}^{\star}$ of the approximate model perform when used in the original
model $\mathcal{M}$? We answer this question by bounding a weighted norm of the
difference between the value function of $\hat{\pi}^\star $ when used in
$\mathcal{M}$ and the optimal value function of $\mathcal{M}$. We then extend
our results and obtain potentially tighter upper bounds by considering affine
transformations of the per-step cost. We further provide upper bounds that
explicitly depend on the weighted distance between cost functions and weighted
distance between transition kernels of the original and approximate models. We
present examples to illustrate our results.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08818" title="Abstract">arXiv:2402.08818</a> (cross-list from stat.ML) [<a href="/pdf/2402.08818" title="Download PDF">pdf</a>, <a href="/format/2402.08818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corridor Geometry in Gradient-Based Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dherin%2C+B">Benoit Dherin</a>, 
<a href="/search/stat?searchtype=author&query=Rosca%2C+M">Mihaela Rosca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">We characterize regions of a loss surface as corridors when the continuous
curves of steepest descent -- the solutions of the gradient flow -- become
straight lines. We show that corridors provide insights into gradient-based
optimization, since corridors are exactly the regions where gradient descent
and the gradient flow follow the same trajectory, while the loss decreases
linearly. As a result, inside corridors there are no implicit regularization
effects or training instabilities that have been shown to occur due to the
drift between gradient descent and the gradient flow. Using the loss linear
decrease on corridors, we devise a learning rate adaptation scheme for gradient
descent; we call this scheme Corridor Learning Rate (CLR). The CLR formulation
coincides with a special case of Polyak step-size, discovered in the context of
convex optimization. The Polyak step-size has been shown recently to have also
good convergence properties for neural networks; we further confirm this here
with results on CIFAR-10 and ImageNet.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08821" title="Abstract">arXiv:2402.08821</a> (cross-list from math.OC) [<a href="/pdf/2402.08821" title="Download PDF">pdf</a>, <a href="/format/2402.08821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Problem-Parameter-Free Decentralized Nonconvex Stochastic Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jiaxiang Li</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+X">Xuxing Chen</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+S">Shiqian Ma</a>, 
<a href="/search/math?searchtype=author&query=Hong%2C+M">Mingyi Hong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Existing decentralized algorithms usually require knowledge of problem
parameters for updating local iterates. For example, the hyperparameters (such
as learning rate) usually require the knowledge of Lipschitz constant of the
global gradient or topological information of the communication networks, which
are usually not accessible in practice. In this paper, we propose D-NASA, the
first algorithm for decentralized nonconvex stochastic optimization that
requires no prior knowledge of any problem parameters. We show that D-NASA has
the optimal rate of convergence for nonconvex objectives under very mild
conditions and enjoys the linear-speedup effect, i.e. the computation becomes
faster as the number of nodes in the system increases. Extensive numerical
experiments are conducted to support our findings.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08827" title="Abstract">arXiv:2402.08827</a> (cross-list from math.OC) [<a href="/pdf/2402.08827" title="Download PDF">pdf</a>, <a href="/ps/2402.08827" title="Download PostScript">ps</a>, <a href="/format/2402.08827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Mixed Integer Trust Region Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Del+Pia%2C+A">Alberto Del Pia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">In this paper we consider the problem of minimizing a general quadratic
function over the mixed integer points in an ellipsoid. This problem is
strongly NP-hard, NP-hard to approximate within a constant factor, and optimal
solutions can be irrational. In our main result we show that an arbitrarily
good solution can be found in polynomial time, if we fix the number of integer
variables. This algorithm provides a natural extension to the mixed integer
setting, of the polynomial solvability of the trust region problem proven by
Ye, Karmarkar, Vavasis, and Zippel. Our result removes a key bottleneck in the
design and analysis of model trust region methods for mixed integer nonlinear
optimization problems. The techniques introduced to prove this result are of
independent interest and can be used in other mixed integer programming
problems involving quadratic functions. As an example we consider the problem
of minimizing a general quadratic function over the mixed integer points in a
polyhedron. For this problem, we show that a solution satisfying weak bounds
with respect to optimality can be computed in polynomial time, provided that
the number of integer variables is fixed. It is well-known that finding a
solution satisfying stronger bounds cannot be done in polynomial time, unless
P=NP.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08847" title="Abstract">arXiv:2402.08847</a> (cross-list from stat.ML) [<a href="/pdf/2402.08847" title="Download PDF">pdf</a>, <a href="/format/2402.08847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space-Time Bridge-Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Behjoo%2C+H">Hamidreza Behjoo</a>, 
<a href="/search/stat?searchtype=author&query=Chertkov%2C+M">Michael Chertkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, we introduce a novel method for generating new synthetic
samples that are independent and identically distributed (i.i.d.) from
high-dimensional real-valued probability distributions, as defined implicitly
by a set of Ground Truth (GT) samples. Central to our method is the integration
of space-time mixing strategies that extend across temporal and spatial
dimensions. Our methodology is underpinned by three interrelated stochastic
processes designed to enable optimal transport from an easily tractable initial
probability distribution to the target distribution represented by the GT
samples: (a) linear processes incorporating space-time mixing that yield
Gaussian conditional probability densities, (b) their bridge-diffusion analogs
that are conditioned to the initial and final state vectors, and (c) nonlinear
stochastic processes refined through score-matching techniques. The crux of our
training regime involves fine-tuning the nonlinear model, and potentially the
linear models - to align closely with the GT data. We validate the efficacy of
our space-time diffusion approach with numerical experiments, laying the
groundwork for more extensive future theory and experiments to fully
authenticate the method, particularly providing a more efficient (possibly
simulation-free) inference.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08879" title="Abstract">arXiv:2402.08879</a> (cross-list from econ.EM) [<a href="/pdf/2402.08879" title="Download PDF">pdf</a>, <a href="/format/2402.08879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inference for an Algorithmic Fairness-Accuracy Frontier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Liu%2C+Y">Yiqi Liu</a>, 
<a href="/search/econ?searchtype=author&query=Molinari%2C+F">Francesca Molinari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Decision-making processes increasingly rely on the use of algorithms. Yet,
algorithms' predictive ability frequently exhibit systematic variation across
subgroups of the population. While both fairness and accuracy are desirable
properties of an algorithm, they often come at the cost of one another. What
should a fairness-minded policymaker do then, when confronted with finite data?
In this paper, we provide a consistent estimator for a theoretical
fairness-accuracy frontier put forward by Liang, Lu and Mu (2023) and propose
inference methods to test hypotheses that have received much attention in the
fairness literature, such as (i) whether fully excluding a covariate from use
in training the algorithm is optimal and (ii) whether there are less
discriminatory alternatives to an existing algorithm. We also provide an
estimator for the distance between a given algorithm and the fairest point on
the frontier, and characterize its asymptotic distribution. We leverage the
fact that the fairness-accuracy frontier is part of the boundary of a convex
set that can be fully represented by its support function. We show that the
estimated support function converges to a tight Gaussian process as the sample
size increases, and then express policy-relevant hypotheses as restrictions on
the support function to construct valid test statistics.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08890" title="Abstract">arXiv:2402.08890</a> (cross-list from astro-ph.SR) [<a href="/pdf/2402.08890" title="Download PDF">pdf</a>, <a href="/format/2402.08890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting the Emergence of Solar Active Regions Using Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Kasapis%2C+S">Spiridon Kasapis</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kitiashvili%2C+I+N">Irina N. Kitiashvili</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kosovichev%2C+A+G">Alexander G. Kosovichev</a>, 
<a href="/search/astro-ph?searchtype=author&query=Stefan%2C+J+T">John T. Stefan</a>, 
<a href="/search/astro-ph?searchtype=author&query=Apte%2C+B">Bhairavi Apte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, IAU Symposium 365 Proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Solar and Stellar Astrophysics (astro-ph.SR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">To create early warning capabilities for upcoming Space Weather disturbances,
we have selected a dataset of 61 emerging active regions, which allows us to
identify characteristic features in the evolution of acoustic power density to
predict continuum intensity emergence. For our study, we have utilized Doppler
shift and continuum intensity observations from the Helioseismic and Magnetic
Imager (HMI) onboard the Solar Dynamics Observatory (SDO). The local tracking
of 30.66 x 30.66-degree patches in the vicinity of active regions allowed us to
trace the evolution of active regions starting from the pre-emergence state. We
have developed a machine learning model to capture the acoustic power flux
density variations associated with upcoming magnetic flux emergence. The
trained Long Short-Term Memory (LSTM) model is able to predict 5 hours ahead
whether, in a given area of the solar surface, continuum intensity values will
decrease. The performed study allows us to investigate the potential of the
machine learning approach to predict the emergence of active regions using
acoustic power maps as input.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08898" title="Abstract">arXiv:2402.08898</a> (cross-list from eess.AS) [<a href="/pdf/2402.08898" title="Download PDF">pdf</a>, <a href="/format/2402.08898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fan%2C+R">Ruchao Fan</a>, 
<a href="/search/eess?searchtype=author&query=Shanka%2C+N+B">Natarajan Balaji Shanka</a>, 
<a href="/search/eess?searchtype=author&query=Alwan%2C+A">Abeer Alwan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in IEEE Signal Processing Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">Non-autoregressive automatic speech recognition (NASR) models have gained
attention due to their parallelism and fast inference. The encoder-based NASR,
e.g. connectionist temporal classification (CTC), can be initialized from the
speech foundation models (SFM) but does not account for any dependencies among
intermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based
single-step non-autoregressive transformer (CASS-NAT), can mitigate the
dependency problem but is not able to efficiently integrate SFM. Inspired by
the success of recent work of speech-text joint pre-training with a shared
transformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to
combine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an
encoder as the major module, which can be the SFM. The encoder plays the role
of both the CASS-NAT encoder and decoder by two forward passes. The first pass
of the encoder accepts the speech signal as input, while the concatenation of
the speech signal and the token-level acoustic embedding is used as the input
for the second pass. Examined on the Librispeech 100h, MyST, and Aishell1
datasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results
and is better or comparable to CASS-NAT with only an encoder and hence, fewer
model parameters. Our codes are publicly available.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08904" title="Abstract">arXiv:2402.08904</a> (cross-list from eess.AS) [<a href="/pdf/2402.08904" title="Download PDF">pdf</a>, <a href="/format/2402.08904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sound Field Reconstruction Using a Compact Acoustics-informed Neural  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ma%2C+F">Fei Ma</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+S">Sipei Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Burnett%2C+I+S">Ian S. Burnett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Sound field reconstruction (SFR) augments the information of a sound field
captured by a microphone array. Conventional SFR methods using basis function
decomposition are straightforward and computationally efficient, but may
require more microphones than needed to measure the sound field. Recent studies
show that pure data-driven and learning-based methods are promising in some SFR
tasks, but they are usually computationally heavy and may fail to reconstruct a
physically valid sound field. This paper proposes a compact acoustics-informed
neural network (AINN) method for SFR, whereby the Helmholtz equation is
exploited to regularize the neural network. As opposed to pure data-driven
approaches that solely rely on measured sound pressures, the integration of the
Helmholtz equation improves robustness of the neural network against variations
during the measurement processes and prompts the generation of physically valid
reconstructions. The AINN is designed to be compact, and is able to predict not
only the sound pressures but also sound pressure gradients within a spatial
region of interest based on measured sound pressures along the boundary.
Numerical experiments with acoustic transfer functions measured in different
environments demonstrate the superiority of the AINN method over the
traditional cylinder harmonic decomposition and the singular value
decomposition methods.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08905" title="Abstract">arXiv:2402.08905</a> (cross-list from econ.GN) [<a href="/pdf/2402.08905" title="Download PDF">pdf</a>, <a href="/ps/2402.08905" title="Download PostScript">ps</a>, <a href="/format/2402.08905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time preference, wealth and utility inequality: A microeconomic  interaction and dynamic macroeconomic model connection approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Kato%2C+T">Takeshi Kato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Multiagent Systems (cs.MA); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Based on interactions between individuals and others and references to social
norms, this study reveals the impact of heterogeneity in time preference on
wealth distribution and inequality. We present a novel approach that connects
the interactions between microeconomic agents that generate heterogeneity to
the dynamic equations for capital and consumption in macroeconomic models.
Using this approach, we estimate the impact of changes in the discount rate due
to microeconomic interactions on capital, consumption and utility and the
degree of inequality. The results show that intercomparisons with others
regarding consumption significantly affect capital, i.e. wealth inequality.
Furthermore, the impact on utility is never small and social norms can reduce
this impact. Our supporting evidence shows that the quantitative results of
inequality calculations correspond to survey data from cohort and
cross-cultural studies. This study's micro-macro connection approach can be
deployed to connect microeconomic interactions, such as exchange, interest and
debt, redistribution, mutual aid and time preference, to dynamic macroeconomic
models.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08916" title="Abstract">arXiv:2402.08916</a> (cross-list from eess.SP) [<a href="/pdf/2402.08916" title="Download PDF">pdf</a>, <a href="/format/2402.08916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight Deep Learning Based Channel Estimation for Extremely  Large-Scale Massive MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gao%2C+S">Shen Gao</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+P">Peihao Dong</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+Z">Zhiwen Pan</a>, 
<a href="/search/eess?searchtype=author&query=You%2C+X">Xiaohu You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Vehicular Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Extremely large-scale massive multiple-input multiple-output (XL-MIMO)
systems introduce the much higher channel dimensionality and incur the
additional near-field propagation effect, aggravating the computation load and
the difficulty to acquire the prior knowledge for channel estimation. In this
article, an XL-MIMO channel network (XLCNet) is developed to estimate the
high-dimensional channel, which is a universal solution for both the near-field
users and far-field users with different channel statistics. Furthermore, a
compressed XLCNet (C-XLCNet) is designed via weight pruning and quantization to
accelerate the model inference as well as to facilitate the model storage and
transmission. Simulation results show the performance superiority and
universality of XLCNet. Compared to XLCNet, C-XLCNet incurs the limited
performance loss while reducing the computational complexity and model size by
about $10 \times$ and $36 \times$, respectively.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08932" title="Abstract">arXiv:2402.08932</a> (cross-list from eess.AS) [<a href="/pdf/2402.08932" title="Download PDF">pdf</a>, <a href="/format/2402.08932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Listening to Multi-talker Conversations: Modular and End-to-end  Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Raj%2C+D">Desh Raj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ph.D. dissertation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Since the first speech recognition systems were built more than 30 years ago,
improvement in voice technology has enabled applications such as smart
assistants and automated customer support. However, conversation intelligence
of the future requires recognizing free-flowing multi-party conversations,
which is a crucial and challenging component that still remains unsolved. In
this dissertation, we focus on this problem of speaker-attributed multi-talker
speech recognition, and propose two perspectives which result from its
probabilistic formulation.
<br />In the modular perspective, we build a pipeline of sub-tasks involving
speaker diarization, target speaker extraction, and speech recognition. Our
first contribution is a method to perform overlap-aware diarization by
reformulating spectral clustering as a constrained optimization problem. We
also describe an algorithm to ensemble diarization outputs, either to combine
overlap-aware systems or to perform multi-channel diarization by late fusion.
Once speaker segments are identified, we robustly extract single-speaker
utterances from the mixture using a GPU-accelerated implementation of guided
source separation, which allows us to use an off-the-shelf ASR system to obtain
speaker-attributed transcripts.
<br />Since the modular approach suffers from error propagation, we propose an
alternate "end-to-end" perspective on the problem. For this, we describe the
Streaming Unmixing and Recognition Transducer (SURT). We show how to train SURT
models efficiently by carefully designing the network architecture, objective
functions, and mixture simulation techniques. Finally, we add an auxiliary
speaker branch to enable joint prediction of speaker labels synchronized with
the speech tokens. We demonstrate that training on synthetic mixtures and
adapting with real data helps these models transfer well for streaming
transcription of real meeting sessions.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08934" title="Abstract">arXiv:2402.08934</a> (cross-list from eess.IV) [<a href="/pdf/2402.08934" title="Download PDF">pdf</a>, <a href="/format/2402.08934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme Video Compression with Pre-trained Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+B">Bohan Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yiming Liu</a>, 
<a href="/search/eess?searchtype=author&query=Niu%2C+X">Xueyan Niu</a>, 
<a href="/search/eess?searchtype=author&query=Bai%2C+B">Bo Bai</a>, 
<a href="/search/eess?searchtype=author&query=Deng%2C+L">Lei Deng</a>, 
<a href="/search/eess?searchtype=author&query=G%C3%BCnd%C3%BCz%2C+D">Deniz G&#xfc;nd&#xfc;z</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diffusion models have achieved remarkable success in generating high quality
image and video data. More recently, they have also been used for image
compression with high perceptual quality. In this paper, we present a novel
approach to extreme video compression leveraging the predictive power of
diffusion-based generative models at the decoder. The conditional diffusion
model takes several neural compressed frames and generates subsequent frames.
When the reconstruction quality drops below the desired level, new frames are
encoded to restart prediction. The entire video is sequentially encoded to
achieve a visually pleasing reconstruction, considering perceptual quality
metrics such as the learned perceptual image patch similarity (LPIPS) and the
Frechet video distance (FVD), at bit rates as low as 0.02 bits per pixel (bpp).
Experimental results demonstrate the effectiveness of the proposed scheme
compared to standard codecs such as H.264 and H.265 in the low bpp regime. The
results showcase the potential of exploiting the temporal relations in video
data using generative models. Code is available at:
https://github.com/ElesionKyrie/Extreme-Video-Compression-With-Prediction-Using-Pre-trainded-Diffusion-Models-
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08952" title="Abstract">arXiv:2402.08952</a> (cross-list from quant-ph) [<a href="/pdf/2402.08952" title="Download PDF">pdf</a>, <a href="/format/2402.08952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A two-stage solution to quantum process tomography: error analysis and  optimal design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Xiao%2C+S">Shuixin Xiao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+Y">Yuanlong Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Dong%2C+D">Daoyi Dong</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mooney%2C+G+J">Gary J. Mooney</a>, 
<a href="/search/quant-ph?searchtype=author&query=Petersen%2C+I+R">Ian R. Petersen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yonezawa%2C+H">Hidehiro Yonezawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Quantum process tomography is a critical task for characterizing the dynamics
of quantum systems and achieving precise quantum control. In this paper, we
propose a two-stage solution for both trace-preserving and non-trace-preserving
quantum process tomography. Utilizing a tensor structure, our algorithm
exhibits a computational complexity of $O(MLd^2)$ where $d$ is the dimension of
the quantum system and $ M $, $ L $ represent the numbers of different input
states and measurement operators, respectively. We establish an analytical
error upper bound and then design the optimal input states and the optimal
measurement operators, which are both based on minimizing the error upper bound
and maximizing the robustness characterized by the condition number. Numerical
examples and testing on IBM quantum devices are presented to demonstrate the
performance and efficiency of our algorithm.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08953" title="Abstract">arXiv:2402.08953</a> (cross-list from math.PR) [<a href="/pdf/2402.08953" title="Download PDF">pdf</a>, <a href="/ps/2402.08953" title="Download PostScript">ps</a>, <a href="/format/2402.08953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy Jump and Entropic Central Limit Theorem for Independent Sum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yao%2C+L">Liuquan Yao</a>, 
<a href="/search/math?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">It is a manuscript for results about entropic central limit theorem for
independent sum under finite Poincar\'e constant conditions.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08977" title="Abstract">arXiv:2402.08977</a> (cross-list from math.FA) [<a href="/pdf/2402.08977" title="Download PDF">pdf</a>, <a href="/format/2402.08977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Derivative sampling expansions in shift-invariant spaces with error  estimates covering discontinuous signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Priyanka%2C+K">Kumari Priyanka</a>, 
<a href="/search/math?searchtype=author&query=Selvan%2C+A+A">A. Antony Selvan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">This paper is concerned with the problem of sampling and interpolation
involving derivatives in shift-invariant spaces and the error analysis of the
derivative sampling expansions for fundamentally large classes of functions. A
new type of polynomials based on derivative samples is introduced, which is
different from the Euler-Frobenius polynomials for the multiplicity $r&gt;1$. A
complete characterization of uniform sampling with derivatives is given using
Laurent operators. The rate of approximation of a signal (not necessarily
continuous) by the derivative sampling expansions in shift-invariant spaces
generated by compactly supported functions is established in terms of $L^p$-
average modulus of smoothness. Finally, several typical examples illustrating
the various problems are discussed in detail.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08987" title="Abstract">arXiv:2402.08987</a> (cross-list from eess.IV) [<a href="/pdf/2402.08987" title="Download PDF">pdf</a>, <a href="/format/2402.08987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modality transrectal ultrasound vudei classification for  identification of clinically significant prostate cancer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+H">Hong Wu</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+J">Juan Fu</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+H">Hongsheng Ye</a>, 
<a href="/search/eess?searchtype=author&query=Zhong%2C+Y">Yuming Zhong</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+X">Xuebin Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jianhua Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Prostate cancer is the most common noncutaneous cancer in the world.
Recently, multi-modality transrectal ultrasound (TRUS) has increasingly become
an effective tool for the guidance of prostate biopsies. With the aim of
effectively identifying prostate cancer, we propose a framework for the
classification of clinically significant prostate cancer (csPCa) from
multi-modality TRUS videos. The framework utilizes two 3D ResNet-50 models to
extract features from B-mode images and shear wave elastography images,
respectively. An adaptive spatial fusion module is introduced to aggregate two
modalities' features. An orthogonal regularized loss is further used to
mitigate feature redundancy. The proposed framework is evaluated on an in-house
dataset containing 512 TRUS videos, and achieves favorable performance in
identifying csPCa with an area under curve (AUC) of 0.84. Furthermore, the
visualized class activation mapping (CAM) images generated from the proposed
framework may provide valuable guidance for the localization of csPCa, thus
facilitating the TRUS-guided targeted biopsy. Our code is publicly available at
https://github.<a href="/abs/com/2313595">com/2313595</a>986/ProstateTRUS.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08991" title="Abstract">arXiv:2402.08991</a> (cross-list from stat.ML) [<a href="/pdf/2402.08991" title="Download PDF">pdf</a>, <a href="/ps/2402.08991" title="Download PostScript">ps</a>, <a href="/format/2402.08991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Model-Based Reinforcement Learning Against Adversarial  Corruption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ye%2C+C">Chenlu Ye</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+J">Jiafan He</a>, 
<a href="/search/stat?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This study tackles the challenges of adversarial corruption in model-based
reinforcement learning (RL), where the transition dynamics can be corrupted by
an adversary. Existing studies on corruption-robust RL mostly focus on the
setting of model-free RL, where robust least-square regression is often
employed for value function estimation. However, these techniques cannot be
directly applied to model-based RL. In this paper, we focus on model-based RL
and take the maximum likelihood estimation (MLE) approach to learn transition
model. Our work encompasses both online and offline settings. In the online
setting, we introduce an algorithm called corruption-robust optimistic MLE
(CR-OMLE), which leverages total-variation (TV)-based information ratios as
uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of
$\tilde{\mathcal{O}}(\sqrt{T} + C)$, where $C$ denotes the cumulative
corruption level after $T$ episodes. We also prove a lower bound to show that
the additive dependence on $C$ is optimal. We extend our weighting technique to
the offline setting, and propose an algorithm named corruption-robust
pessimistic MLE (CR-PMLE). Under a uniform coverage condition, CR-PMLE exhibits
suboptimality worsened by $\mathcal{O}(C/n)$, nearly matching the lower bound.
To the best of our knowledge, this is the first work on corruption-robust
model-based RL algorithms with provable guarantees.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08992" title="Abstract">arXiv:2402.08992</a> (cross-list from math.OC) [<a href="/pdf/2402.08992" title="Download PDF">pdf</a>, <a href="/ps/2402.08992" title="Download PostScript">ps</a>, <a href="/format/2402.08992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance Reduction and Low Sample Complexity in Stochastic Optimization  via Proximal Point Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+J">Jiaming Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper proposes a stochastic proximal point method to solve a stochastic
convex composite optimization problem. High probability results in stochastic
optimization typically hinge on restrictive assumptions on the stochastic
gradient noise, for example, sub-Gaussian distributions. Assuming only weak
conditions such as bounded variance of the stochastic gradient, this paper
establishes a low sample complexity to obtain a high probability guarantee on
the convergence of the proposed method. Additionally, a notable aspect of this
work is the development of a subroutine to solve the proximal subproblem, which
also serves as a novel technique for variance reduction.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09018" title="Abstract">arXiv:2402.09018</a> (cross-list from stat.ML) [<a href="/pdf/2402.09018" title="Download PDF">pdf</a>, <a href="/format/2402.09018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Operators Meet Energy-based Theory: Operator Learning for  Hamiltonian and Dissipative PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tanaka%2C+Y">Yusuke Tanaka</a>, 
<a href="/search/stat?searchtype=author&query=Yaguchi%2C+T">Takaharu Yaguchi</a>, 
<a href="/search/stat?searchtype=author&query=Iwata%2C+T">Tomoharu Iwata</a>, 
<a href="/search/stat?searchtype=author&query=Ueda%2C+N">Naonori Ueda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The operator learning has received significant attention in recent years,
with the aim of learning a mapping between function spaces. Prior works have
proposed deep neural networks (DNNs) for learning such a mapping, enabling the
learning of solution operators of partial differential equations (PDEs).
However, these works still struggle to learn dynamics that obeys the laws of
physics. This paper proposes Energy-consistent Neural Operators (ENOs), a
general framework for learning solution operators of PDEs that follows the
energy conservation or dissipation law from observed solution trajectories. We
introduce a novel penalty function inspired by the energy-based theory of
physics for training, in which the energy functional is modeled by another DNN,
allowing one to bias the outputs of the DNN-based solution operators to ensure
energetic consistency without explicit PDEs. Experiments on multiple physical
systems show that ENO outperforms existing DNN models in predicting solutions
from data, especially in super-resolution settings.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09057" title="Abstract">arXiv:2402.09057</a> (cross-list from eess.SP) [<a href="/pdf/2402.09057" title="Download PDF">pdf</a>, <a href="/format/2402.09057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Sensing Along Fibres for Smart Clothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hannigan%2C+B+C">Brett C. Hannigan</a>, 
<a href="/search/eess?searchtype=author&query=Cuthbert%2C+T+J">Tyler J. Cuthbert</a>, 
<a href="/search/eess?searchtype=author&query=Ahmadizadeh%2C+C">Chakaveh Ahmadizadeh</a>, 
<a href="/search/eess?searchtype=author&query=Menon%2C+C">Carlo Menon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 7 figures, accepted version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Textile sensors transform our everyday clothing into a means to track
movement and bio-signals in a completely unobtrusive way. One major hindrance
to the adoption of "smart" clothing is the difficulty encountered with
connections and space when scaling up the number of sensors. There is a lack of
research addressing a key limitation in wearable electronics: connections
between rigid and textile elements are often unreliable and they require
interfacing sensors in a way incompatible with textile mass production methods.
We introduce a prototype garment, compact readout circuit, and algorithm to
measure localized strain along multiple regions of a fibre. We employ a helical
auxetic yarn sensor with tunable sensitivity along its length to selectively
respond to strain signals. We demonstrate distributed sensing in clothing,
monitoring arm joint angles from a single continuous fibre. Compared to optical
motion capture, we achieve around 5{\deg} error in reconstructing shoulder,
elbow, and wrist joint angles.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09076" title="Abstract">arXiv:2402.09076</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.09076" title="Download PDF">pdf</a>, <a href="/format/2402.09076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preserving system activity while controlling epidemic spreading in  adaptive temporal networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Mancastroppa%2C+M">Marco Mancastroppa</a>, 
<a href="/search/physics?searchtype=author&query=Vezzani%2C+A">Alessandro Vezzani</a>, 
<a href="/search/physics?searchtype=author&query=Colizza%2C+V">Vittoria Colizza</a>, 
<a href="/search/physics?searchtype=author&query=Burioni%2C+R">Raffaella Burioni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI); Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">Human behaviour strongly influences the spread of infectious diseases:
understanding the interplay between epidemic dynamics and adaptive behaviours
is essential to improve response strategies to epidemics, with the goal of
containing the epidemic while preserving a sufficient level of operativeness in
the population. Through activity-driven temporal networks, we formulate a
general framework which models a wide range of adaptive behaviours and
mitigation strategies, observed in real populations. We analytically derive the
conditions for a widespread diffusion of epidemics in the presence of arbitrary
adaptive behaviours, highlighting the crucial role of correlations between
agents behaviour in the infected and in the susceptible state. We focus on the
effects of sick-leave, comparing the effectiveness of different strategies in
reducing the impact of the epidemic and preserving the system operativeness. We
show the critical relevance of heterogeneity in individual behavior: in
homogeneous networks, all sick-leave strategies are equivalent and poorly
effective, while in heterogeneous networks, strategies targeting the most
vulnerable nodes are able to effectively mitigate the epidemic, also avoiding a
deterioration in system activity and maintaining a low level of absenteeism.
Interestingly, with targeted strategies both the minimum of population activity
and the maximum of absenteeism anticipate the infection peak, which is
effectively flattened and delayed, so that full operativeness is almost
restored when the infection peak arrives. We also provide realistic estimates
of the model parameters for influenza-like illness, thereby suggesting
strategies for managing epidemics and absenteeism in realistic populations.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09081" title="Abstract">arXiv:2402.09081</a> (cross-list from math.OC) [<a href="/pdf/2402.09081" title="Download PDF">pdf</a>, <a href="/ps/2402.09081" title="Download PostScript">ps</a>, <a href="/format/2402.09081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Rank Extragradient Methods for Scalable Semidefinite Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kaplan%2C+D+G+A">Dan Garber. Atara Kaplan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider several classes of highly important semidefinite optimization
problems that involve both a convex objective function (smooth or nonsmooth)
and additional linear or nonlinear smooth and convex constraints, which are
ubiquitous in statistics, machine learning, combinatorial optimization, and
other domains. We focus on high-dimensional and plausible settings in which the
problem admits a low-rank solution which also satisfies a low-rank
complementarity condition. We provide several theoretical results proving that,
under these circumstances, the well-known Extragradient method, when
initialized in the proximity of an optimal primal-dual solution, converges to a
solution of the constrained optimization problem with its standard convergence
rates guarantees, using only low-rank singular value decompositions (SVD) to
project onto the positive semidefinite cone, as opposed to
computationally-prohibitive full-rank SVDs required in worst-case. Our approach
is supported by numerical experiments conducted with a dataset of Max-Cut
instances.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09101" title="Abstract">arXiv:2402.09101</a> (cross-list from eess.IV) [<a href="/pdf/2402.09101" title="Download PDF">pdf</a>, <a href="/format/2402.09101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DestripeCycleGAN: Stripe Simulation CycleGAN for Unsupervised Infrared  Image Destriping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+S">Shiqi Yang</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+H">Hanlin Qin</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+X">Xiang Yan</a>, 
<a href="/search/eess?searchtype=author&query=Rahmani%2C+H">Hossein Rahmani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">CycleGAN has been proven to be an advanced approach for unsupervised image
restoration. This framework consists of two generators: a denoising one for
inference and an auxiliary one for modeling noise to fulfill cycle-consistency
constraints. However, when applied to the infrared destriping task, it becomes
challenging for the vanilla auxiliary generator to consistently produce
vertical noise under unsupervised constraints. This poses a threat to the
effectiveness of the cycle-consistency loss, leading to stripe noise residual
in the denoised image. To address the above issue, we present a novel framework
for single-frame infrared image destriping, named DestripeCycleGAN. In this
model, the conventional auxiliary generator is replaced with a priori stripe
generation model (SGM) to introduce vertical stripe noise in the clean data,
and the gradient map is employed to re-establish cycle-consistency. Meanwhile,
a Haar wavelet background guidance module (HBGM) has been designed to minimize
the divergence of background details between the different domains. To preserve
vertical edges, a multi-level wavelet U-Net (MWUNet) is proposed as the
denoising generator, which utilizes the Haar wavelet transform as the sampler
to decline directional information loss. Moreover, it incorporates the group
fusion block (GFB) into skip connections to fuse the multi-scale features and
build the context of long-distance dependencies. Extensive experiments on real
and synthetic data demonstrate that our DestripeCycleGAN surpasses the
state-of-the-art methods in terms of visual quality and quantitative
evaluation. Our code will be made public at
https://github.com/0wuji/DestripeCycleGAN.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09108" title="Abstract">arXiv:2402.09108</a> (cross-list from quant-ph) [<a href="/pdf/2402.09108" title="Download PDF">pdf</a>, <a href="/format/2402.09108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Web 3.0 and Quantum Security: Long-Distance Free-Space QSDC for Global  Web 3.0 Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wong%2C+Y+K">Yew Kee Wong</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhou%2C+Y">Yifan Zhou</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhou%2C+X">Xinlin Zhou</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liang%2C+Y+S">Yan Shing Liang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+Z+Y">Zi Yan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 3 figures, 7 flowcharts
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">With the advent of Web 3.0, the swift advancement of technology confronts an
imminent threat from quantum computing. Security protocols safeguarding the
integrity of Web 2.0 and Web 3.0 are growing more susceptible to both quantum
attacks and sophisticated classical threats. The article introduces
long-distance free-space quantum secure direct communication (LF QSDC) as a
method to safeguard against security breaches in both quantum and classical
contexts. Differing from techniques like quantum key distribution (QKD), LF
QSDC surpasses constraints by facilitating encrypted data transmission sans key
exchanges, thus diminishing the inherent weaknesses of key-based systems. The
distinctiveness of this attribute, coupled with its quantum mechanics base,
protects against quantum computer assaults and advanced non-quantum dangers,
harmonizing seamlessly with the untrustworthy tenets of the Web 3.0 age. The
focus of our study is the incorporation of LF QSDC into network
infrastructures, highlighting its efficacy for extended-range communication via
memory DL04 protocol, quantum-aware low-density parity check (LDPC), and
pointing, acquisition, and tracking (PAT) technologies. Utilizing this method
not only bolsters the security of worldwide Web 3.0 networks but also
guarantees their endurance in a time when quantum and sophisticated classical
threats exist simultaneously. Consequently, LF QSDC stands out as a robust
security solution, well-suited for Web 3.0 systems amidst the constantly
evolving digital environment.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09116" title="Abstract">arXiv:2402.09116</a> (cross-list from quant-ph) [<a href="/pdf/2402.09116" title="Download PDF">pdf</a>, <a href="/format/2402.09116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-entropy encoders and simultaneous decoders in identification via  quantum channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Colomer%2C+P">Pau Colomer</a>, 
<a href="/search/quant-ph?searchtype=author&query=Deppe%2C+C">Christian Deppe</a>, 
<a href="/search/quant-ph?searchtype=author&query=Boche%2C+H">Holger Boche</a>, 
<a href="/search/quant-ph?searchtype=author&query=Winter%2C+A">Andreas Winter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Motivated by deterministic identification via (classical) channels, where the
encoder is not allowed to use randomization, we revisit the problem of
identification via quantum channels but now with the additional restriction
that the message encoding must use pure quantum states, rather than general
mixed states. Together with the previously considered distinction between
simultaneous and general decoders, this suggests a two-dimensional spectrum of
different identification capacities, whose behaviour could a priori be very
different.
<br />We demonstrate two new results as our main findings: first, we show that all
four combinations (pure/mixed encoder, simultaneous/general decoder) have a
double-exponentially growing code size, and that indeed the corresponding
identification capacities are lower bounded by the classical transmission
capacity for a general quantum channel, which is given by the
Holevo-Schumacher-Westmoreland Theorem. Secondly, we show that the simultaneous
identification capacity of a quantum channel equals the simultaneous
identification capacity with pure state encodings, thus leaving three linearly
ordered identification capacities. By considering some simple examples, we
finally show that these three are all different: general identification
capacity can be larger than pure-state-encoded identification capacity, which
in turn can be larger than pure-state-encoded simultaneous identification
capacity.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09120" title="Abstract">arXiv:2402.09120</a> (cross-list from eess.SP) [<a href="/pdf/2402.09120" title="Download PDF">pdf</a>, <a href="/format/2402.09120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Communication and Sensing for 6G -- A Cross-Layer Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wymeersch%2C+H">Henk Wymeersch</a>, 
<a href="/search/eess?searchtype=author&query=Saleh%2C+S">Sharief Saleh</a>, 
<a href="/search/eess?searchtype=author&query=Nimr%2C+A">Ahmad Nimr</a>, 
<a href="/search/eess?searchtype=author&query=Halili%2C+R">Rreze Halili</a>, 
<a href="/search/eess?searchtype=author&query=Berkvens%2C+R">Rafael Berkvens</a>, 
<a href="/search/eess?searchtype=author&query=Moghaddam%2C+M+H">Mohammad H. Moghaddam</a>, 
<a href="/search/eess?searchtype=author&query=Mateos-Ramos%2C+J+M">Jos&#xe9; Miguel Mateos-Ramos</a>, 
<a href="/search/eess?searchtype=author&query=Stavridis%2C+A">Athanasios Stavridis</a>, 
<a href="/search/eess?searchtype=author&query=W%C3%A4nstedt%2C+S">Stefan W&#xe4;nstedt</a>, 
<a href="/search/eess?searchtype=author&query=Barmpounakis%2C+S">Sokratis Barmpounakis</a>, 
<a href="/search/eess?searchtype=author&query=Priyanto%2C+B">Basuki Priyanto</a>, 
<a href="/search/eess?searchtype=author&query=Beale%2C+M">Martin Beale</a>, 
<a href="/search/eess?searchtype=author&query=van+de+Beek%2C+J">Jaap van de Beek</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+Z">Zi Ye</a>, 
<a href="/search/eess?searchtype=author&query=Manalastas%2C+M">Marvin Manalastas</a>, 
<a href="/search/eess?searchtype=author&query=Kousaridas%2C+A">Apostolos Kousaridas</a>, 
<a href="/search/eess?searchtype=author&query=Fettweis%2C+G+P">Gerhard P. Fettweis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">As 6G emerges, cellular systems are envisioned to integrate sensing with
communication capabilities, leading to multi-faceted communication and sensing
(JCAS). This paper presents a comprehensive cross-layer overview of the
Hexa-X-II project's endeavors in JCAS, aligning 6G use cases with service
requirements and pinpointing distinct scenarios that bridge communication and
sensing. This work relates to these scenarios through the lens of the
cross-layer physical and networking domains, covering models, deployments,
resource allocation, storage challenges, computational constraints, interfaces,
and innovative functions.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09122" title="Abstract">arXiv:2402.09122</a> (cross-list from stat.ML) [<a href="/pdf/2402.09122" title="Download PDF">pdf</a>, <a href="/format/2402.09122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed-Output Gaussian Process Latent Variable Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Odgers%2C+J">James Odgers</a>, 
<a href="/search/stat?searchtype=author&query=Kappatou%2C+C">Chrysoula Kappatou</a>, 
<a href="/search/stat?searchtype=author&query=Misener%2C+R">Ruth Misener</a>, 
<a href="/search/stat?searchtype=author&query=Filippi%2C+S">Sarah Filippi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work develops a Bayesian non-parametric approach to signal separation
where the signals may vary according to latent variables. Our key contribution
is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate
the case where each data point comprises the weighted sum of a known number of
pure component signals, observed across several input locations. Our framework
allows the use of a range of priors for the weights of each observation. This
flexibility enables us to represent use cases including sum-to-one constraints
for estimating fractional makeup, and binary weights for classification. Our
contributions are particularly relevant to spectroscopy, where changing
conditions may cause the underlying pure component signals to vary from sample
to sample. To demonstrate the applicability to both spectroscopy and other
domains, we consider several applications: a near-infrared spectroscopy data
set with varying temperatures, a simulated data set for identifying flow
configuration through a pipe, and a data set for determining the type of rock
from its reflectance.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09131" title="Abstract">arXiv:2402.09131</a> (cross-list from math.CO) [<a href="/pdf/2402.09131" title="Download PDF">pdf</a>, <a href="/format/2402.09131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General penny graphs are at most 43/18-dense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sagdeev%2C+A">Arsenii Sagdeev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Metric Geometry (math.MG)

</div>
<p class="mathjax">We prove that among $n$ points in the plane in general position, the shortest
distance occurs at most $43n/18$ times, improving upon the upper bound of
$17n/7$ obtained by T\'oth in 1997.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09135" title="Abstract">arXiv:2402.09135</a> (cross-list from physics.optics) [<a href="/pdf/2402.09135" title="Download PDF">pdf</a>, <a href="/ps/2402.09135" title="Download PostScript">ps</a>, <a href="/format/2402.09135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unconventional Computing based on Four Wave Mixing in Highly Nonlinear  Waveguides
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sozos%2C+K">Kostas Sozos</a>, 
<a href="/search/physics?searchtype=author&query=Deligiannidis%2C+S">Stavros Deligiannidis</a>, 
<a href="/search/physics?searchtype=author&query=Mesaritakis%2C+C">Charis Mesaritakis</a>, 
<a href="/search/physics?searchtype=author&query=Bogris%2C+A">Adonis Bogris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work we numerically analyze a photonic unconventional accelerator
based on the four-wave mixing effect in highly nonlinear waveguides. The
proposed scheme can act as a fully analogue system for nonlinear signal
processing directly in the optical domain. By exploiting the rich Kerr-induced
nonlinearities, multiple nonlinear transformations of an input signal can be
generated and used for solving complex nonlinear tasks. We first evaluate the
performance of our scheme in the Santa-Fe chaotic time-series prediction. The
true power of this processor is revealed in the all-optical nonlinearity
compensation in an optical communication scenario where we provide results
superior to those offered by strong machine learning algorithms with reduced
power consumption and computational complexity. Finally, we showcase how the
FWM module can be used as a reconfigurable nonlinear activation module being
capable of reproducing characteristic functions such as sigmoid or rectified
linear unit.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09137" title="Abstract">arXiv:2402.09137</a> (cross-list from eess.IV) [<a href="/pdf/2402.09137" title="Download PDF">pdf</a>, <a href="/format/2402.09137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Diffusion Model for Brain Age Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ijishakin%2C+A">Ayodeji Ijishakin</a>, 
<a href="/search/eess?searchtype=author&query=Martin%2C+S">Sophie Martin</a>, 
<a href="/search/eess?searchtype=author&query=Townend%2C+F">Florence Townend</a>, 
<a href="/search/eess?searchtype=author&query=Agosta%2C+F">Federica Agosta</a>, 
<a href="/search/eess?searchtype=author&query=Spinelli%2C+E+G">Edoardo Gioele Spinelli</a>, 
<a href="/search/eess?searchtype=author&query=Basaia%2C+S">Silvia Basaia</a>, 
<a href="/search/eess?searchtype=author&query=Schito%2C+P">Paride Schito</a>, 
<a href="/search/eess?searchtype=author&query=Falzone%2C+Y">Yuri Falzone</a>, 
<a href="/search/eess?searchtype=author&query=Filippi%2C+M">Massimo Filippi</a>, 
<a href="/search/eess?searchtype=author&query=Cole%2C+J">James Cole</a>, 
<a href="/search/eess?searchtype=author&query=Malaspina%2C+A">Andrea Malaspina</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Deep Generative Models for Health Workshop, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Brain age prediction models have succeeded in predicting clinical outcomes in
neurodegenerative diseases, but can struggle with tasks involving faster
progressing diseases and low quality data. To enhance their performance, we
employ a semi-supervised diffusion model, obtaining a 0.83(p&lt;0.01) correlation
between chronological and predicted age on low quality T1w MR images. This was
competitive with state-of-the-art non-generative methods. Furthermore, the
predictions produced by our model were significantly associated with survival
length (r=0.24, p&lt;0.05) in Amyotrophic Lateral Sclerosis. Thus, our approach
demonstrates the value of diffusion-based architectures for the task of brain
age prediction.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09155" title="Abstract">arXiv:2402.09155</a> (cross-list from eess.SP) [<a href="/pdf/2402.09155" title="Download PDF">pdf</a>, <a href="/ps/2402.09155" title="Download PostScript">ps</a>, <a href="/format/2402.09155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint and Robust Beamforming Framework for Integrated Sensing and  Communication Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Choi%2C+J">Jinseok Choi</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+J">Jeonghun Park</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+N">Namyoon Lee</a>, 
<a href="/search/eess?searchtype=author&query=Alkhateeb%2C+A">Ahmed Alkhateeb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted for possible IEEE publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Integrated sensing and communication (ISAC) is widely recognized as a
fundamental enabler for future wireless communications. In this paper, we
present a joint communication and radar beamforming framework for maximizing a
sum spectral efficiency (SE) while guaranteeing desired radar performance with
imperfect channel state information (CSI) in multi-user and multi-target ISAC
systems. To this end, we adopt either a radar transmit beam mean square error
(MSE) or receive signal-to-clutter-plus-noise ratio (SCNR) as a radar
performance constraint of a sum SE maximization problem. To resolve inherent
challenges such as non-convexity and imperfect CSI, we reformulate the problems
and identify first-order optimality conditions for the joint radar and
communication beamformer. Turning the condition to a nonlinear eigenvalue
problem with eigenvector dependency (NEPv), we develop an alternating method
which finds the joint beamformer through power iteration and a Lagrangian
multiplier through binary search. The proposed framework encompasses both the
radar metrics and is robust to channel estimation error with low complexity.
Simulations validate the proposed methods. In particular, we observe that the
MSE and SCNR constraints exhibit complementary performance depending on the
operating environment, which manifests the importance of the proposed
comprehensive and robust optimization framework.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09156" title="Abstract">arXiv:2402.09156</a> (cross-list from eess.IV) [<a href="/pdf/2402.09156" title="Download PDF">pdf</a>, <a href="/format/2402.09156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crop and Couple: cardiac image segmentation using interlinked specialist  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khan%2C+A">Abbas Khan</a>, 
<a href="/search/eess?searchtype=author&query=Asad%2C+M">Muhammad Asad</a>, 
<a href="/search/eess?searchtype=author&query=Benning%2C+M">Martin Benning</a>, 
<a href="/search/eess?searchtype=author&query=Roney%2C+C">Caroline Roney</a>, 
<a href="/search/eess?searchtype=author&query=Slabaugh%2C+G">Gregory Slabaugh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diagnosis of cardiovascular disease using automated methods often relies on
the critical task of cardiac image segmentation. We propose a novel strategy
that performs segmentation using specialist networks that focus on a single
anatomy (left ventricle, right ventricle, or myocardium). Given an input
long-axis cardiac MR image, our method performs a ternary segmentation in the
first stage to identify these anatomical regions, followed by cropping the
original image to focus subsequent processing on the anatomical regions. The
specialist networks are coupled through an attention mechanism that performs
cross-attention to interlink features from different anatomies, serving as a
soft relative shape prior. Central to our approach is an additive attention
block (E-2A block), which is used throughout our architecture thanks to its
efficiency.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09181" title="Abstract">arXiv:2402.09181</a> (cross-list from eess.IV) [<a href="/pdf/2402.09181" title="Download PDF">pdf</a>, <a href="/format/2402.09181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for  Medical LVLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yutao Hu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+T">Tianbin Li</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+Q">Quanfeng Lu</a>, 
<a href="/search/eess?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+J">Junjun He</a>, 
<a href="/search/eess?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Large Vision-Language Models (LVLMs) have demonstrated remarkable
capabilities in various multimodal tasks. However, their potential in the
medical domain remains largely unexplored. A significant challenge arises from
the scarcity of diverse medical images spanning various modalities and
anatomical regions, which is essential in real-world medical applications. To
solve this problem, in this paper, we introduce OmniMedVQA, a novel
comprehensive medical Visual Question Answering (VQA) benchmark. This benchmark
is collected from 75 different medical datasets, including 12 different
modalities and covering more than 20 distinct anatomical regions. Importantly,
all images in this benchmark are sourced from authentic medical scenarios,
ensuring alignment with the requirements of the medical field and suitability
for evaluating LVLMs. Through our extensive experiments, we have found that
existing LVLMs struggle to address these medical VQA problems effectively.
Moreover, what surprises us is that medical-specialized LVLMs even exhibit
inferior performance to those general-domain models, calling for a more
versatile and robust LVLM in the biomedical field. The evaluation results not
only reveal the current limitations of LVLM in understanding real medical
images but also highlight our dataset's significance. Our dataset will be made
publicly available.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09245" title="Abstract">arXiv:2402.09245</a> (cross-list from eess.AS) [<a href="/pdf/2402.09245" title="Download PDF">pdf</a>, <a href="/format/2402.09245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Marinoni%2C+C">Christian Marinoni</a>, 
<a href="/search/eess?searchtype=author&query=Gramaccioni%2C+R+F">Riccardo Fosco Gramaccioni</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+C">Changan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Uncini%2C+A">Aurelio Uncini</a>, 
<a href="/search/eess?searchtype=author&query=Comminiello%2C+D">Danilo Comminiello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP
2023 is to promote and support collaborative research on machine learning for
3D audio signal processing, with a specific emphasis on 3D speech enhancement
and 3D Sound Event Localization and Detection in Extended Reality applications.
As part of our latest competition, we provide a brand-new dataset, which
maintains the same general characteristics of the L3DAS21 and L3DAS22 datasets,
but with first-order Ambisonics recordings from multiple reverberant simulated
environments. Moreover, we start exploring an audio-visual scenario by
providing images of these environments, as perceived by the different
microphone positions and orientations. We also propose updated baseline models
for both tasks that can now support audio-image couples as input and a
supporting API to replicate our results. Finally, we present the results of the
participants. Further details about the challenge are available at
https://www.l3das.com/icassp2023.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09251" title="Abstract">arXiv:2402.09251</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.09251" title="Download PDF">pdf</a>, <a href="/ps/2402.09251" title="Download PostScript">ps</a>, <a href="/format/2402.09251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Machine Learning Kohn-Sham Hamiltonian for Materials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zhong%2C+Y">Yang Zhong</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+J">Jihui Yang</a>, 
<a href="/search/physics?searchtype=author&query=Xiang%2C+H">Hongjun Xiang</a>, 
<a href="/search/physics?searchtype=author&query=Gong%2C+X">Xingao Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While density functional theory (DFT) serves as a prevalent computational
approach in electronic structure calculations, its computational demands and
scalability limitations persist. Recently, leveraging neural networks to
parameterize the Kohn-Sham DFT Hamiltonian has emerged as a promising avenue
for accelerating electronic structure computations. Despite advancements,
challenges such as the necessity for computing extensive DFT training data to
explore new systems and the complexity of establishing accurate ML models for
multi-elemental materials still exist. Addressing these hurdles, this study
introduces a universal electronic Hamiltonian model trained on Hamiltonian
matrices obtained from first-principles DFT calculations of nearly all crystal
structures on the Materials Project. We demonstrate its generality in
predicting electronic structures across the whole periodic table, including
complex multi-elemental systems. By offering a reliable efficient framework for
computing electronic properties, this universal Hamiltonian model lays the
groundwork for advancements in diverse fields related to electronic structures.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09319" title="Abstract">arXiv:2402.09319</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.09319" title="Download PDF">pdf</a>, <a href="/format/2402.09319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eulerian Formulation of the Tensor-Based Morphology Equations for  Strain-Based Blood Damage Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Dirkes%2C+N">Nico Dirkes</a>, 
<a href="/search/physics?searchtype=author&query=Key%2C+F">Fabian Key</a>, 
<a href="/search/physics?searchtype=author&query=Behr%2C+M">Marek Behr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">The development of blood-handling medical devices, such as ventricular assist
devices, requires the analysis of their biocompatibility. Among other aspects,
this includes hemolysis, i.e., red blood cell damage. For this purpose,
computational fluid dynamics (CFD) methods are employed to predict blood flow
in prototypes. The most basic hemolysis models directly estimate red blood cell
damage from fluid stress in the resulting flow field. More advanced models
explicitly resolve cell deformation. On the downside, these models are
typically written in a Lagrangian formulation, i.e., they require pathline
tracking. We present a new Eulerian description of cell deformation, enabling
the evaluation of the solution across the whole domain. The resulting hemolysis
model can be applied to any converged CFD simulation due to one-way coupling
with the fluid velocity field. We discuss the efficient numerical treatment of
the model equations in a stabilized finite element context. We validate the
model by comparison to the original Lagrangian formulation in selected
benchmark flows. Two more complex test cases demonstrate the method's
capabilities in real-world applications. The results highlight the advantages
over previous hemolysis models. In conclusion, the model holds great potential
for the design process of future generations of medical devices.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09328" title="Abstract">arXiv:2402.09328</a> (cross-list from stat.ML) [<a href="/pdf/2402.09328" title="Download PDF">pdf</a>, <a href="/format/2402.09328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connecting Algorithmic Fairness to Quality Dimensions in Machine  Learning in Official Statistics and Survey Production
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Schenk%2C+P+O">Patrick Oliver Schenk</a>, 
<a href="/search/stat?searchtype=author&query=Kern%2C+C">Christoph Kern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">National Statistical Organizations (NSOs) increasingly draw on Machine
Learning (ML) to improve the timeliness and cost-effectiveness of their
products. When introducing ML solutions, NSOs must ensure that high standards
with respect to robustness, reproducibility, and accuracy are upheld as
codified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA;
Yung et al. 2022). At the same time, a growing body of research focuses on
fairness as a pre-condition of a safe deployment of ML to prevent disparate
social impacts in practice. However, fairness has not yet been explicitly
discussed as a quality aspect in the context of the application of ML at NSOs.
We employ Yung et al. (2022)'s QF4SA quality framework and present a mapping of
its quality dimensions to algorithmic fairness. We thereby extend the QF4SA
framework in several ways: we argue for fairness as its own quality dimension,
we investigate the interaction of fairness with other dimensions, and we
explicitly address data, both on its own and its interaction with applied
methodology. In parallel with empirical illustrations, we show how our mapping
can contribute to methodology in the domains of official statistics,
algorithmic fairness, and trustworthy machine learning.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09330" title="Abstract">arXiv:2402.09330</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.09330" title="Download PDF">pdf</a>, <a href="/format/2402.09330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D-based RNA function prediction tools in rnaglib
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Oliver%2C+C">Carlos Oliver</a>, 
<a href="/search/q-bio?searchtype=author&query=Mallet%2C+V">Vincent Mallet</a>, 
<a href="/search/q-bio?searchtype=author&query=Waldisp%C3%BChl%2C+J">J&#xe9;r&#xf4;me Waldisp&#xfc;hl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding the connection between complex structural features of RNA and
biological function is a fundamental challenge in evolutionary studies and in
RNA design. However, building datasets of RNA 3D structures and making
appropriate modeling choices remains time-consuming and lacks standardization.
In this chapter, we describe the use of rnaglib, to train supervised and
unsupervised machine learning-based function prediction models on datasets of
RNA 3D structures.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09335" title="Abstract">arXiv:2402.09335</a> (cross-list from quant-ph) [<a href="/pdf/2402.09335" title="Download PDF">pdf</a>, <a href="/ps/2402.09335" title="Download PostScript">ps</a>, <a href="/format/2402.09335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Unitary T-designs from Random Sums
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+C">Chi-Fang Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Docter%2C+J">Jordan Docter</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xu%2C+M">Michelle Xu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bouland%2C+A">Adam Bouland</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hayden%2C+P">Patrick Hayden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 112 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS); Probability (math.PR)

</div>
<p class="mathjax">Unitary $T$-designs play an important role in quantum information, with
diverse applications in quantum algorithms, benchmarking, tomography, and
communication. Until now, the most efficient construction of unitary
$T$-designs for $n$-qudit systems has been via random local quantum circuits,
which have been shown to converge to approximate $T$-designs in the diamond
norm using $O(T^{5+o(1)} n^2)$ quantum gates. In this work, we provide a new
construction of $T$-designs via random matrix theory using $\tilde{O}(T^2 n^2)$
quantum gates. Our construction leverages two key ideas. First, in the spirit
of central limit theorems, we approximate the Gaussian Unitary Ensemble (GUE)
by an i.i.d. sum of random Hermitian matrices. Second, we show that the product
of just two exponentiated GUE matrices is already approximately Haar random.
Thus, multiplying two exponentiated sums over rather simple random matrices
yields a unitary $T$-design, via Hamiltonian simulation. A central feature of
our proof is a new connection between the polynomial method in quantum query
complexity and the large-dimension ($N$) expansion in random matrix theory. In
particular, we show that the polynomial method provides exponentially improved
bounds on the high moments of certain random matrix ensembles, without
requiring intricate Weingarten calculations. In doing so, we define and solve a
new type of moment problem on the unit circle, asking whether a finite number
of equally weighted points, corresponding to eigenvalues of unitary matrices,
can reproduce a given set of moments.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09338" title="Abstract">arXiv:2402.09338</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.09338" title="Download PDF">pdf</a>, <a href="/format/2402.09338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Networks asymptotic behaviours suitable for the resolution of  inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Del+Debbio%2C+L">Luigi Del Debbio</a>, 
<a href="/search/physics?searchtype=author&query=Naviglio%2C+M">Manuel Naviglio</a>, 
<a href="/search/physics?searchtype=author&query=Tarantelli%2C+F">Francesco Tarantelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Artificial Intelligence (cs.AI); High Energy Physics - Lattice (hep-lat); High Energy Physics - Theory (hep-th)

</div>
<p class="mathjax">In this paper, we perform a study on the effectiveness of Neural Network (NN)
techniques for deconvolution inverse problems. We consider NN's asymptotic
limits, corresponding to Gaussian Processes (GPs), where parameter
non-linearities are lost. Using these resulting GPs, we address the
deconvolution inverse problem in the case of a quantum harmonic oscillator
simulated through Monte Carlo techniques on a lattice. A scenario with a known
analytical solution. Our findings indicate that solving the deconvolution
inverse problem with a fully connected NN yields less performing results than
those obtained using the GPs derived from NN's asymptotic limits. Furthermore,
we observe the trained NN's accuracy approaching that of GPs with increasing
layer width. Notably, one of these GPs defies interpretation as a probabilistic
model, offering a novel perspective compared to established methods in the
literature. Additionally, the NNs, in their asymptotic limit, provide
cost-effective analytical solutions.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09341" title="Abstract">arXiv:2402.09341</a> (cross-list from eess.IV) [<a href="/pdf/2402.09341" title="Download PDF">pdf</a>, <a href="/format/2402.09341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Registration of Longitudinal Spine CTs for Monitoring Lesion Growth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sanhinova%2C+M">Malika Sanhinova</a>, 
<a href="/search/eess?searchtype=author&query=Haouchine%2C+N">Nazim Haouchine</a>, 
<a href="/search/eess?searchtype=author&query=Pieper%2C+S+D">Steve D. Pieper</a>, 
<a href="/search/eess?searchtype=author&query=Wells%2C+W+M">William M. Wells III</a>, 
<a href="/search/eess?searchtype=author&query=Balboni%2C+T+A">Tracy A. Balboni</a>, 
<a href="/search/eess?searchtype=author&query=Spektor%2C+A">Alexander Spektor</a>, 
<a href="/search/eess?searchtype=author&query=Huynh%2C+M+A">Mai Anh Huynh</a>, 
<a href="/search/eess?searchtype=author&query=Guenette%2C+J+P">Jeffrey P. Guenette</a>, 
<a href="/search/eess?searchtype=author&query=Czajkowski%2C+B">Bryan Czajkowski</a>, 
<a href="/search/eess?searchtype=author&query=Caplan%2C+S">Sarah Caplan</a>, 
<a href="/search/eess?searchtype=author&query=Doyle%2C+P">Patrick Doyle</a>, 
<a href="/search/eess?searchtype=author&query=Kang%2C+H">Heejoo Kang</a>, 
<a href="/search/eess?searchtype=author&query=Hackney%2C+D+B">David B. Hackney</a>, 
<a href="/search/eess?searchtype=author&query=Alkalay%2C+R+N">Ron N. Alkalay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted for publication at SPIE Medical Imaging 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Accurate and reliable registration of longitudinal spine images is essential
for assessment of disease progression and surgical outcome. Implementing a
fully automatic and robust registration is crucial for clinical use, however,
it is challenging due to substantial change in shape and appearance due to
lesions. In this paper we present a novel method to automatically align
longitudinal spine CTs and accurately assess lesion progression. Our method
follows a two-step pipeline where vertebrae are first automatically localized,
labeled and 3D surfaces are generated using a deep learning model, then
longitudinally aligned using a Gaussian mixture model surface registration. We
tested our approach on 37 vertebrae, from 5 patients, with baseline CTs and 3,
6, and 12 months follow-ups leading to 111 registrations. Our experiment showed
accurate registration with an average Hausdorff distance of 0.65 mm and average
Dice score of 0.92.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09359" title="Abstract">arXiv:2402.09359</a> (cross-list from eess.IV) [<a href="/pdf/2402.09359" title="Download PDF">pdf</a>, <a href="/format/2402.09359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D  Ultrasound Localization Microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rauby%2C+B">Brice Rauby</a> (1 and 2), 
<a href="/search/eess?searchtype=author&query=Xing%2C+P">Paul Xing</a> (1), 
<a href="/search/eess?searchtype=author&query=Por%C3%A9e%2C+J">Jonathan Por&#xe9;e</a> (1), 
<a href="/search/eess?searchtype=author&query=Gasse%2C+M">Maxime Gasse</a> (1, 2 and 3), 
<a href="/search/eess?searchtype=author&query=Provost%2C+J">Jean Provost</a> (1 and 4) ((1) Polytechnique Montr&#xe9;al, (2) Mila - Quebec Artificial Intelligence Institute, (3) ServiceNow Inc., (4) Montreal Heart Institute)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Ultrasound Localization Microscopy (ULM) is a non-invasive technique that
allows for the imaging of micro-vessels in vivo, at depth and with a resolution
on the order of ten microns. ULM is based on the sub-resolution localization of
individual microbubbles injected in the bloodstream. Mapping the whole
angioarchitecture requires the accumulation of microbubbles trajectories from
thousands of frames, typically acquired over a few minutes. ULM acquisition
times can be reduced by increasing the microbubble concentration, but requires
more advanced algorithms to detect them individually. Several deep learning
approaches have been proposed for this task, but they remain limited to 2D
imaging, in part due to the associated large memory requirements. Herein, we
propose to use sparse tensor neural networks to reduce memory usage in 2D and
to improve the scaling of the memory requirement for the extension of deep
learning architecture to 3D. We study several approaches to efficiently convert
ultrasound data into a sparse format and study the impact of the associated
loss of information. When applied in 2D, the sparse formulation reduces the
memory requirements by a factor 2 at the cost of a small reduction of
performance when compared against dense networks. In 3D, the proposed approach
reduces memory requirements by two order of magnitude while largely
outperforming conventional ULM in high concentration settings. We show that
Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense
deep learning based method in 2D ULM i.e. the use of higher concentration in
silico and reduced acquisition time.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09372" title="Abstract">arXiv:2402.09372</a> (cross-list from eess.IV) [<a href="/pdf/2402.09372" title="Download PDF">pdf</a>, <a href="/format/2402.09372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Rib Fracture Instance Segmentation and Classification from CT on  the RibFrac Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+J">Jiancheng Yang</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+R">Rui Shi</a>, 
<a href="/search/eess?searchtype=author&query=Jin%2C+L">Liang Jin</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+X">Xiaoyang Huang</a>, 
<a href="/search/eess?searchtype=author&query=Kuang%2C+K">Kaiming Kuang</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+D">Donglai Wei</a>, 
<a href="/search/eess?searchtype=author&query=Gu%2C+S">Shixuan Gu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jianying Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/eess?searchtype=author&query=Chai%2C+Z">Zhizhong Chai</a>, 
<a href="/search/eess?searchtype=author&query=Xiao%2C+Y">Yongjie Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+L">Liming Xu</a>, 
<a href="/search/eess?searchtype=author&query=Du%2C+B">Bang Du</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+X">Xiangyi Yan</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/eess?searchtype=author&query=Alessio%2C+A">Adam Alessio</a>, 
<a href="/search/eess?searchtype=author&query=Holste%2C+G">Gregory Holste</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jiapeng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xiaoming Wang</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+J">Jianye He</a>, 
<a href="/search/eess?searchtype=author&query=Che%2C+L">Lixuan Che</a>, 
<a href="/search/eess?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+B">Bingbing Ni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Challenge paper for MICCAI RibFrac Challenge (<a href="https://ribfrac.grand-challenge.org/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Rib fractures are a common and potentially severe injury that can be
challenging and labor-intensive to detect in CT scans. While there have been
efforts to address this field, the lack of large-scale annotated datasets and
evaluation benchmarks has hindered the development and validation of deep
learning algorithms. To address this issue, the RibFrac Challenge was
introduced, providing a benchmark dataset of over 5,000 rib fractures from 660
CT scans, with voxel-level instance mask annotations and diagnosis labels for
four clinical categories (buckle, nondisplaced, displaced, or segmental). The
challenge includes two tracks: a detection (instance segmentation) track
evaluated by an FROC-style metric and a classification track evaluated by an
F1-style metric. During the MICCAI 2020 challenge period, 243 results were
evaluated, and seven teams were invited to participate in the challenge
summary. The analysis revealed that several top rib fracture detection
solutions achieved performance comparable or even better than human experts.
Nevertheless, the current rib fracture classification solutions are hardly
clinically applicable, which can be an interesting area in the future. As an
active benchmark and research resource, the data and online evaluation of the
RibFrac Challenge are available at the challenge website. As an independent
contribution, we have also extended our previous internal baseline by
incorporating recent advancements in large-scale pretrained networks and
point-based rib segmentation techniques. The resulting FracNet+ demonstrates
competitive performance in rib fracture detection, which lays a foundation for
further research and development in AI-assisted rib fracture detection and
diagnosis.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09378" title="Abstract">arXiv:2402.09378</a> (cross-list from eess.AS) [<a href="/pdf/2402.09378" title="Download PDF">pdf</a>, <a href="/format/2402.09378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot  Text-to-Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ji%2C+S">Shengpeng Ji</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Ziyue Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hanting Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zuo%2C+J">Jialong Zuo</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Zero-shot text-to-speech (TTS) has gained significant attention due to its
powerful voice cloning capabilities, requiring only a few seconds of unseen
speaker voice prompts. However, all previous work has been developed for
cloud-based systems. Taking autoregressive models as an example, although these
approaches achieve high-fidelity voice cloning, they fall short in terms of
inference speed, model size, and robustness. Therefore, we propose
MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech
system based on mobile devices for the first time. Specifically: 1) leveraging
discrete codec, we design a parallel speech mask decoder module called SMD,
which incorporates hierarchical information from the speech codec and weight
mechanisms across different codec layers during the generation process.
Moreover, to bridge the gap between text and speech, we introduce a high-level
probabilistic mask that simulates the progression of information flow from less
to more during speech generation. 2) For speaker prompts, we extract
fine-grained prompt duration from the prompt speech and incorporate text,
prompt speech by cross attention in SMD. We demonstrate the effectiveness of
MobileSpeech on multilingual datasets at different levels, achieving
state-of-the-art results in terms of generating speed and speech quality.
MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully
deployed MobileSpeech on mobile devices. Audio samples are available at
\url{https://mobilespeech.github.io/} .
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09384" title="Abstract">arXiv:2402.09384</a> (cross-list from econ.TH) [<a href="/pdf/2402.09384" title="Download PDF">pdf</a>, <a href="/ps/2402.09384" title="Download PostScript">ps</a>, <a href="/format/2402.09384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persuasion, Delegation, and Private Information in Algorithm-Assisted  Decisions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Xu%2C+R">Ruqing Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">A principal designs an algorithm that generates a publicly observable
prediction of a binary state. She must decide whether to act directly based on
the prediction or to delegate the decision to an agent with private information
but potential misalignment. We study the optimal design of the prediction
algorithm and the delegation rule in such environments. Three key findings
emerge: (1) Delegation is optimal if and only if the principal would make the
same binary decision as the agent had she observed the agent's information. (2)
Providing the most informative algorithm may be suboptimal even if the
principal can act on the algorithm's prediction. Instead, the optimal algorithm
may provide more information about one state and restrict information about the
other. (3) Common restrictions on algorithms, such as keeping a
"human-in-the-loop" or requiring maximal prediction accuracy, strictly worsen
decision quality in the absence of perfectly aligned agents and state-revealing
signals. These findings predict the underperformance of human-machine
collaborations if no measures are taken to mitigate common preference
misalignment between algorithms and human decision-makers.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09387" title="Abstract">arXiv:2402.09387</a> (cross-list from physics.plasm-ph) [<a href="/pdf/2402.09387" title="Download PDF">pdf</a>, <a href="/format/2402.09387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs  with Neural Differential Equations and Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+A+M">Allen M. Wang</a>, 
<a href="/search/physics?searchtype=author&query=So%2C+O">Oswin So</a>, 
<a href="/search/physics?searchtype=author&query=Dawson%2C+C">Charles Dawson</a>, 
<a href="/search/physics?searchtype=author&query=Garnier%2C+D+T">Darren T. Garnier</a>, 
<a href="/search/physics?searchtype=author&query=Rea%2C+C">Cristina Rea</a>, 
<a href="/search/physics?searchtype=author&query=Fan%2C+C">Chuchu Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Plasma Physics (physics.plasm-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The tokamak offers a promising path to fusion energy, but plasma disruptions
pose a major economic risk, motivating considerable advances in disruption
avoidance. This work develops a reinforcement learning approach to this problem
by training a policy to safely ramp-down the plasma current while avoiding
limits on a number of quantities correlated with disruptions. The policy
training environment is a hybrid physics and machine learning model trained on
simulations of the SPARC primary reference discharge (PRD) ramp-down, an
upcoming burning plasma scenario which we use as a testbed. To address physics
uncertainty and model inaccuracies, the simulation environment is massively
parallelized on GPU with randomized physics parameters during policy training.
The trained policy is then successfully transferred to a higher fidelity
simulator where it successfully ramps down the plasma while avoiding
user-specified disruptive limits. We also address the crucial issue of safety
criticality by demonstrating that a constraint-conditioned policy can be used
as a trajectory design assistant to design a library of feed-forward
trajectories to handle different physics conditions and user settings. As a
library of trajectories is more interpretable and verifiable offline, we argue
such an approach is a promising path for leveraging the capabilities of
reinforcement learning in the safety-critical context of burning plasma
tokamaks. Finally, we demonstrate how the training environment can be a useful
platform for other feed-forward optimization approaches by using an
evolutionary algorithm to perform optimization of feed-forward trajectories
that are robust to physics uncertainty
</p>
</div>
</dd>
</dl>
<h3>Replacements for Thu, 15 Feb 24</h3>
<dl>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1606.00963" title="Abstract">arXiv:1606.00963</a> (replaced) [<a href="/pdf/1606.00963" title="Download PDF">pdf</a>, <a href="/format/1606.00963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal quantization for a probability measure on a nonuniform stretched  Sierpi&#x144;ski triangle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandey%2C+M">Megha Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Roychowdhury%2C+M+K">Mrinal Kanti Roychowdhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/1605.02281">arXiv:1605.02281</a>, <a href="/abs/1605.09701">arXiv:1605.09701</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1909.11930" title="Abstract">arXiv:1909.11930</a> (replaced) [<a href="/pdf/1909.11930" title="Download PDF">pdf</a>, <a href="/format/1909.11930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> String Indexing with Compressed Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bille%2C+P">Philip Bille</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B8rtz%2C+I+L">Inge Li G&#xf8;rtz</a>, 
<a href="/search/cs?searchtype=author&query=Steiner%2C+T+A">Teresa Anna Steiner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted journal version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> journal: ACM Trans. Algorithms, volume 19, pages 32:1-32:19, year
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.13884" title="Abstract">arXiv:1910.13884</a> (replaced) [<a href="/pdf/1910.13884" title="Download PDF">pdf</a>, <a href="/ps/1910.13884" title="Download PostScript">ps</a>, <a href="/format/1910.13884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction, Communication, and Computing Duration Optimization for VR  Video Streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenyang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shengqian Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in IEEE TCOM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Multimedia (cs.MM); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.04128" title="Abstract">arXiv:2007.04128</a> (replaced) [<a href="/pdf/2007.04128" title="Download PDF">pdf</a>, <a href="/format/2007.04128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> String Indexing for Top-$k$ Close Consecutive Occurrences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bille%2C+P">Philip Bille</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B8rtz%2C+I+L">Inge Li G&#xf8;rtz</a>, 
<a href="/search/cs?searchtype=author&query=Pedersen%2C+M+R">Max Rish&#xf8;j Pedersen</a>, 
<a href="/search/cs?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>, 
<a href="/search/cs?searchtype=author&query=Steiner%2C+T+A">Teresa Anna Steiner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated to accepted journal version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> journal: Theor. Comput. Sci. volume: 927 pages: 133 - 147 year:
  2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2101.05754" title="Abstract">arXiv:2101.05754</a> (replaced) [<a href="/pdf/2101.05754" title="Download PDF">pdf</a>, <a href="/format/2101.05754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Strong Bisimulation for a Classical Term Calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonelli%2C+E">Eduardo Bonelli</a>, 
<a href="/search/cs?searchtype=author&query=Kesner%2C+D">Delia Kesner</a>, 
<a href="/search/cs?searchtype=author&query=Viso%2C+A">Andr&#xe9;s Viso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1906.09370">arXiv:1906.09370</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.12801" title="Abstract">arXiv:2105.12801</a> (replaced) [<a href="/pdf/2105.12801" title="Download PDF">pdf</a>, <a href="/format/2105.12801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dialectica Petri Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Di+Lavore%2C+E">Elena Di Lavore</a>, 
<a href="/search/math?searchtype=author&query=Leal%2C+W">Wilmer Leal</a>, 
<a href="/search/math?searchtype=author&query=de+Paiva%2C+V">Valeria de Paiva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Reorder sections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.11798" title="Abstract">arXiv:2106.11798</a> (replaced) [<a href="/pdf/2106.11798" title="Download PDF">pdf</a>, <a href="/ps/2106.11798" title="Download PostScript">ps</a>, <a href="/format/2106.11798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The failure of cut-elimination in cyclic proof for first-order logic  with inductive definitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masuoka%2C+Y">Yukihiro Masuoka</a>, 
<a href="/search/cs?searchtype=author&query=Brotherston%2C+J">James Brotherston</a>, 
<a href="/search/cs?searchtype=author&query=Tatsuta%2C+M">Makoto Tatsuta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Logic and Computation, 2023;, exad068
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.11935" title="Abstract">arXiv:2106.11935</a> (replaced) [<a href="/pdf/2106.11935" title="Download PDF">pdf</a>, <a href="/format/2106.11935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Efficient Representation Selection in Low-rank Markov Decision  Processes: From Online to Offline RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weitong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiafan He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dongruo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Amy Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 2 figures, 7 tables, In UAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.08613" title="Abstract">arXiv:2108.08613</a> (replaced) [<a href="/pdf/2108.08613" title="Download PDF">pdf</a>, <a href="/ps/2108.08613" title="Download PostScript">ps</a>, <a href="/format/2108.08613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fine-Grained Complexity of Episode Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bille%2C+P">Philip Bille</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B8rtz%2C+I+L">Inge Li G&#xf8;rtz</a>, 
<a href="/search/cs?searchtype=author&query=Mozes%2C+S">Shay Mozes</a>, 
<a href="/search/cs?searchtype=author&query=Steiner%2C+T+A">Teresa Anna Steiner</a>, 
<a href="/search/cs?searchtype=author&query=Weimann%2C+O">Oren Weimann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the full version of a paper accepted to CPM 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.14465" title="Abstract">arXiv:2110.14465</a> (replaced) [<a href="/pdf/2110.14465" title="Download PDF">pdf</a>, <a href="/format/2110.14465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbiased Statistical Estimation and Valid Confidence Intervals Under  Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Covington%2C+C">Christian Covington</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+X">Xi He</a>, 
<a href="/search/stat?searchtype=author&query=Honaker%2C+J">James Honaker</a>, 
<a href="/search/stat?searchtype=author&query=Kamath%2C+G">Gautam Kamath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Cryptography and Security (cs.CR); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.10985" title="Abstract">arXiv:2202.10985</a> (replaced) [<a href="/pdf/2202.10985" title="Download PDF">pdf</a>, <a href="/format/2202.10985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Registered Report: A Laboratory Experiment on Using Different  Financial-Incentivization Schemes in Software-Engineering Experimentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kr%C3%BCger%2C+J">Jacob Kr&#xfc;ger</a> (1), 
<a href="/search/cs?searchtype=author&query=%C3%87al%C4%B1kl%C4%B1%2C+G">G&#xfc;l &#xc7;al&#x131;kl&#x131;</a> (2), 
<a href="/search/cs?searchtype=author&query=Bershadskyy%2C+D">Dmitri Bershadskyy</a> (3), 
<a href="/search/cs?searchtype=author&query=Heyer%2C+R">Robert Heyer</a> (3), 
<a href="/search/cs?searchtype=author&query=Zabel%2C+S">Sarah Zabel</a> (3 and 4), 
<a href="/search/cs?searchtype=author&query=Otto%2C+S">Siegmar Otto</a> (3 and 4) ((1) Ruhr-University Bochum Germany (2) University of Glasgow UK, (3) Otto-von-Guericke University Magdeburg Germany, (4) University of Hohenheim Germany)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Registered Report accepted at PCI RR. Last update fixed an error in the code screenshot (was missing one line of code)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.00187" title="Abstract">arXiv:2203.00187</a> (replaced) [<a href="/pdf/2203.00187" title="Download PDF">pdf</a>, <a href="/ps/2203.00187" title="Download PostScript">ps</a>, <a href="/format/2203.00187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robots Autonomously Detecting People: A Multimodal Deep Contrastive  Learning Method Robust to Intraclass Variations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fung%2C+A">Angus Fung</a>, 
<a href="/search/cs?searchtype=author&query=Benhabib%2C+B">Beno Benhabib</a>, 
<a href="/search/cs?searchtype=author&query=Nejat%2C+G">Goldie Nejat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.14379" title="Abstract">arXiv:2203.14379</a> (replaced) [<a href="/pdf/2203.14379" title="Download PDF">pdf</a>, <a href="/format/2203.14379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructive Separations and Their Consequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lijie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Ce Jin</a>, 
<a href="/search/cs?searchtype=author&query=Santhanam%2C+R">Rahul Santhanam</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+R">Ryan Williams</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Abstract shortened to fit arXiv requirements
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> TheoretiCS, Volume 3 (2024), Article 3, 1-41
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.08742" title="Abstract">arXiv:2204.08742</a> (replaced) [<a href="/pdf/2204.08742" title="Download PDF">pdf</a>, <a href="/format/2204.08742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoFHEE: A Co-processor for Fully Homomorphic Encryption Execution  (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nabeel%2C+M">Mohammed Nabeel</a>, 
<a href="/search/cs?searchtype=author&query=Gamil%2C+H">Homer Gamil</a>, 
<a href="/search/cs?searchtype=author&query=Soni%2C+D">Deepraj Soni</a>, 
<a href="/search/cs?searchtype=author&query=Ashraf%2C+M">Mohammed Ashraf</a>, 
<a href="/search/cs?searchtype=author&query=Gebremichael%2C+M+A">Mizan Abraha Gebremichael</a>, 
<a href="/search/cs?searchtype=author&query=Chielle%2C+E">Eduardo Chielle</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+R">Ramesh Karri</a>, 
<a href="/search/cs?searchtype=author&query=Sanduleanu%2C+M">Mihai Sanduleanu</a>, 
<a href="/search/cs?searchtype=author&query=Maniatakos%2C+M">Michail Maniatakos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.09815" title="Abstract">arXiv:2204.09815</a> (replaced) [<a href="/pdf/2204.09815" title="Download PDF">pdf</a>, <a href="/format/2204.09815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ozsar%2C+E">Ege Ozsar</a>, 
<a href="/search/math?searchtype=author&query=Kilmer%2C+M">Misha Kilmer</a>, 
<a href="/search/math?searchtype=author&query=Miller%2C+E">Eric Miller</a>, 
<a href="/search/math?searchtype=author&query=de+Sturler%2C+E">Eric de Sturler</a>, 
<a href="/search/math?searchtype=author&query=Saibaba%2C+A">Arvind Saibaba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 35 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.10164" title="Abstract">arXiv:2204.10164</a> (replaced) [<a href="/pdf/2204.10164" title="Download PDF">pdf</a>, <a href="/ps/2204.10164" title="Download PostScript">ps</a>, <a href="/format/2204.10164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linearised Calder&#xf3;n problem: Reconstruction and Lipschitz stability  for infinite-dimensional spaces of unbounded perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Garde%2C+H">Henrik Garde</a>, 
<a href="/search/math?searchtype=author&query=Hyv%C3%B6nen%2C+N">Nuutti Hyv&#xf6;nen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.11921" title="Abstract">arXiv:2205.11921</a> (replaced) [<a href="/pdf/2205.11921" title="Download PDF">pdf</a>, <a href="/format/2205.11921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression-aware Training of Neural Networks using Frank-Wolfe
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zimmer%2C+M">Max Zimmer</a>, 
<a href="/search/cs?searchtype=author&query=Spiegel%2C+C">Christoph Spiegel</a>, 
<a href="/search/cs?searchtype=author&query=Pokutta%2C+S">Sebastian Pokutta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 pages references, 14 pages appendix, 8 figures, and 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.02990" title="Abstract">arXiv:2206.02990</a> (replaced) [<a href="/pdf/2206.02990" title="Download PDF">pdf</a>, <a href="/format/2206.02990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Distributional Stability among Sub-populations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiashuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiayun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+J">Jie Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+P">Peng Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at International Conference on Artificial Intelligence and Statistics (AISTATS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.09032" title="Abstract">arXiv:2206.09032</a> (replaced) [<a href="/pdf/2206.09032" title="Download PDF">pdf</a>, <a href="/format/2206.09032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conjunctive Queries with Free Access Patterns under Updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kara%2C+A">Ahmet Kara</a>, 
<a href="/search/cs?searchtype=author&query=Nikolic%2C+M">Milos Nikolic</a>, 
<a href="/search/cs?searchtype=author&query=Olteanu%2C+D">Dan Olteanu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haozhe Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended and polished version. Added new Section 11 on the dynamic evaluation of conjunctive queries with free access patterns over probabilistic databases
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.01390" title="Abstract">arXiv:2207.01390</a> (replaced) [<a href="/pdf/2207.01390" title="Download PDF">pdf</a>, <a href="/format/2207.01390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FakeNews: GAN-based generation of realistic 3D volumetric data -- A  systematic review and taxonomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+A">Andr&#xe9; Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianning Li</a>, 
<a href="/search/cs?searchtype=author&query=Pomykala%2C+K+L">Kelsey L. Pomykala</a>, 
<a href="/search/cs?searchtype=author&query=Kleesiek%2C+J">Jens Kleesiek</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+V">Victor Alves</a>, 
<a href="/search/cs?searchtype=author&query=Egger%2C+J">Jan Egger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 88 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Medical Image Analysis, 103100 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Databases (cs.DB); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.03915" title="Abstract">arXiv:2208.03915</a> (replaced) [<a href="/pdf/2208.03915" title="Download PDF">pdf</a>, <a href="/ps/2208.03915" title="Download PostScript">ps</a>, <a href="/format/2208.03915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Maintenance of Kernel Density Estimation Data Structure: From  Practice to Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiehao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhaozhuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Junze Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+D">Danyang Zhuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.04188" title="Abstract">arXiv:2208.04188</a> (replaced) [<a href="/pdf/2208.04188" title="Download PDF">pdf</a>, <a href="/ps/2208.04188" title="Download PostScript">ps</a>, <a href="/format/2208.04188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A quadratic estimation for the K&#xfc;hnel conjecture on embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dzhenzher%2C+S">S. Dzhenzher</a>, 
<a href="/search/math?searchtype=author&query=Skopenkov%2C+A">A. Skopenkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, no figures, exposition improved
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Algebraic Topology (math.AT); Geometric Topology (math.GT)

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.13504" title="Abstract">arXiv:2208.13504</a> (replaced) [<a href="/pdf/2208.13504" title="Download PDF">pdf</a>, <a href="/format/2208.13504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-scale unsupervised spatio-temporal semantic analysis of vast  regions from satellite images sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Echegoyen%2C+C">Carlos Echegoyen</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+A">Aritz P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Santaf%C3%A9%2C+G">Guzm&#xe1;n Santaf&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-Goya%2C+U">Unai P&#xe9;rez-Goya</a>, 
<a href="/search/cs?searchtype=author&query=Ugarte%2C+M+D">Mar&#xed;a Dolores Ugarte</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Statistics and Computing, Volume 34, article number 71, (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.01146" title="Abstract">arXiv:2209.01146</a> (replaced) [<a href="/pdf/2209.01146" title="Download PDF">pdf</a>, <a href="/ps/2209.01146" title="Download PostScript">ps</a>, <a href="/format/2209.01146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Principal-Agency: Contracts, Information, Games and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+J">Jiarui Gan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+M">Minbiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jibang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.14905" title="Abstract">arXiv:2209.14905</a> (replaced) [<a href="/pdf/2209.14905" title="Download PDF">pdf</a>, <a href="/format/2209.14905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance Covariance Regularization Enforces Pairwise Independence in  Self-Supervised Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mialon%2C+G">Gr&#xe9;goire Mialon</a>, 
<a href="/search/cs?searchtype=author&query=Balestriero%2C+R">Randall Balestriero</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02042" title="Abstract">arXiv:2210.02042</a> (replaced) [<a href="/pdf/2210.02042" title="Download PDF">pdf</a>, <a href="/format/2210.02042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedMT: Federated Learning with Mixed-type Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Talhouk%2C+A">Aline Talhouk</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+G">Gang Niu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.09974" title="Abstract">arXiv:2210.09974</a> (replaced) [<a href="/pdf/2210.09974" title="Download PDF">pdf</a>, <a href="/format/2210.09974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical Guarantees for Permutation-Equivariant Quantum Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Schatzki%2C+L">Louis Schatzki</a>, 
<a href="/search/quant-ph?searchtype=author&query=Larocca%2C+M">Martin Larocca</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nguyen%2C+Q+T">Quynh T. Nguyen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sauvage%2C+F">Frederic Sauvage</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cerezo%2C+M">M. Cerezo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15+21 pages, 5 + 5 figures. Prior generalization bounds replaced with more general theorem. Comments added about hardness of simulation and narrow gorges
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> npj Quantum Inf 10, 12 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.01595" title="Abstract">arXiv:2211.01595</a> (replaced) [<a href="/pdf/2211.01595" title="Download PDF">pdf</a>, <a href="/format/2211.01595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning in Non-Markovian Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chandak%2C+S">Siddharth Chandak</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+P">Pratik Shah</a>, 
<a href="/search/eess?searchtype=author&query=Borkar%2C+V+S">Vivek S Borkar</a>, 
<a href="/search/eess?searchtype=author&query=Dodhia%2C+P">Parth Dodhia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, accepted for publication at Systems and Control Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08257" title="Abstract">arXiv:2211.08257</a> (replaced) [<a href="/pdf/2211.08257" title="Download PDF">pdf</a>, <a href="/format/2211.08257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoTherm: A Dataset and Benchmark for Thermal Comfort Estimation  Indoors and in Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colley%2C+M">Mark Colley</a>, 
<a href="/search/cs?searchtype=author&query=Hartwig%2C+S">Sebastian Hartwig</a>, 
<a href="/search/cs?searchtype=author&query=Zeqiri%2C+A">Albin Zeqiri</a>, 
<a href="/search/cs?searchtype=author&query=Ropinski%2C+T">Timo Ropinski</a>, 
<a href="/search/cs?searchtype=author&query=Rukzio%2C+E">Enrico Rukzio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10936" title="Abstract">arXiv:2211.10936</a> (replaced) [<a href="/pdf/2211.10936" title="Download PDF">pdf</a>, <a href="/format/2211.10936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop  Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Wen Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yaoxin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11700" title="Abstract">arXiv:2211.11700</a> (replaced) [<a href="/pdf/2211.11700" title="Download PDF">pdf</a>, <a href="/format/2211.11700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Dimensional Undirected Graphical Models for Arbitrary Mixed Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=G%C3%B6bler%2C+K">Konstantin G&#xf6;bler</a>, 
<a href="/search/stat?searchtype=author&query=Miloschewski%2C+A">Anne Miloschewski</a>, 
<a href="/search/stat?searchtype=author&query=Drton%2C+M">Mathias Drton</a>, 
<a href="/search/stat?searchtype=author&query=Mukherjee%2C+S">Sach Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 2 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15118" title="Abstract">arXiv:2211.15118</a> (replaced) [<a href="/pdf/2211.15118" title="Download PDF">pdf</a>, <a href="/format/2211.15118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Faster $k$-means++ Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiehao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Sarkhel%2C+S">Somdeb Sarkhel</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+C">Chenbo Yin</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Junze Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+D">Danyang Zhuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00214" title="Abstract">arXiv:2212.00214</a> (replaced) [<a href="/pdf/2212.00214" title="Download PDF">pdf</a>, <a href="/format/2212.00214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Mixup Augmentation for Data and Class-Specific Uncertainty  Estimation in Deep Learning Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hansang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Haeil Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">Helen Hong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junmo Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10672" title="Abstract">arXiv:2212.10672</a> (replaced) [<a href="/pdf/2212.10672" title="Download PDF">pdf</a>, <a href="/ps/2212.10672" title="Download PostScript">ps</a>, <a href="/format/2212.10672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Additive estimates of the permanent using Gaussian fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mukerji%2C+T">Tantrik Mukerji</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+W">Wei-Shih Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added section 4, various corrections, and new introduction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Data Structures and Algorithms (cs.DS); Combinatorics (math.CO); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.07530" title="Abstract">arXiv:2301.07530</a> (replaced) [<a href="/pdf/2301.07530" title="Download PDF">pdf</a>, <a href="/format/2301.07530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimistically Tempered Online Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haddouche%2C+M">Maxime Haddouche</a>, 
<a href="/search/cs?searchtype=author&query=Wintenberger%2C+O">Olivier Wintenberger</a>, 
<a href="/search/cs?searchtype=author&query=Guedj%2C+B">Benjamin Guedj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13185" title="Abstract">arXiv:2301.13185</a> (replaced) [<a href="/pdf/2301.13185" title="Download PDF">pdf</a>, <a href="/format/2301.13185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Decision Tree Policies for Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vos%2C+D">Dani&#xeb;l Vos</a>, 
<a href="/search/cs?searchtype=author&query=Verwer%2C+S">Sicco Verwer</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the Thirty-Second International Joint Conference on
  Artificial Intelligence Main Track, 5457-5465 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13289" title="Abstract">arXiv:2301.13289</a> (replaced) [<a href="/pdf/2301.13289" title="Download PDF">pdf</a>, <a href="/format/2301.13289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Statistical Benefits of Temporal Difference Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheikhi%2C+D">David Cheikhi</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+D">Daniel Russo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 7 figures, submitted to ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06239" title="Abstract">arXiv:2302.06239</a> (replaced) [<a href="/pdf/2302.06239" title="Download PDF">pdf</a>, <a href="/format/2302.06239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite element hybridization of port-Hamiltonian systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Brugnoli%2C+A">Andrea Brugnoli</a>, 
<a href="/search/math?searchtype=author&query=Rashad%2C+R">Ramy Rashad</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/math?searchtype=author&query=Stramigioli%2C+S">Stefano Stramigioli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10809" title="Abstract">arXiv:2302.10809</a> (replaced) [<a href="/pdf/2302.10809" title="Download PDF">pdf</a>, <a href="/format/2302.10809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Explanations for Sequential Decision-Making in Multi-Agent  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gyevnar%2C+B">Balint Gyevnar</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+C+G">Christopher G. Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in 23rd International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13937" title="Abstract">arXiv:2302.13937</a> (replaced) [<a href="/pdf/2302.13937" title="Download PDF">pdf</a>, <a href="/format/2302.13937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human and Machine Intelligence in n-Person Games with Partial Knowledge:  Theory and Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Ismail%2C+M+S">Mehmet S. Ismail</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14699" title="Abstract">arXiv:2302.14699</a> (replaced) [<a href="/pdf/2302.14699" title="Download PDF">pdf</a>, <a href="/format/2302.14699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Tennenbaum&#x27;s Theorem in Constructive Type Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hermes%2C+M">Marc Hermes</a>, 
<a href="/search/math?searchtype=author&query=Kirst%2C+D">Dominik Kirst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, extension of conference paper published at FSCD 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02186" title="Abstract">arXiv:2303.02186</a> (replaced) [<a href="/pdf/2303.02186" title="Download PDF">pdf</a>, <a href="/ps/2303.02186" title="Download PostScript">ps</a>, <a href="/format/2303.02186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berrevoets%2C+J">Jeroen Berrevoets</a>, 
<a href="/search/cs?searchtype=author&query=Kacprzyk%2C+K">Krzysztof Kacprzyk</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhaozhi Qian</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2212.00911">arXiv:2212.00911</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03271" title="Abstract">arXiv:2303.03271</a> (replaced) [<a href="/pdf/2303.03271" title="Download PDF">pdf</a>, <a href="/ps/2303.03271" title="Download PostScript">ps</a>, <a href="/format/2303.03271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fibrational Tale of Operational Logical Relations: Pure, Effectful and  Differential
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dagnino%2C+F">Francesco Dagnino</a>, 
<a href="/search/cs?searchtype=author&query=Gavazzo%2C+F">Francesco Gavazzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04073" title="Abstract">arXiv:2303.04073</a> (replaced) [<a href="/pdf/2303.04073" title="Download PDF">pdf</a>, <a href="/format/2303.04073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operationalizing AI in Future Networks: A Bird&#x27;s Eye View from the  System Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianzhu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hemmatpour%2C+M">Masoud Hemmatpour</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Han Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+S">Chung Shue Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mellia%2C+M">Marco Mellia</a>, 
<a href="/search/cs?searchtype=author&query=Aghasaryan%2C+A">Armen Aghasaryan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05739" title="Abstract">arXiv:2303.05739</a> (replaced) [<a href="/pdf/2303.05739" title="Download PDF">pdf</a>, <a href="/format/2303.05739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEDetection: A Simple Framework for Semi-Supervised Few-Shot Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+P+V">Phi Vu Tran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS 2024. The code is available at <a href="https://github.com/lexisnexis-risk-open-source/ledetection">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10165" title="Abstract">arXiv:2303.10165</a> (replaced) [<a href="/pdf/2303.10165" title="Download PDF">pdf</a>, <a href="/ps/2303.10165" title="Download PostScript">ps</a>, <a href="/format/2303.10165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junkai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weitong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 1 figure, 2 tables. In ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10456" title="Abstract">arXiv:2303.10456</a> (replaced) [<a href="/pdf/2303.10456" title="Download PDF">pdf</a>, <a href="/format/2303.10456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Multiple-Access Channel with Entangled Transmitters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pereg%2C+U">Uzi Pereg</a>, 
<a href="/search/quant-ph?searchtype=author&query=Deppe%2C+C">Christian Deppe</a>, 
<a href="/search/quant-ph?searchtype=author&query=Boche%2C+H">Holger Boche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14750" title="Abstract">arXiv:2303.14750</a> (replaced) [<a href="/pdf/2303.14750" title="Download PDF">pdf</a>, <a href="/format/2303.14750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Approach for Generating Families of Energetically Optimal Gaits from  Passive Dynamic Walking Gaits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosa%2C+N">Nelson Rosa</a>, 
<a href="/search/cs?searchtype=author&query=Katamish%2C+B">Bassel Katamish</a>, 
<a href="/search/cs?searchtype=author&query=Raff%2C+M">Maximilian Raff</a>, 
<a href="/search/cs?searchtype=author&query=Remy%2C+C+D">C. David Remy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the IROS 2023 proceedings. This version has several updated sections and figures. The proceedings contains a supplementary video, see <a href="https://ieeexplore.ieee.org/document/10342322/media#media">this https URL</a> . An implementation of the results are also available, see <a href="https://github.com/nr-codes/OptimalGaitsForCompassGait">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Detroit, MI, USA, 2023, pp. 8551-8557
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16266" title="Abstract">arXiv:2303.16266</a> (replaced) [<a href="/pdf/2303.16266" title="Download PDF">pdf</a>, <a href="/format/2303.16266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-line reinforcement learning for optimization of real-life energy  trading strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lepak%2C+%C5%81">&#x141;ukasz Lepak</a>, 
<a href="/search/cs?searchtype=author&query=Wawrzy%C5%84ski%2C+P">Pawe&#x142; Wawrzy&#x144;ski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Trading and Market Microstructure (q-fin.TR)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06011" title="Abstract">arXiv:2304.06011</a> (replaced) [<a href="/pdf/2304.06011" title="Download PDF">pdf</a>, <a href="/ps/2304.06011" title="Download PostScript">ps</a>, <a href="/format/2304.06011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MABL: Bi-Level Latent-Variable World Model for Sample-Efficient  Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venugopal%2C+A">Aravind Venugopal</a>, 
<a href="/search/cs?searchtype=author&query=Milani%2C+S">Stephanie Milani</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Ravindran%2C+B">Balaraman Ravindran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08460" title="Abstract">arXiv:2304.08460</a> (replaced) [<a href="/pdf/2304.08460" title="Download PDF">pdf</a>, <a href="/format/2304.08460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LongForm: Effective Instruction Tuning with Reverse Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C3%B6ksal%2C+A">Abdullatif K&#xf6;ksal</a>, 
<a href="/search/cs?searchtype=author&query=Schick%2C+T">Timo Schick</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version extends the evaluation with new metrics and NLU tasks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12606" title="Abstract">arXiv:2304.12606</a> (replaced) [<a href="/pdf/2304.12606" title="Download PDF">pdf</a>, <a href="/ps/2304.12606" title="Download PostScript">ps</a>, <a href="/format/2304.12606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistics of Random Binning Based on Tsallis Divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kavian%2C+M">Masoud Kavian</a>, 
<a href="/search/cs?searchtype=author&query=Mojahedian%2C+M+M">Mohammad Mahdi Mojahedian</a>, 
<a href="/search/cs?searchtype=author&query=Yassaee%2C+M+H">Mohammad Hossein Yassaee</a>, 
<a href="/search/cs?searchtype=author&query=Mirmohseni%2C+M">Mahtab Mirmohseni</a>, 
<a href="/search/cs?searchtype=author&query=Aref%2C+M+R">Mohammad Reza Aref</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03584" title="Abstract">arXiv:2305.03584</a> (replaced) [<a href="/pdf/2305.03584" title="Download PDF">pdf</a>, <a href="/format/2305.03584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Now It Sounds Like You: Learning Personalized Vocabulary On Device
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sid Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shenoy%2C+A">Ashish Shenoy</a>, 
<a href="/search/cs?searchtype=author&query=Chuang%2C+P">Pierce Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+J">John Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Federated Learning, Personalization, On-device NLP, Accepted at AAAI Spring Symposium 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04079" title="Abstract">arXiv:2305.04079</a> (replaced) [<a href="/pdf/2305.04079" title="Download PDF">pdf</a>, <a href="/format/2305.04079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study on Governance in Bitcoin&#x27;s Consensus Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Notland%2C+J+S">Jakob Svennevik Notland</a>, 
<a href="/search/cs?searchtype=author&query=Nowostawski%2C+M">Mariusz Nowostawski</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingyue Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05629" title="Abstract">arXiv:2305.05629</a> (replaced) [<a href="/pdf/2305.05629" title="Download PDF">pdf</a>, <a href="/ps/2305.05629" title="Download PostScript">ps</a>, <a href="/format/2305.05629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured condition numbers for a linear function of the solution of  the generalized saddle point problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ahmad%2C+S+S">Sk. Safique Ahmad</a>, 
<a href="/search/math?searchtype=author&query=Khatun%2C+P">Pinki Khatun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08785" title="Abstract">arXiv:2305.08785</a> (replaced) [<a href="/pdf/2305.08785" title="Download PDF">pdf</a>, <a href="/format/2305.08785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Assumptions with Respect to Occlusions in Urban Environments for  Automated Vehicle Speed Decisions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Graubohm%2C+R">Robert Graubohm</a>, 
<a href="/search/eess?searchtype=author&query=Salem%2C+N+F">Nayel Fabian Salem</a>, 
<a href="/search/eess?searchtype=author&query=Nolte%2C+M">Marcus Nolte</a>, 
<a href="/search/eess?searchtype=author&query=Maurer%2C+M">Markus Maurer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC), Bilbao, Spain, September 24-28, 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE 26th International Conference on Intelligent
  Transportation Systems (ITSC), Bilbao, Spain, 2023, pp. 738-745
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11481" title="Abstract">arXiv:2305.11481</a> (replaced) [<a href="/pdf/2305.11481" title="Download PDF">pdf</a>, <a href="/format/2305.11481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xingjian He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yisi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiachen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiangyun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11766" title="Abstract">arXiv:2305.11766</a> (replaced) [<a href="/pdf/2305.11766" title="Download PDF">pdf</a>, <a href="/format/2305.11766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer operators on graphs: Spectral clustering and beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Klus%2C+S">Stefan Klus</a>, 
<a href="/search/stat?searchtype=author&query=Trower%2C+M">Maia Trower</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11934" title="Abstract">arXiv:2305.11934</a> (replaced) [<a href="/pdf/2305.11934" title="Download PDF">pdf</a>, <a href="/format/2305.11934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inductive Simulation of Calorimeter Showers with Normalizing Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Buckley%2C+M+R">Matthew R. Buckley</a>, 
<a href="/search/physics?searchtype=author&query=Krause%2C+C">Claudius Krause</a>, 
<a href="/search/physics?searchtype=author&query=Pang%2C+I">Ian Pang</a>, 
<a href="/search/physics?searchtype=author&query=Shih%2C+D">David Shih</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 15 figures; v2: title changed, matches published version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. D 109, 033006 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); High Energy Physics - Phenomenology (hep-ph); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12131" title="Abstract">arXiv:2305.12131</a> (replaced) [<a href="/pdf/2305.12131" title="Download PDF">pdf</a>, <a href="/format/2305.12131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-stationary Online Convex Optimization with Arbitrary Delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yuanyu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+C">Chang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingli Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12569" title="Abstract">arXiv:2305.12569</a> (replaced) [<a href="/pdf/2305.12569" title="Download PDF">pdf</a>, <a href="/format/2305.12569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Generative Modeling for High-dimensional Marked Temporal  Point Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dong%2C+Z">Zheng Dong</a>, 
<a href="/search/stat?searchtype=author&query=Fan%2C+Z">Zekai Fan</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+S">Shixiang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13450" title="Abstract">arXiv:2305.13450</a> (replaced) [<a href="/pdf/2305.13450" title="Download PDF">pdf</a>, <a href="/format/2305.13450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Fine-Grained Synchronization of Dependent GPU Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jangda%2C+A">Abhinav Jangda</a>, 
<a href="/search/cs?searchtype=author&query=Maleki%2C+S">Saeed Maleki</a>, 
<a href="/search/cs?searchtype=author&query=Dehnavi%2C+M+M">Maryam Mehri Dehnavi</a>, 
<a href="/search/cs?searchtype=author&query=Musuvathi%2C+M">Madan Musuvathi</a>, 
<a href="/search/cs?searchtype=author&query=Saarikivi%2C+O">Olli Saarikivi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CGO 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14771" title="Abstract">arXiv:2305.14771</a> (replaced) [<a href="/pdf/2305.14771" title="Download PDF">pdf</a>, <a href="/format/2305.14771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> David helps Goliath: Inference-Time Collaboration Between Small  Specialized and Large General Diffusion LMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaochuang Han</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sachin Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>, 
<a href="/search/cs?searchtype=author&query=Ghazvininejad%2C+M">Marjan Ghazvininejad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15742" title="Abstract">arXiv:2305.15742</a> (replaced) [<a href="/pdf/2305.15742" title="Download PDF">pdf</a>, <a href="/format/2305.15742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Generative Models for Time-Varying Treatments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wu%2C+S">Shenghao Wu</a>, 
<a href="/search/stat?searchtype=author&query=Zhou%2C+W">Wenbin Zhou</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+M">Minshuo Chen</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+S">Shixiang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17033" title="Abstract">arXiv:2305.17033</a> (replaced) [<a href="/pdf/2305.17033" title="Download PDF">pdf</a>, <a href="/format/2305.17033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kazerooni%2C+A+F">Anahita Fathi Kazerooni</a>, 
<a href="/search/eess?searchtype=author&query=Khalili%2C+N">Nastaran Khalili</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xinyang Liu</a>, 
<a href="/search/eess?searchtype=author&query=Haldar%2C+D">Debanjan Haldar</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Zhifan Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Anwar%2C+S+M">Syed Muhammed Anwar</a>, 
<a href="/search/eess?searchtype=author&query=Albrecht%2C+J">Jake Albrecht</a>, 
<a href="/search/eess?searchtype=author&query=Adewole%2C+M">Maruf Adewole</a>, 
<a href="/search/eess?searchtype=author&query=Anazodo%2C+U">Udunna Anazodo</a>, 
<a href="/search/eess?searchtype=author&query=Anderson%2C+H">Hannah Anderson</a>, 
<a href="/search/eess?searchtype=author&query=Bagheri%2C+S">Sina Bagheri</a>, 
<a href="/search/eess?searchtype=author&query=Baid%2C+U">Ujjwal Baid</a>, 
<a href="/search/eess?searchtype=author&query=Bergquist%2C+T">Timothy Bergquist</a>, 
<a href="/search/eess?searchtype=author&query=Borja%2C+A+J">Austin J. Borja</a>, 
<a href="/search/eess?searchtype=author&query=Calabrese%2C+E">Evan Calabrese</a>, 
<a href="/search/eess?searchtype=author&query=Chung%2C+V">Verena Chung</a>, 
<a href="/search/eess?searchtype=author&query=Conte%2C+G">Gian-Marco Conte</a>, 
<a href="/search/eess?searchtype=author&query=Dako%2C+F">Farouk Dako</a>, 
<a href="/search/eess?searchtype=author&query=Eddy%2C+J">James Eddy</a>, 
<a href="/search/eess?searchtype=author&query=Ezhov%2C+I">Ivan Ezhov</a>, 
<a href="/search/eess?searchtype=author&query=Familiar%2C+A">Ariana Familiar</a>, 
<a href="/search/eess?searchtype=author&query=Farahani%2C+K">Keyvan Farahani</a>, 
<a href="/search/eess?searchtype=author&query=Haldar%2C+S">Shuvanjan Haldar</a>, 
<a href="/search/eess?searchtype=author&query=Iglesias%2C+J+E">Juan Eugenio Iglesias</a>, 
<a href="/search/eess?searchtype=author&query=Janas%2C+A">Anastasia Janas</a>, 
<a href="/search/eess?searchtype=author&query=Johansen%2C+E">Elaine Johansen</a>, 
<a href="/search/eess?searchtype=author&query=Jones%2C+B+V">Blaise V Jones</a>, 
<a href="/search/eess?searchtype=author&query=Kofler%2C+F">Florian Kofler</a>, 
<a href="/search/eess?searchtype=author&query=LaBella%2C+D">Dominic LaBella</a>, 
<a href="/search/eess?searchtype=author&query=Lai%2C+H+A">Hollie Anne Lai</a>, 
<a href="/search/eess?searchtype=author&query=Van+Leemput%2C+K">Koen Van Leemput</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H+B">Hongwei Bran Li</a>, 
<a href="/search/eess?searchtype=author&query=Maleki%2C+N">Nazanin Maleki</a>, 
<a href="/search/eess?searchtype=author&query=McAllister%2C+A+S">Aaron S McAllister</a>, 
<a href="/search/eess?searchtype=author&query=Meier%2C+Z">Zeke Meier</a>, 
<a href="/search/eess?searchtype=author&query=Menze%2C+B">Bjoern Menze</a>, 
<a href="/search/eess?searchtype=author&query=Moawad%2C+A+W">Ahmed W Moawad</a>, 
<a href="/search/eess?searchtype=author&query=Nandolia%2C+K+K">Khanak K Nandolia</a>, 
<a href="/search/eess?searchtype=author&query=Pavaine%2C+J">Julija Pavaine</a>, 
<a href="/search/eess?searchtype=author&query=Piraud%2C+M">Marie Piraud</a>, 
<a href="/search/eess?searchtype=author&query=Poussaint%2C+T">Tina Poussaint</a>, 
<a href="/search/eess?searchtype=author&query=Prabhu%2C+S+P">Sanjay P Prabhu</a>, 
<a href="/search/eess?searchtype=author&query=Reitman%2C+Z">Zachary Reitman</a>, 
<a href="/search/eess?searchtype=author&query=Rodriguez%2C+A">Andres Rodriguez</a>, 
<a href="/search/eess?searchtype=author&query=Rudie%2C+J+D">Jeffrey D Rudie</a>, 
<a href="/search/eess?searchtype=author&query=Shaikh%2C+I+S">Ibraheem Salman Shaikh</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+L+M">Lubdha M. Shah</a>, 
<a href="/search/eess?searchtype=author&query=Sheth%2C+N">Nakul Sheth</a>, 
<a href="/search/eess?searchtype=author&query=Shinohara%2C+R+T">Russel Taki Shinohara</a>,  et al. (23 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18484" title="Abstract">arXiv:2305.18484</a> (replaced) [<a href="/pdf/2305.18484" title="Download PDF">pdf</a>, <a href="/format/2305.18484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Fourier Transform: A General Approach to Equivariant  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Koyama%2C+M">Masanori Koyama</a>, 
<a href="/search/stat?searchtype=author&query=Fukumizu%2C+K">Kenji Fukumizu</a>, 
<a href="/search/stat?searchtype=author&query=Hayashi%2C+K">Kohei Hayashi</a>, 
<a href="/search/stat?searchtype=author&query=Miyato%2C+T">Takeru Miyato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18671" title="Abstract">arXiv:2305.18671</a> (replaced) [<a href="/pdf/2305.18671" title="Download PDF">pdf</a>, <a href="/format/2305.18671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty  Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+Y">Yifei Liu</a>, 
<a href="/search/stat?searchtype=author&query=Shen%2C+R">Rex Shen</a>, 
<a href="/search/stat?searchtype=author&query=Shen%2C+X">Xiaotong Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19685" title="Abstract">arXiv:2305.19685</a> (replaced) [<a href="/pdf/2305.19685" title="Download PDF">pdf</a>, <a href="/format/2305.19685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Stochastic Mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Orlova%2C+E">Elena Orlova</a>, 
<a href="/search/cs?searchtype=author&query=Ustimenko%2C+A">Aleksei Ustimenko</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Ruoxi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+P+Y">Peter Y. Lu</a>, 
<a href="/search/cs?searchtype=author&query=Willett%2C+R">Rebecca Willett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantum Physics (quant-ph); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00740" title="Abstract">arXiv:2306.00740</a> (replaced) [<a href="/pdf/2306.00740" title="Download PDF">pdf</a>, <a href="/format/2306.00740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Limitations of Temperature Scaling for Distributions with  Overlaps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chidambaram%2C+M">Muthu Chidambaram</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+R">Rong Ge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 9 Figures, published in ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02775" title="Abstract">arXiv:2306.02775</a> (replaced) [<a href="/pdf/2306.02775" title="Download PDF">pdf</a>, <a href="/format/2306.02775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Input-gradient space particle inference for neural network ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Trinh%2C+T">Trung Trinh</a>, 
<a href="/search/stat?searchtype=author&query=Heinonen%2C+M">Markus Heinonen</a>, 
<a href="/search/stat?searchtype=author&query=Acerbi%2C+L">Luigi Acerbi</a>, 
<a href="/search/stat?searchtype=author&query=Kaski%2C+S">Samuel Kaski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR 2024 (spotlight presentation)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02895" title="Abstract">arXiv:2306.02895</a> (replaced) [<a href="/pdf/2306.02895" title="Download PDF">pdf</a>, <a href="/format/2306.02895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evading Black-box Classifiers Without Breaking Eggs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Debenedetti%2C+E">Edoardo Debenedetti</a>, 
<a href="/search/cs?searchtype=author&query=Carlini%2C+N">Nicholas Carlini</a>, 
<a href="/search/cs?searchtype=author&query=Tram%C3%A8r%2C+F">Florian Tram&#xe8;r</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code at <a href="https://github.com/ethz-privsec/realistic-adv-examples.">this https URL</a> Accepted at IEEE SaTML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02939" title="Abstract">arXiv:2306.02939</a> (replaced) [<a href="/pdf/2306.02939" title="Download PDF">pdf</a>, <a href="/format/2306.02939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Stability and Generalization Guarantees of the Decentralized  SGD Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bars%2C+B+L">Batiste Le Bars</a>, 
<a href="/search/cs?searchtype=author&query=Bellet%2C+A">Aur&#xe9;lien Bellet</a>, 
<a href="/search/cs?searchtype=author&query=Tommasi%2C+M">Marc Tommasi</a>, 
<a href="/search/cs?searchtype=author&query=Scaman%2C+K">Kevin Scaman</a>, 
<a href="/search/cs?searchtype=author&query=Neglia%2C+G">Giovanni Neglia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03218" title="Abstract">arXiv:2306.03218</a> (replaced) [<a href="/pdf/2306.03218" title="Download PDF">pdf</a>, <a href="/format/2306.03218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal transport for automatic alignment of untargeted metabolomic data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Breeur%2C+M">Marie Breeur</a>, 
<a href="/search/q-bio?searchtype=author&query=Stepaniants%2C+G">George Stepaniants</a>, 
<a href="/search/q-bio?searchtype=author&query=Keski-Rahkonen%2C+P">Pekka Keski-Rahkonen</a>, 
<a href="/search/q-bio?searchtype=author&query=Rigollet%2C+P">Philippe Rigollet</a>, 
<a href="/search/q-bio?searchtype=author&query=Viallon%2C+V">Vivian Viallon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04828" title="Abstract">arXiv:2306.04828</a> (replaced) [<a href="/pdf/2306.04828" title="Download PDF">pdf</a>, <a href="/format/2306.04828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Effective GNN Training with Linearized Random Spanning Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonchi%2C+F">Francesco Bonchi</a>, 
<a href="/search/cs?searchtype=author&query=Gentile%2C+C">Claudio Gentile</a>, 
<a href="/search/cs?searchtype=author&query=Nerini%2C+F+P">Francesco Paolo Nerini</a>, 
<a href="/search/cs?searchtype=author&query=Panisson%2C+A">Andr&#xe9; Panisson</a>, 
<a href="/search/cs?searchtype=author&query=Vitale%2C+F">Fabio Vitale</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05350" title="Abstract">arXiv:2306.05350</a> (replaced) [<a href="/pdf/2306.05350" title="Download PDF">pdf</a>, <a href="/format/2306.05350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches  For Speech Emotion Recognition Using Pre-trained Speech Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tiantian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+S">Shrikanth Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was accepted to the 11th International Conference on Affective Computing and Intelligent Interaction (ACII), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09434" title="Abstract">arXiv:2306.09434</a> (replaced) [<a href="/pdf/2306.09434" title="Download PDF">pdf</a>, <a href="/format/2306.09434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECO-CHIP: Estimation of Carbon Footprint of Chiplet-based Architectures  for Sustainable VLSI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sudarshan%2C+C+C">Chetan Choppali Sudarshan</a>, 
<a href="/search/cs?searchtype=author&query=Matkar%2C+N">Nikhil Matkar</a>, 
<a href="/search/cs?searchtype=author&query=Vrudhula%2C+S">Sarma Vrudhula</a>, 
<a href="/search/cs?searchtype=author&query=Sapatnekar%2C+S+S">Sachin S. Sapatnekar</a>, 
<a href="/search/cs?searchtype=author&query=Chhabria%2C+V+A">Vidya A. Chhabria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at International Symposium on High-Performance Computer Architecture (HPCA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09690" title="Abstract">arXiv:2306.09690</a> (replaced) [<a href="/pdf/2306.09690" title="Download PDF">pdf</a>, <a href="/format/2306.09690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Physiological and Psychological Responses in Virtual  Reality and Flat Screen Gaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vatsal%2C+R">Ritik Vatsal</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Shrivatsa Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Thareja%2C+R">Rushil Thareja</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+M">Mrinmoy Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+O">Ojaswa Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Shukla%2C+J">Jainendra Shukla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted in the IEEE Transactions on Affective Computing. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10816" title="Abstract">arXiv:2306.10816</a> (replaced) [<a href="/pdf/2306.10816" title="Download PDF">pdf</a>, <a href="/format/2306.10816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\texttt{causalAssembly}$: Generating Realistic Production Data for  Benchmarking Causal Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=G%C3%B6bler%2C+K">Konstantin G&#xf6;bler</a>, 
<a href="/search/stat?searchtype=author&query=Windisch%2C+T">Tobias Windisch</a>, 
<a href="/search/stat?searchtype=author&query=Drton%2C+M">Mathias Drton</a>, 
<a href="/search/stat?searchtype=author&query=Pychynski%2C+T">Tim Pychynski</a>, 
<a href="/search/stat?searchtype=author&query=Sonntag%2C+S">Steffen Sonntag</a>, 
<a href="/search/stat?searchtype=author&query=Roth%2C+M">Martin Roth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12214" title="Abstract">arXiv:2306.12214</a> (replaced) [<a href="/pdf/2306.12214" title="Download PDF">pdf</a>, <a href="/format/2306.12214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More PAC-Bayes bounds: From bounded losses, to losses with general tail  behaviors, to anytime-validity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rodr%C3%ADguez-G%C3%A1lvez%2C+B">Borja Rodr&#xed;guez-G&#xe1;lvez</a>, 
<a href="/search/stat?searchtype=author&query=Thobaben%2C+R">Ragnar Thobaben</a>, 
<a href="/search/stat?searchtype=author&query=Skoglund%2C+M">Mikael Skoglund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages: ~13.5 of main text, ~4.5 of references, and ~13 of appendices. Sections 2 and 3 are presented as short papers in the "PAC-Bayes Meets Interactive Learning" workshop at ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12644" title="Abstract">arXiv:2306.12644</a> (replaced) [<a href="/pdf/2306.12644" title="Download PDF">pdf</a>, <a href="/format/2306.12644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Chance-constrained Game for Coordinating Renewable Microgrids with  Service Delivery Risk: A Bayesian Optimization Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ding%2C+Y">Yifu Ding</a>, 
<a href="/search/math?searchtype=author&query=Hobbs%2C+B">Benjamin Hobbs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Received and Revised in 2023; IEEE Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12772" title="Abstract">arXiv:2306.12772</a> (replaced) [<a href="/pdf/2306.12772" title="Download PDF">pdf</a>, <a href="/ps/2306.12772" title="Download PostScript">ps</a>, <a href="/format/2306.12772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the rate of convergence of Yosida approximation for the nonlocal  Cahn-Hilliard equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gwiazda%2C+P">Piotr Gwiazda</a>, 
<a href="/search/math?searchtype=author&query=Skrzeczkowski%2C+J">Jakub Skrzeczkowski</a>, 
<a href="/search/math?searchtype=author&query=Trussardi%2C+L">Lara Trussardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12951" title="Abstract">arXiv:2306.12951</a> (replaced) [<a href="/pdf/2306.12951" title="Download PDF">pdf</a>, <a href="/format/2306.12951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Public Attitudes Toward ChatGPT on Twitter: Sentiments, Topics, and  Occupations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koonchanok%2C+R">Ratanond Koonchanok</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yanling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+H">Hyeju Jang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14291" title="Abstract">arXiv:2306.14291</a> (replaced) [<a href="/pdf/2306.14291" title="Download PDF">pdf</a>, <a href="/format/2306.14291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic  Distance Enhances Open World Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doan%2C+T">Thang Doan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Behpour%2C+S">Sima Behpour</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wenbin He</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+L">Liang Gou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+L">Liu Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2024 || keywords: Open World Object Detection, Hyperbolic Distance, Unknown Detection, Deformable Transformers, Hierarchical Representation Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15056" title="Abstract">arXiv:2306.15056</a> (replaced) [<a href="/pdf/2306.15056" title="Download PDF">pdf</a>, <a href="/format/2306.15056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Differentially Private Model Training with Public Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lowy%2C+A">Andrew Lowy</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zeman Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tianjian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Razaviyayn%2C+M">Meisam Razaviyayn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V2 changed the title and added high-dimensional approximate semi-DP lower bounds
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16717" title="Abstract">arXiv:2306.16717</a> (replaced) [<a href="/pdf/2306.16717" title="Download PDF">pdf</a>, <a href="/format/2306.16717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Pathologies of Deep Heteroskedastic Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wong-Toi%2C+E">Eliot Wong-Toi</a>, 
<a href="/search/stat?searchtype=author&query=Boyd%2C+A">Alex Boyd</a>, 
<a href="/search/stat?searchtype=author&query=Fortuin%2C+V">Vincent Fortuin</a>, 
<a href="/search/stat?searchtype=author&query=Mandt%2C+S">Stephan Mandt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05432" title="Abstract">arXiv:2307.05432</a> (replaced) [<a href="/pdf/2307.05432" title="Download PDF">pdf</a>, <a href="/format/2307.05432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Learning with Lie Symmetries for Partial Differential  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mialon%2C+G">Gr&#xe9;goire Mialon</a>, 
<a href="/search/cs?searchtype=author&query=Garrido%2C+Q">Quentin Garrido</a>, 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+H">Hannah Lawrence</a>, 
<a href="/search/cs?searchtype=author&query=Rehman%2C+D">Danyal Rehman</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>, 
<a href="/search/cs?searchtype=author&query=Kiani%2C+B+T">Bobak T. Kiani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06556" title="Abstract">arXiv:2307.06556</a> (replaced) [<a href="/pdf/2307.06556" title="Download PDF">pdf</a>, <a href="/ps/2307.06556" title="Download PostScript">ps</a>, <a href="/format/2307.06556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metal Oxide-based Gas Sensor Array for the VOCs Analysis in Complex  Mixtures using Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Singh%2C+S">Shivam Singh</a>, 
<a href="/search/physics?searchtype=author&query=S%2C+S">Sajana S</a>, 
<a href="/search/physics?searchtype=author&query=Poornima">Poornima</a>, 
<a href="/search/physics?searchtype=author&query=Sreelekha%2C+G">Gajje Sreelekha</a>, 
<a href="/search/physics?searchtype=author&query=Adak%2C+C">Chandranath Adak</a>, 
<a href="/search/physics?searchtype=author&query=Shukla%2C+R+P">Rajendra P. Shukla</a>, 
<a href="/search/physics?searchtype=author&query=Kamble%2C+V">Vinayak Kamble</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applied Physics (physics.app-ph)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11747" title="Abstract">arXiv:2307.11747</a> (replaced) [<a href="/pdf/2307.11747" title="Download PDF">pdf</a>, <a href="/format/2307.11747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation of Turing machines with analytic discrete ODEs: FPTIME and  FPSPACE over the reals characterised with discrete ordinary differential  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blanc%2C+M">Manon Blanc</a>, 
<a href="/search/cs?searchtype=author&query=Bournez%2C+O">Olivier Bournez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2209.13404">arXiv:2209.13404</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Logic in Computer Science (cs.LO); Dynamical Systems (math.DS); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12156" title="Abstract">arXiv:2307.12156</a> (replaced) [<a href="/pdf/2307.12156" title="Download PDF">pdf</a>, <a href="/format/2307.12156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability Constrained Optimization in High IBR-Penetrated Power  Systems-Part II: Constraint Validation and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chu%2C+Z">Zhongda Chu</a>, 
<a href="/search/eess?searchtype=author&query=Teng%2C+F">Fei Teng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13763" title="Abstract">arXiv:2307.13763</a> (replaced) [<a href="/pdf/2307.13763" title="Download PDF">pdf</a>, <a href="/format/2307.13763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sobolev Space Regularised Pre Density Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kozdoba%2C+M">Mark Kozdoba</a>, 
<a href="/search/stat?searchtype=author&query=Perets%2C+B">Binyamin Perets</a>, 
<a href="/search/stat?searchtype=author&query=Mannor%2C+S">Shie Mannor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14223" title="Abstract">arXiv:2307.14223</a> (replaced) [<a href="/pdf/2307.14223" title="Download PDF">pdf</a>, <a href="/format/2307.14223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rewriting and Completeness of Sum-Over-Paths in Dyadic Fragments of  Quantum Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vilmart%2C+R">Renaud Vilmart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2205.02600">arXiv:2205.02600</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15506" title="Abstract">arXiv:2307.15506</a> (replaced) [<a href="/pdf/2307.15506" title="Download PDF">pdf</a>, <a href="/format/2307.15506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving image quality of sparse-view lung tumor CT images with U-Net
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ries%2C+A">Annika Ries</a>, 
<a href="/search/cs?searchtype=author&query=Dorosti%2C+T">Tina Dorosti</a>, 
<a href="/search/cs?searchtype=author&query=Thalhammer%2C+J">Johannes Thalhammer</a>, 
<a href="/search/cs?searchtype=author&query=Sasse%2C+D">Daniel Sasse</a>, 
<a href="/search/cs?searchtype=author&query=Sauter%2C+A">Andreas Sauter</a>, 
<a href="/search/cs?searchtype=author&query=Meurer%2C+F">Felix Meurer</a>, 
<a href="/search/cs?searchtype=author&query=Benne%2C+A">Ashley Benne</a>, 
<a href="/search/cs?searchtype=author&query=Lasser%2C+T">Tobias Lasser</a>, 
<a href="/search/cs?searchtype=author&query=Pfeiffer%2C+F">Franz Pfeiffer</a>, 
<a href="/search/cs?searchtype=author&query=Schaff%2C+F">Florian Schaff</a>, 
<a href="/search/cs?searchtype=author&query=Pfeiffer%2C+D">Daniela Pfeiffer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01937" title="Abstract">arXiv:2308.01937</a> (replaced) [<a href="/pdf/2308.01937" title="Download PDF">pdf</a>, <a href="/format/2308.01937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Data Protection with Compositional Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golatkar%2C+A">Aditya Golatkar</a>, 
<a href="/search/cs?searchtype=author&query=Achille%2C+A">Alessandro Achille</a>, 
<a href="/search/cs?searchtype=author&query=Swaminathan%2C+A">Ashwin Swaminathan</a>, 
<a href="/search/cs?searchtype=author&query=Soatto%2C+S">Stefano Soatto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06254" title="Abstract">arXiv:2308.06254</a> (replaced) [<a href="/pdf/2308.06254" title="Download PDF">pdf</a>, <a href="/format/2308.06254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Better-Than-1.6-Approximation for Prize-Collecting TSP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blauth%2C+J">Jannis Blauth</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+N">Nathan Klein</a>, 
<a href="/search/cs?searchtype=author&query=N%C3%A4gele%2C+M">Martin N&#xe4;gele</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06979" title="Abstract">arXiv:2308.06979</a> (replaced) [<a href="/pdf/2308.06979" title="Download PDF">pdf</a>, <a href="/format/2308.06979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fabbro%2C+G">Giorgio Fabbro</a>, 
<a href="/search/eess?searchtype=author&query=Uhlich%2C+S">Stefan Uhlich</a>, 
<a href="/search/eess?searchtype=author&query=Lai%2C+C">Chieh-Hsin Lai</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+W">Woosung Choi</a>, 
<a href="/search/eess?searchtype=author&query=Mart%C3%ADnez-Ram%C3%ADrez%2C+M">Marco Mart&#xed;nez-Ram&#xed;rez</a>, 
<a href="/search/eess?searchtype=author&query=Liao%2C+W">Weihsiang Liao</a>, 
<a href="/search/eess?searchtype=author&query=Gadelha%2C+I">Igor Gadelha</a>, 
<a href="/search/eess?searchtype=author&query=Ramos%2C+G">Geraldo Ramos</a>, 
<a href="/search/eess?searchtype=author&query=Hsu%2C+E">Eddie Hsu</a>, 
<a href="/search/eess?searchtype=author&query=Rodrigues%2C+H">Hugo Rodrigues</a>, 
<a href="/search/eess?searchtype=author&query=St%C3%B6ter%2C+F">Fabian-Robert St&#xf6;ter</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%A9fossez%2C+A">Alexandre D&#xe9;fossez</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+Y">Yi Luo</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+J">Jianwei Yu</a>, 
<a href="/search/eess?searchtype=author&query=Chakraborty%2C+D">Dipam Chakraborty</a>, 
<a href="/search/eess?searchtype=author&query=Mohanty%2C+S">Sharada Mohanty</a>, 
<a href="/search/eess?searchtype=author&query=Solovyev%2C+R">Roman Solovyev</a>, 
<a href="/search/eess?searchtype=author&query=Stempkovskiy%2C+A">Alexander Stempkovskiy</a>, 
<a href="/search/eess?searchtype=author&query=Habruseva%2C+T">Tatiana Habruseva</a>, 
<a href="/search/eess?searchtype=author&query=Goswami%2C+N">Nabarun Goswami</a>, 
<a href="/search/eess?searchtype=author&query=Harada%2C+T">Tatsuya Harada</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+M">Minseok Kim</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J+H">Jun Hyung Lee</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+Y">Yuanliang Dong</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xinran Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jiafeng Liu</a>, 
<a href="/search/eess?searchtype=author&query=Mitsufuji%2C+Y">Yuki Mitsufuji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Transactions of the International Society for Music Information Retrieval
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06981" title="Abstract">arXiv:2308.06981</a> (replaced) [<a href="/pdf/2308.06981" title="Download PDF">pdf</a>, <a href="/format/2308.06981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing  Track
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Uhlich%2C+S">Stefan Uhlich</a>, 
<a href="/search/eess?searchtype=author&query=Fabbro%2C+G">Giorgio Fabbro</a>, 
<a href="/search/eess?searchtype=author&query=Hirano%2C+M">Masato Hirano</a>, 
<a href="/search/eess?searchtype=author&query=Takahashi%2C+S">Shusuke Takahashi</a>, 
<a href="/search/eess?searchtype=author&query=Wichern%2C+G">Gordon Wichern</a>, 
<a href="/search/eess?searchtype=author&query=Roux%2C+J+L">Jonathan Le Roux</a>, 
<a href="/search/eess?searchtype=author&query=Chakraborty%2C+D">Dipam Chakraborty</a>, 
<a href="/search/eess?searchtype=author&query=Mohanty%2C+S">Sharada Mohanty</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+K">Kai Li</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+Y">Yi Luo</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+J">Jianwei Yu</a>, 
<a href="/search/eess?searchtype=author&query=Gu%2C+R">Rongzhi Gu</a>, 
<a href="/search/eess?searchtype=author&query=Solovyev%2C+R">Roman Solovyev</a>, 
<a href="/search/eess?searchtype=author&query=Stempkovskiy%2C+A">Alexander Stempkovskiy</a>, 
<a href="/search/eess?searchtype=author&query=Habruseva%2C+T">Tatiana Habruseva</a>, 
<a href="/search/eess?searchtype=author&query=Sukhovei%2C+M">Mikhail Sukhovei</a>, 
<a href="/search/eess?searchtype=author&query=Mitsufuji%2C+Y">Yuki Mitsufuji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for Transactions of the International Society for Music Information Retrieval
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12645" title="Abstract">arXiv:2308.12645</a> (replaced) [<a href="/pdf/2308.12645" title="Download PDF">pdf</a>, <a href="/format/2308.12645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An All Deep System for Badminton Game Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+P">Po-Yung Chou</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+Y">Yu-Chun Lo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+B">Bo-Zheng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Cheng-Hung Lin</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+Y">Yu-Yung Kao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Golden Award for IJCAI CoachAI Challenge 2023: Team NTNUEE AIoTLab
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03780" title="Abstract">arXiv:2309.03780</a> (replaced) [<a href="/pdf/2309.03780" title="Download PDF">pdf</a>, <a href="/format/2309.03780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduced Simulations for High-Energy Physics, a Middle Ground for  Data-Driven Physics Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ex?searchtype=author&query=Odyurt%2C+U">Uraz Odyurt</a>, 
<a href="/search/hep-ex?searchtype=author&query=Swatman%2C+S+N">Stephen Nicholas Swatman</a>, 
<a href="/search/hep-ex?searchtype=author&query=Varbanescu%2C+A">Ana-Lucia Varbanescu</a>, 
<a href="/search/hep-ex?searchtype=author&query=Caron%2C+S">Sascha Caron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Experiment (hep-ex)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03914" title="Abstract">arXiv:2309.03914</a> (replaced) [<a href="/pdf/2309.03914" title="Download PDF">pdf</a>, <a href="/format/2309.03914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DevGPT: Studying Developer-ChatGPT Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Treude%2C+C">Christoph Treude</a>, 
<a href="/search/cs?searchtype=author&query=Hata%2C+H">Hideaki Hata</a>, 
<a href="/search/cs?searchtype=author&query=Matsumoto%2C+K">Kenichi Matsumoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MSR 2024 Mining Challenge Proposal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05119" title="Abstract">arXiv:2309.05119</a> (replaced) [<a href="/pdf/2309.05119" title="Download PDF">pdf</a>, <a href="/ps/2309.05119" title="Download PostScript">ps</a>, <a href="/format/2309.05119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reaction-diffusion systems derived from kinetic theory for Multiple  Sclerosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Travaglini%2C+R">Romina Travaglini</a>, 
<a href="/search/math?searchtype=author&query=Oliveira%2C+J+M">Jo&#xe3;o Miguel Oliveira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11883" title="Abstract">arXiv:2309.11883</a> (replaced) [<a href="/pdf/2309.11883" title="Download PDF">pdf</a>, <a href="/ps/2309.11883" title="Download PostScript">ps</a>, <a href="/format/2309.11883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-the-Fly SfM: What you capture is What you get
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Z">Zongqian Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Rui Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yifei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yibo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13287" title="Abstract">arXiv:2309.13287</a> (replaced) [<a href="/pdf/2309.13287" title="Download PDF">pdf</a>, <a href="/format/2309.13287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conjunctive Queries on Probabilistic Graphs: The Limits of  Approximability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amarilli%2C+A">Antoine Amarilli</a>, 
<a href="/search/cs?searchtype=author&query=van+Bremen%2C+T">Timothy van Bremen</a>, 
<a href="/search/cs?searchtype=author&query=Meel%2C+K+S">Kuldeep S. Meel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages. Up to minor changes (including the correction of a minor error in the proof of Theorem 6.3), this article is identical to the ICDT'24 publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13602" title="Abstract">arXiv:2309.13602</a> (replaced) [<a href="/pdf/2309.13602" title="Download PDF">pdf</a>, <a href="/format/2309.13602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 6G Positioning and Sensing Through the Lens of Sustainability,  Inclusiveness, and Trustworthiness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wymeersch%2C+H">Henk Wymeersch</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+H">Hao Guo</a>, 
<a href="/search/eess?searchtype=author&query=Keskin%2C+M+F">Musa Furkan Keskin</a>, 
<a href="/search/eess?searchtype=author&query=Khorsandi%2C+B+M">Bahare M. Khorsandi</a>, 
<a href="/search/eess?searchtype=author&query=Moghaddam%2C+M+H">Mohammad H. Moghaddam</a>, 
<a href="/search/eess?searchtype=author&query=Ramirez%2C+A">Alejandro Ramirez</a>, 
<a href="/search/eess?searchtype=author&query=Schindhelm%2C+K">Kim Schindhelm</a>, 
<a href="/search/eess?searchtype=author&query=Stavridis%2C+A">Athanasios Stavridis</a>, 
<a href="/search/eess?searchtype=author&query=Svensson%2C+T">Tommy Svensson</a>, 
<a href="/search/eess?searchtype=author&query=Yajnanarayana%2C+V">Vijaya Yajnanarayana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted and under review for IEEE Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14309" title="Abstract">arXiv:2309.14309</a> (replaced) [<a href="/pdf/2309.14309" title="Download PDF">pdf</a>, <a href="/format/2309.14309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple Different Black Box Explanations for Image Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chockler%2C+H">Hana Chockler</a>, 
<a href="/search/cs?searchtype=author&query=Kelly%2C+D+A">David A. Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Kroening%2C+D">Daniel Kroening</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15274" title="Abstract">arXiv:2309.15274</a> (replaced) [<a href="/pdf/2309.15274" title="Download PDF">pdf</a>, <a href="/format/2309.15274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-Efficient Continual Learning Object Segmentation for Long Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nazemi%2C+A">Amir Nazemi</a>, 
<a href="/search/cs?searchtype=author&query=Shafiee%2C+M+J">Mohammad Javad Shafiee</a>, 
<a href="/search/cs?searchtype=author&query=Gharaee%2C+Z">Zahra Gharaee</a>, 
<a href="/search/cs?searchtype=author&query=Fieguth%2C+P">Paul Fieguth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15562" title="Abstract">arXiv:2309.15562</a> (replaced) [<a href="/pdf/2309.15562" title="Download PDF">pdf</a>, <a href="/format/2309.15562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from SAM: Harnessing a Foundation Model for Sim2Real Adaptation  by Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonani%2C+M+E">Mayara E. Bonani</a>, 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+M">Max Schwarz</a>, 
<a href="/search/cs?searchtype=author&query=Behnke%2C+S">Sven Behnke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15732" title="Abstract">arXiv:2309.15732</a> (replaced) [<a href="/pdf/2309.15732" title="Download PDF">pdf</a>, <a href="/ps/2309.15732" title="Download PostScript">ps</a>, <a href="/format/2309.15732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-based Analysis of Basins of Attraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valle%2C+D">David Valle</a>, 
<a href="/search/cs?searchtype=author&query=Wagemakers%2C+A">Alexandre Wagemakers</a>, 
<a href="/search/cs?searchtype=author&query=Sanju%C3%A1n%2C+M+A+F">Miguel A.F. Sanju&#xe1;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16036" title="Abstract">arXiv:2309.16036</a> (replaced) [<a href="/pdf/2309.16036" title="Download PDF">pdf</a>, <a href="/format/2309.16036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multichannel Voice Trigger Detection Based on  Transform-average-concatenate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Higuchi%2C+T">Takuya Higuchi</a>, 
<a href="/search/eess?searchtype=author&query=Brueggeman%2C+A">Avamarie Brueggeman</a>, 
<a href="/search/eess?searchtype=author&query=Delfarah%2C+M">Masood Delfarah</a>, 
<a href="/search/eess?searchtype=author&query=Shum%2C+S">Stephen Shum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at HSCMA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16597" title="Abstract">arXiv:2309.16597</a> (replaced) [<a href="/pdf/2309.16597" title="Download PDF">pdf</a>, <a href="/ps/2309.16597" title="Download PostScript">ps</a>, <a href="/format/2309.16597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer Learning for Bayesian Optimization on Heterogeneous Search  Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhou Fan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xinran Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zi Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (TMLR), February 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16779" title="Abstract">arXiv:2309.16779</a> (replaced) [<a href="/pdf/2309.16779" title="Download PDF">pdf</a>, <a href="/format/2309.16779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intriguing properties of generative classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaini%2C+P">Priyank Jaini</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+K">Kevin Clark</a>, 
<a href="/search/cs?searchtype=author&query=Geirhos%2C+R">Robert Geirhos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00076" title="Abstract">arXiv:2310.00076</a> (replaced) [<a href="/pdf/2310.00076" title="Download PDF">pdf</a>, <a href="/format/2310.00076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness of AI-Image Detectors: Fundamental Limits and Practical  Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saberi%2C+M">Mehrdad Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Sadasivan%2C+V+S">Vinu Sankar Sadasivan</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+K">Keivan Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aounon Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Chegini%2C+A">Atoosa Chegini</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02075" title="Abstract">arXiv:2310.02075</a> (replaced) [<a href="/pdf/2310.02075" title="Download PDF">pdf</a>, <a href="/format/2310.02075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Quantum Processes with Quantum Statistical Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wadhwa%2C+C">Chirag Wadhwa</a>, 
<a href="/search/quant-ph?searchtype=author&query=Doosti%2C+M">Mina Doosti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages, 3 figures. v2: Added lower bounds (Section 6)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03254" title="Abstract">arXiv:2310.03254</a> (replaced) [<a href="/pdf/2310.03254" title="Download PDF">pdf</a>, <a href="/format/2310.03254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-coresident family as a driver of migration change in a crisis: The  case of the COVID-19 pandemic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kan%2C+U">Unchitta Kan</a>, 
<a href="/search/cs?searchtype=author&query=McLeod%2C+J">Jericho McLeod</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez%2C+E">Eduardo L&#xf3;pez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04884" title="Abstract">arXiv:2310.04884</a> (replaced) [<a href="/pdf/2310.04884" title="Download PDF">pdf</a>, <a href="/ps/2310.04884" title="Download PostScript">ps</a>, <a href="/format/2310.04884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret Analysis of Repeated Delegated Choice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajiaghayi%2C+M">MohammadTaghi Hajiaghayi</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavi%2C+M">Mohammad Mahdavi</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+K">Keivan Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Suho Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06514" title="Abstract">arXiv:2310.06514</a> (replaced) [<a href="/pdf/2310.06514" title="Download PDF">pdf</a>, <a href="/format/2310.06514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AttributionLab: Faithfulness of Feature Attribution Under Controllable  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+H">Hannah Brown</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+M">Mina Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Bischl%2C+B">Bernd Bischl</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Khakzar%2C+A">Ashkan Khakzar</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appear at NeurIPS 2023 Workshop XAIA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08739" title="Abstract">arXiv:2310.08739</a> (replaced) [<a href="/pdf/2310.08739" title="Download PDF">pdf</a>, <a href="/format/2310.08739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voyager: MTD-Based Aggregation Protocol for Mitigating Poisoning Attacks  on DFL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Celdran%2C+A+H">Alberto Huertas Celdran</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+M">Michael Vuong</a>, 
<a href="/search/cs?searchtype=author&query=Bovet%2C+G">Gerome Bovet</a>, 
<a href="/search/cs?searchtype=author&query=Stiller%2C+B">Burkhard Stiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08748" title="Abstract">arXiv:2310.08748</a> (replaced) [<a href="/pdf/2310.08748" title="Download PDF">pdf</a>, <a href="/ps/2310.08748" title="Download PostScript">ps</a>, <a href="/format/2310.08748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary Dynamic Optimization and Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boulesnane%2C+A">Abdennour Boulesnane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a preprint of the following chapter: Abdennour Boulesnane, Evolutionary Dynamic Optimization and Machine Learning, published in Advanced Machine Learning with Evolutionary and Metaheuristic Techniques, Computational Intelligence Methods and Applications, edited by J. Valadi et al. (eds.),2024, Springer Nature
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08881" title="Abstract">arXiv:2310.08881</a> (replaced) [<a href="/pdf/2310.08881" title="Download PDF">pdf</a>, <a href="/ps/2310.08881" title="Download PostScript">ps</a>, <a href="/format/2310.08881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Resource Sharing via Dynamic Max-Min Fairness: Efficiency,  Robustness and Non-Stationarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fikioris%2C+G">Giannis Fikioris</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Siddhartha Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Tardos%2C+%C3%89">&#xc9;va Tardos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09196" title="Abstract">arXiv:2310.09196</a> (replaced) [<a href="/pdf/2310.09196" title="Download PDF">pdf</a>, <a href="/format/2310.09196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A 4-approximation algorithm for min max correlation clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heidrich%2C+H">Holger Heidrich</a>, 
<a href="/search/cs?searchtype=author&query=Irmai%2C+J">Jannik Irmai</a>, 
<a href="/search/cs?searchtype=author&query=Andres%2C+B">Bjoern Andres</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AISTATS 2024; 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09400" title="Abstract">arXiv:2310.09400</a> (replaced) [<a href="/pdf/2310.09400" title="Download PDF">pdf</a>, <a href="/format/2310.09400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Semantic Alignment in Recommendation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liangwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaolong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingdai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yueqing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09639" title="Abstract">arXiv:2310.09639</a> (replaced) [<a href="/pdf/2310.09639" title="Download PDF">pdf</a>, <a href="/format/2310.09639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPZero: Private Fine-Tuning of Language Models without Backpropagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingcong Li</a>, 
<a href="/search/cs?searchtype=author&query=Thekumparampil%2C+K+K">Kiran Koshy Thekumparampil</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sewoong Oh</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12065" title="Abstract">arXiv:2310.12065</a> (replaced) [<a href="/pdf/2310.12065" title="Download PDF">pdf</a>, <a href="/format/2310.12065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Persuasive Approach to Combating Misinformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+S">Safwan Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Mladenovic%2C+A">Andjela Mladenovic</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12115" title="Abstract">arXiv:2310.12115</a> (replaced) [<a href="/pdf/2310.12115" title="Download PDF">pdf</a>, <a href="/format/2310.12115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMD-based Variable Importance for Distributional Random Forest
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=B%C3%A9nard%2C+C">Cl&#xe9;ment B&#xe9;nard</a>, 
<a href="/search/stat?searchtype=author&query=N%C3%A4f%2C+J">Jeffrey N&#xe4;f</a>, 
<a href="/search/stat?searchtype=author&query=Josse%2C+J">Julie Josse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12294" title="Abstract">arXiv:2310.12294</a> (replaced) [<a href="/pdf/2310.12294" title="Download PDF">pdf</a>, <a href="/format/2310.12294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Set Multivariate Time-Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+T">Thomas Lai</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T+K+K">Thi Kieu Khanh Ho</a>, 
<a href="/search/cs?searchtype=author&query=Armanfard%2C+N">Narges Armanfard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14941" title="Abstract">arXiv:2310.14941</a> (replaced) [<a href="/pdf/2310.14941" title="Download PDF">pdf</a>, <a href="/ps/2310.14941" title="Download PostScript">ps</a>, <a href="/format/2310.14941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is dynamic dedicated path protection tractable?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Szcze%C5%9Bniak%2C+I">Ireneusz Szcze&#x15b;niak</a>, 
<a href="/search/cs?searchtype=author&query=Olszewski%2C+I">Ireneusz Olszewski</a>, 
<a href="/search/cs?searchtype=author&query=Wo%C5%BAna-Szcze%C5%9Bniak%2C+B">Bo&#x17c;ena Wo&#x17a;na-Szcze&#x15b;niak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14968" title="Abstract">arXiv:2310.14968</a> (replaced) [<a href="/pdf/2310.14968" title="Download PDF">pdf</a>, <a href="/format/2310.14968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Active Learning in the Presence of Nuisance Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sloman%2C+S+J">Sabina J. Sloman</a>, 
<a href="/search/cs?searchtype=author&query=Bharti%2C+A">Ayush Bharti</a>, 
<a href="/search/cs?searchtype=author&query=Martinelli%2C+J">Julien Martinelli</a>, 
<a href="/search/cs?searchtype=author&query=Kaski%2C+S">Samuel Kaski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15164" title="Abstract">arXiv:2310.15164</a> (replaced) [<a href="/pdf/2310.15164" title="Download PDF">pdf</a>, <a href="/format/2310.15164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LINC: A Neurosymbolic Approach for Logical Reasoning by Combining  Language Models with First-Order Logic Provers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olausson%2C+T+X">Theo X. Olausson</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+A">Alex Gu</a>, 
<a href="/search/cs?searchtype=author&query=Lipkin%2C+B">Benjamin Lipkin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C+E">Cedegao E. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Solar-Lezama%2C+A">Armando Solar-Lezama</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+R">Roger Levy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP Main 2023 (Outstanding Paper Award)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing, pages 5153-5176, Singapore. Association for
  Computational Linguistics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15450" title="Abstract">arXiv:2310.15450</a> (replaced) [<a href="/pdf/2310.15450" title="Download PDF">pdf</a>, <a href="/format/2310.15450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General Identifiability and Achievability for Causal Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Var%C4%B1c%C4%B1%2C+B">Burak Var&#x131;c&#x131;</a>, 
<a href="/search/cs?searchtype=author&query=Acart%C3%BCrk%2C+E">Emre Acart&#xfc;rk</a>, 
<a href="/search/cs?searchtype=author&query=Shanmugam%2C+K">Karthikeyan Shanmugam</a>, 
<a href="/search/cs?searchtype=author&query=Tajer%2C+A">Ali Tajer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AISTATS 2024 (oral presentation). Also appeared at CRL Workshop @ NeurIPS 2023 (oral presentation) titled as "Score-based Causal Representation Learning: Nonparametric Identifiability"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17534" title="Abstract">arXiv:2310.17534</a> (replaced) [<a href="/pdf/2310.17534" title="Download PDF">pdf</a>, <a href="/format/2310.17534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: Pitfalls in Evaluating Black-Box Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suya%2C+F">Fnu Suya</a>, 
<a href="/search/cs?searchtype=author&query=Suri%2C+A">Anshuman Suri</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jingtao Hong</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+D">David Evans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SaTML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19668" title="Abstract">arXiv:2310.19668</a> (replaced) [<a href="/pdf/2310.19668" title="Download PDF">pdf</a>, <a href="/format/2310.19668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DrM: Mastering Visual Reinforcement Learning through Dormant Ratio  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guowei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yongyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhecheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Tianying Ji</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiaxin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+P">Pu Hua</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuzhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Ze%2C+Y">Yanjie Ze</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at The Twelfth International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20310" title="Abstract">arXiv:2310.20310</a> (replaced) [<a href="/pdf/2310.20310" title="Download PDF">pdf</a>, <a href="/format/2310.20310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Conserving Higher Order Mixed Finite Element Discretizations of  Maxwell&#x27;s Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Arya%2C+A">Archana Arya</a>, 
<a href="/search/math?searchtype=author&query=Kalyanaraman%2C+K">Kaushik Kalyanaraman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In this version, we have added an additional computational example. We have also reorganized our examples to provide a thematic ordering to them. In addition, we fixed minor typographical errors throughout the document. Finally, we made a small change to the equations for the initialization of our implicit leapfrog scheme sot that the proof of Corollary 7.1 will work out correctly
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20605" title="Abstract">arXiv:2310.20605</a> (replaced) [<a href="/pdf/2310.20605" title="Download PDF">pdf</a>, <a href="/format/2310.20605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Lyapunov-Stable Polynomial Dynamical Systems Through Imitation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abyaneh%2C+A">Amin Abyaneh</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hsiu-Chin Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In 7th Annual Conference on Robot Learning 2023 Aug 30
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00800" title="Abstract">arXiv:2311.00800</a> (replaced) [<a href="/pdf/2311.00800" title="Download PDF">pdf</a>, <a href="/ps/2311.00800" title="Download PostScript">ps</a>, <a href="/format/2311.00800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond still images: Temporal features and input variance resilience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fadaei%2C+A+H">Amir Hosein Fadaei</a>, 
<a href="/search/cs?searchtype=author&query=Dehaqani%2C+M+A">Mohammad-Reza A. Dehaqani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00847" title="Abstract">arXiv:2311.00847</a> (replaced) [<a href="/pdf/2311.00847" title="Download PDF">pdf</a>, <a href="/format/2311.00847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signatures From Pseudorandom States via $\bot$-PRFs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barhoush%2C+M">Mohammed Barhoush</a>, 
<a href="/search/cs?searchtype=author&query=Behera%2C+A">Amit Behera</a>, 
<a href="/search/cs?searchtype=author&query=Ozer%2C+L">Lior Ozer</a>, 
<a href="/search/cs?searchtype=author&query=Salvail%2C+L">Louis Salvail</a>, 
<a href="/search/cs?searchtype=author&query=Sattath%2C+O">Or Sattath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00992" title="Abstract">arXiv:2311.00992</a> (replaced) [<a href="/pdf/2311.00992" title="Download PDF">pdf</a>, <a href="/ps/2311.00992" title="Download PostScript">ps</a>, <a href="/format/2311.00992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing random $r$-orthogonal Latin squares
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bereg%2C+S">Sergey Bereg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01517" title="Abstract">arXiv:2311.01517</a> (replaced) [<a href="/pdf/2311.01517" title="Download PDF">pdf</a>, <a href="/format/2311.01517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Infinite-Dimensional Continuum Robot States From the Tip
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tongjia Zheng</a>, 
<a href="/search/cs?searchtype=author&query=McFarland%2C+C">Ciera McFarland</a>, 
<a href="/search/cs?searchtype=author&query=Coad%2C+M">Margaret Coad</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hai Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02333" title="Abstract">arXiv:2311.02333</a> (replaced) [<a href="/pdf/2311.02333" title="Download PDF">pdf</a>, <a href="/format/2311.02333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Natural Language of DNA using Encoder-Decoder  Foundation Models with Byte-level Precision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malusare%2C+A">Aditya Malusare</a>, 
<a href="/search/cs?searchtype=author&query=Kothandaraman%2C+H">Harish Kothandaraman</a>, 
<a href="/search/cs?searchtype=author&query=Tamboli%2C+D">Dipesh Tamboli</a>, 
<a href="/search/cs?searchtype=author&query=Lanman%2C+N+A">Nadia A. Lanman</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Genomics (q-bio.GN)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02357" title="Abstract">arXiv:2311.02357</a> (replaced) [<a href="/pdf/2311.02357" title="Download PDF">pdf</a>, <a href="/format/2311.02357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Deep Nonnegative Matrix Factorization for Community  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuecheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jialong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024. Source Code available at: <a href="https://github.com/6lyc/CDNMF.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06668" title="Abstract">arXiv:2311.06668</a> (replaced) [<a href="/pdf/2311.06668" title="Download PDF">pdf</a>, <a href="/format/2311.06668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-context Vectors: Making In Context Learning More Effective and  Controllable Through Latent Space Steering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haotian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+L">Lei Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07454" title="Abstract">arXiv:2311.07454</a> (replaced) [<a href="/pdf/2311.07454" title="Download PDF">pdf</a>, <a href="/format/2311.07454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete Nonparametric Causal Discovery Under Latent Class Confounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazaheri%2C+B">Bijan Mazaheri</a>, 
<a href="/search/cs?searchtype=author&query=Gordon%2C+S">Spencer Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Rabani%2C+Y">Yuval Rabani</a>, 
<a href="/search/cs?searchtype=author&query=Schulman%2C+L">Leonard Schulman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07601" title="Abstract">arXiv:2311.07601</a> (replaced) [<a href="/pdf/2311.07601" title="Download PDF">pdf</a>, <a href="/format/2311.07601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Advertisements with LLMs: Opportunities and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>, 
<a href="/search/cs?searchtype=author&query=Hajiaghayi%2C+M">MohammadTaghi Hajiaghayi</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+K">Keivan Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Suho Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11342" title="Abstract">arXiv:2311.11342</a> (replaced) [<a href="/pdf/2311.11342" title="Download PDF">pdf</a>, <a href="/format/2311.11342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Communication Complexity of Decentralized Bilevel Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Thai%2C+M+T">My T. Thai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongchang Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14334" title="Abstract">arXiv:2311.14334</a> (replaced) [<a href="/pdf/2311.14334" title="Download PDF">pdf</a>, <a href="/format/2311.14334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximizing Discrimination Capability of Knowledge Distillation with  Energy Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seonghak Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ham%2C+G">Gyeongdo Ham</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Suin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+D">Donggon Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Daeshik Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 7 figures. This work has been submitted to the Elsevier for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15377" title="Abstract">arXiv:2311.15377</a> (replaced) [<a href="/pdf/2311.15377" title="Download PDF">pdf</a>, <a href="/format/2311.15377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increased Compute Efficiency and the Diffusion of AI Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pilz%2C+K">Konstantin Pilz</a>, 
<a href="/search/cs?searchtype=author&query=Heim%2C+L">Lennart Heim</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+N">Nicholas Brown</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15674" title="Abstract">arXiv:2311.15674</a> (replaced) [<a href="/pdf/2311.15674" title="Download PDF">pdf</a>, <a href="/format/2311.15674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOT-DETR: 3D Single Shot Detection and Tracking with Transformers to  build 3D representations for Agro-Food Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rapado-Rincon%2C+D">David Rapado-Rincon</a>, 
<a href="/search/cs?searchtype=author&query=Nap%2C+H">Henk Nap</a>, 
<a href="/search/cs?searchtype=author&query=Smolenova%2C+K">Katarina Smolenova</a>, 
<a href="/search/cs?searchtype=author&query=van+Henten%2C+E+J">Eldert J. van Henten</a>, 
<a href="/search/cs?searchtype=author&query=Kootstra%2C+G">Gert Kootstra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16856" title="Abstract">arXiv:2311.16856</a> (replaced) [<a href="/pdf/2311.16856" title="Download PDF">pdf</a>, <a href="/format/2311.16856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attentional Graph Neural Networks for Robust Massive Network  Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Wenzhong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Juntao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+F">Feng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zoubir%2C+A+M">Abdelhak M. Zoubir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17165" title="Abstract">arXiv:2311.17165</a> (replaced) [<a href="/pdf/2311.17165" title="Download PDF">pdf</a>, <a href="/ps/2311.17165" title="Download PostScript">ps</a>, <a href="/format/2311.17165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Ir)rationality in AI: State of the Art, Research Challenges and Open  Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Macmillan-Scott%2C+O">Olivia Macmillan-Scott</a>, 
<a href="/search/cs?searchtype=author&query=Musolesi%2C+M">Mirco Musolesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17438" title="Abstract">arXiv:2311.17438</a> (replaced) [<a href="/pdf/2311.17438" title="Download PDF">pdf</a>, <a href="/format/2311.17438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLOMO: Counterfactual Logical Modification with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+R">Ruixin Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhicheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Changshui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17515" title="Abstract">arXiv:2311.17515</a> (replaced) [<a href="/pdf/2311.17515" title="Download PDF">pdf</a>, <a href="/ps/2311.17515" title="Download PostScript">ps</a>, <a href="/format/2311.17515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion of Single and Integral Multispectral Aerial Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Youssef%2C+M">Mohamed Youssef</a>, 
<a href="/search/eess?searchtype=author&query=Bimber%2C+O">Oliver Bimber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17704" title="Abstract">arXiv:2311.17704</a> (replaced) [<a href="/pdf/2311.17704" title="Download PDF">pdf</a>, <a href="/format/2311.17704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Algorithm for Unbalanced 1D Transportation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gouvine%2C+G">Gabriel Gouvine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18138" title="Abstract">arXiv:2311.18138</a> (replaced) [<a href="/pdf/2311.18138" title="Download PDF">pdf</a>, <a href="/format/2311.18138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmic Persuasion Through Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harris%2C+K">Keegan Harris</a>, 
<a href="/search/cs?searchtype=author&query=Immorlica%2C+N">Nicole Immorlica</a>, 
<a href="/search/cs?searchtype=author&query=Lucier%2C+B">Brendan Lucier</a>, 
<a href="/search/cs?searchtype=author&query=Slivkins%2C+A">Aleksandrs Slivkins</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02203" title="Abstract">arXiv:2312.02203</a> (replaced) [<a href="/pdf/2312.02203" title="Download PDF">pdf</a>, <a href="/format/2312.02203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning High-Order Relationships of Brain Regions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Qiu%2C+W">Weikang Qiu</a>, 
<a href="/search/q-bio?searchtype=author&query=Chu%2C+H">Huangrui Chu</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+S">Selena Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zuo%2C+H">Haolan Zuo</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhao%2C+Y">Yize Zhao</a>, 
<a href="/search/q-bio?searchtype=author&query=Ying%2C+R">Rex Ying</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02544" title="Abstract">arXiv:2312.02544</a> (replaced) [<a href="/pdf/2312.02544" title="Download PDF">pdf</a>, <a href="/ps/2312.02544" title="Download PostScript">ps</a>, <a href="/format/2312.02544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterization of Locality in Spin States and Forced Moves for  Optimizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sato%2C+Y">Yoshiki Sato</a>, 
<a href="/search/physics?searchtype=author&query=Konoshima%2C+M">Makiko Konoshima</a>, 
<a href="/search/physics?searchtype=author&query=Tamura%2C+H">Hirotaka Tamura</a>, 
<a href="/search/physics?searchtype=author&query=Ohkubo%2C+J">Jun Ohkubo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applied Physics (physics.app-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05328" title="Abstract">arXiv:2312.05328</a> (replaced) [<a href="/pdf/2312.05328" title="Download PDF">pdf</a>, <a href="/format/2312.05328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bad Students Make Great Teachers: Active Learning Accelerates  Large-Scale Visual Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Evans%2C+T">Talfan Evans</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+S">Shreya Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Merzic%2C+H">Hamza Merzic</a>, 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+J">Jonathan Schwarz</a>, 
<a href="/search/cs?searchtype=author&query=Tanno%2C+R">Ryutaro Tanno</a>, 
<a href="/search/cs?searchtype=author&query=Henaff%2C+O+J">Olivier J. Henaff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05419" title="Abstract">arXiv:2312.05419</a> (replaced) [<a href="/pdf/2312.05419" title="Download PDF">pdf</a>, <a href="/ps/2312.05419" title="Download PostScript">ps</a>, <a href="/format/2312.05419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete-time Negative Imaginary Systems from ZOH Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shi%2C+K">Kanghong Shi</a>, 
<a href="/search/eess?searchtype=author&query=Petersen%2C+I+R">Ian R. Petersen</a>, 
<a href="/search/eess?searchtype=author&query=Vladimirov%2C+I+G">Igor G. Vladimirov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07540" title="Abstract">arXiv:2312.07540</a> (replaced) [<a href="/pdf/2312.07540" title="Download PDF">pdf</a>, <a href="/format/2312.07540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> diff History for Neural Language Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piterbarg%2C+U">Ulyana Piterbarg</a>, 
<a href="/search/cs?searchtype=author&query=Pinto%2C+L">Lerrel Pinto</a>, 
<a href="/search/cs?searchtype=author&query=Fergus%2C+R">Rob Fergus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08801" title="Abstract">arXiv:2312.08801</a> (replaced) [<a href="/pdf/2312.08801" title="Download PDF">pdf</a>, <a href="/format/2312.08801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Process Planning Based on a Semantic Capability Model and SMT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C3%B6cher%2C+A">Aljosha K&#xf6;cher</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+L+M+V">Luis Miguel Vieira da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Fay%2C+A">Alexander Fay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at CAIPI Workshop at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10323" title="Abstract">arXiv:2312.10323</a> (replaced) [<a href="/pdf/2312.10323" title="Download PDF">pdf</a>, <a href="/format/2312.10323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Prompt Generation from Linear Combination of Discrete Prompt  Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Passigan%2C+P">Pascal Passigan</a>, 
<a href="/search/cs?searchtype=author&query=Yohannes%2C+K">Kidus Yohannes</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+J">Joshua Pereira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10637" title="Abstract">arXiv:2312.10637</a> (replaced) [<a href="/pdf/2312.10637" title="Download PDF">pdf</a>, <a href="/format/2312.10637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Evaluation of GPT-4V and Gemini in Online VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chongyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gurari%2C+D">Danna Gurari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12276" title="Abstract">arXiv:2312.12276</a> (replaced) [<a href="/pdf/2312.12276" title="Download PDF">pdf</a>, <a href="/format/2312.12276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> POND: Multi-Source Time Series Domain Adaptation with Information-Aware  Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+G">Guangji Bai</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengzhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12720" title="Abstract">arXiv:2312.12720</a> (replaced) [<a href="/pdf/2312.12720" title="Download PDF">pdf</a>, <a href="/format/2312.12720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdvST: Revisiting Data Augmentations for Single Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guangtao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huai%2C+M">Mengdi Huai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aidong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14440" title="Abstract">arXiv:2312.14440</a> (replaced) [<a href="/pdf/2312.14440" title="Download PDF">pdf</a>, <a href="/format/2312.14440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahgir%2C+H+S">Haz Sameen Shahgir</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xianghao Kong</a>, 
<a href="/search/cs?searchtype=author&query=Steeg%2C+G+V">Greg Ver Steeg</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yue Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15326" title="Abstract">arXiv:2312.15326</a> (replaced) [<a href="/pdf/2312.15326" title="Download PDF">pdf</a>, <a href="/ps/2312.15326" title="Download PostScript">ps</a>, <a href="/format/2312.15326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Connected Strongly-Proportional Cake-Cutting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jank%C3%B3%2C+Z">Zsuzsanna Jank&#xf3;</a>, 
<a href="/search/math?searchtype=author&query=Jo%C3%B3%2C+A">Attila Jo&#xf3;</a>, 
<a href="/search/math?searchtype=author&query=Segal-Halevi%2C+E">Erel Segal-Halevi</a>, 
<a href="/search/math?searchtype=author&query=Yuen%2C+S+M">Sheung Man Yuen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We have a polynomial-time characterization for hungry agents, and an exponential-time characterization for general agents. Can you find a polynomial-time characterization for general agents?
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computer Science and Game Theory (cs.GT); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16307" title="Abstract">arXiv:2312.16307</a> (replaced) [<a href="/pdf/2312.16307" title="Download PDF">pdf</a>, <a href="/format/2312.16307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation  via Incentivized Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Ngo%2C+D">Daniel Ngo</a>, 
<a href="/search/econ?searchtype=author&query=Harris%2C+K">Keegan Harris</a>, 
<a href="/search/econ?searchtype=author&query=Agarwal%2C+A">Anish Agarwal</a>, 
<a href="/search/econ?searchtype=author&query=Syrgkanis%2C+V">Vasilis Syrgkanis</a>, 
<a href="/search/econ?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17058" title="Abstract">arXiv:2312.17058</a> (replaced) [<a href="/pdf/2312.17058" title="Download PDF">pdf</a>, <a href="/ps/2312.17058" title="Download PostScript">ps</a>, <a href="/format/2312.17058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the optimality of Shapley mechanism for funding public excludable  goods under Sybil strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazorra%2C+B">Bruno Mazorra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17528" title="Abstract">arXiv:2312.17528</a> (replaced) [<a href="/pdf/2312.17528" title="Download PDF">pdf</a>, <a href="/ps/2312.17528" title="Download PostScript">ps</a>, <a href="/format/2312.17528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing the Role of Complex Power in Small-Signal Synchronization  Stability of Multi-Converter Power Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ma%2C+F">Fuyilong Ma</a>, 
<a href="/search/eess?searchtype=author&query=Xin%2C+H">Huanhai Xin</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhiyi Li</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+L">Linbin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00608" title="Abstract">arXiv:2401.00608</a> (replaced) [<a href="/pdf/2401.00608" title="Download PDF">pdf</a>, <a href="/format/2401.00608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bringing Back the Context: Camera Trap Species Identification as Link  Prediction on Multimodal Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pahuja%2C+V">Vardaan Pahuja</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weidi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yu Gu</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+C">Cheng-Hao Tu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong-You Chen</a>, 
<a href="/search/cs?searchtype=author&query=Berger-Wolf%2C+T">Tanya Berger-Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Stewart%2C+C">Charles Stewart</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Song Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+W">Wei-Lun Chao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01552" title="Abstract">arXiv:2401.01552</a> (replaced) [<a href="/pdf/2401.01552" title="Download PDF">pdf</a>, <a href="/format/2401.01552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRA-PCN: Point Cloud Completion with Intra- and Inter-level  Cross-Resolution Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rong%2C+Y">Yi Rong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haoran Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lixin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+C">Cheng Mei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tong Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04857" title="Abstract">arXiv:2401.04857</a> (replaced) [<a href="/pdf/2401.04857" title="Download PDF">pdf</a>, <a href="/format/2401.04857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transportation Marketplace Rate Forecast Using Signature Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+H">Haotian Gu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+T+L">Timothy L. Jacobs</a>, 
<a href="/search/cs?searchtype=author&query=Kaminsky%2C+P">Philip Kaminsky</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinyu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05845" title="Abstract">arXiv:2401.05845</a> (replaced) [<a href="/pdf/2401.05845" title="Download PDF">pdf</a>, <a href="/ps/2401.05845" title="Download PostScript">ps</a>, <a href="/format/2401.05845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Reconstruction via MIS Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konrad%2C+C">Christian Konrad</a>, 
<a href="/search/cs?searchtype=author&query=O%27Sullivan%2C+C">Conor O&#x27;Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Traistaru%2C+V">Victor Traistaru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Lower bound for deterministic query algorithms added in this version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06072" title="Abstract">arXiv:2401.06072</a> (replaced) [<a href="/pdf/2401.06072" title="Download PDF">pdf</a>, <a href="/format/2401.06072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain of History: Learning and Forecasting with LLMs for Temporal  Knowledge Graph Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+R">Ruilin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+T">Tianle Gu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoling Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zicheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiayi Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages; typos corrected, references added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06184" title="Abstract">arXiv:2401.06184</a> (replaced) [<a href="/pdf/2401.06184" title="Download PDF">pdf</a>, <a href="/ps/2401.06184" title="Download PostScript">ps</a>, <a href="/format/2401.06184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyclic and Negacyclic Codes with Optimal and Best Known Minimum  Distances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yanan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08097" title="Abstract">arXiv:2401.08097</a> (replaced) [<a href="/pdf/2401.08097" title="Download PDF">pdf</a>, <a href="/format/2401.08097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study of Fairness Concerns in AI-based Mobile App Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasab%2C+A+R">Ali Rezaei Nasab</a>, 
<a href="/search/cs?searchtype=author&query=Dashti%2C+M">Maedeh Dashti</a>, 
<a href="/search/cs?searchtype=author&query=Shahin%2C+M">Mojtaba Shahin</a>, 
<a href="/search/cs?searchtype=author&query=Zahedi%2C+M">Mansooreh Zahedi</a>, 
<a href="/search/cs?searchtype=author&query=Khalajzadeh%2C+H">Hourieh Khalajzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+C">Chetan Arora</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Peng Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 4 images, 2 tables, Manuscript submitted to a Journal (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08273" title="Abstract">arXiv:2401.08273</a> (replaced) [<a href="/pdf/2401.08273" title="Download PDF">pdf</a>, <a href="/format/2401.08273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Null-Shot Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taveekitworachai%2C+P">Pittawat Taveekitworachai</a>, 
<a href="/search/cs?searchtype=author&query=Abdullah%2C+F">Febri Abdullah</a>, 
<a href="/search/cs?searchtype=author&query=Thawonmas%2C+R">Ruck Thawonmas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages; added Gemini Pro results, error analysis, and a discussion on confabulation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08685" title="Abstract">arXiv:2401.08685</a> (replaced) [<a href="/pdf/2401.08685" title="Download PDF">pdf</a>, <a href="/ps/2401.08685" title="Download PostScript">ps</a>, <a href="/format/2401.08685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Apple Vision Pro: Comments in Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+E">Ezequiel Santos</a>, 
<a href="/search/cs?searchtype=author&query=Castillo%2C+V">Vanessa Castillo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09339" title="Abstract">arXiv:2401.09339</a> (replaced) [<a href="/pdf/2401.09339" title="Download PDF">pdf</a>, <a href="/format/2401.09339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Central Limit Theorem for Two-Timescale Stochastic Approximation with  Markovian Noise: Theory and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hu%2C+J">Jie Hu</a>, 
<a href="/search/stat?searchtype=author&query=Doshi%2C+V">Vishwaraj Doshi</a>, 
<a href="/search/stat?searchtype=author&query=Eun%2C+D+Y">Do Young Eun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09686" title="Abstract">arXiv:2401.09686</a> (replaced) [<a href="/pdf/2401.09686" title="Download PDF">pdf</a>, <a href="/format/2401.09686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study on the Impact of Positional Encoding in  Transformer-based Monaural Speech Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Q">Qiquan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Ge%2C+M">Meng Ge</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+H">Hongxu Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Ambikairajah%2C+E">Eliathamby Ambikairajah</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+Q">Qi Song</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+Z">Zhaoheng Ni</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10187" title="Abstract">arXiv:2401.10187</a> (replaced) [<a href="/pdf/2401.10187" title="Download PDF">pdf</a>, <a href="/format/2401.10187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Kronecker Matrix-Matrix Multiplication on GPUs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jangda%2C+A">Abhinav Jangda</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+M">Mohit Yadav</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at PPoPP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11624" title="Abstract">arXiv:2401.11624</a> (replaced) [<a href="/pdf/2401.11624" title="Download PDF">pdf</a>, <a href="/format/2401.11624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-context Learning with Retrieved Demonstrations for Language Models: A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Man Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pasupat%2C+P">Panupong Pasupat</a>, 
<a href="/search/cs?searchtype=author&query=Kazemi%2C+M">Mehran Kazemi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12490" title="Abstract">arXiv:2401.12490</a> (replaced) [<a href="/pdf/2401.12490" title="Download PDF">pdf</a>, <a href="/ps/2401.12490" title="Download PostScript">ps</a>, <a href="/format/2401.12490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A low-rank augmented Lagrangian method for large-scale semidefinite  programming based on a hybrid convex-nonconvex approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Monteiro%2C+R+D+C">Renato D.C. Monteiro</a>, 
<a href="/search/math?searchtype=author&query=Sujanani%2C+A">Arnesh Sujanani</a>, 
<a href="/search/math?searchtype=author&query=Cifuentes%2C+D">Diego Cifuentes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12513" title="Abstract">arXiv:2401.12513</a> (replaced) [<a href="/pdf/2401.12513" title="Download PDF">pdf</a>, <a href="/format/2401.12513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT  and SimCLR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Turnbull%2C+R">Robert Turnbull</a>, 
<a href="/search/cs?searchtype=author&query=Mannix%2C+E">Evelyn Mannix</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13839" title="Abstract">arXiv:2401.13839</a> (replaced) [<a href="/pdf/2401.13839" title="Download PDF">pdf</a>, <a href="/format/2401.13839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge-coloring sparse graphs with $&#x394;$ colors in quasilinear time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kowalik%2C+L">Lukasz Kowalik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14579" title="Abstract">arXiv:2401.14579</a> (replaced) [<a href="/pdf/2401.14579" title="Download PDF">pdf</a>, <a href="/ps/2401.14579" title="Download PostScript">ps</a>, <a href="/format/2401.14579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recognizing Multiple Ingredients in Food Images Using a  Single-Ingredient Classification Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+K">Kun Fu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Ying Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 21 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15857" title="Abstract">arXiv:2401.15857</a> (replaced) [<a href="/e-print/2401.15857" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leadership Dynamics in Social Multiplex Networks with Mono and  Bi-directional Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Talebi%2C+A">Amirreza Talebi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This submission has not been submitted to any conference or journal. There are serious technical problems in the submission which is being revised and the replacement is a going to be completely different from the previous versions. In addition, there are new contributors who are not ok for the previous versions to be shown
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16263" title="Abstract">arXiv:2401.16263</a> (replaced) [<a href="/pdf/2401.16263" title="Download PDF">pdf</a>, <a href="/format/2401.16263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaboration Petri Nets: Verification, Equivalence, and Discovery  (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benzin%2C+J">Janik-Vasily Benzin</a>, 
<a href="/search/cs?searchtype=author&query=Rinderle-Ma%2C+S">Stefanie Rinderle-Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00133" title="Abstract">arXiv:2402.00133</a> (replaced) [<a href="/pdf/2402.00133" title="Download PDF">pdf</a>, <a href="/ps/2402.00133" title="Download PostScript">ps</a>, <a href="/format/2402.00133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Constant-Depth Circuit Complexity of Generating Quasigroups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Collins%2C+N+A">Nathaniel A. Collins</a>, 
<a href="/search/cs?searchtype=author&query=Grochow%2C+J+A">Joshua A. Grochow</a>, 
<a href="/search/cs?searchtype=author&query=Levet%2C+M">Michael Levet</a>, 
<a href="/search/cs?searchtype=author&query=Wei%C3%9F%2C+A">Armin Wei&#xdf;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS); Combinatorics (math.CO); Group Theory (math.GR)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00467" title="Abstract">arXiv:2402.00467</a> (replaced) [<a href="/pdf/2402.00467" title="Download PDF">pdf</a>, <a href="/format/2402.00467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can you see me now? Blind spot estimation for autonomous vehicles using  scenario-based simulation with random reference sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uecker%2C+M">Marc Uecker</a>, 
<a href="/search/cs?searchtype=author&query=Z%C3%B6llner%2C+J+M">J.Marius Z&#xf6;llner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00522" title="Abstract">arXiv:2402.00522</a> (replaced) [<a href="/pdf/2402.00522" title="Download PDF">pdf</a>, <a href="/ps/2402.00522" title="Download PostScript">ps</a>, <a href="/format/2402.00522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Expressive Power and Mechanisms of Transformer for  Sequence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingze Wang</a>, 
<a href="/search/cs?searchtype=author&query=E%2C+W">Weinan E</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 65 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00910" title="Abstract">arXiv:2402.00910</a> (replaced) [<a href="/pdf/2402.00910" title="Download PDF">pdf</a>, <a href="/format/2402.00910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radwan%2C+A">Ahmed Radwan</a>, 
<a href="/search/cs?searchtype=author&query=Zaafarani%2C+L">Layan Zaafarani</a>, 
<a href="/search/cs?searchtype=author&query=Abudawood%2C+J">Jetana Abudawood</a>, 
<a href="/search/cs?searchtype=author&query=AlZahrani%2C+F">Faisal AlZahrani</a>, 
<a href="/search/cs?searchtype=author&query=Fourati%2C+F">Fares Fourati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01207" title="Abstract">arXiv:2402.01207</a> (replaced) [<a href="/pdf/2402.01207" title="Download PDF">pdf</a>, <a href="/format/2402.01207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Causal Graph Discovery Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiralerspong%2C+T">Thomas Jiralerspong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoyin Chen</a>, 
<a href="/search/cs?searchtype=author&query=More%2C+Y">Yash More</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+V">Vedant Shah</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01274" title="Abstract">arXiv:2402.01274</a> (replaced) [<a href="/pdf/2402.01274" title="Download PDF">pdf</a>, <a href="/format/2402.01274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heggan%2C+C">Calum Heggan</a>, 
<a href="/search/cs?searchtype=author&query=Budgett%2C+S">Sam Budgett</a>, 
<a href="/search/cs?searchtype=author&query=Hospedales%2C+T">Timothy Hospedales</a>, 
<a href="/search/cs?searchtype=author&query=Yaghoobi%2C+M">Mehrdad Yaghoobi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready version as submitted to ICASSP SASB Workshop 2024. 5 pages, 2 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01535" title="Abstract">arXiv:2402.01535</a> (replaced) [<a href="/pdf/2402.01535" title="Download PDF">pdf</a>, <a href="/format/2402.01535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Analysis of Diversity in Argument Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Meer%2C+M">Michiel van der Meer</a>, 
<a href="/search/cs?searchtype=author&query=Vossen%2C+P">Piek Vossen</a>, 
<a href="/search/cs?searchtype=author&query=Jonker%2C+C+M">Catholijn M. Jonker</a>, 
<a href="/search/cs?searchtype=author&query=Murukannaiah%2C+P+K">Pradeep K. Murukannaiah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL2024 (main proceedings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01677" title="Abstract">arXiv:2402.01677</a> (replaced) [<a href="/pdf/2402.01677" title="Download PDF">pdf</a>, <a href="/format/2402.01677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Ontologies via Incorporating Extensional and Intensional  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Keyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaoyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianxing Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitting to IJCAI2024; 9 pages and 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01721" title="Abstract">arXiv:2402.01721</a> (replaced) [<a href="/pdf/2402.01721" title="Download PDF">pdf</a>, <a href="/format/2402.01721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Consensual Synthetic Intimate Imagery: Prevalence, Attitudes, and  Knowledge in 10 Countries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Umbach%2C+R">Rebecca Umbach</a>, 
<a href="/search/cs?searchtype=author&query=Henry%2C+N">Nicola Henry</a>, 
<a href="/search/cs?searchtype=author&query=Beard%2C+G">Gemma Beard</a>, 
<a href="/search/cs?searchtype=author&query=Berryessa%2C+C">Colleen Berryessa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02162" title="Abstract">arXiv:2402.02162</a> (replaced) [<a href="/pdf/2402.02162" title="Download PDF">pdf</a>, <a href="/format/2402.02162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bayesian cluster validity index
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wiroonsri%2C+N">Nathakhun Wiroonsri</a>, 
<a href="/search/stat?searchtype=author&query=Preedasawakul%2C+O">Onthada Preedasawakul</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02304" title="Abstract">arXiv:2402.02304</a> (replaced) [<a href="/pdf/2402.02304" title="Download PDF">pdf</a>, <a href="/format/2402.02304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Numerical Wave Propagation Enhanced By An End-to-End Deep  Learning Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kaiser%2C+L">Luis Kaiser</a>, 
<a href="/search/math?searchtype=author&query=Tsai%2C+R">Richard Tsai</a>, 
<a href="/search/math?searchtype=author&query=Klingenberg%2C+C">Christian Klingenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03487" title="Abstract">arXiv:2402.03487</a> (replaced) [<a href="/pdf/2402.03487" title="Download PDF">pdf</a>, <a href="/format/2402.03487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shooting Methods for Fractional Dirichlet-Type Boundary Value Problems  of Order $&#x3b1;\in (1,2)$ With Caputo Derivatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Diethelm%2C+K">Kai Diethelm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03781" title="Abstract">arXiv:2402.03781</a> (replaced) [<a href="/pdf/2402.03781" title="Download PDF">pdf</a>, <a href="/format/2402.03781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MolTC: Towards Molecular Relational Modeling In Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Fang%2C+J">Junfeng Fang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+C">Chang Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Yang%2C+Z">Zhengyi Yang</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S">Sihang Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Du%2C+W">Wenjie Du</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+X">Xiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03966" title="Abstract">arXiv:2402.03966</a> (replaced) [<a href="/pdf/2402.03966" title="Download PDF">pdf</a>, <a href="/format/2402.03966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On dimensionality of feature vectors in MPNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bravo%2C+C">C&#xe9;sar Bravo</a>, 
<a href="/search/cs?searchtype=author&query=Kozachinskiy%2C+A">Alexander Kozachinskiy</a>, 
<a href="/search/cs?searchtype=author&query=Rojas%2C+C">Crist&#xf3;bal Rojas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 figures. Changes to the previous version: added reference to Amir et al.~(NeurIPS'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04139" title="Abstract">arXiv:2402.04139</a> (replaced) [<a href="/pdf/2402.04139" title="Download PDF">pdf</a>, <a href="/format/2402.04139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> U-shaped Vision Mamba for Single Image Dehazing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuoran Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chen Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04222" title="Abstract">arXiv:2402.04222</a> (replaced) [<a href="/pdf/2402.04222" title="Download PDF">pdf</a>, <a href="/format/2402.04222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What is &#x27;Typological Diversity&#x27; in NLP?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ploeger%2C+E">Esther Ploeger</a>, 
<a href="/search/cs?searchtype=author&query=Poelman%2C+W">Wessel Poelman</a>, 
<a href="/search/cs?searchtype=author&query=de+Lhoneux%2C+M">Miryam de Lhoneux</a>, 
<a href="/search/cs?searchtype=author&query=Bjerva%2C+J">Johannes Bjerva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04362" title="Abstract">arXiv:2402.04362</a> (replaced) [<a href="/pdf/2402.04362" title="Download PDF">pdf</a>, <a href="/format/2402.04362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Networks Learn Statistics of Increasing Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belrose%2C+N">Nora Belrose</a>, 
<a href="/search/cs?searchtype=author&query=Pope%2C+Q">Quintin Pope</a>, 
<a href="/search/cs?searchtype=author&query=Quirke%2C+L">Lucia Quirke</a>, 
<a href="/search/cs?searchtype=author&query=Mallen%2C+A">Alex Mallen</a>, 
<a href="/search/cs?searchtype=author&query=Fern%2C+X">Xiaoli Fern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04838" title="Abstract">arXiv:2402.04838</a> (replaced) [<a href="/pdf/2402.04838" title="Download PDF">pdf</a>, <a href="/format/2402.04838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jinghui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuejing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Can Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05048" title="Abstract">arXiv:2402.05048</a> (replaced) [<a href="/pdf/2402.05048" title="Download PDF">pdf</a>, <a href="/format/2402.05048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How VADER is your AI? Towards a definition of artificial intelligence  systems appropriate for regulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bezerra%2C+L+C+T">Leonardo C. T. Bezerra</a>, 
<a href="/search/cs?searchtype=author&query=Brownlee%2C+A+E+I">Alexander E. I. Brownlee</a>, 
<a href="/search/cs?searchtype=author&query=Alvarenga%2C+L+F">Luana Ferraz Alvarenga</a>, 
<a href="/search/cs?searchtype=author&query=Moioli%2C+R+C">Renan Cipriano Moioli</a>, 
<a href="/search/cs?searchtype=author&query=Batista%2C+T+V">Thais Vasconcelos Batista</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05050" title="Abstract">arXiv:2402.05050</a> (replaced) [<a href="/pdf/2402.05050" title="Download PDF">pdf</a>, <a href="/format/2402.05050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning Can Find Friends That Are Beneficial
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tupitsa%2C+N">Nazarii Tupitsa</a>, 
<a href="/search/cs?searchtype=author&query=Horv%C3%A1th%2C+S">Samuel Horv&#xe1;th</a>, 
<a href="/search/cs?searchtype=author&query=Tak%C3%A1%C4%8D%2C+M">Martin Tak&#xe1;&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Gorbunov%2C+E">Eduard Gorbunov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05128" title="Abstract">arXiv:2402.05128</a> (replaced) [<a href="/pdf/2402.05128" title="Download PDF">pdf</a>, <a href="/format/2402.05128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Textbook Question Answering Task with Large Language Models  and Retrieval Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alawwad%2C+H+A">Hessa Abdulrahman Alawwad</a>, 
<a href="/search/cs?searchtype=author&query=Alhothali%2C+A">Areej Alhothali</a>, 
<a href="/search/cs?searchtype=author&query=Naseem%2C+U">Usman Naseem</a>, 
<a href="/search/cs?searchtype=author&query=Alkhathlan%2C+A">Ali Alkhathlan</a>, 
<a href="/search/cs?searchtype=author&query=Jamal%2C+A">Amani Jamal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06044" title="Abstract">arXiv:2402.06044</a> (replaced) [<a href="/pdf/2402.06044" title="Download PDF">pdf</a>, <a href="/format/2402.06044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind  Reasoning Capabilities of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hainiu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Runcong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lixing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jinhua Du</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06187" title="Abstract">arXiv:2402.06187</a> (replaced) [<a href="/pdf/2402.06187" title="Download PDF">pdf</a>, <a href="/format/2402.06187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask  Representation via Temporal Action-Driven Contrastive Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yongyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shuang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Langford%2C+J">John Langford</a>, 
<a href="/search/cs?searchtype=author&query=Palanisamy%2C+P">Praveen Palanisamy</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+K+S">Kalyan Shankar Basu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06378" title="Abstract">arXiv:2402.06378</a> (replaced) [<a href="/pdf/2402.06378" title="Download PDF">pdf</a>, <a href="/format/2402.06378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FD-Vision Mamba for Endoscopic Exposure Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuoran Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2402.04139">arXiv:2402.04139</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06819" title="Abstract">arXiv:2402.06819</a> (replaced) [<a href="/pdf/2402.06819" title="Download PDF">pdf</a>, <a href="/format/2402.06819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monitored Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parisi%2C+S">Simone Parisi</a>, 
<a href="/search/cs?searchtype=author&query=Mohammedalamen%2C+M">Montaser Mohammedalamen</a>, 
<a href="/search/cs?searchtype=author&query=Kazemipour%2C+A">Alireza Kazemipour</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+M+E">Matthew E. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Bowling%2C+M">Michael Bowling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAMAS 2024, Main Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07354" title="Abstract">arXiv:2402.07354</a> (replaced) [<a href="/pdf/2402.07354" title="Download PDF">pdf</a>, <a href="/format/2402.07354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-DiffiNet: Modeling discrepancies loss in tumor segmentation using  diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ren%2C+T">Tianyi Ren</a>, 
<a href="/search/eess?searchtype=author&query=Sharma%2C+A">Abhishek Sharma</a>, 
<a href="/search/eess?searchtype=author&query=Rivera%2C+J+H">Juampablo Heras Rivera</a>, 
<a href="/search/eess?searchtype=author&query=Rebala%2C+H">Harshitha Rebala</a>, 
<a href="/search/eess?searchtype=author&query=Honey%2C+E">Ethan Honey</a>, 
<a href="/search/eess?searchtype=author&query=Chopra%2C+A">Agamdeep Chopra</a>, 
<a href="/search/eess?searchtype=author&query=Ruzevick%2C+J">Jacob Ruzevick</a>, 
<a href="/search/eess?searchtype=author&query=Kurt%2C+M">Mehmet Kurt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07440" title="Abstract">arXiv:2402.07440</a> (replaced) [<a href="/pdf/2402.07440" title="Download PDF">pdf</a>, <a href="/format/2402.07440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking and Building Long-Context Retrieval Models with LoCo and  M2-BERT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saad-Falcon%2C+J">Jon Saad-Falcon</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Simran Arora</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+N">Neel Guha</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9%2C+C">Christopher R&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07441" title="Abstract">arXiv:2402.07441</a> (replaced) [<a href="/pdf/2402.07441" title="Download PDF">pdf</a>, <a href="/ps/2402.07441" title="Download PostScript">ps</a>, <a href="/format/2402.07441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Dynamic Geometric Vertex Cover and Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhore%2C+S">Sujoy Bhore</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+M">Timothy M. Chan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07477" title="Abstract">arXiv:2402.07477</a> (replaced) [<a href="/pdf/2402.07477" title="Download PDF">pdf</a>, <a href="/format/2402.07477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Food Recommendation as Language Processing (F-RLP): A Personalized and  Contextual Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rostami%2C+A">Ali Rostami</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Ramesh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07703" title="Abstract">arXiv:2402.07703</a> (replaced) [<a href="/pdf/2402.07703" title="Download PDF">pdf</a>, <a href="/format/2402.07703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Sequential Decision-Making with Unknown Delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Ping Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengyang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07744" title="Abstract">arXiv:2402.07744</a> (replaced) [<a href="/pdf/2402.07744" title="Download PDF">pdf</a>, <a href="/format/2402.07744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unified Alignment Between Agents, Humans, and Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonghan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">An Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zijun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kaiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+F">Fangzhou Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zeyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qingyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fuwen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhicheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project webpage: <a href="https://agent-force.github.io/unified-alignment-for-agents.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07799" title="Abstract">arXiv:2402.07799</a> (replaced) [<a href="/pdf/2402.07799" title="Download PDF">pdf</a>, <a href="/format/2402.07799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalising Planning Environment Redesign
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pozanco%2C+A">Alberto Pozanco</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+R+F">Ramon Fraga Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Borrajo%2C+D">Daniel Borrajo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted at AAAI'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08068" title="Abstract">arXiv:2402.08068</a> (replaced) [<a href="/pdf/2402.08068" title="Download PDF">pdf</a>, <a href="/ps/2402.08068" title="Download PostScript">ps</a>, <a href="/format/2402.08068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Blocklace: A Universal, Byzantine Fault-Tolerant, Conflict-free  Replicated Data Type
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almeida%2C+P+S">Paulo S&#xe9;rgio Almeida</a>, 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+E">Ehud Shapiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08082" title="Abstract">arXiv:2402.08082</a> (replaced) [<a href="/pdf/2402.08082" title="Download PDF">pdf</a>, <a href="/ps/2402.08082" title="Download PostScript">ps</a>, <a href="/format/2402.08082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-based generative models break the curse of dimensionality in  learning a family of sub-Gaussian probability distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cole%2C+F">Frank Cole</a>, 
<a href="/search/stat?searchtype=author&query=Lu%2C+Y">Yulong Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, to appear in the proceedings of 12th International Conference on Learning Representations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08095" title="Abstract">arXiv:2402.08095</a> (replaced) [<a href="/pdf/2402.08095" title="Download PDF">pdf</a>, <a href="/ps/2402.08095" title="Download PostScript">ps</a>, <a href="/format/2402.08095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence Analysis of Discrete Diffusion Model: Exact Implementation  through Uniformization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+H">Hongrui Chen</a>, 
<a href="/search/stat?searchtype=author&query=Ying%2C+L">Lexing Ying</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08113" title="Abstract">arXiv:2402.08113</a> (replaced) [<a href="/pdf/2402.08113" title="Download PDF">pdf</a>, <a href="/format/2402.08113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing cognitive bias in medical language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmidgall%2C+S">Samuel Schmidgall</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+C">Carl Harris</a>, 
<a href="/search/cs?searchtype=author&query=Essien%2C+I">Ime Essien</a>, 
<a href="/search/cs?searchtype=author&query=Olshvang%2C+D">Daniel Olshvang</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+T">Tawsifur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J+W">Ji Woong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ziaei%2C+R">Rojin Ziaei</a>, 
<a href="/search/cs?searchtype=author&query=Eshraghian%2C+J">Jason Eshraghian</a>, 
<a href="/search/cs?searchtype=author&query=Abadir%2C+P">Peter Abadir</a>, 
<a href="/search/cs?searchtype=author&query=Chellappa%2C+R">Rama Chellappa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08132" title="Abstract">arXiv:2402.08132</a> (replaced) [<a href="/pdf/2402.08132" title="Download PDF">pdf</a>, <a href="/ps/2402.08132" title="Download PostScript">ps</a>, <a href="/format/2402.08132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Resurgence of Recurrent Models for Long Sequences -- Survey and  Research Opportunities in the Transformer Era
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiezzi%2C+M">Matteo Tiezzi</a>, 
<a href="/search/cs?searchtype=author&query=Casoni%2C+M">Michele Casoni</a>, 
<a href="/search/cs?searchtype=author&query=Betti%2C+A">Alessandro Betti</a>, 
<a href="/search/cs?searchtype=author&query=Guidi%2C+T">Tommaso Guidi</a>, 
<a href="/search/cs?searchtype=author&query=Gori%2C+M">Marco Gori</a>, 
<a href="/search/cs?searchtype=author&query=Melacci%2C+S">Stefano Melacci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08228" title="Abstract">arXiv:2402.08228</a> (replaced) [<a href="/pdf/2402.08228" title="Download PDF">pdf</a>, <a href="/format/2402.08228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Out-of-Distribution Generalization of GNNs: An  Architecture Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+K">Kai Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hongzhi Wen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yaming Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08303" title="Abstract">arXiv:2402.08303</a> (replaced) [<a href="/pdf/2402.08303" title="Download PDF">pdf</a>, <a href="/format/2402.08303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatCell: Facilitating Single-Cell Analysis with Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kangwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xinle Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Penghui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiaohui Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; 15 pages, 6 Tables, 9 Figures; Project homepage: <a href="https://zjunlp.github.io/project/ChatCell">this https URL</a> Code: <a href="https://github.com/zjunlp/ChatCell">this https URL</a> Dataset: <a href="https://huggingface.co/datasets/zjunlp/ChatCell-Instructions">this https URL</a> Demo: <a href="https://huggingface.co/spaces/zjunlp/Chatcell">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08309" title="Abstract">arXiv:2402.08309</a> (replaced) [<a href="/pdf/2402.08309" title="Download PDF">pdf</a>, <a href="/format/2402.08309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompted Contextual Vectors for Spear-Phishing Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nahmias%2C+D">Daniel Nahmias</a>, 
<a href="/search/cs?searchtype=author&query=Engelberg%2C+G">Gal Engelberg</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Shabtai%2C+A">Asaf Shabtai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08431" title="Abstract">arXiv:2402.08431</a> (replaced) [<a href="/pdf/2402.08431" title="Download PDF">pdf</a>, <a href="/format/2402.08431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Java Methods: An Empirical Assessment of Four AI-Based Code  Assistants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corso%2C+V">Vincenzo Corso</a>, 
<a href="/search/cs?searchtype=author&query=Mariani%2C+L">Leonardo Mariani</a>, 
<a href="/search/cs?searchtype=author&query=Micucci%2C+D">Daniela Micucci</a>, 
<a href="/search/cs?searchtype=author&query=Riganelli%2C+O">Oliviero Riganelli</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 32nd IEEE/ACM International Conference on
  Program Comprehension (ICPC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08479" title="Abstract">arXiv:2402.08479</a> (replaced) [<a href="/pdf/2402.08479" title="Download PDF">pdf</a>, <a href="/format/2402.08479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plausible Extractive Rationalization through Semi-Supervised Entailment  Signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jie%2C+Y+W">Yeo Wei Jie</a>, 
<a href="/search/cs?searchtype=author&query=Satapathy%2C+R">Ranjan Satapathy</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08571" title="Abstract">arXiv:2402.08571</a> (replaced) [<a href="/pdf/2402.08571" title="Download PDF">pdf</a>, <a href="/format/2402.08571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Glass Segmentation with Multi Scales and Primary Prediction Guiding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingliang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08601" title="Abstract">arXiv:2402.08601</a> (replaced) [<a href="/pdf/2402.08601" title="Download PDF">pdf</a>, <a href="/format/2402.08601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Inversion with Timestep-aware Sampling for Training-free  Non-rigid Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+Y">Yunji Jung</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seokju Lee</a>, 
<a href="/search/cs?searchtype=author&query=Djanibekov%2C+T">Tair Djanibekov</a>, 
<a href="/search/cs?searchtype=author&query=Shim%2C+H">Hyunjung Shim</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08638" title="Abstract">arXiv:2402.08638</a> (replaced) [<a href="/pdf/2402.08638" title="Download PDF">pdf</a>, <a href="/format/2402.08638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ousidhoum%2C+N">Nedjma Ousidhoum</a>, 
<a href="/search/cs?searchtype=author&query=Muhammad%2C+S+H">Shamsuddeen Hassan Muhammad</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+M">Mohamed Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Abdulmumin%2C+I">Idris Abdulmumin</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+I+S">Ibrahim Said Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+S">Sanchit Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>, 
<a href="/search/cs?searchtype=author&query=Araujo%2C+V">Vladimir Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Ayele%2C+A+A">Abinew Ali Ayele</a>, 
<a href="/search/cs?searchtype=author&query=Baswani%2C+P">Pavan Baswani</a>, 
<a href="/search/cs?searchtype=author&query=Beloucif%2C+M">Meriem Beloucif</a>, 
<a href="/search/cs?searchtype=author&query=Biemann%2C+C">Chris Biemann</a>, 
<a href="/search/cs?searchtype=author&query=Bourhim%2C+S">Sofia Bourhim</a>, 
<a href="/search/cs?searchtype=author&query=De+Kock%2C+C">Christine De Kock</a>, 
<a href="/search/cs?searchtype=author&query=Dekebo%2C+G+S">Genet Shanko Dekebo</a>, 
<a href="/search/cs?searchtype=author&query=Hourrane%2C+O">Oumaima Hourrane</a>, 
<a href="/search/cs?searchtype=author&query=Kanumolu%2C+G">Gopichand Kanumolu</a>, 
<a href="/search/cs?searchtype=author&query=Madasu%2C+L">Lokesh Madasu</a>, 
<a href="/search/cs?searchtype=author&query=Rutunda%2C+S">Samuel Rutunda</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+M">Manish Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Solorio%2C+T">Thamar Solorio</a>, 
<a href="/search/cs?searchtype=author&query=Surange%2C+N">Nirmal Surange</a>, 
<a href="/search/cs?searchtype=author&query=Tilaye%2C+H+G">Hailegnaw Getaneh Tilaye</a>, 
<a href="/search/cs?searchtype=author&query=Vishnubhotla%2C+K">Krishnapriya Vishnubhotla</a>, 
<a href="/search/cs?searchtype=author&query=Winata%2C+G">Genta Winata</a>, 
<a href="/search/cs?searchtype=author&query=Yimam%2C+S+M">Seid Muhie Yimam</a>, 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+S+M">Saif M. Mohammad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item327">Cross-lists</a></li>
<li><a href="#item390">Replacements</a></li>
</ul>
<small>[ total of 619 entries:  <b>1-619</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
